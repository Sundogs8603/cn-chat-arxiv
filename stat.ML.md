# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Credal Learning Theory](https://rss.arxiv.org/abs/2402.00957) | 本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。 |
| [^2] | [Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds](https://arxiv.org/abs/2403.00715) | 通过引入竞争分析框架，我们提出了调整FTRL学习率的更新规则，使其在常数因子内达到最佳竞争比，并且展示了当惩罚项具有近似单调性时的竞争比特性。 |
| [^3] | [Defining Expertise: Applications to Treatment Effect Estimation](https://arxiv.org/abs/2403.00694) | 专家决策者的行动自然地编码了其领域知识的一部分，可以帮助在同一领域内进行推断，从而在治疗效果估计中利用专业知识作为归纳偏差可能是有益的。 |
| [^4] | [Scalable Learning of Item Response Theory Models](https://arxiv.org/abs/2403.00680) | 该论文提出了一种从大数据中学习项目反应理论模型中的潜在变量的方法，利用这些模型与逻辑回归之间的相似性来提高计算的效率和可伸缩性。 |
| [^5] | [Sharp bounds for the max-sliced Wasserstein distance](https://arxiv.org/abs/2403.00666) | 对于可分希尔伯特空间上的概率测度与其经验分布之间的最大切片1-Wasserstein距离，得到了尖锐的上下界限。 |
| [^6] | [Indirectly Parameterized Concrete Autoencoders](https://arxiv.org/abs/2403.00563) | 本文提出了间接参数化CAEs（IP-CAEs）来解决具体自编码器（CAEs）在稳定联合优化方面的问题，IP-CAEs在多个数据集上表现出显著且一致的改进。 |
| [^7] | [Epsilon-Greedy Thompson Sampling to Bayesian Optimization](https://arxiv.org/abs/2403.00540) | 将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。 |
| [^8] | [Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis](https://arxiv.org/abs/2403.00423) | 本研究探讨了ML-UQ校准统计量的使用问题，发现一些统计量对于生成分布的选择过于敏感，可能影响校准诊断。 |
| [^9] | [Shifted Interpolation for Differential Privacy](https://arxiv.org/abs/2403.00278) | 本文在统一框架下建立了“通过迭代实现隐私放大”现象，提高了先前分析的水平，并由此获得了其他差分隐私概念更紧密的隐私核算。 |
| [^10] | ["Lossless" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach](https://arxiv.org/abs/2403.00258) | 本研究利用神经切线核和随机矩阵理论，提出了一种新颖的压缩方法，能够在高维度情形下对宽而全连接的深度神经网络进行有效压缩。 |
| [^11] | [Causal Bandits with General Causal Models and Interventions](https://arxiv.org/abs/2403.00233) | 该论文在三个方向上推进了因果赌博机的结果：在一般因果模型下进行干预设计，实现广义软干预以及提供一般的遗憾上下界。 |
| [^12] | [On Cyclical MCMC Sampling](https://arxiv.org/abs/2403.00230) | 循环MCMC是一种新的MCMC框架，可以很好地估计目标分布的局部形状，但在慢混合核的情况下可能无法产生来自期望分布的样本。 |
| [^13] | [Substitute adjustment via recovery of latent variables](https://arxiv.org/abs/2403.00202) | 通过恢复潜变量实现替代调整，提出了一种估计调整后的回归目标参数的方法，为条件独立情况下的回归变量提供了一种替代调整算法。 |
| [^14] | [Entry-Specific Bounds for Low-Rank Matrix Completion under Highly Non-Uniform Sampling](https://arxiv.org/abs/2403.00184) | 该论文提出了针对高度非均匀采样条件下低秩矩阵完成问题的入口特定界限，通过定制每个条目的误差上界来匹配一定条件下的极小下界。 |
| [^15] | [Transformer-based Parameter Estimation in Statistics](https://arxiv.org/abs/2403.00019) | 提出了一种基于Transformer的参数估计方法，相较于传统方法，不需要封闭形式解或数学推导，也不需要概率密度函数，仅需经过训练的Transformer模型进行一次推断即可估计潜在分布的参数。 |
| [^16] | [PIP-Net: Pedestrian Intention Prediction in the Wild](https://arxiv.org/abs/2402.12810) | PIP-Net是一个新型框架，通过综合利用动态学数据和场景空间特征，采用循环和时间注意力机制解决方案，成功预测行人通过马路的意图，性能优于现有技术。 |
| [^17] | [How to validate average calibration for machine learning regression tasks ?](https://arxiv.org/abs/2402.10043) | 本文提出了两种验证机器学习回归任务平均校准性的方法，将校准误差与平均绝对误差之间的差值和将平均平方z-分数与1进行比较。研究发现，前者对不确定性分布敏感，而后者在该方面提供了最可靠的方法。 |
| [^18] | [On Rate-Optimal Partitioning Classification from Observable and from Privatised Data](https://arxiv.org/abs/2312.14889) | 研究了在放宽条件下的分区分类方法的收敛速率，提出了绝对连续分量的新特性，计算了分类错误概率的精确收敛率 |
| [^19] | [Distributional Bellman Operators over Mean Embeddings](https://arxiv.org/abs/2312.07358) | 提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。 |
| [^20] | [Neural Additive Models for Location Scale and Shape: A Framework for Interpretable Neural Regression Beyond the Mean](https://arxiv.org/abs/2301.11862) | 提出了神经加性模型具有位置尺度和形状的神经加性模型(NAMLSS)框架，结合了经典深度学习模型的预测能力和分布回归的优势 |
| [^21] | [Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees](https://arxiv.org/abs/2210.01282) | 本论文提出了一种单循环估计算法，具有有限时间保证，能够处理高维状态空间的马尔可夫决策过程的结构估计问题，而不会损害奖励估计精度。 |
| [^22] | [Bayesian Robust Optimization for Imitation Learning](https://arxiv.org/abs/2007.12315) | 提出了基于贝叶斯鲁棒优化的模仿学习方法，以在状态不确定性下同时考虑激进和保守的策略优化。 |
| [^23] | [Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control](https://arxiv.org/abs/1909.12077) | 通过设计哈密顿动力学与控制，Symplectic ODE-Net (SymODEN)可以透明地学习潜在动力学，从而揭示系统的相关物理方面。 |
| [^24] | [Optimal Transport for Measures with Noisy Tree Metric.](http://arxiv.org/abs/2310.13653) | 本文提出了一种针对树度量有噪声的优化传输方法，通过引入新的不确定性集合，解决了实际应用中树结构扰动的问题。 |
| [^25] | [A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model.](http://arxiv.org/abs/2310.11143) | 本研究提出了一种基于机器学习的概率暴露模型，可以更准确地估计德国室内氡气分布，并具有更高的空间分辨率。 |
| [^26] | [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation.](http://arxiv.org/abs/2310.02304) | 本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。 |
| [^27] | [Implicit regularization of deep residual networks towards neural ODEs.](http://arxiv.org/abs/2309.01213) | 本文建立了深度残差网络向神经常微分方程的隐式正则化，通过对用梯度流训练的非线性网络的研究，证明了在网络以神经常微分方程的离散化形式初始化后，这种离散化将在整个训练过程中保持不变，并提供了收敛性的条件。 |
| [^28] | [Spectral Ranking Inferences based on General Multiway Comparisons.](http://arxiv.org/abs/2308.02918) | 本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。 |
| [^29] | [Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data.](http://arxiv.org/abs/2308.01839) | 本文提出了一种谱流形对齐和推断（SMAI）框架，通过提供一种统计检验方法来确定单细胞数据集之间的对齐性，避免误导性推断，并保持数据整合的结构和可解释性。 |
| [^30] | [Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training.](http://arxiv.org/abs/2306.08173) | 本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。 |
| [^31] | [Global universal approximation of functional input maps on weighted spaces.](http://arxiv.org/abs/2306.03303) | 本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。 |
| [^32] | [The Risks of Recourse in Binary Classification.](http://arxiv.org/abs/2306.00497) | 研究发现，在二分分类中提供追索权会增加错误率，导致更多错误的发生。提供算法追索权可能也会在系统级别上给予不利。 |
| [^33] | [Bures-Wasserstein Means of Graphs.](http://arxiv.org/abs/2305.19738) | 该论文提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。实验结果表明，在各种任务中都有很好的表现。 |
| [^34] | [Escaping mediocrity: how two-layer networks learn hard single-index models with SGD.](http://arxiv.org/abs/2305.18502) | 研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。 |
| [^35] | [Hierarchical clustering with dot products recovers hidden tree structure.](http://arxiv.org/abs/2305.15022) | 本文发现一种基于点积的层次聚类算法，可以通过最大平均点积合并聚类，并且输出的树结构可用于准确估计数据的生成层次结构，树形恢复性能优于现有方法。 |
| [^36] | [CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models.](http://arxiv.org/abs/2305.14916) | 本文提出了两种无需调参的基于粒子的变分推断算法，其中一种是通过考虑边缘最大似然估计为自由能泛函最小化得到的，另一种是用于优化该问题的算法，完全无需调参。 |
| [^37] | [Global Convergence Rate of Deep Equilibrium Models with General Activations.](http://arxiv.org/abs/2302.05797) | 该论文研究了具有一般激活函数的深度平衡模型（DEQ）的全局收敛速度，证明了梯度下降以线性收敛速度收敛到全局最优解，并解决了限制平衡点Gram矩阵最小特征值的挑战。 |
| [^38] | [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits.](http://arxiv.org/abs/2301.12357) | 本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。 |
| [^39] | [Making SGD Parameter-Free.](http://arxiv.org/abs/2205.02160) | 该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。 |
| [^40] | [Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring.](http://arxiv.org/abs/1907.02677) | 本文扩展了多变量大数据分析（MBDA）方法，提出了一种自动推导特征的解决方案，结合可解释性和交互式模型的优势以及并行处理的能力，应用于网络监测和诊断，最终在UGR'16和Dartmouth'18两个数据集上取得成功。 |

# 详细

[^1]: 信任学习理论

    Credal Learning Theory

    [https://rss.arxiv.org/abs/2402.00957](https://rss.arxiv.org/abs/2402.00957)

    本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。

    

    统计学习理论是机器学习的基础，为从未知概率分布中学习到的模型的风险提供理论边界。然而，在实际部署中，数据分布可能会变化，导致领域适应/泛化问题。在本文中，我们建立了一个“信任”学习理论的基础，使用概率的凸集（信任集）来建模数据生成分布的变异性。我们认为，这样的信任集可以从有限样本的训练集中推断出来。对于有限假设空间（无论是否可实现）和无限模型空间，推导出界限，这直接推广了经典结果。

    Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
    
[^2]: 自适应学习率的FTRL算法的竞争比分析和最佳方案研究

    Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds

    [https://arxiv.org/abs/2403.00715](https://arxiv.org/abs/2403.00715)

    通过引入竞争分析框架，我们提出了调整FTRL学习率的更新规则，使其在常数因子内达到最佳竞争比，并且展示了当惩罚项具有近似单调性时的竞争比特性。

    

    Follow-The-Regularized-Leader (FTRL)被认为是在线学习中一种有效且多功能的方法，其中学习率的恰当选择对于减小后悔是至关重要的。为此，我们将调整FTRL学习率的问题构建为一个顺序决策问题，并引入竞争分析框架。我们建立了竞争比的下界，并提出了学习率的更新规则，使其在一个常数因子内达到下界的上界。具体地，我们说明了最优竞争比是由惩罚项的组成部分的（近似）单调性所决定的，表明如果惩罚项的组成部分形成单调非增序列，则可以实现常数竞争比，并推导出了在惩罚项$\xi$近似单调非增时的紧密竞争比。我们提出的更新规则被称为...

    arXiv:2403.00715v1 Announce Type: new  Abstract: Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL's learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to a
    
[^3]: 定义专业知识：在治疗效果估计中的应用

    Defining Expertise: Applications to Treatment Effect Estimation

    [https://arxiv.org/abs/2403.00694](https://arxiv.org/abs/2403.00694)

    专家决策者的行动自然地编码了其领域知识的一部分，可以帮助在同一领域内进行推断，从而在治疗效果估计中利用专业知识作为归纳偏差可能是有益的。

    

    决策者通常是其领域的专家，并基于其领域知识采取行动。本文讨论了在治疗效果估计领域中专业知识的重要性，以及利用专业知识作为归纳偏差的潜力。

    arXiv:2403.00694v1 Announce Type: cross  Abstract: Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and "expertise" is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise - particularly the type of expertise the decision-makers of a domain are likely to have - can be informative in designin
    
[^4]: 可扩展的项目反应理论模型学习

    Scalable Learning of Item Response Theory Models

    [https://arxiv.org/abs/2403.00680](https://arxiv.org/abs/2403.00680)

    该论文提出了一种从大数据中学习项目反应理论模型中的潜在变量的方法，利用这些模型与逻辑回归之间的相似性来提高计算的效率和可伸缩性。

    

    项目反应理论（IRT）模型旨在评估 $n$ 名考生的潜在能力以及 $m$ 个测验项目的隐含难度特征，这些项目是从表明其对应答案质量的分类数据中得出的。传统的心理测量评估基于相对较少的考生和项目，例如一个由 $200$ 名学生解决包含 $10$ 道题目的考试的班级。而近年来的全球大规模评估，如PISA，或互联网研究，可能导致参与者数量显著增加。此外，在机器学习领域，算法扮演考生角色，数据分析问题扮演项目角色，$n$ 和 $m$ 都可能变得非常大，挑战计算的效率和可伸缩性。为了从大数据中学习IRT模型中的潜在变量，我们利用这些模型与逻辑回归之间的相似性，后者可以使用s准确地近似。

    arXiv:2403.00680v1 Announce Type: new  Abstract: Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using s
    
[^5]: 最大切片Wasserstein距离的尖锐界限

    Sharp bounds for the max-sliced Wasserstein distance

    [https://arxiv.org/abs/2403.00666](https://arxiv.org/abs/2403.00666)

    对于可分希尔伯特空间上的概率测度与其经验分布之间的最大切片1-Wasserstein距离，得到了尖锐的上下界限。

    

    我们得到了关于在可分希尔伯特空间上的概率测度与从$n$个样本中获得的经验分布之间期望的最大切片1-Wasserstein距离的尖锐上下界。我们还得到了一个适用于Banach空间上的概率测度的版本。

    arXiv:2403.00666v1 Announce Type: cross  Abstract: We obtain sharp upper and lower bounds for the expected max-sliced 1-Wasserstein distance between a probability measure on a separable Hilbert space and its empirical distribution from $n$ samples. A version of this result for probability measures on Banach spaces is also obtained.
    
[^6]: 间接参数化具体自编码器

    Indirectly Parameterized Concrete Autoencoders

    [https://arxiv.org/abs/2403.00563](https://arxiv.org/abs/2403.00563)

    本文提出了间接参数化CAEs（IP-CAEs）来解决具体自编码器（CAEs）在稳定联合优化方面的问题，IP-CAEs在多个数据集上表现出显著且一致的改进。

    

    特征选择在数据高维或获取完整特征集成本高昂的情况下至关重要。最近基于神经网络的嵌入式特征选择的发展在广泛应用中表现出有希望的结果。具体自编码器（CAEs）被认为是嵌入式特征选择中的最先进技术，但可能难以实现稳定的联合优化，从而影响其训练时间和泛化能力。本文发现这种不稳定性与CAE学习重复选择有关。为了解决这个问题，我们提出了一种简单有效的改进：间接参数化CAEs（IP-CAEs）。IP-CAEs学习一个嵌入和从它到Gumbel-Softmax分布参数的映射。尽管实现简单，IP-CAE在多个数据集上的重构和分类任务中均表现出显著且一致的改进，无论是在泛化还是训练时间上。

    arXiv:2403.00563v1 Announce Type: new  Abstract: Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classifi
    
[^7]: Epsilon-Greedy Thompson Sampling用于贝叶斯优化

    Epsilon-Greedy Thompson Sampling to Bayesian Optimization

    [https://arxiv.org/abs/2403.00540](https://arxiv.org/abs/2403.00540)

    将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。

    

    Thompson采样（TS）被认为是解决贝叶斯优化中开发-探索困境的解决方案。 虽然它通过随机生成和最大化高斯过程（GP）后验的样本路径来优先进行探索，但TS在每次执行探索后通过收集关于真实目标函数的信息来弱化其开发功能。 本研究将在TS中引入$\varepsilon$-greedy策略，这是一种在强化学习中被广泛应用的选择策略，以改进其开发功能。 我们首先描述了TS应用于BO的两个极端，即通用TS和样本平均TS。前者和后者分别提倡探索和开发。 然后我们使用$\varepsilon$-greedy策略在两个极端之间随机切换。 $\varepsilon \in (0,1)$的小值优先考虑开发，反之亦然。 我们实证表明$\varepsilon$-greedy T

    arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
    
[^8]: 使用模拟参考值验证ML-UQ校准统计量：一项敏感性分析

    Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis

    [https://arxiv.org/abs/2403.00423](https://arxiv.org/abs/2403.00423)

    本研究探讨了ML-UQ校准统计量的使用问题，发现一些统计量对于生成分布的选择过于敏感，可能影响校准诊断。

    

    一些流行的机器学习不确定性量化（ML-UQ）校准统计量没有预定义的参考值，主要用于比较研究。因此，校准几乎从不被验证，诊断留给读者的判断。提出了基于实际不确定性导出的合成校准数据集的模拟参考值，以弥补这一问题。由于用于模拟合成误差的生成概率分布通常没有约束，所以模拟参考值对生成分布选择的敏感性可能会成为问题，对校准诊断产生怀疑。本研究探讨了这一问题的各个方面，并显示一些统计量对于用于验证时生成分布的选择过于敏感，当生成分布未知时。例如，

    arXiv:2403.00423v1 Announce Type: cross  Abstract: Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration statistics do not have predefined reference values and are mostly used in comparative studies. In consequence, calibration is almost never validated and the diagnostic is left to the appreciation of the reader. Simulated reference values, based on synthetic calibrated datasets derived from actual uncertainties, have been proposed to palliate this problem. As the generative probability distribution for the simulation of synthetic errors is often not constrained, the sensitivity of simulated reference values to the choice of generative distribution might be problematic, shedding a doubt on the calibration diagnostic. This study explores various facets of this problem, and shows that some statistics are excessively sensitive to the choice of generative distribution to be used for validation when the generative distribution is unknown. This is the case, for instan
    
[^9]: 差分隐私的平移插值

    Shifted Interpolation for Differential Privacy

    [https://arxiv.org/abs/2403.00278](https://arxiv.org/abs/2403.00278)

    本文在统一框架下建立了“通过迭代实现隐私放大”现象，提高了先前分析的水平，并由此获得了其他差分隐私概念更紧密的隐私核算。

    

    喧嚣的梯度下降及其变种是差分隐私机器学习中主导的算法。量化它们的隐私泄漏是一个基本问题，然而即使在凸损失的基础设置中，紧致的表征仍然是开放的。本文通过在$f$-差分隐私的统一框架下建立（和改进）“通过迭代实现隐私放大”现象，提高了先前分析的水平--这种方法紧紧捕捉了隐私损失的所有方面，并立即获得了其他差分隐私概念（如$(\varepsilon,\delta)$-DP和Renyi DP）更紧密的隐私核算。我们的关键技术见解是构建了揭示了流行的平移散度论证的平移插值过程，使得超越基于散度的差分隐私放宽的泛化成为可能。值得注意的是，这导致了在强凸基础设置中的第一个精确隐私分析。

    arXiv:2403.00278v1 Announce Type: new  Abstract: Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy--which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex
    
[^10]: “无损”压缩深度神经网络：一种高维神经切线核方法

    "Lossless" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach

    [https://arxiv.org/abs/2403.00258](https://arxiv.org/abs/2403.00258)

    本研究利用神经切线核和随机矩阵理论，提出了一种新颖的压缩方法，能够在高维度情形下对宽而全连接的深度神经网络进行有效压缩。

    

    现代深度神经网络（DNNs）非常强大；然而，这是以增加深度并使每一层的参数更多为代价的，这使得它们的训练和推断变得更具挑战性。为了应对这一关键限制，人们致力于对这些大规模机器学习模型进行压缩（如稀疏化和/或量化），以便它们可以部署在低功耗的物联网设备上。本文基于神经切线核（NTK）和随机矩阵理论（RMT）的最新进展，提出了一种新颖的压缩方法，适用于宽而全连接的\emph{深}神经网络。具体而言，我们展示了在高维度情形下，当数据点的数量$n$和它们的维度$p$都很大，并且数据遵循高斯混合模型时，对于一大类DNN，NTK矩阵之间存在\emph{渐近谱等价}。

    arXiv:2403.00258v1 Announce Type: cross  Abstract: Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or quantization) of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN m
    
[^11]: 具有一般因果模型和干预的因果赌博机

    Causal Bandits with General Causal Models and Interventions

    [https://arxiv.org/abs/2403.00233](https://arxiv.org/abs/2403.00233)

    该论文在三个方向上推进了因果赌博机的结果：在一般因果模型下进行干预设计，实现广义软干预以及提供一般的遗憾上下界。

    

    本文考虑了因果赌博机（CBs）用于因果系统中干预的顺序设计。其目标是通过最小化与事后最佳干预序列相比的累积遗憾度量来优化奖励函数。本文将CBs的结果推进了三个方向。首先，假设结构因果模型（SCMs）未知，并且任意从Lipschitz连续函数类$\mathcal{F}$中绘制。现有结果通常集中在（广义）线性SCMs上。其次，假设干预是广义软干预，具有任意所需粒度的水平，导致无限数量的可能干预。相比之下，现有文献通常采用原子和硬干预。第三，我们提供了关于遗憾的一般上下界。上界包含（并改进）特殊情况的已知界限。下界是gene

    arXiv:2403.00233v1 Announce Type: cross  Abstract: This paper considers causal bandits (CBs) for the sequential design of interventions in a causal system. The objective is to optimize a reward function via minimizing a measure of cumulative regret with respect to the best sequence of interventions in hindsight. The paper advances the results on CBs in three directions. First, the structural causal models (SCMs) are assumed to be unknown and drawn arbitrarily from a general class $\mathcal{F}$ of Lipschitz-continuous functions. Existing results are often focused on (generalized) linear SCMs. Second, the interventions are assumed to be generalized soft with any desired level of granularity, resulting in an infinite number of possible interventions. The existing literature, in contrast, generally adopts atomic and hard interventions. Third, we provide general upper and lower bounds on regret. The upper bounds subsume (and improve) known bounds for special cases. The lower bounds are gene
    
[^12]: 关于循环MCMC采样

    On Cyclical MCMC Sampling

    [https://arxiv.org/abs/2403.00230](https://arxiv.org/abs/2403.00230)

    循环MCMC是一种新的MCMC框架，可以很好地估计目标分布的局部形状，但在慢混合核的情况下可能无法产生来自期望分布的样本。

    

    循环MCMC是张等人(2019年)最近提出的一种新的MCMC框架，旨在解决深度学习中出现的高维多模后验分布所带来的挑战。该算法通过生成一个非齐次马尔可夫链，以时间周期性地跟踪目标分布的温和版本来工作。我们在这项工作中展示了，在使用快混合的马尔可夫核和足够长的周期的设置下，循环MCMC会收敛到期望的概率分布。然而，在较常见的慢混合核设置下，该算法可能无法产生来自期望分布的样本。特别地，在一个具有不同方差的简单混合示例中，我们通过模拟显示循环MCMC无法收敛到期望极限。最后，我们展示了循环MCMC通常很好地估计了每个模式周围目标分布的局部形状。

    arXiv:2403.00230v1 Announce Type: cross  Abstract: Cyclical MCMC is a novel MCMC framework recently proposed by Zhang et al. (2019) to address the challenge posed by high-dimensional multimodal posterior distributions like those arising in deep learning. The algorithm works by generating a nonhomogeneous Markov chain that tracks -- cyclically in time -- tempered versions of the target distribution. We show in this work that cyclical MCMC converges to the desired probability distribution in settings where the Markov kernels used are fast mixing, and sufficiently long cycles are employed. However in the far more common settings of slow mixing kernels, the algorithm may fail to produce samples from the desired distribution. In particular, in a simple mixture example with unequal variance, we show by simulation that cyclical MCMC fails to converge to the desired limit. Finally, we show that cyclical MCMC typically estimates well the local shape of the target distribution around each mode, 
    
[^13]: 通过恢复潜变量进行替代调整

    Substitute adjustment via recovery of latent variables

    [https://arxiv.org/abs/2403.00202](https://arxiv.org/abs/2403.00202)

    通过恢复潜变量实现替代调整，提出了一种估计调整后的回归目标参数的方法，为条件独立情况下的回归变量提供了一种替代调整算法。

    

    arXiv:2403.00202v1 通告类型: 跨界 摘要: deconfounder被提议作为一个在有多个原因和未观察到的混杂情况下估计因果参数的方法。它基于从观察到的原因恢复潜变量。我们将因果解释与统计估计问题区分开来，并展示deconfounder通常估计调整后的回归目标参数。它通过为恢复的潜变量调整的结果回归来做到这一点，称为替代物。我们将不涉及因果假设的一般算法称为替代调整。我们提供理论结果来支持当在给定潜变量的条件下回归变量是条件独立时，替代调整估计调整后的回归参数。我们还介绍了我们的替代调整算法的一种变体，它估计具有最小模型假设的瘦假设目标参数。然后我们给出了有限样本界限和渐进

    arXiv:2403.00202v1 Announce Type: cross  Abstract: The deconfounder was proposed as a method for estimating causal parameters in a context with multiple causes and unobserved confounding. It is based on recovery of a latent variable from the observed causes. We disentangle the causal interpretation from the statistical estimation problem and show that the deconfounder in general estimates adjusted regression target parameters. It does so by outcome regression adjusted for the recovered latent variable termed the substitute. We refer to the general algorithm, stripped of causal assumptions, as substitute adjustment. We give theoretical results to support that substitute adjustment estimates adjusted regression parameters when the regressors are conditionally independent given the latent variable. We also introduce a variant of our substitute adjustment algorithm that estimates an assumption-lean target parameter with minimal model assumptions. We then give finite sample bounds and asymp
    
[^14]: 低秩矩阵完成在高度非均匀采样下的入口特定界限

    Entry-Specific Bounds for Low-Rank Matrix Completion under Highly Non-Uniform Sampling

    [https://arxiv.org/abs/2403.00184](https://arxiv.org/abs/2403.00184)

    该论文提出了针对高度非均匀采样条件下低秩矩阵完成问题的入口特定界限，通过定制每个条目的误差上界来匹配一定条件下的极小下界。

    

    低秩矩阵完成涉及使用稀疏一组观测条目来估计矩阵中未观察到的条目的问题。我们考虑观测到的条目是用高度变化的概率进行采样的非均匀设置，可能具有不同的渐近标度。我们证明在结构化采样概率下，通常情况下以及有时在较小的子矩阵上运行估计算法比在整个矩阵上更好，甚至是最佳的。特别是，我们证明了定制到每个条目的误差上界，这些上界在某些条件下与极小下界相匹配。我们的界限以局部采样概率的函数的形式刻画了估计每个条目的难度。我们提供了数值实验，证实了我们的理论发现。

    arXiv:2403.00184v1 Announce Type: cross  Abstract: Low-rank matrix completion concerns the problem of estimating unobserved entries in a matrix using a sparse set of observed entries. We consider the non-uniform setting where the observed entries are sampled with highly varying probabilities, potentially with different asymptotic scalings. We show that under structured sampling probabilities, it is often better and sometimes optimal to run estimation algorithms on a smaller submatrix rather than the entire matrix. In particular, we prove error upper bounds customized to each entry, which match the minimax lower bounds under certain conditions. Our bounds characterize the hardness of estimating each entry as a function of the localized sampling probabilities. We provide numerical experiments that confirm our theoretical findings.
    
[^15]: 基于Transformer的统计学参数估计

    Transformer-based Parameter Estimation in Statistics

    [https://arxiv.org/abs/2403.00019](https://arxiv.org/abs/2403.00019)

    提出了一种基于Transformer的参数估计方法，相较于传统方法，不需要封闭形式解或数学推导，也不需要概率密度函数，仅需经过训练的Transformer模型进行一次推断即可估计潜在分布的参数。

    

    参数估计是统计学中最重要的任务之一，有助于帮助人们理解观测样本背后的分布。传统上，参数估计通过封闭形式解（例如，高斯分布的最大似然估计）或通过迭代数值方法（例如，Beta分布时封闭形式解不存在的情况下使用Newton-Raphson方法）来完成。本文提出了基于Transformer的参数估计方法。与现有解决方案相比，我们的方法不需要封闭形式解或任何数学推导。它甚至不需要知道概率密度函数，这是数值方法所需的。训练了Transformer模型后，仅需要进行一次推断，即可基于观测样本估计潜在分布的参数。在实证研究中，我们比较了我们的方法。

    arXiv:2403.00019v1 Announce Type: new  Abstract: Parameter estimation is one of the most important tasks in statistics, and is key to helping people understand the distribution behind a sample of observations. Traditionally parameter estimation is done either by closed-form solutions (e.g., maximum likelihood estimation for Gaussian distribution), or by iterative numerical methods such as Newton-Raphson method when closed-form solution does not exist (e.g., for Beta distribution).   In this paper we propose a transformer-based approach to parameter estimation. Compared with existing solutions, our approach does not require a closed-form solution or any mathematical derivations. It does not even require knowing the probability density function, which is needed by numerical methods. After the transformer model is trained, only a single inference is needed to estimate the parameters of the underlying distribution based on a sample of observations. In the empirical study we compared our ap
    
[^16]: PIP-Net：城市中行人意图预测

    PIP-Net: Pedestrian Intention Prediction in the Wild

    [https://arxiv.org/abs/2402.12810](https://arxiv.org/abs/2402.12810)

    PIP-Net是一个新型框架，通过综合利用动态学数据和场景空间特征，采用循环和时间注意力机制解决方案，成功预测行人通过马路的意图，性能优于现有技术。

    

    精准的自动驾驶车辆（AVs）对行人意图的预测是当前该领域的一项研究挑战。在本文中，我们介绍了PIP-Net，这是一个新颖的框架，旨在预测AVs在现实世界城市场景中的行人过马路意图。我们提供了两种针对不同摄像头安装和设置设计的PIP-Net变种。利用来自行驶场景的动力学数据和空间特征，所提出的模型采用循环和时间注意力机制的解决方案，性能优于现有技术。为了增强道路用户的视觉表示及其与自车的相关性，我们引入了一个分类深度特征图，结合局部运动流特征，为场景动态提供丰富的洞察。此外，我们探讨了将摄像头的视野从一个扩展到围绕自车的三个摄像头的影响，以提升

    arXiv:2402.12810v1 Announce Type: cross  Abstract: Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the
    
[^17]: 如何验证机器学习回归任务的平均校准性？

    How to validate average calibration for machine learning regression tasks ?

    [https://arxiv.org/abs/2402.10043](https://arxiv.org/abs/2402.10043)

    本文提出了两种验证机器学习回归任务平均校准性的方法，将校准误差与平均绝对误差之间的差值和将平均平方z-分数与1进行比较。研究发现，前者对不确定性分布敏感，而后者在该方面提供了最可靠的方法。

    

    机器学习回归任务的平均校准性可以通过两种方式进行测试。一种方式是将校准误差（CE）估计为平均绝对误差（MSE）与平均方差（MV）或平均平方不确定性之间的差值。另一种方式是将平均平方z-分数或缩放误差（ZMS）与1进行比较。两种方法可能得出不同的结论，正如来自最近的机器学习不确定性量化文献中的数据集集合所示。研究表明，CE对不确定性分布非常敏感，特别是对于离群不确定性的存在，因此无法可靠地用于校准测试。相比之下，ZMS统计量不具有这种敏感性问题，在这种情况下提供了最可靠的方法。文章还讨论了对条件校准验证的影响。

    arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
    
[^18]: 论从可观测和私密数据中实现速率最优分区分类

    On Rate-Optimal Partitioning Classification from Observable and from Privatised Data

    [https://arxiv.org/abs/2312.14889](https://arxiv.org/abs/2312.14889)

    研究了在放宽条件下的分区分类方法的收敛速率，提出了绝对连续分量的新特性，计算了分类错误概率的精确收敛率

    

    在这篇论文中，我们重新审视了分区分类的经典方法，并研究了在放宽条件下的收敛速率，包括可观测（非私密）和私密数据。我们假设特征向量$X$取值于$\mathbb{R}^d$，其标签为$Y$。之前关于分区分类器的结果基于强密度假设，这种假设限制较大，我们通过简单的例子加以证明。我们假设$X$的分布是绝对连续分布和离散分布的混合体，其中绝对连续分量集中于一个$d_a$维子空间。在这里，我们在更宽松的条件下研究了这个问题：除了标准的Lipschitz和边际条件外，我们还引入了绝对连续分量的一个新特性，通过该特性计算了分类错误概率的精确收敛率，对于...

    arXiv:2312.14889v2 Announce Type: replace-cross  Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. Let the feature vector $X$ take values in $\mathbb{R}^d$ and denote its label by $Y$. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. We assume that the distribution of $X$ is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. Here, we study the problem under much milder assumptions: in addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is calculated, both for the
    
[^19]: 基于均值嵌入的分布式贝尔曼算子

    Distributional Bellman Operators over Mean Embeddings

    [https://arxiv.org/abs/2312.07358](https://arxiv.org/abs/2312.07358)

    提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。

    

    我们提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架。 我们基于这一框架推导出了几种新的动态规划和时序差分学习算法，提供了渐近收敛理论，并对这些算法在一系列表格任务上的实证表现进行了检验。此外，我们展示了这种方法可以与深度强化学习直接结合，得到一种新的深度强化学习代理，该代理在 Arcade Learning Environment 上优于基线分布式方法。

    arXiv:2312.07358v2 Announce Type: replace-cross  Abstract: We propose a novel algorithmic framework for distributional reinforcement learning, based on learning finite-dimensional mean embeddings of return distributions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence theory, and examine the empirical performance of the algorithms on a suite of tabular tasks. Further, we show that this approach can be straightforwardly combined with deep reinforcement learning, and obtain a new deep RL agent that improves over baseline distributional approaches on the Arcade Learning Environment.
    
[^20]: 具有位置尺度和形状的神经加性模型：超越平均值的可解释神经回归框架

    Neural Additive Models for Location Scale and Shape: A Framework for Interpretable Neural Regression Beyond the Mean

    [https://arxiv.org/abs/2301.11862](https://arxiv.org/abs/2301.11862)

    提出了神经加性模型具有位置尺度和形状的神经加性模型(NAMLSS)框架，结合了经典深度学习模型的预测能力和分布回归的优势

    

    深度神经网络(DNNs)已被证明在各种任务中非常有效，使其成为需要高级预测能力的问题的首选方法。尽管取得成功，但DNNs的内部运作通常不透明，使其难以解释或理解。这种缺乏可解释性导致近年来对固有可解释性神经网络的研究不断增加。神经加性模型（NAMs）等模型通过将经典统计方法与DNNs相结合实现了视觉可解释性。然而，这些方法仅集中于平均响应预测，忽略了底层数据响应分布的其他特性。我们提出了具有位置尺度和形状的神经加性模型(NAMLSS)，这是一个建模框架，将经典深度学习模型的预测能力与分布回归的固有优势相结合。

    arXiv:2301.11862v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) have proven to be highly effective in a variety of tasks, making them the go-to method for problems requiring high-level predictive power. Despite this success, the inner workings of DNNs are often not transparent, making them difficult to interpret or understand. This lack of interpretability has led to increased research on inherently interpretable neural networks in recent years. Models such as Neural Additive Models (NAMs) achieve visual interpretability through the combination of classical statistical methods with DNNs. However, these approaches only concentrate on mean response predictions, leaving out other properties of the response distribution of the underlying data. We propose Neural Additive Models for Location Scale and Shape (NAMLSS), a modelling framework that combines the predictive power of classical deep learning models with the inherent advantages of distributional regression while
    
[^21]: 高维状态空间中马尔可夫决策过程的结构估计与有限时间保证

    Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees

    [https://arxiv.org/abs/2210.01282](https://arxiv.org/abs/2210.01282)

    本论文提出了一种单循环估计算法，具有有限时间保证，能够处理高维状态空间的马尔可夫决策过程的结构估计问题，而不会损害奖励估计精度。

    

    我们考虑基于可观测的行为历史和访问状态来估计人类代理动态决策的结构模型的任务。问题具有固有的嵌套结构：在内部问题中，确定给定奖励函数的最优策略，而在外部问题中，最大化适合度度量。已经提出了几种方法来减轻这种嵌套循环结构的计算负担，但当状态空间要么是具有大基数的离散空间，要么是高维连续空间时，这些方法仍然面临高复杂度的问题。逆强化学习(IRL)文献中的其他方法强调策略估计，但却以降低奖励估计精度为代价。在本文中，我们提出了一种具有有限时间保证的单循环估计算法，适用于处理高维状态空间而不会损害奖励。

    arXiv:2210.01282v3 Announce Type: replace-cross  Abstract: We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising rewar
    
[^22]: 基于贝叶斯鲁棒优化的模仿学习

    Bayesian Robust Optimization for Imitation Learning

    [https://arxiv.org/abs/2007.12315](https://arxiv.org/abs/2007.12315)

    提出了基于贝叶斯鲁棒优化的模仿学习方法，以在状态不确定性下同时考虑激进和保守的策略优化。

    

    模仿学习中的一个主要挑战是确定在演示的状态分布之外时代理应采取什么行动。逆强化学习（IRL）可以通过学习参数化奖励函数实现对新状态的泛化，但这些方法仍然面临对真实奖励函数和相应最优策略的不确定性。现有基于IRL的安全模仿学习方法使用maxmin框架处理这种不确定性，该框架在假设有一个对抗性奖励函数的情况下优化策略，而风险中立的IRL方法则优化均值或MAP奖励函数的策略。完全忽视风险可能会导致过于激进和不安全的策略，而完全以对抗性方式优化也是有问题的，因为它可能导致表现不佳的过度保守策略。为了在这两个极端之间建立桥梁，我们提出了一种基于贝叶斯鲁棒优化的方法，该方法在状态的置信区间内对策略进行优化。

    arXiv:2007.12315v4 Announce Type: replace  Abstract: One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we p
    
[^23]: Symplectic ODE-Net: 使用控制学习哈密顿动力学

    Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control

    [https://arxiv.org/abs/1909.12077](https://arxiv.org/abs/1909.12077)

    通过设计哈密顿动力学与控制，Symplectic ODE-Net (SymODEN)可以透明地学习潜在动力学，从而揭示系统的相关物理方面。

    

    在本文中，我们介绍了Symplectic ODE-Net（SymODEN），这是一个深度学习框架，可以从观测到的状态轨迹中推断出由普通微分方程（ODE）给定的物理系统的动力学。为了在更少的训练样本上实现更好的泛化，SymODEN通过以物理为基础的方式设计相关的计算图，引入适当的归纳偏差。特别地，我们通过设计哈密顿动力学与控制以透明的方式学习潜在动力学，从而可以利用这种动力学来揭示系统的相关物理方面，如质量和势能。此外，我们提出了一种参数化方法，可以在广义坐标数据嵌入到高维空间或者只能访问速度数据而不是广义动量时，也能强制执行这种哈密顿形式。这一框架通过提供可解释的、与物理一致的信息

    arXiv:1909.12077v5 Announce Type: replace  Abstract: In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consisten
    
[^24]: 采用有噪声树度量的优化传输方法

    Optimal Transport for Measures with Noisy Tree Metric. (arXiv:2310.13653v1 [stat.ML])

    [http://arxiv.org/abs/2310.13653](http://arxiv.org/abs/2310.13653)

    本文提出了一种针对树度量有噪声的优化传输方法，通过引入新的不确定性集合，解决了实际应用中树结构扰动的问题。

    

    本研究探讨了在树度量空间上支持的概率测度的优化传输（OT）问题。已知这种OT问题（即树-瓦瓦斯坦（TW））具有闭合形式表达式，但基本上取决于输入测度支持上的底层树结构。然而，在实际操作中，由于噪声或对抗性测量，给定的树结构可能会被扰动。为了缓解这个问题，我们采取了最大-最小鲁棒OT方法，该方法考虑了在一个树度量的不确定性集合上两个输入测度之间的最大可能距离。总体上说，由于其非凸性和非光滑性，这种方法很难计算，即便是在支持为1维空间的测度情况下，这妨碍了它的实际应用，特别是在大规模情景下。在本文中，我们从边缘删除/添加的角度提出了一种新颖的树度量的不确定性集合，这个集合在一个优雅的框架下涵盖了多样的树结构。

    We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, 
    
[^25]: 一种基于机器学习的概率暴露模型的德国高分辨率室内氡气地图

    A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model. (arXiv:2310.11143v1 [stat.ML])

    [http://arxiv.org/abs/2310.11143](http://arxiv.org/abs/2310.11143)

    本研究提出了一种基于机器学习的概率暴露模型，可以更准确地估计德国室内氡气分布，并具有更高的空间分辨率。

    

    室内氡气是一种致癌的放射性气体，可以在室内积累。通常情况下，全国范围内的室内氡暴露是基于广泛的测量活动估计得来的。然而，样本的特征往往与人口特征不同，这是由于许多相关因素，如地质源氡气的可用性或楼层水平。此外，样本大小通常不允许以高空间分辨率进行暴露估计。我们提出了一种基于模型的方法，可以比纯数据方法更加现实地估计室内氡分布，并具有更高的空间分辨率。我们采用了两阶段建模方法：1）应用分位数回归森林，使用环境和建筑数据作为预测因子，估计了德国每个住宅楼的每个楼层的室内氡概率分布函数；2）使用概率蒙特卡罗抽样技术使它们组合和。

    Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and
    
[^26]: 自学优化器（STOP）：递归自我改进的代码生成

    Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])

    [http://arxiv.org/abs/2310.02304](http://arxiv.org/abs/2310.02304)

    本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。

    

    最近几年的人工智能系统（例如思维树和程序辅助语言模型）取得了一些重要进展，通过提供一个“脚手架”程序来解决问题，该程序构建了多次调用语言模型以生成更好的输出。脚手架程序通常使用Python等编程语言编写。在这项工作中，我们使用了一个融合了语言模型的脚手架程序来改进自身。我们从一个种子“改进器”开始，通过多次查询语言模型并返回最佳解决方案，根据给定的效用函数来改进输入程序。然后，我们运行这个种子改进器来改进自身。在一系列细分任务中，得到的改进改进器生成的程序在性能上明显优于种子改进器。随后，我们对语言模型提出的各种自我改进策略进行了分析，包括波束搜索、遗传算法和模拟退火。由于语言模型本身没有改变，这并不是一种增长领域。

    Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
    
[^27]: 深度残差网络的隐式正则化与神经常微分方程的关联

    Implicit regularization of deep residual networks towards neural ODEs. (arXiv:2309.01213v1 [stat.ML])

    [http://arxiv.org/abs/2309.01213](http://arxiv.org/abs/2309.01213)

    本文建立了深度残差网络向神经常微分方程的隐式正则化，通过对用梯度流训练的非线性网络的研究，证明了在网络以神经常微分方程的离散化形式初始化后，这种离散化将在整个训练过程中保持不变，并提供了收敛性的条件。

    

    残差神经网络是先进的深度学习模型。它们的连续深度模拟称为神经常微分方程（ODE），也被广泛使用。尽管它们取得了成功，但离散模型与连续模型之间的联系仍缺乏坚实的数学基础。在本文中，我们通过建立一个针对用梯度流训练的非线性网络的深度残差网络向神经常微分方程的隐式正则化来朝着这个方向迈出了一步。我们证明，如果网络的初始化是神经常微分方程的离散化，则这种离散化在整个训练过程中保持不变。我们的结果对于有限的训练时间和训练时间趋于无穷大都成立，只要网络满足Polyak-Lojasiewicz条件。重要的是，这个条件适用于一个残差网络家族，其中残差是两层感知机，在宽度上只是线性超参数化，并且暗示了梯度流的收敛性。

    Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to
    
[^28]: 基于广义多维比较的光谱排名推断

    Spectral Ranking Inferences based on General Multiway Comparisons. (arXiv:2308.02918v1 [stat.ME])

    [http://arxiv.org/abs/2308.02918](http://arxiv.org/abs/2308.02918)

    本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。

    

    本文研究了在一个非常普遍和更加真实的情景中，使用光谱方法对未观察到的比较实体的偏好分数进行估计和不确定性量化的性能。在这种情况下，比较图由可能具有异构大小的超边组成，对于给定的超边，比较数量可能仅为1。这种设置在实际应用中普遍存在，避免了需要指定图的随机性以及在常用的Bradley-Terry-Luce (BTL)或Plackett-Luce (PL)模型中施加的限制性均匀采样假设。此外，在适用BTL或PL模型的情况下，我们揭示了光谱估计量与最大似然估计量（MLE）之间的关系。我们发现，通过应用从等权重传统光谱方法估计得到的最佳加权，可以实现与MLE相同的渐近效率的双步光谱方法。考虑到渐近情况，

    This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptot
    
[^29]: 您的数据可对齐吗？基于原则和可解释性的单细胞数据对齐性测试和整合

    Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data. (arXiv:2308.01839v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.01839](http://arxiv.org/abs/2308.01839)

    本文提出了一种谱流形对齐和推断（SMAI）框架，通过提供一种统计检验方法来确定单细胞数据集之间的对齐性，避免误导性推断，并保持数据整合的结构和可解释性。

    

    单细胞数据整合可以提供细胞的全面分子视图，并且已经开发出许多算法来消除不需要的技术或生物变异，并整合异质的单细胞数据集。尽管这些方法被广泛使用，但现有方法存在一些基本限制。特别是，我们缺乏一种严谨的统计检验方法，用于判断两个高维单细胞数据集是否可以对齐（因此是否应该进行对齐）。此外，流行的方法在对齐过程中可能会明显畸变数据，使得对齐后的数据和下游分析难以解释。为了克服这些限制，我们提出了一种基于谱流形对齐和推断（SMAI）框架，该框架可以实现基于原则和可解释的单细胞数据对齐性测试和保持结构的整合。SMAI提供了一种统计检验方法，能够稳健地确定数据集之间的对齐性，以避免误导性推断，同时其方法也经过了高维数据的正当性证明。

    Single-cell data integration can provide a comprehensive molecular view of cells, and many algorithms have been developed to remove unwanted technical or biological variations and integrate heterogeneous single-cell datasets. Despite their wide usage, existing methods suffer from several fundamental limitations. In particular, we lack a rigorous statistical test for whether two high-dimensional single-cell datasets are alignable (and therefore should even be aligned). Moreover, popular methods can substantially distort the data during alignment, making the aligned data and downstream analysis difficult to interpret. To overcome these limitations, we present a spectral manifold alignment and inference (SMAI) framework, which enables principled and interpretable alignability testing and structure-preserving integration of single-cell data. SMAI provides a statistical test to robustly determine the alignability between datasets to avoid misleading inference, and is justified by high-dimen
    
[^30]: 在多模态人工智能中保护数据：一种差分隐私方法用于CLIP训练

    Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])

    [http://arxiv.org/abs/2306.08173](http://arxiv.org/abs/2306.08173)

    本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。

    

    多模态人工智能的成功引发了视觉和语言任务中数据隐私的关注。虽然CLIP通过对图像和文本的联合训练彻底改变了多模态学习，但其可能无意中披露敏感信息的潜力需要集成保护隐私的机制。我们引入了对比语言-图像预训练（CLIP）模型的差分隐私改进，有效地解决了隐私问题，同时保持准确性。我们提出的方法Dp-CLIP在包括图像分类和视觉问答等多样的视觉和语言任务的基准数据集上进行了严格评估。我们证明了我们的方法保持了与标准的非私有CLIP模型同等的性能。此外，我们在线性表示设置下分析了我们提出的算法。我们推导了算法的收敛速度，并展示了在梯度被剪辑时实用性和隐私之间的权衡。

    The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe
    
[^31]: 带权重空间上功能性输入映射的全局普适逼近

    Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])

    [http://arxiv.org/abs/2306.03303](http://arxiv.org/abs/2306.03303)

    本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。

    

    我们引入了所谓的功能性输入神经网络，定义在可能是无限维带权重空间上，其值也在可能是无限维的输出空间中。为此，我们使用一个加性族作为隐藏层映射，以及一个非线性激活函数应用于每个隐藏层。依靠带权重空间上的Stone-Weierstrass定理，我们可以证明连续函数的推广的全局普适逼近结果，超越了常规紧集逼近。这特别适用于通过功能性输入神经网络逼近（非先见之明的）路径空间函数。作为带权Stone-Weierstrass定理的进一步应用，我们证明了线性函数签名的全局普适逼近结果。我们还在这个设置中引入了高斯过程回归的观点，并展示了签名内核的再生核希尔伯特空间是某些高斯过程的Cameron-Martin空间。

    We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
    
[^32]: 二分分类中追索权的风险

    The Risks of Recourse in Binary Classification. (arXiv:2306.00497v1 [cs.LG])

    [http://arxiv.org/abs/2306.00497](http://arxiv.org/abs/2306.00497)

    研究发现，在二分分类中提供追索权会增加错误率，导致更多错误的发生。提供算法追索权可能也会在系统级别上给予不利。

    

    算法追索权提供解释，以帮助用户推翻机器学习系统的不利决策。但到目前为止，很少有人关注提供追索权是否有益。我们引入了一个抽象的学习理论框架，比较了具有和没有算法追索权的分类的风险（即期望损失）。这使我们能够回答在整个人群水平上提供追索权何时有益或有害的问题。令人惊讶的是，我们发现在许多可信的情况下，提供追索权反而会有害，因为它将用户推向更高类别不确定性的区域，因此会导致更多的错误。我们进一步研究了部署分类器的一方是否有动机针对提供追索权的情况进行策略规划，我们发现有时候确实存在这种现象，这对他们的用户不利。因此，提供算法追索权在系统级别上可能也是有害的。

    Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e. expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level
    
[^33]: 图的Bures-Wasserstein平均值

    Bures-Wasserstein Means of Graphs. (arXiv:2305.19738v1 [stat.ML])

    [http://arxiv.org/abs/2305.19738](http://arxiv.org/abs/2305.19738)

    该论文提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。实验结果表明，在各种任务中都有很好的表现。

    

    在机器学习和统计学中，找到采样数据的平均值是一项基本任务。然而，在数据样本为图对象的情况下，定义平均值是一项困难的任务。我们提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。通过在此嵌入空间中找到平均值，我们可以恢复保留结构信息的平均图。我们确定了新的图平均值的存在和唯一性，并提供了一个迭代算法来计算它。为了展示我们的框架作为机器学习中的一个有价值的工具，我们在各种任务中进行了评估，包括结构化图的k-means聚类、功能性脑网络的分类以及多层图的半监督节点分类。我们的实验结果表明，我们的方法实现了一致的p。

    Finding the mean of sampled data is a fundamental task in machine learning and statistics. However, in cases where the data samples are graph objects, defining a mean is an inherently difficult task. We propose a novel framework for defining a graph mean via embeddings in the space of smooth graph signal distributions, where graph similarity can be measured using the Wasserstein metric. By finding a mean in this embedding space, we can recover a mean graph that preserves structural information. We establish the existence and uniqueness of the novel graph mean, and provide an iterative algorithm for computing it. To highlight the potential of our framework as a valuable tool for practical applications in machine learning, it is evaluated on various tasks, including k-means clustering of structured graphs, classification of functional brain networks, and semi-supervised node classification in multi-layer graphs. Our experimental results demonstrate that our approach achieves consistent p
    
[^34]: 逃离平庸：双层神经网络如何在 SGD 下学习困难的单指标模型

    Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])

    [http://arxiv.org/abs/2305.18502](http://arxiv.org/abs/2305.18502)

    研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。

    

    本研究探讨了在随机梯度下降（SGD）下双层神经网络学习单指数目标函数的样本复杂度问题，重点关注在初始化时存在许多平坦方向的挑战性情况。已经有研究表明，这种情况下通常需要 $n=O(d\log{d})$ 个样本。但是，我们提供了在高维度和不同宽度情况下的前置因子的精确结果。值得注意的是，我们的发现表明，在这个问题类中，过参数化只会增加一定因子的收敛性。这些见解基于 SGD 动态的低维度随机过程模型，其中逃离平庸等同于计算出站出时间。然而，我们证明这个过程的确定性近似足以代表逃逸时间，这意味着在这种情况下随机性的作用可能很小。

    This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.
    
[^35]: 基于点积的层次聚类可以恢复隐藏的树形结构

    Hierarchical clustering with dot products recovers hidden tree structure. (arXiv:2305.15022v1 [stat.ML])

    [http://arxiv.org/abs/2305.15022](http://arxiv.org/abs/2305.15022)

    本文发现一种基于点积的层次聚类算法，可以通过最大平均点积合并聚类，并且输出的树结构可用于准确估计数据的生成层次结构，树形恢复性能优于现有方法。

    

    本文提供了一个对于已有凝聚聚类算法的新视角，专注于层次结构的恢复。我们建议一种简单的标准算法变体，其中聚类是通过最大平均点积而不是最小距离或簇内方差来合并的。我们证明了此算法输出的树可以作为数据生成层次结构的可靠估计。关键技术创新在于理解模型中的层次信息如何转化为可从数据中恢复的树形几何信息，并同时增长样本大小和数据维数的好处。我们在真实数据上展示了优于现有方法（如UPGMA、Ward's方法和HDBSCAN）的树形恢复性能。

    In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.
    
[^36]: CoinEM：无需调参的基于粒子的潜变量模型变分推断方法

    CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models. (arXiv:2305.14916v1 [stat.ML])

    [http://arxiv.org/abs/2305.14916](http://arxiv.org/abs/2305.14916)

    本文提出了两种无需调参的基于粒子的变分推断算法，其中一种是通过考虑边缘最大似然估计为自由能泛函最小化得到的，另一种是用于优化该问题的算法，完全无需调参。

    

    本文提出两种基于粒子的新型算法，用于通过边际最大似然估计学习潜变量模型，其中一种完全无需调参。我们的方法基于将边际最大似然估计视为优化问题的角度：即将其视为自由能泛函的最小化。解决这个问题的一种方法是考虑自由能关联的梯度流的离散化。我们研究了一种类似于流行的 Stein 变分梯度下降算法的方法。特别地，我们为此算法建立了下降引理，保证了自由能在每次迭代中下降。但此方法和其他由梯度流的离散化得到的方法都必须依赖于学习率，该学习率必须由从业者仔细调整，以确保以合适的速率收敛。为此，我们还提出了另一种算法用于优化这个问题，该算法是完全无需调参的。

    We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is to consider the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of the popular Stein variational gradient descent algorithm. In particular, we establish a descent lemma for this algorithm, which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, will necessarily depend on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the
    
[^37]: 具有一般激活函数的深度平衡模型的全局收敛速度

    Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05797](http://arxiv.org/abs/2302.05797)

    该论文研究了具有一般激活函数的深度平衡模型（DEQ）的全局收敛速度，证明了梯度下降以线性收敛速度收敛到全局最优解，并解决了限制平衡点Gram矩阵最小特征值的挑战。

    

    在最近的一篇论文中，Ling等人研究了具有ReLU激活函数的过参数化深度平衡模型（DEQ）。他们证明了对于二次损失函数，梯度下降方法以线性收敛速度收敛到全局最优解。本文表明，对于具有任何具有有界一阶和二阶导数的激活函数的DEQ，该事实仍然成立。由于新的激活函数通常是非线性的，限制平衡点的Gram矩阵的最小特征值尤其具有挑战性。为了完成这个任务，我们需要创建一个新的总体Gram矩阵，并开发一种具有Hermite多项式展开的新形式的双重激活函数。

    In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
    
[^38]: SPEED: 线性异方差 Bandit 策略评估的实验设计

    SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12357](http://arxiv.org/abs/2301.12357)

    本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。

    

    本文研究了线性 Bandit 下策略评估的最优数据收集问题。在策略评估中，我们需要估计多臂赌博机环境中执行目标策略将获得的期望收益。本文是首个专注于解决线性 Bandit 环境下包含异方差奖励噪声的策略评估的最优数据收集策略的工作。我们首先在线性 Bandit 环境下制定了加权最小二乘估计的最优设计，以减少目标策略价值的均方误差。接着，我们使用该设计来推导出数据收集期间每个动作的最优样本分配。然后，我们引入了一种名为 SPEED（Structured Policy Evaluation Experimental Design）的新算法，该算法跟踪最优设计，并计算其与最优设计的遗憾。最后，我们通过实验证明 SPEED 可以实现带有均方误差比较小的策略评估。

    In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa
    
[^39]: 使随机梯度下降法无参数化

    Making SGD Parameter-Free. (arXiv:2205.02160v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2205.02160](http://arxiv.org/abs/2205.02160)

    该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。

    

    我们开发了一种无参数随机凸优化（SCO）算法，其收敛速度仅比对应的已知参数设置的最优速度多一个双对数因子。相比之下，先前已知的无参数SCO的最佳速度是基于在线无参数后悔界的，与已知参数的对应方法相比包含不可避免的额外对数项。我们的算法具有概念上的简单性，具有高概率保证，并且部分适应未知梯度范数、平滑性和强凸性。我们的成果的核心是SGD步长选择的新型无参数证书，以及假设在SGD迭代上没有先验界限的时间一致集中结果。

    We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameter-free SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.
    
[^40]: 多变量大数据分析中的可解释性学习用于网络监测

    Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring. (arXiv:1907.02677v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/1907.02677](http://arxiv.org/abs/1907.02677)

    本文扩展了多变量大数据分析（MBDA）方法，提出了一种自动推导特征的解决方案，结合可解释性和交互式模型的优势以及并行处理的能力，应用于网络监测和诊断，最终在UGR'16和Dartmouth'18两个数据集上取得成功。

    

    开发新的数据驱动模型以评估通信网络性能越来越受到关注。对于许多应用程序，比如网络监测和故障排除，如果不能被人类操作员解释，数据模型就没多大用处。在本文中，我们提出了多变量大数据分析（MBDA）方法的扩展，这是一种近期提出的可解释性数据分析工具。在这个扩展中，我们提出了自动推导特征的解决方案，这是当数据量庞大时应用MBDA的重要步骤。所得到的网络监测方法允许我们检测和诊断不同的网络异常，采用一种将可解释性和交互式模型的优势与并行处理的能力相结合的数据分析工作流。我们将扩展的MBDA应用于两个案例研究：UGR'16，用于异常检测的基准流量实际数据集，以及Dartmouth'18，最长和最具挑战性的数据集之一。

    There is an increasing interest in the development of new data-driven models useful to assess the performance of communication networks. For many applications, like network monitoring and troubleshooting, a data model is of little use if it cannot be interpreted by a human operator. In this paper, we present an extension of the Multivariate Big Data Analysis (MBDA) methodology, a recently proposed interpretable data analysis tool. In this extension, we propose a solution to the automatic derivation of features, a cornerstone step for the application of MBDA when the amount of data is massive. The resulting network monitoring approach allows us to detect and diagnose disparate network anomalies, with a data-analysis workflow that combines the advantages of interpretable and interactive models with the power of parallel processing. We apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and l
    

