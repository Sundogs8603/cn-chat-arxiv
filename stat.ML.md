# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Stabilizing Estimates of Shapley Values with Control Variates.](http://arxiv.org/abs/2310.07672) | 使用控制变量的方法稳定Shapley值的估计，减少了模型解释的不确定性，适用于任何机器学习模型。 |
| [^2] | [Deep Backtracking Counterfactuals for Causally Compliant Explanations.](http://arxiv.org/abs/2310.07665) | 本研究提供了一种实用方法，用于在深度生成组件的结构因果模型中计算回溯反事实。通过在因果模型的结构化潜在空间中解决优化问题，我们的方法能够生成反事实，并且与其他方法相比具备了多功能、模块化和符合因果关系的特点。 |
| [^3] | [Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning.](http://arxiv.org/abs/2310.07558) | 这项研究提出了一种具有非参数需求学习和平滑自适应的动态定价算法，通过使用自相似条件实现了最小化极限遗憾。 |
| [^4] | [Uncovering ECG Changes during Healthy Aging using Explainable AI.](http://arxiv.org/abs/2310.07463) | 本文使用可解释的人工智能技术分析了健康个体的心电图数据，并识别出随年龄增长呼吸率的下降及SDANN值异常高作为老年人的指标。 |
| [^5] | [Non-backtracking Graph Neural Networks.](http://arxiv.org/abs/2310.07430) | 非回溯图神经网络(NBA-GNN)通过不考虑先前访问节点的消息来解决图神经网络本地更新中的冗余问题，并且在随机块模型恢复方面表现出良好的性能。 |
| [^6] | [Randomized Runge-Kutta-Nystr\"om.](http://arxiv.org/abs/2310.07399) | 本文介绍了5/2阶和7/2阶$L^2$-准确的随机Runge-Kutta-Nystr\"om方法，用于近似底层的哈密顿流，并展示了它在高维目标分布中的卓越效率。 |
| [^7] | [Orthogonal Random Features: Explicit Forms and Sharp Inequalities.](http://arxiv.org/abs/2310.07370) | 该论文通过分析正交随机特征的核近似的偏差和方差，提供了明确的表达式，并得出了尖锐指数界限，支持正交随机特征比随机傅里叶特征更具信息性。 |
| [^8] | [Functional Generalized Canonical Correlation Analysis for studying multiple longitudinal variables.](http://arxiv.org/abs/2310.07330) | 这篇论文介绍了函数广义规范典范相关分析（FGCCA）的新框架，可以用于探究多个共同观察到的随机过程之间的关联。该框架对稀疏和不规则观测数据具有鲁棒性，并通过引入贝叶斯方法来估计典范组件。同时，还扩展了框架，允许将单变量或多变量响应整合到分析中，为预测应用提供了可能性。 |
| [^9] | [Consistency of some sequential experimental design strategies for excursion set estimation based on vector-valued Gaussian processes.](http://arxiv.org/abs/2310.07315) | 该论文研究了基于向量值高斯过程的序贯实验设计策略在集合估计中的一致性，通过将已有的结果推广到向量值情况，解决了伪逆映射的不连续性带来的问题。 |
| [^10] | [Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction.](http://arxiv.org/abs/2310.07313) | METRO是一个机器学习模型，利用最小模板进行反应预测，解决了溯源预测中的计算开销大和解释性差的问题，实现了最先进的效果。 |
| [^11] | [Why Does Sharpness-Aware Minimization Generalize Better Than SGD?.](http://arxiv.org/abs/2310.07269) | 这项研究填补了Sharpness-Aware Minimization（SAM）相对于随机梯度下降（SGD）的一定数据模型和卷积神经网络中泛化更好的空白，解释了SAM的优势，尤其是在早期阶段防止噪声学习的能力。 |
| [^12] | [Surrogate modeling for stochastic crack growth processes in structural health monitoring applications.](http://arxiv.org/abs/2310.07241) | 本文提出了一种代理模型用于预测结构中裂纹的扩展，并成功地编码了不同的随机不确定性来源。该模型基于高斯过程回归模型，能够生成先验分布用于贝叶斯结构健康监测任务。 |
| [^13] | [Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes.](http://arxiv.org/abs/2310.07216) | 通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。 |
| [^14] | [Neural networks: deep, shallow, or in between?.](http://arxiv.org/abs/2310.07190) | 本研究探讨了深度和宽度对于神经网络的影响，结果表明只有深度趋近无穷大的神经网络才可能达到比熵数更好的速度，而固定深度并让宽度趋近无穷大则没有收益。 |
| [^15] | [Kernel Cox partially linear regression: building predictive models for cancer patients' survival.](http://arxiv.org/abs/2310.07187) | 本文提出了一种基于核Cox部分线性回归的方法来构建癌症患者生存预测模型，通过核机器方法描述复杂的生存和预测因子关系，并利用正则化加权核机器方法自动去除不相关的因子。与其他竞争方法相比，该方法在模拟中表现最好。 |
| [^16] | [Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions.](http://arxiv.org/abs/2310.07174) | 本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。 |
| [^17] | [Exponential Quantum Communication Advantage in Distributed Learning.](http://arxiv.org/abs/2310.07136) | 在分布式学习中，我们提出了一个基于量子网络的框架，可以使用指数级较少的通信和相对较小的时间和空间复杂度开销进行推理和训练。这是第一个展示了具有密集经典数据的通用机器学习问题具有指数量子优势的例子。 |
| [^18] | [Risk Assessment and Statistical Significance in the Age of Foundation Models.](http://arxiv.org/abs/2310.07132) | 本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。 |
| [^19] | [Positivity-free Policy Learning with Observational Data.](http://arxiv.org/abs/2310.06969) | 本研究提出了一种无偏性的政策学习框架，该框架利用观测数据进行政策学习，并克服了现实情境中无法满足假设的难题。该框架利用增量倾向分数策略调整倾向分数值，从而实现了快速的收敛率。 |
| [^20] | [Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing.](http://arxiv.org/abs/2310.06306) | 本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。 |
| [^21] | [Multi-timestep models for Model-based Reinforcement Learning.](http://arxiv.org/abs/2310.05672) | 多步模型的基于模型的强化学习算法通过使用多步目标来训练一步模型，解决了轨迹长度增长时一步预测误差的累积问题，并在噪声数据上表现出显著的性能提升。 |
| [^22] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^23] | [Entropy-MCMC: Sampling from Flat Basins with Ease.](http://arxiv.org/abs/2310.05401) | 本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。 |
| [^24] | [Cross-Prediction-Powered Inference.](http://arxiv.org/abs/2309.16598) | 本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。 |
| [^25] | [On the Probability of Immunity.](http://arxiv.org/abs/2309.11942) | 本文研究了免疫的概率，提出了免疫的必要和充分条件，以及ε-有界免疫的条件。同时，借助随机对照试验估计受益概率，并得到比现有边界更紧密的概率边界。此外，介绍了间接免疫的概念，并提出了一种用于处理未测量混淆的免疫概率敏感性分析方法。 |
| [^26] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^27] | [The Bayesian Context Trees State Space Model for time series modelling and forecasting.](http://arxiv.org/abs/2308.00913) | 该论文介绍了基于贝叶斯上下文树状态空间模型的时间序列建模和预测方法，通过层级贝叶斯框架将离散状态和实值时间序列模型组合，构建出灵活且可解释的混合模型，并提出了有效的算法来进行贝叶斯推断和预测。 |
| [^28] | [Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.](http://arxiv.org/abs/2307.06555) | 本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。 |
| [^29] | [DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting.](http://arxiv.org/abs/2306.01984) | 提出了一种新的扩散模型，其结合了数据中编码的时间动态，自然地编码了多步和长程预测能力，具有灵活的采样轨迹和折衷性能与加速采样的能力，同时提高了计算效率，可在时空预测方面取得竞争性表现。 |
| [^30] | [Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching.](http://arxiv.org/abs/2305.17884) | 本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。 |
| [^31] | [Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation.](http://arxiv.org/abs/2303.13429) | 本文提出了一种相互作用粒子 Langevin 算法，用于最大边缘似然估计。使用此算法，估计器的优化误差具有非渐近浓度界限。 |
| [^32] | [Automatic Change-Point Detection in Time Series via Deep Learning.](http://arxiv.org/abs/2211.03860) | 本文介绍了一种通过训练神经网络自动生成新的离线检测方法的方式，应用于时间序列中的自动变点检测，其性能可与标准CUSUM分类器性能竞争。 |
| [^33] | [Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions.](http://arxiv.org/abs/2201.02958) | 本文提出了一种嵌套模拟的新方法，它能够在保持条件期望足够平滑的情况下，有效地缓解高维度中的维度灾难，以桥接标准嵌套模拟的立方根收敛率和标准蒙特卡洛模拟的平方根收敛率之间的差距。 |
| [^34] | [A Machine Learning Approach for Modelling Parking Duration in Urban Land-use.](http://arxiv.org/abs/2008.01674) | 本研究提出了一个机器学习模型来分析汽车用户的特征对停车时长的影响，采用人工神经网络捕捉其相互关系，并运用Garson算法和LIME进行模型解释。 |
| [^35] | [High-dimensional and universally consistent k-sample tests.](http://arxiv.org/abs/1910.08883) | 本文证明了独立性测试实现了普遍一致的k样本检验，并且发现非参数独立性测试通常比多元方差分析(MANOVA)测试在高斯分布情况下表现更好。 |

# 详细

[^1]: 用控制变量稳定Shapley值的估计

    Stabilizing Estimates of Shapley Values with Control Variates. (arXiv:2310.07672v1 [stat.ML])

    [http://arxiv.org/abs/2310.07672](http://arxiv.org/abs/2310.07672)

    使用控制变量的方法稳定Shapley值的估计，减少了模型解释的不确定性，适用于任何机器学习模型。

    

    Shapley值是解释黑盒机器学习模型预测最流行的工具之一。然而，它们的计算成本很高，因此采用抽样近似来减少不确定性。为了稳定这些模型解释，我们提出了一种基于控制变量的蒙特卡洛技术的方法，称为ControlSHAP。我们的方法适用于任何机器学习模型，并且几乎不需要额外的计算或建模工作。在多个高维数据集上，我们发现它可以显著减少Shapley估计的蒙特卡洛变异性。

    Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
    
[^2]: 深度回溯对因果一致解释的反事实推理

    Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])

    [http://arxiv.org/abs/2310.07665](http://arxiv.org/abs/2310.07665)

    本研究提供了一种实用方法，用于在深度生成组件的结构因果模型中计算回溯反事实。通过在因果模型的结构化潜在空间中解决优化问题，我们的方法能够生成反事实，并且与其他方法相比具备了多功能、模块化和符合因果关系的特点。

    

    反事实推理可以通过回答在改变情况下会观察到什么来提供有价值的见解，条件是根据实际观察。虽然经典的介入式解释已经得到了广泛研究，回溯原则被提出作为一种保持所有因果定律完整性的替代哲学，但其研究较少。在本研究中，我们介绍了在由深度生成组件组成的结构因果模型中计算回溯反事实的实用方法。为此，我们对结构分配施加了条件，通过在因果模型的结构化潜在空间中解决一个可行的约束优化问题来生成反事实。我们的方法还可以与反事实解释领域的方法进行比较。与这些方法相比，我们的方法代表了一种多功能、模块化和遵守因果的替代方案。

    Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
    
[^3]: 具有非参数需求学习的平滑自适应动态定价

    Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning. (arXiv:2310.07558v1 [stat.ML])

    [http://arxiv.org/abs/2310.07558](http://arxiv.org/abs/2310.07558)

    这项研究提出了一种具有非参数需求学习和平滑自适应的动态定价算法，通过使用自相似条件实现了最小化极限遗憾。

    

    我们研究了需求函数为非参数和Holder平滑的动态定价问题，并且我们专注于适应未知的Holder平滑参数β的能力。传统上，最优的动态定价算法严重依赖于对β的了解，以达到一个最小化极限遗憾的效果，即O(T^((β+1)/(2β+1)))。然而，我们通过证明没有定价策略能够在不知道β的情况下自适应地达到这个最小化极限遗憾，突显了这个动态定价问题的适应性挑战。受到不可能性结果的启发，我们提出了一种自相似条件来实现适应性。重要的是，我们证明了自相似条件不会损害问题本身的复杂性，因为它保持了渐近遗憾下界Ω(T^((β+1)/(2β+1)))。此外，我们开发了一种平滑自适应的动态定价算法，并理论上证明了该算法的有效性。

    We study the dynamic pricing problem where the demand function is nonparametric and H\"older smooth, and we focus on adaptivity to the unknown H\"older smoothness parameter $\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\Omega(T^{\frac{\beta+1}{2\beta+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves t
    
[^4]: 使用可解释的人工智能揭示健康衰老过程中的心电图变化

    Uncovering ECG Changes during Healthy Aging using Explainable AI. (arXiv:2310.07463v1 [eess.SP])

    [http://arxiv.org/abs/2310.07463](http://arxiv.org/abs/2310.07463)

    本文使用可解释的人工智能技术分析了健康个体的心电图数据，并识别出随年龄增长呼吸率的下降及SDANN值异常高作为老年人的指标。

    

    心血管疾病仍然是全球领先的死因。这需要对心脏衰老过程有深入的了解，以诊断心血管健康状况的限制。传统上，对个体心电图（ECG）特征随年龄变化的分析提供了这些见解。然而，这些特征虽然有信息量，但可能掩盖了底层数据关系。在本文中，我们使用深度学习模型和基于树的模型分析来自健康个体的ECG数据，包括原始信号和ECG特征格式。然后，我们使用可解释的AI技术来识别对于区分年龄组别最有辨别力的ECG特征或原始信号特征。我们的分析与基于树的分类器揭示了随年龄增长呼吸率下降，并识别出SDANN值异常高作为老年人的指标，可将其与年轻人区分开来。

    Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adul
    
[^5]: 非回溯图神经网络

    Non-backtracking Graph Neural Networks. (arXiv:2310.07430v1 [cs.LG])

    [http://arxiv.org/abs/2310.07430](http://arxiv.org/abs/2310.07430)

    非回溯图神经网络(NBA-GNN)通过不考虑先前访问节点的消息来解决图神经网络本地更新中的冗余问题，并且在随机块模型恢复方面表现出良好的性能。

    

    著名的图神经网络的消息传递更新允许使用本地和计算上可跟踪的更新来表示大规模图。然而，本地更新受到回溯的影响，即消息通过同一条边两次流动并重访先前访问的节点。由于消息流的数量随着更新的次数呈指数级增加，本地更新中的冗余阻碍了图神经网络准确识别下游任务的特定消息流。在这项工作中，我们通过非回溯的图神经网络（NBA-GNN）解决了这种冗余，该网络在更新消息时不考虑先前访问节点的消息。我们进一步研究了NBA-GNN如何缓解GNN的过度压缩，并建立了NBA-GNN和非回溯更新在随机块模型恢复方面出色性能之间的联系。我们通过实验证实了我们的NBA-

    The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-
    
[^6]: 随机Runge-Kutta-Nystr\"om方法在非可逆马尔科夫链中的应用

    Randomized Runge-Kutta-Nystr\"om. (arXiv:2310.07399v1 [math.NA])

    [http://arxiv.org/abs/2310.07399](http://arxiv.org/abs/2310.07399)

    本文介绍了5/2阶和7/2阶$L^2$-准确的随机Runge-Kutta-Nystr\"om方法，用于近似底层的哈密顿流，并展示了它在高维目标分布中的卓越效率。

    

    本文介绍了5/2阶和7/2阶$L^2$-准确的随机Runge-Kutta-Nystr\"om方法，用于近似底层的哈密顿流，包括不调整的哈密顿蒙特卡洛和不调整的动力学朗之万链。通过在势能函数的梯度和海森矩阵的Lipschitz假设下提供了量化的5/2阶$L^2$-准确度上限。对于一些“良好行为”的高维目标分布，通过数值实验对应的马尔科夫链表现出很高的效率。

    We present 5/2- and 7/2-order $L^2$-accurate randomized Runge-Kutta-Nystr\"om methods to approximate the Hamiltonian flow underlying various non-reversible Markov chain Monte Carlo chains including unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin chains. Quantitative 5/2-order $L^2$-accuracy upper bounds are provided under gradient and Hessian Lipschitz assumptions on the potential energy function. The superior complexity of the corresponding Markov chains is numerically demonstrated for a selection of `well-behaved', high-dimensional target distributions.
    
[^7]: 正交随机特征: 明确形式和尖锐不等式

    Orthogonal Random Features: Explicit Forms and Sharp Inequalities. (arXiv:2310.07370v1 [cs.LG])

    [http://arxiv.org/abs/2310.07370](http://arxiv.org/abs/2310.07370)

    该论文通过分析正交随机特征的核近似的偏差和方差，提供了明确的表达式，并得出了尖锐指数界限，支持正交随机特征比随机傅里叶特征更具信息性。

    

    随机特征通过随机化技术被引入以扩展核方法。特别地，随机傅里叶特征和正交随机特征被用来近似流行的高斯核。前者通过随机高斯矩阵执行，并在平均后得到了完全符合高斯核的结果。在这项工作中，我们分析了基于用到Haar正交矩阵的正交随机特征的核近似的偏差和方差。我们使用归一化贝塞尔函数提供了这些量的明确表达式，并推导了支持正交随机特征比随机傅里叶特征更具信息性的尖锐指数界限。

    Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
    
[^8]: 用于研究多个纵向变量的函数广义规范典范相关分析

    Functional Generalized Canonical Correlation Analysis for studying multiple longitudinal variables. (arXiv:2310.07330v1 [stat.ME])

    [http://arxiv.org/abs/2310.07330](http://arxiv.org/abs/2310.07330)

    这篇论文介绍了函数广义规范典范相关分析（FGCCA）的新框架，可以用于探究多个共同观察到的随机过程之间的关联。该框架对稀疏和不规则观测数据具有鲁棒性，并通过引入贝叶斯方法来估计典范组件。同时，还扩展了框架，允许将单变量或多变量响应整合到分析中，为预测应用提供了可能性。

    

    在这篇论文中，我们介绍了一种新的框架，函数广义规范典范相关分析（FGCCA），用于探究多个共同观察到的随机过程之间的关联。该框架基于多区块正则化广义规范典范相关分析（RGCCA）框架。它对稀疏和不规则观测数据具有鲁棒性，适用于许多场景。我们建立了求解过程的单调性质，并引入了一种贝叶斯方法来估计典范组件。我们提出了该框架的扩展，允许将单变量或多变量响应整合到分析中，为预测应用铺平了道路。我们通过模拟研究评估了该方法的效率，并在一个纵向数据集上展示了一个应用案例。

    In this paper, we introduce Functional Generalized Canonical Correlation Analysis (FGCCA), a new framework for exploring associations between multiple random processes observed jointly. The framework is based on the multiblock Regularized Generalized Canonical Correlation Analysis (RGCCA) framework. It is robust to sparsely and irregularly observed data, making it applicable in many settings. We establish the monotonic property of the solving procedure and introduce a Bayesian approach for estimating canonical components. We propose an extension of the framework that allows the integration of a univariate or multivariate response into the analysis, paving the way for predictive applications. We evaluate the method's efficiency in simulation studies and present a use case on a longitudinal dataset.
    
[^9]: 基于向量值高斯过程的序贯实验设计策略在集合估计中的一致性

    Consistency of some sequential experimental design strategies for excursion set estimation based on vector-valued Gaussian processes. (arXiv:2310.07315v1 [math.ST])

    [http://arxiv.org/abs/2310.07315](http://arxiv.org/abs/2310.07315)

    该论文研究了基于向量值高斯过程的序贯实验设计策略在集合估计中的一致性，通过将已有的结果推广到向量值情况，解决了伪逆映射的不连续性带来的问题。

    

    我们研究了对于步进式不确定性减少序贯实验设计策略在向量值情况下的一致性结果，这些结果建立在《关于高斯过程基于序贯实验设计的超级鞅方法，Bernoulli 25, 2019》一文中。首先，我们就紧致索引集的情况下，阐明了连续高斯过程与连续函数Banach空间上的高斯测度之间的联系如何推广到向量值情况。在此基础上，可以轻松地推广一些上述文献中的概念和性质。然而，向量值情况对于一些结果确实增加了复杂性，主要是由于伪逆映射的不连续性影响了有限个点观察的条件均值和协方差函数。我们将获得的结果应用于集成伯努利方差和期望测度方差的不确定性函数。

    We tackle the extension to the vector-valued case of consistency results for Stepwise Uncertainty Reduction sequential experimental design strategies established in [Bect et al., A supermartingale approach to Gaussian process based sequential design of experiments, Bernoulli 25, 2019]. This lead us in the first place to clarify, assuming a compact index set, how the connection between continuous Gaussian processes and Gaussian measures on the Banach space of continuous functions carries over to vector-valued settings. From there, a number of concepts and properties from the aforementioned paper can be readily extended. However, vector-valued settings do complicate things for some results, mainly due to the lack of continuity for the pseudo-inverse mapping that affects the conditional mean and covariance function given finitely many pointwise observations. We apply obtained results to the Integrated Bernoulli Variance and the Expected Measure Variance uncertainty functionals employed in
    
[^10]: 用于高效准确的溯源预测的分子编辑模板

    Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction. (arXiv:2310.07313v1 [cs.LG])

    [http://arxiv.org/abs/2310.07313](http://arxiv.org/abs/2310.07313)

    METRO是一个机器学习模型，利用最小模板进行反应预测，解决了溯源预测中的计算开销大和解释性差的问题，实现了最先进的效果。

    

    溯源涉及确定从简单前体合成复杂分子的一系列反应。由于这在有机化学中是一个挑战，机器学习提供了解决方案，特别是用于预测给定目标分子的可能反应底物。这些解决方案主要分为基于模板和基于非模板两类。前者高效但依赖于大量预定义的反应模式，而后者虽然更灵活，但计算开销大且解释性较差。为解决这些问题，我们引入了METRO（用于溯源合成的分子编辑模板），这是一个利用最小模板进行反应预测的机器学习模型，简化了反应模式，减少了计算开销，并在标准基准测试中取得了最先进的结果。

    Retrosynthesis involves determining a sequence of reactions to synthesize complex molecules from simpler precursors. As this poses a challenge in organic chemistry, machine learning has offered solutions, particularly for predicting possible reaction substrates for a given target molecule. These solutions mainly fall into template-based and template-free categories. The former is efficient but relies on a vast set of predefined reaction patterns, while the latter, though more flexible, can be computationally intensive and less interpretable. To address these issues, we introduce METRO (Molecule-Edit Templates for RetrOsynthesis), a machine-learning model that predicts reactions using minimal templates - simplified reaction patterns capturing only essential molecular changes - reducing computational overhead and achieving state-of-the-art results on standard benchmarks.
    
[^11]: 为什么锐度感知最小化比随机梯度下降更好地推广？(arXiv:2310.07269v1 [cs.LG])

    Why Does Sharpness-Aware Minimization Generalize Better Than SGD?. (arXiv:2310.07269v1 [cs.LG])

    [http://arxiv.org/abs/2310.07269](http://arxiv.org/abs/2310.07269)

    这项研究填补了Sharpness-Aware Minimization（SAM）相对于随机梯度下降（SGD）的一定数据模型和卷积神经网络中泛化更好的空白，解释了SAM的优势，尤其是在早期阶段防止噪声学习的能力。

    

    过拟合的挑战在大型神经网络的训练中变得越来越重要，它指的是模型记忆训练数据，但在测试数据上无法推广。为了解决这个挑战，锐度感知最小化（SAM）已经成为一种有前途的训练方法，可以在存在标签噪声的情况下改善神经网络的泛化性能。然而，对于非线性神经网络和分类任务的情况下，SAM的工作方式仍然缺乏深入理解。本文通过展示为什么在特定数据模型和两层卷积ReLU网络中，SAM比随机梯度下降更好地推广，来弥补这一空白。我们所研究问题的损失景观是非光滑的，因此目前关于SAM成功的解释基于Hessian信息是不足够的。我们的结果解释了SAM的优势，特别是它在早期阶段防止了噪声学习的能力，从而提高了泛化性能。

    The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby f
    
[^12]: 结构健康监测应用中的随机裂纹扩展过程的代理模型

    Surrogate modeling for stochastic crack growth processes in structural health monitoring applications. (arXiv:2310.07241v1 [stat.ML])

    [http://arxiv.org/abs/2310.07241](http://arxiv.org/abs/2310.07241)

    本文提出了一种代理模型用于预测结构中裂纹的扩展，并成功地编码了不同的随机不确定性来源。该模型基于高斯过程回归模型，能够生成先验分布用于贝叶斯结构健康监测任务。

    

    疲劳裂纹扩展是金属结构中最常见的一种破坏类型，对其可靠性有重要影响。最近在结构健康监测领域的进展促使使用结构响应数据来预测不确定条件下未来的裂纹扩展，以实现向预测性维修的过渡。准确地表示随机裂纹扩展过程中不同的不确定性来源是一项非常困难的任务。本研究在基于物理模型的随机裂纹扩展建模的基础上进行了探索，考虑了材料和载荷相关的不确定性。本文旨在构建计算效率高、概率代理模型，能够成功地编码这些不同的不确定性来源。采用了受潜变量建模启发的方法，利用高斯过程回归模型使代理模型可以用于为不同的贝叶斯结构健康监测任务生成先验分布。

    Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as th
    
[^13]: 在流形上通过黎曼扩散过程的混合生成模型

    Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes. (arXiv:2310.07216v1 [cs.LG])

    [http://arxiv.org/abs/2310.07216](http://arxiv.org/abs/2310.07216)

    通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。

    

    在非欧几里得空间中建模数据的分布对于来自不同科学领域的许多应用都至关重要。然而，现有的流形上的生成模型存在着计算复杂的散度或依赖于热核的近似的问题。这些限制限制了它们在简单几何形状上的适用性，并阻碍了在高维空间中的可扩展性。在这项工作中，我们引入了黎曼扩散混合模型，这是一个在流形上构建生成过程的原则性框架，它是一组以端点条件扩散过程作为混合的生成过程，而不依赖于先前扩散模型的去噪方法，对于这些模型，生成过程的特性是它的漂移导向与流形的几何形状相对应的最可能的终点。我们进一步提出了一个简单而高效的训练目标，用于学习混合过程，它可以直接应用于一般流形。我们的方法表现得很好。

    Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outp
    
[^14]: 神经网络：深层、浅层还是中间层？

    Neural networks: deep, shallow, or in between?. (arXiv:2310.07190v1 [stat.ML])

    [http://arxiv.org/abs/2310.07190](http://arxiv.org/abs/2310.07190)

    本研究探讨了深度和宽度对于神经网络的影响，结果表明只有深度趋近无穷大的神经网络才可能达到比熵数更好的速度，而固定深度并让宽度趋近无穷大则没有收益。

    

    我们给出了一种估计方法，用于测量通过宽度为W、深度为l的前馈神经网络以及满足Lipschitz激活函数的输出进行近似的误差。我们证明了，在对数因子解除，仅有深度l趋近无穷大的神经网络才有可能达到比熵数更好的速度，而如果固定深度然后让宽度W趋近无穷大，则没有任何收益。

    We give estimates from below for the error of approximation of a compact subset from a Banach space by the outputs of feed-forward neural networks with width W, depth l and Lipschitz activation functions. We show that, modulo logarithmic factors, rates better that entropy numbers' rates are possibly attainable only for neural networks for which the depth l goes to infinity, and that there is no gain if we fix the depth and let the width W go to infinity.
    
[^15]: 基于核Cox部分线性回归：构建癌症患者生存预测模型

    Kernel Cox partially linear regression: building predictive models for cancer patients' survival. (arXiv:2310.07187v1 [stat.ML])

    [http://arxiv.org/abs/2310.07187](http://arxiv.org/abs/2310.07187)

    本文提出了一种基于核Cox部分线性回归的方法来构建癌症患者生存预测模型，通过核机器方法描述复杂的生存和预测因子关系，并利用正则化加权核机器方法自动去除不相关的因子。与其他竞争方法相比，该方法在模拟中表现最好。

    

    癌症患者的生存存在广泛的异质性，从几个月到几十年不等。为了准确预测临床结果，建立一个能够将患者的分子特征与生存情况关联起来的精确预测模型至关重要。由于生存与高维分子预测因子之间存在复杂的关系，同时进行非参数建模和去除不相关的预测因子是具有挑战性的。本文建立了一个核Cox比例风险半参数模型，并提出了一种新颖的正则化加权核机器（RegGKM）方法来拟合模型。我们使用核机器方法描述生存和预测因子之间的复杂关系，同时通过LASSO惩罚自动去除不相关的参数和非参数预测因子。为所提出的方法开发了一个高维算法。与模拟中的其他竞争方法相比，所提出的方法总是表现最好。

    Wide heterogeneity exists in cancer patients' survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates patients' molecular profiles with patients' survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct non-parametric modeling and irrelevant predictors removing simultaneously. In this paper, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and non-parametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method alway
    
[^16]: 具有无误差的可微分交换函数的广义神经排序网络

    Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])

    [http://arxiv.org/abs/2310.07174](http://arxiv.org/abs/2310.07174)

    本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。

    

    排序是所有计算机系统的基本操作，一直是一个长期的重要研究课题。除了传统排序算法的问题表述，我们通过神经排序网络考虑了更抽象但具有表达力的输入，例如多位数字图像和图像片段。为了学习从高维输入到次序变量的映射，需要保证排序网络的可微分性。在本文中，我们通过可微分的交换函数定义一个柔化误差，并开发了一个无误差的交换函数，该函数满足非减和可微分的条件。此外，采用了具有多头注意力机制的置换等变Transformer网络，以捕捉给定输入之间的依赖关系，并利用其自注意力的模型能力。在多样的排序基准上进行的实验证明，我们的方法优于或与基准方法相当。

    Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
    
[^17]: 分布式学习中的指数量子通信优势

    Exponential Quantum Communication Advantage in Distributed Learning. (arXiv:2310.07136v1 [quant-ph])

    [http://arxiv.org/abs/2310.07136](http://arxiv.org/abs/2310.07136)

    在分布式学习中，我们提出了一个基于量子网络的框架，可以使用指数级较少的通信和相对较小的时间和空间复杂度开销进行推理和训练。这是第一个展示了具有密集经典数据的通用机器学习问题具有指数量子优势的例子。

    

    使用超过单个设备内存容量的大型机器学习模型进行训练和推理需要设计分布式架构，必须考虑通信限制。我们提出了一种在量子网络上进行分布式计算的框架，其中数据被编码为特殊的量子态。我们证明，在该框架内的某些模型中，使用梯度下降进行推理和训练的通信开销相对于其经典对应模型可以指数级降低，并且相对于标准基于梯度的方法，时间和空间复杂性开销相对较小。据我们所知，这是第一个在具有密集经典数据的通用机器学习问题的情况下，无论数据编码成本如何，都具有指数量子优势的示例。此外，我们还展示了该类模型可以编码输入的高度非线性特征，并且它们的表达能力呈指数增加。

    Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints. We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for certain models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest time and space complexity overheads relative to standard gradient-based methods. To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems with dense classical data that holds regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponenti
    
[^18]: 在基础模型时代的风险评估和统计显著性

    Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])

    [http://arxiv.org/abs/2310.07132](http://arxiv.org/abs/2310.07132)

    本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。

    

    我们提出了一个分布框架，用于评估具有统计显著性的基础模型的社会技术风险。我们的方法依赖于一种基于实际随机变量的一阶和二阶随机优势的新的统计相对测试。我们表明，这个测试中的二阶统计与在计量经济学和数学金融中常用的平均风险模型相联系，用于在选择方案时平衡风险和效用。利用这个框架，我们正式开发了一种基于风险意识的基础模型选择方法，给定由指定度量量化的防护栏。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，作为聚合一系列度量的手段，并根据这些组合的随机优势进行模型选择。我们的测试的统计显著性在理论上由通过中心极限的渐近分析支持。

    We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
    
[^19]: 无偏性政策学习与观测数据

    Positivity-free Policy Learning with Observational Data. (arXiv:2310.06969v1 [stat.ME])

    [http://arxiv.org/abs/2310.06969](http://arxiv.org/abs/2310.06969)

    本研究提出了一种无偏性的政策学习框架，该框架利用观测数据进行政策学习，并克服了现实情境中无法满足假设的难题。该框架利用增量倾向分数策略调整倾向分数值，从而实现了快速的收敛率。

    

    利用观测数据进行政策学习在各个领域都非常重要，其目标是学习最优的处理分配策略，同时满足特定的约束条件，如公平性、预算和简单性。本研究引入了一种新颖的无偏性（随机）政策学习框架，旨在应对现实情境中无法满足假设的困境。该框架利用增量倾向分数策略来调整倾向分数值，而不是给治疗分配固定值。我们对这些增量倾向分数策略进行了表征，并建立了识别条件，利用半参数效率理论提出了能够实现快速收敛率的高效估计器，即使是与先进的机器学习算法集成在一起。本文对政策学习的理论保证进行了深入探讨，并验证了所提出的框架。

    Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed fr
    
[^20]: 面向制造业的AI孵化的上下文强化学习集成活跃学习方法

    Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. (arXiv:2310.06306v1 [cs.LG])

    [http://arxiv.org/abs/2310.06306](http://arxiv.org/abs/2310.06306)

    本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。

    

    工业物联网系统中的在线感知和计算资源促进了基于AI的决策。然而，数据质量问题，如类别不平衡，阻碍了离线训练的AI模型。为了解决这个问题，AI模型会通过流式数据进行在线更新以持续改进。然而，由于注释约束，监督学习模型在选择用于更新的优质流式样本方面面临挑战。文献中的主动学习方法通过关注不足或过度表示的区域来提供解决方案。在不断变化的制造背景下平衡这些策略是具有挑战性的。AI学习到的一些获取准则可以动态适应，但可能无法始终处理频繁的变化。我们引入了一种集成主动学习方法CBEAL，专门利用主动学习代理进行探索或利用。代理的权重根据决策有效性进行调整。CBEAL可以优化地指导数据获取，实现数据效果的最小化。

    Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minim
    
[^21]: 多步模型的基于模型的强化学习

    Multi-timestep models for Model-based Reinforcement Learning. (arXiv:2310.05672v1 [cs.LG])

    [http://arxiv.org/abs/2310.05672](http://arxiv.org/abs/2310.05672)

    多步模型的基于模型的强化学习算法通过使用多步目标来训练一步模型，解决了轨迹长度增长时一步预测误差的累积问题，并在噪声数据上表现出显著的性能提升。

    

    在基于模型的强化学习中，大多数算法依赖于从数据中学习到的一步动力学模型来模拟轨迹。这种方法的一个关键挑战是随着轨迹长度的增长，一步预测误差的累积。本文通过使用多步目标来训练一步模型来解决这个问题。我们的目标是在各种未来时间段上的一个损失函数（例如，负对数似然）的加权和。我们探索和测试了一系列权重方案。我们发现指数衰减权重导致模型在长时间段的R2得分显著提高。当模型在噪声数据上进行评估时，这种改进尤为明显。最后，我们在纯批量强化学习（RL）和迭代批量RL场景中使用软件演员-评论家（SAC）代理，发现我们的多步模型优于或与标准的一步模型相匹配。这在考虑环境的噪声变体中尤为明显。

    In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered envi
    
[^22]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^23]: Entropy-MCMC: 轻松从平坦盆地进行采样

    Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05401](http://arxiv.org/abs/2310.05401)

    本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。

    

    贝叶斯深度学习依赖于对后验分布的质量估计。然而，深度神经网络的后验分布在性质上是高度多模态的，局部模式表现出不同的泛化性能。在有限的计算资源下，从原始后验分布中进行采样可能会导致次优性能，因为一些样本可能会陷入“坏”模式并出现过拟合。基于观察到低泛化误差的“好”模式通常存在于能量景观的平坦盆地中，我们提出通过偏置采样朝向这些平坦区域的后验。具体而言，我们引入了一个辅助引导变量，其稳态分布类似于平滑后验分布，并且没有尖锐的模态，以引导MCMC采样器在平坦的盆地中采样。通过将此引导变量与模型参数相结合，我们创建了一个简单的联合分布，可以在最小计算开销下实现高效采样。我们证明了我们的元算法的收敛性。

    Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
    
[^24]: 基于交叉预测的推理

    Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])

    [http://arxiv.org/abs/2309.16598](http://arxiv.org/abs/2309.16598)

    本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。

    

    可靠的数据驱动决策依赖于高质量的标注数据，然而获取高质量的标注数据经常需要繁琐的人工标注或者缓慢昂贵的科学测量。机器学习作为一种替代方案正变得越来越有吸引力，因为精密的预测技术可以快速、廉价地产生大量预测标签；例如，预测的蛋白质结构被用来补充实验得到的结构，卫星图像预测的社会经济指标被用来补充准确的调查数据等。由于预测具有不完美和潜在偏差的特点，这种做法对下游推理的有效性产生了质疑。我们引入了基于机器学习的交叉预测方法，用于有效的推理。通过一个小的标记数据集和一个大的未标记数据集，交叉预测通过机器学习填补缺失的标签，并应用一种去偏差的方法来纠正预测不准确性。

    While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
    
[^25]: 关于免疫的概率的研究

    On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])

    [http://arxiv.org/abs/2309.11942](http://arxiv.org/abs/2309.11942)

    本文研究了免疫的概率，提出了免疫的必要和充分条件，以及ε-有界免疫的条件。同时，借助随机对照试验估计受益概率，并得到比现有边界更紧密的概率边界。此外，介绍了间接免疫的概念，并提出了一种用于处理未测量混淆的免疫概率敏感性分析方法。

    

    本文致力于研究免疫的概率，即无论暴露与否，效果都会发生。我们导出了免疫的必要和充分条件以及ε-有界免疫的条件，前者允许我们从随机对照试验中估计受益的概率（即只有在暴露的情况下效果才会发生），后者允许我们得到比现有的边界更紧密的受益概率边界。我们还引入了间接免疫的概念（通过介质），并对其进行了前述分析。最后，我们提出了一种用于在未测量混淆情况下进行免疫概率敏感性分析的方法。

    This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
    
[^26]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^27]: 基于贝叶斯上下文树状态空间模型的时间序列建模和预测

    The Bayesian Context Trees State Space Model for time series modelling and forecasting. (arXiv:2308.00913v1 [stat.ME])

    [http://arxiv.org/abs/2308.00913](http://arxiv.org/abs/2308.00913)

    该论文介绍了基于贝叶斯上下文树状态空间模型的时间序列建模和预测方法，通过层级贝叶斯框架将离散状态和实值时间序列模型组合，构建出灵活且可解释的混合模型，并提出了有效的算法来进行贝叶斯推断和预测。

    

    引入了一个层级贝叶斯框架，用于开发用于真实值时间序列的丰富混合模型，以及一系列有效的学习和推断工具。在顶层，通过适当量化最近样本的一些有意义的离散状态来进行鉴定。这些可观察状态的集合被描述为离散的上下文树模型。然后，在底层，将一个不同的、任意的实值时间序列模型（基本模型）与每个状态相关联。这定义了一个非常通用的框架，可以与任何现有模型类一起使用，构建灵活且可解释的混合模型。我们将其称为贝叶斯上下文树状态空间模型，或者BCT-X框架。引入了高效的算法，可以实现有效的、精确的贝叶斯推断；特别是可以确定最大后验概率（MAP）上下文树模型。这些算法可以顺序更新，以便实现有效的推断和预测。

    A hierarchical Bayesian framework is introduced for developing rich mixture models for real-valued time series, along with a collection of effective tools for learning and inference. At the top level, meaningful discrete states are identified as appropriately quantised values of some of the most recent samples. This collection of observable states is described as a discrete context-tree model. Then, at the bottom level, a different, arbitrary model for real-valued time series - a base model - is associated with each state. This defines a very general framework that can be used in conjunction with any existing model class to build flexible and interpretable mixture models. We call this the Bayesian Context Trees State Space Model, or the BCT-X framework. Efficient algorithms are introduced that allow for effective, exact Bayesian inference; in particular, the maximum a posteriori probability (MAP) context-tree model can be identified. These algorithms can be updated sequentially, facili
    
[^28]: 深度网络逼近：从ReLU到多种激活函数

    Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])

    [http://arxiv.org/abs/2307.06555](http://arxiv.org/abs/2307.06555)

    本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。

    

    本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。

    This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
    
[^29]: DYffusion：面向时空预测的动态扩散模型

    DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. (arXiv:2306.01984v1 [cs.LG])

    [http://arxiv.org/abs/2306.01984](http://arxiv.org/abs/2306.01984)

    提出了一种新的扩散模型，其结合了数据中编码的时间动态，自然地编码了多步和长程预测能力，具有灵活的采样轨迹和折衷性能与加速采样的能力，同时提高了计算效率，可在时空预测方面取得竞争性表现。

    

    尽管扩散模型可以成功地生成数据和做出预测，但它们主要是为静态图像设计的。我们提出了一种方法，可以训练用于动态预测的扩散模型，利用编码在数据中的时间动态，直接将其与网络中的扩散步骤耦合。我们训练了一个随机的、时间条件的插值器和一个骨干预测网络，分别模仿传统扩散模型的前向和后向过程。这种设计选择自然地编码了多步和长程预测能力，允许高度灵活的连续时间采样轨迹，并在推理时能够折衷性能与加速采样的能力。此外，面向动态的扩散过程强加了强的归纳偏差，相比传统基于高斯噪声的扩散模型，可以提高计算效率。我们的方法在概率滑雪预测任务上表现出竞争力。

    While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for training diffusion models for dynamics forecasting that leverages the temporal dynamics encoded in the data, directly coupling it with the diffusion steps in the network. We train a stochastic, time-conditioned interpolator and a backbone forecaster network that mimic the forward and reverse processes of conventional diffusion models, respectively. This design choice naturally encodes multi-step and long-range forecasting capabilities, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process imposes a strong inductive bias, allowing for improved computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic ski
    
[^30]: 通过草图技术，将粒子方法和张量网络方法结合用于偏微分方程求解

    Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching. (arXiv:2305.17884v1 [math.NA])

    [http://arxiv.org/abs/2305.17884](http://arxiv.org/abs/2305.17884)

    本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。

    

    本文提出了一种解决高维偏微分方程的张量网络框架，其中我们采用粒子模拟更新解决方案，并使用最近提出的张量列车草图技术将新解决方案重新估计为张量网络。我们的方法还可以被解释为通过假设粒子来自底层张量网络来执行粒子数控制的可替代方法。我们通过将其应用于两种特定的情景来展示我们方法的通用性和灵活性：通过Langevin动力学模拟Fokker-Planck方程和通过辅助场量子蒙特卡罗模拟量子虚时间演化。

    In this paper, we propose a general framework for solving high-dimensional partial differential equations with tensor networks. Our approach offers a comprehensive solution methodology, wherein we employ a combination of particle simulations to update the solution and re-estimations of the new solution as a tensor-network using a recently proposed tensor train sketching technique. Our method can also be interpreted as an alternative approach for performing particle number control by assuming the particles originate from an underlying tensor network. We demonstrate the versatility and flexibility of our approach by applying it to two specific scenarios: simulating the Fokker-Planck equation through Langevin dynamics and quantum imaginary time evolution via auxiliary-field quantum Monte Carlo.
    
[^31]: 最大边缘似然估计的相互作用粒子 Langevin 算法

    Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation. (arXiv:2303.13429v1 [stat.CO])

    [http://arxiv.org/abs/2303.13429](http://arxiv.org/abs/2303.13429)

    本文提出了一种相互作用粒子 Langevin 算法，用于最大边缘似然估计。使用此算法，估计器的优化误差具有非渐近浓度界限。

    

    本文研究了一类相互作用粒子系统，用于实现潜变量模型参数的最大边缘似然估计过程。为此，我们提出了一种连续时间相互作用粒子系统，它可以被看作是在扩展的状态空间上的 Langevin漂移，其中在经典的优化中，粒子数量作为相反温度参数。使用Langevin漂移，我们证明了最大边缘似然估计器的优化误差的非渐近浓度界限，这些界限与粒子系统中的粒子数量，算法的迭代次数以及时间离散化分析的步长参数有关。

    We study a class of interacting particle systems for implementing a marginal maximum likelihood estimation (MLE) procedure to optimize over the parameters of a latent variable model. To do so, we propose a continuous-time interacting particle system which can be seen as a Langevin diffusion over an extended state space, where the number of particles acts as the inverse temperature parameter in classical settings for optimisation. Using Langevin diffusions, we prove nonasymptotic concentration bounds for the optimisation error of the maximum marginal likelihood estimator in terms of the number of particles in the particle system, the number of iterations of the algorithm, and the step-size parameter for the time discretisation analysis.
    
[^32]: 通过深度学习实现时间序列中的自动变点检测

    Automatic Change-Point Detection in Time Series via Deep Learning. (arXiv:2211.03860v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.03860](http://arxiv.org/abs/2211.03860)

    本文介绍了一种通过训练神经网络自动生成新的离线检测方法的方式，应用于时间序列中的自动变点检测，其性能可与标准CUSUM分类器性能竞争。

    

    数据中的变点检测具有挑战性，我们通过训练神经网络自动生成新的离线检测方法，并提出了一种理论量化此方法的误差率及其与训练数据量的关系。实证结果表明，即使有限的训练数据，其性能也可与用于检测中变化的标准CUSUM分类器的性能竞争。

    Detecting change-points in data is challenging because of the range of possible types of change and types of behaviour of data when there is no change. Statistically efficient methods for detecting a change will depend on both of these features, and it can be difficult for a practitioner to develop an appropriate detection method for their application of interest. We show how to automatically generate new offline detection methods based on training a neural network. Our approach is motivated by many existing tests for the presence of a change-point being representable by a simple neural network, and thus a neural network trained with sufficient data should have performance at least as good as these methods. We present theory that quantifies the error rate for such an approach, and how it depends on the amount of training data. Empirical results show that, even with limited training data, its performance is competitive with the standard CUSUM-based classifier for detecting a change in m
    
[^33]: 平滑的嵌套模拟方法：在高维度中桥接立方和平方根收敛率

    Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions. (arXiv:2201.02958v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2201.02958](http://arxiv.org/abs/2201.02958)

    本文提出了一种嵌套模拟的新方法，它能够在保持条件期望足够平滑的情况下，有效地缓解高维度中的维度灾难，以桥接标准嵌套模拟的立方根收敛率和标准蒙特卡洛模拟的平方根收敛率之间的差距。

    

    嵌套模拟是通过模拟来估计条件期望的功能。本文提出了一种基于核岭回归的新方法，以利用条件期望作为多维调节变量的平滑函数。渐近分析表明，只要条件期望足够平滑，所提出的方法能够在模拟次数增加时有效地减少维度灾难的影响。平滑性桥接了立方根收敛率（即标准嵌套模拟的最优收敛率）和平方根收敛率（即标准蒙特卡洛模拟的规范收敛率）之间的差距。我们通过组合风险管理和输入不确定性量化的数值示例，展示了所提出方法的性能。

    Nested simulation concerns estimating functionals of a conditional expectation via simulation. In this paper, we propose a new method based on kernel ridge regression to exploit the smoothness of the conditional expectation as a function of the multidimensional conditioning variable. Asymptotic analysis shows that the proposed method can effectively alleviate the curse of dimensionality on the convergence rate as the simulation budget increases, provided that the conditional expectation is sufficiently smooth. The smoothness bridges the gap between the cubic root convergence rate (that is, the optimal rate for the standard nested simulation) and the square root convergence rate (that is, the canonical rate for the standard Monte Carlo simulation). We demonstrate the performance of the proposed method via numerical examples from portfolio risk management and input uncertainty quantification.
    
[^34]: 机器学习方法用于建模城市土地使用中的停车时长问题

    A Machine Learning Approach for Modelling Parking Duration in Urban Land-use. (arXiv:2008.01674v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2008.01674](http://arxiv.org/abs/2008.01674)

    本研究提出了一个机器学习模型来分析汽车用户的特征对停车时长的影响，采用人工神经网络捕捉其相互关系，并运用Garson算法和LIME进行模型解释。

    

    停车是发展中国家中不可避免的问题。随着车辆数量的增加，需要为停车分配更多的城市土地。然而，在印度等发展中国家，停车问题一直没有得到足够的关注。本研究提出了一个模型，用于分析汽车用户的社会经济和出行特征对停车时长的影响。具体而言，采用人工神经网络（ANNs）来捕捉驾驶员特征和停车时长之间的相互关系。ANNs在学习和识别参数之间的连接上非常高效，以最佳预测结果。然而，由于其黑盒特性，ANNs的实用性受到严重限制。因此，本研究使用Garson算法和局部可解释模型不受限的解释（LIME）进行模型解释。LIME通过将局部数据与开发的可解释模型进行局部逼近，展示了任何分类的预测结果。

    Parking is an inevitable issue in the fast-growing developing countries. Increasing number of vehicles require more and more urban land to be allocated for parking. However, a little attention has been conferred to the parking issues in developing countries like India. This study proposes a model for analysing the influence of car users' socioeconomic and travel characteristics on parking duration. Specifically, artificial neural networks (ANNs) is deployed to capture the interrelationship between driver characteristics and parking duration. ANNs are highly efficient in learning and recognizing connections between parameters for best prediction of an outcome. Since, utility of ANNs has been critically limited due to its Black Box nature, the study involves the use of Garson algorithm and Local interpretable model-agnostic explanations (LIME) for model interpretations. LIME shows the prediction for any classification, by approximating it locally with the developed interpretable model. T
    
[^35]: 高维度和普遍一致的k样本检验

    High-dimensional and universally consistent k-sample tests. (arXiv:1910.08883v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1910.08883](http://arxiv.org/abs/1910.08883)

    本文证明了独立性测试实现了普遍一致的k样本检验，并且发现非参数独立性测试通常比多元方差分析(MANOVA)测试在高斯分布情况下表现更好。

    

    k样本检验问题涉及确定$k$组数据点是否都来自同一个分布。尽管多元方差分析(MANOVA)是生物医学中常用的k样本检验方法，但它依赖于强大且通常不合适的参数假设。此外，独立性测试和k样本测试密切相关，一些普遍一致的高维独立性测试，如距离相关(Discrepancy)和Hilbert-Schmidt独立性准则(Hsic)，具有坚实的理论和实证性质。在本文中，我们证明了独立性测试实现了普遍一致的k样本检验，并且k样本统计量，如Energy和Maximum Mean Discrepancy(MMD)，与Discrepancy完全等价。对非参数独立性测试的实证评估表明，它们通常比流行的MANOVA测试表现更好，即使在高斯分布的场景中也是如此。

    The k-sample testing problem involves determining whether $k$ groups of data points are each drawn from the same distribution. The standard method for k-sample testing in biomedicine is Multivariate analysis of variance (MANOVA), despite that it depends on strong, and often unsuitable, parametric assumptions. Moreover, independence testing and k-sample testing are closely related, and several universally consistent high-dimensional independence tests such as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic) enjoy solid theoretical and empirical properties. In this paper, we prove that independence tests achieve universally consistent k-sample testing and that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD) are precisely equivalent to Dcorr. An empirical evaluation of nonparametric independence tests showed that they generally perform better than the popular MANOVA test, even in Gaussian distributed scenarios. The evaluation included se
    

