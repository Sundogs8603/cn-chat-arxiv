# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Penalized deep neural networks estimator with general loss functions under weak dependence.](http://arxiv.org/abs/2305.06230) | 本文提出了一种稀疏惩罚深度神经网络预测方法，适用于学习弱相关性过程，并在特定情况下使用$\theta_\infty$系数。文中还提供了相应的神谕不等式和收敛速度，对于目标函数足够光滑的情况，超额风险的收敛速度接近于$\mathcal{O}(n^{-1/3})$。其中提供了模拟结果和应用于预测Vitória颗粒物的案例。 |
| [^2] | [Computationally Efficient and Statistically Optimal Robust High-Dimensional Linear Regression.](http://arxiv.org/abs/2305.06199) | 本文介绍了一种计算高效且统计优化的稀疏线性回归和低秩线性回归方法，适用于高维数据下的异常噪声情况。 |
| [^3] | [A proof of convergence of inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.06137) | 本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。 |
| [^4] | [Best Arm Identification in Bandits with Limited Precision Sampling.](http://arxiv.org/abs/2305.06082) | 本文研究了在有限精度下采样的多臂赌博机问题，提出了一种修改的跟踪算法来处理最优分配的非唯一性，并证明了其渐进最优性。 |
| [^5] | [Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods.](http://arxiv.org/abs/2305.06044) | 本文比较了不同的缺失数据处理方法对相关图的影响，建议使用直接参数估计法(DPER)来绘制相关图 |
| [^6] | [Pearson-Matthews correlation coefficients for binary and multinary classification and hypothesis testing.](http://arxiv.org/abs/2305.05974) | 本文介绍了Pearson-Matthews相关系数（MCC）及其扩展用于多元分类的指标，包括$\text{R}_{\text{K}}$、$\text{MCC}_k$和$\text{Q}_k$。此外，还提供了计算这些系数的实用建议，并展示了它们的性能和适用性作为假设检验。 |
| [^7] | [Fair principal component analysis (PCA): minorization-maximization algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA.](http://arxiv.org/abs/2305.05963) | 本文提出了一种新的迭代算法，可以解决公平主成分分析问题，并提出了处理数据中的离群值和约束估计的公平主成分稀疏度的算法。该算法高效且每次迭代都会增加各自的设计目标。 |
| [^8] | [Best-Effort Adaptation.](http://arxiv.org/abs/2305.05816) | 研究了最佳努力适应性问题，提出了一种新的基于差异的理论分析方法以及用于标准域适应性问题的改进学习算法，表现出很好的实验效果。 |
| [^9] | [Testing for Overfitting.](http://arxiv.org/abs/2305.05792) | 本文提出了一种能够使用训练数据进行评估模型性能的假设检验方法，可以准确地定义和检测过拟合。 |
| [^10] | [On the average-case complexity of learning output distributions of quantum circuits.](http://arxiv.org/abs/2305.05765) | 本文证明了砖墙随机量子电路输出分布学习是一个平均复杂度困难的问题，需要进行超多项式次数的查询才能有效解决。 |
| [^11] | [Ranking & Reweighting Improves Group Distributional Robustness.](http://arxiv.org/abs/2305.05759) | 本文提出了一种利用折扣累积增益（DCG）排序并加权处理训练数据以提高模型对低代表性组的鲁棒性的方法，实验证明其优于先前方法。 |
| [^12] | [An ensemble of convolution-based methods for fault detection using vibration signals.](http://arxiv.org/abs/2305.05532) | 本文提出了集成了三种基于卷积的方法，用于解决振动信号的故障检测问题。实验结果表明，该方法在准确率上优于其他方法，达到了超过98.8\%的准确率。 |
| [^13] | [Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization.](http://arxiv.org/abs/2303.10758) | 本论文证明了在平稳随机凸优化中，GD和SGD的泛化下界可以降低，并且长时间的训练可能导致更差的泛化能力，这与其他研究成果不同。 |
| [^14] | [Fast Attention Requires Bounded Entries.](http://arxiv.org/abs/2302.13214) | 本文研究了内积关注计算的快速算法问题，提出了两个结果，证明了在B = O(sqrt(log n))时存在一种n ^（1 + O（1））时间算法。 |
| [^15] | [Approximately Bayes-Optimal Pseudo Label Selection.](http://arxiv.org/abs/2302.08883) | 本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。 |
| [^16] | [Optimally-Weighted Estimators of the Maximum Mean Discrepancy for Likelihood-Free Inference.](http://arxiv.org/abs/2301.11674) | 该论文提出了一种新的MMD估计方法，其样本复杂度显着提高，特别适用于计算成本昂贵、平滑的模拟器和低维到中维的输入。 |
| [^17] | [Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features.](http://arxiv.org/abs/2212.13881) | 本文确定并表征了深度全连接神经网络学习特征的机制，提出了深度神经特征假设并解释了深度学习现象，同时也引领了对递归学习特征的核方法中特征学习的更广泛的理解。 |
| [^18] | [Double Robust Bayesian Inference on Average Treatment Effects.](http://arxiv.org/abs/2211.16298) | 本文研究了双重鲁棒贝叶斯推断程序，实现了平均处理效应的偏差校正并形成了可信区间。 |
| [^19] | [$2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations.](http://arxiv.org/abs/2211.01703) | 本文研究了带承诺和噪声观测的$2\times 2$零和博弈，发现平衡点总是存在的；领导者的动作观测结果对于追随者来说要么是有益的，要么是无关紧要的；该博弈的收益在均衡点上被上界限制为纯策略下的SE的收益，下界为混合策略下的纳什均衡的收益。 |
| [^20] | [A Double Machine Learning Trend Model for Citizen Science Data.](http://arxiv.org/abs/2210.15524) | 该论文提出了一种新颖的双机器学习趋势模型，能够在控制公民科学数据中的年际混淆的同时，估计物种种群趋势，具有较高的实用性和应用价值。 |
| [^21] | [Instance-dependent uniform tail bounds for empirical processes.](http://arxiv.org/abs/2209.10053) | 该论文提出了一个经验过程的统一尾部界，该尾部界以函数的个体偏差而不是在考虑的类中的最坏情况偏差为基础。 |
| [^22] | [From Modern CNNs to Vision Transformers: Assessing the Performance, Robustness, and Classification Strategies of Deep Learning Models in Histopathology.](http://arxiv.org/abs/2204.05044) | 本文评估了现代CNN和视觉Transformer模型在组织病理学中的性能、鲁棒性和分类策略。在乳腺癌、胃癌和结直肠癌全切片图像等五个数据集上进行了广泛测试，结果表明ViT模型在分类准确性方面优于其他CNN，而Swin Transformer模型在对抗染色变化的鲁棒性方面表现最佳。 |
| [^23] | [Improved Image Wasserstein Attacks and Defenses.](http://arxiv.org/abs/2004.12478) | 本文提出了一种基于Wasserstein距离限制的更好的威胁模型，可以更准确地限制图像扰动。作者指出并纠正了先前Wasserstein威胁模型定义中的缺陷，并展示了更强的攻击和防御。然而，当前的Wasserstein-鲁棒模型在防御现实世界中出现的扰动方面可能受到限制。 |

# 详细

[^1]: 带有一般损失函数的惩罚深度神经网络估计器在弱相关性下的应用

    Penalized deep neural networks estimator with general loss functions under weak dependence. (arXiv:2305.06230v1 [stat.ML])

    [http://arxiv.org/abs/2305.06230](http://arxiv.org/abs/2305.06230)

    本文提出了一种稀疏惩罚深度神经网络预测方法，适用于学习弱相关性过程，并在特定情况下使用$\theta_\infty$系数。文中还提供了相应的神谕不等式和收敛速度，对于目标函数足够光滑的情况，超额风险的收敛速度接近于$\mathcal{O}(n^{-1/3})$。其中提供了模拟结果和应用于预测Vitória颗粒物的案例。

    

    本文使用广泛的损失函数，对于学习弱相关性过程进行了稀疏惩罚深度神经网络预测。我们处理包括回归估计、分类、时间序列预测在内的一般框架，考虑了$\psi$弱相关结构，并针对有界观测的特定情况，也使用了$\theta_\infty$系数。在这种$\theta_\infty$弱相依的情况下，提供了一种在深度神经网络预测器类中提供非渐近泛化界限。对于学习$\psi$和$\theta_\infty$弱相关的过程，确定了稀疏惩罚深度神经网络估计器超额风险的神谕不等式。当目标函数足够光滑时，这些超额风险的收敛速度接近于$\mathcal{O}(n^{-1/3})$。提供了一些模拟结果，并应用于Vitória颗粒物的预测。

    This paper carries out sparse-penalized deep neural networks predictors for learning weakly dependent processes, with a broad class of loss functions. We deal with a general framework that includes, regression estimation, classification, times series prediction, $\cdots$ The $\psi$-weak dependence structure is considered, and for the specific case of bounded observations, $\theta_\infty$-coefficients are also used. In this case of $\theta_\infty$-weakly dependent, a non asymptotic generalization bound within the class of deep neural networks predictors is provided. For learning both $\psi$ and $\theta_\infty$-weakly dependent processes, oracle inequalities for the excess risk of the sparse-penalized deep neural networks estimators are established. When the target function is sufficiently smooth, the convergence rate of these excess risk is close to $\mathcal{O}(n^{-1/3})$. Some simulation results are provided, and application to the forecast of the particulate matter in the Vit\'{o}ria
    
[^2]: 高维稳健线性回归的计算高效和统计优化研究

    Computationally Efficient and Statistically Optimal Robust High-Dimensional Linear Regression. (arXiv:2305.06199v1 [math.ST])

    [http://arxiv.org/abs/2305.06199](http://arxiv.org/abs/2305.06199)

    本文介绍了一种计算高效且统计优化的稀疏线性回归和低秩线性回归方法，适用于高维数据下的异常噪声情况。

    

    在重尾噪声或异常值污染下进行高维线性回归具有挑战性，无论是计算上还是统计上。凸优化方法已被证明在统计上是最优的，但通常由于鲁棒损失函数是非光滑的而产生高计算成本。最近，通过子梯度下降提出了计算速度快的非凸优化方法，但即使在子高斯噪声下，这些方法也无法提供统计一致估计。本文介绍了一种用于稀疏线性回归和低秩线性回归问题的投影子梯度下降算法。该算法不仅具有线性收敛的计算效率，而且在噪声为高斯噪声，或具有有限1 + epsilon矩的重尾噪声下，也具有统计优化的性质。收敛理论适用于一个通用框架，并研究了其对绝对值损失、Huber损失和分位数损失的具体应用。

    High-dimensional linear regression under heavy-tailed noise or outlier corruption is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since the robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a projected sub-gradient descent algorithm for both the sparse linear regression and low-rank linear regression problems. The algorithm is not only computationally efficient with linear convergence but also statistically optimal, be the noise Gaussian or heavy-tailed with a finite 1 + epsilon moment. The convergence theory is established for a general framework and its specific applications to absolute loss, Huber loss and quantile loss are investigated. Compar
    
[^3]: 多目标优化的逆强化学习的收敛性证明研究

    A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])

    [http://arxiv.org/abs/2305.06137](http://arxiv.org/abs/2305.06137)

    本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。

    

    本文通过将等效于多目标优化的WIRL问题的逆问题与投影次梯度法相结合，证明了Wasserstein逆强化学习（WIRL）在多目标优化中的收敛性。此外，我们还证明了逆强化学习（最大熵逆强化学习，导引成本学习）在多目标优化中的收敛性。

    We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
    
[^4]: 有限精度采样下的赌博机最优臂识别问题研究

    Best Arm Identification in Bandits with Limited Precision Sampling. (arXiv:2305.06082v1 [cs.LG])

    [http://arxiv.org/abs/2305.06082](http://arxiv.org/abs/2305.06082)

    本文研究了在有限精度下采样的多臂赌博机问题，提出了一种修改的跟踪算法来处理最优分配的非唯一性，并证明了其渐进最优性。

    

    本文研究了一种多臂赌博机问题的变体，在该问题中，学习者在选择臂时有限的精度。学习者只能通过特定的探索组合（即所谓的箱子）采样臂。具体而言，在每个采样时刻，学习者选择一个箱子，然后根据箱子特定的概率分布拉动臂，揭示被拉动的臂及其瞬时收益，其目标是通过最小化期望停止时间来找到最佳臂，而误差概率受到上限约束。

    We study best arm identification in a variant of the multi-armed bandit problem where the learner has limited precision in arm selection. The learner can only sample arms via certain exploration bundles, which we refer to as boxes. In particular, at each sampling epoch, the learner selects a box, which in turn causes an arm to get pulled as per a box-specific probability distribution. The pulled arm and its instantaneous reward are revealed to the learner, whose goal is to find the best arm by minimising the expected stopping time, subject to an upper bound on the error probability. We present an asymptotic lower bound on the expected stopping time, which holds as the error probability vanishes. We show that the optimal allocation suggested by the lower bound is, in general, non-unique and therefore challenging to track. We propose a modified tracking-based algorithm to handle non-unique optimal allocations, and demonstrate that it is asymptotically optimal. We also present non-asympto
    
[^5]: 缺失值下的相关性可视化：填充法和直接参数估计法的比较

    Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods. (arXiv:2305.06044v1 [cs.LG])

    [http://arxiv.org/abs/2305.06044](http://arxiv.org/abs/2305.06044)

    本文比较了不同的缺失数据处理方法对相关图的影响，建议使用直接参数估计法(DPER)来绘制相关图

    

    相关矩阵可视化对于理解数据集中变量之间的关系至关重要，但是缺失数据会对相关系数的估计产生显著挑战。本文比较了不同的缺失数据处理方法对相关图的影响，重点关注两种常见的缺失模式：随机和单调。我们旨在为研究人员和实践者提供实用的策略和建议，以创建和分析相关图。我们的实验结果表明，虽然填充法通常用于缺失数据，但使用填充的数据来生成相关矩阵图可能会导致对特征之间关系的误导性推断。我们建议基于其在实验中的表现，使用DPER，一种直接参数估计方法，绘制相关矩阵图。

    Correlation matrix visualization is essential for understanding the relationships between variables in a dataset, but missing data can pose a significant challenge in estimating correlation coefficients. In this paper, we compare the effects of various missing data methods on the correlation plot, focusing on two common missing patterns: random and monotone. We aim to provide practical strategies and recommendations for researchers and practitioners in creating and analyzing the correlation plot. Our experimental results suggest that while imputation is commonly used for missing data, using imputed data for plotting the correlation matrix may lead to a significantly misleading inference of the relation between the features. We recommend using DPER, a direct parameter estimation approach, for plotting the correlation matrix based on its performance in the experiments.
    
[^6]: 二分类和多分类及假设检验中的Pearson-Matthews相关系数

    Pearson-Matthews correlation coefficients for binary and multinary classification and hypothesis testing. (arXiv:2305.05974v1 [eess.SP])

    [http://arxiv.org/abs/2305.05974](http://arxiv.org/abs/2305.05974)

    本文介绍了Pearson-Matthews相关系数（MCC）及其扩展用于多元分类的指标，包括$\text{R}_{\text{K}}$、$\text{MCC}_k$和$\text{Q}_k$。此外，还提供了计算这些系数的实用建议，并展示了它们的性能和适用性作为假设检验。

    

    Pearson-Matthews相关系数（通常缩写为MCC）被认为是用于二元分类或假设检验方法性能的最有用的指标之一（为了简洁，我们将始终使用分类术语，但本文中讨论的概念和方法完全适用于假设检验）。对于多元分类任务（具有两个以上的类），现有的MCC扩展通常称为$\text{R}_{\text{K}}$指标，在许多应用中也已经成功使用。本文首先介绍了MCC的某些方面，然后继续讨论本文的主要焦点——多元分类——尽管其在实践和理论上非常重要，但看起来没有二元分类主题那么发达。我们的$\text{R}_{\text{K}}$讨论后是引入另外两种用于多元分类的度量，分别为$\text{MCC}_k$和$\text{Q}_k$。该论文还提供了计算这些系数的实用建议，并提供了模拟结果，以说明它们的性能及其作为假设检验的适用性。

    The Pearson-Matthews correlation coefficient (usually abbreviated MCC) is considered to be one of the most useful metrics for the performance of a binary classification or hypothesis testing method (for the sake of conciseness we will use the classification terminology throughout, but the concepts and methods discussed in the paper apply verbatim to hypothesis testing as well). For multinary classification tasks (with more than two classes) the existing extension of MCC, commonly called the $\text{R}_{\text{K}}$ metric, has also been successfully used in many applications. The present paper begins with an introductory discussion on certain aspects of MCC. Then we go on to discuss the topic of multinary classification that is the main focus of this paper and which, despite its practical and theoretical importance, appears to be less developed than the topic of binary classification. Our discussion of the $\text{R}_{\text{K}}$ is followed by the introduction of two other metrics for mult
    
[^7]: 公平主成分分析（PCA）：公平PCA，公平鲁棒PCA和公平稀疏PCA的最小化-最大化算法

    Fair principal component analysis (PCA): minorization-maximization algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA. (arXiv:2305.05963v1 [stat.ML])

    [http://arxiv.org/abs/2305.05963](http://arxiv.org/abs/2305.05963)

    本文提出了一种新的迭代算法，可以解决公平主成分分析问题，并提出了处理数据中的离群值和约束估计的公平主成分稀疏度的算法。该算法高效且每次迭代都会增加各自的设计目标。

    

    本文提出了一种解决公平PCA（FPCA）问题的新迭代算法。我们从[1]提出的最大最小公平PCA公式开始，推导出一个简单而高效的迭代算法，它基于最小化最大化（MM）方法。所提出的算法依赖于半正交约束的松弛，证明了算法的每次迭代都很紧密。所提议的算法的香草版需要在每次迭代中解决一个半正定规划（SDP），可以通过构造代理最大化问题的对偶问题将其进一步简化为一个二次规划。我们还提出了公平PCA问题的两个重要重构：a）公平鲁棒PCA——可以处理数据中的离群值；b）公平稀疏PCA——可以对估计的公平主成分进行稀疏约束。所提议的算法在计算上非常高效，并且会单调增加各自的设计目标。

    In this paper we propose a new iterative algorithm to solve the fair PCA (FPCA) problem. We start with the max-min fair PCA formulation originally proposed in [1] and derive a simple and efficient iterative algorithm which is based on the minorization-maximization (MM) approach. The proposed algorithm relies on the relaxation of a semi-orthogonality constraint which is proved to be tight at every iteration of the algorithm. The vanilla version of the proposed algorithm requires solving a semi-definite program (SDP) at every iteration, which can be further simplified to a quadratic program by formulating the dual of the surrogate maximization problem. We also propose two important reformulations of the fair PCA problem: a) fair robust PCA -- which can handle outliers in the data, and b) fair sparse PCA -- which can enforce sparsity on the estimated fair principal components. The proposed algorithms are computationally efficient and monotonically increase their respective design objectiv
    
[^8]: 最佳努力适应性

    Best-Effort Adaptation. (arXiv:2305.05816v1 [cs.LG])

    [http://arxiv.org/abs/2305.05816](http://arxiv.org/abs/2305.05816)

    研究了最佳努力适应性问题，提出了一种新的基于差异的理论分析方法以及用于标准域适应性问题的改进学习算法，表现出很好的实验效果。

    

    我们研究了一个由多个应用和考虑因素激发出的最佳努力适应性问题，其中包括确定一个精确的预测器以用于目标域，虽然只有适量的已标记样本可用，但利用来自另一个拥有大量已标记样本的域的信息。我们提出了一种新的和通用的基于差异的理论分析样本重新加权方法，包括在权重上均匀保持的界限。我们展示了这些边界如何指导我们详细讨论的学习算法的设计。我们进一步展示了我们的学习保证和算法为标准域适应性问题提供了改进的解决方案，其中目标域只有少量标记数据或没有标记数据可用。最后，我们报告了一系列实验的结果，展示了我们的最佳努力适应性和域适应算法的有效性，以及与几个基线的比较。

    We study a problem of best-effort adaptation motivated by several applications and considerations, which consists of determining an accurate predictor for a target domain, for which a moderate amount of labeled samples are available, while leveraging information from another domain for which substantially more labeled samples are at one's disposal. We present a new and general discrepancy-based theoretical analysis of sample reweighting methods, including bounds holding uniformly over the weights. We show how these bounds can guide the design of learning algorithms that we discuss in detail. We further show that our learning guarantees and algorithms provide improved solutions for standard domain adaptation problems, for which few labeled data or none are available from the target domain. We finally report the results of a series of experiments demonstrating the effectiveness of our best-effort adaptation and domain adaptation algorithms, as well as comparisons with several baselines. 
    
[^9]: 过拟合的测试

    Testing for Overfitting. (arXiv:2305.05792v1 [stat.ML])

    [http://arxiv.org/abs/2305.05792](http://arxiv.org/abs/2305.05792)

    本文提出了一种能够使用训练数据进行评估模型性能的假设检验方法，可以准确地定义和检测过拟合。

    

    在机器学习中，高复杂度的模型常见过拟合现象，即模型能够很好地代表数据，但无法推广到基础数据生成过程。解决过拟合的典型方法是在留置集上计算经验风险，一旦风险开始增加，就停止（或标记何时停止）。虽然这种方法输出了良好泛化的模型，但其实现原理主要是启发式的。本文讨论过拟合问题，解释了为什么使用训练数据进行评估时，标准渐近和浓度结果不成立。我们随后提出并阐述了一个假设检验，通过该检验可以对使用训练数据评估模型性能，并量化地定义和检测过拟合。我们依靠确保经验均值应该高概率地近似其真实均值的浓度界限，以得出他们应该相互接近的结论。

    High complexity models are notorious in machine learning for overfitting, a phenomenon in which models well represent data but fail to generalize an underlying data generating process. A typical procedure for circumventing overfitting computes empirical risk on a holdout set and halts once (or flags that/when) it begins to increase. Such practice often helps in outputting a well-generalizing model, but justification for why it works is primarily heuristic.  We discuss the overfitting problem and explain why standard asymptotic and concentration results do not hold for evaluation with training data. We then proceed to introduce and argue for a hypothesis test by means of which both model performance may be evaluated using training data, and overfitting quantitatively defined and detected. We rely on said concentration bounds which guarantee that empirical means should, with high probability, approximate their true mean to conclude that they should approximate each other. We stipulate co
    
[^10]: 关于量子电路输出分布学习的平均复杂度

    On the average-case complexity of learning output distributions of quantum circuits. (arXiv:2305.05765v1 [quant-ph])

    [http://arxiv.org/abs/2305.05765](http://arxiv.org/abs/2305.05765)

    本文证明了砖墙随机量子电路输出分布学习是一个平均复杂度困难的问题，需要进行超多项式次数的查询才能有效解决。

    

    本文研究了砖墙随机量子电路的输出分布学习问题，并证明在统计查询模型下，该问题的平均复杂度是困难的。具体地，对于深度为$d$、由$n$个量子比特构成的砖墙随机量子电路，我们得出了三个主要结论：在超对数电路深度$d=\omega(\log(n))$时，任何学习算法都需要进行超多项式次数的查询才能在随机实例上实现恒定的成功概率。存在一个$d=O(n)$，这意味着任何学习算法需要进行$\Omega(2^n)$次查询才能在随机实例上实现$O(2^{-n})$的成功概率。在无限电路深度$d\to\infty$时，任何学习算法都需要进行$2^{2^{\Omega(n)}}$次查询才能在随机实例上实现$2^{-2^{\Omega(n)}}$的成功概率。作为一个独立的辅助结果，我们还证明了......（文章内容截断）

    In this work, we show that learning the output distributions of brickwork random quantum circuits is average-case hard in the statistical query model. This learning model is widely used as an abstract computational model for most generic learning algorithms. In particular, for brickwork random quantum circuits on $n$ qubits of depth $d$, we show three main results:  - At super logarithmic circuit depth $d=\omega(\log(n))$, any learning algorithm requires super polynomially many queries to achieve a constant probability of success over the randomly drawn instance.  - There exists a $d=O(n)$, such that any learning algorithm requires $\Omega(2^n)$ queries to achieve a $O(2^{-n})$ probability of success over the randomly drawn instance.  - At infinite circuit depth $d\to\infty$, any learning algorithm requires $2^{2^{\Omega(n)}}$ many queries to achieve a $2^{-2^{\Omega(n)}}$ probability of success over the randomly drawn instance.  As an auxiliary result of independent interest, we show 
    
[^11]: 排名和重新加权提高了组的分布鲁棒性

    Ranking & Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])

    [http://arxiv.org/abs/2305.05759](http://arxiv.org/abs/2305.05759)

    本文提出了一种利用折扣累积增益（DCG）排序并加权处理训练数据以提高模型对低代表性组的鲁棒性的方法，实验证明其优于先前方法。

    

    最近的研究表明，通过经验风险最小化（ERM）进行标准训练可能会产生在平均精度上表现出色但在低代表性组上准确性较低的模型，这是由于表征中虚假特征的普遍存在所致。解决这个组鲁棒性问题的主要方法是在训练数据上最小化最坏的组误差（类似于极小值策略），希望它会在测试数据上有良好的泛化性能。然而，这种方法往往是次优的，尤其是当测试数据集中包含以前未见过的组时。本文受信息检索和Learning-to-Rank文献的启发，首先提出使用折扣累积增益（DCG）作为模型质量度量标准，以促进更好的超参数调整和模型选择。作为一种基于排序的度量标准，DCG加权多个性能较差的组（而不仅仅是考虑性能最差的组）。作为自然的下一步，我们基于这些结果提出了一种新的组重新加权方法，在训练期间鼓励模型集中于低代表性的组。在几个基准数据集上的实验证明，我们提出的方法优于先前的最先进方法，并显著提高了对组不平衡的鲁棒性。

    Recent work has shown that standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on underrepresented groups due to the prevalence of spurious features. A predominant approach to tackle this group robustness problem minimizes the worst group error (akin to a minimax strategy) on the training data, hoping it will generalize well on the testing data. However, this is often suboptimal, especially when the out-of-distribution (OOD) test data contains previously unseen groups. Inspired by ideas from the information retrieval and learning-to-rank literature, this paper first proposes to use Discounted Cumulative Gain (DCG) as a metric of model quality for facilitating better hyperparameter tuning and model selection. Being a ranking-based metric, DCG weights multiple poorly-performing groups (instead of considering just the group with the worst performance). As a natural next step, we build on our results to propose a
    
[^12]: 基于卷积的方法集合用于振动信号的故障检测

    An ensemble of convolution-based methods for fault detection using vibration signals. (arXiv:2305.05532v1 [eess.SP])

    [http://arxiv.org/abs/2305.05532](http://arxiv.org/abs/2305.05532)

    本文提出了集成了三种基于卷积的方法，用于解决振动信号的故障检测问题。实验结果表明，该方法在准确率上优于其他方法，达到了超过98.8\%的准确率。

    

    本文研究了使用测试平台上行星齿轮箱振动信号的多元时间序列解决故障检测问题。对于多元时间序列分类问题，常见的机器学习和深度学习方法包括基于距离、基于功能数据、基于特征和基于卷积核的方法。最近的研究表明，使用ROCKET、ResNet和FCN等基于卷积核的方法对多元时间序列数据分类具有强大的性能。我们提出了三种基于卷积核的方法的集合，并通过优于其他方法并实现超过98.8\%准确率的实验结果，证明了其在解决故障检测问题中的有效性。

    This paper focuses on solving a fault detection problem using multivariate time series of vibration signals collected from planetary gearboxes in a test rig. Various traditional machine learning and deep learning methods have been proposed for multivariate time-series classification, including distance-based, functional data-oriented, feature-driven, and convolution kernel-based methods. Recent studies have shown using convolution kernel-based methods like ROCKET, and 1D convolutional neural networks with ResNet and FCN, have robust performance for multivariate time-series data classification. We propose an ensemble of three convolution kernel-based methods and show its efficacy on this fault detection problem by outperforming other approaches and achieving an accuracy of more than 98.8\%.
    
[^13]: 平稳随机凸优化中GD和SGD的泛化下界降低

    Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization. (arXiv:2303.10758v1 [cs.LG])

    [http://arxiv.org/abs/2303.10758](http://arxiv.org/abs/2303.10758)

    本论文证明了在平稳随机凸优化中，GD和SGD的泛化下界可以降低，并且长时间的训练可能导致更差的泛化能力，这与其他研究成果不同。

    

    最近，学习理论界在刻画一般凸损失梯度方法的泛化误差方面取得了进展。本文侧重于讨论在泛化光滑随机凸优化（SCO）问题中训练时间如何影响泛化能力。我们首先为一般的不可实现SCO问题提供了严格的下界。此外，现有的上界结果表明，假设损失可实现（即最优解同时最小化所有数据点）可以提高样本复杂度。但是，当训练时间长且缺乏下界时，这种改进会受到损害。我们对此进行了研究，提供了对于梯度下降（GD）和随机梯度下降（SGD）在两种可实现情况下的过量风险下界：1）实现需$T = O(n)$，和（2）实现需$T = \Omega(n)$，其中$T$表示训练迭代次数，$n$为训练数据集的大小。这些下界的证明使用了来自优化的现代工具，包括对偶理论和镜像下降。我们的结果表明，在可实现的SCO中，更长的训练时间可能会导致更差的泛化，这与文献中的先前发现形成鲜明对比。我们还提供了支持我们结果的数值实验。

    Recent progress was made in characterizing the generalization error of gradient methods for general convex loss by the learning theory community. In this work, we focus on how training longer might affect generalization in smooth stochastic convex optimization (SCO) problems. We first provide tight lower bounds for general non-realizable SCO problems. Furthermore, existing upper bound results suggest that sample complexity can be improved by assuming the loss is realizable, i.e. an optimal solution simultaneously minimizes all the data points. However, this improvement is compromised when training time is long and lower bounds are lacking. Our paper examines this observation by providing excess risk lower bounds for gradient descent (GD) and stochastic gradient descent (SGD) in two realizable settings: 1) realizable with $T = O(n)$, and (2) realizable with $T = \Omega(n)$, where $T$ denotes the number of training iterations and $n$ is the size of the training dataset. These bounds are 
    
[^14]: 快速注意力需要有界条目

    Fast Attention Requires Bounded Entries. (arXiv:2302.13214v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13214](http://arxiv.org/abs/2302.13214)

    本文研究了内积关注计算的快速算法问题，提出了两个结果，证明了在B = O(sqrt(log n))时存在一种n ^（1 + O（1））时间算法。

    

    在现代机器学习中，内积关注计算是训练大型语言模型（如Transformer，GPT-1，BERT，GPT-2，GPT-3和ChatGPT）的基本任务。形式上，在这个问题中，输入三个矩阵$Q，K，V \in [-B，B]^{n \times d}$，目标是构造矩阵$\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A V \in \mathbb{R}^{n \times d}$，其中 $A = \exp(QK^\top/d)$ 是“注意力矩阵”，$\exp$是分量应用。本文研究是否可以通过隐式利用矩阵 $A$ 来实现更快的算法。我们提出了两个结果，证明了在 $B = \Theta(\sqrt{\log n})$处存在一个尖锐的转换。$\bullet$ 如果 $d = O(\log n)$，$B = o(\sqrt{\log n})$，则存在一个$n^{1+o(1)}$时间算法来近似$\mathbb{R}^{n \times d}$中的 $\mathrm{Att}(Q,K,V)$。

    In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $Q, K, V \in [-B,B]^{n \times d}$, and the goal is to construct the matrix $\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A V \in \mathbb{R}^{n \times d}$, where $A = \exp(QK^\top/d)$ is the `attention matrix', and $\exp$ is applied entry-wise. Straightforward methods for this problem explicitly compute the $n \times n$ attention matrix $A$, and hence require time $\Omega(n^2)$ even when $d = n^{o(1)}$ is small.  In this paper, we investigate whether faster algorithms are possible by implicitly making use of the matrix $A$. We present two results, showing that there is a sharp transition at $B = \Theta(\sqrt{\log n})$.  $\bullet$ If $d = O(\log n)$ and $B = o(\sqrt{\log n})$, there is an $n^{1+o(1)}$ time algorithm to approximate $\math
    
[^15]: 近乎贝叶斯最优的伪标签选择

    Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08883](http://arxiv.org/abs/2302.08883)

    本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。

    

    自训练的半监督学习严重依赖于伪标签选择（PLS）。选择通常取决于初始模型拟合标记数据的程度。过早的过拟合可能通过选择具有过度自信但错误的预测的实例（通常称为确认偏差）而传播到最终模型。本文介绍了BPLS，这是一种用于PLS的贝叶斯框架，旨在减轻这个问题。其核心是选择标签实例的标准：伪样本的后验预测的分析近似。我们通过证明伪样本的后验预测的贝叶斯最优性获得了这种选择标准。我们进一步通过解析逼近克服计算难题。它与边际似然的关系使我们能够提出基于拉普拉斯方法和高斯积分的逼近。我们针对参数广义线性和非参数广义加性模型对BPLS进行了实证评估。

    Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
    
[^16]: 无似然推断的最大均值差异的最优加权估计

    Optimally-Weighted Estimators of the Maximum Mean Discrepancy for Likelihood-Free Inference. (arXiv:2301.11674v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.11674](http://arxiv.org/abs/2301.11674)

    该论文提出了一种新的MMD估计方法，其样本复杂度显着提高，特别适用于计算成本昂贵、平滑的模拟器和低维到中维的输入。

    

    无似然推断方法通常使用模拟数据和真实数据之间的距离。其中一种常见方法是最大均值差异（MMD），其先前已用于近似贝叶斯计算、最小距离估计、广义贝叶斯推断和非参数学习框架中。 MMD通常以根- $m$速率进行估计，其中$ m $是模拟样本数。这可能会导致重大的计算挑战，因为需要大量的$ m $才能获得准确的估计结果，这对于参数估计至关重要。在本文中，我们提出了一种新的MMD估计方法，其样本复杂度显着提高。 这个估计器特别适用于计算昂贵，平滑的模拟器和低维到中维的输入。该论文通过理论结果和对基准模拟器的广泛模拟研究支持该主张。

    Likelihood-free inference methods typically make use of a distance between simulated and real data. A common example is the maximum mean discrepancy (MMD), which has previously been used for approximate Bayesian computation, minimum distance estimation, generalised Bayesian inference, and within the nonparametric learning framework. The MMD is commonly estimated at a root-$m$ rate, where $m$ is the number of simulated samples. This can lead to significant computational challenges since a large $m$ is required to obtain an accurate estimate, which is crucial for parameter estimation. In this paper, we propose a novel estimator for the MMD with significantly improved sample complexity. The estimator is particularly well suited for computationally expensive smooth simulators with low- to mid-dimensional inputs. This claim is supported through both theoretical results and an extensive simulation study on benchmark simulators.
    
[^17]: 深度全连接网络和递归学习特征的核机器的特征学习机制

    Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. (arXiv:2212.13881v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13881](http://arxiv.org/abs/2212.13881)

    本文确定并表征了深度全连接神经网络学习特征的机制，提出了深度神经特征假设并解释了深度学习现象，同时也引领了对递归学习特征的核方法中特征学习的更广泛的理解。

    

    近年来，神经网络在许多技术和科学任务中取得了令人瞩目的成果。然而，这些模型自动选择用于预测的特征或数据模式的机制仍不清楚。确定这样的机制是推动神经网络性能和可解释性以及促进这些模型在科学应用中可靠采用的关键。在本文中，我们确定并表征了深度全连接神经网络学习特征的机制。我们提出了深度神经特征假设，该假设表明神经特征学习是通过实现平均梯度外积来加强与模型输出密切相关的特征。我们的假设揭示了各种深度学习现象，包括假特征的出现和简单性偏差以及如何修剪网络可以提高性能，《彩票假设》。此外，我们的工作中确定的机制也引领了对递归学习特征的核方法中特征学习的更广泛的理解。

    In recent years neural networks have achieved impressive results on many technological and scientific tasks. Yet, the mechanism through which these models automatically select features, or patterns in data, for prediction remains unclear. Identifying such a mechanism is key to advancing performance and interpretability of neural networks and promoting reliable adoption of these models in scientific applications. In this paper, we identify and characterize the mechanism through which deep fully connected neural networks learn features. We posit the Deep Neural Feature Ansatz, which states that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output. Our ansatz sheds light on various deep learning phenomena including emergence of spurious features and simplicity biases and how pruning networks can increase performance, the "lottery ticket hypothesis." Moreover, the mechanism identified in our work leads to a
    
[^18]: 平均处理效应的双重鲁棒贝叶斯推断

    Double Robust Bayesian Inference on Average Treatment Effects. (arXiv:2211.16298v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2211.16298](http://arxiv.org/abs/2211.16298)

    本文研究了双重鲁棒贝叶斯推断程序，实现了平均处理效应的偏差校正并形成了可信区间。

    

    我们研究了无偏性下的平均处理效应（ATE）的双重鲁棒贝叶斯推断程序。我们的鲁棒贝叶斯方法包括两个调整步骤：首先，我们对条件均值函数的先验分布进行校正；其次，我们在产生的ATE的后验分布上引入一个重新居中术语。我们通过建立双重鲁棒性下的半参数Bernstein-von Mises定理，证明了我们的贝叶斯估计量和双重鲁棒频率估计量的渐近等价性；即，条件均值函数的缺乏平滑性可以通过概率得分的高规则性进行补偿，反之亦然。因此，产生的贝叶斯点估计内在化了频率型双重鲁棒估计量的偏差校正，而贝叶斯可信集形成的置信区间具有渐近精确的覆盖概率。在模拟中，我们发现这种鲁棒的贝叶斯程序导致了显着的...

    We study a double robust Bayesian inference procedure on the average treatment effect (ATE) under unconfoundedness. Our robust Bayesian approach involves two adjustment steps: first, we make a correction for prior distributions of the conditional mean function; second, we introduce a recentering term on the posterior distribution of the resulting ATE. We prove asymptotic equivalence of our Bayesian estimator and double robust frequentist estimators by establishing a new semiparametric Bernstein-von Mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. Consequently, the resulting Bayesian point estimator internalizes the bias correction as the frequentist-type doubly robust estimator, and the Bayesian credible sets form confidence intervals with asymptotically exact coverage probability. In simulations, we find that this robust Bayesian procedure leads to significant
    
[^19]: 带承诺和噪声观测的$2\times 2$零和博弈研究

    $2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations. (arXiv:2211.01703v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2211.01703](http://arxiv.org/abs/2211.01703)

    本文研究了带承诺和噪声观测的$2\times 2$零和博弈，发现平衡点总是存在的；领导者的动作观测结果对于追随者来说要么是有益的，要么是无关紧要的；该博弈的收益在均衡点上被上界限制为纯策略下的SE的收益，下界为混合策略下的纳什均衡的收益。

    

    本文研究了在以下假设下的$2\times 2$零和博弈：$(1)$其中一位玩家（领导者）承诺通过采样给定的概率分布（策略）来选择他的动作;$(2)$领导者宣布他的动作，这个动作通过二进制信道被对手（追随者）观察到;$(3)$追随者基于领导者的策略和领导者动作的噪声观测来选择她的策略。在这些条件下，平衡点被证明总是存在的。有趣的是，即使受到噪声的影响，观察领导者的行动对追随者来说实质上要么是有益的，要么是无关紧要的。具体而言，在这个博弈的均衡点上，收益被上界限制为纯策略下SE的收益；并且下界为纳什均衡的收益，这等价于混合策略下的SE。最后，我们提供了必要和充分的条件来观察均衡点的收益。

    In this paper, $2\times2$ zero-sum games are studied under the following assumptions: $(1)$ One of the players (the leader) commits to choose its actions by sampling a given probability measure (strategy); $(2)$ The leader announces its action, which is observed by its opponent (the follower) through a binary channel; and $(3)$ the follower chooses its strategy based on the knowledge of the leader's strategy and the noisy observation of the leader's action. Under these conditions, the equilibrium is shown to always exist. Interestingly, even subject to noise, observing the actions of the leader is shown to be either beneficial or immaterial for the follower. More specifically, the payoff at the equilibrium of this game is upper bounded by the payoff at the Stackelberg equilibrium (SE) in pure strategies; and lower bounded by the payoff at the Nash equilibrium, which is equivalent to the SE in mixed strategies.Finally, necessary and sufficient conditions for observing the payoff at equi
    
[^20]: 一种用于公民科学数据的双机器学习趋势模型

    A Double Machine Learning Trend Model for Citizen Science Data. (arXiv:2210.15524v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2210.15524](http://arxiv.org/abs/2210.15524)

    该论文提出了一种新颖的双机器学习趋势模型，能够在控制公民科学数据中的年际混淆的同时，估计物种种群趋势，具有较高的实用性和应用价值。

    

    公民和社区科学(CS)数据集因为每年全球收集的大量数据具有估计种群变化的年际模式的巨大潜力。然而，允许许多 CS 项目收集大量数据的灵活协议通常缺乏保持一致的采样结构所必需的结构。这导致了年际混淆，因为观测过程随时间的改变与物种种群大小的改变混淆在一起。这篇论文描述了一种新颖的建模方法，旨在估计物种种群趋势，同时控制公民科学数据中普遍存在的年际混淆。该方法基于双机器学习，这是一个使用机器学习方法估计种群变化和用于调整数据中发现的混淆的倾向得分的统计框架。此外，我们开发了一种模拟方法，以识别和调整未能发现的残余混淆。

    1. Citizen and community-science (CS) datasets have great potential for estimating interannual patterns of population change given the large volumes of data collected globally every year. Yet, the flexible protocols that enable many CS projects to collect large volumes of data typically lack the structure necessary to keep consistent sampling across years. This leads to interannual confounding, as changes to the observation process over time are confounded with changes in species population sizes.  2. Here we describe a novel modeling approach designed to estimate species population trends while controlling for the interannual confounding common in citizen science data. The approach is based on Double Machine Learning, a statistical framework that uses machine learning methods to estimate population change and the propensity scores used to adjust for confounding discovered in the data. Additionally, we develop a simulation method to identify and adjust for residual confounding missed b
    
[^21]: 经验过程的实例相关的一致尾部界

    Instance-dependent uniform tail bounds for empirical processes. (arXiv:2209.10053v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2209.10053](http://arxiv.org/abs/2209.10053)

    该论文提出了一个经验过程的统一尾部界，该尾部界以函数的个体偏差而不是在考虑的类中的最坏情况偏差为基础。

    

    我们提出了一个经验过程的统一尾部界，该尾部界以函数类为指标，以函数的个体偏差而不是在考虑的类中的最坏情况偏差为基础。通过将标准通用链接论证引入一个最初的“泄气”步骤来建立尾部界。生成的尾部界有一个主要的复杂度组成部分，即一个关于泄气函数类的 Talagrand $\gamma$ 函数的变体，以及一个实例相关的偏差项，通过一个适当缩放的适当范数的版本来衡量。这些项都使用基于相关的母函数的某些系数来表达。当函数类在给定的（指数型）Orlicz空间中时，我们还提供了更明确的近似值来描述所提到的系数。

    We formulate a uniform tail bound for empirical processes indexed by a class of functions, in terms of the individual deviations of the functions rather than the worst-case deviation in the considered class. The tail bound is established by introducing an initial "deflation" step to the standard generic chaining argument. The resulting tail bound has a main complexity component, a variant of Talagrand's $\gamma$ functional for the deflated function class, as well as an instance-dependent deviation term, measured by an appropriately scaled version of a suitable norm. Both of these terms are expressed using certain coefficients formulated based on the relevant cumulant generating functions. We also provide more explicit approximations for the mentioned coefficients, when the function class lies in a given (exponential type) Orlicz space.
    
[^22]: 从现代CNN到视觉Transformer：评估深度学习模型在组织病理学中的性能、鲁棒性和分类策略

    From Modern CNNs to Vision Transformers: Assessing the Performance, Robustness, and Classification Strategies of Deep Learning Models in Histopathology. (arXiv:2204.05044v2 [eess.IV] CROSS LISTED)

    [http://arxiv.org/abs/2204.05044](http://arxiv.org/abs/2204.05044)

    本文评估了现代CNN和视觉Transformer模型在组织病理学中的性能、鲁棒性和分类策略。在乳腺癌、胃癌和结直肠癌全切片图像等五个数据集上进行了广泛测试，结果表明ViT模型在分类准确性方面优于其他CNN，而Swin Transformer模型在对抗染色变化的鲁棒性方面表现最佳。

    

    机器学习正在改变组织病理学领域，但该领域缺乏全面评估最新模型的方法，不仅要考虑简单的分类准确性，还要考虑其他质量要求。为此，我们开发了一种新的方法，对一系列分类模型进行了广泛评估，包括最新的视觉Transformer和卷积神经网络，如ConvNeXt、ResNet（BiT）、Inception、ViT和Swin Transformer，并在有监督或无监督预训练的情况下进行了测试。我们对包含乳腺癌、胃癌和结直肠癌全切片图像的五个广泛使用的组织病理学数据集进行了全面测试，并开发了一种新方法，使用图像转换模型来评估癌症分类模型对染色变化的鲁棒性。此外，我们扩展了现有的可解释性方法，系统地揭示了它们学到的特征。我们的评估表明，ViT模型在分类准确性方面优于其他CNN，而Swin Transformer模型在对抗染色变化的鲁棒性方面表现最佳。此外，我们证明了预训练可以提高大多数模型的分类性能和鲁棒性。最后，我们的分析揭示了这些模型学到的特征，包括空间频率信息和肿瘤特征。

    While machine learning is currently transforming the field of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classification accuracy. In order to fill this gap, we developed a new methodology to extensively evaluate a wide range of classification models, including recent vision transformers, and convolutional neural networks such as: ConvNeXt, ResNet (BiT), Inception, ViT and Swin transformer, with and without supervised or self-supervised pretraining. We thoroughly tested the models on five widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classification model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of th
    
[^23]: 提高图像Wasserstein攻击和防御的效果

    Improved Image Wasserstein Attacks and Defenses. (arXiv:2004.12478v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.12478](http://arxiv.org/abs/2004.12478)

    本文提出了一种基于Wasserstein距离限制的更好的威胁模型，可以更准确地限制图像扰动。作者指出并纠正了先前Wasserstein威胁模型定义中的缺陷，并展示了更强的攻击和防御。然而，当前的Wasserstein-鲁棒模型在防御现实世界中出现的扰动方面可能受到限制。

    

    近期文献中已经广泛研究了对于$\ell_p$限制下受到图像扰动的鲁棒性。但是，现实中的扰动很少表现出$\ell_p$威胁模型所假定的像素独立性。最近提出了一个基于Wasserstein距离限制的威胁模型，它限制扰动为像素质量移动。我们指出并纠正了先前Wasserstein威胁模型定义中的缺陷，并在我们更好地定义的框架下探索了更强的攻击和防御。最后，我们讨论了当前Wasserstein-鲁棒模型在防御现实世界中出现的扰动方面的无能为力。我们的代码和训练模型可在https://github.com/edwardjhu/improved_wasserstein找到。

    Robustness against image perturbations bounded by a $\ell_p$ ball have been well-studied in recent literature. Perturbations in the real-world, however, rarely exhibit the pixel independence that $\ell_p$ threat models assume. A recently proposed Wasserstein distance-bounded threat model is a promising alternative that limits the perturbation to pixel mass movements. We point out and rectify flaws in previous definition of the Wasserstein threat model and explore stronger attacks and defenses under our better-defined framework. Lastly, we discuss the inability of current Wasserstein-robust models in defending against perturbations seen in the real world. Our code and trained models are available at https://github.com/edwardjhu/improved_wasserstein .
    

