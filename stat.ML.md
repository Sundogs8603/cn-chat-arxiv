# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition.](http://arxiv.org/abs/2309.17395) | AV-CPL是一种用于音频-视觉语音识别的连续伪标记方法，通过使用同一个模型进行监督训练和伪标签生成，提高了VSR性能并保持了实用性能。 |
| [^2] | [Adversarial Imitation Learning from Visual Observations using Latent Information.](http://arxiv.org/abs/2309.17371) | 本文研究了从视觉观察中进行模仿学习的问题，提出了一种名为潜在对抗观察模仿的算法，通过结合离策略对抗学习技术和从观察序列中学习的代理状态的潜在表示来解决这个问题。实验证明，这种算法能与最先进的方法相匹配。 |
| [^3] | [Graph-based Neural Weather Prediction for Limited Area Modeling.](http://arxiv.org/abs/2309.17370) | 该论文提出了一种基于图像神经网络的有限区域天气预测方法，并通过使用北欧地区的本地模型进行了验证。 |
| [^4] | [Robust Stochastic Optimization via Gradient Quantile Clipping.](http://arxiv.org/abs/2309.17316) | 本文介绍了一种基于梯度分位数剪切的鲁棒性随机优化策略，适用于光滑目标且能容忍异常值和尾重样本。对于强凸目标，迭代收敛到集中分布并导出了估计误差的概率界。在非凸情况下，极限分布局部化在低梯度邻域上。使用滚动分位数实现的算法具有很强的鲁棒性和高效性。 |
| [^5] | [In search of dispersed memories: Generative diffusion models are associative memory networks.](http://arxiv.org/abs/2309.17290) | 本研究将生成扩散模型解释为基于能量的模型，证明其在训练离散模式时与现代Hopfield网络的能量函数等效。这种等效性使得我们可以将扩散模型的有监督训练解释为在权重结构中编码现代Hopfield网络的关联动力学的突触学习过程。 |
| [^6] | [The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation.](http://arxiv.org/abs/2309.17283) | 多重治疗和多个结果的并行研究在治疗效果估计中可以互相协助实现因果识别。 |
| [^7] | [Estimation and Inference in Distributional Reinforcement Learning.](http://arxiv.org/abs/2309.17262) | 本文研究了分布式强化学习中的估计和推断问题，通过使用等价确定法，在提供生成模型的情况下以高效的方式解决了分布式策略评估问题。 |
| [^8] | [Current Methods for Drug Property Prediction in the Real World.](http://arxiv.org/abs/2309.17161) | 该论文介绍了当前药物属性预测方法的实际应用情况，并强调了不同数据集和方法之间的关联。研究发现最好的预测方法取决于数据集，使用经过工程处理的特征与经典方法结合的效果较好。 |
| [^9] | [Efficient Agnostic Learning with Average Smoothness.](http://arxiv.org/abs/2309.17016) | 该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。 |
| [^10] | [Statistical physics, Bayesian inference and neural information processing.](http://arxiv.org/abs/2309.17006) | 该论文讨论了通过统计物理的视角来探索神经信息处理的课程讲义。主要内容包括贝叶斯推断与学习泛化的Gibbs描述的联系，广义线性模型作为背景剔除法的可控替代方法，以及线性和非线性降维技术。 |
| [^11] | [Controlling Continuous Relaxation for Combinatorial Optimization.](http://arxiv.org/abs/2309.16965) | 本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。 |
| [^12] | [Water quality prediction using machine learning and neural network approaches.](http://arxiv.org/abs/2309.16951) | 本研究通过比较线性回归、随机森林、XGBoost、LightGBM和MLP神经网络五种模型在佐治亚州预测水质pH值方面的效果，发现LightGBM表现最好。基于树的模型在回归问题中优势显著，而MLP神经网络对特征缩放敏感。同时，本研究还探讨了与原研究相比，机器学习模型能够取得更好性能的原因。 |
| [^13] | [Symmetry Leads to Structured Constraint of Learning.](http://arxiv.org/abs/2309.16932) | 本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。 |
| [^14] | [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing.](http://arxiv.org/abs/2309.16883) | 本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。 |
| [^15] | [Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach.](http://arxiv.org/abs/2309.16858) | 我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。 |
| [^16] | [Optimal Nonlinearities Improve Generalization Performance of Random Features.](http://arxiv.org/abs/2309.16846) | 通过研究等效模型的参数，本研究发现获得的参数可以定义一组最优非线性性，如二阶多项式和分段线性函数。这些非线性性能优化了泛化性能，无论其实际形式如何，对回归和分类问题均有效。 |
| [^17] | [A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression.](http://arxiv.org/abs/2309.16843) | 这项研究提出了一种在高维线性回归中进行经验贝叶斯估计的均值场方法，通过变分经验贝叶斯逼近先验分布并建立了渐近一致性，从而实现了计算上可行的贝叶斯推断。 |
| [^18] | [An analysis of the derivative-free loss method for solving PDEs.](http://arxiv.org/abs/2309.16829) | 本研究分析了一种无导数损失方法在使用神经网络求解椭圆型偏微分方程的方法。我们发现训练损失偏差与时间间隔和空间梯度成正比，与行走者大小成反比，同时时间间隔必须足够长。我们提供了数值测试结果以支持我们的分析。 |
| [^19] | [Intriguing properties of generative classifiers.](http://arxiv.org/abs/2309.16779) | 生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。 |
| [^20] | [Discovering environments with XRM.](http://arxiv.org/abs/2309.16748) | 本文提出了一种用于发现环境的算法 XRM，它通过训练两个孪生网络，每个网络从训练数据的一半中学习，并模仿其兄弟网络的错误分类，解决了现有方法需要依赖人工注释环境信息的问题。 |
| [^21] | [Implicit Gaussian process representation of vector fields over arbitrary latent manifolds.](http://arxiv.org/abs/2309.16746) | 这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。 |
| [^22] | [Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks.](http://arxiv.org/abs/2308.06203) | 这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。 |
| [^23] | [Leveraging Task Structures for Improved Identifiability in Neural Network Representations.](http://arxiv.org/abs/2306.14861) | 本文提出一种基于任务结构的可识别性理论，拓展了先前仅限于单任务分类的工作。任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放。在假设任务之间存在因果关系时，该方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。 |
| [^24] | [SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States.](http://arxiv.org/abs/2306.04817) | 本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。 |
| [^25] | [A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration.](http://arxiv.org/abs/2306.00740) | 深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。 |
| [^26] | [Chain of Log-Concave Markov Chains.](http://arxiv.org/abs/2305.19473) | 该论文提出了一种新的采样算法，基于对数凹条件概率密度，使用等向性高斯平滑来解决高维下抽样难题。 |
| [^27] | [Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks.](http://arxiv.org/abs/2305.16891) | 本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。 |
| [^28] | [On the Generalization Capacities of Neural Controlled Differential Equations.](http://arxiv.org/abs/2305.16791) | 本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。 |
| [^29] | [Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank.](http://arxiv.org/abs/2305.16038) | 在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。 |
| [^30] | [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning.](http://arxiv.org/abs/2305.15912) | 本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。 |
| [^31] | [Risk-Adaptive Approaches to Learning and Decision Making: A Survey.](http://arxiv.org/abs/2212.00856) | 本文调查了过去25年中风险测度的快速发展，介绍了其在各个领域的应用，以及与效用理论和分布鲁棒优化的关系，并指出了公平机器学习等新兴应用领域。 |
| [^32] | [Generalized Balancing Weights via Deep Neural Networks.](http://arxiv.org/abs/2211.07533) | 本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。 |
| [^33] | [Relations Between Adjacency and Modularity Graph Partitioning.](http://arxiv.org/abs/1505.03481) | 本文发现了邻接矩阵的特征向量与模块化矩阵的主特征向量之间的精确线性关系，并提出了一种逼近模块化矩阵主特征向量的方法。此外，还证明了归一化邻接聚类与归一化模块化聚类的等价性。实验结果显示，归一化邻接聚类的效率可以是归一化模块化聚类的两倍。 |

# 详细

[^1]: AV-CPL：用于音频-视觉语音识别的连续伪标记方法

    AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition. (arXiv:2309.17395v1 [cs.LG])

    [http://arxiv.org/abs/2309.17395](http://arxiv.org/abs/2309.17395)

    AV-CPL是一种用于音频-视觉语音识别的连续伪标记方法，通过使用同一个模型进行监督训练和伪标签生成，提高了VSR性能并保持了实用性能。

    

    音频-视觉语音包含了提供跨模态监督的同步音频和视觉信息，用于学习自动语音识别（ASR）和视觉语音识别（VSR）的表示。我们引入了连续伪标记方法（AV-CPL）用于音频-视觉语音识别，这是一种半监督的方法，通过对标记和未标记视频进行持续生成伪标签来训练音频-视觉语音识别模型（AVSR）。我们的模型从音频-视觉输入中训练语音识别，并可以使用音频和视觉模态进行语音识别，或者只使用一种模态。我们的方法使用相同的音频-视觉模型进行监督训练和伪标签生成，减轻了需要外部语音识别模型生成伪标签的需求。AV-CPL在LRS3数据集上显著提高了VSR性能，同时保持了实用的ASR和AVSR性能。

    Audio-visual speech contains synchronized audio and visual information that provides cross-modal supervision to learn representations for both automatic speech recognition (ASR) and visual speech recognition (VSR). We introduce continuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a semi-supervised method to train an audio-visual speech recognition (AVSR) model on a combination of labeled and unlabeled videos with continuously regenerated pseudo-labels. Our models are trained for speech recognition from audio-visual inputs and can perform speech recognition using both audio and visual modalities, or only one modality. Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels. AV-CPL obtains significant improvements in VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. Finally, using visual-only speech 
    
[^2]: 利用潜在信息从视觉观察中进行对抗性模仿学习

    Adversarial Imitation Learning from Visual Observations using Latent Information. (arXiv:2309.17371v1 [cs.LG])

    [http://arxiv.org/abs/2309.17371](http://arxiv.org/abs/2309.17371)

    本文研究了从视觉观察中进行模仿学习的问题，提出了一种名为潜在对抗观察模仿的算法，通过结合离策略对抗学习技术和从观察序列中学习的代理状态的潜在表示来解决这个问题。实验证明，这种算法能与最先进的方法相匹配。

    

    我们专注于从视觉观察中进行模仿学习的问题，学习代理只能访问专家的视频作为其唯一的学习源。这个框架的挑战包括缺乏专家的动作和环境的局部可观测性，因为地面真实状态只能从像素中推断出来。为了解决这个问题，我们首先对部分可观测环境中的模仿学习进行了理论分析。我们在专家和代理潜在状态转换分布之间的差异度上建立了学习代理子优度的上界。受到这个分析的启发，我们引入了一种称为潜在对抗观察模仿的算法，它将离策略对抗学习技术与从观察序列中学习的代理状态的潜在表示相结合。在高维连续机器人任务的实验证明，我们的算法与最先进的方法相匹配。

    We focus on the problem of imitation learning from visual observations, where the learning agent has access to videos of experts as its sole learning source. The challenges of this framework include the absence of expert actions and the partial observability of the environment, as the ground-truth states can only be inferred from pixels. To tackle this problem, we first conduct a theoretical analysis of imitation learning in partially observable environments. We establish upper bounds on the suboptimality of the learning agent with respect to the divergence between the expert and the agent latent state-transition distributions. Motivated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from Observations, which combines off-policy adversarial imitation techniques with a learned latent representation of the agent's state from sequences of observations. In experiments on high-dimensional continuous robotic tasks, we show that our algorithm matches state-of-t
    
[^3]: 基于图像神经网络的有限区域天气预测

    Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])

    [http://arxiv.org/abs/2309.17370](http://arxiv.org/abs/2309.17370)

    该论文提出了一种基于图像神经网络的有限区域天气预测方法，并通过使用北欧地区的本地模型进行了验证。

    

    高精度的机器学习方法在天气预报领域的应用为模拟大气的可能性带来了新的变革。在气候变化时代，获取像这样的高分辨率预报模型的能力变得越来越重要。虽然大多数现有的神经网络天气预报方法都是针对全球预测，但如何将这些技术应用于有限区域建模是一个重要问题。本文将基于图像的神经网络天气预测方法应用于有限区域，并提出了多尺度分层模型扩展。通过使用北欧地区的本地模型进行实验证实了我们的方法的有效性。

    The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
    
[^4]: 通过梯度分位数剪切实现鲁棒性随机优化

    Robust Stochastic Optimization via Gradient Quantile Clipping. (arXiv:2309.17316v1 [stat.ML])

    [http://arxiv.org/abs/2309.17316](http://arxiv.org/abs/2309.17316)

    本文介绍了一种基于梯度分位数剪切的鲁棒性随机优化策略，适用于光滑目标且能容忍异常值和尾重样本。对于强凸目标，迭代收敛到集中分布并导出了估计误差的概率界。在非凸情况下，极限分布局部化在低梯度邻域上。使用滚动分位数实现的算法具有很强的鲁棒性和高效性。

    

    我们提出了一种基于梯度范数分位数作为剪切阈值的策略，用于随机梯度下降 (SGD)。我们证明了这种新策略在光滑目标（凸或非凸）下提供了一种鲁棒且高效的优化算法，能够容忍尾重样本（包括无限方差）和数据流中的异常值，类似于 Huber 污染模型。我们的数学分析利用了恒定步长的 SGD 和马尔可夫链之间的联系，并以独特的方式处理剪切引入的偏差。对于强凸目标，我们证明迭代收敛到一个集中分布，并导出了最终估计误差的高概率界。在非凸情况下，我们证明极限分布局部化在低梯度邻域上。我们提出了一种使用滚动分位数实现此算法的方法，从而得到了一种高效的优化过程，具有很强的鲁棒性。

    We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantiles of the gradient norm as clipping thresholds. We prove that this new strategy provides a robust and efficient optimization algorithm for smooth objectives (convex or non-convex), that tolerates heavy-tailed samples (including infinite variance) and a fraction of outliers in the data stream akin to Huber contamination. Our mathematical analysis leverages the connection between constant step size SGD and Markov chains and handles the bias introduced by clipping in an original way. For strongly convex objectives, we prove that the iteration converges to a concentrated distribution and derive high probability bounds on the final estimation error. In the non-convex case, we prove that the limit distribution is localized on a neighborhood with low gradient. We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustn
    
[^5]: 搜索分散的记忆：生成扩散模型是关联记忆网络

    In search of dispersed memories: Generative diffusion models are associative memory networks. (arXiv:2309.17290v1 [stat.ML])

    [http://arxiv.org/abs/2309.17290](http://arxiv.org/abs/2309.17290)

    本研究将生成扩散模型解释为基于能量的模型，证明其在训练离散模式时与现代Hopfield网络的能量函数等效。这种等效性使得我们可以将扩散模型的有监督训练解释为在权重结构中编码现代Hopfield网络的关联动力学的突触学习过程。

    

    Hopfield网络被广泛用作神经科学中的简化理论模型，用于生物关联记忆。原始的Hopfield网络通过编码二元关联模式来存储记忆，从而产生了一种称为Hebbian学习规则的突触学习机制。现代的Hopfield网络可以通过使用高度非线性的能量函数来实现指数级容量扩展。然而，这些新模型的能量函数不能直接压缩为二元突触耦合，并且也不能直接提供新的突触学习规则。在本研究中，我们展示了生成扩散模型可以被解释为基于能量的模型，并且在训练离散模式时，它们的能量函数与现代的Hopfield网络相等。这种等价性使我们能够将扩散模型的有监督训练解释为在权重结构中编码现代Hopfield网络的关联动力学的突触学习过程。

    Hopfield networks are widely used in neuroscience as simplified theoretical models of biological associative memory. The original Hopfield networks store memories by encoding patterns of binary associations, which result in a synaptic learning mechanism known as Hebbian learning rule. Modern Hopfield networks can achieve exponential capacity scaling by using highly non-linear energy functions. However, the energy function of these newer models cannot be straightforwardly compressed into binary synaptic couplings and it does not directly provide new synaptic learning rules. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is equivalent to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure 
    
[^6]: 多重治疗和多个结果在治疗效果估计中的益处

    The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation. (arXiv:2309.17283v1 [stat.ME])

    [http://arxiv.org/abs/2309.17283](http://arxiv.org/abs/2309.17283)

    多重治疗和多个结果的并行研究在治疗效果估计中可以互相协助实现因果识别。

    

    在存在未观测混杂的情况下评估因果效应是一个具有挑战性的问题。现有研究利用代理变量或多重治疗来调整混杂偏差。尤其是后一种方法将单一结果的影响归因于多重治疗，可以估计混杂控制的潜在变量。然而，这些方法主要关注单一结果，而在许多实际场景中，更感兴趣的是研究对多个结果的影响。此外，这些结果通常与多个治疗相关。例如，重症监护病房（ICU）中，医疗提供者评估治疗对多个健康指标的有效性。为了适应这些场景，我们考虑了一个新的设置，即多重治疗和多个结果。然后我们证明，在这种设置中涉及多个结果的并行研究可以互相协助实现因果识别，即

    Assessing causal effects in the presence of unobserved confounding is a challenging problem. Existing studies leveraged proxy variables or multiple treatments to adjust for the confounding bias. In particular, the latter approach attributes the impact on a single outcome to multiple treatments, allowing estimating latent variables for confounding control. Nevertheless, these methods primarily focus on a single outcome, whereas in many real-world scenarios, there is greater interest in studying the effects on multiple outcomes. Besides, these outcomes are often coupled with multiple treatments. Examples include the intensive care unit (ICU), where health providers evaluate the effectiveness of therapies on multiple health indicators. To accommodate these scenarios, we consider a new setting dubbed as multiple treatments and multiple outcomes. We then show that parallel studies of multiple outcomes involved in this setting can assist each other in causal identification, in the sense that
    
[^7]: 分布式强化学习中的估计和推断

    Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])

    [http://arxiv.org/abs/2309.17262](http://arxiv.org/abs/2309.17262)

    本文研究了分布式强化学习中的估计和推断问题，通过使用等价确定法，在提供生成模型的情况下以高效的方式解决了分布式策略评估问题。

    

    本文从统计效率的角度研究了分布式强化学习。我们研究了分布式策略评估，旨在估计由给定策略π获得的随机回报的完整分布（表示为η^π）。在提供生成模型的情况下，我们使用等价确定法构造了估计器η^π。我们证明，在这种情况下，通过具有大小为O(|S||A|/(ε^(2p)(1-γ)^(2p+2)))的数据集可以保证估计器η^π和真实分布η^π之间的p-Wasserstein距离小于ε的概率很高。这意味着分布式策略评估问题可以以高效利用样本的方式解决。此外，我们还证明，在不同的温和假设下，通过具有大小为O(|S||A|/(ε^2(1-γ)^4))的数据集就足以确保Kolmogorov距离和总变差。

    In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.  We use the certainty-equivalence method to construct our estimator $\hat\eta^\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}}\right)$ to guarantee a $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}}\right)$ suffices to ensure the Kolmogorov metric and total variation m
    
[^8]: 当前的药物属性预测方法在实际应用中的应用

    Current Methods for Drug Property Prediction in the Real World. (arXiv:2309.17161v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.17161](http://arxiv.org/abs/2309.17161)

    该论文介绍了当前药物属性预测方法的实际应用情况，并强调了不同数据集和方法之间的关联。研究发现最好的预测方法取决于数据集，使用经过工程处理的特征与经典方法结合的效果较好。

    

    在药物发现中，预测药物属性对于在昂贵的临床试验之前降低风险、快速找到高活性化合物是至关重要的。机器学习社区的兴趣导致了多种基准数据集和提出的方法的发布。然而，对于从业者来说，不同文件在不同数据集和方法上进行基准测试，得出的结论也不易于比较，因此仍然不清楚哪种方法或方法最合适。我们的大规模实证研究将之前的许多不同数据集和方法的研究链接在一起，从而提供了现有属性类别、数据集及其与不同方法的相互作用的综合概述。我们强调不确定性量化的重要性，以及应用这些方法在药物开发决策周期中的时间和成本问题。我们发现最好的方法取决于数据集，而使用经过工程处理的特征与经典方法结合的效果较好。

    Predicting drug properties is key in drug discovery to enable de-risking of assets before expensive clinical trials, and to find highly active compounds faster. Interest from the Machine Learning community has led to the release of a variety of benchmark datasets and proposed methods. However, it remains unclear for practitioners which method or approach is most suitable, as different papers benchmark on different datasets and methods, leading to varying conclusions that are not easily compared. Our large-scale empirical study links together numerous earlier works on different datasets and methods; thus offering a comprehensive overview of the existing property classes, datasets, and their interactions with different methods. We emphasise the importance of uncertainty quantification and the time and therefore cost of applying these methods in the drug development decision-making cycle. We discover that the best method depends on the dataset, and that engineered features with classical 
    
[^9]: 具有平均光滑度的高效无偏学习

    Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])

    [http://arxiv.org/abs/2309.17016](http://arxiv.org/abs/2309.17016)

    该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。

    

    我们研究了在非参数回归中无分布限制的平均光滑度概念，该概念由Ashlagi等人（2021）提出，用于衡量函数相对于任意未知潜在分布的"有效"光滑度。最近的Hanneke等人（2023）的研究在可实现情况下建立了平均光滑函数的紧密一致收敛界限，并提供了具有高效可实现性的学习算法，但这些结果目前在普遍无偏（即有噪声）情况下尚缺乏类似结果。在这项工作中，我们完全填补了这些差距。首先，我们为无偏设置中的平均光滑类提供了一个无分布一致收敛界限。其次，我们将所得到的样本复杂度与一个具有高效无偏学习算法相匹配。我们的结果以数据的内在几何形状为基础，适用于任何全有界度量空间，并展示了最近在可实现情况下获得的保证。

    We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz
    
[^10]: 统计物理、贝叶斯推断与神经信息处理

    Statistical physics, Bayesian inference and neural information processing. (arXiv:2309.17006v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2309.17006](http://arxiv.org/abs/2309.17006)

    该论文讨论了通过统计物理的视角来探索神经信息处理的课程讲义。主要内容包括贝叶斯推断与学习泛化的Gibbs描述的联系，广义线性模型作为背景剔除法的可控替代方法，以及线性和非线性降维技术。

    

    《统计物理机器学习》课程中Sara A. Solla教授讲座的讲义。讲义通过统计物理的视角探讨神经信息处理。内容包括贝叶斯推断及其与学习和泛化的Gibbs描述的关系，广义线性模型作为背景剔除法的可控替代方法，以及线性和非线性降维技术。

    Lecture notes from the course given by Professor Sara A. Solla at the Les Houches summer school on "Statistical physics of Machine Learning". The notes discuss neural information processing through the lens of Statistical Physics. Contents include Bayesian inference and its connection to a Gibbs description of learning and generalization, Generalized Linear Models as a controlled alternative to backpropagation through time, and linear and non-linear techniques for dimensionality reduction.
    
[^11]: 控制组合优化的连续放松

    Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])

    [http://arxiv.org/abs/2309.16965](http://arxiv.org/abs/2309.16965)

    本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。

    

    最近在组合优化（CO）问题中，图神经网络（GNNs）显示出巨大潜力。通过无监督学习找到近似解的受物理启发的GNN（PI-GNN）求解器在大规模CO问题上引起了极大关注。然而，对于相对密集图上的CO问题，贪婪算法的性能恶化，但对于PI-GNN求解器的性能却没有太多讨论。此外，由于PI-GNN求解器采用了放松策略，学习后需要从连续空间人工转换回原始离散空间，可能会破坏解的鲁棒性。本文通过数值实验证明了PI-GNN求解器在密集图上的CO问题的学习早期可能陷入局部解的情况，其中所有变量都为零。然后，我们通过控制连续性和离散性来解决这些问题。

    Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
    
[^12]: 机器学习和神经网络方法在水质预测中的应用

    Water quality prediction using machine learning and neural network approaches. (arXiv:2309.16951v1 [stat.ML])

    [http://arxiv.org/abs/2309.16951](http://arxiv.org/abs/2309.16951)

    本研究通过比较线性回归、随机森林、XGBoost、LightGBM和MLP神经网络五种模型在佐治亚州预测水质pH值方面的效果，发现LightGBM表现最好。基于树的模型在回归问题中优势显著，而MLP神经网络对特征缩放敏感。同时，本研究还探讨了与原研究相比，机器学习模型能够取得更好性能的原因。

    

    水资源是人类生计和经济进步的基础，与公共健康和环境福祉有着内在的联系。准确预测水质是改善水资源管理和对抗污染的关键因素。本研究采用多种性能指标，评估了五种不同模型（线性回归，随机森林，XGBoost，LightGBM和MLP神经网络）在美国佐治亚州预测pH值方面的效果。同时，LightGBM在所有模型中取得了最高的平均精度。基于树的模型凸显了它们在处理回归问题中的优势。此外，MLP神经网络的性能对特征缩放具有敏感性。我们还详细阐述并分析了机器学习模型在时间依赖性和空间考虑因素方面与原研究相比所取得的优越性能的原因。

    Water resources serve as the cornerstone of human livelihoods and economic progress, with intrinsic links to both public health and environmental well-being. The accurate prediction of water quality stands as a pivotal factor in enhancing water resource management and combating pollution. This research, employing diverse performance metrics, assesses the efficacy of five distinct models, namely, linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within Georgia, USA. Concurrently, LightGBM attains the highest average precision among all models examined. Tree-based models underscore their supremacy in addressing regression challenges. Furthermore, the performance of MLP neural network is sensitive to feature scaling. Additionally, we expound upon and dissect the reasons behind the superior precision of the machine learning models when they are compared to the original study, which factors in time dependencies and spatial considerations. 
    
[^13]: 对称性导致学习的结构性约束

    Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])

    [http://arxiv.org/abs/2309.16932](http://arxiv.org/abs/2309.16932)

    本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。

    

    由于常见的架构设计，对称性在当代神经网络中广泛存在。在这项工作中，我们揭示了损失函数对称性对影响机器学习模型的学习行为的重要性。我们证明了损失函数的每个镜像对称性都会导致一种结构性约束，当权重衰减或梯度噪声较大时，这种约束将成为首选解。作为直接推论，我们展示了重新缩放对称性导致稀疏性，旋转对称性导致低秩性，置换对称性导致同质集成。然后，我们展示了理论框架可以解释神经网络中的可塑性丧失和各种崩溃现象，并提出了如何利用对称性设计可微分实施硬性约束的算法。

    Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
    
[^14]: 增强随机平滑的Lipschitz-方差-边界权衡

    The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])

    [http://arxiv.org/abs/2309.16883](http://arxiv.org/abs/2309.16883)

    本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。

    

    面对噪声输入和对抗性攻击时，深度神经网络的实际应用受到其不稳定的预测的阻碍。在这种情况下，认证半径是模型鲁棒性的关键指标。然而，如何设计一个具有足够认证半径的高效分类器呢？随机平滑通过在输入中注入噪声来获得平滑且更鲁棒的分类器的框架提供了有希望的解决方案。本文首先展示了随机平滑引入的方差与分类器的另外两个重要属性，即其Lipschitz常数和边界之间的密切关系。更具体地说，我们的工作强调了基分类器的Lipschitz常数对平滑分类器和经验方差的双重影响。此外，为了增加认证鲁棒半径，我们引入了一种不同的单纯形投影技术，以便通过Bernst的方差-边界权衡来利用基分类器。

    Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
    
[^15]: Transductive Learning的尖锐泛化：一种Transductive Local Rademacher Complexity方法

    Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])

    [http://arxiv.org/abs/2309.16858](http://arxiv.org/abs/2309.16858)

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。

    

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们的工作将传统的local rademacher complexity (LRC)的思想扩展到了transductive设置中，相对于典型的LRC方法在归纳设置中的分析有了相当大的变化。我们提出了一种基于Rademacher complex的局部化工具，可以应用于各种transductive learning问题，并在适当条件下得到了尖锐的界限。与LRC的发展类似，我们通过从独立变量的方差信息开始构建TLRC，将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。经过精心设计的...

    We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
    
[^16]: 最优非线性性能改进随机特征的泛化性能

    Optimal Nonlinearities Improve Generalization Performance of Random Features. (arXiv:2309.16846v1 [cs.LG])

    [http://arxiv.org/abs/2309.16846](http://arxiv.org/abs/2309.16846)

    通过研究等效模型的参数，本研究发现获得的参数可以定义一组最优非线性性，如二阶多项式和分段线性函数。这些非线性性能优化了泛化性能，无论其实际形式如何，对回归和分类问题均有效。

    

    通过非线性激活函数的随机特征模型在训练和泛化误差方面已被证明与高斯模型渐进等效。等效模型的分析揭示了激活函数发挥的重要但尚未完全理解的作用。为了解决这个问题，我们研究等效模型的“参数”，以实现对给定监督学习问题的改进的泛化性能。我们展示了从高斯模型获取的参数使我们能够定义一组最优非线性性。我们提供了这组最优非线性性的两个示例类，例如二阶多项式和分段线性函数。这些函数被优化以改进泛化性能，无论其实际形式如何。我们对回归和分类问题进行了实验，包括合成和真实数据（如CIFAR10）。我们的数值结果验证了优化的非线性性能优于wid。

    Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the "parameters" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than wid
    
[^17]: 高维线性回归中经验贝叶斯估计的均值场方法

    A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression. (arXiv:2309.16843v1 [math.ST])

    [http://arxiv.org/abs/2309.16843](http://arxiv.org/abs/2309.16843)

    这项研究提出了一种在高维线性回归中进行经验贝叶斯估计的均值场方法，通过变分经验贝叶斯逼近先验分布并建立了渐近一致性，从而实现了计算上可行的贝叶斯推断。

    

    我们研究了高维线性回归中的经验贝叶斯估计。为了实现对潜在先验的计算有效估计，我们采用了变分经验贝叶斯方法，最初由Carbonetto和Stephens（2012）以及Kim等人（2022）引入。在对设计和先验做出温和假设的前提下，我们建立了非参数最大似然估计器（NPMLE）及其（可计算的）朴素均值场变分代理的渐近一致性。在假定朴素均值场逼近具有占优解的基础上，我们开发了一种计算效率高的近似正则分布的方法，并在1-Wasserstein度量下验证了其准确性。这使得贝叶斯推断可以在计算上可行，例如构建具有平均覆盖保证的后验可信区间，回归系数的贝叶斯最优估计，非空比例的估计等。我们的分析涵盖了...

    We study empirical Bayes estimation in high-dimensional linear regression. To facilitate computationally efficient estimation of the underlying prior, we adopt a variational empirical Bayes approach, introduced originally in Carbonetto and Stephens (2012) and Kim et al. (2022). We establish asymptotic consistency of the nonparametric maximum likelihood estimator (NPMLE) and its (computable) naive mean field variational surrogate under mild assumptions on the design and the prior. Assuming, in addition, that the naive mean field approximation has a dominant optimizer, we develop a computationally efficient approximation to the oracle posterior distribution, and establish its accuracy under the 1-Wasserstein metric. This enables computationally feasible Bayesian inference; e.g., construction of posterior credible intervals with an average coverage guarantee, Bayes optimal estimation for the regression coefficients, estimation of the proportion of non-nulls, etc. Our analysis covers both 
    
[^18]: 无导数损失方法在求解偏微分方程中的分析

    An analysis of the derivative-free loss method for solving PDEs. (arXiv:2309.16829v1 [math.NA])

    [http://arxiv.org/abs/2309.16829](http://arxiv.org/abs/2309.16829)

    本研究分析了一种无导数损失方法在使用神经网络求解椭圆型偏微分方程的方法。我们发现训练损失偏差与时间间隔和空间梯度成正比，与行走者大小成反比，同时时间间隔必须足够长。我们提供了数值测试结果以支持我们的分析。

    

    本研究分析了无导数损失方法在使用神经网络求解一类椭圆型偏微分方程中的应用。无导数损失方法采用费曼-卡克公式，结合随机行走者及其对应的平均值。我们考察了费曼-卡克公式中与时间间隔相关的影响，以及行走者大小对计算效率、可训练性和采样误差的影响。我们的分析表明，训练损失偏差与时间间隔和神经网络的空间梯度成正比，与行走者大小成反比。同时，我们还表明时间间隔必须足够长才能训练网络。这些分析结果说明，在时间间隔的最优下界基础上，我们可以选择尽可能小的行走者大小。我们还提供了支持我们分析的数值测试。

    This study analyzes the derivative-free loss method to solve a certain class of elliptic PDEs using neural networks. The derivative-free loss method uses the Feynman-Kac formulation, incorporating stochastic walkers and their corresponding average values. We investigate the effect of the time interval related to the Feynman-Kac formulation and the walker size in the context of computational efficiency, trainability, and sampling errors. Our analysis shows that the training loss bias is proportional to the time interval and the spatial gradient of the neural network while inversely proportional to the walker size. We also show that the time interval must be sufficiently long to train the network. These analytic results tell that we can choose the walker size as small as possible based on the optimal lower bound of the time interval. We also provide numerical tests supporting our analysis.
    
[^19]: 生成分类器的有趣属性

    Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])

    [http://arxiv.org/abs/2309.16779](http://arxiv.org/abs/2309.16779)

    生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。

    

    识别对象的最佳范式是判别式推理（快速但潜在容易出现快捷学习）还是使用生成模型（较慢但潜在更稳健）？我们借鉴了最新的生成模型进展，将文本到图像模型转化为分类器。这使得我们能够研究其行为，并将其与判别模型和人类心理物理数据进行比较。我们报道了生成分类器的四个有趣的新兴特性：它们显示出破纪录的人类形状偏好（对于Imagen达到99%），接近人类级别的超出分布准确性，与人类分类错误的最先进对齐以及它们理解某些知觉幻象。我们的结果表明，尽管目前模拟人类物体识别的主导范式是判别式推理，零样本生成模型出奇地接近人类物体识别数据。

    What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
    
[^20]: 用XRM发现环境

    Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])

    [http://arxiv.org/abs/2309.16748](http://arxiv.org/abs/2309.16748)

    本文提出了一种用于发现环境的算法 XRM，它通过训练两个孪生网络，每个网络从训练数据的一半中学习，并模仿其兄弟网络的错误分类，解决了现有方法需要依赖人工注释环境信息的问题。

    

    成功的跨领域泛化需要环境注释。然而，这些注释的获取是资源密集型的，并且它们对模型性能的影响受人类注释者的期望和感知偏差的限制。因此，为了实现应用领域全面泛化的鲁棒性AI系统，我们必须开发一种算法来自动发现引发广泛泛化的环境。目前的提案根据训练误差将示例划分为不同的类，但存在一个根本问题。这些方法添加了超参数和早停策略，而这些参数是无法在没有人类注释环境的验证集的情况下进行调整的，而这些信息正是要发现的信息。在本文中，我们提出了 Cross-Risk-Minimization (XRM) 来解决这个问题。XRM 训练两个孪生网络，每个网络从训练数据的一个随机一半中学习，同时模仿其兄弟网络所做的自信的错误分类。XRM 提供了超参数调整的方法，并且不需要依赖人工注释的环境信息。

    Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
    
[^21]: 隐性高斯过程表示任意潜在流形上的向量场

    Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])

    [http://arxiv.org/abs/2309.16746](http://arxiv.org/abs/2309.16746)

    这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。

    

    高斯过程（GPs）是用于学习未知函数和量化数据中的时空不确定性的流行非参数统计模型。最近的研究扩展了GPs，用于建模分布在非欧几里得域上的标量和向量数据，包括出现在计算机视觉、动力系统和神经科学等众多领域中的平滑流形。然而，这些方法假设数据的潜在流形是已知的，限制了它们的实际效用。我们引入了RVGP，一种用于学习潜在黎曼流形上的向量信号的GP的推广。我们的方法使用与切向丛关联的连接Laplacian的特征函数进行位置编码，这些特征函数可以从基于图的常见数据近似中轻松推导出来。我们证明了RVGP在流形上具有全局规律性，使得其能够在保留奇异性的同时超分辨率和修复向量场。此外，我们使用RVGP来重构高密度数据。

    Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
    
[^22]: 为机器人堆积方块任务构建因果性概率框架

    Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])

    [http://arxiv.org/abs/2308.06203](http://arxiv.org/abs/2308.06203)

    这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。

    

    现实世界中的不确定性意味着系统设计者无法预测并明确设计出机器人可能遇到的所有场景。因此，以这种方式设计的机器人在高度受控的环境之外容易出现故障。因果模型提供了一个原则性的框架，用于编码机器人与其环境相互作用的因果关系的形式化知识，并结合现实世界机器人通常遇到的噪声和不确定性的概率表示。结合因果推断，这些模型使自主代理能够理解、推理和解释其环境。在这项工作中，我们关注机器人堆积方块任务的问题，因为它展示了许多应用所需的基本感知和操作能力，包括仓库物流和家庭人工支持机器人。我们提出了一个新颖的因果性概率框架，将物理模拟功能嵌入到这个任务中。

    Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
    
[^23]: 利用任务结构提高神经网络表示的可识别性

    Leveraging Task Structures for Improved Identifiability in Neural Network Representations. (arXiv:2306.14861v1 [stat.ML])

    [http://arxiv.org/abs/2306.14861](http://arxiv.org/abs/2306.14861)

    本文提出一种基于任务结构的可识别性理论，拓展了先前仅限于单任务分类的工作。任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放。在假设任务之间存在因果关系时，该方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。

    

    本文扩展了监督学习中可辨别性的理论，考虑了在拥有任务分布的情况下的后果。在这种情况下，我们展示了即使在回归的情况下也可以实现可识别性，扩展了先前仅限于单任务分类情况的工作。此外，我们展示了任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放，这是一个更强大和更有用的结果。当我们进一步假设这些任务之间存在因果关系时，我们的方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。在经验上，我们验证了我们的模型在恢复合成和现实世界数据的规范表示方面优于更一般的无监督模型。

    This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.
    
[^24]: SiBBlInGS: 使用跨状态的图形相似性驱动模块推理的建模块方法

    SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])

    [http://arxiv.org/abs/2306.04817](http://arxiv.org/abs/2306.04817)

    本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。

    

    对于多维时间序列来说，提取有意义的模块是发现复杂系统中有价值见解的关键。本文提出了一种基于图形相似性驱动的模块推理框架(SiBBlInGS)，用于发现模块，同时考虑到数据中的状态间和状态内关系，能够提取非正交组件，并允许状态之间的会话计数和持续时间差异。此外，SiBBlInGS还允许跨状态变化模块结构和每次试验的时间变异，并可识别特定状态与状态非特定模块。

    Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
    
[^25]: 深度学习中的一致置信现象及其对校准的影响

    A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])

    [http://arxiv.org/abs/2306.00740](http://arxiv.org/abs/2306.00740)

    深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。

    

    尽管深度神经网络具有惊人的泛化能力，但它们屡次表现出在预测不确定性方面估计不佳的情况——换句话说，它们在错误时经常过度自信。解决这个问题被称为模型校准，并以修改训练方案和训练后校准程序的形式受到了广泛关注。在本文中，我们提出了一个现代模型校准的重要障碍：深度神经网络在它们的训练点周围有大的几乎确定的置信邻域。我们在实验中证明了这种现象在很多模型和数据集对中都会出现（在图像分类的背景下）。此外，我们证明了当这种现象出现时，在类别之间存在重叠的大类数据分布中，即使在应用校准后也不能获得比随机更好的渐近校准模型（在渐近意义下）。

    Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
    
[^26]: 对数凹马尔可夫链之链

    Chain of Log-Concave Markov Chains. (arXiv:2305.19473v1 [stat.ML])

    [http://arxiv.org/abs/2305.19473](http://arxiv.org/abs/2305.19473)

    该论文提出了一种新的采样算法，基于对数凹条件概率密度，使用等向性高斯平滑来解决高维下抽样难题。

    

    马尔科夫链蒙特卡罗（MCMC）是一种从未标准化密度中抽样的通用算法类。在高维情况下，MCMC面临两个众所周知的问题：(i)感兴趣的分布在由小概率块隔开的区域中集中;(ii)对数凹性的小概率块本身通常存在病态问题。我们引入了一种采用等向性高斯平滑来解决这些问题的框架。我们证明，无论密度函数的最小假设是什么，从密度函数中采样总是可以分解为通过等噪声测量的累积，从对数凹性条件密度中采样的序列。该构造跟踪了样本历史，因此作为一个整体而言是非马尔可夫的，但历史仅以经验均值的形式出现，从而保证了内存印迹的最小化。我们的采样算法推广了步行跳跃采样（1）。"走"阶段变成了对数凹链的(非马尔可夫)链。

    Markov chain Monte Carlo (MCMC) is a class of general-purpose algorithms for sampling from unnormalized densities. There are two well-known problems facing MCMC in high dimensions: (i) The distributions of interest are concentrated in pockets separated by large regions with small probability mass, and (ii) The log-concave pockets themselves are typically ill-conditioned. We introduce a framework to tackle these problems using isotropic Gaussian smoothing. We prove one can always decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. This construction keeps track of a history of samples, making it non-Markovian as a whole, but the history only shows up in the form of an empirical mean, making the memory footprint minimal. Our sampling algorithm generalizes walk-jump sampling [1]. The "walk" phase becomes a (non-Markovian) chain of log-co
    
[^27]: 梯度下降在多层神经网络中的泛化保证

    Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])

    [http://arxiv.org/abs/2305.16891](http://arxiv.org/abs/2305.16891)

    本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。

    

    近年来，通过算法稳定性方法，对梯度下降训练的神经网络（NN）的泛化进行了重大进展。然而，大部分现有研究集中在单隐藏层NN上，并没有解决不同网络缩放参数的影响。本文通过对GD在多层NN上进行全面的稳定性和泛化分析，极大地扩展了以往的工作。对于双层NN，我们的结果是在一般的网络缩放参数下建立的，放宽了以前的条件。对于三层NN，我们的技术贡献在于利用一种新的归纳策略，深入探讨了过度参数化的影响，证明了它的几乎协同约束性。通过我们的一般性发现的直接应用，我们得出了GD算法在双层和三层NN中的过量风险速率为$O(1/\sqrt{n})$。

    Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\sqrt{n})$ for GD algorithms in both two-layer and thre
    
[^28]: 神经控制微分方程的泛化能力研究

    On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])

    [http://arxiv.org/abs/2305.16791](http://arxiv.org/abs/2305.16791)

    本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。

    

    本文研究了使用神经控制微分方程（Kidger，Morrill等，2020）从不规则采样的时间序列样本中预测结果的监督学习设置。在我们的框架中，时间序列是一个未观察到的连续路径的离散化，结果通过一个具有未知向量场的控制微分方程依赖于这个路径。使用离散数据进行学习会引入离散偏差，我们精确地量化了这种偏差。通过使用关于控制微分方程流的连续性的理论结果，我们展示了逼近偏差直接与由浅层神经网络定义生成模型的利普希茨函数的逼近误差相关。通过结合最近的工作将神经网络的利普希茨常数与其泛化能力联系起来，我们上界了经验风险最小化器达到的期望损失与贝叶斯最优风险之间的泛化差距。

    We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
    
[^29]: $L_{2}$正则线性深度神经网络中隐性SGD偏差：从高秩到低秩的单向跳跃。

    Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])

    [http://arxiv.org/abs/2305.16038](http://arxiv.org/abs/2305.16038)

    在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。

    

    具有多个隐藏层的深度线性网络（DLN）的$L_{2}$正则化损失具有多个局部最小值，对应于具有不同秩的矩阵。在矩阵完成等任务中，目标是收敛到最小秩局部最小值，该局部最小值仍适合训练数据。虽然可以轻松避免低估秩的局部最小值，因为它们不适合数据，但梯度下降可能会陷入高估秩的局部最小值。我们证明，使用SGD，总是有从更高秩最小值跳跃到更低秩最小值的概率，但跳回的概率为零。更精确地说，我们定义了一系列集合$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$，使得$B_{r}$包含秩$r$或更少的所有最小值（而不是更多），对于足够小的岭参数$\lambda$和学习率$\eta$，它们是吸收的：SGD离开$B_{r}$的概率为0，从任何起点开始，SGD进入$B_{r}$的概率非零。

    The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
    
[^30]: 改进ReLU网络特征学习的神经特征激活值分析

    Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])

    [http://arxiv.org/abs/2305.15912](http://arxiv.org/abs/2305.15912)

    本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。

    

    本文研究了神经网络中单个ReLU单元的特征激活值。我们将ReLU单元在输入空间中对应的特征激活值集合称为ReLU单元的特征激活集。我们建立了特征激活集与ReLU网络中学习特征之间的明确联系，并揭示了现代深度学习架构中使用的各种神经网络规范化技术如何规范化和稳定SGD优化。利用这些洞见，我们提出了一种几何方法来参数化ReLU网络以改进特征学习。我们经验性地验证了其有用性，使用了不那么精心选择的初始化方案和更大的学习率。我们报告了更好的优化稳定性，更快的收敛速度和更好的泛化性能。

    We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
    
[^31]: 学习和决策的风险自适应方法：一项调查

    Risk-Adaptive Approaches to Learning and Decision Making: A Survey. (arXiv:2212.00856v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.00856](http://arxiv.org/abs/2212.00856)

    本文调查了过去25年中风险测度的快速发展，介绍了其在各个领域的应用，以及与效用理论和分布鲁棒优化的关系，并指出了公平机器学习等新兴应用领域。

    

    不确定性在工程设计、统计学习和决策制定中普遍存在。由于固有的风险规避和对假设的模糊性，通常通过制定和解决使用风险和相关概念的保守优化模型来解决不确定性问题。我们对过去25年来风险测度的快速发展进行了调查。从它们在金融工程领域的起步，我们回顾了它们在几乎所有领域的工程和应用数学中的应用。风险测度扎根于凸分析，为处理不确定性提供了一个具有重要计算和理论优势的通用框架。我们描述了关键事实，列举了几种具体算法，并提供了大量参考文献供进一步阅读。该调查还回顾了与效用理论和分布鲁棒优化的联系，指出了新兴应用领域，如公平机器学习，并定义了相对测度。

    Uncertainty is prevalent in engineering design, statistical learning, and decision making broadly. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount the spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of rel
    
[^32]: 基于深度神经网络的广义平衡权重

    Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07533](http://arxiv.org/abs/2211.07533)

    本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。

    

    从观测数据中估计因果效应是许多领域中的一个中心问题。一种广泛使用的方法是平衡协变量的权重，使得数据的分布类似于随机化。我们提出了一种称为神经平衡权重（NBW）的广义平衡权重，以估计任意混合离散和连续干预的因果效应。通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重。为此，我们选择了 $\alpha$-差异作为优化的目标函数，因为它具有样本复杂度独立于其地面实况值和无偏小批量梯度的估计器，而且对于梯度消失问题具有优势。此外，我们提供了以下两种方法来估计平衡权重：提高平衡权重的泛化性能和检查其效果。

    Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking 
    
[^33]: 邻接性和模块化图分割之间的关系

    Relations Between Adjacency and Modularity Graph Partitioning. (arXiv:1505.03481v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1505.03481](http://arxiv.org/abs/1505.03481)

    本文发现了邻接矩阵的特征向量与模块化矩阵的主特征向量之间的精确线性关系，并提出了一种逼近模块化矩阵主特征向量的方法。此外，还证明了归一化邻接聚类与归一化模块化聚类的等价性。实验结果显示，归一化邻接聚类的效率可以是归一化模块化聚类的两倍。

    

    本文研究了不规范模块化矩阵的主特征向量与邻接矩阵的特征向量之间的精确线性关系。我们提出了一种逼近模块化矩阵主特征向量的方法，并推导了逼近误差。我们还给出了归一化邻接聚类与归一化模块化聚类之间的等价性证明。数字实验表明，归一化邻接聚类的效率可以是归一化模块化聚类的两倍。

    This paper develops the exact linear relationship between the leading eigenvector of the unnormalized modularity matrix and the eigenvectors of the adjacency matrix. We propose a method for approximating the leading eigenvector of the modularity matrix, and we derive the error of the approximation. There is also a complete proof of the equivalence between normalized adjacency clustering and normalized modularity clustering. Numerical experiments show that normalized adjacency clustering can be as twice efficient as normalized modularity clustering.
    

