# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating Superhuman Models with Consistency Checks.](http://arxiv.org/abs/2306.09983) | 本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。 |
| [^2] | [You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks.](http://arxiv.org/abs/2306.09951) | 本文对现代机器学习中的鲁棒性问题和对抗攻击进行了调查，发现不必要采用高成本的强鲁棒机器学习，可以通过其他设计选择来缓解对抗攻击的风险。 |
| [^3] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^4] | [Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction.](http://arxiv.org/abs/2306.09882) | 本文提出了一种新型的时空Tweedie模型STTD，旨在解决高分辨率OD矩阵中稀疏和长尾特征的问题，并成功量化预测不确定性，具有很高的应用前景。 |
| [^5] | [Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima.](http://arxiv.org/abs/2306.09850) | 该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。 |
| [^6] | [Gradient is All You Need?.](http://arxiv.org/abs/2306.09778) | 本文提供了一种新的角度分析了基于梯度的学习算法，将一种新的多粒子无导数优化方法解释为梯度下降的随机松弛方法。此优化方法证明了零阶方法并不一定低效或不具备泛化能力，并且可以在丰富类别的非光滑和非凸目标函数下全局收敛于全局最小值。 |
| [^7] | [Stabilized Neural Differential Equations for Learning Constrained Dynamics.](http://arxiv.org/abs/2306.09739) | 本文提出了一种稳定神经微分方程（SNDEs）的方法，可以强制使用任意流形约束。该方法通过添加稳定项使约束流形成为渐进稳定的，并且在实验中表现优于现有方法。 |
| [^8] | [Linear convergence of Nesterov-1983 with the strong convexity.](http://arxiv.org/abs/2306.09694) | 本文使用高分辨率微分方程框架回答了Nesterov-1983和FISTA是否在强凸函数上线性收敛的问题，并指出线性收敛性不依赖于强凸性条件。 |
| [^9] | [Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation.](http://arxiv.org/abs/2306.09646) | 本文研究了基于变分自编码器（VAE）的无监督异常检测方法，通过引入图像的紧性特征来纠正VAE模型中的理论缺陷并缩小内点与离群点之间的距离，同时结合建模技术和结构知识提出了一种无监督异常检测算法。 |
| [^10] | [Structural Restricted Boltzmann Machine for image denoising and classification.](http://arxiv.org/abs/2306.09628) | 本文提出了一种结构限制的玻尔兹曼机模型，可以用于图像建模中的去噪和分类任务，通过限制隐藏单元与可见单元的子集的连接，大大降低了可训练参数的数量并保持了模型的性能。 |
| [^11] | [Power-law Dynamic arising from machine learning.](http://arxiv.org/abs/2306.09624) | 该论文研究了机器学习中的幂律动态，证明了其具有唯一平稳分布且可以通过比较连续和离散化幂律动态的出现时间来指导机器学习算法。 |
| [^12] | [Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?.](http://arxiv.org/abs/2306.09586) | 本文探讨了将不确定性表示为一个置信集而非单一概率分布的方法。并发现，在二元分类中，信任集的体积是一种有意义的衡量认知不确定性的方法，但在多类分类中则没有这种效果。 |
| [^13] | [Geometric-Based Pruning Rules For Change Point Detection in Multiple Independent Time Series.](http://arxiv.org/abs/2306.09555) | 该论文提出了一些基于几何形状的扩展函数剪枝规则，用于解决在多个独立时间序列中检测多个变点的问题，在小维度情况下可以比函数剪枝更快地准确检测更好。 |
| [^14] | [Online Heavy-tailed Change-point detection.](http://arxiv.org/abs/2306.09548) | 本文提出了一种在线变点检测算法，可以应对重尾分布且保证有限的假阳性率。 |
| [^15] | [Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding.](http://arxiv.org/abs/2306.09520) | 本文提出了一种名为Caus-Modens的算法，通过调制集合来描述因果结果区间，相比符合性预测方法，能够在实践中给出更紧密的结果区间。 |
| [^16] | [Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens.](http://arxiv.org/abs/2306.09444) | 本研究提出了一个机器学习管道用于大规模场景下量子可分性的近似解，通过有效算法近似查找最近的可分离密度矩阵，并将量子可分性视为分类问题，对任何二维混合状态都适用。 |
| [^17] | [Unsupervised Anomaly Detection via Nonlinear Manifold Learning.](http://arxiv.org/abs/2306.09441) | 该篇论文提出了一种基于非线性流形学习的无监督异常检测方法，可以鲁棒、高效、可解释地检测数据中的异常样本。 |
| [^18] | [Differentiating Metropolis-Hastings to Optimize Intractable Densities.](http://arxiv.org/abs/2306.07961) | 本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。 |
| [^19] | [Multi-study R-learner for Heterogeneous Treatment Effect Estimation.](http://arxiv.org/abs/2306.01086) | 本文提出了一种名为多研究R-learner的方法，能够很好地估计多研究中的异质性处理效应，具有鲁棒性，并在现实癌症数据实验中表现出更小的估计误差。 |
| [^20] | [ChemCrow: Augmenting large-language models with chemistry tools.](http://arxiv.org/abs/2304.05376) | 本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。 |
| [^21] | [Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language.](http://arxiv.org/abs/2303.03363) | 本文提出了一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务，并在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。 |
| [^22] | [On Consistency and Asymptotic Normality of Least Absolute Deviation Estimators for 2-dimensional Sinusoidal Model.](http://arxiv.org/abs/2301.03229) | 本文提出了一种鲁棒的最小绝对偏差估计器，用于2维正弦模型参数估计，在数据存在异常值或重尾噪声时具有优越性并得到强一致性和渐近正态性的保证。 |
| [^23] | [Compressed Sensing MRI Reconstruction Regularized by VAEs with Structured Image Covariance.](http://arxiv.org/abs/2210.14586) | 该论文提出了一种利用生成模型作为反问题先验知识、利用变分自编码器生成包括协方差信息在内的图像并提供新距离度量的方法来解决采样稀疏和有噪声的MRI重建问题，实现了最先进的重建性能并突出了解剖结构。 |
| [^24] | [Training Debiased Subnetworks with Contrastive Weight Pruning.](http://arxiv.org/abs/2210.05247) | 本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。 |
| [^25] | [Data-Driven Influence Functions for Optimization-Based Causal Inference.](http://arxiv.org/abs/2208.13701) | 本文提出了一种利用有限差分逼近统计泛函Gateaux导数的构造算法，并研究了从数据中进行概率分布估计的情况下的Gateaux导数估计。研究结果为因果推断和动态治疗方案等问题提供了解决方案。 |
| [^26] | [Resolving the Human Subjects Status of Machine Learning's Crowdworkers.](http://arxiv.org/abs/2206.04039) | 机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。 |
| [^27] | [Dynamic treatment effects: high-dimensional inference under model misspecification.](http://arxiv.org/abs/2111.06818) | 本文提出了一种新的鲁棒估计方法来解决动态治疗效应估计中的挑战，提高了在模型错误下的高维环境中的估计鲁棒性和可靠性。 |
| [^28] | [Boosting Simple Learners.](http://arxiv.org/abs/2001.11704) | 本论文探讨的是提升学习器的方法，关注弱学习器属于一个容量受限的类的假设，并重点关注需要多少个弱学习器才能生成准确的假设。通过设计新颖的算法，只需要约$\tilde{O}({1}/{\gamma})$个弱假设就能够规避经典下界。 |

# 详细

[^1]: 用一致性检查评估超人模型

    Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])

    [http://arxiv.org/abs/2306.09983](http://arxiv.org/abs/2306.09983)

    本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。

    

    如果机器学习模型在各种推理或决策任务上实现了超人能力，那么我们该如何评估这些模型，考虑到人类代理会产生偏差? 在本文中，我们提出了一个用一致性检查评估超人模型的框架。我们的前提是，虽然评估超人决策的正确性可能是不可能的，但是如果模型的决策未能满足某些逻辑上、可解释的规则，我们仍然可以发现错误。我们将我们的框架实现在三个任务上，这些任务的决策正确性由于超人模型能力或其他缺乏基本事实而难以评估：评估国际象棋局面、预测未来事件和作出法律判断。我们表明，无论模型在这些任务上的表现如何(可能是超人的)，我们都能发现决策制定中的逻辑不一致性。例如：国际象棋引擎给出对局中棋子相对估值的不同排列。

    If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
    
[^2]: 不需要强鲁棒机器学习来管理对抗攻击风险

    You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks. (arXiv:2306.09951v1 [cs.LG])

    [http://arxiv.org/abs/2306.09951](http://arxiv.org/abs/2306.09951)

    本文对现代机器学习中的鲁棒性问题和对抗攻击进行了调查，发现不必要采用高成本的强鲁棒机器学习，可以通过其他设计选择来缓解对抗攻击的风险。

    

    现代机器学习（ML）模型的鲁棒性已成为社区内日益关注的问题。能够通过对输入进行貌似无关的更改来破坏模型，从而导致错误预测的能力令人震惊，而我们在构建具有鲁棒性的模型方面的成效也不容乐观。现有研究取得了一定的进展，但当前的缓解措施带来了很高的成本，同时也降低了模型的准确性。然而，当存在其他设计选择可以避免这种风险时，这样的权衡可能并不必要。在本调查中，我们通过眼光关注实践中如何缓解这些攻击，生产部署的风险以及管理这些风险。在此过程中，我们阐明了许多AML威胁不足以证明这种成本和权衡的必要性。

    The robustness of modern machine learning (ML) models has become an increasing concern within the community. The ability to subvert a model into making errant predictions using seemingly inconsequential changes to input is startling, as is our lack of success in building models robust to this concern. Existing research shows progress, but current mitigations come with a high cost and simultaneously reduce the model's accuracy. However, such trade-offs may not be necessary when other design choices could subvert the risk. In this survey we review the current literature on attacks and their real-world occurrences, or limited evidence thereof, to critically evaluate the real-world risks of adversarial machine learning (AML) for the average entity. This is done with an eye toward how one would then mitigate these attacks in practice, the risks for production deployment, and how those risks could be managed. In doing so we elucidate that many AML threats do not warrant the cost and trade-of
    
[^3]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^4]: 时空Tweedie模型在预测存在零膨胀和长尾旅行需求中的应用及不确定性量化

    Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction. (arXiv:2306.09882v1 [cs.LG])

    [http://arxiv.org/abs/2306.09882](http://arxiv.org/abs/2306.09882)

    本文提出了一种新型的时空Tweedie模型STTD，旨在解决高分辨率OD矩阵中稀疏和长尾特征的问题，并成功量化预测不确定性，具有很高的应用前景。

    

    传统的时空深度学习模型难以解决高分辨率OD矩阵中稀疏和长尾特征的问题，从而难以量化预测不确定性，而这对于交通管理至关重要。为了解决这些挑战，本文提出了一种新颖的方法：空间-Tweedie图神经网络（STTD）。STTD将Tweedie分布作为传统的“零膨胀”模型的有力替代品，并利用空间和时间嵌入来参数化旅行需求分布。我们使用真实世界的数据集进行评估，结果表明STTD在高分辨率场景下提供了准确的预测和精确的置信区间，具有优越性。

    crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
    
[^5]: 实用的锐度感知优化算法不能全程向最优点收敛

    Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])

    [http://arxiv.org/abs/2306.09850](http://arxiv.org/abs/2306.09850)

    该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。

    

    锐度感知优化(SAM)是一种优化器，它基于当前点$x_t$的梯度，在扰动$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$处进行下降。现有研究证明了SAM对于平滑函数的收敛性，但是它们假设扰动的大小$\rho$逐渐衰减和/或在$y_t$中没有梯度归一化，这与实践不符。为了弥补这一差距，我们研究了具有实用配置（即常数$\rho$和$y_t$中的梯度归一化）的确定性/随机版本的SAM，并探讨了它们在具有（非）凸性假设的平滑函数上的收敛性质。令人惊讶的是，在许多情况下，我们发现SAM在收敛到全局最小值或稳定点方面具有有限的能力。对于平滑强凸函数，我们展示了确定性SAM具有严格的全局收敛率为$\tilde\Theta(\frac{1}{T^2})$，而随机SAM的收敛界则受到噪声水平降低的影响，这表明了平面目标表面的尖锐度和平缓性之间平衡的挑战。

    Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
    
[^6]: 梯度真的是你所需要的一切吗？

    Gradient is All You Need?. (arXiv:2306.09778v1 [cs.LG])

    [http://arxiv.org/abs/2306.09778](http://arxiv.org/abs/2306.09778)

    本文提供了一种新的角度分析了基于梯度的学习算法，将一种新的多粒子无导数优化方法解释为梯度下降的随机松弛方法。此优化方法证明了零阶方法并不一定低效或不具备泛化能力，并且可以在丰富类别的非光滑和非凸目标函数下全局收敛于全局最小值。

    

    本文提供了一种新的分析方法，通过将一种新的多粒子无导数优化方法结合梯度下降看作随机松弛方法，来解释基于梯度的学习算法的理论理解。通过粒子之间的通讯，这种优化方法表现出类似于随机梯度下降的行为，证明了零阶方法并不一定低效或不具备泛化能力，并且可以在非光滑和非凸目标函数的丰富类别下全局收敛于全局最小值。

    In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradi
    
[^7]: 学习受限动力学的稳定神经微分方程

    Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])

    [http://arxiv.org/abs/2306.09739](http://arxiv.org/abs/2306.09739)

    本文提出了一种稳定神经微分方程（SNDEs）的方法，可以强制使用任意流形约束。该方法通过添加稳定项使约束流形成为渐进稳定的，并且在实验中表现优于现有方法。

    

    最近出现了许多成功的从数据学习动态系统的方法。然而，确保推断出的动态系统保留已知约束条件（例如守恒定律或对允许的系统状态的限制）仍然具有挑战性。我们提出了稳定神经微分方程（SNDEs）的方法，这是一种用于神经微分方程强制使用任意流形约束的方法。我们的方法基于一个稳定项，当添加到原始动态系统中时，可以将约束流形成为渐进稳定的。由于其简单性，我们的方法与所有常见的神经常微分方程（NODE）模型兼容并广泛适用。在广泛的经验评估中，我们证明SNDE在扩展可纳入NODE训练的约束类型方面胜过现有方法。

    Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
    
[^8]: 具有强凸性的 Nesterov-1983 的线性收敛性

    Linear convergence of Nesterov-1983 with the strong convexity. (arXiv:2306.09694v1 [math.OC])

    [http://arxiv.org/abs/2306.09694](http://arxiv.org/abs/2306.09694)

    本文使用高分辨率微分方程框架回答了Nesterov-1983和FISTA是否在强凸函数上线性收敛的问题，并指出线性收敛性不依赖于强凸性条件。

    

    对于现代基于梯度的优化，Nesterov 的加速梯度下降法是一个开创性里程碑，该方法在[Nesterov，1983]中提出，简称为Nesterov-1983。此后，重要的进展之一是它的近端推广，名为快速迭代收缩阈值算法（FISTA），广泛应用于图像科学和工程。然而，目前仍未知道Nesterov-1983和FISTA是否在强凸函数上线性收敛，而这已被列为综合评审[Chambolle和Pock，2016，附录B]中的未解决问题。本文通过使用高分辨率微分方程框架来回答这个问题。与先前采用的相空间表示一起，构造Lyapunov函数的关键区别在于动能的系数随迭代而变化。此外，我们指出，上述两种算法的线性收敛性没有依赖于强凸函数的条件。

    For modern gradient-based optimization, a developmental landmark is Nesterov's accelerated gradient descent method, which is proposed in [Nesterov, 1983], so shorten as Nesterov-1983. Afterward, one of the important progresses is its proximal generalization, named the fast iterative shrinkage-thresholding algorithm (FISTA), which is widely used in image science and engineering. However, it is unknown whether both Nesterov-1983 and FISTA converge linearly on the strongly convex function, which has been listed as the open problem in the comprehensive review [Chambolle and Pock, 2016, Appendix B]. In this paper, we answer this question by the use of the high-resolution differential equation framework. Along with the phase-space representation previously adopted, the key difference here in constructing the Lyapunov function is that the coefficient of the kinetic energy varies with the iteration. Furthermore, we point out that the linear convergence of both the two algorithms above has no d
    
[^9]: 用于压缩潜在表示的无监督异常检测方法

    Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation. (arXiv:2306.09646v1 [stat.ML])

    [http://arxiv.org/abs/2306.09646](http://arxiv.org/abs/2306.09646)

    本文研究了基于变分自编码器（VAE）的无监督异常检测方法，通过引入图像的紧性特征来纠正VAE模型中的理论缺陷并缩小内点与离群点之间的距离，同时结合建模技术和结构知识提出了一种无监督异常检测算法。

    

    在真实世界中，对于任何机器学习模型的部署和操作，检测异常值至关重要。对于深度神经网络而言，这一点尤为重要，因为这些网络对于此类输入显示出过度自信。此外，即使是允许估计输入概率密度的深度生成模型也难以完成此任务。本文主要集中于这类模型中的一种：变分自编码器（VAE）。首先，我们揭示了经典VAE模型假设中的一个重大理论缺陷。其次，我们通过引入紧性作为从深度神经映射到潜在空间的图像的拓扑特征来纠正这一缺陷，并获得将图像压缩在确定限制内的可证界限来同时压缩内点和离群点的手段。我们采用两种方法实现紧性：（i）亚历山大夫扩展和（ii）对VAE编码器的映射进行固定的Lipschitz连续性常数。最后但也最重要的是，我们提出了一种基于利用已有建模技术和结构知识的无监督异常检测算法。

    Detection of the outliers is pivotal for any machine learning model deployed and operated in real-world. It is essential for the Deep Neural Networks that were shown to be overconfident with such inputs. Moreover, even deep generative models that allow estimation of the probability density of the input fail in achieving this task. In this work, we concentrate on the specific type of these models: Variational Autoencoders (VAEs). First, we unveil a significant theoretical flaw in the assumption of the classical VAE model. Second, we enforce an accommodating topological property to the image of the deep neural mapping to the latent space: compactness to alleviate the flaw and obtain the means to provably bound the image within the determined limits by squeezing both inliers and outliers together. We enforce compactness using two approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity constant on the mapping of the encoder of the VAEs. Finally and most importantly, we di
    
[^10]: 结构限制玻尔兹曼机用于图像去噪和分类

    Structural Restricted Boltzmann Machine for image denoising and classification. (arXiv:2306.09628v1 [cs.CV])

    [http://arxiv.org/abs/2306.09628](http://arxiv.org/abs/2306.09628)

    本文提出了一种结构限制的玻尔兹曼机模型，可以用于图像建模中的去噪和分类任务，通过限制隐藏单元与可见单元的子集的连接，大大降低了可训练参数的数量并保持了模型的性能。

    

    限制玻尔兹曼机是生成模型，由一层隐变量连接到另一层可见单元，用于建模可见变量的分布。为了获得更高的表征能力，通常使用许多隐藏单元，这与大量可见单元结合使用会导致训练参数数量很大。在这项工作中，我们引入了结构限制玻尔兹曼机模型，它利用手头数据的结构，将隐藏单元的连接限制到可见单元的子集上，以显著降低可训练参数的数量，而不影响性能。作为可能的应用领域，我们专注于图像建模。基于图像的特性，连接的结构是通过图像像素上的空间邻域给出的，这些像素构成模型的可见变量。我们进行了广泛的实验。

    Restricted Boltzmann Machines are generative models that consist of a layer of hidden variables connected to another layer of visible units, and they are used to model the distribution over visible variables. In order to gain a higher representability power, many hidden units are commonly used, which, in combination with a large number of visible units, leads to a high number of trainable parameters. In this work we introduce the Structural Restricted Boltzmann Machine model, which taking advantage of the structure of the data in hand, constrains connections of hidden units to subsets of visible units in order to reduce significantly the number of trainable parameters, without compromising performance. As a possible area of application, we focus on image modelling. Based on the nature of the images, the structure of the connections is given in terms of spatial neighbourhoods over the pixels of the image that constitute the visible variables of the model. We conduct extensive experiment
    
[^11]: 机器学习中出现的幂律动态研究

    Power-law Dynamic arising from machine learning. (arXiv:2306.09624v1 [stat.ML])

    [http://arxiv.org/abs/2306.09624](http://arxiv.org/abs/2306.09624)

    该论文研究了机器学习中的幂律动态，证明了其具有唯一平稳分布且可以通过比较连续和离散化幂律动态的出现时间来指导机器学习算法。

    

    我们研究了一种新的随机微分方程，这种方程起源于机器学习中的优化研究，我们称之为幂律动态，因为其平稳分布不能具有亚高斯尾部并服从幂律。我们证明，只要学习率足够小，幂律动态是遍历的且具有唯一的平稳分布。我们调查了它的首次存在时间。特别是，我们比较了连续的幂律动态及其离散化在退出时间上的差异。这种比较可以帮助指导机器学习算法。

    We study a kind of new SDE that was arisen from the research on optimization in machine learning, we call it power-law dynamic because its stationary distribution cannot have sub-Gaussian tail and obeys power-law. We prove that the power-law dynamic is ergodic with unique stationary distribution, provided the learning rate is small enough. We investigate its first exist time. In particular, we compare the exit times of the (continuous) power-law dynamic and its discretization. The comparison can help guide machine learning algorithm.
    
[^12]: 一个置信集的数量是否是一种衡量认知不确定性的好方法？

    Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?. (arXiv:2306.09586v1 [cs.LG])

    [http://arxiv.org/abs/2306.09586](http://arxiv.org/abs/2306.09586)

    本文探讨了将不确定性表示为一个置信集而非单一概率分布的方法。并发现，在二元分类中，信任集的体积是一种有意义的衡量认知不确定性的方法，但在多类分类中则没有这种效果。

    

    充分的不确定性表示和量化在各种科学学科中变得非常重要，特别是在机器学习和人工智能领域。作为表示不确定性的一种替代方法，我们考虑信任集（一组概率分布的凸集）。信任集的几何表示作为$d$维多面体意味着对（认知）不确定性的几何直觉。在本文中，我们展示了在二元分类的情况下，信任集的几何表示的体积是认知不确定性的一种有意义的度量方法，但在多类分类时则不那么有效。我们的理论发现强调了在机器学习中指定和使用正确的不确定性度量方法以及意识到可能的风险的关键作用。

    Adequate uncertainty representation and quantification have become imperative in various scientific disciplines, especially in machine learning and artificial intelligence. As an alternative to representing uncertainty via one single probability measure, we consider credal sets (convex sets of probability measures). The geometric representation of credal sets as $d$-dimensional polytopes implies a geometric intuition about (epistemic) uncertainty. In this paper, we show that the volume of the geometric representation of a credal set is a meaningful measure of epistemic uncertainty in the case of binary classification, but less so for multi-class classification. Our theoretical findings highlight the crucial role of specifying and employing uncertainty measures in machine learning in an appropriate way, and for being aware of possible pitfalls.
    
[^13]: 基于几何的规则在多个独立时间序列中进行变点检测的剪枝

    Geometric-Based Pruning Rules For Change Point Detection in Multiple Independent Time Series. (arXiv:2306.09555v1 [stat.ME])

    [http://arxiv.org/abs/2306.09555](http://arxiv.org/abs/2306.09555)

    该论文提出了一些基于几何形状的扩展函数剪枝规则，用于解决在多个独立时间序列中检测多个变点的问题，在小维度情况下可以比函数剪枝更快地准确检测更好。

    

    我们考虑检测多个独立时间序列中的多个变点的问题。寻找最佳分割可以表达为在给定成本函数上的最小化问题。我们专注于解决此问题的动态规划算法。当变化次数与数据长度成比例时，PELT算法中编码的基于不等式的剪枝规则会导致线性时间复杂度。另一种称为函数剪枝的剪枝方法，对于分析单变量时间序列而言，无论变化次数如何，其时间复杂度都接近于线性。我们提出了一些基于使用简单几何形状（球体和超矩形）的函数剪枝的扩展，重点关注高斯情况，但我们的一些规则可以轻松扩展到指数族。在模拟研究中，我们比较了不同基于几何的剪枝规则的计算效率。我们表明，在小维度情况下，使用超矩形和球体进行剪枝可以比函数剪枝更快速地准确检测更好。对于较大维度，超矩形变得不那么高效，而球体仅在高信噪比的情况下仍然具有竞争力。

    We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series.  We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for smal
    
[^14]: 在线重尾变点检测

    Online Heavy-tailed Change-point detection. (arXiv:2306.09548v1 [stat.ML])

    [http://arxiv.org/abs/2306.09548](http://arxiv.org/abs/2306.09548)

    本文提出了一种在线变点检测算法，可以应对重尾分布且保证有限的假阳性率。

    

    我们研究了在线变点检测 (OCPD) 的算法，其中样本可能是重尾分布，一个接一个地呈现，并且必须尽早检测到底层均值的变化。我们提出了一种基于裁剪随机梯度下降 (SGD) 的算法，即使我们仅假定数据生成过程的第二阶矩有界，该算法也能正常工作。我们派生了在所有具有有界第二矩的分布族中最坏情况下的有限样本假阳性率 (FPR) 的保证。因此，我们的方法是第一个保证有限样本 FPR 的 OCPD 算法，即使数据是高维的，底层分布是重尾的。我们论文的技术贡献是展示了裁剪 SGD 可以估计随机向量的均值并同时在所有置信度值上提供置信度界限。我们将这个稳健的估计与并集边界论证相结合，构建一个有限的顺序变点算法。

    We study algorithms for online change-point detection (OCPD), where samples that are potentially heavy-tailed, are presented one at a time and a change in the underlying mean must be detected as early as possible. We present an algorithm based on clipped Stochastic Gradient Descent (SGD), that works even if we only assume that the second moment of the data generating process is bounded. We derive guarantees on worst-case, finite-sample false-positive rate (FPR) over the family of all distributions with bounded second moment. Thus, our method is the first OCPD algorithm that guarantees finite-sample FPR, even if the data is high dimensional and the underlying distributions are heavy-tailed. The technical contribution of our paper is to show that clipped-SGD can estimate the mean of a random vector and simultaneously provide confidence bounds at all confidence values. We combine this robust estimate with a union bound argument and construct a sequential change-point algorithm with finite
    
[^15]: 针对潜在混淆下的因果结果的更紧密预测区间

    Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])

    [http://arxiv.org/abs/2306.09520](http://arxiv.org/abs/2306.09520)

    本文提出了一种名为Caus-Modens的算法，通过调制集合来描述因果结果区间，相比符合性预测方法，能够在实践中给出更紧密的结果区间。

    

    在存在隐藏混淆因素的情况下进行确切个体治疗结果的因果推断很少可能。因此，最近的研究改进了符合性预测方法，以产生结果区间。不幸的是，这类方法往往过于保守，有时会给出无信息量的区间。我们介绍了一种另类方法Caus-Modens，用于通过调制集合来描述因果结果区间。受到贝叶斯统计和集成不确定性量化的启发，Caus-Modens在实践中给出更紧密的结果区间，并通过三个分离基准测试的必要区间大小来实现足够的覆盖率。最后一个基准是使用未知但可探明的基础事实开展观察实验的GPT-4的新型用途。

    Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
    
[^16]: 基于可复制的机器学习方法的大规模量子可分性研究

    Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v1 [quant-ph])

    [http://arxiv.org/abs/2306.09444](http://arxiv.org/abs/2306.09444)

    本研究提出了一个机器学习管道用于大规模场景下量子可分性的近似解，通过有效算法近似查找最近的可分离密度矩阵，并将量子可分性视为分类问题，对任何二维混合状态都适用。

    

    量子可分性问题是指如何判断一个二分体密度矩阵是纠缠的还是可分的。我们提出了一种机器学习管道，用于在大规模场景下找到此NP-难问题的近似解。我们提供了一种基于Frank-Wolfe的有效算法来近似查找最近的可分离密度矩阵，并推导了一种系统的方法将密度矩阵标记为可分离的或纠缠的，使我们能够将量子可分性视为分类问题。我们的方法适用于任何二维混合状态。对3-和7维度中的量子态进行的数值实验验证了所提出的程序的效率，并证明它可以扩展到上千个密度矩阵，并具有高量子纠缠检测精度。这一进展有助于基准测试量子可分性，并支持更强大的纠缠检测技术的发展。

    The quantum separability problem consists in deciding whether a bipartite density matrix is entangled or separable. In this work, we propose a machine learning pipeline for finding approximate solutions for this NP-hard problem in large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to approximately seek the nearest separable density matrix and derive a systematic way for labeling density matrices as separable or entangled, allowing us to treat quantum separability as a classification problem. Our method is applicable to any two-qudit mixed states. Numerical experiments with quantum states of 3- and 7-dimensional qudits validate the efficiency of the proposed procedure, and demonstrate that it scales up to thousands of density matrices with a high quantum entanglement detection accuracy. This takes a step towards benchmarking quantum separability to support the development of more powerful entanglement detection techniques.
    
[^17]: 无监督非线性流形学习进行异常检测

    Unsupervised Anomaly Detection via Nonlinear Manifold Learning. (arXiv:2306.09441v1 [stat.ML])

    [http://arxiv.org/abs/2306.09441](http://arxiv.org/abs/2306.09441)

    该篇论文提出了一种基于非线性流形学习的无监督异常检测方法，可以鲁棒、高效、可解释地检测数据中的异常样本。

    

    异常样本指的是与其他数据显著偏离的样本，其检测在构建可靠的机器学习模型中起着重要作用。现有的大多数异常检测方法要么仅适用于（半）监督设置，要么在没有带标记异常样本的无监督应用中表现很差。为了填补这一研究空白，我们介绍了一种基于非线性流形学习的鲁棒、高效且可解释的方法，用于在无监督设置下检测异常。

    Anomalies are samples that significantly deviate from the rest of the data and their detection plays a major role in building machine learning models that can be reliably used in applications such as data-driven design and novelty detection. The majority of existing anomaly detection methods either are exclusively developed for (semi) supervised settings, or provide poor performance in unsupervised applications where there is no training data with labeled anomalous samples. To bridge this research gap, we introduce a robust, efficient, and interpretable methodology based on nonlinear manifold learning to detect anomalies in unsupervised settings. The essence of our approach is to learn a low-dimensional and interpretable latent representation (aka manifold) for all the data points such that normal samples are automatically clustered together and hence can be easily and robustly identified. We learn this low-dimensional manifold by designing a learning algorithm that leverages either a 
    
[^18]: 通过不偏微分对抗复杂密度生成，基于互联马尔科夫链不偏微分优化 MH 采样方法

    Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])

    [http://arxiv.org/abs/2306.07961](http://arxiv.org/abs/2306.07961)

    本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。

    

    在概率模型推理中，目标密度函数通常变得难以计算，需要使用 Monte Carlo 计算。本文开发了一种不偏微分 Metropolis-Hastings 采样器的方法，使我们可以通过概率推理来进行微分。通过将随机微分的最新进展与 Markov 链耦合方法相结合，可以实现无偏，低方差和自动的程序。这使我们能够将基于梯度的优化应用于由于繁琐的目标密度导致期望的情况下。我们通过在高斯混合模型中找到一个模棱两可的观察和在 Ising 模型中最大化比热来演示了我们的方法。

    When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
    
[^19]: 多研究R-learner用于异质性处理效应估计

    Multi-study R-learner for Heterogeneous Treatment Effect Estimation. (arXiv:2306.01086v1 [stat.ME])

    [http://arxiv.org/abs/2306.01086](http://arxiv.org/abs/2306.01086)

    本文提出了一种名为多研究R-learner的方法，能够很好地估计多研究中的异质性处理效应，具有鲁棒性，并在现实癌症数据实验中表现出更小的估计误差。

    

    我们提出了一种通用的算法类来估计多个研究中的异质性处理效应。我们的方法称为多研究R-learner，可以概括R-learner以考虑研究间的异质性并实现调整混淆的跨研究鲁棒性。多研究R-learner能够灵活地融合许多机器学习技术以估计异质性处理效应、困扰函数和成员概率。我们表明，多研究R-learner处理效应估计器在序列估计框架内是渐近正常的。此外，我们通过现实癌症数据实验证明，随着研究间的异质性增加，与R-learner相比，我们的方法估计误差更小。

    We propose a general class of algorithms for estimating heterogeneous treatment effects on multiple studies. Our approach, called the multi-study R-learner, generalizes the R-learner to account for between-study heterogeneity and achieves cross-study robustness of confounding adjustment. The multi-study R-learner is flexible in its ability to incorporate many machine learning techniques for estimating heterogeneous treatment effects, nuisance functions, and membership probabilities. We show that the multi-study R-learner treatment effect estimator is asymptotically normal within the series estimation framework. Moreover, we illustrate via realistic cancer data experiments that our approach results in lower estimation error than the R-learner as between-study heterogeneity increases.
    
[^20]: ChemCrow:用化学工具增强大型语言模型

    ChemCrow: Augmenting large-language models with chemistry tools. (arXiv:2304.05376v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.05376](http://arxiv.org/abs/2304.05376)

    本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。

    

    近期大型语言模型(LLMs)在跨领域的任务表现出一定的优势，但在化学相关问题上却表现不佳。此外，这些模型缺乏访问外部知识源，限制了它们在科学应用中的有用性。在本研究中，我们介绍了ChemCrow，一种LLM化学代理，旨在完成有机合成、药物发现和材料设计等任务。通过整合13个专家设计的工具，ChemCrow提高了LLM在化学中的性能，并产生了新的能力。我们的评估，包括LLM和人类专家评估，证明了ChemCrow在自动化各种化学任务方面的有效性。令人惊讶的是，我们发现GPT-4作为评估器无法区分明显错误的GPT-4完成和GPT-4 + ChemCrow性能。这种工具的滥用有很大的风险，我们讨论了它们的潜在危害。在负责任的情况下，ChemCrow不仅可以帮助专业化学家并降低成本。

    Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers ba
    
[^21]: 利用人类语言理解能力增强药物研发中的活性预测模型

    Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2303.03363](http://arxiv.org/abs/2303.03363)

    本文提出了一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务，并在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。

    

    活性和性质预测模型是药物研发和材料科学中的核心工作，但目前它们必须经过训练或微调才能适应新任务。科学语言模型具有零数据和少数据样本的能力，因此对于此类低数据任务，无需训练或微调即可使用。然而，它们在活性预测方面的预测质量不足。本文提出一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务。为此，我们提出了一种新的结构，具有化学和自然语言输入的分离模块，以及大型生物化学数据库中的对比预训练目标。通过大量实验证明，我们的方法CLAMP在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。我们认为我们的方法的进展归因于情境感知模型和对比学习策略的结合，以及用于结合数据源的对抗性自编码器。

    Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pre-training objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. We attribute the advances of our method to the mo
    
[^22]: 关于两维正弦模型最小绝对偏差估计的一致性和渐近正态性

    On Consistency and Asymptotic Normality of Least Absolute Deviation Estimators for 2-dimensional Sinusoidal Model. (arXiv:2301.03229v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2301.03229](http://arxiv.org/abs/2301.03229)

    本文提出了一种鲁棒的最小绝对偏差估计器，用于2维正弦模型参数估计，在数据存在异常值或重尾噪声时具有优越性并得到强一致性和渐近正态性的保证。

    

    在数字信号处理和时间序列分析中，2维正弦模型参数估计是一个基本问题。本文提出了一种鲁棒的最小绝对偏差 (LAD) 估计器，用于参数估计。该方法提供了一种抗干扰的替代方法，可以应对数据中存在异常值或重尾噪声等非鲁棒估计技术无法处理的情况。我们研究了LAD估计器的重要渐近性质，并证明了2维正弦模型信号参数的LAD估计器具有强一致性和渐近正态性。通过大量的模拟研究，我们进一步说明了使用LAD估计器优于最小二乘估计器的优势。对2维纹理数据的数据分析表明了所提出的LAD方法的实际应用性。

    Estimation of the parameters of a 2-dimensional sinusoidal model is a fundamental problem in digital signal processing and time series analysis. In this paper, we propose a robust least absolute deviation (LAD) estimators for parameter estimation. The proposed methodology provides a robust alternative to non-robust estimation techniques like the least squares estimators, in situations where outliers are present in the data or in the presence of heavy tailed noise. We study important asymptotic properties of the LAD estimators and establish the strong consistency and asymptotic normality of the LAD estimators of the signal parameters of a 2-dimensional sinusoidal model. We further illustrate the advantage of using LAD estimators over least squares estimators through extensive simulation studies. Data analysis of a 2-dimensional texture data indicates practical applicability of the proposed LAD approach.
    
[^23]: 带有结构图像协方差的VAE正则化的压缩感知MRI重建

    Compressed Sensing MRI Reconstruction Regularized by VAEs with Structured Image Covariance. (arXiv:2210.14586v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.14586](http://arxiv.org/abs/2210.14586)

    该论文提出了一种利用生成模型作为反问题先验知识、利用变分自编码器生成包括协方差信息在内的图像并提供新距离度量的方法来解决采样稀疏和有噪声的MRI重建问题，实现了最先进的重建性能并突出了解剖结构。

    

    本文研究了如何利用基于真实图像训练的生成模型作为反问题的先验知识，惩罚与生成器无法产生的图像之间的重建差异，旨在实现学习的正则化先验适应于复杂反问题，同时仍保留变分正则方法的控制和洞察力。我们利用变分自编码器（VAE）生成不仅包含图像信息，还包含每个图像的协方差信息。协方差可以建模图像中的结构变化引起的不确定性依赖关系，并提供了从学习图像流形中产生新距离度量的方法。我们将这种方法应用于MRI重建任务，对采样稀疏和有噪声的k空间数据进行了评估。实验结果表明，该方法实现了最先进的重建性能，同时提供了可解释的协方差矩阵，改善了重建质量并突出了解剖结构。此外，我们学习的协方差矩阵可以用于生成数据增强的真实感扰动。

    Objective: This paper investigates how generative models, trained on ground-truth images, can be used \changes{as} priors for inverse problems, penalizing reconstructions far from images the generator can produce. The aim is that learned regularization will provide complex data-driven priors to inverse problems while still retaining the control and insight of a variational regularization method. Moreover, unsupervised learning, without paired training data, allows the learned regularizer to remain flexible to changes in the forward problem such as noise level, sampling pattern or coil sensitivities in MRI.  Approach: We utilize variational autoencoders (VAEs) that generate not only an image but also a covariance uncertainty matrix for each image. The covariance can model changing uncertainty dependencies caused by structure in the image, such as edges or objects, and provides a new distance metric from the manifold of learned images.  Main results: We evaluate these novel generative re
    
[^24]: 使用对比剪枝权重训练去偏置子网络

    Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05247](http://arxiv.org/abs/2210.05247)

    本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。

    

    神经网络通常存在偏置性，导致提供具有误导性的统计证据，不能很好地推广。因此，提出了在偏置网络中提取最优无偏功能子网络的问题。本文首先提出了现有算法在探索具有强假相关性的无偏子网络存在限制的理论洞见，然后进一步阐明了偏差冲突样本对结构学习的重要性，并基于学习的（伪）无偏样本和选择性偏差冲突样本，提出了去偏置对比剪枝（DCWP）算法。在图像分类、语言模型和强化学习等各种应用中验证了 DCWP 的有效性。

    Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
    
[^25]: 基于数据驱动的最优化因果推断影响函数

    Data-Driven Influence Functions for Optimization-Based Causal Inference. (arXiv:2208.13701v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.13701](http://arxiv.org/abs/2208.13701)

    本文提出了一种利用有限差分逼近统计泛函Gateaux导数的构造算法，并研究了从数据中进行概率分布估计的情况下的Gateaux导数估计。研究结果为因果推断和动态治疗方案等问题提供了解决方案。

    

    本研究探讨了一种利用有限差分逼近统计泛函Gateaux导数的构造算法，重点研究了在因果推断中出现的泛函。我们研究了概率分布未知但需要从数据中进行估计的情况。这些估计分布引导了经验Gateaux导数，我们研究了经验、数值和解析Gateaux导数之间的关系。从干预均值（平均潜在结果）的案例入手，我们勾勒了有限差分和解析Gateaux导数之间的关系。然后，我们得出了关于扰动和平滑的数值逼近速率要求，以保持单步调整的统计优势，例如速率双重强健性。接下来，我们研究了更复杂的泛函，如动态治疗方案、无限时段Markov决策中策略优化的线性规划形式。

    We study a constructive algorithm that approximates Gateaux derivatives for statistical functionals by finite differencing, with a focus on functionals that arise in  causal inference. We study the case where probability distributions are not known a priori but need to be estimated from data. These estimated distributions lead to empirical Gateaux derivatives, and we study the relationships between empirical, numerical, and analytical Gateaux derivatives. Starting with a case study of the interventional mean (average potential outcome), we delineate the relationship between finite differences and the analytical Gateaux derivative. We then derive requirements on the rates of numerical approximation in perturbation and smoothing that preserve the statistical benefits of one-step adjustments, such as rate double robustness. We then study more complicated functionals such as dynamic treatment regimes, the linear-programming formulation for policy optimization in infinite-horizon Markov dec
    
[^26]: 解决机器学习众包工作者的人类受试者身份问题

    Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2206.04039](http://arxiv.org/abs/2206.04039)

    机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。

    

    近年来，机器学习(Machine Learning, ML)在构建数据集和解决需要人类交互或判断的研究问题方面，已经严重依赖于众包工作者。由于执行的任务多样化和数据用途的多样性，很难确定何时将众包工作者视为工人(而非人类受试者)。这些困难加剧了政策的冲突，一些机构和研究人员将所有ML众包工作者视为人类受试者，而其他人则认为它们很少构成人类受试者。值得注意的是，包括众包工作的鲜有ML论文提到IRB的监督，引发了违反道德和法规要求的可能性。我们研究了ML众包研究的适当划定，并关注自然语言处理领域暴露出的独特研究监督挑战。至关重要的是，在美国公共规则下，这些判断取决于关于问题的确定，涉及谁(或什么)的问题。

    In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
    
[^27]: 动态治疗效应：模型错误下的高维推断

    Dynamic treatment effects: high-dimensional inference under model misspecification. (arXiv:2111.06818v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2111.06818](http://arxiv.org/abs/2111.06818)

    本文提出了一种新的鲁棒估计方法来解决动态治疗效应估计中的挑战，提高了在模型错误下的高维环境中的估计鲁棒性和可靠性。

    

    估计动态治疗效应在各个学科中都是至关重要的，可以提供有关干预的时变因果影响的微妙见解。然而，由于“维数灾难”和时变混杂的存在，这种估计存在着挑战，可能导致估计偏误。此外，正确地规定日益增多的治疗分配和多重暴露的结果模型似乎过于复杂。鉴于这些挑战，双重鲁棒性的概念，在允许模型错误的情况下，是非常有价值的，然而在实际应用中并没有实现。本文通过提出新的鲁棒估计方法来解决这个问题，同时对治疗分配和结果模型进行鲁棒估计。我们提出了一种“序列模型双重鲁棒性”的解决方案，证明了当每个时间暴露都是双重鲁棒性的时，可以在多个时间点上实现双重鲁棒性。这种方法提高了高维环境下动态治疗效应估计的鲁棒性和可靠性。

    Estimating dynamic treatment effects is essential across various disciplines, offering nuanced insights into the time-dependent causal impact of interventions. However, this estimation presents challenges due to the "curse of dimensionality" and time-varying confounding, which can lead to biased estimates. Additionally, correctly specifying the growing number of treatment assignments and outcome models with multiple exposures seems overly complex. Given these challenges, the concept of double robustness, where model misspecification is permitted, is extremely valuable, yet unachieved in practical applications. This paper introduces a new approach by proposing novel, robust estimators for both treatment assignments and outcome models. We present a "sequential model double robust" solution, demonstrating that double robustness over multiple time points can be achieved when each time exposure is doubly robust. This approach improves the robustness and reliability of dynamic treatment effe
    
[^28]: 提升简单学习器的能力

    Boosting Simple Learners. (arXiv:2001.11704v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.11704](http://arxiv.org/abs/2001.11704)

    本论文探讨的是提升学习器的方法，关注弱学习器属于一个容量受限的类的假设，并重点关注需要多少个弱学习器才能生成准确的假设。通过设计新颖的算法，只需要约$\tilde{O}({1}/{\gamma})$个弱假设就能够规避经典下界。

    

    提升是一种著名的机器学习方法，基于结合弱且具有适度错误的假设，以生成强且准确的假设。本研究探讨一种假设：弱学习器属于一个有边界容量的类。该假设来自于常见的传统做法，即将弱学习器视为“规则”或“基础类”中的“经验法则”（Schapire和Freund'12、Shalev-Shwartz和Ben-David'14）。具体而言，我们假设弱学习器的VC维度受到限制。我们主要关注两个问题：（i）Oracle复杂度：需要多少个弱学习器才能生成准确的假设？我们设计了一种新颖的提升算法，并证明它规避了Freund和Schapire的经典下界（'95，'12）。尽管下界表明有时需要具有$\gamma$-间隙的$\Omega({1}/{\gamma^2})$个弱假设，但我们的新方法只需要提供约$\tilde{O}({1}/{\gamma})$个弱假设，假设其

    Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are "rules-of-thumbs" from an "easy-to-learn class". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that $\Omega({1}/{\gamma^2})$ weak hypotheses with $\gamma$-margin are sometimes necessary, our new method requires only $\tilde{O}({1}/{\gamma})$ weak hypothesis, provided that the
    

