# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Price of Adaptivity in Stochastic Convex Optimization](https://arxiv.org/abs/2402.10898) | 该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。 |
| [^2] | [Trading off Consistency and Dimensionality of Convex Surrogates for the Mode](https://arxiv.org/abs/2402.10818) | 通过在低维替代空间中的凸多面体顶点上嵌入结果，并探究单纯形中的一致性区域，权衡了替代损失维度、问题实例数量。 |
| [^3] | [Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2402.10810) | 本文提出了一种基于模型的算法，Variational Primal-Dual Policy Optimization (VPDPO)，通过实现Lagrangian和Fenchel对偶来将原始受限问题重构为无约束的原始-对偶优化，并且采用乐观原则更新原始变量和梯度上升更新对偶变量。 |
| [^4] | [BlackJAX: Composable Bayesian inference in JAX](https://arxiv.org/abs/2402.10797) | BlackJAX是一个实现在JAX中组合式贝叶斯推断的库，采用函数式方法提高易用性、速度和模块化，适用于需要尖端方法、研究人员和想要了解工作原理的人。 |
| [^5] | [Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants](https://arxiv.org/abs/2402.10774) | 本文研究了一种现代形式的错误反馈EF21，将其依赖的通信复杂度从平方平均值改进为更小的算术平均值，在实践中表现良好。 |
| [^6] | [Stochastic Localization via Iterative Posterior Sampling](https://arxiv.org/abs/2402.10758) | 本论文提出了一种名为SLIPS的方法，通过迭代后验抽样实现随机定位，填补了从非标准化目标密度中抽样的问题的空白。 |
| [^7] | [Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules](https://arxiv.org/abs/2402.10727) | 通过引入风险分解和适当评分规则，我们提出了一个通用框架来量化预测不确定性的不同来源，并澄清了它们之间的关系。 |
| [^8] | [Conformalized Credal Set Predictors](https://arxiv.org/abs/2402.10723) | 本论文提出了一种利用一致预测方法来学习可信集合预测器的方法，能有效表示预测中的不确定性。 |
| [^9] | [Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model](https://arxiv.org/abs/2402.10677) | 研究了嵌套矩阵张量模型中多视图聚类的性能差距，量化了张量方法和矩阵方法之间的性能差距，并发现了展开方法的算法阈值，展示了类似BBP过渡行为。 |
| [^10] | [Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification](https://arxiv.org/abs/2402.10592) | 提出了一种统一模型，同时考虑了实验内部性能和实验后结果，在优化大规模人群中的表现方面提供了尖锐理论，揭示了新颖的见解 |
| [^11] | [Nowcasting with mixed frequency data using Gaussian processes](https://arxiv.org/abs/2402.10574) | 使用高斯过程和贝叶斯添加回归树作为线性惩罚估计的灵活扩展，解决了混合频率数据中的频率不匹配问题，提高了现场预测的准确性。 |
| [^12] | [Resilience of the quadratic Littlewood-Offord problem](https://arxiv.org/abs/2402.10504) | 论文研究了二次Littlewood-Offord问题的统计鲁棒性，估计了对抗性噪声对二次Radamecher混沌的影响，并提供了对二次和双线性Rademacher混沌的统计鲁棒性的下限估计。 |
| [^13] | [Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise](https://arxiv.org/abs/2402.10482) | 自蒸馏在多类别分类中扮演着标签平均化的角色，有助于模型关注与特定实例相关的特征簇以预测标签，但随着蒸馏轮次增加，性能会降低。此外，在标签噪声情景下自蒸馏被证明是有效的，找到了实现100%分类准确率所需的最小蒸馏轮次。 |
| [^14] | [One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression](https://arxiv.org/abs/2402.10474) | 通过正则化回归，在超参数化范围内，根据特定选择的凸函数并适当增加一个正则化项，可以实现稀疏和一位解决方案，其性能几乎与最佳分类性能相同。 |
| [^15] | [Theoretical Understanding of Learning from Adversarial Perturbations](https://arxiv.org/abs/2402.10470) | 研究提供了一个理论框架，表明各种对抗性扰动（甚至是几个像素的扰动）包含足够的类特征用于泛化，进一步揭示了从扰动中学习时的决策边界 |
| [^16] | [Generative Modeling for Tabular Data via Penalized Optimal Transport Network](https://arxiv.org/abs/2402.10456) | 提出了一种名为POTNet的生成建模网络，基于边缘惩罚的Wasserstein损失，能够有效地建模同时包含分类和连续特征的表格数据。 |
| [^17] | [Collaborative Learning with Different Labeling Functions](https://arxiv.org/abs/2402.10445) | 研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。 |
| [^18] | [Fixed Confidence Best Arm Identification in the Bayesian Setting](https://arxiv.org/abs/2402.10429) | 该研究在贝叶斯设置中探讨了固定置信度最佳臂识别问题，证明了传统频率设定下的算法在此设置下表现次优，并引入了一种性能与理论下限相匹配的连续排除变种。 |
| [^19] | [Learnability is a Compact Property](https://arxiv.org/abs/2402.10360) | 监督学习问题的困难性具有紧凑的有限特性表征。 |
| [^20] | [Efficient Sampling on Riemannian Manifolds via Langevin MCMC](https://arxiv.org/abs/2402.10357) | 通过Langevin MCMC在黎曼流形上高效采样，并证明了在特定条件下迭代步数为$\tilde{O}(\epsilon^{-2})$时，Langevin MCMC的迭代会与目标分布在$\epsilon$-Wasserstein距离内。 |
| [^21] | [Mathematical Opportunities in Digital Twins (MATH-DT)](https://arxiv.org/abs/2402.10326) | 数字孪生中的数学机遇需要基础数学进展，与传统方法不同，数字孪生从特定现实出发，需要多尺度建模和耦合，通过传感器将数据输入，帮助人类做出决策。 |
| [^22] | [An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM](https://arxiv.org/abs/2402.10291) | KCUSUM算法是一种非参数扩展算法，用于在高容量数据情景下实时检测突变变化，相比于现有算法，其能够更灵活地在在线环境中进行变点检测。 |
| [^23] | [Thompson Sampling in Partially Observable Contextual Bandits](https://arxiv.org/abs/2402.10289) | 研究了在部分观察到的上下文特征老虎机中使用Thompson Sampling策略，以学习选择最佳臂的问题 |
| [^24] | [Information Capacity Regret Bounds for Bandits with Mediator Feedback](https://arxiv.org/abs/2402.10282) | 该研究提出了一种基于信息容量的新遗憾界限方法，适用于具有中介反馈的赌博机问题，同时在敌对和随机设置中提供了近乎匹配的下界。 |
| [^25] | [Online Control of Linear Systems with Unbounded and Degenerate Noise](https://arxiv.org/abs/2402.10252) | 这项研究揭示了在线控制问题中，对于凸成本，可以实现 $ \widetilde{O}(\sqrt{T}) $ 的遗憾界，甚至在存在无界噪声的情况下；同时，在成本具有强凸性时，可以在不需要噪声协方差是非退化的情况下建立 $ O({\rm poly} (\log T)) $ 的遗憾界。 |
| [^26] | [Simple, unified analysis of Johnson-Lindenstrauss with applications](https://arxiv.org/abs/2402.10232) | 这项工作提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，简化和统一了各种构造，包括球形、高斯、二进制硬币和次高斯模型，通过创新性地将Hanson-Wright不等式拓展到高维度，标志着对数据固有几何的保持取得重大进展。 |
| [^27] | [HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments](https://arxiv.org/abs/2402.10228) | HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。 |
| [^28] | [Correlational Lagrangian Schr\"odinger Bridge: Learning Dynamics with Population-Level Regularization](https://arxiv.org/abs/2402.10227) | 提出了一个名为相关拉格朗日薛定谔桥（CLSB）的新框架，通过人口级正则化来学习跨截面样本中的系统动态，适应个体粒子行为的异质性。 |
| [^29] | [GPT-4's assessment of its performance in a USMLE-based case study](https://arxiv.org/abs/2402.09654) | 本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。 |
| [^30] | [Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process](https://arxiv.org/abs/2402.04146) | 这篇论文提出了一种基于潜变量高斯过程的多源数据融合框架，用于解决多个数据源之间质量和全面性差异给系统优化带来的问题。 |
| [^31] | [Sample Path Regularity of Gaussian Processes from the Covariance Kernel](https://arxiv.org/abs/2312.14886) | 本文提供了关于高斯过程样本路径正则性的新颖和紧凑的特征描述，通过协方差核对应的GP样本路径达到一定正则性的充分必要条件，对常用于机器学习应用中的GPs的样本路径正则性进行了探讨。 |
| [^32] | [Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems](https://arxiv.org/abs/2312.01127) | 首次将均场 Langevin 动力学扩展到概率分布上的最小最大优化，提出了 MFL-AG 和 MFL-ABR 两种算法，实现了对混合 Nash 平衡的收敛，还研究了时间和粒子离散化制度以及传播混沌结果。 |
| [^33] | [Interpretable Deep Learning Methods for Multiview Learning](https://arxiv.org/abs/2302.07930) | 提出iDeepViewLearn（可解释的多视图学习深度学习方法）用于多视图数据的非线性关系学习和特征选择，融合深度学习的灵活性和统计优势，给出可解释结果。 |
| [^34] | [Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble](https://arxiv.org/abs/2211.11278) | 提出了一种基于最优扩展邻域规则的集成方法，通过新规则确定邻居和模型选择策略来解决传统$k$最近邻方法的局限性和提升集成性能。 |
| [^35] | [Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions](https://arxiv.org/abs/2207.04771) | 提供了条件矩限制问题的功能化广义经验似然估计方法，并探索了其在小样本性能上的优势，同时提供了基于核和神经网络的实现方式。 |
| [^36] | [Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification](https://arxiv.org/abs/2206.12532) | 本文引入了因果评分作为一种新型方法，支持决策制定，提供洞察力，并可用于效应估计、效应排序和效应分类。 |
| [^37] | [Modeling Attrition in Recommender Systems with Departing Bandits](https://arxiv.org/abs/2203.13423) | 本论文提出了一个新型多臂赌博机设置，捕捉了推荐系统中用户离开的情况，首次证明了在所有用户共享相同类型时，基于UCB的算法是最优的。 |
| [^38] | [The Efficient Shrinkage Path: Maximum Likelihood of Minimum MSE Risk](https://arxiv.org/abs/2103.05161) | 提出了新的广义岭回归收缩路径，通过最小均方误差风险的最大似然实现最佳方差-偏差权衡，提供了宝贵的数据分析见解。 |
| [^39] | [Fr\'echet random forests for metric space valued regression with non euclidean predictors](https://arxiv.org/abs/1906.01741) | 本文介绍了Fréchet随机森林，允许处理值在一般度量空间的数据，并引入了新的树节点分裂方式，扩展了预测过程，提出了一致性定理，并适用于数据驱动分区的Fréchet纯一致随机树 |
| [^40] | [Robust Estimation of Pareto's Scale Parameter from Grouped Data.](http://arxiv.org/abs/2401.14593) | 本文介绍了一种新的稳健估计方法（MTuM），用于从分组数据中估计Pareto分布的尾指数。该方法通过应用中心极限定理和模拟研究验证了其推理合理性。 |
| [^41] | [Simple and Asymmetric Graph Contrastive Learning without Augmentations.](http://arxiv.org/abs/2310.18884) | 本文提出了一种无需增强的简单非对称图对比学习方法GraphACL，通过考虑邻居节点的非对称视图，该方法能够有效地在同类和异类图上进行对比学习，对于建模异类图非常重要。 |
| [^42] | [On the Stability of Iterative Retraining of Generative Models on their own Data.](http://arxiv.org/abs/2310.00429) | 本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。 |
| [^43] | [Quasi-Monte Carlo for 3D Sliced Wasserstein.](http://arxiv.org/abs/2309.11713) | 本文提出了准蒙特卡洛（QMC）方法用于三维切片Wasserstein（SW）的近似计算，并通过多种方法在三维单位超球面上构造了QMC点集。此外，还介绍了将QSW扩展为随机准切片Wasserstein（RQSW）的方法。 |
| [^44] | [Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators.](http://arxiv.org/abs/2303.08431) | 本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。 |
| [^45] | [Differential Good Arm Identification.](http://arxiv.org/abs/2303.07154) | 本文提出了DGAI算法，它可以在好手臂识别问题中通过深度学习的方式减少样本复杂性，并且在具有给定阈值的情况下进一步提高多臂赌博问题的性能。 |
| [^46] | [Invertible normalizing flow neural networks by JKO scheme.](http://arxiv.org/abs/2212.14424) | 本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。 |

# 详细

[^1]: 随机凸优化中适应性的代价

    The Price of Adaptivity in Stochastic Convex Optimization

    [https://arxiv.org/abs/2402.10898](https://arxiv.org/abs/2402.10898)

    该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。

    

    我们证明了在非光滑随机凸优化中适应性的不可能性结果。给定一组我们希望适应的问题参数，我们定义了“适应性的代价”（PoA），粗略地说，它衡量了由于这些参数的不确定性而导致的次优性的乘法增加。当初始距离最优解未知但梯度范数有界时，我们证明PoA至少对于期望次优性是对数级别，对于中位数次优性是双对数级别。当距离和梯度范数都存在不确定性时，我们表明PoA必须是与不确定性水平多项式相关的。我们的下界几乎与现有的上界相匹配，并且确定了没有无参数午餐的结论。

    arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
    
[^2]: 在模型的凸替代品的一致性和维度之间进行权衡

    Trading off Consistency and Dimensionality of Convex Surrogates for the Mode

    [https://arxiv.org/abs/2402.10818](https://arxiv.org/abs/2402.10818)

    通过在低维替代空间中的凸多面体顶点上嵌入结果，并探究单纯形中的一致性区域，权衡了替代损失维度、问题实例数量。

    

    在多类分类中，必须将结果嵌入到至少有$n-1$维的实数空间中，以设计一种一致的替代损失函数，这会导致"正确"的分类，而不受数据分布的影响。在信息检索和结构化预测任务等需要大量n时，优化n-1维替代常常是棘手的。我们研究了在多类分类中如何权衡替代损失维度、问题实例数量以及在单纯形上约束一致性区域的方法。我们跟随过去的研究，探讨了一种直观的嵌入过程，将结果映射到低维替代空间中的凸多面体的顶点上。我们展示了在每个点质量分布周围存在单纯形的全维子集，其中一致性成立，但是，少于n-1维度的情况下，存在一些分布，对于这些分布，一种现象性是

    arXiv:2402.10818v1 Announce Type: new  Abstract: In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the "correct" classification, regardless of the data distribution. For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomeno
    
[^3]: 双重对偶：用于受限制强化学习的变分原始对偶策略优化

    Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning

    [https://arxiv.org/abs/2402.10810](https://arxiv.org/abs/2402.10810)

    本文提出了一种基于模型的算法，Variational Primal-Dual Policy Optimization (VPDPO)，通过实现Lagrangian和Fenchel对偶来将原始受限问题重构为无约束的原始-对偶优化，并且采用乐观原则更新原始变量和梯度上升更新对偶变量。

    

    我们研究受限凸马尔可夫决策过程（MDP），目标是最小化访问度量的凸泛函，受到凸约束的限制。为受限凸MDP设计算法面临几个挑战，包括（1）处理大状态空间，（2）管理探索/开拓的权衡，以及（3）解决约束优化，其中目标和约束都是访问度量的非线性函数。在这项工作中，我们提出了一种基于模型的算法，变分原始对偶策略优化（VPDPO），其中Lagrangian 和 Fenchel 对偶被用于将原始受限问题重新公式化为无限制原始-对偶优化。此外，原始变量通过基于模型的值迭代更新，遵循不确定性面前的乐观原则（OFU），而对偶变量则通过梯度上升更新。

    arXiv:2402.10810v1 Announce Type: new  Abstract: We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover,
    
[^4]: BlackJAX: JAX中的组合式贝叶斯推断

    BlackJAX: Composable Bayesian inference in JAX

    [https://arxiv.org/abs/2402.10797](https://arxiv.org/abs/2402.10797)

    BlackJAX是一个实现在JAX中组合式贝叶斯推断的库，采用函数式方法提高易用性、速度和模块化，适用于需要尖端方法、研究人员和想要了解工作原理的人。

    

    BlackJAX是一个库，实现了在贝叶斯计算中常用的抽样和变分推断算法。它通过采用函数式方法实现算法，旨在提高易用性、速度和模块化。BlackJAX使用Python编写，利用JAX在CPU、GPU和TPU上编译和运行类似Numpy的抽样器和变分方法。该库通过直接处理（非正则化）目标对数密度函数，与概率编程语言很好地集成。BlackJAX旨在成为基本统计“基元”的低级可组合实现的集合，可组合执行定义良好的贝叶斯推断，同时还提供高级例程以提高易用性。它面向需要尖端方法的用户、希望创建复杂抽样方法的研究人员，以及想要了解这些方法工作原理的人。

    arXiv:2402.10797v1 Announce Type: cross  Abstract: BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.
    
[^5]: 错误反馈重新加载：从平方到平滑度常数的算术平均值

    Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants

    [https://arxiv.org/abs/2402.10774](https://arxiv.org/abs/2402.10774)

    本文研究了一种现代形式的错误反馈EF21，将其依赖的通信复杂度从平方平均值改进为更小的算术平均值，在实践中表现良好。

    

    错误反馈（EF）是一种非常流行且极其有效的机制，用于解决分布式训练方法（如分布式GD或SGD）中由于与贪婪通信压缩技术（如TopK）结合而产生的收敛问题。尽管EF提出已有近十年时间（Seide等人，2014年），并且尽管社区为推进对该机制的理论理解而集中努力，仍有很多尚待探索之处。在本文中，我们研究了一种名为EF21（Richtarik等人，2021年）的现代形式的错误反馈，它提供了目前已知的最佳理论保证，在最弱的假设下也在实践中运行良好。特别地，虽然EF21的理论通信复杂度取决于某些平滑度参数的平方均值，我们将这种依赖性改进为它们的算术平均值，后者始终更小，尤其是在...

    arXiv:2402.10774v1 Announce Type: cross  Abstract: Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in
    
[^6]: 通过迭代后验抽样实现随机定位

    Stochastic Localization via Iterative Posterior Sampling

    [https://arxiv.org/abs/2402.10758](https://arxiv.org/abs/2402.10758)

    本论文提出了一种名为SLIPS的方法，通过迭代后验抽样实现随机定位，填补了从非标准化目标密度中抽样的问题的空白。

    

    建立在基于得分学习的基础上，近期对随机定位技术产生了新的兴趣。在这些模型中，人们通过随机过程（称为观测过程）为数据分布中的样本引入噪声，并逐渐学习与该动力学关联的去噪器。除了特定应用之外，对于从非标准化目标密度中抽样的问题，对随机定位的使用尚未得到广泛探讨。本项工作旨在填补这一空白。我们考虑了一个通用的随机定位框架，并引入了一类明确的观测过程，与灵活的去噪时间表相关联。我们提供了一种完整的方法论，即“通过迭代后验抽样实现随机定位”（SLIPS），以获得该动力学的近似样本，并作为副产品，样本来自目标分布。我们的方案基于马尔可夫链蒙特卡洛估计。

    arXiv:2402.10758v1 Announce Type: cross  Abstract: Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimati
    
[^7]: 通过风险分解实现严格适当评分规则的预测不确定性量化

    Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules

    [https://arxiv.org/abs/2402.10727](https://arxiv.org/abs/2402.10727)

    通过引入风险分解和适当评分规则，我们提出了一个通用框架来量化预测不确定性的不同来源，并澄清了它们之间的关系。

    

    在各个领域的预测模型应用中，区分预测不确定性的来源至关重要。尽管提出了许多不确定性度量，但并没有严格的定义来解开它们。此外，不同不确定性量化措施之间的关系仍然有些不清晰。在这项工作中，我们引入了一个根植于统计推理的通用框架，不仅允许创建新的不确定性度量，还澄清了它们之间的相互关系。我们的方法利用统计风险来区分aleatoric和epistemic不确定性成分，并利用适当的评分规则对其进行量化。为了使其在实践中易于处理，我们提出了在这一框架中整合贝叶斯推理的想法，并讨论了所提近似的性质。

    arXiv:2402.10727v1 Announce Type: cross  Abstract: Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation.
    
[^8]: 统一的可信集合预测器

    Conformalized Credal Set Predictors

    [https://arxiv.org/abs/2402.10723](https://arxiv.org/abs/2402.10723)

    本论文提出了一种利用一致预测方法来学习可信集合预测器的方法，能有效表示预测中的不确定性。

    

    可信集合是被视为不确定已知真实分布的候选概率分布集合。在机器学习中，由于可信集合能够表示预测中的不确定性，尤其是能够表示预测的aleatoric和epistemic不确定性，因此近来已经引起了关注。然而，设计用于学习可信集合预测器的方法仍然是一个具有挑战性的问题。在本文中，我们利用一致预测来解决这个问题。更具体地，我们提出了一种用于在分类任务中预测可信集合的方法，该方法利用标记为概率分布的训练数据。由于我们的方法继承了一致预测的覆盖性保证，我们的一致可信集合有很高的概率保证是有效的（而无需对模型或分布做出任何假设）。我们展示了我们的方法在自然语言中的适用性。

    arXiv:2402.10723v1 Announce Type: cross  Abstract: Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method to natural languag
    
[^9]: 多视图聚类在嵌套矩阵张量模型下的性能差异

    Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model

    [https://arxiv.org/abs/2402.10677](https://arxiv.org/abs/2402.10677)

    研究了嵌套矩阵张量模型中多视图聚类的性能差距，量化了张量方法和矩阵方法之间的性能差距，并发现了展开方法的算法阈值，展示了类似BBP过渡行为。

    

    我们研究了最近引入的嵌套矩阵张量模型中隐藏的植入信号的估计，该模型是经典尖峰秩一张量模型的扩展，受多视图聚类的启发。先前的工作在理论上研究了基于张量的方法的性能，该方法依赖于找到最佳秩一逼近，这是一个计算难题。一个可行的替代方法是计算观测到的张量数据展开的最佳秩一（矩阵）逼近，但其性能迄今为止未知。我们在这里量化了这两种方法之间的性能差距，特别是通过推导出展开方法的精确算法阈值，并展示它展现出类似BBP过渡行为。因此，这项工作与最近的贡献一致，这些贡献加深了我们对为什么张量方法优于矩阵方法的理解。

    arXiv:2402.10677v1 Announce Type: cross  Abstract: We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a BBP-type transition behavior. This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in 
    
[^10]: 优化自适应实验：最小化后悔和最佳臂识别的统一方法

    Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification

    [https://arxiv.org/abs/2402.10592](https://arxiv.org/abs/2402.10592)

    提出了一种统一模型，同时考虑了实验内部性能和实验后结果，在优化大规模人群中的表现方面提供了尖锐理论，揭示了新颖的见解

    

    进行自适应实验的从业者通常面临两个竞争性优先级：通过在实验过程中有效地分配治疗来降低实验成本，以及迅速收集信息以结束实验并在整个人群中实施治疗。当前，文献意见分歧，有关最小化后悔的研究独立地处理前者的优先级，而有关最佳臂识别的研究则专注于后者。本文提出了一种统一模型，考虑到实验内部性能和实验后结果。我们随后提供了一个针对大规模人群的最佳性能的尖锐理论，将文献中的经典结果统一起来。这种统一还揭示了新的见解。例如，理论揭示了类似最近提出的顶部两个Thompson抽样算法等熟悉算法可被调整以优化广泛类别的目标。

    arXiv:2402.10592v1 Announce Type: new  Abstract: Practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population. Currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter. This paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes. We then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature. This unification also uncovers novel insights. For example, the theory reveals that familiar algorithms, like the recently proposed top-two Thompson sampling algorithm, can be adapted to optimize a broad class of obj
    
[^11]: 使用高斯过程进行混合频率数据的现场预测

    Nowcasting with mixed frequency data using Gaussian processes

    [https://arxiv.org/abs/2402.10574](https://arxiv.org/abs/2402.10574)

    使用高斯过程和贝叶斯添加回归树作为线性惩罚估计的灵活扩展，解决了混合频率数据中的频率不匹配问题，提高了现场预测的准确性。

    

    我们提出并讨论了用于混合数据采样（MIDAS）回归的贝叶斯机器学习方法。这涉及使用受限和非受限的MIDAS变体处理频率不匹配，并指定许多预测变量与因变量之间的函数关系。我们使用高斯过程（GP）和贝叶斯添加回归树（BART）作为线性惩罚估计的灵活扩展。在现场预测和预测练习中，我们专注于季度美国产出增长和GDP价格指数的通货膨胀。这些新模型以计算效率的方式利用宏观经济大数据，并在多个维度上提供了预测准确度的增益。

    arXiv:2402.10574v1 Announce Type: new  Abstract: We propose and discuss Bayesian machine learning methods for mixed data sampling (MIDAS) regressions. This involves handling frequency mismatches with restricted and unrestricted MIDAS variants and specifying functional relationships between many predictors and the dependent variable. We use Gaussian processes (GP) and Bayesian additive regression trees (BART) as flexible extensions to linear penalized estimation. In a nowcasting and forecasting exercise we focus on quarterly US output growth and inflation in the GDP deflator. The new models leverage macroeconomic Big Data in a computationally efficient way and offer gains in predictive accuracy along several dimensions.
    
[^12]: 二次Littlewood-Offord问题的弹性

    Resilience of the quadratic Littlewood-Offord problem

    [https://arxiv.org/abs/2402.10504](https://arxiv.org/abs/2402.10504)

    论文研究了二次Littlewood-Offord问题的统计鲁棒性，估计了对抗性噪声对二次Radamecher混沌的影响，并提供了对二次和双线性Rademacher混沌的统计鲁棒性的下限估计。

    

    我们研究了高维数据的统计鲁棒性。我们的结果提供了关于对抗性噪声对二次Radamecher混沌$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$反集中特性的影响的估计，其中$M$是一个固定的（高维）矩阵，$\boldsymbol{\xi}$是一个共形Rademacher向量。具体来说，我们探讨了$\boldsymbol{\xi}$能够承受多少对抗性符号翻转而不“膨胀”$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$，从而“去除”原始分布导致更“有粒度”和对抗性偏倚的分布。我们的结果为二次和双线性Rademacher混沌的统计鲁棒性提供了下限估计；这些结果在关键区域被证明是渐近紧的。

    arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
    
[^13]: 理解带有标签噪音的多类别分类中的自蒸馏和部分标签学习

    Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise

    [https://arxiv.org/abs/2402.10482](https://arxiv.org/abs/2402.10482)

    自蒸馏在多类别分类中扮演着标签平均化的角色，有助于模型关注与特定实例相关的特征簇以预测标签，但随着蒸馏轮次增加，性能会降低。此外，在标签噪声情景下自蒸馏被证明是有效的，找到了实现100%分类准确率所需的最小蒸馏轮次。

    

    自蒸馏（SD）是使用教师模型的输出训练学生模型的过程，两个模型共享相同的架构。我们的研究从理论上考察了使用交叉熵损失的多类别分类中的SD，探索了多轮SD和具有精炼教师输出的SD，这些灵感来自部分标签学习（PLL）。通过推导学生模型输出的封闭形式解，我们发现SD本质上是在具有高特征相关性的实例之间进行标签平均。最初有益的平均化有助于模型专注于与给定实例相关联的特征簇以预测标签。然而，随着蒸馏轮次的增加，性能会下降。此外，我们展示了SD在标签噪声情景中的有效性，并确定实现100%分类准确率所需的标签损坏条件和最小蒸馏轮次数。

    arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur
    
[^14]: 一位量化和稀疏化用于多类线性分类的正则化回归

    One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression

    [https://arxiv.org/abs/2402.10474](https://arxiv.org/abs/2402.10474)

    通过正则化回归，在超参数化范围内，根据特定选择的凸函数并适当增加一个正则化项，可以实现稀疏和一位解决方案，其性能几乎与最佳分类性能相同。

    

    我们研究了在线性回归中用于多类分类的问题，这些问题在超参数化范围内，训练数据中一些标记错误。在这种情况下，为了避免过度拟合错误标记的数据，需要添加一个显式的正则化项，$\lambda f(w)$，其中$f(\cdot)$是某个凸函数。在我们的分析中，我们假设数据是从一个具有相等类大小的高斯混合模型中采样的，并且每个类别的训练标签中有一部分比例为$c$是错误的。在这些假设下，我们证明了当$f(\cdot) = \|\cdot\|^2_2$且$\lambda \to \infty$时，可以获得最佳的分类性能。然后我们继续分析了在大$\lambda$范围内$f(\cdot) = \|\cdot\|_1$和$f(\cdot) = \|\cdot\|_\infty$的分类错误，并且注意到通常可以找到稀疏和一位解决方案，分别表现几乎与$f(\cdot) = \|\cdot\|^2_2$相同。

    arXiv:2402.10474v1 Announce Type: new  Abstract: We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\lambda f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and $\lambda \to \infty$. We then proceed to analyze the classification errors for $f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large $\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\cdot) = \|\
    
[^15]: 对从对抗性扰动中学习的理论理解

    Theoretical Understanding of Learning from Adversarial Perturbations

    [https://arxiv.org/abs/2402.10470](https://arxiv.org/abs/2402.10470)

    研究提供了一个理论框架，表明各种对抗性扰动（甚至是几个像素的扰动）包含足够的类特征用于泛化，进一步揭示了从扰动中学习时的决策边界

    

    尚未完全理解为什么对抗性示例可以欺骗神经网络并在不同网络之间传递。为了阐明这一点，几项研究假设，尽管对抗性扰动看似是噪音，但实际上包含类特征。这得到了通过实证证据支持，即对于在错误标记的对抗性示例上训练的网络仍然可以很好地推广到正确标记的测试样本。然而，对于扰动如何包含类特征并促进泛化的理论理解是有限的。在本研究中，我们提供了一个理论框架，用于理解通过在相互正交样本上训练的单隐藏层网络从扰动中学习。我们的结果突显了各种对抗性扰动，甚至是几个像素的扰动，均包含足够的类特征用于泛化。此外，我们揭示了从扰动学习时的决策边界

    arXiv:2402.10470v1 Announce Type: new  Abstract: It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations m
    
[^16]: 通过惩罚最优输运网络对表格数据进行生成建模

    Generative Modeling for Tabular Data via Penalized Optimal Transport Network

    [https://arxiv.org/abs/2402.10456](https://arxiv.org/abs/2402.10456)

    提出了一种名为POTNet的生成建模网络，基于边缘惩罚的Wasserstein损失，能够有效地建模同时包含分类和连续特征的表格数据。

    

    准确学习表格数据中行的概率分布并生成真实的合成样本的任务既关键又非平凡。Wasserstein生成对抗网络(WGAN)在生成建模中取得了显著进展，解决了其前身生成对抗网络所面临的挑战。然而，由于表格数据中存在混合数据类型和多模态性，生成器和鉴别器之间的微妙平衡以及Wasserstein距离在高维度中的固有不稳定性，WGAN通常无法生成高保真样本。因此，我们提出了POTNet（惩罚最优输运网络），这是一种基于新颖、强大且可解释的边际惩罚Wasserstein（MPW）损失的生成深度神经网络。POTNet能够有效地建模包含分类和连续特征的表格数据。

    arXiv:2402.10456v1 Announce Type: cross  Abstract: The task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable improvement in generative modeling, addressing the challenges faced by its predecessor, generative adversarial network. However, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of Wasserstein distance in high dimensions, WGAN often fails to produce high-fidelity samples. To this end, we propose POTNet (Penalized Optimal Transport Network), a generative deep neural network based on a novel, robust, and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can effectively model tabular data containing both categorical and continuous features. Moreover, it offers the flexibil
    
[^17]: 使用不同标注函数的协作学习

    Collaborative Learning with Different Labeling Functions

    [https://arxiv.org/abs/2402.10445](https://arxiv.org/abs/2402.10445)

    研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。

    

    我们研究了一种 Collaborative PAC Learning 的变体，在这种情况下，我们旨在学习每个$n$个数据分布的准确分类器，同时最小化从它们总共抽取的样本数量。与通常的协作学习设置不同，不假设存在一个同时对所有分布准确的单一分类器。我们表明，当数据分布满足较弱的可实现性假设时，仍然可以实现高效的学习。我们给出了一种基于经验风险最小化(ERM)的学习算法，应用于假设类的一个自然增强，分析依赖于对该增强类的VC维的上界。

    arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
    
[^18]: 在贝叶斯设置中的固定置信度最佳臂识别问题

    Fixed Confidence Best Arm Identification in the Bayesian Setting

    [https://arxiv.org/abs/2402.10429](https://arxiv.org/abs/2402.10429)

    该研究在贝叶斯设置中探讨了固定置信度最佳臂识别问题，证明了传统频率设定下的算法在此设置下表现次优，并引入了一种性能与理论下限相匹配的连续排除变种。

    

    我们考虑了贝叶斯设置中的固定置信度最佳臂识别（FC-BAI）问题。该问题旨在在已知先验采样的情况下以固定置信水平找到均值最大的臂。大多数关于FC-BAI问题的研究都是在频率设定中进行的，在该设定下，游戏开始前即确定了赌博模型。我们证明了在贝叶斯设置中，传统的在频率设定中研究的FC-BAI算法（如track-and-stop和top-two算法）会导致任意次优的表现。我们同时证明了在贝叶斯设置下预期样本数的下限，并引入了一种连续排除的变种，其性能与下限相匹配，最多差一个对数因子。仿真验证了理论结果。

    arXiv:2402.10429v1 Announce Type: cross  Abstract: We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian Setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the Bayesian setting. We also prove a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.
    
[^19]: 学习性是一种紧凑性质

    Learnability is a Compact Property

    [https://arxiv.org/abs/2402.10360](https://arxiv.org/abs/2402.10360)

    监督学习问题的困难性具有紧凑的有限特性表征。

    

    最近关于学习的工作取得了一个引人注目的结果：各种问题的可学习性可能是不可判定的，或者与标准集合论ZFC公理无关。此外，这种问题的可学习性可能不是具有有限特性的属性：非正式地说，它不能通过检查问题的有限投影来检测。

    arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
    
[^20]: 通过Langevin MCMC在黎曼流形上高效采样

    Efficient Sampling on Riemannian Manifolds via Langevin MCMC

    [https://arxiv.org/abs/2402.10357](https://arxiv.org/abs/2402.10357)

    通过Langevin MCMC在黎曼流形上高效采样，并证明了在特定条件下迭代步数为$\tilde{O}(\epsilon^{-2})$时，Langevin MCMC的迭代会与目标分布在$\epsilon$-Wasserstein距离内。

    

    我们研究了通过（几何）Langevin MCMC在黎曼流形$M$上高效地从Gibbs分布$d \pi^* = e^{-h} d {vol}_g$中采样的任务；该算法涉及在随机高斯方向上计算指数映射，在实践中可以高效实现。我们对Langevin MCMC的分析的关键在于对几何Euler-Murayama方案的离散化误差做出的界限，假设$\nabla h$是Lipschitz的，且$M$具有有界的曲率截面。我们的误差界限与欧几里得Euler-Murayama在步长依赖性方面的误差相匹配。结合Kendall-Cranston耦合下对几何Langevin扩散的收缩保证，我们证明Langevin MCMC迭代在经过$\tilde{O}(\epsilon^{-2})$步后与$\pi^*$之间的$\epsilon$-Wasserstein距离内，这与欧几里得Langevin MCMC的迭代复杂度相匹配。我们的结果适用于一般设置，其中$h$可以是非凸的。

    arXiv:2402.10357v1 Announce Type: cross  Abstract: We study the task of efficiently sampling from a Gibbs distribution $d \pi^* = e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin MCMC; this algorithm involves computing exponential maps in random Gaussian directions and is efficiently implementable in practice. The key to our analysis of Langevin MCMC is a bound on the discretization error of the geometric Euler-Murayama scheme, assuming $\nabla h$ is Lipschitz and $M$ has bounded sectional curvature. Our error bound matches the error of Euclidean Euler-Murayama in terms of its stepsize dependence. Combined with a contraction guarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling, we prove that the Langevin MCMC iterates lie within $\epsilon$-Wasserstein distance of $\pi^*$ after $\tilde{O}(\epsilon^{-2})$ steps, which matches the iteration complexity for Euclidean Langevin MCMC. Our results apply in general settings where $h$ can be nonc
    
[^21]: 数字孪生中的数学机遇（MATH-DT）

    Mathematical Opportunities in Digital Twins (MATH-DT)

    [https://arxiv.org/abs/2402.10326](https://arxiv.org/abs/2402.10326)

    数字孪生中的数学机遇需要基础数学进展，与传统方法不同，数字孪生从特定现实出发，需要多尺度建模和耦合，通过传感器将数据输入，帮助人类做出决策。

    

    这份报告描述了2023年12月11日至13日在乔治梅森大学举办的“数字孪生中的数学机遇”（MATH-DT）研讨会的讨论。报告指出，数字孪生（DT）需要与传统方法不同的基础数学进展。传统模型在生物学、物理学、工程学或医学中起始于通用物理定律（例如方程），通常是对现实的简化。数字孪生则从特定的生态系统、物体或个人（例如个性化医疗）作为现实出发，需要多尺度、多物理建模和耦合。因此，这些过程在模拟和建模流程的两端开始，需要不同的可靠性标准和不确定性评估。此外，与现有方法不同，数字孪生帮助人类为物理系统做出决策，其通过传感器将数据传输到数字孪生中。

    arXiv:2402.10326v1 Announce Type: cross  Abstract: The report describes the discussions from the Workshop on Mathematical Opportunities in Digital Twins (MATH-DT) from December 11-13, 2023, George Mason University.   It illustrates that foundational Mathematical advances are required for Digital Twins (DTs) that are different from traditional approaches. A traditional model, in biology, physics, engineering or medicine, starts with a generic physical law (e.g., equations) and is often a simplification of reality. A DT starts with a specific ecosystem, object or person (e.g., personalized care) representing reality, requiring multi -scale, -physics modeling and coupling. Thus, these processes begin at opposite ends of the simulation and modeling pipeline, requiring different reliability criteria and uncertainty assessments. Additionally, unlike existing approaches, a DT assists humans to make decisions for the physical system, which (via sensors) in turn feeds data into the DT, and oper
    
[^22]: 使用KCUSUM算法评估实时自适应采样变点检测

    An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM

    [https://arxiv.org/abs/2402.10291](https://arxiv.org/abs/2402.10291)

    KCUSUM算法是一种非参数扩展算法，用于在高容量数据情景下实时检测突变变化，相比于现有算法，其能够更灵活地在在线环境中进行变点检测。

    

    从科学模拟数据流中实时检测突变变化是一项具有挑战性的任务，要求部署准确和高效的算法。本研究引入了基于核的累积和（KCUSUM）算法，一种传统累积和（CUSUM）方法的非参数扩展，以其在较少限制条件下在线变点检测方面的有效性而备受关注。

    arXiv:2402.10291v1 Announce Type: new  Abstract: Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic gr
    
[^23]: Thompson Sampling在部分可观察的上下文特征臂老虎机中的应用

    Thompson Sampling in Partially Observable Contextual Bandits

    [https://arxiv.org/abs/2402.10289](https://arxiv.org/abs/2402.10289)

    研究了在部分观察到的上下文特征老虎机中使用Thompson Sampling策略，以学习选择最佳臂的问题

    

    上下文特征臂老虎机构成了一个经典的不确定性决策框架。在这种情况下，目标是在上下文信息的条件下学习具有最高奖励的臂，同时需要通过实验来学习每个臂的未知奖励参数。因此，一个基本问题是在探索（即拉动不同臂以学习它们的参数）和开发（即拉动最佳臂以获得奖励）之间取得平衡。现有文献大多考虑完全观察到的上下文情境。然而，尽管在理论上更一般且在实践中更有多样性，但部分上下文观察情景至今仍未被探索。我们研究了在观察数据是未观察到的上下文向量的噪声线性函数的情况下学习选择最佳臂的老虎机策略。我们的理论分析表明Thompson采样算法...

    arXiv:2402.10289v1 Announce Type: cross  Abstract: Contextual bandits constitute a classical framework for decision-making under uncertainty. In this setting, the goal is to learn the arms of highest reward subject to contextual information, while the unknown reward parameters of each arm need to be learned by experimenting that specific arm. Accordingly, a fundamental problem is that of balancing exploration (i.e., pulling different arms to learn their parameters), versus exploitation (i.e., pulling the best arms to gain reward). To study this problem, the existing literature mostly considers perfectly observed contexts. However, the setting of partial context observations remains unexplored to date, despite being theoretically more general and practically more versatile. We study bandit policies for learning to select optimal arms based on the data of observations, which are noisy linear functions of the unobserved context vectors. Our theoretical analysis shows that the Thompson sam
    
[^24]: 具有中介反馈的赌博机信息容量遗憾界限

    Information Capacity Regret Bounds for Bandits with Mediator Feedback

    [https://arxiv.org/abs/2402.10282](https://arxiv.org/abs/2402.10282)

    该研究提出了一种基于信息容量的新遗憾界限方法，适用于具有中介反馈的赌博机问题，同时在敌对和随机设置中提供了近乎匹配的下界。

    

    这项工作解决了中介反馈问题，即决策集包括多个策略，每个策略与共同结果空间上的概率分布相关联。选择一个策略后，学习者观察从其分布中采样的结果，并在当前回合中承担分配给该结果的损失。我们引入策略集容量作为衡量策略集复杂性的信息论指标。采用经典的EXP4算法，我们提供了新的遗憾界限，取决于策略集容量在敌对和随机设置中的性能。对于一些策略集家族的选择，我们证明了近乎匹配的下界，与容量类似地扩展。我们还考虑了策略分布在回合之间可以变化的情况，从而解决了相关的具有专家建议的赌博机问题，我们在其先前结果上有所改进。此外，我们证明

    arXiv:2402.10282v1 Announce Type: new  Abstract: This work addresses the mediator feedback problem, a bandit game where the decision set consists of a number of policies, each associated with a probability distribution over a common space of outcomes. Upon choosing a policy, the learner observes an outcome sampled from its distribution and incurs the loss assigned to this outcome in the present round. We introduce the policy set capacity as an information-theoretic measure for the complexity of the policy set. Adopting the classical EXP4 algorithm, we provide new regret bounds depending on the policy set capacity in both the adversarial and the stochastic settings. For a selection of policy set families, we prove nearly-matching lower bounds, scaling similarly with the capacity. We also consider the case when the policies' distributions can vary between rounds, thus addressing the related bandits with expert advice problem, which we improve upon its prior results. Additionally, we prov
    
[^25]: 具有无界和退化噪声的线性系统在线控制

    Online Control of Linear Systems with Unbounded and Degenerate Noise

    [https://arxiv.org/abs/2402.10252](https://arxiv.org/abs/2402.10252)

    这项研究揭示了在线控制问题中，对于凸成本，可以实现 $ \widetilde{O}(\sqrt{T}) $ 的遗憾界，甚至在存在无界噪声的情况下；同时，在成本具有强凸性时，可以在不需要噪声协方差是非退化的情况下建立 $ O({\rm poly} (\log T)) $ 的遗憾界。

    

    本文研究了在可能存在无界和退化噪声的情况下控制线性系统的问题，其中成本函数未知，被称为在线控制问题。与现有的仅假设噪声有界性的研究不同，我们揭示了对于凸成本，即使在存在无界噪声的情况下也可以实现 $ \widetilde{O}(\sqrt{T}) $ 的遗憾界，其中 $ T $ 表示时间跨度。此外，当成本具有强凸性时，我们建立了一个 $ O({\rm poly} (\log T)) $ 的遗憾界，而不需要噪声协方差是非退化的假设，这在文献中是必需的。消除噪声秩的关键是与噪声协方差相关联的系统转化。这同时实现了在线控制算法的参数减少。

    arXiv:2402.10252v1 Announce Type: cross  Abstract: This paper investigates the problem of controlling a linear system under possibly unbounded and degenerate noise with unknown cost functions, known as an online control problem. In contrast to the existing work, which assumes the boundedness of noise, we reveal that for convex costs, an $ \widetilde{O}(\sqrt{T}) $ regret bound can be achieved even for unbounded noise, where $ T $ denotes the time horizon. Moreover, when the costs are strongly convex, we establish an $ O({\rm poly} (\log T)) $ regret bound without the assumption that noise covariance is non-degenerate, which has been required in the literature. The key ingredient in removing the rank assumption on noise is a system transformation associated with the noise covariance. This simultaneously enables the parameter reduction of an online control algorithm.
    
[^26]: Johnson-Lindenstrauss的简单统一分析及其应用

    Simple, unified analysis of Johnson-Lindenstrauss with applications

    [https://arxiv.org/abs/2402.10232](https://arxiv.org/abs/2402.10232)

    这项工作提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，简化和统一了各种构造，包括球形、高斯、二进制硬币和次高斯模型，通过创新性地将Hanson-Wright不等式拓展到高维度，标志着对数据固有几何的保持取得重大进展。

    

    在这项工作中，我们提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，这是处理高维数据至关重要的降维领域中的基石。我们的方法不仅简化了理解，还将各种构造统一到JL框架下，包括球形、高斯、二进制硬币和次高斯模型。这种简化和统一在保持数据固有几何的重要性方面取得了重大进展，对从流算法到强化学习等各种应用至关重要。值得注意的是，我们在这个简化框架内提出了球形构造有效性的第一个严格证明。我们贡献的核心是将Hanson-Wright不等式拓展到高维度，具有明确的常数，这标志着文献中质的飞跃。通过运用简单而强大的概率工具

    arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
    
[^27]: HyperAgent：一种简单、可扩展、高效且可证明用于复杂环境的强化学习框架

    HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments

    [https://arxiv.org/abs/2402.10228](https://arxiv.org/abs/2402.10228)

    HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    

    为了在资源约束下解决复杂任务，强化学习（RL）代理需要简单、高效、可扩展、具有大状态空间和不断积累的交互数据。我们提出了HyperAgent，这是一个具有超模型、索引抽样方案和增量更新机制的RL框架，可以在一般价值函数逼近中进行计算高效的顺序后验逼近和数据高效的动作选择，超越了共轭性。HyperAgent的实现简单，只需要在DDQN中添加一个模块和一行额外代码。在实践中，HyperAgent在大规模深度RL基准测试中表现出稳健的性能，无论是在数据还是计算方面都获得了显着的效率提升。在理论上，在实际可扩展的算法中，HyperAgent是第一个能够实现可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
    
[^28]: 相关拉格朗日薛定谔桥：通过人口级正则化学习动力学

    Correlational Lagrangian Schr\"odinger Bridge: Learning Dynamics with Population-Level Regularization

    [https://arxiv.org/abs/2402.10227](https://arxiv.org/abs/2402.10227)

    提出了一个名为相关拉格朗日薛定谔桥（CLSB）的新框架，通过人口级正则化来学习跨截面样本中的系统动态，适应个体粒子行为的异质性。

    

    系统动态的准确建模在包括细胞动力学和流体力学在内的广泛科学领域中具有引人注目的潜力。然而，当（i）观察仅限于横截面样本（个体轨迹不可学习）时，以及（ii）个体粒子行为异质时（尤其是由于生物多样性而为生物系统）。为了解决这些问题，我们引入了一个名为相关拉格朗日薛定谔桥（CLSB）的新框架，旨在寻求在横截观察之间的演变“桥梁”，同时以最小人口“成本”进行正则化。与先前依赖\textit{个体}级正则化器的方法形成对比（例如，限制个体运动），CLSB在人口水平运行，接受异质性本质，从而产生更具泛化性的模式。

    arXiv:2402.10227v1 Announce Type: new  Abstract: Accurate modeling of system dynamics holds intriguing potential in broad scientific fields including cytodynamics and fluid mechanics. This task often presents significant challenges when (i) observations are limited to cross-sectional samples (where individual trajectories are inaccessible for learning), and moreover, (ii) the behaviors of individual particles are heterogeneous (especially in biological systems due to biodiversity). To address them, we introduce a novel framework dubbed correlational Lagrangian Schr\"odinger bridge (CLSB), aiming to seek for the evolution "bridging" among cross-sectional observations, while regularized for the minimal population "cost". In contrast to prior methods relying on \textit{individual}-level regularizers for all particles \textit{homogeneously} (e.g. restraining individual motions), CLSB operates at the population level admitting the heterogeneity nature, resulting in a more generalizable mode
    
[^29]: GPT-4在基于USMLE的案例研究中的表现评估

    GPT-4's assessment of its performance in a USMLE-based case study

    [https://arxiv.org/abs/2402.09654](https://arxiv.org/abs/2402.09654)

    本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。

    

    本研究调查GPT-4在医疗应用中的表现评估。通过使用简单的提示技术，从美国医学执照考试（USMLE）问卷中提取问题的方式，任务是评估模型在提问之前和提问之后的置信度得分。问卷根据是否有反馈分为两组：反馈组（WF）和无反馈组（NF）。要求模型在每个问题之前和之后提供绝对和相对置信度得分。通过使用统计工具分析实验结果，研究了WF和NF组的置信度变异性。此外，进行了顺序分析以观察WF和NF组的性能变化。结果表明，反馈会影响相对置信度，但并不总是增加或减少。

    arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
    
[^30]: 可解释的多源数据融合通过潜变量高斯过程

    Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process

    [https://arxiv.org/abs/2402.04146](https://arxiv.org/abs/2402.04146)

    这篇论文提出了一种基于潜变量高斯过程的多源数据融合框架，用于解决多个数据源之间质量和全面性差异给系统优化带来的问题。

    

    随着人工智能（AI）和机器学习（ML）的出现，各个科学和工程领域已经利用数据驱动的替代模型来建模来自大量信息源（数据）的复杂系统。这种增加导致了开发出用于执行特定功能的优越系统所需的成本和时间的显著降低。这样的替代模型往往广泛地融合多个数据来源，可能是发表的论文、专利、开放资源库或其他资源。然而，对于已知和未知的信息来源的基础物理参数的质量和全面性的差异，可能对系统优化过程产生后续影响，却没有得到充分的关注。为了解决这个问题，提出了一种基于潜变量高斯过程（LVGP）的多源数据融合框架。

    With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
    
[^31]: 来自协方差核的高斯过程样本路径正则性

    Sample Path Regularity of Gaussian Processes from the Covariance Kernel

    [https://arxiv.org/abs/2312.14886](https://arxiv.org/abs/2312.14886)

    本文提供了关于高斯过程样本路径正则性的新颖和紧凑的特征描述，通过协方差核对应的GP样本路径达到一定正则性的充分必要条件，对常用于机器学习应用中的GPs的样本路径正则性进行了探讨。

    

    高斯过程（GPs）是定义函数空间上的概率分布的最常见形式主义。尽管GPs的应用广泛，但对于GP样本路径的全面理解，即它们定义概率测度的函数空间，尚缺乏。在实践中，GPs不是通过概率测度构建的，而是通过均值函数和协方差核构建的。本文针对协方差核提供了GP样本路径达到给定正则性所需的充分必要条件。我们使用H\"older正则性框架，因为它提供了特别简单的条件，在平稳和各向同性GPs的情况下进一步简化。然后，我们证明我们的结果允许对机器学习应用中常用的GPs的样本路径正则性进行新颖且异常紧凑的表征。

    arXiv:2312.14886v2 Announce Type: replace  Abstract: Gaussian processes (GPs) are the most common formalism for defining probability distributions over spaces of functions. While applications of GPs are myriad, a comprehensive understanding of GP sample paths, i.e. the function spaces over which they define a probability measure, is lacking. In practice, GPs are not constructed through a probability measure, but instead through a mean function and a covariance kernel. In this paper we provide necessary and sufficient conditions on the covariance kernel for the sample paths of the corresponding GP to attain a given regularity. We use the framework of H\"older regularity as it grants particularly straightforward conditions, which simplify further in the cases of stationary and isotropic GPs. We then demonstrate that our results allow for novel and unusually tight characterisations of the sample path regularities of the GPs commonly used in machine learning applications, such as the Mat\'
    
[^32]: 对分布最小最大问题的对称均场 Langevin 动力学

    Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems

    [https://arxiv.org/abs/2312.01127](https://arxiv.org/abs/2312.01127)

    首次将均场 Langevin 动力学扩展到概率分布上的最小最大优化，提出了 MFL-AG 和 MFL-ABR 两种算法，实现了对混合 Nash 平衡的收敛，还研究了时间和粒子离散化制度以及传播混沌结果。

    

    在本文中，我们首次将均场 Langevin 动力学扩展到概率分布上的最小最大优化，使用对称且经过证明收敛的更新。我们提出了均场 Langevin 平均梯度（MFL-AG），这是一种单循环算法，通过新颖的加权平均，在分布空间中实现梯度下降上升，并确立了对混合 Nash 平衡的平均迭代收敛。我们还研究了时间和粒子离散化制度，并证明了一个新的时间均匀传播混沌结果，该结果考虑了粒子相互作用对所有先前分布的依赖性。此外，我们提出了均场 Langevin 锚定最佳响应（MFL-ABR），这是一种基于最佳响应动态的对称双循环算法，具有线性的最迭代收敛。最后，我们研究了零和马尔可夫博弈的应用，并进行了模拟，展示了长期最优性。

    arXiv:2312.01127v2 Announce Type: replace-cross  Abstract: In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose mean-field Langevin averaged gradient (MFL-AG), a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging, and establish average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Furthermore, we propose mean-field Langevin anchored best response (MFL-ABR), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. Finally, we study applications to zero-sum Markov games and conduct simulations demonstrating long-term optimalit
    
[^33]: 可解释的多视图学习深度学习方法

    Interpretable Deep Learning Methods for Multiview Learning

    [https://arxiv.org/abs/2302.07930](https://arxiv.org/abs/2302.07930)

    提出iDeepViewLearn（可解释的多视图学习深度学习方法）用于多视图数据的非线性关系学习和特征选择，融合深度学习的灵活性和统计优势，给出可解释结果。

    

    技术进步已经实现了生成独特且互补类型的数据或视图（例如基因组学、蛋白质组学、代谢组学），并开创了多视图学习研究新时代，有潜力带来新的生物医学发现。我们提出 iDeepViewLearn（Interpretable Deep Learning Method for Multiview Learning），用于学习来自多视图数据中的非线性关系，同时实现特征选择。iDeepViewLearn结合了深度学习的灵活性与数据和基于知识的特征选择的统计优势，以给出可解释的结果。深度神经网络用于通过最小化观察数据与重构数据之间的差异，并对重构数据施加正则化惩罚来学习视图独立的低维嵌入。图的归一化拉普拉斯用于建模变量之间的双边关系。

    arXiv:2302.07930v2 Announce Type: replace  Abstract: Technological advances have enabled the generation of unique and complementary types of data or views (e.g. genomics, proteomics, metabolomics) and opened up a new era in multiview learning research with the potential to lead to new biomedical discoveries. We propose iDeepViewLearn (Interpretable Deep Learning Method for Multiview Learning) for learning nonlinear relationships in data from multiple views while achieving feature selection. iDeepViewLearn combines deep learning flexibility with the statistical benefits of data and knowledge-driven feature selection, giving interpretable results. Deep neural networks are used to learn view-independent low-dimensional embedding through an optimization problem that minimizes the difference between observed and reconstructed data, while imposing a regularization penalty on the reconstructed data. The normalized Laplacian of a graph is used to model bilateral relationships between variables
    
[^34]: 最优扩展邻域规则$k$最近邻集成

    Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble

    [https://arxiv.org/abs/2211.11278](https://arxiv.org/abs/2211.11278)

    提出了一种基于最优扩展邻域规则的集成方法，通过新规则确定邻居和模型选择策略来解决传统$k$最近邻方法的局限性和提升集成性能。

    

    传统的$k$最近邻($k$NN)方法使用一个球形区域内的距离公式来确定训练观测中与测试样本点最接近的$k$个观测。然而，当测试点位于该区域之外时，这种方法可能不起作用。此外，聚合许多基础$k$NN学习器可能会导致由于高分类误差而表现不佳的集成性能。为解决这些问题，本文提出了一种新的基于最优扩展邻域规则的集成方法。该规则从距离未见观测最近的样本点开始，经过$k$步确定邻居，并选择直到达到所需数量的观测数据点。每个基础模型都是在一个随机特征子集上的自举样本上构建的，并且在构建足够数量的模型后基于袋外表现选择最优模型。提出的集成方法与st进行了比较

    arXiv:2211.11278v2 Announce Type: replace-cross  Abstract: The traditional k nearest neighbor (kNN) approach uses a distance formula within a spherical region to determine the k closest training observations to a test sample point. However, this approach may not work well when test point is located outside this region. Moreover, aggregating many base kNN learners can result in poor ensemble performance due to high classification errors. To address these issues, a new optimal extended neighborhood rule based ensemble method is proposed in this paper. This rule determines neighbors in k steps starting from the closest sample point to the unseen observation and selecting subsequent nearest data points until the required number of observations is reached. Each base model is constructed on a bootstrap sample with a random subset of features, and optimal models are selected based on out-of-bag performance after building a sufficient number of models. The proposed ensemble is compared with st
    
[^35]: 条件矩限制的功能化广义经验似然估计

    Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions

    [https://arxiv.org/abs/2207.04771](https://arxiv.org/abs/2207.04771)

    提供了条件矩限制问题的功能化广义经验似然估计方法，并探索了其在小样本性能上的优势，同时提供了基于核和神经网络的实现方式。

    

    在因果推断、经济学习以及更一般的鲁棒机器学习中，重要问题可以被表达为条件矩限制，但由于需要解决一系列的无条件矩限制，估计变得具有挑战性。先前的工作通过将广义矩法（GMM）扩展到连续矩限制来解决这个问题。与此相反，广义经验似然（GEL）提供了一个更一般的框架，并且已经被证明在小样本性能上优于基于GMM的估计量。为了从最近的机器学习发展中受益，我们提供了GEL的功能重构，其中可以利用任意模型。受结果的无限维优化问题的对偶形式启发，我们设计了一个实用方法并探讨了其渐近性质。最后，我们提供了基于核和神经网络的实现方式。

    arXiv:2207.04771v2 Announce Type: replace  Abstract: Important problems in causal inference, economics, and, more generally, robust machine learning can be expressed as conditional moment restrictions, but estimation becomes challenging as it requires solving a continuum of unconditional moment restrictions. Previous works addressed this problem by extending the generalized method of moments (GMM) to continuum moment restrictions. In contrast, generalized empirical likelihood (GEL) provides a more general framework and has been shown to enjoy favorable small-sample properties compared to GMM-based estimators. To benefit from recent developments in machine learning, we provide a functional reformulation of GEL in which arbitrary models can be leveraged. Motivated by a dual formulation of the resulting infinite dimensional optimization problem, we devise a practical method and explore its asymptotic properties. Finally, we provide kernel- and neural network-based implementations of the e
    
[^36]: 因果评分：效应估计、效应排序和效应分类的框架

    Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification

    [https://arxiv.org/abs/2206.12532](https://arxiv.org/abs/2206.12532)

    本文引入了因果评分作为一种新型方法，支持决策制定，提供洞察力，并可用于效应估计、效应排序和效应分类。

    

    本文将因果评分引入到决策制定的背景中作为一种新颖方法，涉及估计支持决策制定的得分，从而提供因果效应的洞察力。我们提出了这些评分的三种有价值的因果解释：效应估计（EE）、效应排序（EO）和效应分类（EC）。在EE解释中，因果评分代表了效应本身。EO解释暗示评分可以作为效应大小的代理，可以根据其因果效应对个体进行排序。EC解释通过预定义的阈值，使个体分为高效应和低效应类别。我们通过两个关键结果展示了这些替代因果解释（EO和EC）的价值。

    arXiv:2206.12532v4 Announce Type: replace-cross  Abstract: This paper introduces causal scoring as a novel approach to frame causal estimation in the context of decision making. Causal scoring entails the estimation of scores that support decision making by providing insights into causal effects. We present three valuable causal interpretations of these scores: effect estimation (EE), effect ordering (EO), and effect classification (EC). In the EE interpretation, the causal score represents the effect itself. The EO interpretation implies that the score can serve as a proxy for the magnitude of the effect, enabling the sorting of individuals based on their causal effects. The EC interpretation enables the classification of individuals into high- and low-effect categories using a predefined threshold. We demonstrate the value of these alternative causal interpretations (EO and EC) through two key results. First, we show that aligning the statistical modeling with the desired causal inte
    
[^37]: 用离开的赌博机模型建议系统中的流失现象

    Modeling Attrition in Recommender Systems with Departing Bandits

    [https://arxiv.org/abs/2203.13423](https://arxiv.org/abs/2203.13423)

    本论文提出了一个新型多臂赌博机设置，捕捉了推荐系统中用户离开的情况，首次证明了在所有用户共享相同类型时，基于UCB的算法是最优的。

    

    在传统的多臂赌博机中，推荐系统的策略影响奖励的获取，但不影响交互的长度。然而，在现实世界中，不满足的用户可能会离开（并永远不再回来）。在这项工作中，我们提出了一个捕捉这种策略依赖性时段的新型多臂赌博机设置。我们的设置包括一个有限的用户类型集合，和多个具有伯努利回报的臂。每个（用户类型，臂）元组对应一个（未知的）奖励概率。每个用户的类型最初是未知的，只能通过其对推荐的响应来推断。此外，如果用户对他们的推荐不满意，他们可能会离开系统。我们首先解决了所有用户共享相同类型的情况，证明了最近基于UCB的算法的最优性。然后，我们转向更具挑战性的情况，即用户分为两类。

    arXiv:2203.13423v2 Announce Type: replace  Abstract: Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisfied users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a finite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user's type is initially unknown and can only be inferred through their response to recommendations. Moreover, if a user is dissatisfied with their recommendation, they might depart the system. We first address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two 
    
[^38]: 高效收缩路径：最小均方误差风险的最大似然

    The Efficient Shrinkage Path: Maximum Likelihood of Minimum MSE Risk

    [https://arxiv.org/abs/2103.05161](https://arxiv.org/abs/2103.05161)

    提出了新的广义岭回归收缩路径，通过最小均方误差风险的最大似然实现最佳方差-偏差权衡，提供了宝贵的数据分析见解。

    

    提出了一种新的广义岭回归收缩路径，该路径是在通过使总体在正态分布理论下实现最佳方差-偏差权衡的回归系数估计向量的限制条件下尽可能短。激励和展示了五种不同类型的岭路径轨迹显示以及其他图形。这些可视化提供了宝贵的数据分析见解，并提高了对将线性模型拟合到病态（混淆）数据的研究人员和数据科学家的自信心。

    arXiv:2103.05161v5 Announce Type: replace-cross  Abstract: A new generalized ridge regression shrinkage path is proposed that is as short as possible under the restriction that it must pass through the vector of regression coefficient estimators that make the overall Optimal Variance-Bias Trade-Off under Normal distribution-theory. Five distinct types of ridge TRACE displays plus other graphics for this efficient path are motivated and illustrated here. These visualizations provide invaluable data-analytic insights and improved self-confidence to researchers and data scientists fitting linear models to ill-conditioned (confounded) data.
    
[^39]: 针对具有非欧几里德预测变量的度量空间回归问题的Fréchet随机森林

    Fr\'echet random forests for metric space valued regression with non euclidean predictors

    [https://arxiv.org/abs/1906.01741](https://arxiv.org/abs/1906.01741)

    本文介绍了Fréchet随机森林，允许处理值在一般度量空间的数据，并引入了新的树节点分裂方式，扩展了预测过程，提出了一致性定理，并适用于数据驱动分区的Fréchet纯一致随机树

    

    随机森林是一种广泛应用于科学研究领域的统计学习方法，因为它能够学习输入和输出变量之间复杂的关系，同时也能处理高维数据。然而，目前的随机森林方法对处理曲线、图像和形状等异质数据的灵活性不够。本文介绍了Fréchet树和Fréchet随机森林，允许处理输入和输出变量取值在一般度量空间中的数据。为此，引入了一种新的树节点分裂方式，并广义化了树和森林的预测过程。随机森林的袋外误差和变量重要性得分自然得到了调整。给出了使用数据驱动分区的Fréchet回归图预测器的一致性定理，并应用于Fréchet纯一致随机树。该方法

    arXiv:1906.01741v3 Announce Type: replace-cross  Abstract: Random forests are a statistical learning method widely used in many areas of scientific research because of its ability to learn complex relationships between input and output variables and also its capacity to handle high-dimensional data. However, current random forest approaches are not flexible enough to handle heterogeneous data such as curves, images and shapes. In this paper, we introduce Fr\'echet trees and Fr\'echet random forests, which allow to handle data for which input and output variables take values in general metric spaces. To this end, a new way of splitting the nodes of trees is introduced and the prediction procedures of trees and forests are generalized. Then, random forests out-of-bag error and variable importance score are naturally adapted. A consistency theorem for Fr\'echet regressogram predictor using data-driven partitions is given and applied to Fr\'echet purely uniformly random trees. The method i
    
[^40]: 从分组数据中稳健估计Pareto的尺度参数

    Robust Estimation of Pareto's Scale Parameter from Grouped Data. (arXiv:2401.14593v1 [stat.ME])

    [http://arxiv.org/abs/2401.14593](http://arxiv.org/abs/2401.14593)

    本文介绍了一种新的稳健估计方法（MTuM），用于从分组数据中估计Pareto分布的尾指数。该方法通过应用中心极限定理和模拟研究验证了其推理合理性。

    

    当可获取的完全观测到的从头至尾的损失严重性样本数据集存在时，存在许多稳健估计器作为最大似然估计器（MLE）的替代方案。然而，当处理分组损失严重性数据时，稳健的MLE替代方案的选择变得非常有限，只有少数方法可用，例如最小二乘法、最小Hellinger距离和最优有界影响函数。本文介绍了一种称为截断矩法的新型稳健估计技术，该方法专门用于从分组数据估计Pareto分布的尾指数。通过应用中心极限定理和通过全面的模拟研究验证了MTuM的推理合理性。

    Numerous robust estimators exist as alternatives to the maximum likelihood estimator (MLE) when a completely observed ground-up loss severity sample dataset is available. However, the options for robust alternatives to MLE become significantly limited when dealing with grouped loss severity data, with only a handful of methods like least squares, minimum Hellinger distance, and optimal bounded influence function available. This paper introduces a novel robust estimation technique, the Method of Truncated Moments (MTuM), specifically designed to estimate the tail index of a Pareto distribution from grouped data. Inferential justification of MTuM is established by employing the central limit theorem and validating them through a comprehensive simulation study.
    
[^41]: 无需增强的简单非对称图对比学习

    Simple and Asymmetric Graph Contrastive Learning without Augmentations. (arXiv:2310.18884v1 [cs.LG])

    [http://arxiv.org/abs/2310.18884](http://arxiv.org/abs/2310.18884)

    本文提出了一种无需增强的简单非对称图对比学习方法GraphACL，通过考虑邻居节点的非对称视图，该方法能够有效地在同类和异类图上进行对比学习，对于建模异类图非常重要。

    

    图对比学习（GCL）在图结构数据的表示学习中显示出了优越的性能。尽管取得了成功，但大多数现有的GCL方法依赖于预制的图增强和同类假设。因此，它们在连通节点可能具有不同类标签和不相似特征的异类图上无法很好地推广。在本文中，我们研究了在同类和异类图上进行对比学习的问题。我们发现，通过考虑邻居节点的非对称视图，我们可以实现有希望的性能。由此产生的简单算法，称为图的非对称对比学习(GraphACL)，易于实现，不依赖于图增强和同类假设。我们提供了理论和实证证据，证明GraphACL能够捕捉单跳本地邻域信息和双跳单一相似性，这两者对于建模异类图非常重要。

    Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results s
    
[^42]: 关于生成模型在其自己的数据上迭代训练的稳定性研究

    On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])

    [http://arxiv.org/abs/2310.00429](http://arxiv.org/abs/2310.00429)

    本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。

    

    深度生成模型在建模复杂数据方面取得了巨大的进展，往往展现出超过典型人类能力的样本真实性辨别能力。这一成功的关键驱动力无疑是这些模型消耗海量网络规模数据的结果。由于这些模型惊人的性能和易得性，网络上将不可避免地出现越来越多的合成内容。这个事实直接意味着生成模型的未来迭代必须面对一个现实：它们的训练数据由清洁数据和先前模型生成的人工数据组成。在本文中，我们开发了一个框架来对混合数据集（包括真实数据和合成数据）上训练生成模型对稳定性的影响进行严格研究。我们首先证明了在初始生成模型足够好地近似数据分布并且真实数据与合成数据的比例适当的情况下，迭代训练的稳定性。

    Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
    
[^43]: 三维切片Wasserstein的准蒙特卡洛方法

    Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])

    [http://arxiv.org/abs/2309.11713](http://arxiv.org/abs/2309.11713)

    本文提出了准蒙特卡洛（QMC）方法用于三维切片Wasserstein（SW）的近似计算，并通过多种方法在三维单位超球面上构造了QMC点集。此外，还介绍了将QSW扩展为随机准切片Wasserstein（RQSW）的方法。

    

    Monte Carlo (MC)方法被用作计算切片Wasserstein (SW)距离的标准方法，因为它在分析形式中具有棘手的期望。然而，MC方法在最小化绝对近似误差方面并不优化。为了提供更好的经验SW类，我们提出了基于准蒙特卡洛（QMC）方法的准切片Wasserstein（QSW）逼近。为了对SW的QMC进行全面的研究，我们专注于三维设置，特别是计算三维概率测度之间的SW。具体而言，我们通过实证验证了在三维单位超球面上构造QMC点集的多种方法，包括基于高斯的映射，等面积映射，广义螺旋点和最优化差异能量。此外，为了获得随机优化的无偏估计，我们通过在所讨论的低维设置中引入随机性，将QSW扩展为随机准切片Wasserstein（RQSW）。

    Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-d
    
[^44]: 政策梯度算法收敛于几乎线性二次型调节器的全局最优策略

    Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])

    [http://arxiv.org/abs/2303.08431](http://arxiv.org/abs/2303.08431)

    本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。

    

    决策者只获得了非完整信息的非线性控制系统在各种应用中普遍存在。本研究探索了强化学习方法，以找到几乎线性二次型调节器系统中最优策略。我们考虑一个动态系统，结合线性和非线性组成部分，并由相同结构的策略进行管理。在假设非线性组成部分包含具有小型Lipschitz系数的内核的情况下，我们对成本函数的优化进行了表征。虽然成本函数通常是非凸的，但我们确立了全局最优解附近局部的强凸性和光滑性。此外，我们提出了一种初始化机制，以利用这些属性。在此基础上，我们设计了一个策略梯度算法，可以保证以线性速率收敛于全局最优解。

    Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
    
[^45]: 不同的好手臂识别

    Differential Good Arm Identification. (arXiv:2303.07154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07154](http://arxiv.org/abs/2303.07154)

    本文提出了DGAI算法，它可以在好手臂识别问题中通过深度学习的方式减少样本复杂性，并且在具有给定阈值的情况下进一步提高多臂赌博问题的性能。

    

    本文针对一种变体的随机多臂赌博问题，称之为好手臂识别（GAI）。 GAI是一个纯探索的赌博问题，其目标是在尽可能少的样本数下输出尽可能多的好手臂，其中好手臂被定义为其期望奖励大于给定阈值的手臂。 在这项工作中，我们提出DGAI-一种可微的好手臂识别算法，以数据驱动方式改进了现有技术HDoC算法的样本复杂性。 我们还展示了DGAI可以进一步提升通用多臂赌博（MAB）问题的性能，给定一个阈值作为先验知识应用于手臂集。 大量实验证实，我们的算法在合成数据集和真实世界数据集中的GAI和MAB任务中显著优于基线算法。

    This paper targets a variant of the stochastic multi-armed bandit problem called good arm identification (GAI). GAI is a pure-exploration bandit problem with the goal to output as many good arms using as few samples as possible, where a good arm is defined as an arm whose expected reward is greater than a given threshold. In this work, we propose DGAI - a differentiable good arm identification algorithm to improve the sample complexity of the state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that the DGAI can further boost the performance of a general multi-arm bandit (MAB) problem given a threshold as a prior knowledge to the arm set. Extensive experiments confirm that our algorithm outperform the baseline algorithms significantly in both synthetic and real world datasets for both GAI and MAB tasks.
    
[^46]: 基于JKO方案的可逆归一化流神经网络

    Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14424](http://arxiv.org/abs/2212.14424)

    本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。

    

    归一化流是一类用于高效采样和密度估计的深度生成模型。实际中，流通常表示为一系列可逆的神经网络模块链; 为了便于训练，现有的工作对流轨迹进行了正则化，并设计了特殊的网络架构。本文提出了受Jordan-Kinderleherer-Otto (JKO)方案启发的神经ODE流网络，它允许有效地按块进行残差块的训练，无需采样SDE轨迹或分数匹配或变分学习的内循环。由于JKO方案展开了梯度流的动态，所提出的模型自然地逐个堆叠残差网络块，降低了内存负载和进行端到端深度流网络训练的难度。我们还开发了自适应时间重新参数化的流网络，通过在概率空间中逐步细化轨迹，提高了模型的训练效率和准确性。

    Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
    

