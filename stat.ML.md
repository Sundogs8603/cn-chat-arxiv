# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit.](http://arxiv.org/abs/2309.16620) | 这项研究通过残差分支尺度和$\mu$P参数化的残差网络，实现了深度学习中超参数的跨宽度和深度的转移。 |
| [^2] | [Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance.](http://arxiv.org/abs/2309.16604) | 该论文介绍了一种用于比较具有节点和边特征的图的扩展Gromov-Wasserstein距离的方法，并提出了新的算法来计算距离和重心。通过实验证明了该方法在图学习任务中的有效性。 |
| [^3] | [Cross-Prediction-Powered Inference.](http://arxiv.org/abs/2309.16598) | 本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。 |
| [^4] | [Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces.](http://arxiv.org/abs/2309.16597) | 本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。 |
| [^5] | [M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning.](http://arxiv.org/abs/2309.16578) | M-OFDFT是一种利用深度学习模型解决分子系统问题的OFDFT方法，通过将非局域性建立在模型中并使用紧凑的密度表示，实现了与Kohn-Sham DFT相近的精确度，并且具有良好的外推能力。 |
| [^6] | [CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption.](http://arxiv.org/abs/2309.16563) | 本文研究了具有任意破坏的多臂赌徒问题，并建立了一个与问题相关的遗憾下界。我们提出了CRIMED算法，该算法在具有已知方差的高斯分布赌徒问题上实现了遗憾下界。 |
| [^7] | [Unsupervised Fact Verification by Language Model Distillation.](http://arxiv.org/abs/2309.16540) | 本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。 |
| [^8] | [Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models.](http://arxiv.org/abs/2309.16521) | 本论文提出了一种使用深度生成时间序列模型和决策理论相结合的新框架，用于生成个性化的胰岛素治疗策略。通过学习生成逼真的个性化治疗和未来结果轨迹，可以为个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。 |
| [^9] | [From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity.](http://arxiv.org/abs/2309.16512) | 本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。 |
| [^10] | [Asset Bundling for Wind Power Forecasting.](http://arxiv.org/abs/2309.16492) | 本研究提出了一种资产捆绑-预测-调整（BPR）框架，将资产捆绑、机器学习和预测协调技术相结合，以实现对风电功率的准确预测和一致性调整。 |
| [^11] | [High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality.](http://arxiv.org/abs/2309.16476) | 本文研究了在高维度和重尾干扰条件下的鲁棒回归估计器的性质，通过研究一类椭圆协变量和噪声数据分布的M-估计器，我们发现在存在重尾噪声的情况下，Huber损失需要进一步正则化才能达到最优性能。同时，我们还推导出了岭回归的超额风险的衰减速率。 |
| [^12] | [A parsimonious, computationally efficient machine learning method for spatial regression.](http://arxiv.org/abs/2309.16448) | MPRS是一种计算高效、具有物理启发的空间回归机器学习方法，通过使用距离相关的“相互作用”来引入空间或时间相关性，能够处理任意空间维度的分散数据。在各种合成和真实世界的测试中，MPRS展现了与标准插值方法相当的预测性能，特别适用于填补粗糙和非高斯数据的缺口。 |
| [^13] | [Selective Nonparametric Regression via Testing.](http://arxiv.org/abs/2309.16412) | 通过检验给定条件方差的假设，我们开发了一种选择性非参数回归方法，允许考虑方差本身的值以及对应方差预测器的不确定性，并证明了估计器的风险的非渐近界。 |
| [^14] | [Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption.](http://arxiv.org/abs/2309.16409) | 本文提出了一种在没有均值互换性假设的情况下构建合成治疗组的方法，通过对源群体的治疗组加权混合来构建目标人群的合成治疗组，并通过最小化条件最大均值差异来估计权重。该方法在均值互换性假设被违反时可以作为一种新颖的补充方法。 |
| [^15] | [A Primer on Bayesian Neural Networks: Review and Debates.](http://arxiv.org/abs/2309.16314) | 本论文综述介绍了贝叶斯神经网络（BNNs）的基本概念和其在解决神经网络局限性方面的重要性。目标读者包括具备贝叶斯方法背景但缺乏深度学习专业知识的统计学家，以及深度神经网络专业但对贝叶斯统计学有限了解的机器学习专家。 |
| [^16] | [A framework for paired-sample hypothesis testing for high-dimensional data.](http://arxiv.org/abs/2309.16274) | 本文提出了一个针对高维数据配对样本的假设检验框架，通过垂直平分线生成评分函数，并利用伪中位数求得最优评分函数。 |
| [^17] | [Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints.](http://arxiv.org/abs/2309.16240) | 本论文提出了一种通过引入多样差异约束推广直接偏好优化（DPO）的方法，该方法消除了对估计方法的需要并简化了奖励和最优策略之间的复杂关系。 |
| [^18] | [Stackelberg Batch Policy Learning.](http://arxiv.org/abs/2309.16188) | Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。 |
| [^19] | [Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples.](http://arxiv.org/abs/2309.16143) | 本文提出了一种基于生成基础模型生成的合成样本进行半监督学习的方法，旨在解决实际应用中无法获取大规模无标签数据集的问题。 |
| [^20] | [Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics.](http://arxiv.org/abs/2309.16109) | 本论文研究了非对比学习中的动力崩溃问题，发现特征归一化可以防止此问题的出现，为解决自监督表示学习的计算效率提供了新的思路。 |
| [^21] | [Nonparametric estimation of a covariate-adjusted counterfactual treatment regimen response curve.](http://arxiv.org/abs/2309.16099) | 该论文提出了一种非参数估计方法，用于对协变量调整的反事实治疗方案响应曲线进行估计，通过提出反概率加权的估计器来平滑曲线函数，并给出了置信区间和收敛性证明。 |
| [^22] | [Improving Adaptive Online Learning Using Refined Discretization.](http://arxiv.org/abs/2309.16044) | 通过一种新颖的连续时间启发式算法，提高了自适应在线学习的效果，将梯度方差的依赖性从次优的$O(\sqrt{V_T\log V_T})$改进到最优速率$O(\sqrt{V_T})$，并可适用于未知Lipschitz常数的情况。 |
| [^23] | [GLM Regression with Oblivious Corruptions.](http://arxiv.org/abs/2309.11657) | 这篇论文介绍了第一个在广义线性模型回归问题中处理加法无意识噪声的算法。算法的目标是通过样本访问来准确地恢复参数向量，使得模型的预测与真实值的误差尽可能小。 |
| [^24] | [Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples.](http://arxiv.org/abs/2309.03847) | 通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。 |
| [^25] | [Computational Lower Bounds for Graphon Estimation via Low-degree Polynomials.](http://arxiv.org/abs/2308.15728) | 通过低次多项式计算图论估计存在计算障碍，传统的优化估计方法具有指数级的计算复杂度，而最优多项式时间估计器只能达到较慢的估计错误率。 |
| [^26] | [Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness.](http://arxiv.org/abs/2308.03666) | 该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。 |
| [^27] | [Lossless Transformations and Excess Risk Bounds in Statistical Inference.](http://arxiv.org/abs/2307.16735) | 在统计推断中，我们研究了无损转换和过量风险的概念。我们提出了无损转换的特征，并构建了一个用于判断给定转换是否是无损的统计量。我们还引入了delta-无损转换的概念，并给出了充分条件。这些研究在分类、非参数回归和投资组合策略等领域具有应用价值。 |
| [^28] | [Flexible and efficient spatial extremes emulation via variational autoencoders.](http://arxiv.org/abs/2307.08079) | 本文提出了一种新的空间极端值模型，通过集成在变分自动编码器的结构中，可以灵活、高效地模拟具有非平稳相关性的极端事件。实验证明，在时间效率和性能上，相对于传统的贝叶斯推断和许多具有平稳相关性的空间极端值模型，我们的方法具有优势。 |
| [^29] | [Transport map unadjusted Langevin algorithms: learning and discretizing perturbed samplers.](http://arxiv.org/abs/2302.07227) | 本研究提出了交通图未调整的 Langevin 算法 (ULA) 和 Riemann 流形 Langevin 动力学 (RMLD)，通过应用交通图可以加速 Langevin 动力学的收敛，并提供了学习度量和扰动的新思路。 |
| [^30] | [Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space.](http://arxiv.org/abs/2302.06807) | 本文提出了一种大间隔分类器，它使用浑拟圆决策边界可以优化测地凸优化问题，实验结果表明其竞争性能优越。 |
| [^31] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^32] | [HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes.](http://arxiv.org/abs/2212.10538) | 本文提出了一种名为HyperBO+的方法，通过使用分层高斯过程的预先训练，实现了在具有不同输入空间的函数上普适于贝叶斯优化。研究人员设计了一种两步预先训练方法，并分析了其吸引人的渐近性质。 |
| [^33] | [Nonparametric plug-in classifier for multiclass classification of S.D.E. paths.](http://arxiv.org/abs/2212.10259) | 这篇论文研究了针对随机微分方程路径的多类别分类问题，提出了一种非参数插件分类器，并在理论上证明了分类过程的一致性和收敛速度，并通过数值研究验证了理论发现。 |
| [^34] | [Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses.](http://arxiv.org/abs/2209.07403) | 本论文研究了具有大的最坏情况Lipschitz参数的差分隐私随机优化问题，并提供了一种不依赖于统一Lipschitz参数的接近最优的过量风险界限方法。 |
| [^35] | [Data Augmentation in the Underparameterized and Overparameterized Regimes.](http://arxiv.org/abs/2202.09134) | 这项研究提供了数据增强如何影响估计的方差和极限分布的确切量化结果，发现数据增强可能会增加估计的不确定性，并且其效果取决于多个因素。同时，该研究还通过随机转换的高维随机向量的函数的极限定理进行了证明。 |
| [^36] | [Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions.](http://arxiv.org/abs/2201.02958) | 本文提出了一种嵌套模拟的新方法，它能够在保持条件期望足够平滑的情况下，有效地缓解高维度中的维度灾难，以桥接标准嵌套模拟的立方根收敛率和标准蒙特卡洛模拟的平方根收敛率之间的差距。 |
| [^37] | [Dynamic Selection in Algorithmic Decision-making.](http://arxiv.org/abs/2108.12547) | 本文研究了算法决策中的动态选择问题，针对在线学习算法中数据的内生性导致的偏差提出了一种基于工具变量的纠正算法，并证明了该算法可以获得真实参数值和较低遗憾水平。研究还提供了统计推断的中心极限定理。 |
| [^38] | [Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy.](http://arxiv.org/abs/1911.09307) | 这项工作提出了一种名为Pani的通用正则化器，它在深度神经网络中进行非局部表示，并将邻域补丁特征进行线性插值，从而构建了一种通用且有效的正则化策略。 |
| [^39] | [Learning Interpretable Characteristic Kernels via Decision Forests.](http://arxiv.org/abs/1812.00029) | 本论文介绍了一种通过决策森林构建可解释的特征核的方法，我们构建了基于叶节点相似性的核平均嵌入随机森林（KMERF），并证明其在离散和连续数据上都表现出渐进特征。实验证明KMERF在多种高维数据测试中优于目前的最先进的基于核的方法。 |

# 详细

[^1]: 残差网络中的深度超参数转移：动态和缩放限制

    Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])

    [http://arxiv.org/abs/2309.16620](http://arxiv.org/abs/2309.16620)

    这项研究通过残差分支尺度和$\mu$P参数化的残差网络，实现了深度学习中超参数的跨宽度和深度的转移。

    

    随着模型大小的增加，深度学习中超参数调整的成本不断上升，促使从业者寻找使用较小网络的代理方法进行调整。其中一个建议使用$\mu$P参数化网络，其中小宽度网络的最佳超参数转移到任意宽度的网络中。然而，在这个方案中，超参数不会在不同深度之间转移。为了解决这个问题，我们研究了具有$1/\sqrt{\text{depth}}$的残差分支尺度和$\mu$P参数化的残差网络。我们通过实验证明，使用这种参数化训练的残差结构，包括卷积ResNet和Vision Transformer，在CIFAR-10和ImageNet上展示了跨宽度和深度的最佳超参数转移。此外，我们的经验发现得到了理论的支持和动机。利用神经网络学习动力学的动态均场理论（DMFT）描述的最新进展，我们展示了

    The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show
    
[^2]: 利用融合网络Gromov-Wasserstein距离中的边特征对图进行挖掘

    Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance. (arXiv:2309.16604v1 [stat.ML])

    [http://arxiv.org/abs/2309.16604](http://arxiv.org/abs/2309.16604)

    该论文介绍了一种用于比较具有节点和边特征的图的扩展Gromov-Wasserstein距离的方法，并提出了新的算法来计算距离和重心。通过实验证明了该方法在图学习任务中的有效性。

    

    图的成对比较在机器学习中的许多应用中至关重要，包括聚类、基于核的分类/回归以及最近的监督图预测。图之间的距离通常依赖于这些结构化对象的信息表达，如子结构包或其他图嵌入。一种最近流行的解决方案是将图表示为度量测度空间，这样可以成功地利用最优输运，它提供了有意义的距离来比较它们：Gromov-Wasserstein距离。然而，这类距离忽略了边属性，而这对于许多结构化对象是至关重要的。在这项工作中，我们介绍了一种扩展Gromov-Wasserstein距离的方法，用于比较具有节点和边特征的图。我们提出了距离和重心计算的新算法。我们通过实验证明了新距离在图出现在输入和输出的学习任务中的有效性

    Pairwise comparison of graphs is key to many applications in Machine learning ranging from clustering, kernel-based classification/regression and more recently supervised graph prediction. Distances between graphs usually rely on informative representations of these structured objects such as bag of substructures or other graph embeddings. A recently popular solution consists in representing graphs as metric measure spaces, allowing to successfully leverage Optimal Transport, which provides meaningful distances allowing to compare them: the Gromov-Wasserstein distances. However, this family of distances overlooks edge attributes, which are essential for many structured objects. In this work, we introduce an extension of Gromov-Wasserstein distance for comparing graphs whose both nodes and edges have features. We propose novel algorithms for distance and barycenter computation. We empirically show the effectiveness of the novel distance in learning tasks where graphs occur in either inp
    
[^3]: 基于交叉预测的推理

    Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])

    [http://arxiv.org/abs/2309.16598](http://arxiv.org/abs/2309.16598)

    本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。

    

    可靠的数据驱动决策依赖于高质量的标注数据，然而获取高质量的标注数据经常需要繁琐的人工标注或者缓慢昂贵的科学测量。机器学习作为一种替代方案正变得越来越有吸引力，因为精密的预测技术可以快速、廉价地产生大量预测标签；例如，预测的蛋白质结构被用来补充实验得到的结构，卫星图像预测的社会经济指标被用来补充准确的调查数据等。由于预测具有不完美和潜在偏差的特点，这种做法对下游推理的有效性产生了质疑。我们引入了基于机器学习的交叉预测方法，用于有效的推理。通过一个小的标记数据集和一个大的未标记数据集，交叉预测通过机器学习填补缺失的标签，并应用一种去偏差的方法来纠正预测不准确性。

    While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
    
[^4]: 异质搜索空间上的贝叶斯优化的迁移学习

    Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])

    [http://arxiv.org/abs/2309.16597](http://arxiv.org/abs/2309.16597)

    本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。

    

    贝叶斯优化是一种流行的黑盒函数优化方法，它基于贝叶斯模型（通常是高斯过程）进行顺序决策。为了确保模型的质量，我们开发了迁移学习方法，通过学习来自“训练”函数的观察结果来自动设计高斯过程先验。这些训练函数通常需要与“测试”函数（待优化的黑盒函数）具有相同的定义域。在本文中，我们介绍了一种名为MPHD的模型预训练方法，它使用神经网络将特定于领域的上下文映射到分层高斯过程的规范。MPHD可以与贝叶斯优化无缝集成，实现异质搜索空间的知识迁移。我们的理论和实证结果证明了MPHD的有效性，并展示了它在具有挑战性的黑盒函数优化任务中的优越性能。

    Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
    
[^5]: M-OFDFT：利用深度学习克服分子系统中的无轨道密度泛函理论的障碍

    M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning. (arXiv:2309.16578v1 [stat.ML])

    [http://arxiv.org/abs/2309.16578](http://arxiv.org/abs/2309.16578)

    M-OFDFT是一种利用深度学习模型解决分子系统问题的OFDFT方法，通过将非局域性建立在模型中并使用紧凑的密度表示，实现了与Kohn-Sham DFT相近的精确度，并且具有良好的外推能力。

    

    无轨道密度泛函理论（OFDFT）是一种具有较低运算成本的量子化学计算方法，比起常用的Kohn-Sham密度泛函理论更加适用于当代分子研究。然而，OFDFT的精确性受到了动能密度泛函的限制，对于非周期性分子系统的近似求解非常困难。本文提出了名为M-OFDFT的方法，利用深度学习的函数模型解决了分子系统的问题。我们将必要的非局域性建立在这个模型中，通过原子基下的展开系数作为紧凑的密度表示来降低成本。通过解决其中的非传统学习挑战的技术，M-OFDFT在一系列OFDFT无法触及的分子上实现了与Kohn-Sham DFT相当的精确度。更有吸引力的是，M-OFDFT在训练时属于更大的分子中有着良好的外推能力，为研究大分子提供了有吸引力的规模效应。

    Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary molecular research. However, its accuracy is limited by the kinetic energy density functional, which is notoriously hard to approximate for non-periodic molecular systems. In this work, we propose M-OFDFT, an OFDFT approach capable of solving molecular systems using a deep-learning functional model. We build the essential nonlocality into the model, which is made affordable by the concise density representation as expansion coefficients under an atomic basis. With techniques to address unconventional learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much larger than those in training, which unleashes the appealing scaling for studying large molecu
    
[^6]: CRIMED：具有无界随机破坏的赌徒问题的遗憾下界和上界

    CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption. (arXiv:2309.16563v1 [stat.ML])

    [http://arxiv.org/abs/2309.16563](http://arxiv.org/abs/2309.16563)

    本文研究了具有任意破坏的多臂赌徒问题，并建立了一个与问题相关的遗憾下界。我们提出了CRIMED算法，该算法在具有已知方差的高斯分布赌徒问题上实现了遗憾下界。

    

    我们研究了在多臂赌徒问题中具有任意破坏的遗憾最小化问题。与经典设定类似，代理接收到的奖励是从每个时间点选择的臂的分布独立生成的。然而，这些奖励并不直接观察到。相反，对于固定的ε∈(0,12)，代理以概率1-ε从选择的臂的分布中观测一个样本，或以概率ε从任意破坏分布中观测。重要的是，我们对这些破坏分布不做任何假设，它们可以是无界的。在这种可能具有无界破坏的情况下，我们为给定的臂分布族建立了一个与问题相关的遗憾下界。我们引入了CRIMED，这是一个渐近最优的算法，它在具有已知方差的高斯分布赌徒问题上实现了遗憾下界。此外，我们还对有限样本进行了分析。

    We investigate the regret-minimisation problem in a multi-armed bandit setting with arbitrary corruptions. Similar to the classical setup, the agent receives rewards generated independently from the distribution of the arm chosen at each time. However, these rewards are not directly observed. Instead, with a fixed $\varepsilon\in (0,\frac{1}{2})$, the agent observes a sample from the chosen arm's distribution with probability $1-\varepsilon$, or from an arbitrary corruption distribution with probability $\varepsilon$. Importantly, we impose no assumptions on these corruption distributions, which can be unbounded. In this setting, accommodating potentially unbounded corruptions, we establish a problem-dependent lower bound on regret for a given family of arm distributions. We introduce CRIMED, an asymptotically-optimal algorithm that achieves the exact lower bound on regret for bandits with Gaussian distributions with known variance. Additionally, we provide a finite-sample analysis of 
    
[^7]: 无监督语言模型蒸馏的事实验证

    Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])

    [http://arxiv.org/abs/2309.16540](http://arxiv.org/abs/2309.16540)

    本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。

    

    无监督事实验证旨在通过可靠知识库中的证据来验证主张，而无需任何形式的数据注释。为了解决这个挑战，算法必须为每个主张生成既语义明确又紧凑的特征，以便与源信息进行语义对齐。与之前的工作不同，前者通过学习包含主张及其相应标签的注释语料库来解决对齐问题。我们提出了SFAVEL（通过语言模型蒸馏的自监督事实验证），这是一个新颖的无监督框架，利用预训练的语言模型将自监督特征蒸馏为高质量的主张-事实对齐，而无需注释。这是通过一种新颖的对比损失函数实现的，该函数鼓励特征在保持语料库间的语义关系的同时实现高质量的主张和证据对齐。值得注意的是，我们展示了达到新颖的状态一.

    Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
    
[^8]: 使用深度条件生成时间序列模型生成个性化的胰岛素治疗策略

    Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models. (arXiv:2309.16521v1 [stat.ML])

    [http://arxiv.org/abs/2309.16521](http://arxiv.org/abs/2309.16521)

    本论文提出了一种使用深度生成时间序列模型和决策理论相结合的新框架，用于生成个性化的胰岛素治疗策略。通过学习生成逼真的个性化治疗和未来结果轨迹，可以为个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。

    

    我们提出了一种新的框架，将深度生成时间序列模型与决策理论相结合，用于生成个性化的治疗策略。它利用历史患者轨迹数据，通过深度生成时间序列模型共同学习生成逼真的个性化治疗和未来结果轨迹。特别地，我们的框架可以根据条件化期望效用最大化训练生成与个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。我们通过为住院糖尿病患者生成个性化的胰岛素治疗策略和血糖预测来展示我们的框架，展示了我们的方法在生成改进的个性化治疗策略方面的潜力。

    We propose a novel framework that combines deep generative time series models with decision theory for generating personalized treatment strategies. It leverages historical patient trajectory data to jointly learn the generation of realistic personalized treatment and future outcome trajectories through deep generative time series models. In particular, our framework enables the generation of novel multivariate treatment strategies tailored to the personalized patient history and trained for optimal expected future outcomes based on conditional expected utility maximization. We demonstrate our framework by generating personalized insulin treatment strategies and blood glucose predictions for hospitalized diabetes patients, showcasing the potential of our approach for generating improved personalized treatment strategies. Keywords: deep generative model, probabilistic decision support, personalized treatment generation, insulin and blood glucose prediction
    
[^9]: 从复杂到清晰：通过Clifford的几何代数和凸优化的分析表达深度神经网络的权重

    From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])

    [http://arxiv.org/abs/2309.16512](http://arxiv.org/abs/2309.16512)

    本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。

    

    本文介绍了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们展示了当使用标准正则化损失进行训练时，深度ReLU神经网络的最优权重由训练样本的楔积给出。此外，训练问题可简化为对楔积特征进行凸优化，在其中编码训练数据集的几何结构。该结构以数据向量生成的三角形和平行体的有符号体积表示。凸问题通过$\ell_1$正则化找到样本的一个小子集，以发现仅相关的楔积特征。我们的分析提供了对深度神经网络内部工作机制的新视角，并揭示了隐藏层的作用。

    In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
    
[^10]: 风电功率预测的资产捆绑

    Asset Bundling for Wind Power Forecasting. (arXiv:2309.16492v1 [stat.ME])

    [http://arxiv.org/abs/2309.16492](http://arxiv.org/abs/2309.16492)

    本研究提出了一种资产捆绑-预测-调整（BPR）框架，将资产捆绑、机器学习和预测协调技术相结合，以实现对风电功率的准确预测和一致性调整。

    

    美国电网中间断性可再生能源的不断增加，特别是风能和太阳能发电，导致运营不确定性增加。在这种背景下，准确的预测非常重要，特别是对于风能发电，由于其变化幅度大且历史上难以预测。为了克服这一挑战，本研究提出了一种新颖的资产捆绑-预测-调整（BPR）框架，将资产捆绑、机器学习和预测协调技术相结合。BPR框架首先学习中间的层次结构（捆绑），然后预测资产、捆绑和整个风电场的风电功率，最后调整所有预测结果以确保一致性。这种方法有效地引入了一个辅助学习任务（预测捆绑层次的时间序列），以帮助主要的学习任务。本文还介绍了能够捕捉风电时间序列的时空动态的新的资产捆绑标准。进行了大量的数值实验。

    The growing penetration of intermittent, renewable generation in US power grids, especially wind and solar generation, results in increased operational uncertainty. In that context, accurate forecasts are critical, especially for wind generation, which exhibits large variability and is historically harder to predict. To overcome this challenge, this work proposes a novel Bundle-Predict-Reconcile (BPR) framework that integrates asset bundling, machine learning, and forecast reconciliation techniques. The BPR framework first learns an intermediate hierarchy level (the bundles), then predicts wind power at the asset, bundle, and fleet level, and finally reconciles all forecasts to ensure consistency. This approach effectively introduces an auxiliary learning task (predicting the bundle-level time series) to help the main learning tasks. The paper also introduces new asset-bundling criteria that capture the spatio-temporal dynamics of wind power time series. Extensive numerical experiments
    
[^11]: 高维度下重尾数据下的鲁棒回归: 渐近性和普适性

    High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality. (arXiv:2309.16476v1 [math.ST])

    [http://arxiv.org/abs/2309.16476](http://arxiv.org/abs/2309.16476)

    本文研究了在高维度和重尾干扰条件下的鲁棒回归估计器的性质，通过研究一类椭圆协变量和噪声数据分布的M-估计器，我们发现在存在重尾噪声的情况下，Huber损失需要进一步正则化才能达到最优性能。同时，我们还推导出了岭回归的超额风险的衰减速率。

    

    我们研究了在协变量和响应函数都受重尾干扰的情况下，鲁棒回归估计量的高维性质。特别地，我们针对一类包含椭圆协变量和噪声数据分布的M-估计器提供了锐利的渐近特征化，包括二阶及以上矩不存在的情况。我们发现，在存在重尾噪声的高维情况下，尽管Huber损失通过最优调整的位置参数$\delta$是一致的，但其在性能上是次优的，突显了进一步正则化以达到最优性能的必要性。这个结果还揭示了$\delta$作为样本复杂度和污染的函数存在的一个有趣的转变。此外，我们还推导出岭回归中超额风险的衰减速率。我们发现，对于具有有限二阶矩的噪声分布，岭回归不仅是最优的，而且是普适的，但其衰减速率可以是...

    We investigate the high-dimensional properties of robust regression estimators in the presence of heavy-tailed contamination of both the covariates and response functions. In particular, we provide a sharp asymptotic characterisation of M-estimators trained on a family of elliptical covariate and noise data distributions including cases where second and higher moments do not exist. We show that, despite being consistent, the Huber loss with optimally tuned location parameter $\delta$ is suboptimal in the high-dimensional regime in the presence of heavy-tailed noise, highlighting the necessity of further regularisation to achieve optimal performance. This result also uncovers the existence of a curious transition in $\delta$ as a function of the sample complexity and contamination. Moreover, we derive the decay rates for the excess risk of ridge regression. We show that, while it is both optimal and universal for noise distributions with finite second moment, its decay rate can be consi
    
[^12]: 一种简约、计算高效的空间回归机器学习方法

    A parsimonious, computationally efficient machine learning method for spatial regression. (arXiv:2309.16448v1 [stat.ML])

    [http://arxiv.org/abs/2309.16448](http://arxiv.org/abs/2309.16448)

    MPRS是一种计算高效、具有物理启发的空间回归机器学习方法，通过使用距离相关的“相互作用”来引入空间或时间相关性，能够处理任意空间维度的分散数据。在各种合成和真实世界的测试中，MPRS展现了与标准插值方法相当的预测性能，特别适用于填补粗糙和非高斯数据的缺口。

    

    我们介绍了改进的平面旋转器方法（MPRS），这是一种受物理启发的用于空间/时间回归的机器学习方法。MPRS是一个非参数模型，通过短程、距离相关的“相互作用”来引入空间或时间相关性，而不需要假设底层概率分布的特定形式。预测是通过完全自主的学习算法进行的，该算法使用平衡条件的蒙特卡罗模拟。MPRS能够处理分散的数据和任意的空间维度。我们在一维、二维和三维上对各种合成和真实世界的数据进行了测试，测试结果表明MPRS的预测性能（无需参数调整）与普通克里金法和逆距离加权法等标准插值方法相当。特别是，MPRS是一种特别有效的填补缺口方法，适用于粗糙和非高斯数据（如每日降水时间序列）。

    We introduce the modified planar rotator method (MPRS), a physically inspired machine learning method for spatial/temporal regression. MPRS is a non-parametric model which incorporates spatial or temporal correlations via short-range, distance-dependent ``interactions'' without assuming a specific form for the underlying probability distribution. Predictions are obtained by means of a fully autonomous learning algorithm which employs equilibrium conditional Monte Carlo simulations. MPRS is able to handle scattered data and arbitrary spatial dimensions. We report tests on various synthetic and real-word data in one, two and three dimensions which demonstrate that the MPRS prediction performance (without parameter tuning) is competitive with standard interpolation methods such as ordinary kriging and inverse distance weighting. In particular, MPRS is a particularly effective gap-filling method for rough and non-Gaussian data (e.g., daily precipitation time series). MPRS shows superior co
    
[^13]: 通过检验进行选择性非参数回归

    Selective Nonparametric Regression via Testing. (arXiv:2309.16412v1 [stat.ML])

    [http://arxiv.org/abs/2309.16412](http://arxiv.org/abs/2309.16412)

    通过检验给定条件方差的假设，我们开发了一种选择性非参数回归方法，允许考虑方差本身的值以及对应方差预测器的不确定性，并证明了估计器的风险的非渐近界。

    

    在具有误差敏感的机器学习应用中，预测中的放弃可能性（或选择性预测）是一个重要问题。虽然分类设置中得到了广泛研究，但对于回归问题的选择性方法发展较少。在这项工作中，我们考虑非参数异方差回归问题，并通过检验给定点处条件方差的假设来开发一个放弃程序。与现有方法不同，提出的方法不仅允许考虑方差本身的值，还允许考虑对应方差预测器的不确定性。我们证明了所得估计器的风险的非渐近界，并展示了几种不同收敛模式的存在。理论分析与一系列在模拟和真实数据上的实验一起进行。

    Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.
    
[^14]: 在没有均值互换性假设的情况下构建合成治疗组

    Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption. (arXiv:2309.16409v1 [stat.ML])

    [http://arxiv.org/abs/2309.16409](http://arxiv.org/abs/2309.16409)

    本文提出了一种在没有均值互换性假设的情况下构建合成治疗组的方法，通过对源群体的治疗组加权混合来构建目标人群的合成治疗组，并通过最小化条件最大均值差异来估计权重。该方法在均值互换性假设被违反时可以作为一种新颖的补充方法。

    

    本文的目的是将多个随机对照试验的信息传递给我们仅有控制组数据的目标人群。以前的工作在很大程度上依赖于均值互换性的假设。然而，正如许多现有研究所指出的，均值互换性假设可能被违反。受合成控制方法的启发，我们通过对源群体的治疗组加权混合构建了目标人群的合成治疗组。我们通过最小化源群体的加权对照组与目标人群之间的条件最大均值差异来估计权重。我们基于筛选半参数理论建立了合成治疗组估计量的渐近正态性。当均值互换性假设被违反时，我们的方法可以作为一种新颖的补充方法。在合成和现实世界数据集上进行实验证明了我们方法的有效性。

    The purpose of this work is to transport the information from multiple randomized controlled trials to the target population where we only have the control group data. Previous works rely critically on the mean exchangeability assumption. However, as pointed out by many current studies, the mean exchangeability assumption might be violated. Motivated by the synthetic control method, we construct a synthetic treatment group for the target population by a weighted mixture of treatment groups of source populations. We estimate the weights by minimizing the conditional maximum mean discrepancy between the weighted control groups of source populations and the target population. We establish the asymptotic normality of the synthetic treatment group estimator based on the sieve semiparametric theory. Our method can serve as a novel complementary approach when the mean exchangeability assumption is violated. Experiments are conducted on synthetic and real-world datasets to demonstrate the effe
    
[^15]: 贝叶斯神经网络：综述和讨论的入门指南

    A Primer on Bayesian Neural Networks: Review and Debates. (arXiv:2309.16314v1 [stat.ML])

    [http://arxiv.org/abs/2309.16314](http://arxiv.org/abs/2309.16314)

    本论文综述介绍了贝叶斯神经网络（BNNs）的基本概念和其在解决神经网络局限性方面的重要性。目标读者包括具备贝叶斯方法背景但缺乏深度学习专业知识的统计学家，以及深度神经网络专业但对贝叶斯统计学有限了解的机器学习专家。

    

    神经网络在各个问题领域取得了显著的性能，但其广泛应用受到了固有局限的限制，如过于自信的预测、缺乏可解释性以及容易受到对抗攻击。为了解决这些挑战，贝叶斯神经网络（BNNs）作为传统神经网络的一个引人注目的扩展，将不确定性估计整合到其预测能力中。本篇综述性入门指南系统介绍了神经网络和贝叶斯推断的基本概念，阐明了它们在BNNs开发中的协同整合。目标读者包括具备贝叶斯方法背景但缺乏深度学习专业知识的统计学家，以及深度神经网络专业但对贝叶斯统计学有限了解的机器学习专家。我们概述了常用的先验知识，考察了它们对模型行为的影响。

    Neural networks have achieved remarkable performance across various problem domains, but their widespread applicability is hindered by inherent limitations such as overconfidence in predictions, lack of interpretability, and vulnerability to adversarial attacks. To address these challenges, Bayesian neural networks (BNNs) have emerged as a compelling extension of conventional neural networks, integrating uncertainty estimation into their predictive capabilities.  This comprehensive primer presents a systematic introduction to the fundamental concepts of neural networks and Bayesian inference, elucidating their synergistic integration for the development of BNNs. The target audience comprises statisticians with a potential background in Bayesian methods but lacking deep learning expertise, as well as machine learners proficient in deep neural networks but with limited exposure to Bayesian statistics. We provide an overview of commonly employed priors, examining their impact on model beh
    
[^16]: 高维数据配对样本假设检验的框架

    A framework for paired-sample hypothesis testing for high-dimensional data. (arXiv:2309.16274v1 [stat.ML])

    [http://arxiv.org/abs/2309.16274](http://arxiv.org/abs/2309.16274)

    本文提出了一个针对高维数据配对样本的假设检验框架，通过垂直平分线生成评分函数，并利用伪中位数求得最优评分函数。

    

    在多维数据的配对样本检验中，标准的方法是对每个特征应用多个单变量检验，然后进行p值调整。然而，当数据含有大量特征时，这种方法存在问题。已有一些研究表明，分类准确率可以作为双样本检验的代理。然而，迄今为止尚未提出理论基础或实际方法来将这种策略扩展到多维配对样本检验。在本研究中，我们提出了一种想法，即通过每对实例连接线段的垂直平分线定义的决策规则来生成评分函数。然后，通过这些规则的伪中位数来估计最优评分函数，我们通过自然地扩展Hodges-Lehmann估计量来进行估计。因此，我们提出了一个两步检验过程的框架。首先，我们估计每个特征的平分线。接下来，我们根据这些平分线定义评分函数，并通过伪中位数求得最优评分函数。

    The standard paired-sample testing approach in the multidimensional setting applies multiple univariate tests on the individual features, followed by p-value adjustments. Such an approach suffers when the data carry numerous features. A number of studies have shown that classification accuracy can be seen as a proxy for two-sample testing. However, neither theoretical foundations nor practical recipes have been proposed so far on how this strategy could be extended to multidimensional paired-sample testing. In this work, we put forward the idea that scoring functions can be produced by the decision rules defined by the perpendicular bisecting hyperplanes of the line segments connecting each pair of instances. Then, the optimal scoring function can be obtained by the pseudomedian of those rules, which we estimate by extending naturally the Hodges-Lehmann estimator. We accordingly propose a framework of a two-step testing procedure. First, we estimate the bisecting hyperplanes for each p
    
[^17]: 超越逆KL：通过多样的差异约束推广直接偏好优化

    Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])

    [http://arxiv.org/abs/2309.16240](http://arxiv.org/abs/2309.16240)

    本论文提出了一种通过引入多样差异约束推广直接偏好优化（DPO）的方法，该方法消除了对估计方法的需要并简化了奖励和最优策略之间的复杂关系。

    

    大型语言模型（LLM）的不断增强能力为人工智能提供了机会，但同时也放大了安全问题，如AI系统的潜在滥用，这需要有效的AI对齐。基于人类反馈的强化学习（RLHF）已经成为AI对齐的一条有希望的路径，但由于其复杂性和对独立奖励模型的依赖性而带来了挑战。直接偏好优化（DPO）被提出作为一种替代方法，在逆KL正则化约束下等同于RLHF。本文提出了f-DPO，一种通过整合多样的差异约束来推广DPO的方法。我们证明，在某些f-散度下，包括Jensen-Shannon散度、正向KL散度和α-散度，奖励和最优策略之间的复杂关系也可以通过解决Karush-Kuhn-Tucker条件来简化。这消除了对估计方法的需要。

    The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimat
    
[^18]: Stackelberg批量策略学习

    Stackelberg Batch Policy Learning. (arXiv:2309.16188v1 [stat.ML])

    [http://arxiv.org/abs/2309.16188](http://arxiv.org/abs/2309.16188)

    Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。

    

    批量强化学习定义了从固定的数据批次中进行学习，缺乏详尽的探索。最坏情况下的最优算法使用经验数据来校准价值函数模型，并在学习模型下执行某种悲观评估，已经成为批量强化学习中一种有前景的范式。然而，对于这个流派的现代研究通常忽视了优化景观中隐藏的分层决策结构。在本文中，我们采用博弈论的观点，将策略学习图表建模为具有领导者-跟随者结构的两人零和博弈。我们提出了一种新颖的基于随机梯度的学习算法：StackelbergLearner，领导者根据其目标的全导数进行更新，而不是通常的个体梯度，而跟随者进行个体更新并确保过渡一致的悲观推理。推导出的学习动力

    Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynam
    
[^19]: 基于元优化合成样本的生成式半监督学习

    Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])

    [http://arxiv.org/abs/2309.16143](http://arxiv.org/abs/2309.16143)

    本文提出了一种基于生成基础模型生成的合成样本进行半监督学习的方法，旨在解决实际应用中无法获取大规模无标签数据集的问题。

    

    半监督学习是使用有标签和无标签数据集来训练深度分类模型的一种有前景的方法。然而，现有的半监督学习方法依赖于大规模的无标签数据集，在许多实际应用中由于法律限制（例如，GDPR）可能无法获取。本文研究一个问题：我们能否在没有实际无标签数据集的情况下训练半监督学习模型？我们提出了一种使用从包含数百万样本的多样领域数据集（例如ImageNet）训练的生成基础模型生成的合成数据集的半监督学习方法。我们的主要思想是识别生成基础模型中仿真无标签样本的合成样本，并使用这些合成样本来训练分类器。为了实现这一点，我们的方法被构建为一个交替优化问题：（i）元学习生成基础模型和（ii）使用真实标记样本和合成样本进行半监督学习的分类器。

    Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: Can we train SSL models without real unlabeled datasets? Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are identifying synthetic samples that emulate unlabeled samples from generative foundation models and training classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthet
    
[^20]: 特征归一化防止非对比学习动力的崩溃

    Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics. (arXiv:2309.16109v1 [cs.LG])

    [http://arxiv.org/abs/2309.16109](http://arxiv.org/abs/2309.16109)

    本论文研究了非对比学习中的动力崩溃问题，发现特征归一化可以防止此问题的出现，为解决自监督表示学习的计算效率提供了新的思路。

    

    对比学习是一种自监督表示学习框架，通过数据增强生成的两个正视图在数据表示空间中通过吸引力使它们相似，而通过排斥力使它们远离负样本。非对比学习通过BYOL和SimSiam等手段去除了负样本，并提高了计算效率。虽然由于缺乏排斥力，学到的表示可能会崩溃成一个单点，但田等人（2021）通过学习动力分析揭示，如果数据增强足够强于正则化，则表示可以避免崩溃。然而，他们的分析没有考虑常用的特征归一化，即在衡量表示相似性之前进行的归一化操作，因此过强的正则化可能会导致动力崩溃，这在特征归一化存在的情况下是不自然的行为。

    Contrastive learning is a self-supervised representation learning framework, where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Non-contrastive learning, represented by BYOL and SimSiam, further gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. (2021) revealed through the learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly-used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may collapse the dynamics, which is an unnatural behavior under the presence of feature normalizat
    
[^21]: 一种对协变量调整的反事实治疗方案响应曲线的非参数估计

    Nonparametric estimation of a covariate-adjusted counterfactual treatment regimen response curve. (arXiv:2309.16099v1 [math.ST])

    [http://arxiv.org/abs/2309.16099](http://arxiv.org/abs/2309.16099)

    该论文提出了一种非参数估计方法，用于对协变量调整的反事实治疗方案响应曲线进行估计，通过提出反概率加权的估计器来平滑曲线函数，并给出了置信区间和收敛性证明。

    

    灵活估计治疗方案下的平均结果（即价值函数）是个性化医学的关键步骤。我们将目标参数定义为给定一组基线协变量的条件价值函数，我们称之为基于分层的价值函数。我们专注于半参数化决策规则的类别，并在该类别中提出了一种基于筛选的非参数协变量调整方案响应曲线估计器。我们的工作在多个方面有所贡献。首先，我们提出了一种反概率加权的非参数高效估计器以平滑的方案响应曲线函数。我们证明了当干扰函数被足够地欠平滑时，渐近线性性得以实现。我们提出了幂法和有限样本准则以进行欠平滑。其次，利用高斯过程理论，我们提出了对平滑的方案响应曲线函数的同时置信区间。第三，我们提供了优化问题的一致性和收敛速度。

    Flexible estimation of the mean outcome under a treatment regimen (i.e., value function) is the key step toward personalized medicine. We define our target parameter as a conditional value function given a set of baseline covariates which we refer to as a stratum based value function. We focus on semiparametric class of decision rules and propose a sieve based nonparametric covariate adjusted regimen-response curve estimator within that class. Our work contributes in several ways. First, we propose an inverse probability weighted nonparametrically efficient estimator of the smoothed regimen-response curve function. We show that asymptotic linearity is achieved when the nuisance functions are undersmoothed sufficiently. Asymptotic and finite sample criteria for undersmoothing are proposed. Second, using Gaussian process theory, we propose simultaneous confidence intervals for the smoothed regimen-response curve function. Third, we provide consistency and convergence rate for the optimiz
    
[^22]: 改进的精细离散化方法提高自适应在线学习

    Improving Adaptive Online Learning Using Refined Discretization. (arXiv:2309.16044v1 [cs.LG])

    [http://arxiv.org/abs/2309.16044](http://arxiv.org/abs/2309.16044)

    通过一种新颖的连续时间启发式算法，提高了自适应在线学习的效果，将梯度方差的依赖性从次优的$O(\sqrt{V_T\log V_T})$改进到最优速率$O(\sqrt{V_T})$，并可适用于未知Lipschitz常数的情况。

    

    我们研究了具有Lipschitz损失的非约束在线线性优化问题。目标是同时达到（i）二阶梯度自适应性；和（ii）比较器范数自适应性，也被称为文献中的“参数自由性”。现有的遗憾界（Cutkosky和Orabona，2018；Mhammedi和Koolen，2020；Jacobsen和Cutkosky，2022）对于梯度方差$V_T$有次优的$O(\sqrt{V_T\log V_T})$依赖性，而本工作利用一种新颖的连续时间启发式算法将其改进为最优速率$O(\sqrt{V_T})$，而无需任何不切实际的加倍技巧。这一结果可以推广到未知Lipschitz常数的情况，消除了先前工作中的范围比率问题（Mhammedi和Koolen，2020）。具体来说，我们首先展示了在问题的连续时间类比中可以相当容易地实现目标的同时适应性，其中环境由任意连续半鞘式建模。然后，我们的关键创新是

    We study unconstrained Online Linear Optimization with Lipschitz losses. The goal is to simultaneously achieve ($i$) second order gradient adaptivity; and ($ii$) comparator norm adaptivity also known as "parameter freeness" in the literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log V_T})$ dependence on the gradient variance $V_T$, while the present work improves it to the optimal rate $O(\sqrt{V_T})$ using a novel continuous-time-inspired algorithm, without any impractical doubling trick. This result can be extended to the setting with unknown Lipschitz constant, eliminating the range ratio problem from prior works (Mhammedi and Koolen, 2020).  Concretely, we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. Then, our key innovation 
    
[^23]: GLM回归与无意识数据损坏

    GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])

    [http://arxiv.org/abs/2309.11657](http://arxiv.org/abs/2309.11657)

    这篇论文介绍了第一个在广义线性模型回归问题中处理加法无意识噪声的算法。算法的目标是通过样本访问来准确地恢复参数向量，使得模型的预测与真实值的误差尽可能小。

    

    我们展示了在广义线性模型（GLMs）的回归问题中，存在加法无意识噪声的第一个算法。我们假设我们有样本访问到的例子$(x, y)$，其中$y$是$g(w^* \cdot x)$的带噪声测量值。特别地，噪声标签的形式为$y = g(w^* \cdot x) + \xi + \epsilon$，其中$\xi$是与$x$独立抽取的无意识噪声满足$\Pr[\xi = 0] \geq o(1)$，而$\epsilon \sim \mathcal N(0, \sigma^2)$。我们的目标是准确地恢复一个参数向量$w$，使得函数$g(w \cdot x)$与真实值$g(w^* \cdot x)$相比具有任意小的误差，而不是与噪声测量$y$相比。我们提出了一个算法，解决了最一般的与分布无关的情况，其中解可能甚至不可识别。我们的算法返回一个准确的估计，如果它是可识别的，否则

    We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.  We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise
    
[^24]: 高斯混合物可以通过多项式数量的样本进行差分隐私学习

    Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])

    [http://arxiv.org/abs/2309.03847](http://arxiv.org/abs/2309.03847)

    通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。

    

    我们研究了在差分隐私(DP)约束下估计高斯混合物的问题。我们的主要结果是，使用$\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$个样本即可在满足$(\varepsilon, \delta)$-DP的条件下估计$k$个高斯混合物，使其达到总变差距离$\alpha$。这是该问题的第一个有限样本复杂性上限，而无需对GMMs做任何结构性假设。为了解决这个问题，我们构建了一个新的框架，该框架对于其他任务可能也有用。在高层次上，我们展示了如果一个分布类（比如高斯分布）是（1）可列表译码的并且（2）在总变差距离方面具有“局部小”覆盖[ BKSW19]，则其混合物类是私密可学习的。证明绕过了一个已知障碍，表明与高斯分布不同，GMMs不具有局部小的覆盖[AAL21]。

    We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
    
[^25]: 通过低次多项式计算图论估计的下界

    Computational Lower Bounds for Graphon Estimation via Low-degree Polynomials. (arXiv:2308.15728v1 [math.ST])

    [http://arxiv.org/abs/2308.15728](http://arxiv.org/abs/2308.15728)

    通过低次多项式计算图论估计存在计算障碍，传统的优化估计方法具有指数级的计算复杂度，而最优多项式时间估计器只能达到较慢的估计错误率。

    

    图论估计是网络分析中最基本的问题之一，在过去十年中受到了相当大的关注。从统计学的角度来看，高等提出了对于随机块模型（SBM）和非参数图论估计的图论估计的极小极差误差率。统计优化估计是基于约束最小二乘法，并且在维度上具有指数级的计算复杂度。从计算的角度来看，已知的最优多项式时间估计器是基于通用奇异值阈值（USVT），但是它只能达到比极小极差错误率慢得多的估计错误率。人们自然会想知道这样的差距是否是必要的。USVT的计算优化性或图论估计中的计算障碍的存在一直是一个长期存在的问题。在这项工作中，我们对此迈出了第一步，并为图论估计的计算障碍提供了严格的证据。

    Graphon estimation has been one of the most fundamental problems in network analysis and has received considerable attention in the past decade. From the statistical perspective, the minimax error rate of graphon estimation has been established by Gao et al (2015) for both stochastic block model (SBM) and nonparametric graphon estimation. The statistical optimal estimators are based on constrained least squares and have computational complexity exponential in the dimension. From the computational perspective, the best-known polynomial-time estimator is based on universal singular value thresholding (USVT), but it can only achieve a much slower estimation error rate than the minimax one. It is natural to wonder if such a gap is essential. The computational optimality of the USVT or the existence of a computational barrier in graphon estimation has been a long-standing open problem. In this work, we take the first step towards it and provide rigorous evidence for the computational barrie
    
[^26]: 架起可信度与开放世界学习的桥梁：一种探索性神经方法，用于增强可解释性、泛化性和鲁棒性

    Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])

    [http://arxiv.org/abs/2308.03666](http://arxiv.org/abs/2308.03666)

    该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。

    

    随着研究人员努力缩小机器智能与人类之间的差距，通过发展人工智能技术，我们必须认识到可信度在开放世界中的关键重要性，在日常生活的各个方面对每个人都已经无处不在。然而，目前的人工智能系统存在几个挑战，可能会导致信任危机：1）对预测结果的解释不足；2）学习模型的泛化性不足；3）对不确定环境的适应能力差。因此，我们探索了一种神经程序，用于架起可信度与开放世界学习之间的桥梁，从单模态扩展到多模态场景，以供读者使用。1）为了增强设计级可解释性，我们首先定制了具有特定物理含义的可信网络；2）然后，通过灵活的学习正则化器设计环境福祉任务接口，以改善可信网络的泛化性能。

    As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
    
[^27]: 无损转换和统计推断中的过量风险界限研究

    Lossless Transformations and Excess Risk Bounds in Statistical Inference. (arXiv:2307.16735v1 [cs.IT])

    [http://arxiv.org/abs/2307.16735](http://arxiv.org/abs/2307.16735)

    在统计推断中，我们研究了无损转换和过量风险的概念。我们提出了无损转换的特征，并构建了一个用于判断给定转换是否是无损的统计量。我们还引入了delta-无损转换的概念，并给出了充分条件。这些研究在分类、非参数回归和投资组合策略等领域具有应用价值。

    

    我们研究了统计推断中的过量最小风险，定义为从观测到的特征向量中估计随机变量的最小期望损失与从特征向量的转换（统计量）中估计相同随机变量的最小期望损失之间的差异。在描述了无损转换（即对于所有损失函数，过量风险为零的转换）之后，我们构建了一个对假设进行分区检验的统计量，用于判断给定转换是否为无损转换，并证明对于i.i.d.数据，该检验是强一致的。更一般地，我们根据信息理论给出了过量风险的上界，该上界在相当一般的损失函数类上都是一致的。基于这些界限，我们引入了“delta-无损转换”的概念，并给出了给定转换普遍是delta-无损的充分条件。该研究在分类、非参数回归、投资组合策略等方面具有应用价值。

    We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector. After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent. More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions. Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless. Applications to classification, nonparametric regression, portfolio strategie
    
[^28]: 通过变分自动 编码器实现灵活高效的空间极端值模拟

    Flexible and efficient spatial extremes emulation via variational autoencoders. (arXiv:2307.08079v1 [stat.ML])

    [http://arxiv.org/abs/2307.08079](http://arxiv.org/abs/2307.08079)

    本文提出了一种新的空间极端值模型，通过集成在变分自动编码器的结构中，可以灵活、高效地模拟具有非平稳相关性的极端事件。实验证明，在时间效率和性能上，相对于传统的贝叶斯推断和许多具有平稳相关性的空间极端值模型，我们的方法具有优势。

    

    许多现实世界的过程具有复杂的尾依赖结构，这种结构无法使用传统的高斯过程来描述。更灵活的空间极端值模型， 如高斯尺度混合模型和单站点调节模型，具有吸引人的极端依赖性质，但往往难以拟合和模拟。本文中，我们提出了一种新的空间极端值模型，具有灵活和非平稳的相关性属性，并将其集成到变分自动编码器 (extVAE) 的编码-解码结构中。 extVAE 可以作为一个时空模拟器，对潜在的机制模型输出状态的分布进行建模，并产生具有与输入相同属性的输出，尤其是在尾部区域。通过广泛的模拟研究，我们证明我们的extVAE比传统的贝叶斯推断更高效，并且在具有 平稳相关性结构的许多空间极端值模型中表现 更好。

    Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence str
    
[^29]: 交通图未调整的 Langevin 算法：学习和离散化扰动采样器

    Transport map unadjusted Langevin algorithms: learning and discretizing perturbed samplers. (arXiv:2302.07227v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.07227](http://arxiv.org/abs/2302.07227)

    本研究提出了交通图未调整的 Langevin 算法 (ULA) 和 Riemann 流形 Langevin 动力学 (RMLD)，通过应用交通图可以加速 Langevin 动力学的收敛，并提供了学习度量和扰动的新思路。

    

    Langevin 动力学被广泛用于抽样高维、非高斯分布，其密度已知但常数未知。特别感兴趣的是未修正的 Langevin 算法 (ULA)，它直接离散化 Langevin 动力学以估计目标分布上的期望。我们研究了使用交通图来近似标准化目标分布的方法，以预处理和加速 Langevin 动力学的收敛。我们展示了在连续时间下，当将交通图应用于 Langevin 动力学时，结果是具有由交通图定义的度量的 Riemann 流形 Langevin 动力学（RMLD）。我们还展示了将交通图应用于不可逆扰动的 ULA 会产生原动力学的几何信息不可逆扰动 （GiIrr）。这些联系表明了学习度量和扰动的更系统的方法，并提供了描述 RMLD 的替代离散化方法。

    Langevin dynamics are widely used in sampling high-dimensional, non-Gaussian distributions whose densities are known up to a normalizing constant. In particular, there is strong interest in unadjusted Langevin algorithms (ULA), which directly discretize Langevin dynamics to estimate expectations over the target distribution. We study the use of transport maps that approximately normalize a target distribution as a way to precondition and accelerate the convergence of Langevin dynamics. We show that in continuous time, when a transport map is applied to Langevin dynamics, the result is a Riemannian manifold Langevin dynamics (RMLD) with metric defined by the transport map. We also show that applying a transport map to an irreversibly-perturbed ULA results in a geometry-informed irreversible perturbation (GiIrr) of the original dynamics. These connections suggest more systematic ways of learning metrics and perturbations, and also yield alternative discretizations of the RMLD described b
    
[^30]: 超似曲空间的大间隔分类的浑拟圆决策边界

    Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space. (arXiv:2302.06807v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06807](http://arxiv.org/abs/2302.06807)

    本文提出了一种大间隔分类器，它使用浑拟圆决策边界可以优化测地凸优化问题，实验结果表明其竞争性能优越。

    

    近年来，用超似曲空间表示层次结构化数据已经越来越流行，同时，文献中也提出了几个针对这些空间中数据分类的算法。这些算法主要使用超平面或测地线作为决策边界，使用大间隔分类器设置，从而导致一个非凸优化问题。在本文中，我们提出了一种基于浑拟圆决策边界的新型大间隔分类器，它可以导致一个测地凸优化问题，可以使用任何黎曼梯度下降技术来优化，保证全局最优解。我们展示了几个实验，展示了我们的分类器相比于 SOTA 的竞争性能。

    Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA.
    
[^31]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^32]: HyperBO+：使用分层高斯过程为贝叶斯优化预先训练通用先验

    HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes. (arXiv:2212.10538v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10538](http://arxiv.org/abs/2212.10538)

    本文提出了一种名为HyperBO+的方法，通过使用分层高斯过程的预先训练，实现了在具有不同输入空间的函数上普适于贝叶斯优化。研究人员设计了一种两步预先训练方法，并分析了其吸引人的渐近性质。

    

    贝叶斯优化在许多黑盒函数优化任务中被证明非常有效，但需要实践者精心选择适合其感兴趣函数的先验概率模型。研究人员已经探索了基于迁移学习的方法，如多任务BO、少样本BO和HyperBO，来自动学习先验概率。然而，这些方法通常假设所有任务的输入空间相同，限制了它们在不同输入空间上使用观测结果或推广学习到的先验概率模型的能力。本文提出了HyperBO+，这是一种基于分层高斯过程的预先训练方法，能够在具有不同输入空间的函数上普遍适用于贝叶斯优化。我们提出了一种两步预先训练方法，并分析了其吸引人的渐近性质及特点。

    Bayesian optimization (BO), while proved highly effective for many black-box function optimization tasks, requires practitioners to carefully select priors that well model their functions of interest. Rather than specifying by hand, researchers have investigated transfer learning based methods to automatically learn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO (Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, those prior learning methods typically assume that the input domains are the same for all tasks, weakening their ability to use observations on functions with different domains or generalize the learned priors to BO on different search spaces. In this work, we present HyperBO+: a pre-training approach for hierarchical Gaussian processes that enables the same prior to work universally for Bayesian optimization on functions with different domains. We propose a two-step pre-training method and analyze its appealing asymptotic properties and 
    
[^33]: 针对随机微分方程路径的多类别分类的非参数插件分类器研究

    Nonparametric plug-in classifier for multiclass classification of S.D.E. paths. (arXiv:2212.10259v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.10259](http://arxiv.org/abs/2212.10259)

    这篇论文研究了针对随机微分方程路径的多类别分类问题，提出了一种非参数插件分类器，并在理论上证明了分类过程的一致性和收敛速度，并通过数值研究验证了理论发现。

    

    我们研究了多类别分类问题，其中特征来自于时间齐次扩散的混合。具体来说，通过它们的漂移函数来区分不同的类别，而扩散系数对于所有类别都是相同且未知的。在这个框架下，我们构建了一个基于漂移函数和扩散函数的非参数估计的插件分类器。我们首先在温和假设下证明了我们的分类过程的一致性，然后在不同的假设集下给出了收敛速度。最后，一个数值研究支持我们的理论发现。

    We study the multiclass classification problem where the features come from the mixture of time-homogeneous diffusions. Specifically, the classes are discriminated by their drift functions while the diffusion coefficient is common to all classes and unknown. In this framework, we build a plug-in classifier which relies on nonparametric estimators of the drift and diffusion functions. We first establish the consistency of our classification procedure under mild assumptions and then provide rates of cnvergence under different set of assumptions. Finally, a numerical study supports our theoretical findings.
    
[^34]: 具有大的最坏情况Lipschitz参数的私有随机优化：（非光滑）凸损失的最优速率及其对非凸损失的扩展

    Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses. (arXiv:2209.07403v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07403](http://arxiv.org/abs/2209.07403)

    本论文研究了具有大的最坏情况Lipschitz参数的差分隐私随机优化问题，并提供了一种不依赖于统一Lipschitz参数的接近最优的过量风险界限方法。

    

    我们研究了具有最坏情况Lipschitz参数可能非常大的损失函数的差分隐私（DP）随机优化（SO）。迄今为止，大部分关于DP SO的工作都假设损失在所有数据点上是均匀Lipschitz连续的（即随机梯度在所有数据点上都有界）。虽然这种假设很方便，但通常会导致悲观的过量风险界限。在许多实际问题中，由于异常值，损失在所有数据点上的最坏情况（统一）Lipschitz参数可能非常大。在这种情况下，DP SO的误差界限与损失的最坏情况Lipschitz参数成比例，将会是空洞的。为了解决这些限制，本工作提供了一种接近最优的过量风险界限，不依赖于损失的统一Lipschitz参数。在最近的工作（Wang等人，2020; Kamath等人，2022）的基础上，我们假设随机梯度具有有界的k阶矩

    We study differentially private (DP) stochastic optimization (SO) with loss functions whose worst-case Lipschitz parameter over all data points may be extremely large. To date, the vast majority of work on DP SO assumes that the loss is uniformly Lipschitz continuous over data (i.e. stochastic gradients are uniformly bounded over all data points). While this assumption is convenient, it often leads to pessimistic excess risk bounds. In many practical problems, the worst-case (uniform) Lipschitz parameter of the loss over all data points may be extremely large due to outliers. In such cases, the error bounds for DP SO, which scale with the worst-case Lipschitz parameter of the loss, are vacuous. To address these limitations, this work provides near-optimal excess risk bounds that do not depend on the uniform Lipschitz parameter of the loss. Building on a recent line of work (Wang et al., 2020; Kamath et al., 2022), we assume that stochastic gradients have bounded $k$-th order moments fo
    
[^35]: 在欠参数化和过参数化的模式中的数据增强

    Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09134](http://arxiv.org/abs/2202.09134)

    这项研究提供了数据增强如何影响估计的方差和极限分布的确切量化结果，发现数据增强可能会增加估计的不确定性，并且其效果取决于多个因素。同时，该研究还通过随机转换的高维随机向量的函数的极限定理进行了证明。

    

    我们提供了确切量化数据增强如何影响估计的方差和极限分布的结果，并详细分析了几个具体模型。结果证实了机器学习实践中的一些观察，但也得出了意外的发现：数据增强可能会增加而不是减少估计的不确定性，比如经验预测风险。它可以充当正则化器，但在某些高维问题中却无法实现，并且可能会改变经验风险的双重下降峰值。总的来说，分析表明数据增强被赋予的几个属性要么是真的，要么是假的，而是取决于多个因素的组合-特别是数据分布，估计器的属性以及样本大小，增强数量和维数的相互作用。我们的主要理论工具是随机转换的高维随机向量的函数的极限定理。

    We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
    
[^36]: 平滑的嵌套模拟方法：在高维度中桥接立方和平方根收敛率

    Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions. (arXiv:2201.02958v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2201.02958](http://arxiv.org/abs/2201.02958)

    本文提出了一种嵌套模拟的新方法，它能够在保持条件期望足够平滑的情况下，有效地缓解高维度中的维度灾难，以桥接标准嵌套模拟的立方根收敛率和标准蒙特卡洛模拟的平方根收敛率之间的差距。

    

    嵌套模拟是通过模拟来估计条件期望的功能。本文提出了一种基于核岭回归的新方法，以利用条件期望作为多维调节变量的平滑函数。渐近分析表明，只要条件期望足够平滑，所提出的方法能够在模拟次数增加时有效地减少维度灾难的影响。平滑性桥接了立方根收敛率（即标准嵌套模拟的最优收敛率）和平方根收敛率（即标准蒙特卡洛模拟的规范收敛率）之间的差距。我们通过组合风险管理和输入不确定性量化的数值示例，展示了所提出方法的性能。

    Nested simulation concerns estimating functionals of a conditional expectation via simulation. In this paper, we propose a new method based on kernel ridge regression to exploit the smoothness of the conditional expectation as a function of the multidimensional conditioning variable. Asymptotic analysis shows that the proposed method can effectively alleviate the curse of dimensionality on the convergence rate as the simulation budget increases, provided that the conditional expectation is sufficiently smooth. The smoothness bridges the gap between the cubic root convergence rate (that is, the optimal rate for the standard nested simulation) and the square root convergence rate (that is, the canonical rate for the standard Monte Carlo simulation). We demonstrate the performance of the proposed method via numerical examples from portfolio risk management and input uncertainty quantification.
    
[^37]: 算法决策中的动态选择问题研究

    Dynamic Selection in Algorithmic Decision-making. (arXiv:2108.12547v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2108.12547](http://arxiv.org/abs/2108.12547)

    本文研究了算法决策中的动态选择问题，针对在线学习算法中数据的内生性导致的偏差提出了一种基于工具变量的纠正算法，并证明了该算法可以获得真实参数值和较低遗憾水平。研究还提供了统计推断的中心极限定理。

    

    本文识别和解决了在线学习算法中的动态选择问题，这些问题与内生数据有关。在上下文多臂赌博模型中，由于数据的内生性影响决策的选择，会产生一种新的偏差（自我实现偏差），从而影响到未来待收集和分析的数据的分布。我们提出了一种基于工具变量的算法，以纠正这种偏差。该算法可以获得真实参数值，并获得较低（类似对数的）遗憾水平。我们还证明了统计推断的中心极限定理。为了建立理论性质，我们开发了一个通用技术，以解开数据和行动之间的相互依赖关系。

    This paper identifies and addresses dynamic selection problems in online learning algorithms with endogenous data. In a contextual multi-armed bandit model, a novel bias (self-fulfilling bias) arises because the endogeneity of the data influences the choices of decisions, affecting the distribution of future data to be collected and analyzed. We propose an instrumental-variable-based algorithm to correct for the bias. It obtains true parameter values and attains low (logarithmic-like) regret levels. We also prove a central limit theorem for statistical inference. To establish the theoretical properties, we develop a general technique that untangles the interdependence between data and actions.
    
[^38]: Patch-level Neighborhood Interpolation: 一种通用且有效的基于图的正则化策略

    Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy. (arXiv:1911.09307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.09307](http://arxiv.org/abs/1911.09307)

    这项工作提出了一种名为Pani的通用正则化器，它在深度神经网络中进行非局部表示，并将邻域补丁特征进行线性插值，从而构建了一种通用且有效的正则化策略。

    

    正则化对于机器学习模型尤其是深度神经网络非常重要。现有的正则化技术主要依赖于独立同分布假设，并且仅考虑当前样本的知识，没有利用样本之间的邻居关系。在这项工作中，我们提出了一种称为“Patch-level Neighborhood Interpolation（Pani）”的通用正则化器，在网络计算中进行非局部表示。我们的提议明确地构建了不同层次的补丁级图，然后线性插值邻域补丁特征，作为一种通用且有效的正则化策略。此外，我们将我们的方法定制为两种流行的正则化方法，即虚拟对抗训练（VAT）和MixUp以及其变体。首先派生的“Pani VAT”通过使用补丁级插值扰动构建非局部对抗平滑度，提出了一种新颖的方法。

    Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \textbf{Patch-level Neighborhood Interpolation~(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. Th
    
[^39]: 通过决策森林学习可解释的特征核

    Learning Interpretable Characteristic Kernels via Decision Forests. (arXiv:1812.00029v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1812.00029](http://arxiv.org/abs/1812.00029)

    本论文介绍了一种通过决策森林构建可解释的特征核的方法，我们构建了基于叶节点相似性的核平均嵌入随机森林（KMERF），并证明其在离散和连续数据上都表现出渐进特征。实验证明KMERF在多种高维数据测试中优于目前的最先进的基于核的方法。

    

    决策森林被广泛用于分类和回归任务。树方法的一个较少被知晓的特性是可以从树构建相似性矩阵，并且这些相似性矩阵是由核诱导的。尽管对于核的应用和性质进行了广泛研究，但对于由决策森林诱导的核的研究相对较少。我们构建了基于叶节点相似性的核平均嵌入随机森林（KMERF），它可以从随机树或森林中诱导核。我们引入了渐进特征核的概念，并证明KMERF核对于离散和连续数据都是渐进特征的。由于KMERF是数据自适应的，我们怀疑它将在有限样本数据上胜过预先选择的核。我们展示了KMERF在各种高维两样本和独立性测试场景中几乎占据了目前的最先进的基于核的测试方法。

    Decision forests are widely used for classification and regression tasks. A lesser known property of tree-based methods is that one can construct a proximity matrix from the tree(s), and these proximity matrices are induced kernels. While there has been extensive research on the applications and properties of kernels, there is relatively little research on kernels induced by decision forests. We construct Kernel Mean Embedding Random Forests (KMERF), which induce kernels from random trees and/or forests using leaf-node proximity. We introduce the notion of an asymptotically characteristic kernel, and prove that KMERF kernels are asymptotically characteristic for both discrete and continuous data. Because KMERF is data-adaptive, we suspected it would outperform kernels selected a priori on finite sample data. We illustrate that KMERF nearly dominates current state-of-the-art kernel-based tests across a diverse range of high-dimensional two-sample and independence testing settings. Furth
    

