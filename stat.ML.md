# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Cross-Entropy Loss Functions: Theoretical Analysis and Applications.](http://arxiv.org/abs/2304.07288) | 本文对交叉熵、广义交叉熵、均方误差等一大类损失函数进行了理论分析，并提出了具有优势的双交叉熵损失函数，特别适用于存在标签噪声或类别不平衡的情况。 |
| [^2] | [Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning.](http://arxiv.org/abs/2304.07278) | 本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。 |
| [^3] | [Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements.](http://arxiv.org/abs/2304.07245) | 本文采用基于机器学习的方法，结合人工神经网络和遗传算法，对盘式联轴器中的柔性盘元件进行改进设计，降低其质量和应力，而不降低扭矩传递和不对齐能力。 |
| [^4] | [Optimal inference of a generalised Potts model by single-layer transformers with factored attention.](http://arxiv.org/abs/2304.07235) | 我们将分析和数值推导结合，在基于广义 Potts 模型的数据上，对经过改进适应这种模型的self-attention机制进行训练，发现经过修改的self-attention机制可以在极限采样下准确学习Potts模型。这个“分解”注意力机制通过从数据中学习相关属性，可以提高Transformer的性能和可解释性。 |
| [^5] | [Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation.](http://arxiv.org/abs/2304.07048) | 本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。 |
| [^6] | [Ledoit-Wolf linear shrinkage with unknown mean.](http://arxiv.org/abs/2304.07045) | 本文研究了在未知均值下的大维协方差矩阵估计问题，并提出了一种新的估计器，证明了其二次收敛性，在实验中表现优于其他标准估计器。 |
| [^7] | [Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit.](http://arxiv.org/abs/2304.07025) | 本文介绍了连续时间自回归递归神经网络(CTRNNs)的应用, 通过连续演化来解决非规则采样的时间序列问题, 以概率预测临床监护设置中的血糖水平。 |
| [^8] | [Detection and Estimation of Structural Breaks in High-Dimensional Functional Time Series.](http://arxiv.org/abs/2304.07003) | 本文提出了一种新的高维函数时间序列突变检验统计量，又保证功率性能，又扩大了具备功率的区域；同时，引入了一种易于实现的聚类算法和信息准则，以一致地估计未知组数和成员资格，以改善后聚类的收敛性质。 |
| [^9] | [Complexity of Gibbs samplers through Bayesian asymptotics.](http://arxiv.org/abs/2304.06993) | 本文介绍了一种基于贝叶斯渐近性工具的方法，用于分析Gibbs抽样器的混合时间的渐近行为，并在随机数据生成假设下获得了对于具有通用似然函数的广泛的二级模型的无维度收敛结果。 |
| [^10] | [Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective.](http://arxiv.org/abs/2304.06833) | 本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。 |
| [^11] | [Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality.](http://arxiv.org/abs/2304.06821) | 本文研究了通用图和具有局部性质的图中的成对比较排序问题。研究表明，最大似然估计（MLE）可以实现符合Cram\'er-Rao下界的逐元估计误差。同时，文章还确定了局部性不会影响的条件，并提出了分治算法以实现类似保障。 |
| [^12] | [Active Cost-aware Labeling of Streaming Data.](http://arxiv.org/abs/2304.06808) | 本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。 |
| [^13] | [Sample Average Approximation for Black-Box VI.](http://arxiv.org/abs/2304.06803) | 该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。 |
| [^14] | [A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions.](http://arxiv.org/abs/2304.06787) | 本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。 |
| [^15] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^16] | [Post-selection Inference for Conformal Prediction: Trading off Coverage for Precision.](http://arxiv.org/abs/2304.06158) | 本论文提出一种使用无分布信赖带的 uniform conformal inference 算法，实现任意数据相关误覆盖水平的有限样本预测保证的统一一致性推理。 |
| [^17] | [Mixed moving average field guided learning for spatio-temporal data.](http://arxiv.org/abs/2301.00736) | 本论文提出了一种理论引导机器学习方法，采用广义贝叶斯算法进行混合移动平均场引导的时空数据建模，可以进行因果未来预测。 |
| [^18] | [FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee.](http://arxiv.org/abs/2211.15072) | 本研究提出了FaiREE算法，它是一种可满足群体公平性约束的公平分类算法，并且具有有限样本和无分布理论保证。在实验中表现优异。 |
| [^19] | [Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions.](http://arxiv.org/abs/2211.09781) | 在医疗保健中，通过监测条件性表现，可以有效地监测考虑混淆医疗干预时基于机器学习的风险预测算法。 |
| [^20] | [Sparsity-Constrained Optimal Transport.](http://arxiv.org/abs/2209.15466) | 这篇论文提出了一种新的带有输运计划显式基数约束的 OT 方法，以确保每个输入令牌都与少量专家匹配，从而提高模型的可解释性。 |
| [^21] | [Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse.](http://arxiv.org/abs/2206.13714) | 研究提出了一种广义策略提升算法，结合了在线方法的策略提升保证和离线策略算法通过样本重用有效利用数据的效率。 |
| [^22] | [A Blessing of Dimensionality in Membership Inference through Regularization.](http://arxiv.org/abs/2205.14055) | 研究探讨了过度参数化对成员推理攻击易受攻击性的影响，发现适当的正则化可以在增加模型参数数量的同时提高隐私和性能，消除了隐私与效用之间的权衡问题。 |
| [^23] | [Making SGD Parameter-Free.](http://arxiv.org/abs/2205.02160) | 该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。 |
| [^24] | [Graph-Based Machine Learning Improves Just-in-Time Defect Prediction.](http://arxiv.org/abs/2110.05371) | 该论文使用基于图的机器学习技术，构建了一个由开发人员和源文件组成的贡献图，利用这个图提取的特征，改进了 JIT 缺陷预测，比传统的机器学习方法更好地预测了易出现缺陷的更改。 |
| [^25] | [Variational Diffusion Models.](http://arxiv.org/abs/2107.00630) | 该论文提出了一族基于扩散的生成模型，通过对噪声时间表的有效优化，这些模型在图像密度估计基准测试中获得了最先进的可能性和视觉质量。 |
| [^26] | [Time-Varying Parameters as Ridge Regressions.](http://arxiv.org/abs/2009.00401) | 该论文提出了一种实际上是基于岭回归的时变参数模型，这比传统的状态空间方法计算更快，调整更容易，有助于研究经济结构性变化。 |

# 详细

[^1]: 交叉熵损失函数：理论分析与应用

    Cross-Entropy Loss Functions: Theoretical Analysis and Applications. (arXiv:2304.07288v1 [cs.LG])

    [http://arxiv.org/abs/2304.07288](http://arxiv.org/abs/2304.07288)

    本文对交叉熵、广义交叉熵、均方误差等一大类损失函数进行了理论分析，并提出了具有优势的双交叉熵损失函数，特别适用于存在标签噪声或类别不平衡的情况。

    

    交叉熵是广泛应用的损失函数。当使用softmax函数时，它与神经网络输出应用于逻辑回归损失函数相符。但是，使用交叉熵作为代理损失函数时，我们能依靠什么保证呢？我们提出了对广泛的损失函数家族进行理论分析，包括交叉熵（或逻辑损失）、广义交叉熵、均方误差和其他交叉熵类函数。我们给出了这些损失函数的第一个$H$-连续性界限。这些都是非渐进保证，以估计代理损失的估计误差为上限，用于特定的假设集$H$。我们进一步展示了这些边界的紧密程度。这些边界取决于称为可最小化间隙的量，这些间隙只取决于损失函数和假设集。为了使它们更具体化，我们对复杂和损失函数的这些间隙进行了具体分析。我们还引入了一种新的损失函数，称为双交叉熵损失，它基于两个交叉熵损失的组合。我们表明，它可以优于标准交叉熵损失，特别是在存在标签噪声或类别不平衡的情况下。

    Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of losses, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other loss cross-entropy-like functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps, which only depend on the loss function and the hypothesis set. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduc
    
[^2]: 强化学习中的极小极大最优无关奖励探索

    Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])

    [http://arxiv.org/abs/2304.07278](http://arxiv.org/abs/2304.07278)

    本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。

    

    本文研究了强化学习中的无关奖励探索，设计了一种算法来改进现有技术。研究了具有S个状态，A个动作和有限时间水平H的非平稳马尔科夫决策过程，并收集了一定数量的无引导奖励信息的样本集，在保证收集的数量满足多项式级别时，算法能够发现所有这些奖励函数的最小值，实现了可证明的极小极大最优探索方案。

    This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log f
    
[^3]: 基于机器学习的柔性盘元件的多目标设计探索

    Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements. (arXiv:2304.07245v1 [cs.NE])

    [http://arxiv.org/abs/2304.07245](http://arxiv.org/abs/2304.07245)

    本文采用基于机器学习的方法，结合人工神经网络和遗传算法，对盘式联轴器中的柔性盘元件进行改进设计，降低其质量和应力，而不降低扭矩传递和不对齐能力。

    

    设计探索是工程设计过程中的一个重要步骤。它涉及寻找满足指定设计标准和完成预定义目标的设计方案。近年来，机器学习方法在工程设计问题中得到了广泛应用。本文展示了将人工神经网络（ANN）架构应用于工程设计问题中，以探索和识别改进的设计方案。本研究的案例问题是柔性盘元件的设计，该元件用于盘式联轴器中。我们需要通过降低质量和应力而不降低扭矩传递和不对齐能力来改进盘元件的设计。为了实现这个目标，我们在设计探索步骤中采用了ANN和遗传算法，以识别符合指定标准（扭矩和不对齐）且具有最小质量和应力的设计方案。结果与优化结果相当。

    Design exploration is an important step in the engineering design process. This involves the search for design/s that meet the specified design criteria and accomplishes the predefined objective/s. In recent years, machine learning-based approaches have been widely used in engineering design problems. This paper showcases Artificial Neural Network (ANN) architecture applied to an engineering design problem to explore and identify improved design solutions. The case problem of this study is the design of flexible disc elements used in disc couplings. We are required to improve the design of the disc elements by lowering the mass and stress without lowering the torque transmission and misalignment capability. To accomplish this objective, we employ ANN coupled with genetic algorithm in the design exploration step to identify designs that meet the specified criteria (torque and misalignment) while having minimum mass and stress. The results are comparable to the optimized results obtained
    
[^4]: 利用分解注意力机制的单层Transformer对广义Potts模型进行最优推断

    Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2304.07235](http://arxiv.org/abs/2304.07235)

    我们将分析和数值推导结合，在基于广义 Potts 模型的数据上，对经过改进适应这种模型的self-attention机制进行训练，发现经过修改的self-attention机制可以在极限采样下准确学习Potts模型。这个“分解”注意力机制通过从数据中学习相关属性，可以提高Transformer的性能和可解释性。

    

    Transformer 是一种革命性的神经网络，在自然语言处理和蛋白质科学方面取得了实践上的成功。它们的关键构建块是一个叫做自注意力机制的机制，它被训练用于预测句子中缺失的词。尽管Transformer在应用中取得了实践上的成功，但是自注意力机制究竟从数据中学到了什么以及它是怎么做到的还不是很清楚。本文针对从具有相互作用的位置和 Potts 颜色中提取的数据在训练的Transformer上给出了精确的分析和数值刻画。我们证明，虽然一般的transformer需要多层学习才能准确学习这个分布，但是经过小改进的自注意力机制在无限采样的极限下可以完美地学习Potts模型。我们还计算了这个修改后的自注意力机制所谓“分解”的泛化误差，并在合成数据上数值演示了我们的发现。我们的结果为解释Transformer的内在工作原理以及提高其性能和可解释性提供了新的思路。

    Transformers are the type of neural networks that has revolutionised natural language processing and protein science. Their key building block is a mechanism called self-attention which is trained to predict missing words in sentences. Despite the practical success of transformers in applications it remains unclear what self-attention learns from data, and how. Here, we give a precise analytical and numerical characterisation of transformers trained on data drawn from a generalised Potts model with interactions between sites and Potts colours. While an off-the-shelf transformer requires several layers to learn this distribution, we show analytically that a single layer of self-attention with a small modification can learn the Potts model exactly in the limit of infinite sampling. We show that this modified self-attention, that we call ``factored'', has the same functional form as the conditional probability of a Potts spin given the other spins, compute its generalisation error using t
    
[^5]: Wasserstein PAC-Bayes 学习：泛化与优化之间的桥梁。

    Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])

    [http://arxiv.org/abs/2304.07048](http://arxiv.org/abs/2304.07048)

    本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。

    

    PAC-Bayes 学习是一种已建立的框架，用于在训练阶段评估学习算法的泛化能力。然而，在训练之前，弄清楚为什么知名算法的输出具有良好的泛化特性而 PAC-Bayes 是否有用仍然具有挑战性。我们通过扩展简要介绍在文献 \cite{amit2022ipm} 中提出的 \emph{Wasserstein PAC-Bayes} 框架来积极回答这个问题。我们提供了新的泛化界限，利用损失函数上的几何假设。使用我们的框架，我们在任何训练之前就证明了 \cite{lambert2022variational} 中算法的输出具有强大的渐近泛化能力。更具体地说，我们展示了如何在泛化框架中将优化结果结合起来，构建了 PAC-Bayes 和优化算法之间的桥梁。

    PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \emph{Wasserstein PAC-Bayes} framework, briefly introduced in \cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.
    
[^6]: Ledoit-Wolf线性收缩方法在未知均值的情况下的应用(arXiv:2304.07045v1 [math.ST])

    Ledoit-Wolf linear shrinkage with unknown mean. (arXiv:2304.07045v1 [math.ST])

    [http://arxiv.org/abs/2304.07045](http://arxiv.org/abs/2304.07045)

    本文研究了在未知均值下的大维协方差矩阵估计问题，并提出了一种新的估计器，证明了其二次收敛性，在实验中表现优于其他标准估计器。

    

    本研究探讨了在未知均值下的大维协方差矩阵估计问题。当维数和样本数成比例并趋向于无穷大时，经验协方差估计器失效，此时称为Kolmogorov渐进性。当均值已知时，Ledoit和Wolf（2004）提出了一个线性收缩估计器，并证明了在这些演进下的收敛性。据我们所知，当均值未知时，尚未提出正式证明。为了解决这个问题，我们提出了一个新的估计器，并在Ledoit和Wolf的假设下证明了它的二次收敛性。最后，我们通过实验证明它胜过了其他标准估计器。

    This work addresses large dimensional covariance matrix estimation with unknown mean. The empirical covariance estimator fails when dimension and number of samples are proportional and tend to infinity, settings known as Kolmogorov asymptotics. When the mean is known, Ledoit and Wolf (2004) proposed a linear shrinkage estimator and proved its convergence under those asymptotics. To the best of our knowledge, no formal proof has been proposed when the mean is unknown. To address this issue, we propose a new estimator and prove its quadratic convergence under the Ledoit and Wolf assumptions. Finally, we show empirically that it outperforms other standard estimators.
    
[^7]: 连续时间递归神经网络：概述及在监护病房血糖预测中的应用

    Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit. (arXiv:2304.07025v1 [stat.ML])

    [http://arxiv.org/abs/2304.07025](http://arxiv.org/abs/2304.07025)

    本文介绍了连续时间自回归递归神经网络(CTRNNs)的应用, 通过连续演化来解决非规则采样的时间序列问题, 以概率预测临床监护设置中的血糖水平。

    

    在很多应用中，非规则采样的时间序列是常见的，包括医学领域。这提供了模型选择的挑战，通常需要插补或类似策略。连续时间自回归递归神经网络(CTRNNs) 是一种深度学习模型，通过在观测值之间融合隐藏状态的连续演化来解决这些问题。这是通过神经常微分方程(ODE)或神经流层来实现的。在本文中，我们概述了这些模型，包括为应对 ongoing medical interventions 等问题而提出的各种结构。此外，我们证明了这些模型在使用电子病历和模拟数据进行血糖概率预测的临床监护设置中的应用。实验证实了添加神经ODE或神经流层的一般性。

    Irregularly measured time series are common in many of the applied settings in which time series modelling is a key statistical tool, including medicine. This provides challenges in model choice, often necessitating imputation or similar strategies. Continuous time autoregressive recurrent neural networks (CTRNNs) are a deep learning model that account for irregular observations through incorporating continuous evolution of the hidden states between observations. This is achieved using a neural ordinary differential equation (ODE) or neural flow layer. In this manuscript, we give an overview of these models, including the varying architectures that have been proposed to account for issues such as ongoing medical interventions. Further, we demonstrate the application of these models to probabilistic forecasting of blood glucose in a critical care setting using electronic medical record and simulated data. The experiments confirm that addition of a neural ODE or neural flow layer general
    
[^8]: 高维函数时间序列中结构性突变的检测和估计

    Detection and Estimation of Structural Breaks in High-Dimensional Functional Time Series. (arXiv:2304.07003v1 [stat.ME])

    [http://arxiv.org/abs/2304.07003](http://arxiv.org/abs/2304.07003)

    本文提出了一种新的高维函数时间序列突变检验统计量，又保证功率性能，又扩大了具备功率的区域；同时，引入了一种易于实现的聚类算法和信息准则，以一致地估计未知组数和成员资格，以改善后聚类的收敛性质。

    

    本文考虑检测和估计高维函数时间序列的异质均值函数中的突变，允许横向相关和时间依赖关系。提出了一种新的检验统计量，结合了函数CUSUM统计量和功率增强组件，其渐近零分布理论可与单个函数时间序列的传统CUSUM理论相媲美。特别地，额外的功率增强组件扩大了所提出的检验具备功率的区域，当替代假设中的突变稀疏时，结果具有稳定的功率性能。此外，我们针对具有异质突变点的对象施加潜在的组结构，并引入一种易于实现的聚类算法和信息准则，以一致地估计未知组数和成员资格。随后，估计的组结构可以改善后聚类的收敛性质。

    In this paper, we consider detecting and estimating breaks in heterogeneous mean functions of high-dimensional functional time series which are allowed to be cross-sectionally correlated and temporally dependent. A new test statistic combining the functional CUSUM statistic and power enhancement component is proposed with asymptotic null distribution theory comparable to the conventional CUSUM theory derived for a single functional time series. In particular, the extra power enhancement component enlarges the region where the proposed test has power, and results in stable power performance when breaks are sparse in the alternative hypothesis. Furthermore, we impose a latent group structure on the subjects with heterogeneous break points and introduce an easy-to-implement clustering algorithm with an information criterion to consistently estimate the unknown group number and membership. The estimated group structure can subsequently improve the convergence property of the post-clusterin
    
[^9]: 基于贝叶斯渐近性的Gibbs抽样器复杂性

    Complexity of Gibbs samplers through Bayesian asymptotics. (arXiv:2304.06993v1 [stat.CO])

    [http://arxiv.org/abs/2304.06993](http://arxiv.org/abs/2304.06993)

    本文介绍了一种基于贝叶斯渐近性工具的方法，用于分析Gibbs抽样器的混合时间的渐近行为，并在随机数据生成假设下获得了对于具有通用似然函数的广泛的二级模型的无维度收敛结果。

    

    Gibbs抽样器是用于近似来自贝叶斯分层模型的后验分布的流行算法。尽管它们非常流行且表现良好，但是关于它们的可扩展性或不可扩展性的定量理论结果相对较少，例如，比基于梯度的抽样方法要少得多。本文介绍了一种基于贝叶斯渐近性工具的新技术，用于分析Gibbs抽样器的混合时间的渐近行为。我们将我们的方法应用于高维分层模型，并在随机数据生成假设下获得了对于具有通用似然函数的广泛的二级模型的无维度收敛结果。讨论了具有高斯、二项式和分类似然的具体示例。

    Gibbs samplers are popular algorithms to approximate posterior distributions arising from Bayesian hierarchical models. Despite their popularity and good empirical performances, however, there are still relatively few quantitative theoretical results on their scalability or lack thereof, e.g. much less than for gradient-based sampling methods. We introduce a novel technique to analyse the asymptotic behaviour of mixing times of Gibbs Samplers, based on tools of Bayesian asymptotics. We apply our methodology to high dimensional hierarchical models, obtaining dimension-free convergence results for Gibbs samplers under random data-generating assumptions, for a broad class of two-level models with generic likelihood function. Specific examples with Gaussian, binomial and categorical likelihoods are discussed.
    
[^10]: 评估-优化方法与集成评估优化法：基于随机优势的观点

    Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])

    [http://arxiv.org/abs/2304.06833](http://arxiv.org/abs/2304.06833)

    本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。

    

    在数据驱动的随机优化中，除了需要优化任务，还需要从数据中估计潜在分布的模型参数。最近的文献表明，通过选择导致最佳经验目标性能的模型参数，可以集成估计和优化过程。当模型被错误地指定时，这种集成方法可以很容易地显示出优于简单的“先估计再优化”的方法。本文认为，在模型类足够丰富以涵盖真实情况的情况下，对于非线性问题，两种方法之间的性能排序在强烈的意义下被颠倒。在受限条件和当上下文特征可用时，类似的结果也成立。

    In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
    
[^11]: 通用图和具有局部性质的图中的成对比较排序

    Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality. (arXiv:2304.06821v1 [stat.ML])

    [http://arxiv.org/abs/2304.06821](http://arxiv.org/abs/2304.06821)

    本文研究了通用图和具有局部性质的图中的成对比较排序问题。研究表明，最大似然估计（MLE）可以实现符合Cram\'er-Rao下界的逐元估计误差。同时，文章还确定了局部性不会影响的条件，并提出了分治算法以实现类似保障。

    

    本技术报告研究了经典的Bradley-Terry-Luce（BTL）模型中的成对比较排序问题，重点关注得分估计。对于通用图，我们表明，通过足够多的样本，最大似然估计（MLE）可以实现一个符合Cram\'er-Rao下界的逐元估计误差，这可以用有效电阻来说明；我们分析的关键是统计估计和通过预处理梯度下降进行迭代优化之间的联系。我们还特别关注具有局部性质的图，其中仅相邻项之间可以连接边缘；我们的分析确定了局部性不会影响的条件，即在图中距离较远的一对项目之间进行得分比较几乎与比较相邻项目对一对项目相似。我们进一步探讨了分治算法，即使在最稀疏的样本区域内，也可以证明能够实现类似的保证，并享受本地方法的计算效率。

    This technical report studies the problem of ranking from pairwise comparisons in the classical Bradley-Terry-Luce (BTL) model, with a focus on score estimation. For general graphs, we show that, with sufficiently many samples, maximum likelihood estimation (MLE) achieves an entrywise estimation error matching the Cram\'er-Rao lower bound, which can be stated in terms of effective resistances; the key to our analysis is a connection between statistical estimation and iterative optimization by preconditioned gradient descent. We are also particularly interested in graphs with locality, where only nearby items can be connected by edges; our analysis identifies conditions under which locality does not hurt, i.e. comparing the scores between a pair of items that are far apart in the graph is nearly as easy as comparing a pair of nearby items. We further explore divide-and-conquer algorithms that can provably achieve similar guarantees even in the regime with the sparsest samples, while enj
    
[^12]: 流式数据主动计费标注

    Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v1 [cs.LG])

    [http://arxiv.org/abs/2304.06808](http://arxiv.org/abs/2304.06808)

    本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。

    

    我们研究了主动标记流数据问题，其中主动学习者面临一系列数据点，并必须通过昂贵的实验精心选择哪些点进行标记，此类问题常常出现在医疗和天文学等领域。我们首先研究的是数据输入属于$K$个离散分布之一的情况，并通过损失函数形式化描述此问题，该损失函数捕捉了标记成本和预测误差。当标记成本为$B$时，我们的算法通过选择标记点，仅在不确定性大于时间和成本相关阈值时进行，可以在$T$轮之后实现$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。我们还提供了更细致的上界，证明了在到达模式更有利时，算法可以适应到达模式，并实现更好的性能。我们还补充了两个上界的匹配下界。接下来，我们研究了在流数据具有不确定性分布的情况下标记流数据的问题，并提供与前面情况类似的结果。

    We study actively labeling streaming data, where an active learner is faced with a stream of data points and must carefully choose which of these points to label via an expensive experiment. Such problems frequently arise in applications such as healthcare and astronomy. We first study a setting when the data's inputs belong to one of $K$ discrete distributions and formalize this problem via a loss that captures the labeling cost and the prediction error. When the labeling cost is $B$, our algorithm, which chooses to label a point if the uncertainty is larger than a time and cost dependent threshold, achieves a worst-case upper bound of $O(B^{\frac{1}{3}} K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuanced upper bound which demonstrates that the algorithm can adapt to the arrival pattern, and achieves better performance when the arrival pattern is more favorable. We complement both upper bounds with matching lower bounds. We next study this pr
    
[^13]: 用于黑盒变分推断的样本平均估计方法

    Sample Average Approximation for Black-Box VI. (arXiv:2304.06803v1 [cs.LG])

    [http://arxiv.org/abs/2304.06803](http://arxiv.org/abs/2304.06803)

    该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。

    

    我们提出了一种新的方法，用于解决随机梯度上升的困难，包括选择步长的任务。我们的方法涉及使用一系列样本平均估计问题（SAA）。通过将随机优化问题转化为确定性问题，SAA逼近了随机优化问题的解。我们使用拟牛顿方法和线性搜索来解决每个确定性优化问题，并提出了一种启发式策略来自动选择超参数。我们的实验表明，我们的方法简化了变分推断问题，并实现了比现有方法更快的性能。

    We present a novel approach for black-box VI that bypasses the difficulties of stochastic gradient ascent, including the task of selecting step-sizes. Our approach involves using a sequence of sample average approximation (SAA) problems. SAA approximates the solution of stochastic optimization problems by transforming them into deterministic ones. We use quasi-Newton methods and line search to solve each deterministic optimization problem and present a heuristic policy to automate hyperparameter selection. Our experiments show that our method simplifies the VI problem and achieves faster performance than existing methods.
    
[^14]: 二元积分布的多项式时间和纯差分隐私估计器

    A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])

    [http://arxiv.org/abs/2304.06787](http://arxiv.org/abs/2304.06787)

    本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。

    

    我们提出了第一个ε-差分隐私、计算有效的算法，可以在总变化距离下准确地估计$\{0,1\}^d$上的乘积分布的均值，同时在多项式对数因子内获得了最优的样本复杂度。之前的工作要么在更弱的隐私概念下有效地解决了这个问题，要么在指数级运行时间内最优地解决了这个问题。

    We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
    
[^15]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^16]: 为一致性预测的后选推理：权衡精度和覆盖范围

    Post-selection Inference for Conformal Prediction: Trading off Coverage for Precision. (arXiv:2304.06158v1 [stat.ME])

    [http://arxiv.org/abs/2304.06158](http://arxiv.org/abs/2304.06158)

    本论文提出一种使用无分布信赖带的 uniform conformal inference 算法，实现任意数据相关误覆盖水平的有限样本预测保证的统一一致性推理。

    

    一致性推理在为具有有限样本保证的黑盒机器学习预测算法提供不确定性量化上发挥了重要作用。传统上，一致性预测推理需要独立于数据的错误覆盖水平规范。在实际应用中，人们可能会在计算出预测集之后更新错误覆盖水平。例如，在二元分类的情况下，分析人员可能会从一个95％的预测集开始，并发现大多数预测集包含所有输出类别。如果两个类别都不可取，分析人员可能会考虑80％的预测集。具有数据相关的误覆盖水平和保证覆盖范围的预测集的构建可以被认为是一个后选推理问题。在这项工作中，我们使用无分布信赖带，开发了具有任意数据相关误覆盖水平的有限样本预测保证的统一一致性推理。

    Conformal inference has played a pivotal role in providing uncertainty quantification for black-box ML prediction algorithms with finite sample guarantees. Traditionally, conformal prediction inference requires a data-independent specification of miscoverage level. In practical applications, one might want to update the miscoverage level after computing the prediction set. For example, in the context of binary classification, the analyst might start with a $95\%$ prediction sets and see that most prediction sets contain all outcome classes. Prediction sets with both classes being undesirable, the analyst might desire to consider, say $80\%$ prediction set. Construction of prediction sets that guarantee coverage with data-dependent miscoverage level can be considered as a post-selection inference problem. In this work, we develop uniform conformal inference with finite sample prediction guarantee with arbitrary data-dependent miscoverage levels using distribution-free confidence bands f
    
[^17]: 混合移动平均场引导的时空数据学习

    Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.00736](http://arxiv.org/abs/2301.00736)

    本论文提出了一种理论引导机器学习方法，采用广义贝叶斯算法进行混合移动平均场引导的时空数据建模，可以进行因果未来预测。

    

    受到混合移动平均场的影响，时空数据的建模是一个多功能的技巧。但是，它们的预测分布通常不可访问。在这个建模假设下，我们定义了一种新的理论引导机器学习方法，采用广义贝叶斯算法进行预测。我们采用Lipschitz预测器（例如线性模型或前馈神经网络），并通过最小化沿空间和时间维度串行相关的数据的新型PAC贝叶斯界限来确定一个随机估计值。进行因果未来预测是我们方法的一个亮点，因为它适用于具有短期和长期相关性的数据。最后，我们通过展示线性预测器和模拟STOU过程的时空数据的示例来展示学习方法的性能。

    Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
    
[^18]: FaiREE：具有有限样本和无分布保证的公平分类算法

    FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.15072](http://arxiv.org/abs/2211.15072)

    本研究提出了FaiREE算法，它是一种可满足群体公平性约束的公平分类算法，并且具有有限样本和无分布理论保证。在实验中表现优异。

    

    算法公平性在机器学习研究中发挥着越来越重要的作用。已经提出了几种群体公平性概念和算法。然而，现有公平分类方法的公平保证主要依赖于特定的数据分布假设，通常需要大样本量，并且在样本量较小的情况下可能会违反公平性，而这在实践中经常发生。本文提出了FaiREE算法，它是一种公平分类算法，可以在有限样本和无分布理论保证下满足群体公平性约束。FaiREE可以适应各种群体公平性概念（例如，机会平等，平衡几率，人口统计学平衡等）并实现最佳准确性。这些理论保证进一步得到了对合成和实际数据的实验支持。FaiREE表现出比最先进的算法更好的性能。

    Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
    
[^19]: 监测考虑混淆医疗干预时基于机器学习的风险预测算法

    Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions. (arXiv:2211.09781v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.09781](http://arxiv.org/abs/2211.09781)

    在医疗保健中，通过监测条件性表现，可以有效地监测考虑混淆医疗干预时基于机器学习的风险预测算法。

    

    在医疗保健中，基于机器学习的风险预测模型的性能监测受混淆医疗干预（CMI）的影响，因为临床医生更可能为高风险患者提供预防性治疗并改变算法所预测的目标。忽略CMI并只监测未接受治疗的患者可能会导致类型I错误的膨胀，不过我们证明如果监控条件性表现，并且符合条件交换性或时间不变选择偏差，则仍然可以进行有效推断。

    Performance monitoring of machine learning (ML)-based risk prediction models in healthcare is complicated by the issue of confounding medical interventions (CMI): when an algorithm predicts a patient to be at high risk for an adverse event, clinicians are more likely to administer prophylactic treatment and alter the very target that the algorithm aims to predict. A simple approach is to ignore CMI and monitor only the untreated patients, whose outcomes remain unaltered. In general, ignoring CMI may inflate Type I error because (i) untreated patients disproportionally represent those with low predicted risk and (ii) evolution in both the model and clinician trust in the model can induce complex dependencies that violate standard assumptions. Nevertheless, we show that valid inference is still possible if one monitors conditional performance and if either conditional exchangeability or time-constant selection bias hold. Specifically, we develop a new score-based cumulative sum (CUSUM) m
    
[^20]: 稀疏受限最优输运

    Sparsity-Constrained Optimal Transport. (arXiv:2209.15466v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15466](http://arxiv.org/abs/2209.15466)

    这篇论文提出了一种新的带有输运计划显式基数约束的 OT 方法，以确保每个输入令牌都与少量专家匹配，从而提高模型的可解释性。

    

    正则化的最优输运 (OT) 现在越来越多地被用作神经网络中的损失或匹配层。使用熵正则化 OT 可以使用 Sinkhorn 算法计算，但它会产生完全密集的运输计划，这意味着所有源都与所有目标（分数）匹配。为了解决这个问题，几篇论文研究了二次正则化。这种正则化保留了稀疏性，并导致了无约束和平滑（半）对偶目标，可以使用现有的梯度方法进行求解。不幸的是，二次正则化不能直接控制运输计划的基数（非零数的数量）。本文提出了一种新的带有输运计划显式基数约束的 OT 方法。我们的工作是由对稀疏专家混合物的应用所驱动的，其中 OT 可以用于将输入令牌（例如图像补丁）与专家模型（例如神经网络）进行匹配。基数限制可以确保每个输入令牌与少量专家匹配，这有助于解释混合权重并构建更具可解释性的模型。我们推导了一种基于 Proximal 梯度方法的稀疏受限 OT 问题，并在合成和真实数据实验中展示了其有效性。

    Regularized optimal transport (OT) is now increasingly used as a loss or as a matching layer in neural networks. Entropy-regularized OT can be computed using the Sinkhorn algorithm but it leads to fully-dense transportation plans, meaning that all sources are (fractionally) matched with all targets. To address this issue, several works have investigated quadratic regularization instead. This regularization preserves sparsity and leads to unconstrained and smooth (semi) dual objectives, that can be solved with off-the-shelf gradient methods. Unfortunately, quadratic regularization does not give direct control over the cardinality (number of nonzeros) of the transportation plan. We propose in this paper a new approach for OT with explicit cardinality constraints on the transportation plan. Our work is motivated by an application to sparse mixture of experts, where OT can be used to match input tokens such as image patches with expert models such as neural networks. Cardinality constraint
    
[^21]: 带理论支持的样本重用的广义策略提升算法

    Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse. (arXiv:2206.13714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13714](http://arxiv.org/abs/2206.13714)

    研究提出了一种广义策略提升算法，结合了在线方法的策略提升保证和离线策略算法通过样本重用有效利用数据的效率。

    

    数据驱动的学习控制方法具有改善复杂系统运行的潜力，而基于模型的深度强化学习代表了一种流行的数据驱动控制方法。然而，现有的算法类别在实际控制部署的两个重要要求之间存在权衡：（i）实际性能保证和（ii）数据效率。离线策略算法通过样本重用有效利用数据，但缺乏理论保证，而在线策略算法保证了训练期间的近似策略改进，但受到高样本复杂度的影响。为了平衡这些竞争目标，我们开发了一类广义策略提升算法，它结合了在线方法的策略提升保证和样本重用的效率。通过对来自DeepMind C的多种连续控制任务进行 extensive 的实验分析，我们证明了这种新类算法的益处。

    Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind C
    
[^22]: 正则化对成员推理中维度的祝福

    A Blessing of Dimensionality in Membership Inference through Regularization. (arXiv:2205.14055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14055](http://arxiv.org/abs/2205.14055)

    研究探讨了过度参数化对成员推理攻击易受攻击性的影响，发现适当的正则化可以在增加模型参数数量的同时提高隐私和性能，消除了隐私与效用之间的权衡问题。

    

    这项研究探讨了过度参数化对分类器在成员推理攻击中的易受攻击性的影响。我们首先展示了模型参数数量如何引发隐私和效用的权衡问题：增加参数数量通常会提高泛化性能，但会降低隐私保障。然而，令人惊讶的是，我们随后证明，如果与适当的正则化相结合，增加模型参数数量实际上可以同时增加其隐私和性能，从而消除隐私与效用之间的权衡问题。在理论上，我们通过对双重特征集合设置的岭回归逻辑回归进行的实验来证明了这一神奇现象。在我们的理论探索之后，我们开发了一种新的leave-one-out分析工具，以精确刻画线性分类器对最佳成员推理攻击的易受攻击性。最后我们还在实验中进行了验证。

    Is overparameterization a privacy liability? In this work, we study the effect that the number of parameters has on a classifier's vulnerability to membership inference attacks. We first demonstrate how the number of parameters of a model can induce a privacy--utility trade-off: increasing the number of parameters generally improves generalization performance at the expense of lower privacy. However, remarkably, we then show that if coupled with proper regularization, increasing the number of parameters of a model can actually simultaneously increase both its privacy and performance, thereby eliminating the privacy--utility trade-off. Theoretically, we demonstrate this curious phenomenon for logistic regression with ridge regularization in a bi-level feature ensemble setting. Pursuant to our theoretical exploration, we develop a novel leave-one-out analysis tool to precisely characterize the vulnerability of a linear classifier to the optimal membership inference attack. We empirically
    
[^23]: 使随机梯度下降法无参数化

    Making SGD Parameter-Free. (arXiv:2205.02160v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2205.02160](http://arxiv.org/abs/2205.02160)

    该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。

    

    我们开发了一种无参数随机凸优化（SCO）算法，其收敛速度仅比对应的已知参数设置的最优速度多一个双对数因子。相比之下，先前已知的无参数SCO的最佳速度是基于在线无参数后悔界的，与已知参数的对应方法相比包含不可避免的额外对数项。我们的算法具有概念上的简单性，具有高概率保证，并且部分适应未知梯度范数、平滑性和强凸性。我们的成果的核心是SGD步长选择的新型无参数证书，以及假设在SGD迭代上没有先验界限的时间一致集中结果。

    We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameter-free SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.
    
[^24]: 基于图的机器学习改进了 JIT 缺陷预测

    Graph-Based Machine Learning Improves Just-in-Time Defect Prediction. (arXiv:2110.05371v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2110.05371](http://arxiv.org/abs/2110.05371)

    该论文使用基于图的机器学习技术，构建了一个由开发人员和源文件组成的贡献图，利用这个图提取的特征，改进了 JIT 缺陷预测，比传统的机器学习方法更好地预测了易出现缺陷的更改。

    

    当今软件不断增加的复杂性需要数千名开发人员的贡献。由于复杂的合作结构，开发人员更可能引入易出现缺陷的更改，导致软件故障。确定这些易出现缺陷的更改被引入的时间已经变得具有挑战性，而使用传统的机器学习方法进行这些决策似乎已经达到了瓶颈。在本研究中，我们构建了由开发人员和源文件组成的贡献图来捕捉构建软件所需的微妙复杂性。通过利用这些贡献图，我们的研究展示了利用基于图的机器学习改进 JIT 缺陷预测的潜力。我们假设从贡献图中提取的特征可能比从软件特征派生的本质特征更好地预测易出现缺陷的更改。我们使用基于图的机器学习来分类表示开发人员之间交互的边，以证实我们的假设。

    The increasing complexity of today's software requires the contribution of thousands of developers. This complex collaboration structure makes developers more likely to introduce defect-prone changes that lead to software faults. Determining when these defect-prone changes are introduced has proven challenging, and using traditional machine learning (ML) methods to make these determinations seems to have reached a plateau. In this work, we build contribution graphs consisting of developers and source files to capture the nuanced complexity of changes required to build software. By leveraging these contribution graphs, our research shows the potential of using graph-based ML to improve Just-In-Time (JIT) defect prediction. We hypothesize that features extracted from the contribution graphs may be better predictors of defect-prone changes than intrinsic features derived from software characteristics. We corroborate our hypothesis using graph-based ML for classifying edges that represent 
    
[^25]: 变分扩散模型

    Variational Diffusion Models. (arXiv:2107.00630v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.00630](http://arxiv.org/abs/2107.00630)

    该论文提出了一族基于扩散的生成模型，通过对噪声时间表的有效优化，这些模型在图像密度估计基准测试中获得了最先进的可能性和视觉质量。

    

    基于扩散的生成模型已经展示了令人印象深刻的合成能力，但它们也能成为很好的基于可能性的模型吗？我们回答了这个问题，并引入了一族基于扩散的生成模型，在标准图像密度估计基准上获得了最先进的可能性。与其他基于扩散的模型不同，我们的方法允许与模型的其余部分共同有效地优化噪声时间表。我们表明，在扩散数据的信噪比方面，变分下限（VLB）简化为一个非常简短的表达式，从而改善了我们对该模型类的理论理解。利用这一见解，我们证明了文献中提出的几个模型之间的等价性。此外，我们表明，连续时间VLB对于噪声时间表是不变的，除了其端点处的信噪比。这使我们能够学习一种噪声时间表，该表针对信噪比最小化变分下限。我们的实验证明，我们的方法在包括CIFAR-10， CelebA-HQ和FFHQ在内的几个基准测试上实现了良好的性能，无论是可能性还是视觉质量。

    Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the vari
    
[^26]: 使用岭回归法的时变参数模型

    Time-Varying Parameters as Ridge Regressions. (arXiv:2009.00401v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2009.00401](http://arxiv.org/abs/2009.00401)

    该论文提出了一种实际上是基于岭回归的时变参数模型，这比传统的状态空间方法计算更快，调整更容易，有助于研究经济结构性变化。

    

    时变参数模型(TVPs)经常被用于经济学中来捕捉结构性变化。我强调了一个被忽视的事实——这些实际上是岭回归。这使得计算、调整和实现比状态空间范式更容易。在高维情况下，解决等价的双重岭问题的计算非常快,关键的“时间变化量”通常是由交叉验证来调整的。使用两步回归岭回归来处理不断变化的波动性。我考虑了基于稀疏性(算法选择哪些参数变化, 哪些不变)和降低秩约束的扩展(变化与因子模型相关联)。为了展示这种方法的有用性, 我使用它来研究加拿大货币政策的演变, 并使用大规模时变局部投影估计约4600个TVPs, 这一任务完全可以利用这种新方法完成。

    Time-varying parameters (TVPs) models are frequently used in economics to capture structural change. I highlight a rather underutilized fact -- that these are actually ridge regressions. Instantly, this makes computations, tuning, and implementation much easier than in the state-space paradigm. Among other things, solving the equivalent dual ridge problem is computationally very fast even in high dimensions, and the crucial "amount of time variation" is tuned by cross-validation. Evolving volatility is dealt with using a two-step ridge regression. I consider extensions that incorporate sparsity (the algorithm selects which parameters vary and which do not) and reduced-rank restrictions (variation is tied to a factor model). To demonstrate the usefulness of the approach, I use it to study the evolution of monetary policy in Canada using large time-varying local projections. The application requires the estimation of about 4600 TVPs, a task well within the reach of the new method.
    

