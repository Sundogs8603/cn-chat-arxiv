# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering.](http://arxiv.org/abs/2307.11030) | 这项工作首次从理论上理解了关系知识蒸馏，通过种群上的谱聚类，我们证明了关系知识蒸馏能够导致低聚类误差，并展示了它在半监督学习中的标签效率。 |
| [^2] | [Amortized Variational Inference: When and Why?.](http://arxiv.org/abs/2307.11018) | 本文研究了分期变分推断作为近似后验推断的一种通用替代方法，探讨了何时能够达到与传统的因子化变分推断相同的最优解。 |
| [^3] | [Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks.](http://arxiv.org/abs/2307.11013) | 本文介绍了流图学习（FML）的框架和实现细节，以及用于学习未知动力系统的基准问题。FML在部分观察系统中产生准确的预测模型，即使没有精确的数学模型。 |
| [^4] | [Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization.](http://arxiv.org/abs/2307.11007) | 本文研究发现，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。我们的结果表明，尖锐性与泛化之间的关系取决于数据分布和模型架构。 |
| [^5] | [Private Federated Learning with Autotuned Compression.](http://arxiv.org/abs/2307.10999) | 本文提出了一种在私有联邦学习中自动调整压缩率的新技术，无需手动设置或调整，通过使用安全聚合和差分隐私提供了可证明的隐私保证，并在真实世界数据集上展示了其有效性。 |
| [^6] | [Dense Sample Deep Learning.](http://arxiv.org/abs/2307.10991) | 密集样本深度学习是一种针对深度学习网络的研究方法，旨在揭示学习机制和表示的未知特性，并解决大规模数据和隐藏单元存在的问题。 |
| [^7] | [Nonlinear Meta-Learning Can Guarantee Faster Rates.](http://arxiv.org/abs/2307.10870) | 非线性元学习可以保证更快的收敛速度。 |
| [^8] | [Addressing caveats of neural persistence with deep graph persistence.](http://arxiv.org/abs/2307.10865) | 本文发现网络权重的方差和大权重的空间集中是影响神经持久性的主要因素，并提出了将神经持久性扩展到整个神经网络的深度图持久性测量方法。 |
| [^9] | [Label Calibration for Semantic Segmentation Under Domain Shift.](http://arxiv.org/abs/2307.10842) | 该论文介绍了一种在域偏移下使用标签校准方法来提升语义分割模型性能的方式。 |
| [^10] | [Feed-Forward Source-Free Domain Adaptation via Class Prototypes.](http://arxiv.org/abs/2307.10787) | 这项工作提出了一种前馈方法来解决源无关域适应问题，通过计算类的原型来处理域偏移，相较于使用预训练模型，该方法在精度和时间上都有显著提升。 |
| [^11] | [Mitigating Voter Attribute Bias for Fair Opinion Aggregation.](http://arxiv.org/abs/2307.10749) | 本文研究了通过考虑选民属性来实现公正的意见汇总的方法，并评估了汇总结果的公正性。 |
| [^12] | [Long-Tail Theory under Gaussian Mixtures.](http://arxiv.org/abs/2307.10736) | 该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。 |
| [^13] | [Conditional expectation network for SHAP.](http://arxiv.org/abs/2307.10654) | 这项工作提出了一种用于计算条件版本的（代理）神经网络方法，该方法可以有效地解释神经网络和其他回归模型的预测结果，并考虑特征之间的依赖关系。同时，该方法还可以应用于复杂回归模型的分析，并提供正确的偏依赖图表示。 |
| [^14] | [Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions.](http://arxiv.org/abs/2307.10644) | 本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。 |
| [^15] | [Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis.](http://arxiv.org/abs/2307.10596) | 本论文基于集成学习方法，通过贝叶斯超参数敏感性分析，提出了一种用于物联网网络安全异常检测的方法，该方法充分利用了物联网数据的异构性和多样性特征。 |
| [^16] | [Multiply Robust Estimator Circumvents Hyperparameter Tuning of Neural Network Models in Causal Inference.](http://arxiv.org/abs/2307.10536) | 这项研究展示了一种名为多重稳健估计器的方法，通过在一个单一的估计器中利用多个模型，解决了选择神经网络模型中最佳超参数集的困难，以实现在因果推断中的平均处理效应估计。 |
| [^17] | [A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints.](http://arxiv.org/abs/2307.10459) | 提出了一种计算简单的方法来实现具有硬约束输出的神经网络。该方法通过映射隐藏参数向量到一个符合约束集的点实现约束，并通过附加的神经网络层来进行映射。该方法还可以处理不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况，并且可以处理不同类型的约束，包括线性和二次约束、等式约束和动态约束。 |
| [^18] | [Determination of the critical points for systems of directed percolation class using machine learning.](http://arxiv.org/abs/2307.10456) | 本研究使用机器学习算法研究了两个模型的非平衡态相变，并使用卷积神经网络和DBSCAN算法确定了有向渗透模型和细胞自动机模型的临界点。 |
| [^19] | [A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data.](http://arxiv.org/abs/2307.10437) | 这项研究提出了一种贝叶斯编程方法，用于校准和验证车辆跟随模型，以捕捉和复制工作区内外的驾驶行为。 |
| [^20] | [A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks.](http://arxiv.org/abs/2307.10436) | 我们提出了一种基于矩阵集合卡尔曼滤波器的多臂神经网络，可以在样本量太小无法训练多臂神经网络时充分近似深度神经网络。该方法还可以对从长短期记忆网络（LSTM）获得的预测赋予不确定性，具有良好的覆盖范围。 |
| [^21] | [Properties of Discrete Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.10352) | 本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。 |
| [^22] | [Analyzing sports commentary in order to automatically recognize events and extract insights.](http://arxiv.org/abs/2307.10303) | 通过分析多种体育赛事的实时评论，利用自然语言处理技术自动识别主要行动，并通过分类和情感分析提取洞见。 |
| [^23] | [Causality-oriented robustness: exploiting general additive interventions.](http://arxiv.org/abs/2307.10299) | 本文提出了一种名为DRIG的方法，通过利用训练数据中的一般性可加干预，在预测模型中结合了内分布预测和因果性，从而实现了对未见干预的鲁棒预测。 |
| [^24] | [An IPW-based Unbiased Ranking Metric in Two-sided Markets.](http://arxiv.org/abs/2307.10204) | 这项研究提出了一种基于IPW的无偏排序度量方法，针对双边市场中用户之间的偏见相互作用，解决了位置偏见和两个用户群体的位置偏差问题。 |
| [^25] | [Privacy Amplification via Importance Sampling.](http://arxiv.org/abs/2307.10187) | 通过重要性采样进行隐私放大，可以同时增强隐私保护和提高效用。我们提供了一个一般的结果来量化选择概率权重对隐私放大的影响，并展示了异质采样概率可以在保持子采样大小不变的情况下获得更好的隐私和效用。 |
| [^26] | [Impatient Bandits: Optimizing for the Long-Term Without Delay.](http://arxiv.org/abs/2307.09943) | 这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。 |
| [^27] | [Multi-view self-supervised learning for multivariate variable-channel time series.](http://arxiv.org/abs/2307.09614) | 本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。 |
| [^28] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^29] | [A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models.](http://arxiv.org/abs/2307.05251) | 本研究提出了一种随机优化方法，用于解决稳健密度功率分歧（DPD）在一般参数密度模型中的计算复杂性问题，并通过应用传统的随机优化理论来验证其有效性。 |
| [^30] | [Understanding Uncertainty Sampling.](http://arxiv.org/abs/2307.02719) | 本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。 |
| [^31] | [Provably Efficient UCB-type Algorithms For Learning Predictive State Representations.](http://arxiv.org/abs/2307.00405) | 这篇论文提出了第一种已知的UCB类型方法用于学习预测状态表示（PSRs），并设计了一个新的奖励项来上界t |
| [^32] | [Correcting Underrepresentation and Intersectional Bias for Fair Classification.](http://arxiv.org/abs/2306.11112) | 本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。 |
| [^33] | [Invariant Causal Set Covering Machines.](http://arxiv.org/abs/2306.04777) | 本文提出了一种名为不变因果集覆盖机的算法，它避免了产生虚假关联，可以在多项式时间内识别感兴趣变量的因果父节点。 |
| [^34] | [Sequential Predictive Two-Sample and Independence Testing.](http://arxiv.org/abs/2305.00143) | 本文提出了一种基于预测的赌博策略来解决高维或结构化数据下非参数双样本和独立性检验问题。 |
| [^35] | [Chordal Averaging on Flag Manifolds and Its Applications.](http://arxiv.org/abs/2303.13501) | 本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。 |
| [^36] | [Towards a Complete Analysis of Langevin Monte Carlo: Beyond Poincar\'e Inequality.](http://arxiv.org/abs/2303.03589) | 本研究通过建立上下界限来超越Poincaré不等式，从而推动Langevin扩散和Langevin Monte Carlo（LMC）在弱Poincaré不等式下的研究。研究结果可以量化初始值对LMC算法性能的影响。 |
| [^37] | [From Graph Generation to Graph Classification.](http://arxiv.org/abs/2302.07989) | 本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。 |
| [^38] | [Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients.](http://arxiv.org/abs/2212.14319) | 该论文提出了一种名为EPGP的高斯过程先验，用于线性偏微分方程系统，并且构造了反映标准谱方法的GP核函数。该方法可以推断线性PDE系统的可能解，并具有算法性强、普适性广、适用于大数据集的稀疏版本。 |
| [^39] | [Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning.](http://arxiv.org/abs/2212.12658) | 提出了一种改进方差网络不确定性量化的方法，通过树状结构学习将特征空间分割为多个区域，并使用区域特定的神经网络预测均值和方差来量化不确定性。该方法利用新的分裂准则，在计算上友好且不需要修剪，还可以构建集合版本来估计总不确定性。 |
| [^40] | [Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments.](http://arxiv.org/abs/2211.10515) | 本文提出了一种基于结构因果模型的回顾中的好奇心方法，用于稀疏奖励或无奖励环境中的探索问题。该方法学习未来的表征，以捕捉每个结果的不可预测部分，并将其用作预测的额外输入，从而获得鲁棒的内在奖励。 |
| [^41] | [Leveraging Offline Data in Online Reinforcement Learning.](http://arxiv.org/abs/2211.04974) | 在这项工作中，我们研究了在线强化学习中利用离线数据的设置，为具有线性结构的MDPs确定了所需的在线样本数量，并提供了实现这一目标的性质和算法。 |
| [^42] | [Representing Random Utility Choice Models with Neural Networks.](http://arxiv.org/abs/2207.12877) | 本论文提出了一种基于神经网络的离散选择模型类，RUMnets，可以近似表示任何随机效用最大化推导出的模型，并且在选择数据上有良好的预测能力。 |
| [^43] | [Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics.](http://arxiv.org/abs/2207.12395) | 我们利用大样本渐近理论对随机梯度算法进行调优，发现使用固定的大步长进行迭代平均可以鲁棒地优化算法，且具有和MLE抽样分布协方差成比例的鲁棒性。我们还提出了一种类似于Bernstein-von Mises的定理用于指导调优，包括针对模型错误规范鲁棒的广义后验。数值实验验证了我们的结果和建议在实际有限样本情况下的有效性。这些成果为分析其他随机梯度Markov Chain Monte Carlo算法提供了基础。 |
| [^44] | [Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design.](http://arxiv.org/abs/2207.02575) | 本文研究了在线实验设计方法在线性MDPs中的应用，提出了一种算法\textsc{Pedel}，该算法在RL中的函数逼近设置下实现了细粒度的依赖于实例的复杂度度量，相对于最低遗憾、最小最大最优算法具有明显收益。 |
| [^45] | [Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case.](http://arxiv.org/abs/2206.08309) | Pythae是一个开源Python库，提供了统一的生成自编码器模型实现和框架，可用于进行多种任务的案例研究基准测试。 |
| [^46] | [Pre-trained Perceptual Features Improve Differentially Private Image Generation.](http://arxiv.org/abs/2205.12900) | 该论文提出了一种利用预先训练的感知特征，通过最小化MMD（最大均值差异）来提高差分隐私图像生成的性能，并成功地生成了CIFAR10级别的图像。 |
| [^47] | [The Unreasonable Effectiveness of Deep Evidential Regression.](http://arxiv.org/abs/2205.10060) | 深度证据回归是一种通过学习证据分布来处理不确定性的方法，在不确定性推理中显示出相对于传统方法和贝叶斯神经网络的优势。然而，它并非精确的不确定性量化方法，而是一种启发式方法。 |
| [^48] | [Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves.](http://arxiv.org/abs/2111.03950) | 本论文提出了一种基于核岭回归的简单非参数估计方法，可以用于估计介导和时变剂量响应曲线。通过引入序贯核嵌入技术，我们实现了对复杂因果估计的简化。通过模拟实验和真实数据的估计结果，证明了该方法的强大性能和普适性。 |
| [^49] | [Model Selection for Generic Contextual Bandits.](http://arxiv.org/abs/2107.03455) | 我们提出了一种自适应背景上下文强化学习算法（ACB），可以在不知道真实模型类别的情况下进行模型选择，并且具有与已知模型类别算法相匹配的遗憾率。模型选择的代价仅对遗憾上界的二阶项有贡献，且具有直观的属性。 |
| [^50] | [Robust Principal Component Analysis: A Median of Means Approach.](http://arxiv.org/abs/2102.03403) | 本文提出了一种基于中位数求和原则的PCA方法，称为中位数求和主成分分析（MoMPCA），以应对异常值对PCA的影响，并在最小假设条件下实现了最优收敛速度。 |
| [^51] | [Implicit Multidimensional Projection of Local Subspaces.](http://arxiv.org/abs/2009.03259) | 本研究提出了一种使用隐式函数导数的可视化方法，旨在理解多维投影对局部子空间的影响。通过分析局部子空间的形状和方向信息，我们能够获取更多关于数据全局结构的洞察力，并将结果可视化为图形进行分析。 |

# 详细

[^1]: 集群感知的半监督学习：关系知识蒸馏可证明的学习聚类

    Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering. (arXiv:2307.11030v1 [stat.ML])

    [http://arxiv.org/abs/2307.11030](http://arxiv.org/abs/2307.11030)

    这项工作首次从理论上理解了关系知识蒸馏，通过种群上的谱聚类，我们证明了关系知识蒸馏能够导致低聚类误差，并展示了它在半监督学习中的标签效率。

    

    尽管基于关系的知识蒸馏在匹配教师和学生模型之间的特征关系方面取得了实证成功和实际意义，但对于各种知识蒸馏范式，其相应的理论解释仍然有限。在这项工作中，我们首次从理论上理解关系知识蒸馏（RKD），并重点关注半监督分类问题。我们首先将RKD视为教师模型揭示的由种群产生的图上的谱聚类。通过衡量预测和基本事实聚类之间差异的聚类误差概念，我们说明了种群上的RKD可证明地导致低聚类误差。此外，我们提供了对于有限无标签样本的RKD的样本复杂度界限。对于半监督学习，我们进一步通过集群感知的半监督学习框架展示了RKD的标签效率。

    Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning th
    
[^2]: 分期变分推断：何时以及为什么使用？

    Amortized Variational Inference: When and Why?. (arXiv:2307.11018v1 [stat.ML])

    [http://arxiv.org/abs/2307.11018](http://arxiv.org/abs/2307.11018)

    本文研究了分期变分推断作为近似后验推断的一种通用替代方法，探讨了何时能够达到与传统的因子化变分推断相同的最优解。

    

    分期变分推断（A-VI）是一种近似处理概率模型中的难以计算的后验分布的方法。A-VI的定义特点是学习一个全局推断函数，将每个观察映射到其局部潜变量的近似后验分布。这与更传统的分解（或均场）变分推断（F-VI）形成对比，后者直接学习每个潜变量的近似分布的参数。在深度生成模型中，A-VI用作加速局部潜变量推断的计算技巧。本文研究A-VI作为近似后验推断的一种通用替代方法。由于分期家族是分解家族的子集，A-VI无法产生比F-VI最优解更低的Kullback-Leibler散度的近似值。因此，一个核心的理论问题是刻画A-VI何时仍然达到F-VI的最优解。

    Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We deri
    
[^3]: 未知动力系统的流图学习：综述、实现和基准测试

    Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks. (arXiv:2307.11013v1 [cs.LG])

    [http://arxiv.org/abs/2307.11013](http://arxiv.org/abs/2307.11013)

    本文介绍了流图学习（FML）的框架和实现细节，以及用于学习未知动力系统的基准问题。FML在部分观察系统中产生准确的预测模型，即使没有精确的数学模型。

    

    流图学习（FML）与深度神经网络（DNN）结合，已经显示出在数据驱动建模未知动力系统方面的潜力。FML的一个显著特点是，即使不存在其精确数学模型，它也能够为部分观察系统产生准确的预测模型。本文提供了FML框架的概述，以及其成功实施的重要计算细节。我们还提供了一组明确定义的基准问题，用于学习未知动力系统。所有这些问题的数值细节都被提供，以及它们的FML结果，以确保问题可供交叉验证，并且结果可重复。

    Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promises for data driven modeling of unknown dynamical systems. A remarkable feature of FML is that it is capable of producing accurate predictive models for partially observed systems, even when their exact mathematical models do not exist. In this paper, we present an overview of the FML framework, along with the important computational details for its successful implementation. We also present a set of well defined benchmark problems for learning unknown dynamical systems. All the numerical details of these problems are presented, along with their FML results, to ensure that the problems are accessible for cross-examination and the results are reproducible.
    
[^4]: 尖锐性最小化算法不仅仅是为了更好地泛化而最小化尖锐性

    Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization. (arXiv:2307.11007v1 [cs.LG])

    [http://arxiv.org/abs/2307.11007](http://arxiv.org/abs/2307.11007)

    本文研究发现，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。我们的结果表明，尖锐性与泛化之间的关系取决于数据分布和模型架构。

    

    尽管进行了广泛的研究，但过参数化的神经网络能够泛化的基本原因仍然不明确。现有的理论表明，常见的随机优化器更倾向于训练损失更平坦的最小化器，因此自然而然的解释是平坦性意味着泛化。本文对这一解释进行了批判性的研究。通过理论和实证调查，我们发现对于两层ReLU网络存在以下三种情况：(1) 平坦性确实暗示泛化；(2) 存在最平坦的非泛化模型，尖锐性最小化算法无法泛化；(3) 更加令人惊讶的是，存在非泛化最平坦的模型，但尖锐性最小化算法仍然能够泛化。我们的研究结果表明，尖锐性与泛化之间的关系在一定程度上取决于数据分布和模型架构，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。

    Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve bet
    
[^5]: 私有联邦学习中使用自动调优压缩的新技术

    Private Federated Learning with Autotuned Compression. (arXiv:2307.10999v1 [cs.LG])

    [http://arxiv.org/abs/2307.10999](http://arxiv.org/abs/2307.10999)

    本文提出了一种在私有联邦学习中自动调整压缩率的新技术，无需手动设置或调整，通过使用安全聚合和差分隐私提供了可证明的隐私保证，并在真实世界数据集上展示了其有效性。

    

    我们提出了一种在私有联邦学习中减少通信的新技术，无需设置或调整压缩率。我们的即时方法会根据训练过程中引入的错误自动调整压缩率，并通过使用安全聚合和差分隐私提供可证明的隐私保证。我们的技术在均值估计方面被证明是实例最优的，意味着它们可以根据问题的“难度”进行最小交互式调整。我们通过在真实世界数据集上实现有利的压缩率，展示了我们方法的有效性。

    We propose new techniques for reducing communication in private federated learning without the need for setting or tuning compression rates. Our on-the-fly methods automatically adjust the compression rate based on the error induced during training, while maintaining provable privacy guarantees through the use of secure aggregation and differential privacy. Our techniques are provably instance-optimal for mean estimation, meaning that they can adapt to the ``hardness of the problem" with minimal interactivity. We demonstrate the effectiveness of our approach on real-world datasets by achieving favorable compression rates without the need for tuning.
    
[^6]: 密集样本深度学习

    Dense Sample Deep Learning. (arXiv:2307.10991v1 [cs.AI])

    [http://arxiv.org/abs/2307.10991](http://arxiv.org/abs/2307.10991)

    密集样本深度学习是一种针对深度学习网络的研究方法，旨在揭示学习机制和表示的未知特性，并解决大规模数据和隐藏单元存在的问题。

    

    深度学习（DL）是20世纪80年代提出的一种神经网络算法的变体，在人工智能（AI）领域取得了令人惊讶的进展，包括语言翻译、蛋白质折叠、自动驾驶汽车，以及最近的类人语言模型（CHATbots）。尽管深度学习（DL）网络的使用越来越广泛，但对于使这些网络在如此广泛的应用中有效的学习机制和表示仍知之甚少。部分原因可能是其大规模架构和大规模数据的使用，但深度学习表示的本质仍然大部分未知。不幸的是，具有数百万或数十亿个标记的训练集存在未知的组合方式，同时数百万或数十亿个隐藏单元的网络难以可视化，其机制也难以揭示。在本文中，我们提出了一种密集样本深度学习的方法。

    Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently. Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications. Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987. But the nature of deep learned representations remain largely unknown. Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed. In this pap
    
[^7]: 非线性元学习可以保证更快的收敛速度

    Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])

    [http://arxiv.org/abs/2307.10870](http://arxiv.org/abs/2307.10870)

    非线性元学习可以保证更快的收敛速度。

    

    最近许多关于元学习的理论研究旨在利用相关任务中的相似表示结构来简化目标任务，并实现收敛速率的保证。然而，在实践中，表示往往是高度非线性的，引入了每个任务中不可简单平均的非平凡偏差。本研究通过非线性表示推导出元学习的理论保证。

    Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
    
[^8]: 通过深度图的持久性解决神经持久性的问题

    Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])

    [http://arxiv.org/abs/2307.10865](http://arxiv.org/abs/2307.10865)

    本文发现网络权重的方差和大权重的空间集中是影响神经持久性的主要因素，并提出了将神经持久性扩展到整个神经网络的深度图持久性测量方法。

    

    神经持久性是一种用于量化神经网络复杂性的重要指标，提出于深度学习中新兴的拓扑数据分析领域。然而，在理论和实证上我们发现，网络权重的方差和大权重的空间集中是影响神经持久性的主要因素。虽然这对于线性分类器有用的信息，但我们发现在深度神经网络的后几层中没有相关的空间结构，使得神经持久性大致等于权重的方差。此外，对于深度神经网络，所提出的层间平均过程没有考虑层间的交互。基于我们的分析，我们提出了对神经持久性基础结构的扩展，从单层改为整个神经网络，这相当于在一个特定矩阵上计算神经持久性。这得到了我们的深度图持久性测量方法。

    Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measur
    
[^9]: 标签校准在域偏移下的语义分割中的应用

    Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])

    [http://arxiv.org/abs/2307.10842](http://arxiv.org/abs/2307.10842)

    该论文介绍了一种在域偏移下使用标签校准方法来提升语义分割模型性能的方式。

    

    预训练的语义分割模型在新领域的数据上表现很可能大幅度降低。我们通过在域偏移下计算软标签原型，并根据与预测类别概率最接近的原型进行预测，来适应未标记的目标领域数据。所提出的适应过程快速且几乎没有计算资源成本，并且能够显著提升性能。我们通过在高度实用的从合成到真实的语义分割问题上展示了标签校准的好处。

    Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
    
[^10]: 通过类原型实现的前馈源无关域适应

    Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])

    [http://arxiv.org/abs/2307.10787](http://arxiv.org/abs/2307.10787)

    这项工作提出了一种前馈方法来解决源无关域适应问题，通过计算类的原型来处理域偏移，相较于使用预训练模型，该方法在精度和时间上都有显著提升。

    

    在没有访问源数据的情况下，源无关域适应因其实用性和无需访问源数据而变得流行。然而，适应过程仍然需要相当长的时间，并且主要基于依赖于反向传播的优化。在这项工作中，我们提出了一种简单的前馈方法，挑战了基于反向传播的适应的必要性。我们的方法基于使用预训练模型计算出的类在域偏移下的原型。与预训练模型相比，它在精度上取得了强大的改进，并且只需要现有域适应方法所需时间的一小部分。

    Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
    
[^11]: 减轻选民属性偏见以实现公正的意见汇总

    Mitigating Voter Attribute Bias for Fair Opinion Aggregation. (arXiv:2307.10749v1 [cs.HC])

    [http://arxiv.org/abs/2307.10749](http://arxiv.org/abs/2307.10749)

    本文研究了通过考虑选民属性来实现公正的意见汇总的方法，并评估了汇总结果的公正性。

    

    多个意见的汇总在决策中起着至关重要的作用，例如在招聘和贷款审核中，以及在为监督学习标记数据时。虽然多数投票和现有的意见汇总模型对于简单任务是有效的，但在没有客观真实标签的任务中，它们并不适用，因为可能会出现分歧。特别是，当选民属性（如性别或种族）引入偏见时，汇总结果可能会因选民属性的组成而异。一个平衡的选民群体对于公平的汇总结果是理想的，但可能难以准备。在本研究中，我们考虑了基于选民属性实现公正意见汇总的方法，并评估了汇总结果的公正性。为此，我们考虑了将多数投票和Dawid和Skene模型（D&S模型）等意见汇总模型与采样加权等公平选项相结合的方法。为了评估汇总结果的公正性。

    The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of 
    
[^12]: 高斯混合下的长尾理论

    Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])

    [http://arxiv.org/abs/2307.10736](http://arxiv.org/abs/2307.10736)

    该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。

    

    我们提出了一个简单的高斯混合模型来生成遵循Feldman的长尾理论（2020）的数据。我们证明，在提出的模型中，线性分类器无法将泛化误差降低到一定水平以下，而具有记忆能力的非线性分类器可以。这证实了对于长尾分布，必须考虑罕见的训练样本以实现对新数据的最佳泛化。最后，我们通过在合成和真实数据上的实验证明，当子群体频率分布的尾部变短时，线性模型和非线性模型之间的性能差距可以减小。

    We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
    
[^13]: 条件期望网络用于SHAP

    Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])

    [http://arxiv.org/abs/2307.10654](http://arxiv.org/abs/2307.10654)

    这项工作提出了一种用于计算条件版本的（代理）神经网络方法，该方法可以有效地解释神经网络和其他回归模型的预测结果，并考虑特征之间的依赖关系。同时，该方法还可以应用于复杂回归模型的分析，并提供正确的偏依赖图表示。

    

    SHAP是一种非常流行的模型无关技术，用于解释预测模型。SHAP的两个最受欢迎的版本是条件期望版本和无条件期望版本（后者也称为干预SHAP）。除了基于树的方法之外，通常使用无条件版本（出于计算原因）。我们提供了一种（代理）神经网络方法，可以高效地计算神经网络和其他回归模型的条件版本，并正确考虑特征组件之间的依赖结构。这个方法还可以用于提供与广义线性模型（GLM）类似的复杂回归模型的drop1和anova分析，并提供考虑特征组件中正确依赖结构的偏依赖图（PDP）的对应版本。

    A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
    
[^14]: Fisher-Rao距离和逆推到SPD锥距离在多元正态分布之间的应用

    Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])

    [http://arxiv.org/abs/2307.10644](http://arxiv.org/abs/2307.10644)

    本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。

    

    许多科学领域，如扩散张量成像、结构张量计算机视觉、雷达信号处理和机器学习等，都存在着多元正态分布的数据集。为了处理这些正态数据集以进行过滤、分类或聚类等下游任务，需要定义合适的正态和它们之间的路径之间的差异度量。Fisher-Rao距离，作为Fisher信息度量引起的Riemann几何距离，是一种合理的度量距离，但除了一些特殊情况外，并没有闭式求解。本文首先报告了一种快速且鲁棒的方法，可以精确地近似计算多元正态分布之间的Fisher-Rao距离。其次，我们介绍了一类基于正态流形到高维对称正定锥的子流形的微分同胚嵌入的距离。

    Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
    
[^15]: 基于集成学习的物联网网络安全异常检测方法通过贝叶斯超参数敏感性分析

    Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis. (arXiv:2307.10596v1 [cs.LG])

    [http://arxiv.org/abs/2307.10596](http://arxiv.org/abs/2307.10596)

    本论文基于集成学习方法，通过贝叶斯超参数敏感性分析，提出了一种用于物联网网络安全异常检测的方法，该方法充分利用了物联网数据的异构性和多样性特征。

    

    物联网整合了全球数十亿智能设备，具备与其他连接设备进行沟通的能力，实现几乎无需人为干预。物联网能够进行大规模的数据聚合和分析，从而提升各个领域的生活质量。特别是，物联网收集的数据对于异常检测非常有用。物联网的异构性既是网络安全的挑战，也是机会。传统的网络安全监测方法通常需要对不同类型的数据进行预处理和处理，这对于包含异构特征的数据集可能会存在问题。然而，异构类型的网络设备往往可以捕获比单一类型设备读数更多样化的信号，这对于异常检测特别有用。在本文中，我们提出了一项关于使用集成机器学习方法增强物联网网络安全异常检测的全面研究。

    The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhanc
    
[^16]: 《在因果推断中，多重稳健估计器绕过神经网络模型的超参数调整》

    Multiply Robust Estimator Circumvents Hyperparameter Tuning of Neural Network Models in Causal Inference. (arXiv:2307.10536v1 [stat.ME])

    [http://arxiv.org/abs/2307.10536](http://arxiv.org/abs/2307.10536)

    这项研究展示了一种名为多重稳健估计器的方法，通过在一个单一的估计器中利用多个模型，解决了选择神经网络模型中最佳超参数集的困难，以实现在因果推断中的平均处理效应估计。

    

    平均处理效应（ATE）的估计通常分为两步进行，第一步是建立处理和结果模型，第二步将预测结果插入ATE估计器。在第一步中，可以使用多种模型来拟合处理和结果，包括使用机器学习算法。然而，选择哪个超参数组会导致最佳的因果效应估计和推断是一项困难的任务。多重稳健（MR）估计器允许我们在一个单一的估计器中利用所有第一步模型。我们证明，如果其中一个第一步处理或结果模型是$n^r$一致的，MR估计器就是$n^r$一致的。我们还证明，MR是一类广义估计方程的解，并且如果其中一个处理模型是$\sqrt{n}$-一致的话，MR是渐近正态的。计算了MR的标准错误，它不需要对第一步的真实模型有所了解。我们的模拟结果表明。。。

    Estimation of the Average Treatment Effect (ATE) is often carried out in 2 steps, wherein the first step, the treatment and outcome are modeled, and in the second step the predictions are inserted into the ATE estimator. In the first steps, numerous models can be fit to the treatment and outcome, including using machine learning algorithms. However, it is a difficult task to choose among the hyperparameter sets which will result in the best causal effect estimation and inference. Multiply Robust (MR) estimator allows us to leverage all the first-step models in a single estimator. We show that MR estimator is $n^r$ consistent if one of the first-step treatment or outcome models is $n^r$ consistent. We also show that MR is the solution to a broad class of estimating equations, and is asymptotically normal if one of the treatment models is $\sqrt{n}$-consistent. The standard error of MR is also calculated which does not require a knowledge of the true models in the first step. Our simulat
    
[^17]: 一种实现具有硬约束输出的神经网络的计算简单方法

    A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])

    [http://arxiv.org/abs/2307.10459](http://arxiv.org/abs/2307.10459)

    提出了一种计算简单的方法来实现具有硬约束输出的神经网络。该方法通过映射隐藏参数向量到一个符合约束集的点实现约束，并通过附加的神经网络层来进行映射。该方法还可以处理不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况，并且可以处理不同类型的约束，包括线性和二次约束、等式约束和动态约束。

    

    提出了一种在神经网络输出值上施加硬凸约束的计算简单方法。该方法的关键思想是通过将网络的隐藏参数向量映射到一个点，确保它在由一组约束定义的可行集内。映射是通过具有输出约束的附加神经网络层实现的。将该方法简单地扩展到不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况。在所提出的方法框架中，可以简单地实现对输出的约束投影方法。展示了如何将不同类型的约束引入到所提出的方法中，包括线性和二次约束、等式约束和动态约束，以及边界形式的约束。该方法的一个重要特点是它的计算简单性。

    A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pa
    
[^18]: 使用机器学习确定有向渗透系统的临界点

    Determination of the critical points for systems of directed percolation class using machine learning. (arXiv:2307.10456v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2307.10456](http://arxiv.org/abs/2307.10456)

    本研究使用机器学习算法研究了两个模型的非平衡态相变，并使用卷积神经网络和DBSCAN算法确定了有向渗透模型和细胞自动机模型的临界点。

    

    最近，机器学习算法在研究平衡态相变中被广泛应用，但在非平衡态相变中应用较少。本研究使用卷积神经网络和密度基准的DBSCAN算法进行有监督和无监督学习，研究两个模型的非平衡态相变。我们分别使用CNN和DBSCAN来确定有向键渗透模型和Domany-Kinzel细胞自动机模型的临界点。这两个模型都被证明属于有向渗透普适类。在有监督学习的情况下，我们使用从蒙特卡洛模拟中生成的图像来训练CNN。我们使用该训练好的CNN来研究这两个模型的相变过程。

    Recently, machine learning algorithms have been used remarkably to study the equilibrium phase transitions, however there are only a few works have been done using this technique in the nonequilibrium phase transitions. In this work, we use the supervised learning with the convolutional neural network (CNN) algorithm and unsupervised learning with the density-based spatial clustering of applications with noise (DBSCAN) algorithm to study the nonequilibrium phase transition in two models. We use CNN and DBSCAN in order to determine the critical points for directed bond percolation (bond DP) model and Domany-Kinzel cellular automaton (DK) model. Both models have been proven to have a nonequilibrium phase transition belongs to the directed percolation (DP) universality class. In the case of supervised learning we train CNN using the images which are generated from Monte Carlo simulations of directed bond percolation. We use that trained CNN in studding the phase transition for the two mod
    
[^19]: 用有限数据进行贝叶斯编程方法的车辆跟随模型校准和验证

    A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data. (arXiv:2307.10437v1 [cs.LG])

    [http://arxiv.org/abs/2307.10437](http://arxiv.org/abs/2307.10437)

    这项研究提出了一种贝叶斯编程方法，用于校准和验证车辆跟随模型，以捕捉和复制工作区内外的驾驶行为。

    

    交通仿真软件被交通研究人员和工程师用于设计和评估道路变化。这些仿真器由微观驾驶行为模型驱动，可以从中推导出宏观测量如流量和拥堵。许多模型设计用于可能的交通场景和道路配置的子集，而其他模型在应用上没有明确的限制。工作区（WZs）是一种到目前为止没有模型能够复现真实驾驶行为的场景。这使得在设计WZ时优化安全和其他指标变得困难。美国联邦公路管理局委托USDOT Volpe中心开发一种微观仿真器中能够准确捕捉和复制WZ内外的驾驶行为的车辆跟随（CF）模型。Volpe还进行了一项自然驾驶研究，收集了在具有WZ的道路上驾驶的车辆的遥测数据，用于模型校准。

    Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During mod
    
[^20]: 基于矩阵集合卡尔曼滤波器的多臂神经网络，以充分近似深度神经网络

    A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks. (arXiv:2307.10436v1 [stat.ML])

    [http://arxiv.org/abs/2307.10436](http://arxiv.org/abs/2307.10436)

    我们提出了一种基于矩阵集合卡尔曼滤波器的多臂神经网络，可以在样本量太小无法训练多臂神经网络时充分近似深度神经网络。该方法还可以对从长短期记忆网络（LSTM）获得的预测赋予不确定性，具有良好的覆盖范围。

    

    深度学习者（DLs）是最先进的预测机制，在许多需要复杂高维数据处理的领域中有广泛应用。虽然传统的DLs通过梯度下降和反向传播进行训练，但已经开发出了不需要梯度计算的基于卡尔曼滤波器（KF）的技术来近似DLs。我们提出了一种基于KF的DL近似器的多臂扩展，当样本量太小无法训练多臂DL时，可以模仿DL。我们提出的矩阵集合卡尔曼滤波器多臂ANN（MEnKF-ANN）还执行显式的模型堆叠，在训练样本具有不等尺寸特征集时具有相关性。我们提出的技术可以近似长短期记忆（LSTM）网络，并给出从这些LSTM预测中获得的值加上不确定性的理想覆盖范围。我们展示了MEnKF-ANN如何“充分”近似一个训练用于分类哪些碳水化合物基质被消化和利用的LSTM网络。

    Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a
    
[^21]: 离散切割Wasserstein损失的性质

    Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])

    [http://arxiv.org/abs/2307.10352](http://arxiv.org/abs/2307.10352)

    本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。

    

    切割Wasserstein（SW）距离已成为比较概率测度的Wasserstein距离的一种流行替代方法。广泛应用包括图像处理、领域自适应和生成建模，常常需要优化一些参数以最小化SW，该参数充当离散概率测度之间的损失函数（因为具有密度的测度在数值上是无法实现的）。所有这些优化问题都存在相同的子问题，即最小化切割Wasserstein能量。在本文中，我们研究了$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$的属性，即两个具有与一个测度的支撑相同数量的离散均匀测度之间的SW距离作为支撑$Y \in \mathbb{R}^{n \times d}$函数的能量。我们研究了这个能量的正则性和优化性质，以及其通过蒙特卡洛近似$\mathcal{E}_p$（使用SW中的期望估计）。

    The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
    
[^22]: 分析体育评论以实现自动识别事件并提取洞见

    Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])

    [http://arxiv.org/abs/2307.10303](http://arxiv.org/abs/2307.10303)

    通过分析多种体育赛事的实时评论，利用自然语言处理技术自动识别主要行动，并通过分类和情感分析提取洞见。

    

    本文中，我们仔细研究了如何利用多种自然语言处理技术和方法，以自动识别体育赛事中的主要行动。我们通过分析不同来源的现场体育评论，并将这些主要行动分类到不同的类别中，来提取洞见。我们还研究了情感分析是否可以帮助检测这些主要行动。

    In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
    
[^23]: 因果性导向的鲁棒性：利用一般性可加干预

    Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])

    [http://arxiv.org/abs/2307.10299](http://arxiv.org/abs/2307.10299)

    本文提出了一种名为DRIG的方法，通过利用训练数据中的一般性可加干预，在预测模型中结合了内分布预测和因果性，从而实现了对未见干预的鲁棒预测。

    

    由于在现实应用中经常发生分布变化，急需开发对这种变化具有鲁棒性的预测模型。现有的框架，如经验风险最小化或分布鲁棒优化，要么对未见分布缺乏通用性，要么依赖于假定的距离度量。相比之下，因果性提供了一种基于数据和结构的稳健预测方法。然而，进行因果推断所需的假设可能过于严格，这种因果模型提供的鲁棒性常常缺乏灵活性。在本文中，我们专注于因果性导向的鲁棒性，并提出了一种名为DRIG（Distributional Robustness via Invariant Gradients）的方法，该方法利用训练数据中的一般性可加干预，以实现对未见干预的鲁棒预测，并在内分布预测和因果性之间自然地进行插值。在线性设置中，我们证明了DRIG产生的预测是

    Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are 
    
[^24]: 基于IPW的双边市场中的无偏排序度量

    An IPW-based Unbiased Ranking Metric in Two-sided Markets. (arXiv:2307.10204v1 [cs.IR])

    [http://arxiv.org/abs/2307.10204](http://arxiv.org/abs/2307.10204)

    这项研究提出了一种基于IPW的无偏排序度量方法，针对双边市场中用户之间的偏见相互作用，解决了位置偏见和两个用户群体的位置偏差问题。

    

    在现代推荐系统中，无偏学习排序（LTR）对于优先考虑来自有偏的隐式用户反馈（如点击数据）的项目是至关重要的。已经提出了一些技术，例如倒数倾向性加权（IPW），用于单边市场。然而，在双边市场（如工作平台或约会服务）中，成功转化需要匹配两个用户的偏好，但对于这种情况关注较少。本文解决了双边市场中用户之间的偏见相互作用，并提出了一种特定的LTR方法。我们首先提出了双边匹配平台中反馈机制的形式化，并指出它们的隐式反馈可能包含来自两个用户群体的位置偏差。基于这一观察，我们扩展了IPW估计器并提出了一种新的估计器，称为双边IPW，以解决双边市场中的位置偏见。我们证明了该估计器满足真实排名的无偏性。

    In modern recommendation systems, unbiased learning-to-rank (LTR) is crucial for prioritizing items from biased implicit user feedback, such as click data. Several techniques, such as Inverse Propensity Weighting (IPW), have been proposed for single-sided markets. However, less attention has been paid to two-sided markets, such as job platforms or dating services, where successful conversions require matching preferences from both users. This paper addresses the complex interaction of biases between users in two-sided markets and proposes a tailored LTR approach. We first present a formulation of feedback mechanisms in two-sided matching platforms and point out that their implicit feedback may include position bias from both user groups. On the basis of this observation, we extend the IPW estimator and propose a new estimator, named two-sided IPW, to address the position bases in two-sided markets. We prove that the proposed estimator satisfies the unbiasedness for the ground-truth ran
    
[^25]: 隐私放大通过重要性采样

    Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])

    [http://arxiv.org/abs/2307.10187](http://arxiv.org/abs/2307.10187)

    通过重要性采样进行隐私放大，可以同时增强隐私保护和提高效用。我们提供了一个一般的结果来量化选择概率权重对隐私放大的影响，并展示了异质采样概率可以在保持子采样大小不变的情况下获得更好的隐私和效用。

    

    我们研究了通过重要性采样对数据集进行子采样作为差分隐私机制的预处理步骤来增强隐私保护的性质。这扩展了已有的通过子采样进行隐私放大的结果到重要性采样，其中每个数据点的权重为其被选择概率的倒数。每个点的选择概率的权重对隐私的影响并不明显。一方面，较低的选择概率会导致更强的隐私放大。另一方面，权重越高，在点被选择时，点对机制输出的影响就越强。我们提供了一个一般的结果来量化这两个影响之间的权衡。我们展示了异质采样概率可以同时比均匀子采样具有更强的隐私和更好的效用，并保持子采样大小不变。特别地，我们制定和解决了隐私优化采样的问题，即寻找...

    We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding
    
[^26]: 这里是翻译过的论文标题: 过去曾翻译《Impatient Bandits: Optimizing for the Long-Term Without Delay》

    Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])

    [http://arxiv.org/abs/2307.09943](http://arxiv.org/abs/2307.09943)

    这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。

    

    这里是翻译过的论文摘要：推荐系统在在线平台上是一个普遍存在的功能。越来越多的情况下，它们明确地被任务为提高用户的长期满意度。在这个背景下，我们研究了一个内容探索任务，将其形式化为一个具有延迟奖励的多臂赌博问题。我们观察到，在选择学习信号时存在明显的权衡：等待完全的奖励可能需要几周时间，这会影响学习发生的速度，而测量短期代理奖励则不完美地反映了实际的长期目标。我们通过两个步骤来解决这个挑战。首先，我们开发了一个预测延迟奖励的模型，该模型可以整合迄今所获得的所有信息。通过贝叶斯滤波器组合完整的观察结果以及部分（短期或中期）的结果，从而得到概率信念。其次，我们设计了一个利用这个新的预测模型的赌博算法。该算法可以快速学习识别内容。

    Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten
    
[^27]: 多视角自监督学习用于多变量通道时间序列

    Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])

    [http://arxiv.org/abs/2307.09614](http://arxiv.org/abs/2307.09614)

    本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。

    

    对多变量生物医学时间序列数据进行标注是一项繁重和昂贵的任务。自监督对比学习通过对未标记数据进行预训练来减少对大型标记数据集的需求。然而，对于多变量时间序列数据，输入通道的集合在不同应用之间通常会有所变化，而大多数现有工作并不允许在具有不同输入通道集合的数据集之间进行迁移学习。我们提出了一种学习一种编码器来分别处理所有输入通道的方法。然后，我们使用传递神经网络在通道之间提取单一表示。我们通过在一个具有六个脑电图通道的数据集上进行预训练，并在一个具有两个不同脑电图通道的数据集上进行微调来展示这种方法的潜力。我们比较了具有传递神经网络和不具有传递神经网络的网络在不同对比损失函数下的性能。我们发现我们的方法结合了TS2Vec损失在大多数设置中的表现优于其他所有方法。

    Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.
    
[^28]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^29]: 用于一般参数密度模型的最小化稳健密度功率分歧的随机优化方法

    A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models. (arXiv:2307.05251v1 [stat.ME])

    [http://arxiv.org/abs/2307.05251](http://arxiv.org/abs/2307.05251)

    本研究提出了一种随机优化方法，用于解决稳健密度功率分歧（DPD）在一般参数密度模型中的计算复杂性问题，并通过应用传统的随机优化理论来验证其有效性。

    

    密度功率分歧（DPD）是一种用于稳健地估计观测数据潜在分布的方法，它包括一个要估计的参数密度模型的幂的积分项。虽然对于一些特定的密度（如正态密度和指数密度）可以得到积分项的显式形式，但DPD的计算复杂性使得其无法应用于更一般的参数密度模型，这已经超过了DPD提出的25年。本研究提出了一种用于一般参数密度模型最小化DPD的随机优化方法，并通过参考随机优化的传统理论说明了其适用性。所提出的方法还可以通过使用未归一化模型来最小化另一个基于密度功率的γ-离差[Kanamori和Fujisawa（2015），Biometrika]。

    Density power divergence (DPD) [Basu et al. (1998), Biometrika], designed to estimate the underlying distribution of the observations robustly, comprises an integral term of the power of the parametric density models to be estimated. While the explicit form of the integral term can be obtained for some specific densities (such as normal density and exponential density), its computational intractability has prohibited the application of DPD-based estimation to more general parametric densities, over a quarter of a century since the proposal of DPD. This study proposes a stochastic optimization approach to minimize DPD for general parametric density models and explains its adequacy by referring to conventional theories on stochastic optimization. The proposed approach also can be applied to the minimization of another density power-based $\gamma$-divergence with the aid of unnormalized models [Kanamori and Fujisawa (2015), Biometrika].
    
[^30]: 理解不确定性采样

    Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])

    [http://arxiv.org/abs/2307.02719](http://arxiv.org/abs/2307.02719)

    本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。

    

    不确定性采样是一种常见的主动学习算法，它顺序地查询当前预测模型对数据样本的不确定性。然而，不确定性采样的使用往往是启发式的：（i）关于在特定任务和特定损失函数下对“不确定性”的准确定义没有共识；（ii）没有理论保证能够给出一个标准协议来实施该算法，例如，在随机梯度下降等优化算法框架下如何处理顺序到达的注释数据。在本研究中，我们系统地研究了流式和池式主动学习下的不确定性采样算法。我们提出了一个等效损失的概念，该概念取决于使用的不确定性度量和原始损失函数，并确立了不确定性采样算法本质上是针对这种等效损失进行优化。这一观点验证了算法的适当性。

    Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
    
[^31]: 可证明高效的UCB类型算法用于学习预测状态表示

    Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v1 [cs.LG])

    [http://arxiv.org/abs/2307.00405](http://arxiv.org/abs/2307.00405)

    这篇论文提出了第一种已知的UCB类型方法用于学习预测状态表示（PSRs），并设计了一个新的奖励项来上界t

    

    一般的顺序决策问题旨在通过基于过去观察和行动的历史来最大化累积奖励。最近的研究表明，如果顺序决策问题可以用预测状态表示（PSRs）建模低秩结构，那么它是可统计学习的。尽管有这些进展，但现有方法通常需要使用预先设计好的步骤或者是计算效率低下的或者是不可计算的。另一方面，上限置信区间（UCB）方法在赌博机和MDPs中被成功地作为计算效率高的方法，但对PSR这种更具挑战性的问题还没有进行研究，这是由于在这种更具挑战性的情况下，乐观型奖励的设计十分困难。本文提出了PSRs的第一种已知的UCB类型方法，其中包含了一个新的奖励项来上界t

    The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are not computationally efficient. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs, due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the t
    
[^32]: 纠正公平分类中的低估偏差和交叉偏差

    Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])

    [http://arxiv.org/abs/2306.11112](http://arxiv.org/abs/2306.11112)

    本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。

    

    我们考虑学习被低估偏差损坏的数据的问题，其中正例在固定数量的敏感组中以不同的未知速率从数据中过滤掉。我们表明，在有少量无偏数据的情况下，我们可以有效地估计每个组的减少参数，即使在交叉组成员资格使得学习每个交叉率变得计算上不可行的情况下。利用这个分组丢失率的估计，我们构造了一个重新加权方案，可以使我们近似评估任何假设在真实分布上的损失，即使我们只能在一个有偏样本上观察到经验误差。最后，我们提出了一个封装了这个学习和重新加权过程的算法，并提供了强PAC风格的保证，即有很高的概率我们对假设在真实分布上的风险的估计将与真实风险任意接近。

    We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
    
[^33]: 不变因果集覆盖机

    Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])

    [http://arxiv.org/abs/2306.04777](http://arxiv.org/abs/2306.04777)

    本文提出了一种名为不变因果集覆盖机的算法，它避免了产生虚假关联，可以在多项式时间内识别感兴趣变量的因果父节点。

    

    基于规则的模型，如决策树，因其可解释的特性受到从业者的欢迎。然而，产生这种模型的学习算法往往容易受到虚假关联的影响，因此不能保证提取的是具有因果关系的洞见。在这项工作中，我们借鉴了不变因果预测文献中的思想，提出了不变的因果集覆盖机，这是一种经典的集覆盖机算法的扩展，用于二值规则的合取/析取，可以证明它避免了虚假关联。我们理论上和实践上证明，我们的方法可以在多项式时间内识别感兴趣变量的因果父节点。

    Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
    
[^34]: 顺序预测双样本和独立性检验

    Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v1 [stat.ML])

    [http://arxiv.org/abs/2305.00143](http://arxiv.org/abs/2305.00143)

    本文提出了一种基于预测的赌博策略来解决高维或结构化数据下非参数双样本和独立性检验问题。

    

    我们研究了顺序非参数双样本和独立性检验的问题。顺序检验在线处理数据，允许使用观察到的数据来决定是否停止并拒绝原假设，或在保持类型I错误控制的同时收集更多数据。我们建立在(非参数)测试赌博原则之上，其中赌徒在未来观察中下注，他们的财富对证据反对原假设进行衡量。最近开发的基于核的赌博策略在简单分布上通常表现良好，但对于高维或结构化数据（如文本和图像）选择合适的核通常是棘手的。为解决这个问题，我们设计了基于预测的赌博策略，依赖于以下事实：如果一个顺序更新的预测器开始一致地确定(a)一个实例从哪个分布中绘制，或者(b)一个实例是从联合分布还是从边缘分布的乘积中绘制的，则分布是不同或相关的。我们的方法灵活，并对基础数据分布和维度不可知，同时保持一定的最优性保证。我们在模拟和实际数据上演示了我们的顺序测试框架的有效性。

    We study the problems of sequential nonparametric two-sample and independence testing. Sequential tests process data online and allow using observed data to decide whether to stop and reject the null hypothesis or to collect more data while maintaining type I error control. We build upon the principle of (nonparametric) testing by betting, where a gambler places bets on future observations and their wealth measures evidence against the null hypothesis. While recently developed kernel-based betting strategies often work well on simple distributions, selecting a suitable kernel for high-dimensional or structured data, such as text and images, is often nontrivial. To address this drawback, we design prediction-based betting strategies that rely on the following fact: if a sequentially updated predictor starts to consistently determine (a) which distribution an instance is drawn from, or (b) whether an instance is drawn from the joint distribution or the product of the marginal distributio
    
[^35]: 旗型流形上的弦均值及其应用

    Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])

    [http://arxiv.org/abs/2303.13501](http://arxiv.org/abs/2303.13501)

    本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。

    

    本文提出了一种新的、可证明收敛的算法，用于在弦度量下计算旗型流形上一组点的旗形均值和旗形中值。旗型流形是一种数学空间，由嵌套的向量空间子空间序列组成，并且在维度上逐渐增加。旗型流形是已知的许多矩阵群的超集，包括Stiefel和Grassmanians，使其成为在各种计算机视觉问题中非常有用的通用对象。为了解决计算一阶旗帜统计数据的挑战，我们首先将问题转化为涉及辅助变量受Stiefel流形约束的问题。Stiefel流形是一组正交框架的空间，利用Stiefel流形优化的数值稳定性和效率，可以有效地计算旗形均值。通过一系列实验证明了我们的方法在Grassmann和旋转均值以及主成分问题中的有效性。

    This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
    
[^36]: 对Langevin Monte Carlo的完整分析：超越Poincaré不等式

    Towards a Complete Analysis of Langevin Monte Carlo: Beyond Poincar\'e Inequality. (arXiv:2303.03589v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2303.03589](http://arxiv.org/abs/2303.03589)

    本研究通过建立上下界限来超越Poincaré不等式，从而推动Langevin扩散和Langevin Monte Carlo（LMC）在弱Poincaré不等式下的研究。研究结果可以量化初始值对LMC算法性能的影响。

    

    在适当的功能不等式假设下，Langevin扩散具有快速收敛性。因此，可以期望在处理离散化误差的附加光滑条件下，它们的离散化方法如Langevin Monte Carlo（LMC）也会以类似的方式收敛。这个研究计划由Vempala和Wibisono（2019）发起，他们建立了在对数Sobolev不等式下的结果。Chewi等人（2022）将结果推广到处理Poincaré不等式的情况。在本文中，我们超越了Poincaré不等式，将这个研究计划推到了极限。我们通过建立Langevin扩散和LMC在满足广泛类别的密度（包括多项式衰减重尾密度，即柯西型密度）的弱Poincaré不等式下的上下界来实现这一目标。我们的结果明确量化了初始值对LMC算法性能的影响。特别地，我们展示了当尾部从s

    Langevin diffusions are rapidly convergent under appropriate functional inequality assumptions. Hence, it is natural to expect that with additional smoothness conditions to handle the discretization errors, their discretizations like the Langevin Monte Carlo (LMC) converge in a similar fashion. This research program was initiated by Vempala and Wibisono (2019), who established results under log-Sobolev inequalities. Chewi et al. (2022) extended the results to handle the case of Poincar\'e inequalities. In this paper, we go beyond Poincar\'e inequalities, and push this research program to its limit. We do so by establishing upper and lower bounds for Langevin diffusions and LMC under weak Poincar\'e inequalities that are satisfied by a large class of densities including polynomially-decaying heavy-tailed densities (i.e., Cauchy-type). Our results explicitly quantify the effect of the initializer on the performance of the LMC algorithm. In particular, we show that as the tail goes from s
    
[^37]: 从图生成到图分类

    From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07989](http://arxiv.org/abs/2302.07989)

    本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。

    

    本文描述了一种利用图生成模型 (GGM) 进行图分类的新方法。假设一个定义了图及其类标签的联合概率分布的 GGM，我推导了计算给定图的类标签概率的分类公式。可以使用新的条件 ELBO 来训练生成图自编码器模型进行区分。虽然利用生成模型进行分类在非关系 i.i.d. 数据中已经得到了很好的研究，但据我们所知，这是一种图分类的新方法。

    This note describes a new approach to classifying graphs that leverages graph generative models (GGM). Assuming a GGM that defines a joint probability distribution over graphs and their class labels, I derive classification formulas for the probability of a class label given a graph. A new conditional ELBO can be used to train a generative graph auto-encoder model for discrimination. While leveraging generative models for classification has been well explored for non-relational i.i.d. data, to our knowledge it is a novel approach to graph classification.
    
[^38]: 系统的线性偏微分方程的高斯过程先验与常系数（翻译自arXiv:2212.14319v3 [stat.ML] 更新）

    Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14319](http://arxiv.org/abs/2212.14319)

    该论文提出了一种名为EPGP的高斯过程先验，用于线性偏微分方程系统，并且构造了反映标准谱方法的GP核函数。该方法可以推断线性PDE系统的可能解，并具有算法性强、普适性广、适用于大数据集的稀疏版本。

    

    偏微分方程（PDE）是建模物理系统的重要工具，将它们纳入机器学习模型是将物理知识纳入的重要方式。对于任何具有常系数的线性PDE系统，我们提出了一族称为EPGP的高斯过程（GP）先验，使得所有实现都是该系统的精确解。我们应用Ehrenpreis-Palamodov基本原理，它作为一种非线性傅里叶变换，构建了GP核函数，反映了标准的谱方法用于GP。我们的方法可以从任何数据（如有噪声的测量数据或点定义的初始和边界条件）推断线性PDE系统的可能解。构造EPGP先验的算法性强，普适性广，并且有一个稀疏版本（S-EPGP），可以学习相关的谱频率，并在大数据集上运行效果更好。我们在三类PDE系统上演示了我们的方法，包括热方程和波方程。

    Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equati
    
[^39]: 提高方差网络不确定性量化的树状结构学习方法

    Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12658](http://arxiv.org/abs/2212.12658)

    提出了一种改进方差网络不确定性量化的方法，通过树状结构学习将特征空间分割为多个区域，并使用区域特定的神经网络预测均值和方差来量化不确定性。该方法利用新的分裂准则，在计算上友好且不需要修剪，还可以构建集合版本来估计总不确定性。

    

    为了提高方差网络的不确定性量化，我们提出了一种新颖的树状结构局部神经网络模型，该模型根据不确定性的异质性将特征空间划分为多个区域。根据训练数据，建立一棵树，其叶节点代表不同的区域，在这些区域特定的神经网络中进行训练，以预测均值和方差以量化不确定性。所提出的不确定性分裂神经回归树 (USNRT)采用了新颖的分裂准则。在每个节点上，首先对全数据进行神经网络训练，然后对残差进行统计检验，找到最佳分裂，对应具有最显著不确定性异质性的两个子区域。USNRT在计算上友好，只需很少的叶节点即可满足要求，无需进行修剪。此外，还可以轻松构建集合版本以估计包括 aleatory 的总不确定性。

    To improve the uncertainty quantification of variance networks, we propose a novel tree-structured local neural network model that partitions the feature space into multiple regions based on uncertainty heterogeneity. A tree is built upon giving the training data, whose leaf nodes represent different regions where region-specific neural networks are trained to predict both the mean and the variance for quantifying uncertainty. The proposed Uncertainty-Splitting Neural Regression Tree (USNRT) employs novel splitting criteria. At each node, a neural network is trained on the full data first, and a statistical test for the residuals is conducted to find the best split, corresponding to the two sub-regions with the most significant uncertainty heterogeneity between them. USNRT is computationally friendly because very few leaf nodes are sufficient and pruning is unnecessary. Furthermore, an ensemble version can be easily constructed to estimate the total uncertainty including the aleatory a
    
[^40]: 回顾中的好奇心：随机环境中的内在探索

    Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10515](http://arxiv.org/abs/2211.10515)

    本文提出了一种基于结构因果模型的回顾中的好奇心方法，用于稀疏奖励或无奖励环境中的探索问题。该方法学习未来的表征，以捕捉每个结果的不可预测部分，并将其用作预测的额外输入，从而获得鲁棒的内在奖励。

    

    本文考虑在稀疏奖励或无奖励环境中的探索问题，如Montezuma's Revenge。在好奇心驱动范式中，代理被奖励实际结果与预测结果的差异。但在随机环境中，使用预测误差作为内在动机是脆弱的，因为代理可能被状态-动作空间中高熵区域（如“噪声电视”）所困住。本文提出了一种基于结构因果模型的自然解决方案：学习未来的表征，精确地捕捉每个结果的不可预测方面，并将其用作预测的额外输入，从而使内在奖励仅反映世界动态的可预测方面。首先，我们提出将这种回顾表征结合到模型中，以将“噪声”与“新奇”区分开来，得到了回顾中的好奇心：一种简单而可扩展的好奇心泛化方法，具有鲁棒性。

    Consider the problem of exploration in sparse-reward or reward-free environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm, the agent is rewarded for how much each realized outcome differs from their predicted outcome. But using predictive error as intrinsic motivation is fragile in stochastic environments, as the agent may become trapped by high-entropy areas of the state-action space, such as a "noisy TV". In this work, we study a natural solution derived from structural causal models of the world: Our key idea is to learn representations of the future that capture precisely the unpredictable aspects of each outcome -- which we use as additional input for predictions, such that intrinsic rewards only reflect the predictable aspects of world dynamics. First, we propose incorporating such hindsight representations into models to disentangle "noise" from "novelty", yielding Curiosity in Hindsight: a simple and scalable generalization of curiosity that is robust t
    
[^41]: 在在线强化学习中利用离线数据

    Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04974](http://arxiv.org/abs/2211.04974)

    在这项工作中，我们研究了在线强化学习中利用离线数据的设置，为具有线性结构的MDPs确定了所需的在线样本数量，并提供了实现这一目标的性质和算法。

    

    强化学习领域出现了两个核心范式：在线强化学习和离线强化学习。在线强化学习中，智能体对环境没有先验知识，必须与环境交互以找到一个 ε-最优策略。离线强化学习中，学习器可以从一个固定的数据集中学习，但无法与环境进行交互，必须通过离线数据获取最佳策略。实际情况通常需要一个中间的设置：如果我们有一些离线数据，并且还可以与环境进行交互，我们如何最好地利用离线数据来减少学习一个 ε-最优策略所需的在线交互次数？在这项工作中，我们考虑了这个设置，我们称之为FineTuneRL设置，用于具有线性结构的MDPs。我们确定了在给定一些离线数据的情况下，在这个设置中需要的在线样本数量，并提供了一些性质和算法来实现这个目标。

    Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\epsilon$-optimal policy?  In this work, we consider this setting, which we call the \textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and de
    
[^42]: 用神经网络表示随机效用选择模型

    Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12877](http://arxiv.org/abs/2207.12877)

    本论文提出了一种基于神经网络的离散选择模型类，RUMnets，可以近似表示任何随机效用最大化推导出的模型，并且在选择数据上有良好的预测能力。

    

    在深度学习的成功之下，我们提出了一种基于神经网络的离散选择模型类，称为RUMnets，受随机效用最大化（RUM）框架的启发。该模型使用样本平均逼近来构建代理人的随机效用函数。我们证明了RUMnets可以对RUM离散选择模型类进行尖锐逼近：任何从随机效用最大化推导出的模型都可以被RUMnet无限接近地逼近。相反地，任何RUMnet都符合RUM原则。我们得到了在选择数据上拟合的RUMnet的泛化误差的上界，并且根据数据集和架构的关键参数，获得了关于其在新的未知数据上预测选择能力的理论洞见。通过利用神经网络的开源库，我们发现RUMnets在预测准确性方面与几种选择建模和机器学习方法具有竞争力。

    Motivated by the successes of deep learning, we propose a class of neural network-based discrete choice models, called RUMnets, inspired by the random utility maximization (RUM) framework. This model formulates the agents' random utility function using a sample average approximation. We show that RUMnets sharply approximate the class of RUM discrete choice models: any model derived from random utility maximization has choice probabilities that can be approximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet is consistent with the RUM principle. We derive an upper bound on the generalization error of RUMnets fitted on choice data, and gain theoretical insights on their ability to predict choices on new, unseen data depending on critical parameters of the dataset and architecture. By leveraging open-source libraries for neural networks, we find that RUMnets are competitive against several choice modeling and machine learning methods in terms of predictive accuracy on two rea
    
[^43]: 通过大样本渐近理论优化随机梯度算法的统计推断

    Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics. (arXiv:2207.12395v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2207.12395](http://arxiv.org/abs/2207.12395)

    我们利用大样本渐近理论对随机梯度算法进行调优，发现使用固定的大步长进行迭代平均可以鲁棒地优化算法，且具有和MLE抽样分布协方差成比例的鲁棒性。我们还提出了一种类似于Bernstein-von Mises的定理用于指导调优，包括针对模型错误规范鲁棒的广义后验。数值实验验证了我们的结果和建议在实际有限样本情况下的有效性。这些成果为分析其他随机梯度Markov Chain Monte Carlo算法提供了基础。

    

    针对优化和抽样的随机梯度算法（SGA）的调优通常基于试错和启发式方法，而不是可推广的理论。我们通过一种联合步长-样本大小缩放极限来表征SGA的大样本统计渐近性。我们证明了使用固定的大步长进行迭代平均是对调优参数选择鲁棒的，并且在渐近意义下，具有和MLE抽样分布协方差成比例的鲁棒性。我们还证明了一种类似于Bernstein-von Mises的定理以指导调优，包括针对模型错误规范鲁棒的广义后验。数值实验验证了我们在实际有限样本范围内的结果和建议。我们的工作为大范围模型的其他随机梯度Markov Chain Monte Carlo算法的系统分析奠定了基础。

    The tuning of stochastic gradient algorithms (SGAs) for optimization and sampling is often based on heuristics and trial-and-error rather than generalizable theory. We address this theory--practice gap by characterizing the large-sample statistical asymptotics of SGAs via a joint step-size--sample-size scaling limit. We show that iterate averaging with a large fixed step size is robust to the choice of tuning parameters and asymptotically has covariance proportional to that of the MLE sampling distribution. We also prove a Bernstein--von Mises-like theorem to guide tuning, including for generalized posteriors that are robust to model misspecification. Numerical experiments validate our results and recommendations in realistic finite-sample regimes. Our work lays the foundation for a systematic analysis of other stochastic gradient Markov chain Monte Carlo algorithms for a wide range of models.
    
[^44]: 基于在线实验设计的线性MDPs中的依赖于实例的近最优策略识别

    Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02575](http://arxiv.org/abs/2207.02575)

    本文研究了在线实验设计方法在线性MDPs中的应用，提出了一种算法\textsc{Pedel}，该算法在RL中的函数逼近设置下实现了细粒度的依赖于实例的复杂度度量，相对于最低遗憾、最小最大最优算法具有明显收益。

    

    在强化学习中，虽然对于最坏情况实例下的最小最大样本复杂度有了很大的进展，但是这种复杂度衡量往往不能真正反映出学习的真正困难。在实践中，对于一个“简单”的实例，我们可能希望能够实现比最坏实例下更好的复杂度。在本文中，我们希望了解在具有线性函数逼近的强化学习设置中，学习近最优策略（PAC RL）的“依赖于实例”的复杂度。我们提出了一个算法\textsc{Pedel}，它实现了一种细粒度的依赖于实例的复杂度度量，这是在函数逼近设置中首次出现的，从而捕捉了在每个特定问题实例上学习的困难程度。通过一个具体的例子，我们展示了\textsc{Pedel}相对于最低遗憾、最小最大最优算法的可证明收益，并且这些算法无法达到这种效果。

    While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL) -- the complexity of learning on the "worst-case" instance -- such measures of complexity often do not capture the true difficulty of learning. In practice, on an "easy" instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand the "instance-dependent" complexity of learning near-optimal policies (PAC RL) in the setting of RL with linear function approximation. We propose an algorithm, \textsc{Pedel}, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that \textsc{Pedel} yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the insta
    
[^45]: Pythae：统一的生成自编码器Python库——基准测试用例

    Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case. (arXiv:2206.08309v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08309](http://arxiv.org/abs/2206.08309)

    Pythae是一个开源Python库，提供了统一的生成自编码器模型实现和框架，可用于进行多种任务的案例研究基准测试。

    

    近年来，深度生成模型因其对复杂分布的建模能力而引起越来越多的关注。在这些模型中，变分自编码器因其在计算上的高效性和在多个领域中产生令人印象深刻的结果而受到广泛关注。在这一突破之后，为了改进原始论文已进行了广泛的研究工作，导致了各种不同的变分自编码器模型以应对不同的任务。本文介绍了Pythae，一个通用的开源Python库，提供了统一的实现和专门的框架，可方便、可重现、可靠地使用生成自编码器模型。然后，我们提出使用该库进行案例研究基准测试，在其中展示并比较了19个生成自编码器模型，这些模型代表了在图像重构、生成、分类、聚类和插值等下游任务上的一些主要改进。

    In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present Pythae, a versatile open-source Python library providing both a unified implementation and a dedicated framework allowing straightforward, reproducible and reliable use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpola
    
[^46]: 预先训练的感知特征提高差分隐私图像生成的性能

    Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.12900](http://arxiv.org/abs/2205.12900)

    该论文提出了一种利用预先训练的感知特征，通过最小化MMD（最大均值差异）来提高差分隐私图像生成的性能，并成功地生成了CIFAR10级别的图像。

    

    使用差分隐私随机梯度下降（DP-SGD）进行中等规模生成模型的训练非常困难：为了保持合理的隐私水平所需的噪声水平过大。相反，我们建议利用信息丰富的公共数据集上的良好相关表征，然后学习使用该表征模型化私有数据。特别的，我们使用从公共数据集中学习的感知特征的核函数，最小化私有目标数据与生成器分布之间的最大均值差异（MMD）。使用MMD，我们可以一次性对数据相关项进行隐私处理，而无需像DP-SGD一样在优化每一步中引入噪声。我们的算法使我们能够生成CIFAR10级别的图像，其 $\epsilon \approx 2$，捕捉了分布中的独特特征，远远超过当前的技术水平，主要集中于数据集，如MNIST和FashionMNIST 以较大的 $\epsilon$。

    Training even moderately-sized generative models with differentially-private stochastic gradient descent (DP-SGD) is difficult: the required level of noise for reasonable levels of privacy is simply too large. We advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation. In particular, we minimize the maximum mean discrepancy (MMD) between private target data and a generator's distribution, using a kernel based on perceptual features learned from a public dataset. With the MMD, we can simply privatize the data-dependent term once and for all, rather than introducing noise at each step of optimization as in DP-SGD. Our algorithm allows us to generate CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive features in the distribution, far surpassing the current state of the art, which mostly focuses on datasets such as MNIST and FashionMNIST at a large $\epsilon \appro
    
[^47]: 深度证据回归的不合理有效性

    The Unreasonable Effectiveness of Deep Evidential Regression. (arXiv:2205.10060v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10060](http://arxiv.org/abs/2205.10060)

    深度证据回归是一种通过学习证据分布来处理不确定性的方法，在不确定性推理中显示出相对于传统方法和贝叶斯神经网络的优势。然而，它并非精确的不确定性量化方法，而是一种启发式方法。

    

    随着机器学习系统在安全关键领域的不断部署，对于基于原则的不确定性推理的需求日益迫切。一种新的不确定性感知回归神经网络（NN）方法，通过学习关于内部变量和外部变量的不确定性的证据分布，显示出相对于传统确定性方法和典型贝叶斯NN的优势，尤其在能够解耦内部和外部不确定性方面。尽管深度证据回归（DER）取得了一定的实证成功，但数学基础存在重要的不足，这引发了为何所提出的技术看似有效的问题。我们详细说明了理论上的不足，并分析了在合成和真实数据集上的性能，表明深度证据回归是一种启发式而非精确的不确定性量化方法。我们继续讨论如何修正和重新定义内部和外部不确定性的提取方法。

    There is a significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware regression-based neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, notably with the capabilities to disentangle aleatoric and epistemic uncertainties. Despite some empirical success of Deep Evidential Regression (DER), there are important gaps in the mathematical foundation that raise the question of why the proposed technique seemingly works. We detail the theoretical shortcomings and analyze the performance on synthetic and real-world data sets, showing that Deep Evidential Regression is a heuristic rather than an exact uncertainty quantification. We go on to discuss corrections and redefinitions of how aleatoric and epistemic uncertainties should be extracte
    
[^48]: 序贯核嵌入用于介导和时变剂量响应曲线

    Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2111.03950](http://arxiv.org/abs/2111.03950)

    本论文提出了一种基于核岭回归的简单非参数估计方法，可以用于估计介导和时变剂量响应曲线。通过引入序贯核嵌入技术，我们实现了对复杂因果估计的简化。通过模拟实验和真实数据的估计结果，证明了该方法的强大性能和普适性。

    

    我们提出了基于核岭回归的介导和时变剂量响应曲线的简单非参数估计器。通过嵌入Pearl的介导公式和Robins的g公式与核函数，我们允许处理、介导者和协变量在一般空间中连续变化，也允许非线性的处理-混淆因素反馈。我们的关键创新是一种称为序贯核嵌入的再生核希尔伯特空间技术，我们使用它来构建复杂因果估计的简单估计器。我们的估计器保留了经典识别的普适性，同时实现了非渐进均匀收敛速度。在具有许多协变量的非线性模拟中，我们展示了强大的性能。我们估计了美国职业训练团的介导和时变剂量响应曲线，并清洁可能成为未来工作基准的数据。我们将我们的结果推广到介导和时变处理效应以及反事实分布，验证了半参数效率。

    We propose simple nonparametric estimators for mediated and time-varying dose response curves based on kernel ridge regression. By embedding Pearl's mediation formula and Robins' g-formula with kernels, we allow treatments, mediators, and covariates to be continuous in general spaces, and also allow for nonlinear treatment-confounder feedback. Our key innovation is a reproducing kernel Hilbert space technique called sequential kernel embedding, which we use to construct simple estimators for complex causal estimands. Our estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates. In nonlinear simulations with many covariates, we demonstrate strong performance. We estimate mediated and time-varying dose response curves of the US Job Corps, and clean data that may serve as a benchmark in future work. We extend our results to mediated and time-varying treatment effects and counterfactual distributions, verifying semiparametric efficiency 
    
[^49]: 通用背景上下文强化学习模型选择

    Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.03455](http://arxiv.org/abs/2107.03455)

    我们提出了一种自适应背景上下文强化学习算法（ACB），可以在不知道真实模型类别的情况下进行模型选择，并且具有与已知模型类别算法相匹配的遗憾率。模型选择的代价仅对遗憾上界的二阶项有贡献，且具有直观的属性。

    

    我们考虑在可实现性假设下的通用随机背景上下文强化学习模型选择问题。我们提出了一种名为自适应背景上下文强化学习（ACB）的基于连续精炼的算法，该算法分为多个阶段，逐步消除那些对给定实例来说过于简单的模型类别。我们证明了该算法是自适应的，即在任何可证明的背景上下文强化学习算法（例如\cite{falcon}）的遗憾率与顺序一致匹配，前提是需要知道真实的模型类别。不知道正确的模型类别的代价实际上只是导致遗憾上界中的二阶项的附加项。这个代价具有直观的属性，当模型类别变得更容易识别时，它变小，反之亦然。我们还展示了一种更简单的探索-利用（ETC）风格的算法也能获得类似的遗憾上界，尽管不知道真实的模型类别。然而，模型选择的代价是...

    We consider the problem of model selection for the general stochastic contextual bandits under the realizability assumption. We propose a successive refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}), that works in phases and successively eliminates model classes that are too simple to fit the given instance. We prove that this algorithm is adaptive, i.e., the regret rate order-wise matches that of any provable contextual bandit algorithm (ex. \cite{falcon}), that needs the knowledge of the true model class. The price of not knowing the correct model class turns out to be only an additive term contributing to the second order term in the regret bound. This cost possess the intuitive property that it becomes smaller as the model class becomes easier to identify, and vice-versa. We also show that a much simpler explore-then-commit (ETC) style algorithm also obtains similar regret bound, despite not knowing the true model class. However, the cost of model selec
    
[^50]: 鲁棒主成分分析：一种中位数求和的方法

    Robust Principal Component Analysis: A Median of Means Approach. (arXiv:2102.03403v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.03403](http://arxiv.org/abs/2102.03403)

    本文提出了一种基于中位数求和原则的PCA方法，称为中位数求和主成分分析（MoMPCA），以应对异常值对PCA的影响，并在最小假设条件下实现了最优收敛速度。

    

    主成分分析（PCA）是数据可视化、去噪和降维的基本工具。它在统计学、机器学习、计算机视觉等领域广泛应用。然而，PCA容易受到异常值的影响，往往无法检测到数据集中真实的低维结构。本文提出了一种基于中位数求和原则的PCA方法，称为中位数求和主成分分析（MoMPCA）。该方法不仅在计算上具有吸引力，而且在最小假设条件下实现了最优收敛速度。具体来说，我们通过Rademacher复杂度来探索所得解的非渐近误差界限。

    Principal Component Analysis (PCA) is a fundamental tool for data visualization, denoising, and dimensionality reduction. It is widely popular in Statistics, Machine Learning, Computer Vision, and related fields. However, PCA is well-known to fall prey to outliers and often fails to detect the true underlying low-dimensional structure within the dataset. Following the Median of Means (MoM) philosophy, recent supervised learning methods have shown great success in dealing with outlying observations without much compromise to their large sample theoretical properties. This paper proposes a PCA procedure based on the MoM principle. Called the \textbf{M}edian of \textbf{M}eans \textbf{P}rincipal \textbf{C}omponent \textbf{A}nalysis (MoMPCA), the proposed method is not only computationally appealing but also achieves optimal convergence rates under minimal assumptions. In particular, we explore the non-asymptotic error bounds of the obtained solution via the aid of the Rademacher complexiti
    
[^51]: 隐式多维投影局部子空间的可视化方法

    Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.03259](http://arxiv.org/abs/2009.03259)

    本研究提出了一种使用隐式函数导数的可视化方法，旨在理解多维投影对局部子空间的影响。通过分析局部子空间的形状和方向信息，我们能够获取更多关于数据全局结构的洞察力，并将结果可视化为图形进行分析。

    

    我们提出了一种使用隐式函数导数的可视化方法，来理解多维投影对局部子空间的影响。在这里，我们将局部子空间理解为数据点的多维局部邻域。现有的方法主要关注多维数据点的投影，忽略了邻域信息。我们的方法能够分析局部子空间的形状和方向信息，通过感知局部结构来获取更多有关数据全局结构的洞察力。局部子空间通过由基向量张成的多维椭圆拟合。我们提出了一种准确高效的向量转换方法，基于将多维投影表示为隐式函数的解析微分。结果以图形的形式进行可视化，并利用一套完整的专门设计的交互操作，在我们高效的基于Web的可视化工具中进行分析。

    We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness o
    

