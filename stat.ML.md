# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Assessing the capacity of a denoising diffusion probabilistic model to reproduce spatial context.](http://arxiv.org/abs/2309.10817) | 本论文评估了去噪扩散概率模型(DDPMs)重现医学图像空间上下文的能力。通过使用随机上下文模型(SCMs)生成训练数据，对DDPMs学习医学图像相关空间上下文的能力进行了系统评估。 |
| [^2] | [On the different regimes of Stochastic Gradient Descent.](http://arxiv.org/abs/2309.10688) | 这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。 |
| [^3] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^4] | [Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization.](http://arxiv.org/abs/2309.10553) | 本研究提出了一种基于混合状态空间的学习方法，通过联合优化循环神经网络和时间序列模型，实现了对序列数据的非线性预测和建模，克服了传统模型中的特征工程问题，并取得了显著的效果。 |
| [^5] | [Diffusion-based speech enhancement with a weighted generative-supervised learning loss.](http://arxiv.org/abs/2309.10457) | 本文通过在扩散训练目标中增加均方差（MSE）损失，改进了基于扩散的语音增强方法。实验证明了该方法的有效性。 |
| [^6] | [Unsupervised speech enhancement with diffusion-based generative models.](http://arxiv.org/abs/2309.10450) | 这篇论文介绍了一种无监督的语音增强方法，利用扩散模型的生成能力，在训练阶段通过学习干净语音先验分布和噪声模型，实现对语音信号的推断。这是目前尚无的处理语音增强问题的方法。 |
| [^7] | [Posterior sampling algorithms for unsupervised speech enhancement with recurrent variational autoencoder.](http://arxiv.org/abs/2309.10439) | 本文提出了一种基于递归变分自动编码器的后采样算法用于无监督语音增强，该算法通过采样后验分布来提高计算效率并获得更好的语音增强效果。 |
| [^8] | [Minimum width for universal approximation using ReLU networks on compact domain.](http://arxiv.org/abs/2309.10402) | 本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\mathbb R^{d_y}$所需的最小宽度为$\max\{d_x,d_y,2\}$，从而表明在紧致域上的逼近比在${\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\min}\ge d_y+1$（当$d_x<d_y\le2d_x$）。 |
| [^9] | [Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization.](http://arxiv.org/abs/2309.10370) | 本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。 |
| [^10] | [Information geometric bound on general chemical reaction networks.](http://arxiv.org/abs/2309.10334) | 通过信息几何方法，我们提出了一种非线性系统来推导化学反应网络（CRNs）的反应速率的上界。我们的方法在特定类别的CRNs中展示了更快的收敛速度，并且与传统方法相比更为有效。 |
| [^11] | [Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms.](http://arxiv.org/abs/2309.10301) | 该论文研究了领域自适应中条件不变组件的作用，提出了一种基于条件不变惩罚的新算法，该算法在目标风险保证方面具有优势。 |
| [^12] | [Diffusion Methods for Generating Transition Paths.](http://arxiv.org/abs/2309.10276) | 本文提出了两种新方法来生成高质量的分子系统转变路径：一种是通过偏置原始动力学来促进转变，另一种是将原始转变分解为较小的转变。这些方法在数据丰富和数据稀缺的情况下都展现出了有效性。 |
| [^13] | [The Kernel Density Integral Transformation.](http://arxiv.org/abs/2309.10194) | 本文提出了一种使用核密度积分转换作为特征预处理步骤的方法，可以替代线性最小最大缩放和分位数转换，并通过调整超参数优化性能。这种方法在统计数据分析中具有应用价值。 |
| [^14] | [A Geometric Framework for Neural Feature Learning.](http://arxiv.org/abs/2309.10140) | 本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。 |
| [^15] | [Invariant Probabilistic Prediction.](http://arxiv.org/abs/2309.10083) | 这篇论文研究了在分布变化下具有不变性和稳健性的概率预测方法。研究发现在适当评分规则下，任意分布偏移一般不具有不变和稳健的概率预测，通过限制分布偏移类别和选择评估指标，可以在特定模型中实现不变性和稳健性。 |
| [^16] | [A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes.](http://arxiv.org/abs/2309.10068) | 本论文提出了一个统一的视角来探讨非平稳核在深层高斯过程中的应用，以提高预测性能和不确定性估计。 |
| [^17] | [Graph topological property recovery with heat and wave dynamics-based features on graphsD.](http://arxiv.org/abs/2309.09924) | 本文提出了一种名为图微分方程网络（GDeNet）的方法，利用热和波动方程动力学特征来恢复图的拓扑属性，能够在各种下游任务中获得优秀的表现，同时在实际应用中也展现了较好的性能。 |
| [^18] | [Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles.](http://arxiv.org/abs/2309.09457) | 本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。 |
| [^19] | [On the dynamics of multi agent nonlinear filtering and learning.](http://arxiv.org/abs/2309.03557) | 本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为，并介绍了在分布式和联邦学习场景中的应用。 |
| [^20] | [BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition.](http://arxiv.org/abs/2308.14906) | BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。 |
| [^21] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^22] | [A State-Space Perspective on Modelling and Inference for Online Skill Rating.](http://arxiv.org/abs/2308.02414) | 本文提供了对竞技体育技能评级主要方法的全面回顾，并提出了采用状态空间模型视角的建议。通过使用状态空间模型视角，玩家的技能可以表示为随时间变化的变量，而比赛结果则是唯一的观测量。该视角有助于解耦建模和推理，并促进通用推理工具的发展。在构建技能评级的状态空间模型方面，本文探讨了基本步骤，同时还讨论了滤波、平滑和参数估计等推理阶段。在面对高维场景中的计算挑战时，本文强调了所采用的近似和简化方法。该文提供了对记录的流行方法的简明总结。 |
| [^23] | [Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models.](http://arxiv.org/abs/2305.06704) | 该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。 |
| [^24] | [On the Dissipation of Ideal Hamiltonian Monte Carlo Sampler.](http://arxiv.org/abs/2209.07438) | 通过变量积分时间和部分速度刷新，理想哈密顿蒙特卡罗采样器在二次势函数上的效率得到了提高，并且随机积分器在模拟哈密顿动力学上也具有优势。 |
| [^25] | [Sharp Analysis of Sketch-and-Project Methods via a Connection to Randomized Singular Value Decomposition.](http://arxiv.org/abs/2208.09585) | 本文通过与随机奇异值分解的连接，对草图投影方法进行了尖锐分析，首次展示了草图大小与收敛速度的线性关系，同时解释了稀疏草图矩阵对每次迭代收敛速度的影响。 |
| [^26] | [Optimal subgroup selection.](http://arxiv.org/abs/2109.01077) | 在回归设置中，我们提出了一个子群选择挑战，以确定回归函数超过预设阈值的特征空间区域。我们的主要贡献是确定了在样本规模和类型I错误概率上遗憾的最佳速率。 |
| [^27] | [Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning.](http://arxiv.org/abs/2108.08655) | 该论文研究了强化学习中在线演员-评论家算法的全局收敛性。通过数学分析证明，随着更新次数趋近无穷大，带有表格参数化的在线演员-评论家算法收敛于常微分方程。研究结果可以帮助我们理解演员-评论家算法在实践中的行为和性质。 |
| [^28] | [Calibrating multi-dimensional complex ODE from noisy data via deep neural networks.](http://arxiv.org/abs/2106.03591) | 该论文提出了一个两阶段非参数方法，通过深度神经网络校准多维复杂常微分方程的噪声数据。该方法能够恢复ODE系统，避免了维度灾难和复杂ODE结构的限制，并在模块化结构和适当选择网络架构的情况下被证明是一致的。 |
| [^29] | [The Lasso with general Gaussian designs with applications to hypothesis testing.](http://arxiv.org/abs/2007.13716) | 本论文推广了套索方法在高斯相关设计中的应用，通过一个更简单的“固定设计”模型来精确刻画套索估计器，解决了高维回归中的渐近正态性问题。 |
| [^30] | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.](http://arxiv.org/abs/1910.10683) | 本文通过引入一种统一的框架，将所有基于文本的语言问题转换为文本到文本格式，从而探索了NLP中的迁移学习技术的全貌，通过对多个任务进行系统研究，实现了许多基准测试上的最新结果。 |
| [^31] | [Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on Graphs.](http://arxiv.org/abs/1804.11242) | 本论文提出了一种使用映射器在图形上进行保持同态的多尺度图形骨架化的方法，通过调整单个参数实现骨架化的多尺度，并提供了一个软件工具来进行交互式探索。 |

# 详细

[^1]: 评估去噪扩散概率模型重现空间上下文的能力

    Assessing the capacity of a denoising diffusion probabilistic model to reproduce spatial context. (arXiv:2309.10817v1 [eess.IV])

    [http://arxiv.org/abs/2309.10817](http://arxiv.org/abs/2309.10817)

    本论文评估了去噪扩散概率模型(DDPMs)重现医学图像空间上下文的能力。通过使用随机上下文模型(SCMs)生成训练数据，对DDPMs学习医学图像相关空间上下文的能力进行了系统评估。

    

    扩散模型已经成为一类热门的深度生成模型(DGMs)。文献中声称，一种扩散模型——去噪扩散概率模型(DDPMs)在图像合成性能方面表现优于生成对抗网络(GANs)。目前，这些声明要么通过针对自然图像设计的集合方法进行评估，要么通过结构相似性等传统图像质量指标进行评估。然而，我们仍然需要了解DDPMs能够可靠地学习医学图像领域相关信息的程度，即本工作中所称的“空间上下文”。为了解决这个问题，首次报告了对DDPMs学习与医学图像应用相关的空间上下文能力的系统评估。研究的一个关键方面是使用随机上下文模型(SCMs)生成训练数据。通过这种方式，评估了DDPMs学习医学图像相关空间上下文的能力。

    Diffusion models have emerged as a popular family of deep generative models (DGMs). In the literature, it has been claimed that one class of diffusion models -- denoising diffusion probabilistic models (DDPMs) -- demonstrate superior image synthesis performance as compared to generative adversarial networks (GANs). To date, these claims have been evaluated using either ensemble-based methods designed for natural images, or conventional measures of image quality such as structural similarity. However, there remains an important need to understand the extent to which DDPMs can reliably learn medical imaging domain-relevant information, which is referred to as `spatial context' in this work. To address this, a systematic assessment of the ability of DDPMs to learn spatial context relevant to medical imaging applications is reported for the first time. A key aspect of the studies is the use of stochastic context models (SCMs) to produce training data. In this way, the ability of the DDPMs 
    
[^2]: 关于随机梯度下降的不同模式

    On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])

    [http://arxiv.org/abs/2309.10688](http://arxiv.org/abs/2309.10688)

    这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。

    

    现代深度网络通过随机梯度下降（SGD）进行训练，其关键参数是每个步骤考虑的数据量或批量大小B以及步长或学习率η。对于小的B和大的η，SGD对应于参数的随机演化，其噪声幅度由“温度”T=η/B控制。然而当批量大小B≥B^*足够大时，这种描述被观察到失效，或者在温度足够小时简化为梯度下降（GD）。理解这些交叉发生的位置仍然是一个中心挑战。在这里，我们解决了这些问题，在一个教师-学生感知器分类模型中，我们展示了我们的关键预测仍适用于深度网络。具体来说，我们在B-η平面上获得了一个相位图，将三个动态阶段分开：（i）受温度控制的噪声主导的SGD，（ii）大步骤主导的SGD和

    Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
    
[^3]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^4]: 基于混合状态空间的学习用于序列数据预测的联合优化

    Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization. (arXiv:2309.10553v1 [stat.ML])

    [http://arxiv.org/abs/2309.10553](http://arxiv.org/abs/2309.10553)

    本研究提出了一种基于混合状态空间的学习方法，通过联合优化循环神经网络和时间序列模型，实现了对序列数据的非线性预测和建模，克服了传统模型中的特征工程问题，并取得了显著的效果。

    

    我们研究在线环境中的非线性预测/回归问题，并引入了一种混合模型，通过状态空间形式的联合机制有效地减轻了传统非线性预测模型中特定领域特征工程问题的需求，并实现了非线性和线性组分的有效混合。我们使用递归结构从原始顺序序列中提取特征，并使用传统线性时间序列模型处理序列数据的复杂性，如季节性、趋势等。与现有的集成或混合模型通常以分离的方式训练基础模型不同，这不仅耗时，而且由于建模的分隔或独立训练而非最优。相反，我们首次在文献中采用联合优化的方法，从原始数据中联合优化增强的循环神经网络（LSTM）自动提取特征和ARMA系列时间序列模型（SARIMAX）进行有效建模。

    We investigate nonlinear prediction/regression in an online setting and introduce a hybrid model that effectively mitigates, via a joint mechanism through a state space formulation, the need for domain-specific feature engineering issues of conventional nonlinear prediction models and achieves an efficient mix of nonlinear and linear components. In particular, we use recursive structures to extract features from raw sequential sequences and a traditional linear time series model to deal with the intricacies of the sequential data, e.g., seasonality, trends. The state-of-the-art ensemble or hybrid models typically train the base models in a disjoint manner, which is not only time consuming but also sub-optimal due to the separation of modeling or independent training. In contrast, as the first time in the literature, we jointly optimize an enhanced recurrent neural network (LSTM) for automatic feature extraction from raw data and an ARMA-family time series model (SARIMAX) for effectivel
    
[^5]: 基于扩散的语音增强方法与加权生成-监督学习损失

    Diffusion-based speech enhancement with a weighted generative-supervised learning loss. (arXiv:2309.10457v1 [cs.CV])

    [http://arxiv.org/abs/2309.10457](http://arxiv.org/abs/2309.10457)

    本文通过在扩散训练目标中增加均方差（MSE）损失，改进了基于扩散的语音增强方法。实验证明了该方法的有效性。

    

    最近，基于扩散的生成模型在语音增强中受到关注，提供了一种替代传统监督方法的方法。这些模型将干净语音训练样本转化为以噪声语音为中心的高斯噪声，并在此基础上学习一个参数化模型来逆转这个过程，有条件地根据噪声语音进行预测。与监督方法不同，基于生成的语音增强方法通常仅依赖于无监督损失，这可能导致对有条件噪声语音的融合不够高效。为了解决这个问题，我们提出使用均方差（MSE）损失来增强原始的扩散训练目标，在每次逆转过程迭代中，测量估计增强语音与真实干净语音之间的差异。实验结果证明了我们所提出方法的有效性。

    Diffusion-based generative models have recently gained attention in speech enhancement (SE), providing an alternative to conventional supervised methods. These models transform clean speech training samples into Gaussian noise centered at noisy speech, and subsequently learn a parameterized model to reverse this process, conditionally on noisy speech. Unlike supervised methods, generative-based SE approaches usually rely solely on an unsupervised loss, which may result in less efficient incorporation of conditioned noisy speech. To address this issue, we propose augmenting the original diffusion training objective with a mean squared error (MSE) loss, measuring the discrepancy between estimated enhanced speech and ground-truth clean speech at each reverse process iteration. Experimental results demonstrate the effectiveness of our proposed methodology.
    
[^6]: 基于扩散式生成模型的无监督语音增强

    Unsupervised speech enhancement with diffusion-based generative models. (arXiv:2309.10450v1 [cs.CV])

    [http://arxiv.org/abs/2309.10450](http://arxiv.org/abs/2309.10450)

    这篇论文介绍了一种无监督的语音增强方法，利用扩散模型的生成能力，在训练阶段通过学习干净语音先验分布和噪声模型，实现对语音信号的推断。这是目前尚无的处理语音增强问题的方法。

    

    最近，在监督语音增强领域，条件得分型扩散模型引起了广泛关注，其表现出了最先进的性能。然而，这些方法在推广到未知条件时可能面临挑战。为了解决这个问题，我们引入了一种在无监督方式下运作的替代方法，利用扩散模型的生成能力。具体而言，在训练阶段，利用基于得分的扩散模型在短时傅里叶变换（STFT）域中学习了一个干净语音先验分布，使其能无条件地从高斯噪声生成干净语音。然后，我们通过将学到的干净语音先验与噪声模型相结合，开发了一种对语音增强进行后验采样的方法来进行语音信号推断。噪声参数是通过迭代的期望最大化（EM）方法与干净语音估计同时学习的。据我们所知，这是第一次开发这种无监督的方法来处理语音增强问题。

    Recently, conditional score-based diffusion models have gained significant attention in the field of supervised speech enhancement, yielding state-of-the-art performance. However, these methods may face challenges when generalising to unseen conditions. To address this issue, we introduce an alternative approach that operates in an unsupervised manner, leveraging the generative power of diffusion models. Specifically, in a training phase, a clean speech prior distribution is learnt in the short-time Fourier transform (STFT) domain using score-based diffusion models, allowing it to unconditionally generate clean speech from Gaussian noise. Then, we develop a posterior sampling methodology for speech enhancement by combining the learnt clean speech prior with a noise model for speech signal inference. The noise parameters are simultaneously learnt along with clean speech estimation through an iterative expectationmaximisation (EM) approach. To the best of our knowledge, this is the first
    
[^7]: 基于递归变分自动编码器的无监督语音增强的后采样算法

    Posterior sampling algorithms for unsupervised speech enhancement with recurrent variational autoencoder. (arXiv:2309.10439v1 [cs.CV])

    [http://arxiv.org/abs/2309.10439](http://arxiv.org/abs/2309.10439)

    本文提出了一种基于递归变分自动编码器的后采样算法用于无监督语音增强，该算法通过采样后验分布来提高计算效率并获得更好的语音增强效果。

    

    本文针对基于递归变分自动编码器（RVAE）的无监督语音增强问题进行了研究。该方法在泛化性能上相较于有监督方法有着良好的表现。然而，测试时涉及到的迭代变分期望最大化（VEM）过程，依赖于变分推理方法，导致了高计算复杂性。为了解决这个问题，我们提出了一种基于朗格朗日动力学和Metropolis-Hasting算法的有效采样技术，用于基于EM的语音增强和RVAE。通过直接对EM过程中的难以计算的后验分布进行采样，我们绕过了变分推理的复杂性。我们进行了一系列实验，将我们提出的方法与VEM以及基于扩散模型的最先进的有监督语音增强方法进行了对比。结果表明，我们基于采样的算法在性能上明显优于VEM，不仅在计算效率上，而且在语音增强效果上也提升了很多。

    In this paper, we address the unsupervised speech enhancement problem based on recurrent variational autoencoder (RVAE). This approach offers promising generalization performance over the supervised counterpart. Nevertheless, the involved iterative variational expectation-maximization (VEM) process at test time, which relies on a variational inference method, results in high computational complexity. To tackle this issue, we present efficient sampling techniques based on Langevin dynamics and Metropolis-Hasting algorithms, adapted to the EM-based speech enhancement with RVAE. By directly sampling from the intractable posterior distribution within the EM process, we circumvent the intricacies of variational inference. We conduct a series of experiments, comparing the proposed methods with VEM and a state-of-the-art supervised speech enhancement approach based on diffusion models. The results reveal that our sampling-based algorithms significantly outperform VEM, not only in terms of com
    
[^8]: 使用ReLU网络在紧致域上进行通用逼近的最小宽度

    Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])

    [http://arxiv.org/abs/2309.10402](http://arxiv.org/abs/2309.10402)

    本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\mathbb R^{d_y}$所需的最小宽度为$\max\{d_x,d_y,2\}$，从而表明在紧致域上的逼近比在${\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\min}\ge d_y+1$（当$d_x<d_y\le2d_x$）。

    

    经过研究，限制宽度网络的通用逼近性质已经作为深度限制网络的经典通用逼近定理的对偶进行研究。已经有几次尝试来表征使得通用逼近性质成立的最小宽度$w_{\min}$，但只有很少几个找到了确切的值。在这项工作中，我们证明了对于从$[0,1]^{d_x}$到$\mathbb R^{d_y}$的$L^p$函数的通用逼近的最小宽度，如果激活函数是ReLU-Like（例如ReLU，GELU，Softplus），那么它的确切值是$\max\{d_x,d_y,2\}$。与已知的结果$w_{\min}=\max\{d_x+1,d_y\}$相比，当域为${\mathbb R^{d_x}}$时，我们的结果首次表明，在紧致域上的逼近要求比在${\mathbb R^{d_x}}$上的要求更小。我们接下来利用包括ReLU在内的一般激活函数进行一致逼近的最小宽度$w_{\min}$证明了一个下界：如果$d_x<d_y\le2d_x$，则$w_{\min}\ge d_y+1$。结合我们的第一个结果，这表明了一个二分法。

    The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x<d_y\le2d_x$. Together with our first result, this shows a dichotomy be
    
[^9]: 浅层神经网络的几何结构和基于${\mathcal L}^2$代价最小化的构造方法

    Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])

    [http://arxiv.org/abs/2309.10370](http://arxiv.org/abs/2309.10370)

    本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。

    

    本文给出了一个几何解释：浅层神经网络的结构由一个隐藏层、一个斜坡激活函数、一个${\mathcal L}^2$谱范类（或者Hilbert-Schmidt）的代价函数、输入空间${\mathbb R}^M$、输出空间${\mathbb R}^Q$（其中$Q\leq M$），以及训练输入样本数量$N>QM$所特征。我们证明了代价函数的最小值具有$O(\delta_P)$的上界，其中$\delta_P$衡量了训练输入的信噪比。我们使用适应于属于同一输出向量$y_j$的训练输入向量$\overline{x_{0,j}}$的投影来获得近似的优化器，其中$j=1,\dots,Q$。在特殊情况$M=Q$下，我们明确确定了代价函数的一个确切退化局部最小值；这个尖锐的值与对于$Q\leq M$所获得的上界之间有一个相对误差$O(\delta_P^2)$。上界证明的方法提供了一个构造性训练的网络；我们证明它测度了$Q$维空间中的给定输出。

    In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
    
[^10]: 一般化学反应网络的信息几何界限

    Information geometric bound on general chemical reaction networks. (arXiv:2309.10334v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.10334](http://arxiv.org/abs/2309.10334)

    通过信息几何方法，我们提出了一种非线性系统来推导化学反应网络（CRNs）的反应速率的上界。我们的方法在特定类别的CRNs中展示了更快的收敛速度，并且与传统方法相比更为有效。

    

    我们研究了化学反应网络（CRNs）的动力学，目标是推导出它们反应速率的上界。由于CRNs的非线性性质和离散结构，这个任务是具有挑战性的。为了解决这个问题，我们采用了信息几何方法，利用自然梯度，构建了一个非线性系统，可以得到CRN动力学的上界。我们通过数值模拟验证了我们的方法，在特定类别的CRNs中展示了更快的收敛速度。这个类别的特征包括化学品的数量、化学反应的化学计量系数的最大值和反应的数量。我们还将我们的方法与传统方法进行了比较，结果表明传统方法无法提供CRNs的反应速率的上界。尽管我们的研究重点是CRNs，但从自然科学到工程学中超图的普遍存在表明我们的方法可能具有更广泛的应用，包括信息科学领域。

    We investigate the dynamics of chemical reaction networks (CRNs) with the goal of deriving an upper bound on their reaction rates. This task is challenging due to the nonlinear nature and discrete structure inherent in CRNs. To address this, we employ an information geometric approach, using the natural gradient, to develop a nonlinear system that yields an upper bound for CRN dynamics. We validate our approach through numerical simulations, demonstrating faster convergence in a specific class of CRNs. This class is characterized by the number of chemicals, the maximum value of stoichiometric coefficients of the chemical reactions, and the number of reactions. We also compare our method to a conventional approach, showing that the latter cannot provide an upper bound on reaction rates of CRNs. While our study focuses on CRNs, the ubiquity of hypergraphs in fields from natural sciences to engineering suggests that our method may find broader applications, including in information scienc
    
[^11]: 领域自适应中条件不变组件的突出作用：理论和算法

    Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms. (arXiv:2309.10301v1 [stat.ML])

    [http://arxiv.org/abs/2309.10301](http://arxiv.org/abs/2309.10301)

    该论文研究了领域自适应中条件不变组件的作用，提出了一种基于条件不变惩罚的新算法，该算法在目标风险保证方面具有优势。

    

    领域自适应是一个统计学习问题，当用于训练模型的源数据分布与用于评估模型的目标数据分布不同时出现。虽然许多领域自适应算法已经证明了相当大的实证成功，但是盲目应用这些算法往往会导致在新的数据集上表现更差。为了解决这个问题，重要的是澄清领域自适应算法在具备良好目标性能的假设下。在这项工作中，我们关注在预测中具备条件不变的组件（CICs）的存在假设，这些组件在源数据和目标数据之间保持条件不变。我们证明了CICs，通过条件不变惩罚（CIP）可以估计，具备在领域自适应中提供目标风险保证的三个突出作用。首先，我们提出了一种基于CICs的新算法，即重要性加权的条件不变惩罚（IW-CIP），它在目标风险保证方面超越了简单的方法。

    Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple 
    
[^12]: 生成过渡路径的扩散方法

    Diffusion Methods for Generating Transition Paths. (arXiv:2309.10276v1 [physics.comp-ph])

    [http://arxiv.org/abs/2309.10276](http://arxiv.org/abs/2309.10276)

    本文提出了两种新方法来生成高质量的分子系统转变路径：一种是通过偏置原始动力学来促进转变，另一种是将原始转变分解为较小的转变。这些方法在数据丰富和数据稀缺的情况下都展现出了有效性。

    

    在这项工作中，我们致力于使用基于评分的生成模型模拟稀有的亚稳态之间的转变。生成高质量的转变路径的高效方法对于研究分子系统非常有价值，因为数据往往难以获取。我们在本文中开发了两种新颖的路径生成方法：基于链的方法和基于中点的方法。第一种方法通过偏置原始动力学来促进转变，而第二种方法则采用分裂技术并将原始转变分解为较小的转变。在M\"uller势和二肽基丙氨酸的生成转变路径的数值结果中，这些方法在数据丰富和数据稀缺的情况下都展现出了有效性。

    In this work, we seek to simulate rare transitions between metastable states using score-based generative models. An efficient method for generating high-quality transition paths is valuable for the study of molecular systems since data is often difficult to obtain. We develop two novel methods for path generation in this paper: a chain-based approach and a midpoint-based approach. The first biases the original dynamics to facilitate transitions, while the second mirrors splitting techniques and breaks down the original transition into smaller transitions. Numerical results of generated transition paths for the M\"uller potential and for Alanine dipeptide demonstrate the effectiveness of these approaches in both the data-rich and data-scarce regimes.
    
[^13]: 核密度积分转换

    The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])

    [http://arxiv.org/abs/2309.10194](http://arxiv.org/abs/2309.10194)

    本文提出了一种使用核密度积分转换作为特征预处理步骤的方法，可以替代线性最小最大缩放和分位数转换，并通过调整超参数优化性能。这种方法在统计数据分析中具有应用价值。

    

    在应用机器学习和统计方法于表格数据时，特征预处理继续发挥关键作用。在本文中，我们提出了使用核密度积分转换作为特征预处理步骤的方法。我们的方法综合了两种主要的特征预处理方法作为极限情况：线性最小最大缩放和分位数转换。我们证明了，在不调整超参数的情况下，核密度积分转换可以作为这两种方法的简单替代方法，对每种方法的弱点具有鲁棒性。另外，通过调整一个连续超参数，我们经常优于这两种方法。最后，我们表明核密度转换可以有益地应用于统计数据分析，特别是在相关性分析和单变量聚类上。

    Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
    
[^14]: 一个基于神经特征学习的几何框架

    A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])

    [http://arxiv.org/abs/2309.10140](http://arxiv.org/abs/2309.10140)

    本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。

    

    我们提出了一个基于神经特征提取器的学习系统设计的新框架，通过利用特征空间中的几何结构。首先，我们引入了特征几何，它将统计依赖和特征统一到同一个具有几何结构的函数空间中。通过应用特征几何，我们将每个学习问题形式化为解决由学习设置指定的依赖组件的最佳特征近似解。我们提出了一种嵌套技术来设计学习算法，从数据样本中学习最佳特征，这可以应用于现有的网络架构和优化器。为了展示嵌套技术的应用，我们进一步讨论了多变量学习问题，包括条件推理和多模态学习，在这些问题中，我们提出了最佳特征并揭示了它们与经典方法的联系。

    We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
    
[^15]: 不变的概率预测

    Invariant Probabilistic Prediction. (arXiv:2309.10083v1 [stat.ME])

    [http://arxiv.org/abs/2309.10083](http://arxiv.org/abs/2309.10083)

    这篇论文研究了在分布变化下具有不变性和稳健性的概率预测方法。研究发现在适当评分规则下，任意分布偏移一般不具有不变和稳健的概率预测，通过限制分布偏移类别和选择评估指标，可以在特定模型中实现不变性和稳健性。

    

    近年来，对于在训练和测试数据之间分布变化下表现稳健的统计方法越来越受关注。虽然大部分相关研究集中在使用平方误差损失的点预测上，但本文将焦点转向了概率预测，旨在全面量化给定协变量的结果变量的不确定性。在基于因果关系的框架下，我们研究了概率预测在适当评分规则下的不变性和稳健性。我们证明了任意分布偏移一般不具有不变和稳健的概率预测，与点预测的情况相反。我们展示了如何选择评估指标并限制分布偏移类别，以实现原型高斯异方差线性模型中的可识别性和不变性。在这些发现的基础上，我们提出了一种方法来产生不变的概率预测。

    In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant pro
    
[^16]: 非平稳核对深层高斯过程的统一视角

    A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])

    [http://arxiv.org/abs/2309.10068](http://arxiv.org/abs/2309.10068)

    本论文提出了一个统一的视角来探讨非平稳核在深层高斯过程中的应用，以提高预测性能和不确定性估计。

    

    高斯过程（GP）是一种流行的用于数据的随机函数近似和不确定性量化的统计技术。在过去的二十年中，由于其优越的预测能力，特别是在数据稀疏情况下，以及其固有的提供强健不确定性估计的能力，GP已被广泛应用于机器学习领域。然而，它们的性能高度依赖于核心方法的复杂定制，这往往在使用标准设置和现成软件工具时使从业者不满意。可以说，GP最重要的组成部分是核函数，它扮演协方差算子的角色。Mat\'ern类的平稳核在大多数应用研究中被使用；低效的预测性能和不现实的不确定性估计往往是其结果。非平稳核表现出更好的性能，但由于其更加复杂的属性，很少被使用。

    The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
    
[^17]: 基于热和波动动力学特征的图拓扑属性恢复

    Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])

    [http://arxiv.org/abs/2309.09924](http://arxiv.org/abs/2309.09924)

    本文提出了一种名为图微分方程网络（GDeNet）的方法，利用热和波动方程动力学特征来恢复图的拓扑属性，能够在各种下游任务中获得优秀的表现，同时在实际应用中也展现了较好的性能。

    

    本文提出了一种名为图微分方程网络（GDeNet）的方法，利用图上的PDE解的表达能力，为各种下游任务获得连续的节点和图级表示。我们推导出了热和波动方程动力学与图的谱特性以及连续时间随机游走在图上行为之间的理论结果。我们通过恢复随机图生成参数、Ricci曲率和持久同调等方式实验证明了这些动力学能够捕捉到图形几何和拓扑的显著方面。此外，我们还展示了GDeNet在包括引用图、药物分子和蛋白质在内的真实世界数据集上的优越性能。

    In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
    
[^18]: 不需要计算复杂性无法解决的预言机，在稀疏线性MDP中探索和学习。

    Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])

    [http://arxiv.org/abs/2309.09457](http://arxiv.org/abs/2309.09457)

    本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。

    

    线性马尔可夫决策过程（MDPs）的基本假设是学习者可以访问已知的特征映射$ \phi（x，a）$，该映射将状态-动作对映射到$d$维向量，并且奖励和转换是此表示中的线性函数。但是这些特征从哪里来？在没有专家领域知识的情况下，一种诱人的策略是使用“厨房水槽”方法，并希望真实特征包含在一个更大的潜在特征集中。在本文中，我们从特征选择的角度重新审视线性MDP。在$k$-稀疏线性MDP中，存在一个未知的大小为$k$的子集$S \subset [d]$，其中包含所有相关特征，目标是在与环境的交互中仅经过poly$(k,\log d)$次学习，学习出近似最优策略。我们的主要结果是这个问题的第一个多项式时间算法。与此相反，早期的研究要么做出了明显的假设，使得探索无关紧要，要么提供了指数复杂度的算法。

    The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o
    
[^19]: 论多智能体非线性滤波和学习的动力学

    On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])

    [http://arxiv.org/abs/2309.03557](http://arxiv.org/abs/2309.03557)

    本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为，并介绍了在分布式和联邦学习场景中的应用。

    

    多智能体系统通过分散一致性寻求动力学来完成高度复杂的学习任务，其在信号处理和计算智能社区引起了极大关注。本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为。为此，提出了多智能体网络系统中一个智能体的行动的一般表述，并给出了实现协同学习行为的条件。重要的是，还介绍了该推导框架在分布式和联邦学习场景中的应用。

    Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
    
[^20]: BayOTIDE: 基于贝叶斯方法的在线多元时间序列插补与函数分解

    BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])

    [http://arxiv.org/abs/2308.14906](http://arxiv.org/abs/2308.14906)

    BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。

    

    在真实世界的场景中，如交通和能源，经常观察到具有缺失值和噪声的大规模时间序列数据，甚至是不规则采样。尽管已经提出了许多插补方法，但它们大多数只适用于局部视角，即将长序列拆分为适当大小的批次进行训练。这种局部视角可能使模型忽略全局趋势或周期性模式。更重要的是，几乎所有方法都假设观测值在规则的时间间隔进行采样，并且无法处理来自不同应用的复杂不规则采样时间序列。此外，大多数现有方法都是在离线状态下进行学习的。因此，对于那些有快速到达的流数据的应用来说，它们并不合适。为了克服这些局限性，我们提出了BayOTIDE：基于贝叶斯方法的在线多元时间序列插补与函数分解。

    In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
    
[^21]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^22]: 对在线技能评级建模和推理的状态空间视角的研究

    A State-Space Perspective on Modelling and Inference for Online Skill Rating. (arXiv:2308.02414v1 [stat.AP])

    [http://arxiv.org/abs/2308.02414](http://arxiv.org/abs/2308.02414)

    本文提供了对竞技体育技能评级主要方法的全面回顾，并提出了采用状态空间模型视角的建议。通过使用状态空间模型视角，玩家的技能可以表示为随时间变化的变量，而比赛结果则是唯一的观测量。该视角有助于解耦建模和推理，并促进通用推理工具的发展。在构建技能评级的状态空间模型方面，本文探讨了基本步骤，同时还讨论了滤波、平滑和参数估计等推理阶段。在面对高维场景中的计算挑战时，本文强调了所采用的近似和简化方法。该文提供了对记录的流行方法的简明总结。

    

    本文全面回顾了用于竞技体育技能评级的主要方法。我们提倡采用状态空间模型视角，将玩家的技能表示为随时间变化的，比赛结果作为唯一的观测量。状态空间模型视角有助于解耦建模和推理，从而使得更加注重模型假设的方法得以突出，并促进了通用推理工具的发展。我们探讨了构建技能评级的状态空间模型的基本步骤，并讨论了推理的三个阶段：滤波、平滑和参数估计。在整个过程中，我们研究了在涉及大量玩家和比赛的高维场景中进行规模扩展的计算挑战，强调了用于有效应对这些挑战的近似和简化方法。我们提供了文献中记录的流行方法的简明总结。

    This paper offers a comprehensive review of the main methodologies used for skill rating in competitive sports. We advocate for a state-space model perspective, wherein players' skills are represented as time-varying, and match results serve as the sole observed quantities. The state-space model perspective facilitates the decoupling of modeling and inference, enabling a more focused approach highlighting model assumptions, while also fostering the development of general-purpose inference tools. We explore the essential steps involved in constructing a state-space model for skill rating before turning to a discussion on the three stages of inference: filtering, smoothing and parameter estimation. Throughout, we examine the computational challenges of scaling up to high-dimensional scenarios involving numerous players and matches, highlighting approximations and reductions used to address these challenges effectively. We provide concise summaries of popular methods documented in the lit
    
[^23]: 滞后多因子模型中领先滞后关系的鲁棒检测

    Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])

    [http://arxiv.org/abs/2305.06704](http://arxiv.org/abs/2305.06704)

    该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。

    

    在多元时间序列系统中，通过发现数据中固有的领先滞后关系，可以获得关键信息，这指的是两个相对时间互移的时间序列之间的依赖关系，可以用于控制、预测或聚类。我们开发了一种基于聚类的方法，用于鲁棒检测滞后多因子模型中的领先滞后关系。在我们的框架中，所设想的管道接收一组时间序列作为输入，并使用滑动窗口方法从每个输入时间序列中提取一组子序列时间序列。然后，我们应用各种聚类技术（例如K-means++和谱聚类），采用各种成对相似性度量，包括非线性的相似性度量。一旦聚类被提取出来，跨聚类的领先滞后估计被聚合起来，以增强对原始宇宙中一致关系的识别。由于多

    In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
    
[^24]: 关于理想哈密顿蒙特卡罗采样器耗散性的研究

    On the Dissipation of Ideal Hamiltonian Monte Carlo Sampler. (arXiv:2209.07438v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2209.07438](http://arxiv.org/abs/2209.07438)

    通过变量积分时间和部分速度刷新，理想哈密顿蒙特卡罗采样器在二次势函数上的效率得到了提高，并且随机积分器在模拟哈密顿动力学上也具有优势。

    

    我们报告了变量积分时间和理想哈密顿蒙特卡罗采样器的部分速度刷新之间似乎存在着有趣的联系，这两者都可以用于减少动力学的耗散行为。更具体地说，我们表明在二次势函数上，通过这些手段，与经典的常量积分时间、完全刷新的HMC相比，效率可以提高一个$\sqrt{\kappa}$的Wasserstein-2距离因子。我们还探索了在更高阶正则性条件下模拟哈密顿动力学的随机积分器的优势。

    We report on what seems to be an intriguing connection between variable integration time and partial velocity refreshment of Ideal Hamiltonian Monte Carlo samplers, both of which can be used for reducing the dissipative behavior of the dynamics. More concretely, we show that on quadratic potentials, efficiency can be improved through these means by a $\sqrt{\kappa}$ factor in Wasserstein-2 distance, compared to classical constant integration time, fully refreshed HMC. We additionally explore the benefit of randomized integrators for simulating the Hamiltonian dynamics under higher order regularity conditions.
    
[^25]: 通过与随机奇异值分解的连接，对草图投影方法进行了尖锐分析

    Sharp Analysis of Sketch-and-Project Methods via a Connection to Randomized Singular Value Decomposition. (arXiv:2208.09585v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.09585](http://arxiv.org/abs/2208.09585)

    本文通过与随机奇异值分解的连接，对草图投影方法进行了尖锐分析，首次展示了草图大小与收敛速度的线性关系，同时解释了稀疏草图矩阵对每次迭代收敛速度的影响。

    

    草图投影是一个统一了许多已知迭代方法以及它们的变体的框架，用于解线性系统和非线性优化问题。本文提出了一个理论框架，以获得草图投影方法的收敛速度的尖锐保证。我们的方法首次展示了：（1）收敛速度至少与草图大小线性增加，当数据矩阵具有一定的谱衰减时，收敛速度增加更快；（2）允许使用稀疏的草图矩阵，这比密集草图更有效率，比子采样方法更稳健。特别是，我们的结果解释了一个观察到的现象，即对草图矩阵进行激进的稀疏化不会影响草图投影的每次迭代收敛速度。

    Sketch-and-project is a framework which unifies many known iterative methods for solving linear systems and their variants, as well as further extensions to non-linear optimization problems. It includes popular methods such as randomized Kaczmarz, coordinate descent, variants of the Newton method in convex optimization, and others. In this paper, we develop a theoretical framework for obtaining sharp guarantees on the convergence rate of sketch-and-project methods. Our approach is the first to: (1) show that the convergence rate improves at least linearly with the sketch size, and even faster when the data matrix exhibits certain spectral decays; and (2) allow for sparse sketching matrices, which are more efficient than dense sketches and more robust than sub-sampling methods. In particular, our results explain an observed phenomenon that a radical sparsification of the sketching matrix does not affect the per iteration convergence rate of sketch-and-project. To obtain our results, we 
    
[^26]: 最佳子群选择

    Optimal subgroup selection. (arXiv:2109.01077v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2109.01077](http://arxiv.org/abs/2109.01077)

    在回归设置中，我们提出了一个子群选择挑战，以确定回归函数超过预设阈值的特征空间区域。我们的主要贡献是确定了在样本规模和类型I错误概率上遗憾的最佳速率。

    

    在临床试验和其他应用中，我们经常看到特征空间中出现了有趣的行为区域，但不清楚这些观察到的现象是否在总体水平上有所反映。针对回归设置，我们考虑子群选择挑战，即识别一个特征空间的区域，在该区域上，回归函数超过了预设的阈值。我们将这个问题形式化为一种约束优化问题，通过寻找一个低复杂度、数据相关的选择集，在这个选择集上，回归函数有至少与阈值一样大的概率，同时要求该区域在边缘特征分布下的质量尽可能大。这导致了一种自然的遗憾概念，我们的主要贡献是确定了遗憾在样本规模和第一类错误概率上的最优值。这个最优值涉及到样本大小和类型I错误概率的微妙相互影响。

    In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interpla
    
[^27]: 在强化学习中，在线演员-评论家算法的ODE极限全局收敛性

    Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.08655](http://arxiv.org/abs/2108.08655)

    该论文研究了强化学习中在线演员-评论家算法的全局收敛性。通过数学分析证明，随着更新次数趋近无穷大，带有表格参数化的在线演员-评论家算法收敛于常微分方程。研究结果可以帮助我们理解演员-评论家算法在实践中的行为和性质。

    

    演员-评论家算法在强化学习中被广泛使用，但由于非独立同分布的在线数据样本的到来，其在数学上分析具有挑战性。数据样本的分布随着模型的更新而动态变化，引入了数据分布和强化学习算法之间复杂的反馈循环。我们证明，在时间重缩放下，带有表格参数化的在线演员-评论家算法在更新次数趋近于无穷大时收敛于常微分方程（ODE）。证明首先在固定的演员策略下建立数据样本的几何遍历性。然后，使用泊松方程，我们证明随着更新次数趋近于无穷大，数据样本关于一种动态概率测度的波动在演变的演员模型的函数下消失。一旦得到ODE极限，我们使用双时间尺度分析研究其收敛特性。

    Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyse due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equation (ODE) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-co
    
[^28]: 通过深度神经网络校准多维复杂常微分方程的噪声数据

    Calibrating multi-dimensional complex ODE from noisy data via deep neural networks. (arXiv:2106.03591v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03591](http://arxiv.org/abs/2106.03591)

    该论文提出了一个两阶段非参数方法，通过深度神经网络校准多维复杂常微分方程的噪声数据。该方法能够恢复ODE系统，避免了维度灾难和复杂ODE结构的限制，并在模块化结构和适当选择网络架构的情况下被证明是一致的。

    

    常微分方程（ODE）被广泛用于建模生物学、化学、工程、金融、物理等领域的复杂动态。使用噪声数据校准复杂ODE系统通常非常困难。在这项工作中，我们提出了一个两阶段非参数方法来解决这个问题。我们首先使用边界核方法提取去噪数据及其高阶导数，然后将它们输入具有ReLU激活函数的稀疏连接深度神经网络中。我们的方法能够恢复ODE系统，而不受维度灾难和复杂ODE结构的限制。当ODE具有一般的模块化结构，每个模块组件仅涉及少量输入变量，并且网络架构被适当选择时，我们的方法被证明是一致的。理论性质通过广泛的模拟研究得到验证，证明了所提出方法的有效性和有效性。

    Ordinary differential equations (ODEs) are widely used to model complex dynamics that arises in biology, chemistry, engineering, finance, physics, etc. Calibration of a complicated ODE system using noisy data is generally very difficult. In this work, we propose a two-stage nonparametric approach to address this problem. We first extract the de-noised data and their higher order derivatives using boundary kernel method, and then feed them into a sparsely connected deep neural network with ReLU activation function. Our method is able to recover the ODE system without being subject to the curse of dimensionality and complicated ODE structure. When the ODE possesses a general modular structure, with each modular component involving only a few input variables, and the network architecture is properly chosen, our method is proven to be consistent. Theoretical properties are corroborated by an extensive simulation study that demonstrates the validity and effectiveness of the proposed method.
    
[^29]: 带有一般高斯设计的套索方法及其在假设检验中的应用

    The Lasso with general Gaussian designs with applications to hypothesis testing. (arXiv:2007.13716v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2007.13716](http://arxiv.org/abs/2007.13716)

    本论文推广了套索方法在高斯相关设计中的应用，通过一个更简单的“固定设计”模型来精确刻画套索估计器，解决了高维回归中的渐近正态性问题。

    

    套索方法是一种高维回归方法，当自变量的数量$p$与观测数量$n$相同或更大时，现在通常使用。由于两个基本原因，经典的渐近正态理论不适用于该模型：(1) 正则化风险是非光滑的；(2) 估计器$\widehat{\boldsymbol{\theta}}$与真实参数向量$\boldsymbol{\theta}^*$之间的距离不能忽略。因此，传统的渐近正态理论的基础——标准的摄动论证失败。另一方面，在$n$和$p$都很大且$n/p$为1阶的情况下，可以精确地描述套索估计器。这个描述首先是在具有独立同分布自变量的高斯设计情况下得到的：我们将其推广到具有非奇异协方差结构的高斯相关设计。这可以通过一个更简单的“固定设计”模型来表达。

    The Lasso is a method for high-dimensional regression, which is now commonly used when the number of covariates $p$ is of the same order or larger than the number of observations $n$. Classical asymptotic normality theory does not apply to this model due to two fundamental reasons: $(1)$ The regularized risk is non-smooth; $(2)$ The distance between the estimator $\widehat{\boldsymbol{\theta}}$ and the true parameters vector $\boldsymbol{\theta}^*$ cannot be neglected. As a consequence, standard perturbative arguments that are the traditional basis for asymptotic normality fail.  On the other hand, the Lasso estimator can be precisely characterized in the regime in which both $n$ and $p$ are large and $n/p$ is of order one. This characterization was first obtained in the case of Gaussian designs with i.i.d. covariates: here we generalize it to Gaussian correlated designs with non-singular covariance structure. This is expressed in terms of a simpler ``fixed-design'' model. We establish
    
[^30]: 探索使用统一的文本到文本转换器进行迁移学习的极限

    Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.10683](http://arxiv.org/abs/1910.10683)

    本文通过引入一种统一的框架，将所有基于文本的语言问题转换为文本到文本格式，从而探索了NLP中的迁移学习技术的全貌，通过对多个任务进行系统研究，实现了许多基准测试上的最新结果。

    

    迁移学习已经成为自然语言处理(NLP)中一种强大的技术，其中模型在进行下游任务的微调之前首先在数据丰富的任务上进行预训练。迁移学习的有效性催生了多种方法、方法论和实践。本文通过引入一种将所有基于文本的语言问题转换为文本到文本格式的统一框架，探索了NLP中的迁移学习技术的全貌。我们对许多语言理解任务进行了系统性的研究，比较了预训练目标、架构、无标签数据集、迁移方法和其他因素。通过将我们探索的见解与规模和我们的新的“庞大干净抓取语料库”相结合，在许多涉及摘要、问答、文本分类等基准测试上取得了最新的成果。为了促进NLP领域的未来迁移学习研究，我们发布了我们的数据集、预训练模型等。

    Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained mode
    
[^31]: 使用映射器在图形上进行保持同态的多尺度图形骨架化

    Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on Graphs. (arXiv:1804.11242v5 [cs.SI] UPDATED)

    [http://arxiv.org/abs/1804.11242](http://arxiv.org/abs/1804.11242)

    本论文提出了一种使用映射器在图形上进行保持同态的多尺度图形骨架化的方法，通过调整单个参数实现骨架化的多尺度，并提供了一个软件工具来进行交互式探索。

    

    节点链接图是一种常用的表示图形的方法，可以捕捉个体、企业、蛋白质和电信端点之间的关系。然而，即使对于几百个节点的中等规模数据，节点链接图可能无法传达有关图结构的见解，因为会有视觉混乱。我们提出将映射器构造应用于图形可视化，该构造在拓扑数据分析中是一种流行的工具，能够在保留核心结构的同时，提供对数据的总结的强大理论基础。我们开发了一种针对加权无向图的映射器构造的变种，称为{\mog}，它生成保持同调性的图形骨架。我们进一步展示了如何通过调整单个参数来实现输入图形的多尺度骨架化。我们提供了一个软件工具，可以实现对这些骨架进行交互式探索，并展示了我们的方法在合成和实际数据上的有效性。

    Node-link diagrams are a popular method for representing graphs that capture relationships between individuals, businesses, proteins, and telecommunication endpoints. However, node-link diagrams may fail to convey insights regarding graph structures, even for moderately sized data of a few hundred nodes, due to visual clutter. We propose to apply the mapper construction -- a popular tool in topological data analysis -- to graph visualization, which provides a strong theoretical basis for summarizing the data while preserving their core structures. We develop a variation of the mapper construction targeting weighted, undirected graphs, called {\mog}, which generates homology-preserving skeletons of graphs. We further show how the adjustment of a single parameter enables multi-scale skeletonization of the input graph. We provide a software tool that enables interactive explorations of such skeletons and demonstrate the effectiveness of our method for synthetic and real-world data.
    

