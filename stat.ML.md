# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML](https://rss.arxiv.org/abs/2311.09200) | 通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。 |
| [^2] | [Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy](https://arxiv.org/abs/2403.04867) | 该论文提出了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。 |
| [^3] | [Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications](https://arxiv.org/abs/2403.03905) | 我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限 |
| [^4] | [Calibrating doubly-robust estimators with unbalanced treatment assignment](https://arxiv.org/abs/2403.01585) | 提出了一个简单的DML估计器扩展，通过对概率得分建模进行欠采样，并校准分数以匹配原始分布，以解决处理分配不平衡问题。 |
| [^5] | [Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes](https://arxiv.org/abs/2402.18477) | 本文在随机过程中开发了一种基于签名核的条件独立性测试，实现了对因果关系的推断，以及开发了约束条件的因果发现算法用于恢复整个有向图。 |
| [^6] | [Batched Nonparametric Contextual Bandits](https://arxiv.org/abs/2402.17732) | 该论文研究了批处理约束下的非参数上下文臂问题，提出了一种名为BaSEDB的方案，在动态分割协变量空间的同时，实现了最优的后悔。 |
| [^7] | [Social Environment Design](https://arxiv.org/abs/2402.14090) | 该论文提出了一种新的研究议程，介绍了社会环境设计作为一种用于自动化政策制定的AI通用框架，旨在捕捉一般经济环境，通过AI模拟系统分析政府和经济政策，并强调未来基于AI的政策制定研究中的关键挑战。 |
| [^8] | [Refined Sample Complexity for Markov Games with Independent Linear Function Approximation](https://arxiv.org/abs/2402.07082) | 本文在独立线性函数逼近的马尔科夫博弈中，通过改进AVLPR框架，提出了基于数据依赖的悲观估计方法，解决了多智能体的诅咒问题。 |
| [^9] | [Efficient Reinforcement Learning from Partial Observability](https://arxiv.org/abs/2311.12244) | 该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。 |
| [^10] | [Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum.](http://arxiv.org/abs/2401.06738) | 本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。 |
| [^11] | [Fun with Flags: Robust Principal Directions via Flag Manifolds.](http://arxiv.org/abs/2401.04071) | 本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。 |
| [^12] | [Tailoring Mixup to Data using Kernel Warping functions.](http://arxiv.org/abs/2311.01434) | 本研究提出了一种利用核扭曲函数对Mixup数据进行个性化处理的方法，通过动态改变插值系数的概率分布来实现更频繁和更强烈的混合相似数据点。实验证明这种方法不仅提高了模型性能，还提高了模型的校准性。 |
| [^13] | [Unreading Race: Purging Protected Features from Chest X-ray Embeddings.](http://arxiv.org/abs/2311.01349) | 论文的目的是利用正交化方法消除胸部X射线嵌入中的保护特征影响，并证明其有效性。研究结果表明保护特征对病理预测有显著影响，而应用正交化方法可以消除这些影响。 |
| [^14] | [The statistical thermodynamics of generative diffusion models.](http://arxiv.org/abs/2310.17467) | 本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。 |
| [^15] | [Fundamental Limits of Membership Inference Attacks on Machine Learning Models.](http://arxiv.org/abs/2310.13786) | 本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。 |
| [^16] | [Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing.](http://arxiv.org/abs/2308.14507) | 本论文研究了针对广义线性模型的参数估计问题，提出了一种通过谱估计器进行预处理的方法。通过对测量进行特征协方差矩阵Σ表示，分析了谱估计器在结构化设计中的性能，并确定了最优预处理以最小化样本数量。 |
| [^17] | [Data-dependent Generalization Bounds via Variable-Size Compressibility.](http://arxiv.org/abs/2303.05369) | 本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。 |
| [^18] | [Label Alignment Regularization for Distribution Shift.](http://arxiv.org/abs/2211.14960) | 这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。 |

# 详细

[^1]: 正则流是否是解锁指数机制的关键？经过准确性和隐私双重约束的差分隐私机器学习的一条路径

    Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML

    [https://rss.arxiv.org/abs/2311.09200](https://rss.arxiv.org/abs/2311.09200)

    通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。

    

    当前的差分隐私机器学习（ML）的最先进且事实标准是差分隐私随机梯度下降（DPSGD）。然而，这种方法本质上是浪费的。通过向每个梯度添加噪声，它会在每个梯度步骤中降低整体隐私。尽管经过15年的丰富研究，推进了组合定理、子采样方法和实现技术，但当前的隐私机器学习方法往往无法达到足够的准确性和隐私保护。与此同时，为了私下优化而设计的指数机制（ExpM）历来被排除在现代机器学习算法的私下训练之外，主要是因为ExpM需要从一种历来难以处理的密度中进行采样。尽管最近发现了正则流模型（NFs），这是一种用于逼近难以处理分布的表达深度网络，但ExpM仍然处于背景中。我们的观点是利用正则流来绕过ExpM的历史障碍是一个潜在的方法。

    The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
    
[^2]: 组隐私放大和子抽样的Rényi差分隐私统一放大

    Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy

    [https://arxiv.org/abs/2403.04867](https://arxiv.org/abs/2403.04867)

    该论文提出了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。

    

    差分隐私(DP)具有多种理想属性，如对后处理的鲁棒性、组隐私和通过子抽样放大，这些属性可以相互独立推导。我们的目标是确定是否通过联合考虑这些属性中的多个可以获得更强的隐私保证。为此，我们专注于组隐私和通过子抽样放大的组合。为了提供适合机器学习算法的保证，我们在Rényi-DP框架中进行了分析，这比$(\epsilon,\delta)$-DP具有更有利的组合属性。作为这个分析的一部分，我们开发了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。我们发现，它不仅让我们改进和泛化现有的放大结果。

    arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
    
[^3]: 黑盒$k$-to-$1$-PCA降维：理论与应用

    Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications

    [https://arxiv.org/abs/2403.03905](https://arxiv.org/abs/2403.03905)

    我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限

    

    $k$-主成分分析（$k$-PCA）问题是一种基本的算法原语，在数据分析和降维应用中被广泛使用。在统计环境中，$k$-PCA的目标是识别一个分布的协方差矩阵的顶部特征空间，我们只能通过样本隐式访问这个矩阵。受这些隐式设置的启发，我们分析黑盒缩减方法作为设计$k$-PCA算法的框架，其中我们通过黑盒$1$-PCA预言模拟对未知目标矩阵的访问，该预言返回一个近似的顶部特征向量，根据两个流行的近似概念。尽管这种黑盒方法可能是设计$k$-PCA算法中最自然的基于降维的方法，这种方法，即通过递归调用$1$-PCA预言调用了$k$次，以前很难理解。

    arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
    
[^4]: 使用不平衡的处理分配校准双重稳健估计器

    Calibrating doubly-robust estimators with unbalanced treatment assignment

    [https://arxiv.org/abs/2403.01585](https://arxiv.org/abs/2403.01585)

    提出了一个简单的DML估计器扩展，通过对概率得分建模进行欠采样，并校准分数以匹配原始分布，以解决处理分配不平衡问题。

    

    机器学习方法，尤其是双机器学习（DML）估计器（Chernozhukov等，2018），越来越受欢迎地用于估计平均处理效应（ATE）。然而，数据集通常表现出处理分配不平衡，只有少数观测值被处理，导致稳健概率得分估计不稳定。我们提出了DML估计器的简单扩展，该扩展对概率得分建模进行了欠采样，并校准分数以匹配原始分布。本文提供了理论结果表明，该估计器保留了DML估计器的渐近特性。模拟研究说明了估计器的有限样本性能。

    arXiv:2403.01585v1 Announce Type: new  Abstract: Machine learning methods, particularly the double machine learning (DML) estimator (Chernozhukov et al., 2018), are increasingly popular for the estimation of the average treatment effect (ATE). However, datasets often exhibit unbalanced treatment assignments where only a few observations are treated, leading to unstable propensity score estimations. We propose a simple extension of the DML estimator which undersamples data for propensity score modeling and calibrates scores to match the original distribution. The paper provides theoretical results showing that the estimator retains the DML estimator's asymptotic properties. A simulation study illustrates the finite sample performance of the estimator.
    
[^5]: 在因果发现中的签名核条件独立性测试用于随机过程

    Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes

    [https://arxiv.org/abs/2402.18477](https://arxiv.org/abs/2402.18477)

    本文在随机过程中开发了一种基于签名核的条件独立性测试，实现了对因果关系的推断，以及开发了约束条件的因果发现算法用于恢复整个有向图。

    

    从观测数据中推断随机动力系统背后的因果结构在科学、健康和金融等领域具有巨大潜力。本文通过利用最近签名核技术的进展，开发了一种基于内核的“路径空间”上条件独立性（CI）测试，用于随机微分方程的解。我们展示了相较于现有方法，在路径空间上，我们提出的CI测试表现出严格更好的性能。此外，我们还为非循环随机动力系统开发了基于约束的因果发现算法，利用时间信息来恢复整个有向图。在假设忠实性和CI预言机的情况下，我们的算法是完备且正确的。

    arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
    
[^6]: 批处理非参数上下文臂

    Batched Nonparametric Contextual Bandits

    [https://arxiv.org/abs/2402.17732](https://arxiv.org/abs/2402.17732)

    该论文研究了批处理约束下的非参数上下文臂问题，提出了一种名为BaSEDB的方案，在动态分割协变量空间的同时，实现了最优的后悔。

    

    我们研究了在批处理约束下的非参数上下文臂问题，在这种情况下，每个动作的期望奖励被建模为协变量的平滑函数，并且策略更新是在每个Observations批次结束时进行的。我们为这种设置建立了一个最小化后悔的下限，并提出了一种名为Batched Successive Elimination with Dynamic Binning（BaSEDB）的方案，可以实现最优的后悔（达到对数因子）。实质上，BaSEDB动态地将协变量空间分割成更小的箱子，并仔细调整它们的宽度以符合批次大小。我们还展示了在批处理约束下静态分箱的非最优性，突出了动态分箱的必要性。另外，我们的结果表明，在完全在线设置中，几乎恒定数量的策略更新可以达到最佳后悔。

    arXiv:2402.17732v1 Announce Type: cross  Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.
    
[^7]: 社会环境设计

    Social Environment Design

    [https://arxiv.org/abs/2402.14090](https://arxiv.org/abs/2402.14090)

    该论文提出了一种新的研究议程，介绍了社会环境设计作为一种用于自动化政策制定的AI通用框架，旨在捕捉一般经济环境，通过AI模拟系统分析政府和经济政策，并强调未来基于AI的政策制定研究中的关键挑战。

    

    人工智能（AI）作为一种用于改善政府和经济政策制定的技术具有潜力。本文提出了一个新的研究议程，介绍了社会环境设计，这是一种用于自动化政策制定的AI通用框架，与强化学习、经济与计算社会选择社区相连接。该框架旨在捕捉一般经济环境，包括对政策目标的投票，并为通过AI模拟对政府和经济政策进行系统分析提供指导。我们强调了未来基于AI的政策制定研究中的关键开放问题。通过解决这些挑战，我们希望实现各种社会福利目标，从而促进更具道德和负责任的决策制定。

    arXiv:2402.14090v1 Announce Type: new  Abstract: Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.
    
[^8]: 基于独立线性函数逼近的马尔科夫博弈的样本复杂度改进

    Refined Sample Complexity for Markov Games with Independent Linear Function Approximation

    [https://arxiv.org/abs/2402.07082](https://arxiv.org/abs/2402.07082)

    本文在独立线性函数逼近的马尔科夫博弈中，通过改进AVLPR框架，提出了基于数据依赖的悲观估计方法，解决了多智能体的诅咒问题。

    

    马尔科夫博弈（MG）是多智能体强化学习（MARL）中的重要模型。长期以来人们一直认为“多智能体的诅咒”（即算法性能随着智能体数量指数级下降）是不可避免的，直到最近几篇作品（Daskalakis等人，2023年；Cui等人，2023年；Wang等人，2023年）。这些作品确实解决了多智能体的诅咒，当状态空间极大且（线性）函数逼近被应用时，它们要么具有更慢的收敛速度$O(T^{-1/4})$，要么在行动数$A_{\max}$上带来多项式依赖——尽管在单智能体情况下即使损失函数可以随时间任意变化（Dai等人，2023年），也可避免这种依赖。本文首先通过Wang等人（2023年）的“AVLPR”框架精化，洞察了基于数据的（即随机的）悲观估计子优化差距，从而允许更广泛的插件算法选择。当专门应用于MGs时，这一方法能够处理独立的情况。

    Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent
    
[^9]: 高效强化学习在部分可观察性下的应用

    Efficient Reinforcement Learning from Partial Observability

    [https://arxiv.org/abs/2311.12244](https://arxiv.org/abs/2311.12244)

    该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。

    

    在大多数实际应用中，状态信息只能部分观测到，这破坏了马尔科夫决策过程的假设，导致将观测与状态相混淆的算法表现不佳。而部分可观测马尔科夫决策过程（POMDP）提供了一个允许在学习、探索和规划中考虑部分可观测性的通用框架，但也带来了显著的计算和统计挑战。为解决这些困难，我们提出了一个基于表示的视角，提供了一个统一的框架和可行的算法方法，用于从部分观测中进行实际的强化学习。我们提供了理论分析来证明所提出算法的统计效率，并经验性地证明了在各种基准测试中，所提出的算法在部分观测下能够超越最先进性能，推动了可靠的强化学习。

    In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
    
[^10]: 噪声自适应（加速）随机重力球动量

    Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum. (arXiv:2401.06738v1 [math.OC])

    [http://arxiv.org/abs/2401.06738](http://arxiv.org/abs/2401.06738)

    本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。

    

    我们分析了在光滑，强凸环境中随机重力球动量（SHB）的收敛性。Kidambi等人（2018）表明，对于二次函数，SHB（带有小批量）无法达到加速的收敛速度，并猜想SHB的实际收益是小批量的副产品。我们通过展示当批量大小大于一定阈值时，SHB可以获得加速的收敛速度来证实这一观点。特别地，对于条件数为$\kappa$的强凸二次函数，我们证明了使用标准步长和动量参数的SHB具有$O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$的收敛速度，其中$T$为迭代次数，$\sigma^2$为随机梯度的方差。为确保收敛到极小值，我们提出了一种多阶段方法，结果是噪声自适应的$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$速度。对于一般的强凸函数，我们在实验中展示了所提方法的有效性。

    We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$ rate. For general strongly-
    
[^11]: 旗帜游戏：通过旗帜流形来获得鲁棒的主方向

    Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])

    [http://arxiv.org/abs/2401.04071](http://arxiv.org/abs/2401.04071)

    本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。

    

    主成分分析（PCA）及其对流形和异常数据的扩展，在计算机视觉和机器学习中是不可或缺的。本研究提出了PCA及其变种的统一形式，引入了基于线性子空间旗帜的框架，即逐渐增加维度的嵌套线性子空间的层次结构，不仅允许共同实现，还产生了新的未曾探索的变种。我们从广义化传统的PCA方法开始，这些方法要么最大化方差，要么最小化重构误差。我们扩展这些解释，通过考虑异常值和数据流形，开发出了大量新的降维算法。为了设计一种通用的计算方法，我们将鲁棒和对偶形式的PCA重新构建为在旗帜流形上的优化问题。然后，我们将主测地线近似（切线PCA）整合到这个基于旗帜的框架中，创造出一种新的方法。

    Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
    
[^12]: 通过核扭曲函数定制Mixup数据

    Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])

    [http://arxiv.org/abs/2311.01434](http://arxiv.org/abs/2311.01434)

    本研究提出了一种利用核扭曲函数对Mixup数据进行个性化处理的方法，通过动态改变插值系数的概率分布来实现更频繁和更强烈的混合相似数据点。实验证明这种方法不仅提高了模型性能，还提高了模型的校准性。

    

    数据增强是学习高效深度学习模型的重要基础。在所有提出的增强技术中，线性插值训练数据点（也称为Mixup）已被证明在许多应用中非常有效。然而，大多数研究都集中在选择合适的点进行混合，或者应用复杂的非线性插值，而我们则对更相似的点进行更频繁和更强烈的混合感兴趣。为此，我们提出了通过扭曲函数动态改变插值系数的概率分布的方法，取决于要组合的数据点之间的相似性。我们定义了一个高效而灵活的框架来实现这一点，以避免多样性的损失。我们进行了广泛的分类和回归任务实验，结果显示我们提出的方法既提高了模型的性能，又提高了模型的校准性。代码可在https://github.com/ENSTA-U2IS/torch-uncertainty上找到。

    Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
    
[^13]: 取消保护特征：从胸部X射线嵌入中消除保护特征

    Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])

    [http://arxiv.org/abs/2311.01349](http://arxiv.org/abs/2311.01349)

    论文的目的是利用正交化方法消除胸部X射线嵌入中的保护特征影响，并证明其有效性。研究结果表明保护特征对病理预测有显著影响，而应用正交化方法可以消除这些影响。

    

    目的：分析并消除深度学习模型中胸部X射线嵌入的保护特征影响。方法：使用正交化方法消除胸部X射线嵌入中的保护特征（如年龄、性别、种族）的影响，确保特征独立的结果。为了验证该方法的有效性，我们使用三个预训练模型（有监督对比、自监督对比和基线分类器模型）对MIMIC和CheXpert数据集进行了回顾性研究。我们的统计分析涉及通过估计保护特征影响和评估使用两种类型嵌入的能力来预测种族、年龄或性别的原始与正交嵌入的比较。结果：我们的实验揭示了保护特征对病理预测的显着影响。应用正交化方法可以消除这些特征影响。除了消除对病理分类的影响之外，

    Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models.  Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings.  Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, whil
    
[^14]: 生成性扩散模型的统计热力学

    The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])

    [http://arxiv.org/abs/2310.17467](http://arxiv.org/abs/2310.17467)

    本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。

    

    生成性扩散模型在生成建模的许多领域取得了惊人的表现。虽然这些模型的基本思想来自非平衡物理学，但本文中我们表明，可以用平衡统计力学的工具来理解这些模型的许多方面。利用这种重构，我们展示了生成性扩散模型经历了与对称性破缺现象相对应的二阶相变。我们认为，这导致了一种稳定性形式，它是生成能力的核心，并可以用一组平均场临界指数来描述。最后，我们根据热力学的公式分析了将扩散模型与关联记忆网络连接的最近研究。

    Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
    
[^15]: 机器学习模型的成员推断攻击的基本限制

    Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])

    [http://arxiv.org/abs/2310.13786](http://arxiv.org/abs/2310.13786)

    本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。

    

    成员推断攻击（MIA）可以揭示特定数据点是否是训练数据集的一部分，可能暴露个人的敏感信息。本文探讨了关于机器学习模型上MIA的基本统计限制。具体而言，我们首先推导了统计量，该统计量决定了这种攻击的有效性和成功率。然后，我们研究了几种情况，并对这个感兴趣的统计量提供了界限。这使我们能够根据样本数量和学习模型的其他结构参数推断潜在攻击的准确性，在某些情况下可以直接从数据集中估计。

    Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
    
[^16]: 通过近似传递消息实现结构化广义线性模型的谱估计器

    Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])

    [http://arxiv.org/abs/2308.14507](http://arxiv.org/abs/2308.14507)

    本论文研究了针对广义线性模型的参数估计问题，提出了一种通过谱估计器进行预处理的方法。通过对测量进行特征协方差矩阵Σ表示，分析了谱估计器在结构化设计中的性能，并确定了最优预处理以最小化样本数量。

    

    我们考虑从广义线性模型中的观测中进行参数估计的问题。谱方法是一种简单而有效的估计方法：它通过对观测进行适当预处理得到的矩阵的主特征向量来估计参数。尽管谱估计器被广泛使用，但对于结构化（即独立同分布的高斯和哈尔）设计，目前仅有对谱估计器的严格性能表征以及对数据进行预处理的基本方法可用。相反，实际的设计矩阵具有高度结构化并且表现出非平凡的相关性。为解决这个问题，我们考虑了捕捉测量的非各向同性特性的相关高斯设计，通过特征协方差矩阵Σ进行表示。我们的主要结果是对于这种情况下谱估计器性能的精确渐近分析。然后，可以通过这一结果来确定最优预处理，从而最小化所需样本的数量。

    We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
    
[^17]: 通过可变大小的压缩性建立数据相关的泛化界限

    Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05369](http://arxiv.org/abs/2303.05369)

    本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。

    

    本文通过引入“可变大小压缩性”框架，建立了一种新的数据相关泛化误差的上界。在这个框架中，算法的泛化误差与其输入数据的可变大小“压缩率”相关联。通过这种方式，我们得到的界限依赖于手头给定输入数据的经验分布，而不是其未知分布。我们建立的新的泛化界限包括尾部界限、期望值的尾部界限和期望界限。此外，我们的框架还可以推导出对输入数据和输出假设随机变量的任何函数的泛化界限。特别是，这些泛化界限包含并可能优于几种现有的基于PAC-Bayes和数据相关内在维度的界限，这些界限作为特殊情况得到复原，从而揭示出我们方法的统一特性。

    In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a "variable-size compressibility" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da
    
[^18]: 分布偏移的标签对齐正则化

    Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14960](http://arxiv.org/abs/2211.14960)

    这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。

    

    最近的研究强调了监督学习中的标签对齐属性（LAP），即数据集中所有标签的向量大部分在数据矩阵的前几个奇异向量的张成空间内。受到这一观察的启发，我们提出了一种无监督领域自适应的正则化方法，鼓励目标域中的预测与其前几个奇异向量对齐。与传统的领域适应方法专注于正则化表示不同，我们相反，通过在源域和目标域中使用LAP，用正则化分类器与无监督目标数据对齐。理论分析表明，在一定的假设下，我们的解决方案位于目标域数据的前几个右奇异向量的张成空间内，并与最优解对齐。通过消除经典领域适应理论中常见的最优联合风险假设的依赖，我们展示了该方法的有效性。

    Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
    

