# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage.](http://arxiv.org/abs/2303.16151) | 本论文介绍了一种用因子模型和收缩的方法预测大型实现协方差矩阵的模型。这种方法通过分解回报协方差矩阵并使用向量异质自回归模型进行估计，相对于标准基准提高了预测精度，并导致对最小方差组合的更好估计。 |
| [^2] | [Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models.](http://arxiv.org/abs/2303.16047) | 提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。 |
| [^3] | [A source separation approach to temporal graph modelling for computer networks.](http://arxiv.org/abs/2303.15950) | 本文针对计算机网络监测中存在的季节性变化问题，提出了一种源分离的方法，可以更好地进行建模和预测。 |
| [^4] | [Sparse Gaussian Processes with Spherical Harmonic Features Revisited.](http://arxiv.org/abs/2303.15948) | 该论文重新审视了带球谐特征的高斯过程模型，并提出了一种新的核函数，在连续深度的深度模型中使用，其中深度可以通过优化证据下界来估计为核超参数。此外，变分学习球谐相位引入了本征基础的稀疏性，使得可以处理比以前更大的输入维度，并允许学习高频变化。 |
| [^5] | [PDExplain: Contextual Modeling of PDEs in the Wild.](http://arxiv.org/abs/2303.15827) | 我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。 |
| [^6] | [qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization.](http://arxiv.org/abs/2303.15746) | 本文介绍了一种新的用于优化偏好反馈的贝叶斯优化函数qEUBO，并展示了它在许多设置中优于现有的采集函数。在充分的条件下，qEUBO的遗憾收敛速度快于现有采集函数qEI。 |
| [^7] | [Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases.](http://arxiv.org/abs/2303.15739) | 本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。 |
| [^8] | [Learning Rate Schedules in the Presence of Distribution Shift.](http://arxiv.org/abs/2303.15634) | 该论文提出了一种学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔，能够对分布转移具有鲁棒性，同时适用于凸损失函数和非凸损失函数。最优学习速率表通常会在数据分布转移的情况下增加，能够用于高维回归模型和神经网络。 |
| [^9] | [Online Learning for Incentive-Based Demand Response.](http://arxiv.org/abs/2303.15617) | 本文研究了在线学习管理需求响应（DR）资源中消费者基准估计的问题，并提出了一种采用最小二乘进行估计的在线学习方案，再通过引入激励价格上的扰动实现勘探和开发的平衡。 |
| [^10] | [Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning.](http://arxiv.org/abs/2303.15579) | 本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。 |
| [^11] | [Mathematical Challenges in Deep Learning.](http://arxiv.org/abs/2303.15464) | 本文总结了深度学习中涉及培训、推理、一般化边界和优化问题的一组数学挑战，为数学家、统计学家和理论计算机科学家提供了与深度学习领域交流的形式化工具。 |
| [^12] | [Spatial-photonic Boltzmann machines: low-rank combinatorial optimization and statistical learning by spatial light modulation.](http://arxiv.org/abs/2303.14993) | 本文提出了一种基于空间光调制的SPBM计算模型，可以高效地解决任何伊辛问题，特别适用于具有低秩相互作用矩阵的问题，并且具有学习、分类和采样的能力。 |
| [^13] | [Measuring Classification Decision Certainty and Doubt.](http://arxiv.org/abs/2303.14568) | 该论文提出了一种名为“确定性”和“不确定性”的得分方法来量化分类决策中预测的质量和不确定性。 |
| [^14] | [Statistical Inference with Stochastic Gradient Methods under $\phi$-mixing Data.](http://arxiv.org/abs/2302.12717) | 本文提出了一种基于 mini-batch SGD 估计器进行 $\varphi$-混合数据统计推断的方法，有效解决了普通方法在构建置信区间时面临的相关性问题。 |
| [^15] | [Communication-Efficient Distributed Estimation and Inference for Cox's Model.](http://arxiv.org/abs/2302.12111) | 我们提出了一种高效的分布式算法，用于在高维稀疏Cox比例风险模型中估计和推断，通过引入一种新的去偏差方法，我们可以产生渐近有效的分布式置信区间，并提供了有效的分布式假设检验。 |
| [^16] | [Learning to Generalize Provably in Learning to Optimize.](http://arxiv.org/abs/2302.11085) | 本文提出了一种统一的数据增强框架，用于学习到可以泛化地优化器和优化对象。该框架可以轻松地与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。 |
| [^17] | [JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models.](http://arxiv.org/abs/2302.09125) | 本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。 |
| [^18] | [Deep Riemannian Networks for EEG Decoding.](http://arxiv.org/abs/2212.10426) | 本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。 |
| [^19] | [A Statistical Model for Predicting Generalization in Few-Shot Classification.](http://arxiv.org/abs/2212.06461) | 提出了一种通过高斯模型估计特征分布参数进行预测泛化误差的方法，通过计算类条件密度距离估计可以提高泛化性能准确度。 |
| [^20] | [Dictionary Learning for the Almost-Linear Sparsity Regime.](http://arxiv.org/abs/2210.10855) | 本文提出了一种高效的谱方法SPORADIC，在几乎线性稀疏度下的字典学习问题中可以恢复超完备字典。 |
| [^21] | [One Transformer Can Understand Both 2D & 3D Molecular Data.](http://arxiv.org/abs/2210.01765) | 本文提出了一个基于Transformer的分子模型，名为Transformer-M，可以处理2D和3D格式的分子数据并生成有意义的语义表示。 |
| [^22] | [New Lower Bounds for Private Estimation and a Generalized Fingerprinting Lemma.](http://arxiv.org/abs/2205.08532) | 本文提供了针对高斯分布隐私协方差估计和有界协方差分布的均值估计的新下界，通过广义化指纹法到指数家族来证明这些下界的正确性。 |
| [^23] | [Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation.](http://arxiv.org/abs/2203.11740) | 该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。 |
| [^24] | [Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers.](http://arxiv.org/abs/2203.01859) | 对于存在模型规格不准确和异常值情况下的集成学习，本文提出了一个新的鲁棒自由能量准则，通过将广义对数得分函数与PAC$^m$结合，实现了更好的模型性能。 |
| [^25] | [Repulsive Deep Ensembles are Bayesian.](http://arxiv.org/abs/2106.11642) | 通过在深度集成的更新规则中引入核化排斥项，可以强制并维护成员之间的多样性，并使集成具有更好的性能表现和不确定性估计。 |
| [^26] | [A likelihood approach to nonparametric estimation of a singular distribution using deep generative models.](http://arxiv.org/abs/2105.04046) | 本文研究了使用深度生成模型进行奇异分布非参数估计的统计学性质，提出了通过样本噪声对数据进行扰动的解决方案，从而实现对潜在分布的一致估计和良好收敛率。 |
| [^27] | [Nonlinear classifiers for ranking problems based on kernelized SVM.](http://arxiv.org/abs/2002.11436) | 本文提出了一种基于核化 SVM 的非线性分类器，用于解决最高相关性样本的排名问题。 |

# 详细

[^1]: 预测大型实现协方差矩阵:因子模型和收缩的好处。

    Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage. (arXiv:2303.16151v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16151](http://arxiv.org/abs/2303.16151)

    本论文介绍了一种用因子模型和收缩的方法预测大型实现协方差矩阵的模型。这种方法通过分解回报协方差矩阵并使用向量异质自回归模型进行估计，相对于标准基准提高了预测精度，并导致对最小方差组合的更好估计。

    

    我们提出了一种模型来预测收益的大型实现协方差矩阵，并对S&P 500的成分股进行了应用。为了解决维数灾难，我们使用标准企业级别因子（如大小、价值和盈利能力）分解回报协方差矩阵，并在残差协方差矩阵中使用部门限制。然后，使用最小绝对收缩和选择运算符（LASSO）的向量异质自回归（VHAR）模型对该限制模型进行估计。相对于标准基准，我们的方法提高了预测精度，并导致对最小方差组合的更好估计。

    We propose a model to forecast large realized covariance matrices of returns, applying it to the constituents of the S\&P 500 daily. To address the curse of dimensionality, we decompose the return covariance matrix using standard firm-level factors (e.g., size, value, and profitability) and use sectoral restrictions in the residual covariance matrix. This restricted model is then estimated using vector heterogeneous autoregressive (VHAR) models with the least absolute shrinkage and selection operator (LASSO). Our methodology improves forecasting precision relative to standard benchmarks and leads to better estimates of minimum variance portfolios.
    
[^2]: 理解和探索稀疏广义可加模型的整个优秀集合

    Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])

    [http://arxiv.org/abs/2303.16047](http://arxiv.org/abs/2303.16047)

    提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。

    

    在实际应用中，机器学习模型与领域专家之间的交互至关重要；然而，通常只生成单个模型的经典机器学习范式不利于此类交互。近似和探索Rashomon集，即所有近乎最优模型的集合，通过提供用户可搜索的空间包含多样性模型的方法，解决了这一实际挑战，领域专家可以从中选择。我们提出了一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术。我们提供了用于近似具有固定支持集的GAMs的Rashomon集的椭球形算法，并使用这些椭球形近似了许多不同支持集的Rashomon集。近似的Rashomon集为解决实际挑战，例如（1）研究模型类的变量重要性；（2）在用户指定约束条件下查找模型，提供了重要的基础。

    In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
    
[^3]: 计算机网络的时间图分离建模方法

    A source separation approach to temporal graph modelling for computer networks. (arXiv:2303.15950v1 [cs.CR])

    [http://arxiv.org/abs/2303.15950](http://arxiv.org/abs/2303.15950)

    本文针对计算机网络监测中存在的季节性变化问题，提出了一种源分离的方法，可以更好地进行建模和预测。

    

    将企业计算机网络中的恶意活动检测视为一项时间链接预测任务：给定一系列代表主机之间通信的图表，目标是预测未来应该或不应该发生的边缘。然而，标准的时间链接预测算法不适用于计算机网络监测，因为它们没有考虑计算机网络活动的短期动态特性，这些特性表现出明显的季节性变化。为了建立一个更好的模型，我们提出了一种受源分离启发的计算机网络活动描述方法：在每个时间步骤，观察到的图是表示各种活动来源的子图的混合物，并且短期动态是由于混合系数的变化而产生的。定量和定性实验证明了我们方法的有效性。

    Detecting malicious activity within an enterprise computer network can be framed as a temporal link prediction task: given a sequence of graphs representing communications between hosts over time, the goal is to predict which edges should--or should not--occur in the future. However, standard temporal link prediction algorithms are ill-suited for computer network monitoring as they do not take account of the peculiar short-term dynamics of computer network activity, which exhibits sharp seasonal variations. In order to build a better model, we propose a source separation-inspired description of computer network activity: at each time step, the observed graph is a mixture of subgraphs representing various sources of activity, and short-term dynamics result from changes in the mixing coefficients. Both qualitative and quantitative experiments demonstrate the validity of our approach.
    
[^4]: 带球谐特征的稀疏高斯过程的再探讨

    Sparse Gaussian Processes with Spherical Harmonic Features Revisited. (arXiv:2303.15948v1 [stat.ML])

    [http://arxiv.org/abs/2303.15948](http://arxiv.org/abs/2303.15948)

    该论文重新审视了带球谐特征的高斯过程模型，并提出了一种新的核函数，在连续深度的深度模型中使用，其中深度可以通过优化证据下界来估计为核超参数。此外，变分学习球谐相位引入了本征基础的稀疏性，使得可以处理比以前更大的输入维度，并允许学习高频变化。

    

    我们重新审视了带有球谐特征的高斯过程模型，并研究了与其相关的RKHS、其特征值结构以及深度模型之间的联系。基于此，我们引入了一类新的核函数，它对应于具有连续深度的深度模型。在我们的公式中，深度可以通过优化证据下界来估计为核超参数。此外，我们通过变分学习球谐相位引入了本征基础的稀疏性。这使得我们可以处理比以前更大的输入维度，并且允许学习高频变化。我们在机器学习基准数据集上验证了我们的方法。

    We revisit the Gaussian process model with spherical harmonic features and study connections between the associated RKHS, its eigenstructure and deep models. Based on this, we introduce a new class of kernels which correspond to deep models of continuous depth. In our formulation, depth can be estimated as a kernel hyper-parameter by optimizing the evidence lower bound. Further, we introduce sparseness in the eigenbasis by variational learning of the spherical harmonic phases. This enables scaling to larger input dimensions than previously, while also allowing for learning of high frequency variations. We validate our approach on machine learning benchmark datasets.
    
[^5]: PDExplain：PDEs 在实际应用中的情境建模

    PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])

    [http://arxiv.org/abs/2303.15827](http://arxiv.org/abs/2303.15827)

    我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。

    

    我们提出了一种解释性的方法PDExplain用于解决偏微分方程。在训练阶段，我们的方法通过一个操作员定义的PDE家族的数据以及这个家族的一般形式进行馈送。在推断阶段，提供了一个从现象中收集到的最小样本，其中样本与 PDE 家族相关，但不一定属于训练阶段看到的具体 PDE 集合。我们展示了算法如何预测未来时间步的PDE解。此外，我们的方法提供了PDE的可解释形式，这种特征可以协助通过物理科学数据来对现象进行建模。为了验证我们的方法，我们进行了大量实验，考察了其在预测误差和可解释性方面的质量。

    We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
    
[^6]: qEUBO: 基于决策理论的、用于优化偏好反馈的贝叶斯优化函数

    qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization. (arXiv:2303.15746v1 [cs.LG])

    [http://arxiv.org/abs/2303.15746](http://arxiv.org/abs/2303.15746)

    本文介绍了一种新的用于优化偏好反馈的贝叶斯优化函数qEUBO，并展示了它在许多设置中优于现有的采集函数。在充分的条件下，qEUBO的遗憾收敛速度快于现有采集函数qEI。

    

    偏好贝叶斯优化(PBO)是一种用于使用偏好反馈优化决策制定者潜在效用函数的框架。本文将最好选项的预期效用(qEUBO)引入PBO作为一种新的采集函数。当决策制定者的响应无噪声时，我们展示qEUBO是一步贝叶斯最优的，并且与流行的知识梯度采集函数等效。我们还展示，当决策制定者的响应受到噪声污染时，qEUBO在一步贝叶斯最优策略上享有附加的近似保证。我们对qEUBO进行了广泛的评估，并证明在许多设置中，它优于PBO的最先进采集函数。最后，我们展示，在充分的正则化条件下，qEUBO的贝叶斯简单遗憾将以$O(1/n)$的速度趋近于零，其中$n$是查询数量。相比之下，我们展示了对于流行的PBO采集函数qEI，简单遗憾收敛速度较慢，为$O(1/\sqrt{n})$。

    Preferential Bayesian optimization (PBO) is a framework for optimizing a decision maker's latent utility function using preference feedback. This work introduces the expected utility of the best option (qEUBO) as a novel acquisition function for PBO. When the decision maker's responses are noise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent to the popular knowledge gradient acquisition function. We also show that qEUBO enjoys an additive constant approximation guarantee to the one-step Bayes-optimal policy when the decision maker's responses are corrupted by noise. We provide an extensive evaluation of qEUBO and demonstrate that it outperforms the state-of-the-art acquisition functions for PBO across many settings. Finally, we show that, under sufficient regularity conditions, qEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as the number of queries, $n$, goes to infinity. In contrast, we show that simple regret under qEI, a popular acquisiti
    
[^7]: 深度ReLU神经网络在过参数化情况下的贝叶斯自由能

    Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])

    [http://arxiv.org/abs/2303.15739](http://arxiv.org/abs/2303.15739)

    本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。

    

    在人工智能的许多研究领域中，深度神经网络已被证明可用于估计高维输入空间中的未知函数。然而，它们的泛化性能尚未从理论角度完全澄清，因为它们是不可识别的和奇异的学习机器。此外，ReLU函数不可微，奇异学习理论中的代数或解析方法无法应用于它。本文研究了一种过参数化情况下的深度ReLU神经网络，并证明了Bayesian自由能是有界的，即使层数比估计未知数据生成函数所必需的层数更多。由于Bayesian广义误差等于样本大小的自由能增加，因此我们的结果也表明，Bayesian广义误差不会增加。

    In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
    
[^8]: 学习速率表在分布转移条件下的应用

    Learning Rate Schedules in the Presence of Distribution Shift. (arXiv:2303.15634v1 [cs.LG])

    [http://arxiv.org/abs/2303.15634](http://arxiv.org/abs/2303.15634)

    该论文提出了一种学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔，能够对分布转移具有鲁棒性，同时适用于凸损失函数和非凸损失函数。最优学习速率表通常会在数据分布转移的情况下增加，能够用于高维回归模型和神经网络。

    

    我们设计了学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔。我们通过随机微分方程的新颖分析，完全表征了在线线性回归的最优学习速率表。对于一般的凸损失函数，我们提出了新的学习速率表，对分布转移具有鲁棒性，我们给出了只有常数差异的后悔上下界。对于非凸损失函数，我们基于估计模型的梯度范数定义了一种后悔概念，并提出了一种学习时间表，以最小化总预期后悔的上限。直观地说，我们预计损失领域的变化需要更多的探索，我们证实了最优学习速率表通常会在数据分布转移的情况下增加。最后，我们提供了针对高维回归模型和神经网络的实验，以说明这些学习速率表的应用。

    We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedule
    
[^9]: 针对基于激励的需求响应的在线学习

    Online Learning for Incentive-Based Demand Response. (arXiv:2303.15617v1 [cs.LG])

    [http://arxiv.org/abs/2303.15617](http://arxiv.org/abs/2303.15617)

    本文研究了在线学习管理需求响应（DR）资源中消费者基准估计的问题，并提出了一种采用最小二乘进行估计的在线学习方案，再通过引入激励价格上的扰动实现勘探和开发的平衡。

    

    本文研究了在线学习管理需求响应（DR）资源的问题。典型DR机制要求DR经理为参与的消费者分配一个基准，其中基准是消费者计数事实消耗的估计，如果不叫消费者提供DR服务，那么基准就是计数的理论消耗。估算基准的挑战在于消费者有鼓励膨胀基准估计的动机。我们考虑学习在线估算基线和在这样的激励下优化一段时间的操作成本。提出了一种在线学习方案，它采用最小二乘进行估计，同时在DR服务或负荷裁剪的激励价格上引入扰动，旨在平衡在线学习中出现的勘探和开发的折衷。我们证明了，我们的提议方案能够实现与最优操作相比非常低的遗憾度（$ \mathcal {O} \left((\log {T})^2\right)$）

    In this paper, we consider the problem of learning online to manage Demand Response (DR) resources. A typical DR mechanism requires the DR manager to assign a baseline to the participating consumer, where the baseline is an estimate of the counterfactual consumption of the consumer had it not been called to provide the DR service. A challenge in estimating baseline is the incentive the consumer has to inflate the baseline estimate. We consider the problem of learning online to estimate the baseline and to optimize the operating costs over a period of time under such incentives. We propose an online learning scheme that employs least-squares for estimation with a perturbation to the reward price (for the DR services or load curtailment) that is designed to balance the exploration and exploitation trade-off that arises with online learning. We show that, our proposed scheme is able to achieve a very low regret of $\mathcal{O}\left((\log{T})^2\right)$ with respect to the optimal operating
    
[^10]: 统计学习中的调整Wasserstein分布鲁棒估计

    Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])

    [http://arxiv.org/abs/2303.15579](http://arxiv.org/abs/2303.15579)

    本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。

    

    我们在统计学习中提出了一种调整的Wasserstein分布鲁棒估计——基于Wasserstein分布鲁棒估计（WDRO）的非线性转换。这种转换将提高WDRO的统计性能，因为调整后的WDRO估计器渐进无偏并且均方误差趋近于零。调整后的WDRO不会削弱WDRO的样本外性能保证。我们提出了调整WDRO估计器的存在的充分条件，并给出了计算调整WDRO估计器的过程。具体而言，我们将展示如何在广义线性模型中开发调整WDRO估计器。数值实验表明，调整后的估计器比经典估计器具有更好的实际性能。

    We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
    
[^11]: 深度学习中的数学挑战

    Mathematical Challenges in Deep Learning. (arXiv:2303.15464v1 [cs.LG])

    [http://arxiv.org/abs/2303.15464](http://arxiv.org/abs/2303.15464)

    本文总结了深度学习中涉及培训、推理、一般化边界和优化问题的一组数学挑战，为数学家、统计学家和理论计算机科学家提供了与深度学习领域交流的形式化工具。

    

    自从2012年的ImageNet挑战以来，深度模型已经主宰了人工智能领域。深度模型的大小从那时起一直在增加，这给在手机、个人电脑、自动驾驶车辆和无线基站等领域应用的这一领域带来了新的挑战。在这里，我们列出了一组问题，涵盖培训、推理、一般化边界和优化问题，并用一些形式化语言来与数学家、统计学家和理论计算机科学家交流这些挑战。这是对深度学习研究问题的主观看法，它有益于长期的技术发展。

    Deep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.
    
[^12]: 空间-光子Boltzmann机：利用空间光调制进行低秩组合优化和统计学习（arXiv:2303.14993v1 [cond-mat.dis-nn] CROSS LISTED）

    Spatial-photonic Boltzmann machines: low-rank combinatorial optimization and statistical learning by spatial light modulation. (arXiv:2303.14993v1 [cond-mat.dis-nn] CROSS LISTED)

    [http://arxiv.org/abs/2303.14993](http://arxiv.org/abs/2303.14993)

    本文提出了一种基于空间光调制的SPBM计算模型，可以高效地解决任何伊辛问题，特别适用于具有低秩相互作用矩阵的问题，并且具有学习、分类和采样的能力。

    

    空间-光子伊辛机（SPIM）[D. Pierangeli et al., Phys. Rev. Lett. 122, 213902 (2019)]是一种使用空间光调制有效解决大规模组合优化问题的光学架构。然而，SPIM仅能容纳具有秩为一的相互作用矩阵的伊辛问题，这限制了其在各种实际问题中的适用性。在本文中，我们提出了一种新的SPIM计算模型，可以在不改变其光学实现的情况下容纳任何伊辛问题。该模型对于具有低秩相互作用矩阵的伊辛问题（如背包问题）特别有效。此外，该模型具有学习能力，因此可以被称为空间光子Boltzmann机（SPBM）。我们证明了使用具有低秩相互作用的SPBM有效地实现了MNIST手写数字图像的学习、分类和采样。因此，所提出的SPBM模型表现出更高的实用性。

    The spatial-photonic Ising machine (SPIM) [D. Pierangeli et al., Phys. Rev. Lett. 122, 213902 (2019)] is a promising optical architecture utilizing spatial light modulation for solving large-scale combinatorial optimization problems efficiently. However, the SPIM can accommodate Ising problems with only rank-one interaction matrices, which limits its applicability to various real-world problems. In this Letter, we propose a new computing model for the SPIM that can accommodate any Ising problem without changing its optical implementation. The proposed model is particularly efficient for Ising problems with low-rank interaction matrices, such as knapsack problems. Moreover, the model acquires learning ability and can thus be termed a spatial-photonic Boltzmann machine (SPBM). We demonstrate that learning, classification, and sampling of the MNIST handwritten digit images are achieved efficiently using SPBMs with low-rank interactions. Thus, the proposed SPBM model exhibits higher practi
    
[^13]: 量化分类决策的确定性和不确定性测量

    Measuring Classification Decision Certainty and Doubt. (arXiv:2303.14568v1 [stat.ML])

    [http://arxiv.org/abs/2303.14568](http://arxiv.org/abs/2303.14568)

    该论文提出了一种名为“确定性”和“不确定性”的得分方法来量化分类决策中预测的质量和不确定性。

    

    确定性和不确定性的定量表征和估计在优化和决策过程中具有基础重要性。本文提出了直观的得分，称为“确定性”和“不确定性”，可在贝叶斯和频率主义框架下用于评估和比较（多）分类决策机器学习问题的预测质量和不确定性。

    Quantitative characterizations and estimations of uncertainty are of fundamental importance in optimization and decision-making processes. Herein, we propose intuitive scores, which we call \textit{certainty} and \textit{doubt}, that can be used in both a Bayesian and frequentist framework to assess and compare the quality and uncertainty of predictions in (multi-)classification decision machine learning problems.
    
[^14]: 基于 $\varphi$-混合数据的随机梯度方法在统计推断中的应用

    Statistical Inference with Stochastic Gradient Methods under $\phi$-mixing Data. (arXiv:2302.12717v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.12717](http://arxiv.org/abs/2302.12717)

    本文提出了一种基于 mini-batch SGD 估计器进行 $\varphi$-混合数据统计推断的方法，有效解决了普通方法在构建置信区间时面临的相关性问题。

    

    随机梯度下降（SGD）是一种可扩展且内存效率高的优化算法，适用于大型数据集和流式数据的处理，因此受到了广泛关注和欢迎。SGD 基于的估计器在统计推断中的应用，如区间估计，也取得了巨大成功。然而，大多数相关工作都是基于独立同分布观测或马尔可夫链的。当观测数据来自一个混合时间序列时，如何进行有效的统计推断尚未研究。事实上，观测数据之间的一般相关性给区间估计带来了挑战。大多数现有方法可能会忽略这种相关性并导致无效的置信区间。本文提出了一种基于 mini-batch SGD 估计器进行 $\varphi$-混合数据统计推断的方法。置信区间是使用相关的 mini-batch bootstrap SGD 程序构建的。通过使用 \cite{yu1994rates} 中的 “独立块” 技巧，我们证明了该方法的有效性。

    Stochastic gradient descent (SGD) is a scalable and memory-efficient optimization algorithm for large datasets and stream data, which has drawn a great deal of attention and popularity. The applications of SGD-based estimators to statistical inference such as interval estimation have also achieved great success. However, most of the related works are based on i.i.d. observations or Markov chains. When the observations come from a mixing time series, how to conduct valid statistical inference remains unexplored. As a matter of fact, the general correlation among observations imposes a challenge on interval estimation. Most existing methods may ignore this correlation and lead to invalid confidence intervals. In this paper, we propose a mini-batch SGD estimator for statistical inference when the data is $\phi$-mixing. The confidence intervals are constructed using an associated mini-batch bootstrap SGD procedure. Using ``independent block'' trick from \cite{yu1994rates}, we show that the
    
[^15]: 面向Cox模型的高效通信式分布式估计和推断

    Communication-Efficient Distributed Estimation and Inference for Cox's Model. (arXiv:2302.12111v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.12111](http://arxiv.org/abs/2302.12111)

    我们提出了一种高效的分布式算法，用于在高维稀疏Cox比例风险模型中估计和推断，通过引入一种新的去偏差方法，我们可以产生渐近有效的分布式置信区间，并提供了有效的分布式假设检验。

    

    针对因隐私和所有权问题无法共享个体数据的多中心生物医学研究，我们开发了高维稀疏Cox比例风险模型的通信高效迭代分布式算法用于估计和推断。我们证明了即使进行了相对较少的迭代，我们的估计值在非常温和的条件下可以达到与理想全样本估计值相同的收敛速度。为了构建高维危险回归系数的线性组合的置信区间，我们引入了一种新的去偏差方法，建立了中心极限定理，并提供了一致的方差估计，可以产生渐近有效的分布式置信区间。此外，我们提供了基于装饰分数检验的任意坐标元素的有效和强大的分布式假设检验。我们还允许时间依赖协变量以及被审查的生存时间。在多种数据集上进行了广泛的数字实验，证明了算法的有效性和效率。

    Motivated by multi-center biomedical studies that cannot share individual data due to privacy and ownership concerns, we develop communication-efficient iterative distributed algorithms for estimation and inference in the high-dimensional sparse Cox proportional hazards model. We demonstrate that our estimator, even with a relatively small number of iterations, achieves the same convergence rate as the ideal full-sample estimator under very mild conditions. To construct confidence intervals for linear combinations of high-dimensional hazard regression coefficients, we introduce a novel debiased method, establish central limit theorems, and provide consistent variance estimators that yield asymptotically valid distributed confidence intervals. In addition, we provide valid and powerful distributed hypothesis tests for any coordinate element based on a decorrelated score test. We allow time-dependent covariates as well as censored survival times. Extensive numerical experiments on both s
    
[^16]: 在「学习优化」中学习一般化的保证（Learning to Generalize Provably in Learning to Optimize）

    Learning to Generalize Provably in Learning to Optimize. (arXiv:2302.11085v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11085](http://arxiv.org/abs/2302.11085)

    本文提出了一种统一的数据增强框架，用于学习到可以泛化地优化器和优化对象。该框架可以轻松地与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。

    

    「学习优化」（Learning to optimize，L2O）已经变得越来越流行，通过数据驱动的方法自动化设计优化器。然而，当前的 L2O 方法在至少两个方面表现不佳：（i）将 L2O 设计的优化器应用于未见过的优化对象时降低其损失函数值（优化器一般化或“可泛化的优化器学习”）；以及（ii）由优化器训练的优化对象（本身为机器学习模型）在准确性上面对未见过的数据表现的测试性能（优化对象一般化或“学习一般化”）。虽然优化器一般化最近已被研究，但优化对象一般化在 L2O 上尚未得到严格研究，这是本文的目的。我们首先理论上建立了局部熵与 Hessian 之间的隐式联系，从而统一了它们在通用优化技术的手工设计中的作用。基于这种联系，我们提出了一个统一的 “数据增强框架" 来学习 L2O 的一般化实现。我们的框架由两部分组成：一部分是 GO 模块，通过随机梯度下降方法最大化通用化目标学习一个通用化算法；另一部分是优化对象模块，通过可微分优化器和标准交叉熵损失函数学习机器学习模型。我们的框架可以轻松与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。

    Learning to optimize (L2O) has gained increasing popularity, which automates the design of optimizers by data-driven approaches. However, current L2O methods often suffer from poor generalization performance in at least two folds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss function values (optimizer generalization, or ``generalizable learning of optimizers"); and (ii) the test performance of an optimizee (itself as a machine learning model), trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization, or ``learning to generalize"). While the optimizer generalization has been recently studied, the optimizee generalization (or learning to generalize) has not been rigorously studied in the L2O context, which is the aim of this paper. We first theoretically establish an implicit connection between the local entropy and the Hessian, and hence unify their roles in the handcrafted design of generalizable optim
    
[^17]: JANA：复杂贝叶斯模型的联合分摊近似神经网络

    JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09125](http://arxiv.org/abs/2302.09125)

    本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。

    

    本文提出了“联合分摊神经网络近似”（JANA）方法，用于处理贝叶斯代理建模和基于模拟的推理中出现的难以计算的似然函数和后验密度。我们以端到端的方式训练三个相互补充的神经网络：1）一个总结网络，将个别数据点、集合或时间序列压缩成信息嵌入向量；2）一个后验网络，学习分摊的近似后验；3）一个似然网络，学习分摊的近似似然。它们的交互为分摊边缘似然和后验预测估计提供了新的途径，这是贝叶斯工作流程的两个重要组成部分，常常对于标准方法来说太昂贵了。我们在各种模拟模型中对JANA的保真度进行了基准测试，与最先进的贝叶斯方法进行了比较，并提出了一种强大而可解释的联合校准诊断方法。此外，我们研究了循环似然网络模拟复杂模型的能力。

    This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp
    
[^18]: EEG解码的深度黎曼网络

    Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10426](http://arxiv.org/abs/2212.10426)

    本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。

    

    当前在电脑脑电图（EEG）解码任务中，最先进的性能通常是由深度学习或基于黎曼几何的解码器实现的。最近，越来越多的人对深度黎曼网络（DRNs）产生了兴趣，可能结合了之前两类方法的优点。然而，还有一系列问题需要进一步洞察，以铺平DRNs在EEG中更广泛应用的道路。这些问题包括架构设计问题，如网络大小和端到端能力，以及模型训练问题。这些因素如何影响模型性能尚未被探索。此外，这些网络中的数据如何转换，以及是否与传统的EEG解码相关也不清楚。本研究旨在通过分析具有广泛超参数的DRNs来奠定这些主题领域的基础。使用两个公共EEG数据集测试了网络，并与最先进的基于黎曼几何的方法进行了比较。

    State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
    
[^19]: 一种预测Few-Shot分类泛化的统计模型

    A Statistical Model for Predicting Generalization in Few-Shot Classification. (arXiv:2212.06461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06461](http://arxiv.org/abs/2212.06461)

    提出了一种通过高斯模型估计特征分布参数进行预测泛化误差的方法，通过计算类条件密度距离估计可以提高泛化性能准确度。

    

    分类器泛化误差的估计通常依赖于验证集。然而，在Few-Shot学习场景中，很难获得这样的验证集，这是该领域中一个高度被忽视的缺点。因此，在这项工作中，我们引入了一个特征分布的高斯模型，通过估计这个模型的参数，我们能够预测在新的Few-Shot分类任务中的分类性能。我们发现，在类条件密度之间准确的距离估计是准确评估泛化性能的关键。因此，我们提出了一个非偏估计器来计算这些距离，并将其集成到我们的数值分析中。我们通过实验证明，我们的方法胜过了其他方法，例如留一法-Cross Validation 策略。

    The estimation of the generalization error of classifiers often relies on a validation set. Such a set is hardly available in few-shot learning scenarios, a highly disregarded shortcoming in the field. In these scenarios, it is common to rely on features extracted from pre-trained neural networks combined with distance-based classifiers such as nearest class mean. In this work, we introduce a Gaussian model of the feature distribution. By estimating the parameters of this model, we are able to predict the generalization error on new classification tasks with few samples. We observe that accurate distance estimates between class-conditional densities are the key to accurate estimates of the generalization performance. Therefore, we propose an unbiased estimator for these distances and integrate it in our numerical analysis. We empirically show that our approach outperforms alternatives such as the leave-one-out cross-validation strategy.
    
[^20]: 几乎线性稀疏度下的字典学习

    Dictionary Learning for the Almost-Linear Sparsity Regime. (arXiv:2210.10855v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10855](http://arxiv.org/abs/2210.10855)

    本文提出了一种高效的谱方法SPORADIC，在几乎线性稀疏度下的字典学习问题中可以恢复超完备字典。

    

    字典学习指的是从形如$\mathbf{y}_i = \mathbf{D}\mathbf{x}_i$的样本中恢复一个矩阵$\mathbf{D} \in \mathbb{R}^{M \times K}$和$N$个$s$-稀疏向量$\mathbf{x}_i \in \mathbb{R}^{K}$的问题。在字典已知的情况下，即使稀疏度线性增长到尺寸$M$，也可以恢复$x_i$，但迄今为止，唯一能在线性稀疏度范围内有保证成功的算法是黎曼信赖区域方法，这种方法仅适用于正交字典，并且基于平方和层次的方法需要超多项式时间才能获得在$M$中衰减的误差。在这项工作中，我们介绍了SPORADIC（SPectral ORAcle DICtionary Learning），这是一种基于一系列加权协方差矩阵的高效谱方法。我们证明，在足够高的维度下，SPORADIC可以恢复满足$K>M$的超完备字典。

    Dictionary learning, the problem of recovering a sparsely used matrix $\mathbf{D} \in \mathbb{R}^{M \times K}$ and $N$ $s$-sparse vectors $\mathbf{x}_i \in \mathbb{R}^{K}$ from samples of the form $\mathbf{y}_i = \mathbf{D}\mathbf{x}_i$, is of increasing importance to applications in signal processing and data science. When the dictionary is known, recovery of $\mathbf{x}_i$ is possible even for sparsity linear in dimension $M$, yet to date, the only algorithms which provably succeed in the linear sparsity regime are Riemannian trust-region methods, which are limited to orthogonal dictionaries, and methods based on the sum-of-squares hierarchy, which requires super-polynomial time in order to obtain an error which decays in $M$. In this work, we introduce SPORADIC (SPectral ORAcle DICtionary Learning), an efficient spectral method on family of reweighted covariance matrices. We prove that in high enough dimensions, SPORADIC can recover overcomplete ($K > M$) dictionaries satisfying the
    
[^21]: 一个Transformer模型可同时处理2D和3D分子数据

    One Transformer Can Understand Both 2D & 3D Molecular Data. (arXiv:2210.01765v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01765](http://arxiv.org/abs/2210.01765)

    本文提出了一个基于Transformer的分子模型，名为Transformer-M，可以处理2D和3D格式的分子数据并生成有意义的语义表示。

    

    与通常有唯一格式的视觉和语言数据不同，分子可以自然地用不同的化学公式进行表征。对于分子表示学习，大多数先前的工作只设计了针对特定数据格式的神经网络，使得学习的模型可能无法处理其他数据格式。我们认为，化学的通用神经网络模型应能够处理跨数据模态的分子任务。为实现此目标，我们开发了一种新型的基于Transformer的分子模型，称为Transformer-M，它可以将2D或3D格式的分子数据作为输入并生成有意义的语义表示。使用标准Transformer作为骨干架构，Transformer-M开发了两个分离的通道来编码2D和3D结构信息，并将它们与网络模块中的原子特征结合起来。

    Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. Whe
    
[^22]: 隐私估计的新下界和广义指纹引理

    New Lower Bounds for Private Estimation and a Generalized Fingerprinting Lemma. (arXiv:2205.08532v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2205.08532](http://arxiv.org/abs/2205.08532)

    本文提供了针对高斯分布隐私协方差估计和有界协方差分布的均值估计的新下界，通过广义化指纹法到指数家族来证明这些下界的正确性。

    

    我们证明了在$(\varepsilon, \delta)$-差分隐私约束下统计估计任务的新的下界。首先，我们给出了高斯分布隐私协方差估计的严格下界。我们证明，在Frobenius范数下估计协方差矩阵需要$\Omega(d^2)$个样本，在谱范数下需要$\Omega(d^{3/2})$个样本，两者都匹配上界，除了对数因子。后一项下界验证了一个关于高斯协方差谱估计的隐私和非隐私样本复杂度的猜想统计差距的存在。我们通过将指纹方法广义化到指数家族来证明这些下界是正确的技术贡献。此外，使用Acharya，Sun和Zhang提出的差分隐私Assouad方法，我们在$\ell_2$-距离下表明了在有界协方差分布的均值估计中，到$\alpha$误差的严格$\Omega(d/(\alpha^2 \varepsilon))$下界。

    We prove new lower bounds for statistical estimation tasks under the constraint of $(\varepsilon, \delta)$-differential privacy. First, we provide tight lower bounds for private covariance estimation of Gaussian distributions. We show that estimating the covariance matrix in Frobenius norm requires $\Omega(d^2)$ samples, and in spectral norm requires $\Omega(d^{3/2})$ samples, both matching upper bounds up to logarithmic factors. The latter bound verifies the existence of a conjectured statistical gap between the private and the non-private sample complexities for spectral estimation of Gaussian covariances. We prove these bounds via our main technical contribution, a broad generalization of the fingerprinting method to exponential families. Additionally, using the private Assouad method of Acharya, Sun, and Zhang, we show a tight $\Omega(d/(\alpha^2 \varepsilon))$ lower bound for estimating the mean of a distribution with bounded covariance to $\alpha$-error in $\ell_2$-distance. Prio
    
[^23]: 基于星形细胞对关键期的神经可塑性神经网络，通过现有和记忆性的大脑可塑性和突触形成实现突触竞争和强度平衡。（arXiv: 2203.11740v12 [cs.NE] UPDATED）

    Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11740](http://arxiv.org/abs/2203.11740)

    该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。

    

    除了突触共享连接权重之外，PNN还包括突触有效范围的权重[14-25]。PNN考虑突触强度平衡在突触吞噬的动态和长度常数之和的静态中[14]，并包含了鱼群行为的先导行为。突触形成在实验和模拟中会抑制树突生成[15]。类似于Spring Boot中的强制韧性，反向回路的记忆持久度梯度也存在。相对较好和较差的梯度信息存储在类似于脑褶的记忆痕迹细胞中，在反向回路的突触形成中。争议认为人类海马神经元的再生能力是否持续到老年，并可能在后期迭代中形成新的更长的回路[17,18]。关闭关键期会导致神经紊乱在实验和模拟中[19]。考虑到负面和正面记忆的持久性，有助于更好地激活突触。

    In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
    
[^24]: 鲁棒PAC$^m$: 在模型规格不准确和存在异常值情况下训练集成模型

    Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers. (arXiv:2203.01859v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01859](http://arxiv.org/abs/2203.01859)

    对于存在模型规格不准确和异常值情况下的集成学习，本文提出了一个新的鲁棒自由能量准则，通过将广义对数得分函数与PAC$^m$结合，实现了更好的模型性能。

    

    传统的贝叶斯学习在模型规格不准确和存在异常值的情况下已知存在泛化能力的不足。PAC-Bayes理论证明了贝叶斯学习所最小化的自由能量准则是在假设未被异常值污染的采样分布下，对Gibbs预测器（即从后验随机抽取的单个模型）的泛化误差的一个上界。该观点提供了贝叶斯学习在模型规格不准确且需要集成，以及数据受到异常值影响时的局限性的证明。最近的工作中，推导出了PAC-Bayes上界 - 称为PAC$^m$ - 引入了自由能量度量，可考虑集合预测器的性能，从而获得在模型不准确的情况下提高模型性能。本文提出了一种新的鲁棒自由能量准则，将广义对数得分函数与PAC$^m$集成上界相结合。建议的自由能量训练...（摘要未完，详情请查看原文）

    Standard Bayesian learning is known to have suboptimal generalization capabilities under model misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training 
    
[^25]: 拒绝性深度集成是贝叶斯的

    Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11642](http://arxiv.org/abs/2106.11642)

    通过在深度集成的更新规则中引入核化排斥项，可以强制并维护成员之间的多样性，并使集成具有更好的性能表现和不确定性估计。

    

    深度集成因其概念上的简单和高效而受到深度学习界的欢迎。然而，对于独立使用梯度下降训练的集成成员之间的功能多样性的维护是具有挑战性的。这可能会导致添加更多集成成员时出现病态，例如集成性能的饱和，它会收敛到单个模型的性能。此外，这不仅影响其预测的质量，而且更加影响集成的不确定性估计，从而影响其在超出分布数据上的性能。我们假设这种限制可以通过阻止不同集成成员坍塌到相同功能来克服。为此，我们在深度集成的更新规则中引入了一个核化的排斥术语。我们表明，这个简单的修改不仅强制并维护成员之间的多样性，而且更重要的是，将最大的后验值转化为...（原文截止此处）

    Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posterio
    
[^26]: 使用深度生成模型的似然方法进行奇异分布的非参数估计

    A likelihood approach to nonparametric estimation of a singular distribution using deep generative models. (arXiv:2105.04046v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.04046](http://arxiv.org/abs/2105.04046)

    本文研究了使用深度生成模型进行奇异分布非参数估计的统计学性质，提出了通过样本噪声对数据进行扰动的解决方案，从而实现对潜在分布的一致估计和良好收敛率。

    

    本文研究了使用深度生成模型进行奇异分布非参数估计的统计学性质。在考虑的模型中，利用深度生成模型建模高维数据，并假设数据集中在一些低维结构周围。由于支撑低维结构的分布在环境空间中对勒贝格测度具有奇异性，因此用通常的似然方法来估计目标分布可能无法达到一致性。本文证明了使用样本噪声对数据进行扰动可以得到新颖而有效的解决方案，从而实现对潜在分布的一致估计和良好收敛率。此外，本文还表征了可以通过深度生成模型有效估计的分布类。这个类足够普遍，可以容纳各种类型的分布。

    We investigate statistical properties of a likelihood approach to nonparametric estimation of a singular distribution using deep generative models. More specifically, a deep generative model is used to model high-dimensional data that are assumed to concentrate around some low-dimensional structure. Estimating the distribution supported on this low-dimensional structure, such as a low-dimensional manifold, is challenging due to its singularity with respect to the Lebesgue measure in the ambient space. In the considered model, a usual likelihood approach can fail to estimate the target distribution consistently due to the singularity. We prove that a novel and effective solution exists by perturbing the data with an instance noise, which leads to consistent estimation of the underlying distribution with desirable convergence rates. We also characterize the class of distributions that can be efficiently estimated via deep generative models. This class is sufficiently general to contain v
    
[^27]: 基于核化 SVM 的排名问题非线性分类器

    Nonlinear classifiers for ranking problems based on kernelized SVM. (arXiv:2002.11436v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.11436](http://arxiv.org/abs/2002.11436)

    本文提出了一种基于核化 SVM 的非线性分类器，用于解决最高相关性样本的排名问题。

    

    许多分类问题仅关注最具相关性的样本的性能而非所有样本。例如，排名问题、顶部准确度或搜索引擎仅关注前几个查询的结果。我们之前已经推导出包括多个线性分类问题类别的通用框架。本文将该框架扩展到非线性分类器。利用 SVM 的相似性，我们对问题进行对偶化处理，添加了核函数，并提出了一个分量对偶上升方法。

    Many classification problems focus on maximizing the performance only on the samples with the highest relevance instead of all samples. As an example, we can mention ranking problems, accuracy at the top or search engines where only the top few queries matter. In our previous work, we derived a general framework including several classes of these linear classification problems. In this paper, we extend the framework to nonlinear classifiers. Utilizing a similarity to SVM, we dualize the problems, add kernels and propose a componentwise dual ascent method.
    

