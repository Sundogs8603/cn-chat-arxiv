# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Large Language Models as Tool Makers.](http://arxiv.org/abs/2305.17126) | 本文提出了一个闭环框架，即LLMs作为工具制造者（LATM），使LLMs能够自主地创建用于解决问题的工具，而不需要依赖于现有的外部工具。 |
| [^2] | [Manifold Regularization for Memory-Efficient Training of Deep Neural Networks.](http://arxiv.org/abs/2305.17119) | 本文提出了一种利用流形正则化目标和诱导偏差网络设计原则的框架来实现深度神经网络的内存高效训练，相对于传统学习技术可获得更好的绝对性能和实证一般化误差，经实验验证有效。 |
| [^3] | [A Policy Gradient Method for Confounded POMDPs.](http://arxiv.org/abs/2305.17083) | 本文提出了一种针对混淆部分可观测马尔可夫决策过程的新型策略梯度方法，该方法在离线设置下可同时处理连续状态和观察空间，具有高效性和准确性。 |
| [^4] | [Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models.](http://arxiv.org/abs/2305.17076) | 本文表明Wasserstein分布式强鲁棒估计器的泛化保证适用于一般模型类别，不受维数灾难所困扰，甚至可以涵盖测试时的分布变化。 |
| [^5] | [Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural Networks.](http://arxiv.org/abs/2305.17063) | 提出了一种基于深度神经网络内部表征的Vecchia高斯过程集成方法，该方法通过将标准高斯过程与DNN相结合，生成一种不仅能够量化不确定性，而且能够提供更准确和更稳健的预测的深度Vecchia集合。 |
| [^6] | [Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach.](http://arxiv.org/abs/2305.17058) | 该论文提出一种精确推理离散统计模型的贝叶斯方法，支持离散采样、连续采样、离散观测、仿射函数、（随机）分支和事件条件。通过概率生成函数实现后验概率、期望、方差和高阶矩的精确计算。该方法性能优于近似蒙特卡洛方法，并避免了近似误差。 |
| [^7] | [Explaining Deep Learning for ECG Analysis: Building Blocks for Auditing and Knowledge Discovery.](http://arxiv.org/abs/2305.17043) | 本文介绍了可解释人工智能(XAI)方法在心电图(ECG)分析中的应用，提出了一套检查措施以确定合理的归因方法，并通过对患者亚组的数据分析，展示了这些XAI技术如何被用于知识发现，如识别心肌梗死亚型。 |
| [^8] | [Better Batch for Deep Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.17028) | 该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。 |
| [^9] | [GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations.](http://arxiv.org/abs/2305.17021) | GLOBE-CE是一种用于全球因果解释的机器学习方法，它可以超越局部解释，提供更有效和交互式的解释工具。 |
| [^10] | [Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets.](http://arxiv.org/abs/2305.17010) | 本文提出了一种名为GFlowNets的机器，可以有效地解决组合优化问题，同时在训练方面进行了优化，结果表明其可以高效地找到高质量的解决方案。 |
| [^11] | [Universal approximation with complex-valued deep narrow neural networks.](http://arxiv.org/abs/2305.16910) | 本文研究了具有有界宽度和任意深度的复值神经网络的普适性，发现当且仅当激活函数既不是全纯的，也不是反全纯的，也不是 $\mathbb{R}$-仿射的时，深窄的复值网络具有普适逼近能力。我们还发现足够的宽度依赖于考虑的激活函数，对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。 |
| [^12] | [Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference.](http://arxiv.org/abs/2305.16905) | 本文提出了拉普拉斯逼近神经加性模型，该模型从贝叶斯角度考虑加性结构，在恢复的特征交互中提供可信区间，提供可处理的边缘似然估计，可用于执行隐式特征选择并对特征对进行排名。 |
| [^13] | [Feature Adaptation for Sparse Linear Regression.](http://arxiv.org/abs/2305.16892) | 本研究提供了一种可自适应的算法，可以在一定程度上容忍协变量中的近似依赖关系数量，从而达到在一定条件下具有近乎最佳样本复杂度的稀疏线性回归。 |
| [^14] | [Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks.](http://arxiv.org/abs/2305.16891) | 本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。 |
| [^15] | [Error Bounds for Flow Matching Methods.](http://arxiv.org/abs/2305.16860) | 本文提出了基于ODE的流匹配方法的误差界限，适用于完全确定性抽样，需要满足$L^2$近似误差范围的规律性条件和数据分布。 |
| [^16] | [Lagrangian Flow Networks for Conservation Laws.](http://arxiv.org/abs/2305.16846) | 该论文提出了LFlows模型，它使用可微和可逆的变换，在时间上规定参数化的微分同胚变换来对基础密度进行转换，以连续地建模流体密度和速度。与传统方法相比，其优势在于速度的解析表达式总是与密度保持一致，无需昂贵的数值求解器，也无需使用惩罚方法。 |
| [^17] | [Randomized Positional Encodings Boost Length Generalization of Transformers.](http://arxiv.org/abs/2305.16843) | 本文提出一种随机位置编码机制，能够提高Transformer的长度普适性，使其在算法推理任务中表现出色。 |
| [^18] | [On the Generalization Capacities of Neural Controlled Differential Equations.](http://arxiv.org/abs/2305.16791) | 本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。 |
| [^19] | [A Closer Look at In-Context Learning under Distribution Shifts.](http://arxiv.org/abs/2305.16704) | 本文探究了分布漂移下的上下文学习，比较了变压器和基于集合的MLP模型的性能，发现二者在分布内评估中都表现出上下文学习的能力，但在防范较小的分布漂移方面，变压器更胜一筹。 |
| [^20] | [Sources of Uncertainty in Machine Learning -- A Statisticians' View.](http://arxiv.org/abs/2305.16703) | 本文讨论了机器学习中不确定性的来源和类型，从统计学家的视角出发，分类别介绍了随机性和认知性不确定性的概念，证明了不确定性来源各异，不可简单归为两类。同时，与统计学概念进行类比，探讨不确定性在机器学习中的作用。 |
| [^21] | [Detecting Errors in Numerical Data via any Regression Model.](http://arxiv.org/abs/2305.16583) | 该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。 |
| [^22] | [Unsupervised Embedding Quality Evaluation.](http://arxiv.org/abs/2305.16562) | 这项工作提出了评估嵌入质量的不同方法，关注如何以稳定的方式进行线性分离。从调查的文献和引入的新方法中，我们可以评估嵌入的质量，从而提高无监督学习的表现。(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.) |
| [^23] | [Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters.](http://arxiv.org/abs/2305.16557) | 本文介绍了一种基于树的扩散薛定谔桥算法(TreeDSB)来解决多元最优输运(mOT)的问题，并可以应用于高维设置如图像插值和贝叶斯融合。 |
| [^24] | [Revisiting Structured Variational Autoencoders.](http://arxiv.org/abs/2305.16543) | 本文重温结构化变分自编码器，开发了现代实现方法并证明其在精度和效率方面优于更一般的替代方法。 |
| [^25] | [Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression.](http://arxiv.org/abs/2305.16536) | 对比学习是一种表示学习技术，对于有监督的情况易于产生类坍塌，无监督情况下易于抑制类别相关的复杂特征；随机梯度下降方法偏向于寻找更简单的解决方案是导致这种现象的关键因素。 |
| [^26] | [Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization.](http://arxiv.org/abs/2305.16534) | 该论文提供了关于通过加权衰减训练的多输出ReLU神经网络的函数类型和相应的解决方案的新见解。 |
| [^27] | [Bi-fidelity Variational Auto-encoder for Uncertainty Quantification.](http://arxiv.org/abs/2305.16530) | 本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。 |
| [^28] | [When can Regression-Adjusted Control Variates Help? Rare Events, Sobolev Embedding and Minimax Optimality.](http://arxiv.org/abs/2305.16527) | 本文研究了使用回归调整控制变量来减少方差的方法，并证明了它在充分光滑的假设下可以实现Minimax最优速度，并可以解决存在罕见和极端事件的方差缩减问题。 |
| [^29] | [Most Neural Networks Are Almost Learnable.](http://arxiv.org/abs/2305.16508) | 本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。 |
| [^30] | [SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise.](http://arxiv.org/abs/2305.16491) | 该论文介绍了一种新的时间序列分析方法，即SAMoSSA。该方法综合了多元奇异谱分析和自回归分析，在学习时间序列中的确定性和随机性成分方面具有良好的理论保证。 |
| [^31] | [Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks.](http://arxiv.org/abs/2305.16475) | 本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。 |
| [^32] | [The Representation Jensen-Shannon Divergence.](http://arxiv.org/abs/2305.16446) | 本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。 |
| [^33] | [Representation Transfer Learning via Multiple Pre-trained models for Linear Regression.](http://arxiv.org/abs/2305.16440) | 本文提出了一种基于表示迁移的学习方法，在给定很少数样本的情况下，通过提供一组在可能不同的数据领域上训练的预训练回归模型，来构建目标模型，使用这种方法可以提高模型的样本复杂度。 |
| [^34] | [SketchOGD: Memory-Efficient Continual Learning.](http://arxiv.org/abs/2305.16424) | SketchOGD提出了一种内存高效的解决灾难性遗忘的方法，通过采用在线草图算法，将模型梯度压缩为固定大小的矩阵，从而改进了现有的算法——正交梯度下降（OGD）。 |
| [^35] | [Data Topology-Dependent Upper Bounds of Neural Network Widths.](http://arxiv.org/abs/2305.16375) | 本文引入了数据拓扑相关的神经网络宽度上界，并通过拓扑方法证明了三层ReLU网络的普适逼近性质。 |
| [^36] | [Neural incomplete factorization: learning preconditioners for the conjugate gradient method.](http://arxiv.org/abs/2305.16368) | 本文提出了一种名为神经不完全分解的新方法，利用自监督训练的图神经网络生成适用于特定问题域的有效预处理器。其通过替换传统手工预处理器显着提高了收敛和计算效率，在合成和真实问题上进行的实验均表现出竞争力。 |
| [^37] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | 介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。 |
| [^38] | [On the Identifiability of Markov Switching Models.](http://arxiv.org/abs/2305.15925) | 本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。 |
| [^39] | [Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting.](http://arxiv.org/abs/2305.15786) | 本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。 |
| [^40] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^41] | [Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks.](http://arxiv.org/abs/2304.03408) | 本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。 |
| [^42] | [Convergence of alternating minimisation algorithms for dictionary learning.](http://arxiv.org/abs/2304.01768) | 本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。 |
| [^43] | [Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning.](http://arxiv.org/abs/2304.00195) | 该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。 |
| [^44] | [Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference.](http://arxiv.org/abs/2303.10472) | 本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。 |
| [^45] | [Bayesian inference with finitely wide neural networks.](http://arxiv.org/abs/2303.02859) | 本文通过多元Edgeworth展开，提出用微分形式表示非高斯分布，来模拟有限宽度神经网络的非高斯后验分布。 |
| [^46] | [Hallucinated Adversarial Control for Conservative Offline Policy Evaluation.](http://arxiv.org/abs/2303.01076) | 本文提出了基于幻想对抗控制的HAMBO算法，可用于离线策略评估，并且能够得出有效的策略表现下限估计。 |
| [^47] | [Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization.](http://arxiv.org/abs/2302.14510) | 本文提出了在贝叶斯优化中使用贝叶斯内核张量分解作为代理模型的方法，以学习具有复杂特征的函数。 |
| [^48] | [On Calibrating Diffusion Probabilistic Models.](http://arxiv.org/abs/2302.10688) | 本文发现了数据分数随机反向过程是一个鞅，提出了一种简单的方法，用于校准任意预先训练的DPM，有效减小模型的得分匹配损失，增加模型似然的下限，并提供了一般校准指南。 |
| [^49] | [The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference.](http://arxiv.org/abs/2302.09163) | 研究分析因子化高斯逼近在变分推断中的应用，发现该方法低估所逼近分布的不确定性。特别地，当用对角协方差矩阵的高斯逼近具有密集协方差矩阵的高斯时，所推断的高斯总是低估了原始高斯的分量方差和熵。 |
| [^50] | [PAC-Bayesian Generalization Bounds for Adversarial Generative Models.](http://arxiv.org/abs/2302.08942) | 将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。 |
| [^51] | [I$^2$SB: Image-to-Image Schr\"odinger Bridge.](http://arxiv.org/abs/2302.05872) | 提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。 |
| [^52] | [Achieving acceleration despite very noisy gradients.](http://arxiv.org/abs/2302.05515) | AGNES是一种能在平滑凸优化任务中实现加速的算法，即使梯度估计的信噪比很小，它也能表现出优异的性能，在深度学习中的应用效果显著优于动量随机梯度下降和Nesterov方法。 |
| [^53] | [Riemannian Flow Matching on General Geometries.](http://arxiv.org/abs/2302.03660) | 本文提出了一种名为黎曼流匹配的方法，可以在一般几何上训练连续标准化流，并在高维度数据上具有优势。 |
| [^54] | [On the Efficacy of Differentially Private Few-shot Image Classification.](http://arxiv.org/abs/2302.01190) | 本文通过一系列实验研究了差分隐私少样本图像分类模型的准确性和易受攻击性，揭示了样本数、隐私级别、模型架构、下游数据集以及可学习参数子集等因素对分类效果的影响。 |
| [^55] | [Versatile Energy-Based Probabilistic Models for High Energy Physics.](http://arxiv.org/abs/2302.00695) | 本文提出了一个多功能的能量概率模型，用于描述高能物理事件，可用于参数化的事件生成，异常信号探测以及粒子识别。 |
| [^56] | [Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression.](http://arxiv.org/abs/2302.00257) | 本文研究发现，隐式正则化可对于稀疏线性回归的良性过拟合现象具有促进作用，并给出了一个模型参数化形式，结合了$\ell_1$和$\ell_2$内插器的优点，通过梯度下降训练可得到一个接近最优测试损失的内插器。 |
| [^57] | [STEEL: Singularity-aware Reinforcement Learning.](http://arxiv.org/abs/2301.13152) | 这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。 |
| [^58] | [Distributed Stochastic Optimization under a General Variance Condition.](http://arxiv.org/abs/2301.12677) | 这项研究通过重新审视联邦平均算法，在最小假设下对分布式非凸目标进行了随机优化，建立了仅满足随机梯度温和条件的收敛结果。 |
| [^59] | [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits.](http://arxiv.org/abs/2301.12357) | 本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。 |
| [^60] | [Maximal Initial Learning Rates in Deep ReLU Networks.](http://arxiv.org/abs/2212.07295) | 本文针对深度学习中的学习率问题，提出了最大初始学习率的概念，并发现其行为与训练后期的最大学习率不同。我们得出结论：在一定条件下，最大初始学习率可以很好地预测为深度×宽度的幂次。 |
| [^61] | [Neural networks trained with SGD learn distributions of increasing complexity.](http://arxiv.org/abs/2211.11567) | 本文证明了随机梯度下降算法训练的神经网络在学习期间会出现分布式简单性偏差（DSB），即最初使用低阶输入统计来分类输入，只有在训练后期才利用更高阶的统计信息。 |
| [^62] | [Multi-mode fiber reservoir computing overcomes shallow neural networks classifiers.](http://arxiv.org/abs/2210.04745) | 多模光纤利用水库计算范例进行分类，精度高于直接训练原始图像和传统的传输矩阵模型。 |
| [^63] | [The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks.](http://arxiv.org/abs/2210.02157) | 本论文分析了无限宽度的深度网络，使用不同的学习规则如GD、FA、DFA、Hebb和GLN进行训练，并发现每种规则下的输出函数演化都受到时间变化的有效神经切向核(eNTK)的影响。通过动态均场理论(DMFT)比较了每种学习规则所引起的特征和预测动力学。 |
| [^64] | [Unsupervised machine learning framework for discriminating major variants of concern during COVID-19.](http://arxiv.org/abs/2208.01439) | 本文提出了一个无监督机器学习框架，利用基因组序列区分和可视化COVID-19主要变异体之间的关联。这一框架可以帮助医疗保健专业人员了解病毒的流行病学和进化动态。 |
| [^65] | [The Multimarginal Optimal Transport Formulation of Adversarial Multiclass Classification.](http://arxiv.org/abs/2204.12676) | 本文研究了一类对抗性多类分类问题，提供了等价的广义几何重心问题和多重边际最优输运问题的重述，揭示了其丰富的几何结构，扩展了之前仅限于二分类设置的相关结果。通过本文提出的方法，可以恢复原始对抗性问题的最优稳健分类规则和最优对抗策略。 |
| [^66] | [On solutions of the distributional Bellman equation.](http://arxiv.org/abs/2202.00081) | 本文研究了分布贝尔曼方程的一般条件，包括解的存在唯一性和回报分布的尾部性质。将分布贝尔曼方程与多元仿射分布方程联系起来，发现任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这一理论适用于分布强化学习领域。 |
| [^67] | [Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions.](http://arxiv.org/abs/2108.11328) | 本文提出了一种可解释的非参数加性模型，使用少量主要和成对交互效应预测调查反应率。该模型可以生成易于可视化和解释的预测面，并取得了 ROAM 数据集上的最先进性能，可以提供改进美国人口普查局和其他调查的反应率议论。 |
| [^68] | [Double-descent curves in neural networks: a new perspective using Gaussian processes.](http://arxiv.org/abs/2102.07238) | 本文利用随机矩阵理论和高斯过程技术解释了神经网络双峰下降现象，建立了NNGP和随机矩阵理论之间的新联系，揭示该现象受到经验核和NNGP核之间差异的影响。 |

# 详细

[^1]: 大型语言模型作为工具制造者

    Large Language Models as Tool Makers. (arXiv:2305.17126v1 [cs.LG])

    [http://arxiv.org/abs/2305.17126](http://arxiv.org/abs/2305.17126)

    本文提出了一个闭环框架，即LLMs作为工具制造者（LATM），使LLMs能够自主地创建用于解决问题的工具，而不需要依赖于现有的外部工具。

    

    最近的研究表明，通过使用外部工具，大型语言模型（LLMs）可以增强其问题解决能力的潜力。然而，在这方面的先前工作依赖于现有工具的可用性。在本文中，我们提出了一个闭环框架，称为LLMs As Tool Makers（LATM），以消除这种依赖性，其中LLMs创建自己的可重用工具来解决问题。我们的方法包括两个关键阶段：1）制造工具：LLM作为工具制造者，为给定任务制作工具，其中工具作为Python实用函数实现。2）使用工具：LLM作为工具用户，应用工具制造者构建的工具来解决问题。工具用户可以是与工具制造者相同或不同的LLM。工具制造使LLM能够不断生成可应用于不同请求的工具，以便将来请求在解决问题时能调用相应的API。

    Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the 
    
[^2]: 深度神经网络的内存高效训练的流形正则化

    Manifold Regularization for Memory-Efficient Training of Deep Neural Networks. (arXiv:2305.17119v1 [cs.LG])

    [http://arxiv.org/abs/2305.17119](http://arxiv.org/abs/2305.17119)

    本文提出了一种利用流形正则化目标和诱导偏差网络设计原则的框架来实现深度神经网络的内存高效训练，相对于传统学习技术可获得更好的绝对性能和实证一般化误差，经实验验证有效。

    

    机器和深度学习领域中一种主流趋势是，采用越来越大的模型以推动最先进的性能。然而，这种趋势使普通从业者难以接触相关技术，不利于民主化知识的生产。本文提出了一种框架，通过利用诱导偏差网络设计原则和基于层的流形正则化目标，实现传统神经网络学习过程中的内存效率提高。使用该框架可以相对于传统学习技术获得更好的绝对性能和实证一般化误差。我们提供了该框架的实证验证，包括其在两个标准图像数据集，即CIFAR-10和CIFAR-100上的有效性的定性和定量证据。该提议的框架可以使用。

    One of the prevailing trends in the machine- and deep-learning community is to gravitate towards the use of increasingly larger models in order to keep pushing the state-of-the-art performance envelope. This tendency makes access to the associated technologies more difficult for the average practitioner and runs contrary to the desire to democratize knowledge production in the field. In this paper, we propose a framework for achieving improved memory efficiency in the process of learning traditional neural networks by leveraging inductive-bias-driven network design principles and layer-wise manifold-oriented regularization objectives. Use of the framework results in improved absolute performance and empirical generalization error relative to traditional learning techniques. We provide empirical validation of the framework, including qualitative and quantitative evidence of its effectiveness on two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed framework can be sea
    
[^3]: 一种针对混淆部分可观测马尔可夫决策过程的策略梯度方法

    A Policy Gradient Method for Confounded POMDPs. (arXiv:2305.17083v1 [stat.ML])

    [http://arxiv.org/abs/2305.17083](http://arxiv.org/abs/2305.17083)

    本文提出了一种针对混淆部分可观测马尔可夫决策过程的新型策略梯度方法，该方法在离线设置下可同时处理连续状态和观察空间，具有高效性和准确性。

    

    本文提出了一种针对具有连续状态和观察空间的混淆部分可观测马尔可夫决策过程（POMDP）的策略梯度方法，在离线设置下使用。我们首先建立了一个新颖的识别结果，以在离线数据下非参数地估计POMDP中的任何历史依赖策略梯度。识别结果使我们能够解决一系列条件矩限制，并采用具有一般函数逼近的最小最大学习过程来估计策略梯度。然后，我们针对预先指定的策略类提供了一个有限样本的非渐近估计界限，以了解样本大小、时间长度、集中度系数和求解条件矩限制的伪正则度量对于均匀估计梯度的影响。最后，通过在梯度上升算法中使用所提出的梯度估计，我们展示了所提出的算法在找到历史依赖性策略梯度方面的全局收敛性。

    In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy gradient under POMDPs using the offline data. The identification enables us to solve a sequence of conditional moment restrictions and adopt the min-max learning procedure with general function approximation for estimating the policy gradient. We then provide a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class in terms of the sample size, length of horizon, concentratability coefficient and the measure of ill-posedness in solving the conditional moment restrictions. Lastly, by deploying the proposed gradient estimation in the gradient ascent algorithm, we show the global convergence of the proposed algorithm in finding the history-depe
    
[^4]: （正则化）Wasserstein分布式强最优模型的确切泛化保证

    Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models. (arXiv:2305.17076v1 [cs.LG])

    [http://arxiv.org/abs/2305.17076](http://arxiv.org/abs/2305.17076)

    本文表明Wasserstein分布式强鲁棒估计器的泛化保证适用于一般模型类别，不受维数灾难所困扰，甚至可以涵盖测试时的分布变化。

    

    Wasserstein分布式强鲁棒估计器已经成为面对不确定性的预测和决策的强大模型。这些估计器提供了有吸引力的泛化保证：训练分布得到的强鲁棒目标是真实风险的一个精确上界，并且高概率成立。然而，现有的保证要么受到维数灾难的困扰，要么仅限于特定的设置，或者会导致虚假的错误术语。在本文中，我们表明这些泛化保证实际上适用于一般的模型类别，不受维数灾难所困扰，甚至可以涵盖测试时的分布变化。我们还证明，这些结果可以推广到新引入的Wasserstein分布式强最优问题的正则化版本。

    Wasserstein distributionally robust estimators have emerged as powerful models for prediction and decision-making under uncertainty. These estimators provide attractive generalization guarantees: the robust objective obtained from the training distribution is an exact upper bound on the true risk with high probability. However, existing guarantees either suffer from the curse of dimensionality, are restricted to specific settings, or lead to spurious error terms. In this paper, we show that these generalization guarantees actually hold on general classes of models, do not suffer from the curse of dimensionality, and can even cover distribution shifts at testing. We also prove that these results carry over to the newly-introduced regularized versions of Wasserstein distributionally robust problems.
    
[^5]: 深度神经网络内部表征上的Vecchia高斯过程集成

    Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural Networks. (arXiv:2305.17063v1 [stat.ML])

    [http://arxiv.org/abs/2305.17063](http://arxiv.org/abs/2305.17063)

    提出了一种基于深度神经网络内部表征的Vecchia高斯过程集成方法，该方法通过将标准高斯过程与DNN相结合，生成一种不仅能够量化不确定性，而且能够提供更准确和更稳健的预测的深度Vecchia集合。

    

    对于回归任务，标准高斯过程(GPs)提供了自然的不确定性量化，而深度神经网络(DNNs)擅长表征学习。我们提出了一种混合方法，将这两种方法协同组合起来，形成一个基于DNN的隐藏层输出构建的GP集合。通过利用最近邻条件独立的Vecchia近似实现了GP的可扩展性。生成的深度Vecchia集合不仅赋予DNN不确定性量化，还可以提供更准确和更稳健的预测。我们在几个数据集上展示了模型的效用，并进行了实验以了解所提出方法的内部机制。

    For regression tasks, standard Gaussian processes (GPs) provide natural uncertainty quantification, while deep neural networks (DNNs) excel at representation learning. We propose to synergistically combine these two approaches in a hybrid method consisting of an ensemble of GPs built on the output of hidden layers of a DNN. GP scalability is achieved via Vecchia approximations that exploit nearest-neighbor conditional independence. The resulting deep Vecchia ensemble not only imbues the DNN with uncertainty quantification but can also provide more accurate and robust predictions. We demonstrate the utility of our model on several datasets and carry out experiments to understand the inner workings of the proposed method.
    
[^6]: 通过概率生成函数的贝叶斯离散模型精确推理：概率编程方法

    Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v1 [cs.PL])

    [http://arxiv.org/abs/2305.17058](http://arxiv.org/abs/2305.17058)

    该论文提出一种精确推理离散统计模型的贝叶斯方法，支持离散采样、连续采样、离散观测、仿射函数、（随机）分支和事件条件。通过概率生成函数实现后验概率、期望、方差和高阶矩的精确计算。该方法性能优于近似蒙特卡洛方法，并避免了近似误差。

    

    我们提出了一种离散统计模型的精确贝叶斯推理方法，即使是对于无限支持和连续先验也可以找到准确的解决方案。为了表达这样的模型，我们引入了一种支持离散和连续采样、离散观测、仿射函数、（随机）分支和事件条件的概率编程语言。我们的关键工具是概率生成函数：它们提供了定义程序的分布的紧凑闭合形式表示，从而实现了后验概率、期望、方差和高阶矩的精确计算。我们的推理方法是可证明正确的、完全自动化的，使用自动微分（特别是泰勒多项式），但不需要计算机代数。我们的实验表明，它在一系列真实世界的例子中的性能与近似蒙特卡洛方法竞争，同时避免了近似误差。

    We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to many discrete inference problems, even with infinite support and continuous priors. To express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on events. Our key tool is probability generating functions: they provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments. Our inference method is provably correct, fully automated and uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra. Our experiments show that its performance on a range of real-world examples is competitive with approximate Monte Carlo methods, while avoiding approximation errors
    
[^7]: 解析心电图分析的深度学习：审计和知识发现的基石

    Explaining Deep Learning for ECG Analysis: Building Blocks for Auditing and Knowledge Discovery. (arXiv:2305.17043v1 [eess.SP])

    [http://arxiv.org/abs/2305.17043](http://arxiv.org/abs/2305.17043)

    本文介绍了可解释人工智能(XAI)方法在心电图(ECG)分析中的应用，提出了一套检查措施以确定合理的归因方法，并通过对患者亚组的数据分析，展示了这些XAI技术如何被用于知识发现，如识别心肌梗死亚型。

    

    由于深度神经网络能够准确识别心脏疾病和隐藏的临床因素，因此它们已经越来越受欢迎地用于分析心电图数据。然而，这些模型的黑匣子特性缺乏透明度，是一个常见的问题。为了解决这个问题，可以使用可解释的人工智能（XAI）方法。在本研究中，我们展示了一种后事解释(XAI)方法的全面分析，研究了局部(每个样本的贡献值)和全局(基于领域专家概念)的视角。我们建立了一套检查措施，以确定合理的归因方法，并提供符合专家规则的定量证据。这种数据集范围的分析超出了个案经验证据，通过汇总患者亚组的数据来实现。此外，我们展示了这些XAI技术如何被用于知识发现，如识别心肌梗死的亚型。我们相信，这些提出的方法可以作为审计和知识发现的基础。

    Deep neural networks have become increasingly popular for analyzing ECG data because of their ability to accurately identify cardiac conditions and hidden clinical factors. However, the lack of transparency due to the black box nature of these models is a common concern. To address this issue, explainable AI (XAI) methods can be employed. In this study, we present a comprehensive analysis of post-hoc XAI methods, investigating the local (attributions per sample) and global (based on domain expert concepts) perspectives. We have established a set of sanity checks to identify sensible attribution methods, and we provide quantitative evidence in accordance with expert rules. This dataset-wide analysis goes beyond anecdotal evidence by aggregating data across patient subgroups. Furthermore, we demonstrate how these XAI techniques can be utilized for knowledge discovery, such as identifying subtypes of myocardial infarction. We believe that these proposed methods can serve as building block
    
[^8]: 深度概率时间序列预测的更好Batch方法

    Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])

    [http://arxiv.org/abs/2305.17028](http://arxiv.org/abs/2305.17028)

    该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。

    

    深度概率时间序列预测因其能够提供有价值的不确定性量化而受到广泛关注。然而，许多现有模型过于简单化问题，假设误差过程是与时间无关的，从而忽略了误差过程中的序列相关性。这可能会降低预测的准确性，使这些模型对决策性任务的有效性减弱。为了克服这一限制，我们提出了一种创新的训练方法，将误差自相关性纳入考虑，以增强概率预测的准确性。我们的方法涉及构造一个mini-batch，作为$D$个连续时间序列段进行模型训练，并显式地学习一个协方差矩阵，覆盖了相邻时间步之间的误差相关性。由此产生的协方差矩阵可用于提高预测准确性和增强不确定性的量化。

    Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
    
[^9]: GLOBE-CE：一种用于全球因果解释的基于翻译的方法

    GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v1 [cs.LG])

    [http://arxiv.org/abs/2305.17021](http://arxiv.org/abs/2305.17021)

    GLOBE-CE是一种用于全球因果解释的机器学习方法，它可以超越局部解释，提供更有效和交互式的解释工具。

    

    因果解释在可解释性方面得到了广泛研究，公平性、追索权和模型理解等领域的应用依赖于一系列方法。然而，这些方法最大的缺点是无法提供超越局部或实例级别的解释。尽管许多作品涉及全局解释的概念，通常建议聚合大量局部解释以确定全局属性，但很少提供可靠且计算可行的框架。同时，实践者需要更有效和交互式的可解释性工具。我们借此机会提出了全局且有效的反事实解释框架(GLOBE-CE)，这是一个灵活的框架，解决了当前最先进框架在高维数据集和连续特征存在的可靠性和可扩展性问题。此外，我们提供了一个独特的数学模型。

    Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global & Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathemati
    
[^10]: 利用GFlowNets解决图形组合优化问题

    Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v1 [cs.LG])

    [http://arxiv.org/abs/2305.17010](http://arxiv.org/abs/2305.17010)

    本文提出了一种名为GFlowNets的机器，可以有效地解决组合优化问题，同时在训练方面进行了优化，结果表明其可以高效地找到高质量的解决方案。

    

    组合优化问题通常是NP难题，因此不适用于精确算法，这使它们成为应用机器学习方法的理想领域。这些问题中高度结构化的限制可能会直接阻碍优化或采样解决方案的空间。另一方面，GFlowNets最近被发现是一种强大的机器，可以顺序地从复合非规范化密度中有效地采样，并具有在CO中分摊此类解决方案搜索过程以及生成不同的解决方案候选项的潜力。在本文中，我们设计了适用于不同组合问题的马尔科夫决策过程（MDP），并提出训练有条件的GFlowNets从解空间中采样的策略。还开发了高效的训练技术来受益于远程信用分配。通过对各种使用合成和实际数据的不同CO任务的广泛实验，我们证明了GFlowNet策略可以有效地找到高质量的解。

    Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quali
    
[^11]: 带有复值的深窄神经网络的普适逼近

    Universal approximation with complex-valued deep narrow neural networks. (arXiv:2305.16910v1 [math.FA])

    [http://arxiv.org/abs/2305.16910](http://arxiv.org/abs/2305.16910)

    本文研究了具有有界宽度和任意深度的复值神经网络的普适性，发现当且仅当激活函数既不是全纯的，也不是反全纯的，也不是 $\mathbb{R}$-仿射的时，深窄的复值网络具有普适逼近能力。我们还发现足够的宽度依赖于考虑的激活函数，对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。

    

    我们研究了具有有界宽度和任意深度的复值神经网络的普适性。在温和的假设下，我们给出了那些激活函数 $\varrho:\mathbb{CC}\to \mathbb{C}$ 的完整描述，这些函数具有这样一个属性：它们关联的网络是普适的，即能够在紧致域上逼近连续函数至任意精度。准确地说，我们表明了当且仅当它们的激活函数既不是全纯的，也不是反全纯的，也不是 $\mathbb{R}$-仿射的，深窄的复值网络是普适的。这是一个比宽度任意、深度固定的对偶设置中更大的函数类。与实值情况不同的是，足够的宽度依赖于考虑的激活函数。我们表明，宽度为 $2n+2m+5$ 总是足够的，并且通常 $\max\{2n,2m\}$ 是必要的。然而，我们证明了对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。

    We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions $\varrho:\mathbb{CC}\to \mathbb{C}$ that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor $\mathbb{R}$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of $2n+2m+5$ is always sufficient and that in general a width of $\max\{2n,2m\}$ is necessary. We prove, however, that a width of $n+m+4$ suffices for a rich subclass of the admissible acti
    
[^12]: 拉普拉斯逼近神经加性模型：贝叶斯推理提高解释性

    Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])

    [http://arxiv.org/abs/2305.16905](http://arxiv.org/abs/2305.16905)

    本文提出了拉普拉斯逼近神经加性模型，该模型从贝叶斯角度考虑加性结构，在恢复的特征交互中提供可信区间，提供可处理的边缘似然估计，可用于执行隐式特征选择并对特征对进行排名。

    

    深度神经网络（DNN）在许多领域取得了成功应用，但它们的黑盒性质阻碍了解释性。神经加性模型（NAM）解决了这个问题，将网络分为加性子网络，从而使输入特征和预测之间的交互变得明显。在本文中，我们从贝叶斯角度考虑加性结构，并开发了一个实用的拉普拉斯逼近方法。这种方法在以下三个方面提高了可解释性：a）它通过估计子网络的函数空间不确定性为恢复的特征交互提供可信区间；b）它提供可处理的边缘似然估计，可用于通过经验贝叶斯过程执行特征的隐式选择；c）它可用于对特征对进行排名，作为精细调整的交互模型候选。我们在几个基准数据集上实证表明，我们提出的拉普拉斯逼近神经加性模型（LA-NAM）提高了NAM模型的可解释性，并进一步揭示了学习到的子网络的交互结构。

    Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
    
[^13]: 稀疏线性回归的特征适应

    Feature Adaptation for Sparse Linear Regression. (arXiv:2305.16892v1 [cs.DS])

    [http://arxiv.org/abs/2305.16892](http://arxiv.org/abs/2305.16892)

    本研究提供了一种可自适应的算法，可以在一定程度上容忍协变量中的近似依赖关系数量，从而达到在一定条件下具有近乎最佳样本复杂度的稀疏线性回归。

    

    稀疏线性回归是高维统计学中的核心问题。本文研究相关随机设计环境，其中协变量是从多元高斯分布$N(0,\Sigma)$中绘制的，我们寻求具有小过量风险的估计器。如果真实信号是$t$-稀疏的，则在信息理论上，仅需$O(t\log n)$个样本就可以获得强大的恢复保证。然而，具有计算效率的算法的样本复杂度是$\Sigma$的某种变体的条件数的线性。即使在协变量中仅有单一的稀疏近似依赖关系的情况下，像Lasso这样的经典算法也可能需要比必要更多的样本。我们提供了一个多项式时间算法，可以在给定$\Sigma$的情况下，自动适应Lasso以容忍小的近似依赖关系数量。特别地，在具有常数稀疏性并且$\Sigma$具有少量“异常”特征值的情况下，我们实现了近乎最佳的样本复杂度。

    Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian $N(0,\Sigma)$, and we seek an estimator with small excess risk.  If the true signal is $t$-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only $O(t\log n)$ samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the condition number of $\Sigma$. Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.  We provide a polynomial-time algorithm that, given $\Sigma$, automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if $\Sigma$ has few ``outlier'' eigenvalues. Our algorithm fits i
    
[^14]: 梯度下降在多层神经网络中的泛化保证

    Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])

    [http://arxiv.org/abs/2305.16891](http://arxiv.org/abs/2305.16891)

    本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。

    

    近年来，通过算法稳定性方法，对梯度下降训练的神经网络（NN）的泛化进行了重大进展。然而，大部分现有研究集中在单隐藏层NN上，并没有解决不同网络缩放参数的影响。本文通过对GD在多层NN上进行全面的稳定性和泛化分析，极大地扩展了以往的工作。对于双层NN，我们的结果是在一般的网络缩放参数下建立的，放宽了以前的条件。对于三层NN，我们的技术贡献在于利用一种新的归纳策略，深入探讨了过度参数化的影响，证明了它的几乎协同约束性。通过我们的一般性发现的直接应用，我们得出了GD算法在双层和三层NN中的过量风险速率为$O(1/\sqrt{n})$。

    Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\sqrt{n})$ for GD algorithms in both two-layer and thre
    
[^15]: 流匹配方法的误差界限

    Error Bounds for Flow Matching Methods. (arXiv:2305.16860v1 [stat.ML])

    [http://arxiv.org/abs/2305.16860](http://arxiv.org/abs/2305.16860)

    本文提出了基于ODE的流匹配方法的误差界限，适用于完全确定性抽样，需要满足$L^2$近似误差范围的规律性条件和数据分布。

    

    基于分数的生成模型是一类依赖于随机微分方程（SDE）的流行生成建模技术。自从它们诞生以来，就已经意识到可以使用普通微分方程（ODE）而不是SDE进行生成。这导致介绍了概率流ODE方法和去噪扩散隐式模型。流匹配方法最近进一步扩展了这些基于ODE的方法，并近似于两个任意概率分布之间的流。以前的工作针对随机抽样模式下的扩散模型推导了近似误差的边界，假设$L^2$损失具有某些限制。我们在完全确定性抽样的情况下提供了流匹配过程的误差界限，假设$L^2$近似误差范围有一定的规律性条件和数据分布。

    Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDE). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODE) rather than SDE. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions.
    
[^16]: 拉格朗日流网络用于守恒定律

    Lagrangian Flow Networks for Conservation Laws. (arXiv:2305.16846v1 [cs.LG])

    [http://arxiv.org/abs/2305.16846](http://arxiv.org/abs/2305.16846)

    该论文提出了LFlows模型，它使用可微和可逆的变换，在时间上规定参数化的微分同胚变换来对基础密度进行转换，以连续地建模流体密度和速度。与传统方法相比，其优势在于速度的解析表达式总是与密度保持一致，无需昂贵的数值求解器，也无需使用惩罚方法。

    

    我们提出了拉格朗日流网络（LFlows），用于连续地建模流体密度和速度。所提出的LFlows基于连续方程的解，其中连续方程是描述不同形式的质量守恒性质的偏微分方程。我们的模型基于这样的思路：连续方程的解可以通过可微和可逆的变换表示为时间依赖的密度变换。因此，我们通过在时间上规定参数化的微分同胚变换来对基础密度进行转换以建模流体密度。与依赖于Neural-ODE或PINNs的方法相比，关键的优势在于速度的解析表达式始终与密度保持一致。此外，无需昂贵的数值求解器，也无需使用惩罚方法来实施偏微分方程。拉格朗日流网络在合成密度数据上显示出了更高的预测精度。

    We introduce Lagrangian Flow Networks (LFlows) for modeling fluid densities and velocities continuously in space and time. The proposed LFlows satisfy by construction the continuity equation, a PDE describing mass conservation in its differentiable form. Our model is based on the insight that solutions to the continuity equation can be expressed as time-dependent density transformations via differentiable and invertible maps. This follows from classical theory of existence and uniqueness of Lagrangian flows for smooth vector fields. Hence, we model fluid densities by transforming a base density with parameterized diffeomorphisms conditioned on time. The key benefit compared to methods relying on Neural-ODE or PINNs is that the analytic expression of the velocity is always consistent with the density. Furthermore, there is no need for expensive numerical solvers, nor for enforcing the PDE with penalty methods. Lagrangian Flow Networks show improved predictive accuracy on synthetic densi
    
[^17]: 随机位置编码提升了Transformer的长度普适性

    Randomized Positional Encodings Boost Length Generalization of Transformers. (arXiv:2305.16843v1 [cs.LG])

    [http://arxiv.org/abs/2305.16843](http://arxiv.org/abs/2305.16843)

    本文提出一种随机位置编码机制，能够提高Transformer的长度普适性，使其在算法推理任务中表现出色。

    

    Transformer在固定长度的任务上拥有惊人的普适性，但它们无法推广到任意长度的序列，甚至是像复制字符串这样看似简单的任务也会失败。此外，由于全局注意机制的二次计算复杂度，仅仅训练更长的序列是低效的。本文表明这种失败模式与长度更长的序列（即使是相对编码）的位置编码在超出分布范围时有关，并引入一种能够克服此问题的新颖位置编码系列。具体而言，我们的随机位置编码机制模拟了更长序列的位置，并随机选择一个有序子集来适应序列的长度。我们在15个算法推理任务的6000种模型的大规模实证评估显示出，我们的方法使Transformer能够推广到未见长度的序列（平均测试准确度提高了12.0%）。

    Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average
    
[^18]: 神经控制微分方程的泛化能力研究

    On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])

    [http://arxiv.org/abs/2305.16791](http://arxiv.org/abs/2305.16791)

    本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。

    

    本文研究了使用神经控制微分方程（Kidger，Morrill等，2020）从不规则采样的时间序列样本中预测结果的监督学习设置。在我们的框架中，时间序列是一个未观察到的连续路径的离散化，结果通过一个具有未知向量场的控制微分方程依赖于这个路径。使用离散数据进行学习会引入离散偏差，我们精确地量化了这种偏差。通过使用关于控制微分方程流的连续性的理论结果，我们展示了逼近偏差直接与由浅层神经网络定义生成模型的利普希茨函数的逼近误差相关。通过结合最近的工作将神经网络的利普希茨常数与其泛化能力联系起来，我们上界了经验风险最小化器达到的期望损失与贝叶斯最优风险之间的泛化差距。

    We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
    
[^19]: 探究分布漂移下的上下文学习：以线性回归为例

    A Closer Look at In-Context Learning under Distribution Shifts. (arXiv:2305.16704v1 [cs.LG])

    [http://arxiv.org/abs/2305.16704](http://arxiv.org/abs/2305.16704)

    本文探究了分布漂移下的上下文学习，比较了变压器和基于集合的MLP模型的性能，发现二者在分布内评估中都表现出上下文学习的能力，但在防范较小的分布漂移方面，变压器更胜一筹。

    

    上下文学习是大型语言模型的一个定义特征，它使模型能够在不需要进行权重更新的情况下即时地从输入样例中学习。本文旨在通过线性回归这一简单而基础的任务，遵循（Garg et al., 2022）提出的设置，从简单的基于集合的多层感知器（MLP）架构的角度，更好地理解上下文学习的普适性和局限性。我们研究的核心问题是：在变化的分布漂移下，变压器是否比一些自然且更简单的架构更擅长执行上下文学习？我们发现，在分布内评估下，变压器和基于集合的MLP模型都表现出了上下文学习的能力，但是变压器更接近于最小二乘法（OLS）的表现。在分布漂移较小的情况下，变压器的韧性也比基于集合的MLP模型更好。

    In-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler architectures at performing in-context learning under varying distribution shifts? To compare transformers, we propose to use a simple architecture based on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based MLPs exhibit in-context learning under in-distribution evaluations, but transformers more closely emulate the performance of ordinary least squares (OLS). Transformers also display better resilience to mild distribution shifts, where set-based MLPs falter. 
    
[^20]: 机器学习中的不确定性来源 -- 一个统计学家的视角

    Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])

    [http://arxiv.org/abs/2305.16703](http://arxiv.org/abs/2305.16703)

    本文讨论了机器学习中不确定性的来源和类型，从统计学家的视角出发，分类别介绍了随机性和认知性不确定性的概念，证明了不确定性来源各异，不可简单归为两类。同时，与统计学概念进行类比，探讨不确定性在机器学习中的作用。

    

    机器学习和深度学习已经取得了令人瞩目的成就，使我们能够回答几年前难以想象的问题。除了这些成功之外，越来越清晰的是，在纯预测之外，量化不确定性也是相关和必要的。虽然近年来已经出现了这方面的第一批概念和思想，但本文采用了一个概念性的视角，并探讨了可能的不确定性来源。通过采用统计学家的视角，我们讨论了与机器学习更常见相关的随机性和认知性不确定性的概念。本文旨在规范这两种类型的不确定性，并证明不确定性的来源各异，并且不总是可以分解为随机性和认知性。通过将统计概念与机器学习中的不确定性进行类比，我们也展示了统计学概念和机器学习中不确定性的作用。

    Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
    
[^21]: 通过任意回归模型检测数值数据中的错误。

    Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])

    [http://arxiv.org/abs/2305.16583](http://arxiv.org/abs/2305.16583)

    该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。

    

    噪声困扰着许多数值数据集，其中数据记录的值可能由于错误的传感器、数据输入/处理错误或不完美的人类估计等原因而无法匹配真实的底层值。我们考虑估计沿数值列哪些数据值是不正确的。我们提出了一种模型不可知的方法，可以利用任何回归器（即基于数据集中的其他变量来预测该列值的统计学或机器学习模型）来解决问题。通过考虑各种不确定性，我们的方法区分了真正的异常和自然数据波动，条件是有可用的数据集信息。我们为我们的方法建立了理论保证，并表明其他方法（如符合性推断）难以检测错误。我们还提供了一个新的误差检测基准，涉及 5 个具有真实世界数字错误的回归数据集（对于其中的真实值）。

    Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
    
[^22]: 无监督嵌入质量评估

    Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v1 [cs.LG])

    [http://arxiv.org/abs/2305.16562](http://arxiv.org/abs/2305.16562)

    这项工作提出了评估嵌入质量的不同方法，关注如何以稳定的方式进行线性分离。从调查的文献和引入的新方法中，我们可以评估嵌入的质量，从而提高无监督学习的表现。(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)

    

    无监督学习，尤其是基于深度学习的方法最近在学术界得到了显著的发展。虽然在各种基准测试中取得了接近监督学习水平的成果，但由于无监督问题的本质，实践中训练和评估 SSL 模型仍然很困难。即使是以有监督的方式训练的网络，在转移到另一个领域时是否能够良好地表现，也往往不清楚。过去的工作通常仅限于评估嵌入中包含的信息量，这对于深度神经网络的自我监督学习最为相关。然而，这项工作选择了不同的方法：我们能否量化数据中如何以稳定的方式进行线性分离？我们调查了相关的文献，并发现三种方法可以用于评估嵌入的质量。此外，我们还介绍了一种基于近期对高维空间理解的最新进展的新方法。

    Unsupervised learning has recently significantly gained in popularity, especially with deep learning-based approaches. Despite numerous successes and approaching supervised-level performance on a variety of academic benchmarks, it is still hard to train and evaluate SSL models in practice due to the unsupervised nature of the problem. Even with networks trained in a supervised fashion, it is often unclear whether they will perform well when transferred to another domain.  Past works are generally limited to assessing the amount of information contained in embeddings, which is most relevant for self-supervised learning of deep neural networks. This works chooses to follow a different approach: can we quantify how easy it is to linearly separate the data in a stable way? We survey the literature and uncover three methods that could be potentially used for evaluating quality of representations. We also introduce one novel method based on recent advances in understanding the high-dimension
    
[^23]: 基于树的扩散薛定谔桥算法在Wasserstein重心中的应用

    Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters. (arXiv:2305.16557v1 [stat.ML])

    [http://arxiv.org/abs/2305.16557](http://arxiv.org/abs/2305.16557)

    本文介绍了一种基于树的扩散薛定谔桥算法(TreeDSB)来解决多元最优输运(mOT)的问题，并可以应用于高维设置如图像插值和贝叶斯融合。

    

    多元最优输运(mOT)是最优输运(OT)的一种推广，其旨在最小化成本函数相对于某些预先指定的边际分布的积分。本文考虑了一个树形二次成本的熵版本，即一种可以写作树节点之间成对成本函数之和的函数。为了解决这个问题，我们开发了Tree-based Diffusion Schr\"odinger Bridge(TreeDSB)，这是扩展了扩散薛定谔桥(DSB)算法的算法。TreeDSB对应于多元Sinkhorn算法的动态连续状态空间。我们方法的一个显著应用是计算Wasserstein重心，它可以被重新转化为基于星形树的mOT问题的解决方案。我们证明了我们的方法可以应用于高维设置，如图像插值和贝叶斯融合。

    Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem, we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB), an extension of the Diffusion Schr\"odinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space counterpart of the multimarginal Sinkhorn algorithm. A notable use case of our methodology is to compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and Bayesian fusion.
    
[^24]: 重温结构化变分自编码器

    Revisiting Structured Variational Autoencoders. (arXiv:2305.16543v1 [stat.ML])

    [http://arxiv.org/abs/2305.16543](http://arxiv.org/abs/2305.16543)

    本文重温结构化变分自编码器，开发了现代实现方法并证明其在精度和效率方面优于更一般的替代方法。

    

    结构化变分自编码器（SVAEs）将概率图模型的先验应用于潜变量，利用深度神经网络将潜变量与观测数据联系起来，并使用结构化算法进行近似后验推断。这些模型对于序列数据特别有吸引力，因为先验可以捕捉时间依赖关系。然而，尽管其理念优美，但实现难度较大，实际应用中更通用的方法更受青睐。本文采用现代机器学习工具重新审视SVAEs，并证明它们在精度和效率方面优于更一般的替代方法。首先，我们开发了现代实现方法，对SVAE核心的消息传递算法进行了硬件加速、并行化和自动求导。其次，我们展示了通过利用先验中的结构，SVAE可以学习更精确的模型和后验分布，这转化为了性能的提升。

    Structured variational autoencoders (SVAEs) combine probabilistic graphical model priors on latent variables, deep neural networks to link latent variables to observed data, and structure-exploiting algorithms for approximate posterior inference. These models are particularly appealing for sequential data, where the prior can capture temporal dependencies. However, despite their conceptual elegance, SVAEs have proven difficult to implement, and more general approaches have been favored in practice. Here, we revisit SVAEs using modern machine learning tools and demonstrate their advantages over more general alternatives in terms of both accuracy and efficiency. First, we develop a modern implementation for hardware acceleration, parallelization, and automatic differentiation of the message passing algorithms at the core of the SVAE. Second, we show that by exploiting structure in the prior, the SVAE learns more accurate models and posterior distributions, which translate into improved p
    
[^25]: 对比学习学到了哪些特征？关于简易偏差在类坍塌和特征抑制中的作用

    Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression. (arXiv:2305.16536v1 [cs.LG])

    [http://arxiv.org/abs/2305.16536](http://arxiv.org/abs/2305.16536)

    对比学习是一种表示学习技术，对于有监督的情况易于产生类坍塌，无监督情况下易于抑制类别相关的复杂特征；随机梯度下降方法偏向于寻找更简单的解决方案是导致这种现象的关键因素。

    

    对比学习具备无监督和有监督学习的表示学习技术，在有监督场景下易于坍塌同一类别内的子类表示，丢失一部分特征信息；而无监督学习则可能通过学习易于处理的类别无关特征而无视一些类别相关的复杂特征信息，这两种方法都会显著地降低表征的质量。本文提出了第一个统一严谨的框架来理解测试时的类坍塌和特征抑制产生的原因，相关分析表明，（随机）梯度下降方法偏向于寻找更简单的解决方案是导致子类表示坍塌和类别相关的复杂特征被抑制的关键因素。此外，我们利用提高嵌入维度和改进数据增强的方法来提供有效的预防措施。

    Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of \textit{class collapse} or \textit{feature suppression} at \textit{test} time. We provide the first unified theoretically rigorous framework to determine \textit{which} features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations 
    
[^26]: 向量值变分空间和DNN的宽度界：关于权重衰减正则化的见解。

    Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. (arXiv:2305.16534v1 [stat.ML])

    [http://arxiv.org/abs/2305.16534](http://arxiv.org/abs/2305.16534)

    该论文提供了关于通过加权衰减训练的多输出ReLU神经网络的函数类型和相应的解决方案的新见解。

    

    深度神经网络(DNNs)通过梯度下降最小化损失项和平方权重和相应，对应于训练加权衰减的常见方法。本文提供了有关这种常见学习框架的新见解。我们表征了训练加权衰减以获得多输出(向量值)ReLU神经网络学习的函数类型。这扩展了先前限于单输出(标量值)网络的表征。这种表征需要定义我们称之为向量值变分(VV)空间的新类神经函数空间。我们通过一种新的表征定理证明，神经网络(NNs)是通过VV空间中提出学习问题的最优解。这个新的表征定理表明，这些学习问题的解存在于宽度受训练数据数限制的向量值神经网络中。接下来，通过与多任务lasso问题的新联系，我们导出了

    Deep neural networks (DNNs) trained to minimize a loss term plus the sum of squared weights via gradient descent corresponds to the common approach of training with weight decay. This paper provides new insights into this common learning framework. We characterize the kinds of functions learned by training with weight decay for multi-output (vector-valued) ReLU neural networks. This extends previous characterizations that were limited to single-output (scalar-valued) networks. This characterization requires the definition of a new class of neural function spaces that we call vector-valued variation (VV) spaces. We prove that neural networks (NNs) are optimal solutions to learning problems posed over VV spaces via a novel representer theorem. This new representer theorem shows that solutions to these learning problems exist as vector-valued neural networks with widths bounded in terms of the number of training data. Next, via a novel connection to the multi-task lasso problem, we derive
    
[^27]: 双保真度变分自编码器用于不确定性量化

    Bi-fidelity Variational Auto-encoder for Uncertainty Quantification. (arXiv:2305.16530v1 [stat.ML])

    [http://arxiv.org/abs/2305.16530](http://arxiv.org/abs/2305.16530)

    本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。

    

    在模型验证中，量化物理系统感兴趣的量的不确定性是一个主要目标。然而，实现这一目标需要平衡计算效率和数值精度之间的需求。为了解决这个问题，我们提出了一种新颖的双保真度变分自编码器（BF-VAE）公式，旨在从物理系统中低、高保真度样本中估计与量感兴趣的量有关的不确定性。该模型通过利用从低保真度样本得出的信息来逼近高保真度量的统计信息。具体而言，我们设计了一个在潜在空间中的双保真度自回归模型，将其整合到VAE的概率编码-解码结构中。我们提出了一种有效的算法，以在存在有限高保真度数据的情况下，最大化高保真度对数似然的变分下界，从而以较低的计算成本合成高保真度的实现。此外，我们在各种数值示例中证明了我们方法的有效性，包括非线性随机系统和计算流体动力学模拟。

    Quantifying the uncertainty of quantities of interest (QoIs) from physical systems is a primary objective in model validation. However, achieving this goal entails balancing the need for computational efficiency with the requirement for numerical accuracy. To address this trade-off, we propose a novel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to estimate the uncertainty associated with a QoI from low-fidelity (LF) and high-fidelity (HF) samples of the QoI. This model allows for the approximation of the statistics of the HF QoI by leveraging information derived from its LF counterpart. Specifically, we design a bi-fidelity auto-regressive model in the latent space that is integrated within the VAE's probabilistic encoder-decoder structure. An effective algorithm is proposed to maximize the variational lower bound of the HF log-likelihood in the presence of limited HF data, resulting in the synthesis of HF realizations with a reduced computational cost. Addit
    
[^28]: 随机事件少的情况下，回归调整控制变量何时有帮助？Sobolev嵌入和Minimax最优性的研究。

    When can Regression-Adjusted Control Variates Help? Rare Events, Sobolev Embedding and Minimax Optimality. (arXiv:2305.16527v1 [math.ST])

    [http://arxiv.org/abs/2305.16527](http://arxiv.org/abs/2305.16527)

    本文研究了使用回归调整控制变量来减少方差的方法，并证明了它在充分光滑的假设下可以实现Minimax最优速度，并可以解决存在罕见和极端事件的方差缩减问题。

    

    本文研究使用基于机器学习的估计器作为控制变量来减少蒙特卡罗采样方差的方法。具体来说，我们旨在揭示影响控制变量效率的关键因素。我们研究一个原型估计问题，涉及根据从（随机的）数值积分节点获取的观测值模拟Sobolev函数的矩。首先，我们为该问题建立了信息论下限。然后，我们研究了一个特定的数值积分规则，它采用非参数回归调整控制变量来降低蒙特卡罗模拟的方差。我们证明了这种数值积分规则可以改善蒙特卡罗速度，并在充分光滑的假设下实现Minimax最优速度。由于 Sobolev 嵌入定理，充分光滑的假设消除了罕见和极端事件的存在。最后，我们表明，在存在罕见和极端事件的情况下，回归调整控制变量可以帮助解决方差缩减问题。

    This paper studies the use of a machine learning-based estimator as a control variate for mitigating the variance of Monte Carlo sampling. Specifically, we seek to uncover the key factors that influence the efficiency of control variates in reducing variance. We examine a prototype estimation problem that involves simulating the moments of a Sobolev function based on observations obtained from (random) quadrature nodes. Firstly, we establish an information-theoretic lower bound for the problem. We then study a specific quadrature rule that employs a nonparametric regression-adjusted control variate to reduce the variance of the Monte Carlo simulation. We demonstrate that this kind of quadrature rule can improve the Monte Carlo rate and achieve the minimax optimal rate under a sufficient smoothness assumption. Due to the Sobolev Embedding Theorem, the sufficient smoothness assumption eliminates the existence of rare and extreme events. Finally, we show that, in the presence of rare and 
    
[^29]: 大部分神经网络几乎是可学习的

    Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])

    [http://arxiv.org/abs/2305.16508](http://arxiv.org/abs/2305.16508)

    本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。

    

    我们提出了一个PTAS来学习随机常数深度网络。我们证明了对于任何固定的$\epsilon>0$和深度$i$，存在一个多项式时间算法，对于$\sqrt{d} \cdot \mathbb{S}^{d-1}$上的任何分布，学习随机Xavier网络的深度$i$，误差为$\epsilon$。该算法的时间和样本复杂度为$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$，其中$\bar d$是网络的大小。对于某些类似于Sigmoid和ReLU的激活函数，可以将误差界限改进为$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$，从而得到一种几乎多项式时间算法来学习常数深度随机网络。

    We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
    
[^30]: SAMoSSA：带随机自回归噪声的多元奇异谱分析

    SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise. (arXiv:2305.16491v1 [cs.LG])

    [http://arxiv.org/abs/2305.16491](http://arxiv.org/abs/2305.16491)

    该论文介绍了一种新的时间序列分析方法，即SAMoSSA。该方法综合了多元奇异谱分析和自回归分析，在学习时间序列中的确定性和随机性成分方面具有良好的理论保证。

    

    时间序列分析的惯例是先估计确定性、非平稳趋势和季节成分，然后学习残差随机、平稳成分。最近已经表明，在没有相关平稳成分的情况下，可以使用多元奇异谱分析（mSSA）准确地学习确定性非平稳成分；同时，在没有确定性非平稳成分的情况下，自回归（AR）平稳成分也可以轻松学习，例如通过普通最小二乘（OLS）。然而，尽管这种两个步骤的学习算法已经普遍存在，但关于同时涉及确定性和平稳成分的多阶段学习算法的理论支撑在文献中还没有解决。我们通过为一种自然的两阶段算法建立理论保证来解决这个开放性问题，其中首先应用mSSA来估计非平稳成分，尽管存在相关性平稳成分。

    The well-established practice of time series analysis involves estimating deterministic, non-stationary trend and seasonality components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correla
    
[^31]: 线性预测器和神经网络的初始化相关样本复杂度

    Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks. (arXiv:2305.16475v1 [cs.LG])

    [http://arxiv.org/abs/2305.16475](http://arxiv.org/abs/2305.16475)

    本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。

    

    我们提供了关于向量值线性预测器(由矩阵参数化)、更一般的神经网络的样本复杂性的新结果。专注于大小无关的界限，在这种情况下，仅控制从某个固定参考矩阵$W_0$的参数的Frobenius范数距离，我们展示了样本复杂度行为可以出人意料地不同于我们在研究标量值线性预测器方面所期望的。这还导致了前馈神经网络的新样本复杂度界限，解决了一些文献中存在的问题，并确立了一个新的凸线性预测问题，证明了它可以在没有统一收敛的情况下被学习。

    We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.
    
[^32]: 基于表示的Jensen-Shannon散度

    The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])

    [http://arxiv.org/abs/2305.16446](http://arxiv.org/abs/2305.16446)

    本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。

    

    统计散度量化概率分布之间的差异，是机器学习中的一种重要方法。但是，由于数据的底层分布通常未知，从经验样本中估计散度是一个基本难题。本文提出了一种基于再生核希尔伯特空间(RKHS)中协方差算子的新型散度——表示Jensen-Shannon散度。我们的方法将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱。我们提供了一个从经验协方差矩阵估计的估计函数，它通过使用Fourier特征将数据映射到RKHS中。此估计函数是灵活、可扩展、可微分的，并且适用于小批量优化问题。此外，我们还提供了一种基于核矩阵的估计函数，而不需要对RKHS进行显式映射。我们证明这个量是Jensen-Shannon散度的一个下界。

    Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
    
[^33]: 多个预训练模型的表示迁移学习在线性回归中的研究

    Representation Transfer Learning via Multiple Pre-trained models for Linear Regression. (arXiv:2305.16440v1 [cs.LG])

    [http://arxiv.org/abs/2305.16440](http://arxiv.org/abs/2305.16440)

    本文提出了一种基于表示迁移的学习方法，在给定很少数样本的情况下，通过提供一组在可能不同的数据领域上训练的预训练回归模型，来构建目标模型，使用这种方法可以提高模型的样本复杂度。

    

    本文研究了在给定很少数样本的情况下，如何在感兴趣的数据领域（目标）上学习线性回归模型。我们提出了一种基于表示迁移的学习方法，通过提供一组在可能不同的数据领域（来源）上训练的预训练回归模型，来构建目标模型。该方法由两个阶段组成：（i）利用不同的源表示来构造适应目标数据的表示，（ii）将所得到的模型作为初始值，通过微调程序，在目标数据上重新训练整个（超参数）回归模型。对于训练方法的每个阶段，我们提供了学习模型与真实数据生成目标模型之间的超额风险限制。导出的限制显示了样本复杂度的提高。

    In this paper, we consider the problem of learning a linear regression model on a data domain of interest (target) given few samples. To aid learning, we are provided with a set of pre-trained regression models that are trained on potentially different data domains (sources). Assuming a representation structure for the data generating linear models at the sources and the target domains, we propose a representation transfer based learning method for constructing the target model. The proposed scheme is comprised of two phases: (i) utilizing the different source representations to construct a representation that is adapted to the target data, and (ii) using the obtained model as an initialization to a fine-tuning procedure that re-trains the entire (over-parameterized) regression model on the target data. For each phase of the training method, we provide excess risk bounds for the learned model compared to the true data generating target model. The derived bounds show a gain in sample co
    
[^34]: SketchOGD：内存高效的持续学习

    SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])

    [http://arxiv.org/abs/2305.16424](http://arxiv.org/abs/2305.16424)

    SketchOGD提出了一种内存高效的解决灾难性遗忘的方法，通过采用在线草图算法，将模型梯度压缩为固定大小的矩阵，从而改进了现有的算法——正交梯度下降（OGD）。

    

    当机器学习模型在一系列任务上持续训练时，它们容易忘记先前任务上学习到的知识，这种现象称为灾难性遗忘。现有的解决灾难性遗忘的方法往往涉及存储过去任务的信息，这意味着内存使用是确定实用性的主要因素。本文提出了一种内存高效的解决灾难性遗忘的方法，改进了一种已有的算法——正交梯度下降（OGD）。OGD利用先前模型梯度来找到维持先前数据点性能的权重更新。然而，由于存储先前模型梯度的内存成本随算法运行时间增长而增加，因此OGD不适用于任意长时间跨度的连续学习。针对这个问题，本文提出了SketchOGD。SketchOGD采用在线草图算法，将模型梯度压缩为固定大小的矩阵。

    When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
    
[^35]: 神经网络宽度与数据拓扑特征相关的上界探究

    Data Topology-Dependent Upper Bounds of Neural Network Widths. (arXiv:2305.16375v1 [cs.LG])

    [http://arxiv.org/abs/2305.16375](http://arxiv.org/abs/2305.16375)

    本文引入了数据拓扑相关的神经网络宽度上界，并通过拓扑方法证明了三层ReLU网络的普适逼近性质。

    

    本文研究了深度神经网络普适逼近性质与数据拓扑特征之间的关系。我们的主要贡献是引入了数据拓扑相关的网络宽度上界。具体而言，我们首先证明了一个三层神经网络，应用ReLU激活函数和最大池化，可以设计来逼近一个在紧凑凸多面体内封装的指示函数。然后我们将其扩展到一个单纯复合体，基于其拓扑结构推导宽度上界。此外，我们还通过选择拓扑空间的Betti数计算上界。最后，我们通过拓扑方法证明了三层ReLU网络的普适逼近性质。我们还验证了梯度下降算法收敛于本研究提出的网络结构。

    This paper investigates the relationship between the universal approximation property of deep neural networks and topological characteristics of datasets. Our primary contribution is to introduce data topology-dependent upper bounds on the network width. Specifically, we first show that a three-layer neural network, applying a ReLU activation function and max pooling, can be designed to approximate an indicator function over a compact set, one that is encompassed by a tight convex polytope. This is then extended to a simplicial complex, deriving width upper bounds based on its topological structure. Further, we calculate upper bounds in relation to the Betti numbers of select topological spaces. Finally, we prove the universal approximation property of three-layer ReLU networks using our topological approach. We also verify that gradient descent converges to the network structure proposed in our study.
    
[^36]: 神经不完全分解：学习共轭梯度法的预处理器

    Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])

    [http://arxiv.org/abs/2305.16368](http://arxiv.org/abs/2305.16368)

    本文提出了一种名为神经不完全分解的新方法，利用自监督训练的图神经网络生成适用于特定问题域的有效预处理器。其通过替换传统手工预处理器显着提高了收敛和计算效率，在合成和真实问题上进行的实验均表现出竞争力。

    

    本文提出了一种新型的数据驱动方法，用于加速科学计算和优化中遇到的大规模线性方程组求解。我们的方法利用自监督训练图神经网络，生成适用于特定问题域的有效预处理器。通过替换与共轭梯度法一起使用的传统手工预处理器，我们的方法（称为神经不完全分解）显着加速了收敛和计算效率。我们的方法的核心是一种受稀疏矩阵理论启发的新型消息传递块，它与寻找矩阵的稀疏分解的目标相一致。我们在合成问题和来自科学计算的真实问题上评估了我们的方法。我们的结果表明，神经不完全分解始终优于最常见的通用预处理器，包括不完全的Cholesky方法，在收敛速度和计算效率方面表现出竞争力。

    In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
    
[^37]: 带扰动生成树的可微聚类方法

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。

    

    我们介绍了一种基于最小权重生成树的可微聚类方法，它是生成树的一种变体，具有多个连通分量。我们的方法依赖于线性规划解的随机扰动，以实现平滑和高效的梯度计算。这使我们能够在端到端可训练的流水线中包含聚类。我们证明了我们的方法即使在嘈杂的数据集和具有挑战性的几何环境下也能良好地工作。我们还利用这种方法制定了一个特别的损失，以有效地从部分聚类数据学习。我们在几个现实世界的数据集上展示了它在监督和半监督任务中的表现。

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^38]: 关于马尔科夫转换模型的可辨识性研究

    On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])

    [http://arxiv.org/abs/2305.15925](http://arxiv.org/abs/2305.15925)

    本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。

    

    最近，潜变量模型的可辨识性因其在可解释性或分布泛化方面的应用而备受关注。本文探讨了作为将最近的结果扩展到序列潜变量模型的第一步的马尔科夫转换模型的可辨识性。我们在第一阶段马尔科夫依赖结构中提出了可辨识性条件，并通过非线性高斯参数化迁移分布。我们的实验展示了我们方法在依赖于政权的因果发现和高维时间序列分割方面的适用性。

    Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
    
[^39]: 学习集合策略的理论保证及其在时间序列预测中的应用

    Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])

    [http://arxiv.org/abs/2305.15786](http://arxiv.org/abs/2305.15786)

    本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。

    

    集合是机器学习中最常用的工具之一，由于其能够有效地减少方差，从而提高泛化性能。针对黑盒基学习器的大多数集合方法都属于“叠加泛化”范畴，即训练一个接受基学习器推理作为输入的机器学习算法。虽然叠加泛化在实践中广泛应用，但其理论性质仍然不为人所知。本文证明了一个新的结果，表明选择基于交叉验证性能的“有限或有限维”叠加泛化中的最佳叠加泛化并不比最优解表现“差得多”。这一结果加强和大大扩展了Van der Laan等人（2007年）的结果。受到理论分析的启发，我们在概率预测的背景下进一步提出了一系列不同敏感性的叠加泛化模型。

    Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
    
[^40]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^41]: 有限宽度核和平均场神经网络中的预测波动动力学分析

    Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])

    [http://arxiv.org/abs/2304.03408](http://arxiv.org/abs/2304.03408)

    本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。

    

    我们分析了宽但有限的特征学习神经网络中有限宽度效应的动力学。与许多先前的分析不同，我们的结果是针对特征学习强度的非微扰有限宽度的结果。从无限宽深度神经网络核和预测动力学的动力学平均场理论（DMFT）描述开始，我们提供了对网络权重的随机初始化下DMFT序参数$\mathcal{O}(1/\sqrt{\text{width}})$波动的表征。在网络训练的懒惰极限中，所有核都是随机的但在时间上静止的，预测方差具有通用形式。然而，在富有特征学习的区域，核和预测的波动是动态耦合且方差可以被自洽计算。在两层网络中，我们展示了特征学习如何动态地减少最终NTK和最终网络预测的方差。我们还展示了如何进行初始化。

    We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
    
[^42]: 字典学习中交替极小化算法的收敛性

    Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])

    [http://arxiv.org/abs/2304.01768](http://arxiv.org/abs/2304.01768)

    本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。

    

    本文导出了针对字典学习两种流行的交替极小化算法 - 最优方向法（MOD）和在线字典学习（ODL）的收敛性足够的条件。我们表明，只要初始值良好，即距离生成的字典不超过$1/\log(K)$或具有一定的结构，确保初始值中的每个元素只指向一个生成元，两种算法将以几何收敛速率收敛于生成的字典。这在具有非均匀分布的数据模型上也能实现，该模型中稀疏系数的支撑集的出现频率可以变化很大，从而更接近真实数据。

    In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.
    
[^43]: 抽象器：基于Transformer的符号消息传递和关系推理模块

    Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])

    [http://arxiv.org/abs/2304.00195](http://arxiv.org/abs/2304.00195)

    该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    

    该论文提出了一个框架，将关系学习转化为Transformer模型，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
    
[^44]: 黑盒变分贝叶斯推理的实用匹配梯度方差界限

    Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])

    [http://arxiv.org/abs/2303.10472](http://arxiv.org/abs/2303.10472)

    本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。

    

    理解黑盒变分推理（BBVI）的梯度方差是建立其收敛性和算法改进的关键一步。然而，现有研究尚未表明BBVI的梯度方差满足用于研究随机梯度下降（SGD）收敛的条件。在本文中，我们展示了当应用于平滑和二次增长的对数似然函数时，BBVI满足与SGD文献中使用的ABC条件相匹配的界限。我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。此外，我们表明，平均场参数化的方差具有经过验证的优越维度依赖性。

    Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
    
[^45]: 有限宽度神经网络的贝叶斯推断

    Bayesian inference with finitely wide neural networks. (arXiv:2303.02859v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2303.02859](http://arxiv.org/abs/2303.02859)

    本文通过多元Edgeworth展开，提出用微分形式表示非高斯分布，来模拟有限宽度神经网络的非高斯后验分布。

    

    当机器学习从业者将宽度很大的神经网络视为贝叶斯设置中的高斯过程时，解析推断，例如预测分布以闭合形式给出，可能是一种吸引人的优势。但是，实际的宽度是有限的，并且在该宽度下，一些随机变量的边际化的高斯假设可能出现偏差。基于多元Edgeworth展开，我们提出了用微分形式表示的非高斯分布，来对来自随机神经网络的有限输出进行建模，并推导出相应的边际和条件属性，从而能够在贝叶斯回归任务中推导出非高斯后验分布。此外，在瓶颈式深度神经网络中，通过边缘核探究了深高斯过程的非高斯特性。

    The analytic inference, e.g. predictive distribution being in closed form, may be an appealing benefit for machine learning practitioners when they treat wide neural networks as Gaussian process in Bayesian setting. The realistic widths, however, are finite and cause weak deviation from the Gaussianity under which partial marginalization of random variables in a model is straightforward. On the basis of multivariate Edgeworth expansion, we propose a non-Gaussian distribution in differential form to model a finite set of outputs from a random neural network, and derive the corresponding marginal and conditional properties. Thus, we are able to derive the non-Gaussian posterior distribution in Bayesian regression task. In addition, in the bottlenecked deep neural networks, a weight space representation of deep Gaussian process, the non-Gaussianity is investigated through the marginal kernel.
    
[^46]: 基于幻想对抗控制的保守离线策略评估

    Hallucinated Adversarial Control for Conservative Offline Policy Evaluation. (arXiv:2303.01076v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01076](http://arxiv.org/abs/2303.01076)

    本文提出了基于幻想对抗控制的HAMBO算法，可用于离线策略评估，并且能够得出有效的策略表现下限估计。

    

    本文研究了保守离线策略评估问题，对于给定其他代理收集的离线环境交互数据集，我们旨在获得一个关于策略性能的(紧)下限估计。这在决定是否部署某个策略满足最小性能/安全标准之前至关重要。为此，我们引入了HAMBO，它建立在一个学习到的传递动态的不确定性感知模型之上。为了形成策略绩效的保守估计，HAMBO会幻想策略可能采取的最坏轨迹，且该轨迹在模型的认知置信区间内。我们证明了结果的COPE估计是有效的下限，并在正则性条件下展示其收敛于真实的预期回报。最后，我们讨论了基于Bayesian神经网络的可扩展变体，并在实验中证明它们产生可靠且紧密的下限。

    We study the problem of conservative off-policy evaluation (COPE) where given an offline dataset of environment interactions, collected by other agents, we seek to obtain a (tight) lower bound on a policy's performance. This is crucial when deciding whether a given policy satisfies certain minimal performance/safety criteria before it can be deployed in the real world. To this end, we introduce HAMBO, which builds on an uncertainty-aware learned model of the transition dynamics. To form a conservative estimate of the policy's performance, HAMBO hallucinates worst-case trajectories that the policy may take, within the margin of the models' epistemic confidence regions. We prove that the resulting COPE estimates are valid lower bounds, and, under regularity conditions, show their convergence to the true expected return. Finally, we discuss scalable variants of our approach based on Bayesian Neural Networks and empirically demonstrate that they yield reliable and tight lower bounds in var
    
[^47]: 贝叶斯内核张量分解作为贝叶斯优化的替代模型

    Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization. (arXiv:2302.14510v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.14510](http://arxiv.org/abs/2302.14510)

    本文提出了在贝叶斯优化中使用贝叶斯内核张量分解作为代理模型的方法，以学习具有复杂特征的函数。

    

    贝叶斯优化（BO）在很大程度上使用高斯过程（GP）作为主要的代理模型，大多使用简单的固定和可分离的内核函数，例如具有自动相关决定（SE-ARD）的平方指数内核。然而，这样的简单内核规格说明不足以学习具有复杂特征的函数，例如非定常，非可分离和多峰。即使在低维空间中，使用局部GP逼近这样的函数也需要大量样本，更不用说在高维环境中了。在本文中，我们提出使用贝叶斯内核张量分解（BKTF）作为$ D $维笛卡尔乘积空间中 BO 的新代理模型。我们的关键思想是使用全贝叶斯低秩张量 CP 分解近似基础的 $ D $ 维实体，在其中我们为每个维度的潜在基础函数放置 GP 先验，以编码局部一致性和平滑性。

    Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) -- as a new surrogate model -- for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, 
    
[^48]: 关于校准扩散概率模型

    On Calibrating Diffusion Probabilistic Models. (arXiv:2302.10688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10688](http://arxiv.org/abs/2302.10688)

    本文发现了数据分数随机反向过程是一个鞅，提出了一种简单的方法，用于校准任意预先训练的DPM，有效减小模型的得分匹配损失，增加模型似然的下限，并提供了一般校准指南。

    

    最近，扩散概率模型（DPM）在各种生成性任务中取得了有希望的结果。一个典型的DPM框架包括一个逐渐扩散数据分布的正向过程和一个从时间相关数据分数中恢复数据分布的随机反向过程。本文观察到数据分数的随机反向过程是一个鞅，从中可以导出数据分数的集中界和随机停止定理。然后，我们发现一种简单的方法，用于校准任意预先训练的DPM，以减小得分匹配损失，并因此增加模型似然的下限。我们提供了各种模型参数化下的一般校准指南。我们的校准方法仅执行一次，并且可以重复使用所得到的模型进行采样。我们在多个数据集上进行实验，以经验性地验证我们的提议。我们的代码位于https://github.com/thudzj/Cal。

    Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for calibrating an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is at https://github.com/thudzj/Cal
    
[^49]: 收缩-解耦平衡：分析因子化高斯逼近在变分推断中的应用

    The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference. (arXiv:2302.09163v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09163](http://arxiv.org/abs/2302.09163)

    研究分析因子化高斯逼近在变分推断中的应用，发现该方法低估所逼近分布的不确定性。特别地，当用对角协方差矩阵的高斯逼近具有密集协方差矩阵的高斯时，所推断的高斯总是低估了原始高斯的分量方差和熵。

    

    当变分推断（VI）使用因子化逼近时，它们往往会低估它们用来逼近的分布的不确定性，如以各种方式测量。我们考虑两种衡量VI不确定性亏损的流行方法：（i）它低估分量方差的程度，（ii）它低估熵的程度。为了更好地理解这些影响以及它们之间的关系，我们考虑了一个信息丰富的设置，可以在其中明确（和优雅地）分析这些影响：使用对角协方差矩阵的高斯（$q$）逼近具有密集协方差矩阵的高斯（$p$）。我们证明了$q$总是低估了$p$的分量方差和熵，尽管不一定低估的程度相同。此外，我们证明$q$的熵由两个相互竞争的因素的平衡决定：它的分量方差收缩会降低它的熵。

    When factorized approximations are used for variational inference (VI), they tend to underestimate the uncertainty -- as measured in various ways -- of the distributions they are meant to approximate. We consider two popular ways to measure the uncertainty deficit of VI: (i) the degree to which it underestimates the componentwise variance, and (ii) the degree to which it underestimates the entropy. To better understand these effects, and the relationship between them, we examine an informative setting where they can be explicitly (and elegantly) analyzed: the approximation of a Gaussian,~$p$, with a dense covariance matrix, by a Gaussian,~$q$, with a diagonal covariance matrix. We prove that $q$ always underestimates both the componentwise variance and the entropy of $p$, \textit{though not necessarily to the same degree}. Moreover we demonstrate that the entropy of $q$ is determined by the trade-off of two competing forces: it is decreased by the shrinkage of its componentwise varianc
    
[^50]: 面向对抗生成模型的PAC-Bayesian泛化界

    PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08942](http://arxiv.org/abs/2302.08942)

    将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。

    

    我们将PAC-Bayesian理论扩展到生成模型，并为基于Wasserstein距离和总变差距离的模型开发了泛化界。我们第一个关于Wasserstein距离的结果假设实例空间是有界的，而我们的第二个结果利用了降维的优势。我们的结果自然适用于Wasserstein GAN和Energy-Based GAN，而我们的界限为这两种GAN提供了新的训练目标。尽管我们的工作主要是理论性的，但我们进行了数值实验，展示了Wasserstein GAN在合成数据集上的非虚空泛化界。

    We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
    
[^51]: I$^2$SB：图像到图像的Schr\"odinger桥

    I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05872](http://arxiv.org/abs/2302.05872)

    提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。

    

    本文提出了一种新的条件扩散模型，即图像到图像的Schr\"odinger桥（I$^2$SB），直接学习两个给定分布之间的非线性扩散过程。这些扩散桥对于图像恢复特别有用，因为退化图像是重构清晰图像的结构信息先验。 I$^2$SB属于一类可处理的Schr\"odinger桥模型，它是得分模型的非线性扩展，其边界对的边缘分布可以在解析上计算。这种通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，进而采用在标准扩散模型中使用的实用技术，使得I$^2$SB训练具有可扩展性。在ImageNet 256x256上，我们验证了I$^2$SB在各种图像恢复任务中的性能，包括修复，超分辨率，去模糊和JPEG恢复，并表明I$^2$SB超过了标准条件扩散模型，具有更可解释的生成过程。

    We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. 
    
[^52]: 实现加速尽管梯度非常嘈杂。

    Achieving acceleration despite very noisy gradients. (arXiv:2302.05515v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05515](http://arxiv.org/abs/2302.05515)

    AGNES是一种能在平滑凸优化任务中实现加速的算法，即使梯度估计的信噪比很小，它也能表现出优异的性能，在深度学习中的应用效果显著优于动量随机梯度下降和Nesterov方法。

    

    我们提出了Nesterov加速梯度下降算法的一般化。如果噪声的强度与梯度的大小成比例，我们的算法（AGNES）可以证明在具有嘈杂梯度估计的平滑凸优化任务中实现加速。如果常数比例超过一，Nesterov加速梯度下降在这种噪声模型下不会收敛。AGNES能修复这种不足，并且可以证明它的收敛速度加快，无论梯度估计的信噪比有多小。实验证明，这是用于超参数过多的深度学习小批量梯度的适当模型。最后，我们证明AGNES在CNN训练中的性能优于动量随机梯度下降和Nesterov的方法。

    We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient. Nesterov's accelerated gradient descent does not converge under this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency and provably achieves an accelerated convergence rate no matter how small the signal to noise ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES outperforms stochastic gradient descent with momentum and Nesterov's method in the training of CNNs.
    
[^53]: 一般几何上的黎曼流匹配

    Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03660](http://arxiv.org/abs/2302.03660)

    本文提出了一种名为黎曼流匹配的方法，可以在一般几何上训练连续标准化流，并在高维度数据上具有优势。

    

    我们提出了一种名为黎曼流匹配（RFM）的框架，用于在流形上训练连续标准化流。现有的流形生成建模方法要么需要昂贵的模拟，要么无法本质上扩展到高维度，要么使用限制量的近似来产生有偏的训练目标。黎曼流匹配绕过了这些限制，并提供了比以前方法更多的优势：它在简单几何上无需模拟，不需要散度计算，并以闭合形式计算其目标向量场。 RFM的关键因素是构建一个相对简单的前度量，以定义目标向量场，其中包括现有的欧几里得情况。为了扩展到一般几何，我们依靠使用谱分解来有效地即兴计算前度量。我们的方法在现实世界的非欧几里得数据集上实现了最先进的性能，并通过在3D网格和双曲空间上训练标准化流来证明其功效。

    We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
    
[^54]: 关于差分隐私少样本图像分类方法有效性的研究

    On the Efficacy of Differentially Private Few-shot Image Classification. (arXiv:2302.01190v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01190](http://arxiv.org/abs/2302.01190)

    本文通过一系列实验研究了差分隐私少样本图像分类模型的准确性和易受攻击性，揭示了样本数、隐私级别、模型架构、下游数据集以及可学习参数子集等因素对分类效果的影响。

    

    近年来，在训练差分隐私（DP）模型方面取得了显著进展，这些DP模型的准确性接近最佳的非私有模型。这些DP模型通常在大规模公共数据集上预训练，然后在相对大且与预训练数据分布相似的私有下游数据集上进行微调。然而，在许多应用中，包括个性化和联合学习，重要的是在少样本情况下良好地表现（i.e. 获取大量标记数据可能有问题），且能够在各种领域的数据集上（即用于各种专业设置）进行良好的分类。为了了解少样本DP何时有效，我们进行了一系列详尽的实验，揭示了每类样本数、隐私级别、模型架构、下游数据集以及可学习参数子集等对少样本DP图像分类模型准确性和易受攻击性的影响。

    There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in 
    
[^55]: 多功能能量概率模型在高能物理中的应用

    Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00695](http://arxiv.org/abs/2302.00695)

    本文提出了一个多功能的能量概率模型，用于描述高能物理事件，可用于参数化的事件生成，异常信号探测以及粒子识别。

    

    作为一种经典的生成建模方法，基于能量的模型具有能量函数形式灵活性的天然优势。最近，基于能量的模型在计算机视觉和自然语言处理中建模高维数据方面取得了巨大成功。与这些进展一致，我们建立了一个多功能能量概率模型，用于描述来自大型强子对撞机的高能物理事件。该框架基于一个强大的生成模型，并描述了更高阶的粒子间相互作用，适用于不同的编码体系结构和隐式生成。在应用方面，它可以作为强大的参数化事件生成器用于物理仿真，一种泛用的无假设关联的异常信号探测器，以及用于粒子识别的增强事件分类器。

    As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
    
[^56]: 隐式正则化对于稀疏线性回归的良性过拟合现象具有促进作用

    Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression. (arXiv:2302.00257v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00257](http://arxiv.org/abs/2302.00257)

    本文研究发现，隐式正则化可对于稀疏线性回归的良性过拟合现象具有促进作用，并给出了一个模型参数化形式，结合了$\ell_1$和$\ell_2$内插器的优点，通过梯度下降训练可得到一个接近最优测试损失的内插器。

    

    在深度学习中，训练过程经常会找到一个内插器（一个0训练损失的解），但测试损失仍然很低。这种被称为良性过拟合的现象，是一个备受关注的重要谜团。良性过拟合的一个常见机制是隐式正则化，训练过程会导致内插器具有额外的性质，常被描述为最小化某些范数。然而，即使对于一个简单的稀疏线性回归问题$y=\beta^{*\top}x+\xi$，最小的$\ell_1$或$\ell_2$范数内插器也不能给出最优的测试损失。在这项工作中，我们给出了一个模型的不同参数化形式，它导致了一种新的隐式正则化效应，结合了$\ell_1$和$\ell_2$内插器的优点。我们证明，通过梯度下降训练我们的新模型，可以得到一个具有接近最优测试损失的内插器。我们的结果基于对训练动力学的细致分析，并提供了一个新的理解隐式正则化的框架。

    In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem $y = \beta^{*\top} x +\xi$ with sparse $\beta^*$, neither minimum $\ell_1$ or $\ell_2$ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of $\ell_1$ and $\ell_2$ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. Our result is based on careful analysis of the training dynamics and prov
    
[^57]: STEEL: 奇异性感知的强化学习

    STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13152](http://arxiv.org/abs/2301.13152)

    这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。

    

    批量强化学习旨在利用预先收集的数据，在动态环境中找到最优策略，以最大化期望总回报。然而，几乎所有现有算法都依赖于目标策略诱导的分布绝对连续假设，以便通过变换测度使用批量数据来校准目标策略。本文提出了一种新的批量强化学习算法，不需要在具有连续状态和行动的无限时马尔可夫决策过程中绝对连续性假设。我们称这个算法为STEEL：SingulariTy-awarE rEinforcement Learning。我们的算法受到关于离线评估的新误差分析的启发，其中我们使用了最大均值偏差，以及带有分布鲁棒优化的策略定向误差评估方法，以确保异常情况下的性能，并提出了一种用于处理奇异情况的定向算法。

    Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
    
[^58]: 通用方差条件下的分布式随机优化

    Distributed Stochastic Optimization under a General Variance Condition. (arXiv:2301.12677v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.12677](http://arxiv.org/abs/2301.12677)

    这项研究通过重新审视联邦平均算法，在最小假设下对分布式非凸目标进行了随机优化，建立了仅满足随机梯度温和条件的收敛结果。

    

    分布式随机优化在解决大规模机器学习问题时表现出了很高的效率。尽管已经提出并成功应用于一般实际问题的算法很多，但它们的理论保证主要依赖于随机梯度的某些有界条件，从均匀有界性到放松增长条件。此外，在代理之间表征数据异质性及其对算法性能的影响依然具有挑战性。出于这样的动机，我们重新考虑了经典的联邦平均（FedAvg）算法，以解决分布式随机优化问题，并在平滑非凸目标函数的随机梯度仅满足温和方差条件的情况下建立了收敛结果。在此条件下，还建立了接近确定的收敛到一个稳态点。此外，我们讨论了一个更具信息性的度量标准。

    Distributed stochastic optimization has drawn great attention recently due to its effectiveness in solving large-scale machine learning problems. Though numerous algorithms have been proposed and successfully applied to general practical problems, their theoretical guarantees mainly rely on certain boundedness conditions on the stochastic gradients, varying from uniform boundedness to the relaxed growth condition. In addition, how to characterize the data heterogeneity among the agents and its impacts on the algorithmic performance remains challenging. In light of such motivations, we revisit the classical Federated Averaging (FedAvg) algorithm for solving the distributed stochastic optimization problem and establish the convergence results under only a mild variance condition on the stochastic gradients for smooth nonconvex objective functions. Almost sure convergence to a stationary point is also established under the condition. Moreover, we discuss a more informative measurement for
    
[^59]: SPEED: 线性异方差 Bandit 策略评估的实验设计

    SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12357](http://arxiv.org/abs/2301.12357)

    本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。

    

    本文研究了线性 Bandit 下策略评估的最优数据收集问题。在策略评估中，我们需要估计多臂赌博机环境中执行目标策略将获得的期望收益。本文是首个专注于解决线性 Bandit 环境下包含异方差奖励噪声的策略评估的最优数据收集策略的工作。我们首先在线性 Bandit 环境下制定了加权最小二乘估计的最优设计，以减少目标策略价值的均方误差。接着，我们使用该设计来推导出数据收集期间每个动作的最优样本分配。然后，我们引入了一种名为 SPEED（Structured Policy Evaluation Experimental Design）的新算法，该算法跟踪最优设计，并计算其与最优设计的遗憾。最后，我们通过实验证明 SPEED 可以实现带有均方误差比较小的策略评估。

    In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa
    
[^60]: 深层ReLU网络中的最大初始学习率

    Maximal Initial Learning Rates in Deep ReLU Networks. (arXiv:2212.07295v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.07295](http://arxiv.org/abs/2212.07295)

    本文针对深度学习中的学习率问题，提出了最大初始学习率的概念，并发现其行为与训练后期的最大学习率不同。我们得出结论：在一定条件下，最大初始学习率可以很好地预测为深度×宽度的幂次。

    

    训练神经网络需要选择适当的学习率，这涉及到速度和有效收敛之间的权衡。尽管对于学习率可以有多大进行了相当大量的理论和实证分析，但大多数先前的工作只关注于后期训练。在这项工作中，我们引入了最大初始学习率$\eta^{*}$——在这个学习率下，一个随机初始化的神经网络可以成功地开始训练并达到（至少）一个给定的阈值精度。使用简单的方法估计$\eta^{*}$，我们观察到，在恒定宽度的完全连接的ReLU网络中，$\eta^{*}$的行为与训练后期的最大学习率不同。具体而言，我们发现，$\eta^{*}$可以很好地预测为深度$\times$宽度的幂次，前提是（i）网络的宽度相对深度足够大，（ii）输入层以相对较小的学习率进行训练。

    Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence. While there has been considerable theoretical and empirical analysis of how large the learning rate can be, most prior work focuses only on late-stage training. In this work, we introduce the maximal initial learning rate $\eta^{\ast}$ - the largest learning rate at which a randomly initialized neural network can successfully begin training and achieve (at least) a given threshold accuracy. Using a simple approach to estimate $\eta^{\ast}$, we observe that in constant-width fully-connected ReLU networks, $\eta^{\ast}$ behaves differently from the maximum learning rate later in training. Specifically, we find that $\eta^{\ast}$ is well predicted as a power of depth $\times$ width, provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. We fu
    
[^61]: 使用随机梯度下降算法训练的神经网络学习日益复杂的分布

    Neural networks trained with SGD learn distributions of increasing complexity. (arXiv:2211.11567v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.11567](http://arxiv.org/abs/2211.11567)

    本文证明了随机梯度下降算法训练的神经网络在学习期间会出现分布式简单性偏差（DSB），即最初使用低阶输入统计来分类输入，只有在训练后期才利用更高阶的统计信息。

    

    深度神经网络即使在插值训练数据时也能很好地进行泛化的能力已经通过各种“简单性偏差”得到了解释。这些理论假设神经网络在学习更复杂的非线性函数之前先学习简单函数，例如线性分类器。同时，数据结构也被认为是良好泛化的关键因素，然而，数据结构在简单性偏差中的作用尚未被理解。本文中，我们展示了使用随机梯度下降算法训练的神经网络最初使用低阶输入统计（如均值和协方差）来对其输入进行分类，只有在训练后期才利用更高阶的统计信息。我们首先在神经网络对合成数据进行训练的可解模型中展示了这种分布式简单性偏差（DSB）。然后，我们在训练于CIFAR10上的一系列深度卷积网络和视觉转换器中经验性地证明了DSB，甚至发现该偏差在更大的数据集和更广泛的神经网络体系结构中也存在。

    The ability of deep neural networks to generalise well even when they interpolate their training data has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first learning simple functions, say a linear classifier, before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias (DSB) in a solvable model of a neural network trained on synthetic data. We empirically demonstrate DSB in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in network
    
[^62]: 多模光纤水库计算克服了浅层神经网络分类器

    Multi-mode fiber reservoir computing overcomes shallow neural networks classifiers. (arXiv:2210.04745v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2210.04745](http://arxiv.org/abs/2210.04745)

    多模光纤利用水库计算范例进行分类，精度高于直接训练原始图像和传统的传输矩阵模型。

    

    在无序光子学领域中，常见的目标是对不透明材料进行表征，以控制光的传递或执行成像。在各种复杂的器件中，多模光纤以其成本效益高、易于操作的特点脱颖而出，使其在几个任务中具有吸引力。在这个背景下，我们利用水库计算范例，将这些光纤转化为随机硬件投影仪，将输入数据集转化为高维斑点图像集。我们的研究目标是证明，通过训练单个逻辑回归层对这些随机数据进行分类，可以提高精度，相比之下直接训练原始图像要更为准确。有趣的是，我们发现使用水库所达到的分类准确性也高于采用传统的传输矩阵模型，后者是描述通过无序器件传递光的广泛接受工具。我们发现，这种改进性能的原因在于水库的动力学具有更高的容量来捕捉复杂的输入输出映射，相对于传输矩阵的线性映射。

    In the field of disordered photonics, a common objective is to characterize optically opaque materials for controlling light delivery or performing imaging. Among various complex devices, multi-mode optical fibers stand out as cost-effective and easy-to-handle tools, making them attractive for several tasks. In this context, we leverage the reservoir computing paradigm to recast these fibers into random hardware projectors, transforming an input dataset into a higher dimensional speckled image set. The goal of our study is to demonstrate that using such randomized data for classification by training a single logistic regression layer improves accuracy compared to training on direct raw images. Interestingly, we found that the classification accuracy achieved using the reservoir is also higher than that obtained with the standard transmission matrix model, a widely accepted tool for describing light transmission through disordered devices. We find that the reason for such improved perfo
    
[^63]: 学习规则对广泛神经网络表征动力学的影响

    The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks. (arXiv:2210.02157v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.02157](http://arxiv.org/abs/2210.02157)

    本论文分析了无限宽度的深度网络，使用不同的学习规则如GD、FA、DFA、Hebb和GLN进行训练，并发现每种规则下的输出函数演化都受到时间变化的有效神经切向核(eNTK)的影响。通过动态均场理论(DMFT)比较了每种学习规则所引起的特征和预测动力学。

    

    现在尚不清楚改变深度神经网络的学习规则如何改变其学习动力学和表征。为了深入了解学习特征、函数逼近和学习规则之间的关系，我们分析了无限宽的深度网络，采用了梯度下降(GD)以及生物可行的替代方法，包括反馈对齐(FA)、直接反馈对齐(DFA)、误差调制黑比学习(Hebb)，以及门控线性网络(GLN)进行训练。

    It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel's evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy 
    
[^64]: 无监督机器学习框架用于区分COVID-19主要变异体

    Unsupervised machine learning framework for discriminating major variants of concern during COVID-19. (arXiv:2208.01439v3 [q-bio.OT] UPDATED)

    [http://arxiv.org/abs/2208.01439](http://arxiv.org/abs/2208.01439)

    本文提出了一个无监督机器学习框架，利用基因组序列区分和可视化COVID-19主要变异体之间的关联。这一框架可以帮助医疗保健专业人员了解病毒的流行病学和进化动态。

    

    由于病毒的高突变率，COVID-19疫情迅速演变。某些病毒变异体，如Delta和Omicron，出现并改变了病毒的特性，导致病例传播和死亡率严重。这些变异体对全球医疗系统造成了沉重负担，对旅行、生产力和世界经济产生了重大影响。无监督机器学习方法具有压缩、描述和可视化未标记数据的能力。本文提出了一个框架，利用无监督机器学习方法，基于基因组序列区分和可视化COVID-19主要变异体之间的关联。这些方法采用一些选定的降维和聚类技术的组合。该框架通过对数据执行k-mer分析来处理RNA序列，并使用包括主成分分析（PCA）、t-分布随机邻域嵌入（t-SNE）和均匀流形逼近和投影（UMAP）的选定降维方法进一步可视化和比较结果。这个框架展示的聚类和关联可以帮助研究人员和医疗保健专业人员了解COVID-19病毒的流行病学和进化动态。

    Due to the high mutation rate of the virus, the COVID-19 pandemic evolved rapidly. Certain variants of the virus, such as Delta and Omicron, emerged with altered viral properties leading to severe transmission and death rates. These variants burdened the medical systems worldwide with a major impact to travel, productivity, and the world economy. Unsupervised machine learning methods have the ability to compress, characterize, and visualize unlabelled data. This paper presents a framework that utilizes unsupervised machine learning methods to discriminate and visualize the associations between major COVID-19 variants based on their genome sequences. These methods comprise a combination of selected dimensionality reduction and clustering techniques. The framework processes the RNA sequences by performing a k-mer analysis on the data and further visualises and compares the results using selected dimensionality reduction methods that include principal component analysis (PCA), t-distribut
    
[^65]: 对抗性多类分类问题的多重边际最优输运公式

    The Multimarginal Optimal Transport Formulation of Adversarial Multiclass Classification. (arXiv:2204.12676v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.12676](http://arxiv.org/abs/2204.12676)

    本文研究了一类对抗性多类分类问题，提供了等价的广义几何重心问题和多重边际最优输运问题的重述，揭示了其丰富的几何结构，扩展了之前仅限于二分类设置的相关结果。通过本文提出的方法，可以恢复原始对抗性问题的最优稳健分类规则和最优对抗策略。

    

    我们研究了一类对抗性多类分类问题，并提供等价重述。其中一种是基于本文引入的一组广义几何重心问题；另一种是使用多重边际最优输运问题，其中边际的数量等于原始分类问题中的类数。这些新的理论结果揭示了多类分类中对抗性学习问题的丰富几何结构，并扩展了之前仅限于二分类设置的相关结果。我们的结果的一个直接计算含义是，通过解决重心问题及其对偶，或者是 MOT 问题及其对偶，我们可以恢复原始对抗性问题的最优稳健分类规则和最优对抗策略。我们使用合成和真实数据的示例来说明我们的结果。

    We study a family of adversarial multiclass classification problems and provide equivalent reformulations in terms of: 1) a family of generalized barycenter problems introduced in the paper and 2) a family of multimarginal optimal transport problems where the number of marginals is equal to the number of classes in the original classification problem. These new theoretical results reveal a rich geometric structure of adversarial learning problems in multiclass classification and extend recent results restricted to the binary classification setting. A direct computational implication of our results is that by solving either the barycenter problem and its dual, or the MOT problem and its dual, we can recover the optimal robust classification rule and the optimal adversarial strategy for the original adversarial problem. Examples with synthetic and real data illustrate our results.
    
[^66]: 论分布贝尔曼方程的解

    On solutions of the distributional Bellman equation. (arXiv:2202.00081v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.00081](http://arxiv.org/abs/2202.00081)

    本文研究了分布贝尔曼方程的一般条件，包括解的存在唯一性和回报分布的尾部性质。将分布贝尔曼方程与多元仿射分布方程联系起来，发现任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这一理论适用于分布强化学习领域。

    

    在分布强化学习中，不仅要考虑预期回报，还要考虑策略的完整回报分布。对于固定的策略，其回报分布是相应分布贝尔曼方程的解。本文考虑一般的分布贝尔曼方程，研究解的存在性和唯一性以及回报分布的尾部性质。我们给出了存在和唯一性回报分布的必要和充分条件，并确定了正则变化的情况。我们将分布贝尔曼方程与多元仿射分布方程联系起来。我们表明，在多元仿射分布方程的解的条件下，任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这使得这种方程的一般理论适用于分布强化学习设置。

    In distributional reinforcement learning not only expected returns but the complete return distributions of a policy are taken into account. The return distribution for a fixed policy is given as the solution of an associated distributional Bellman equation. In this note we consider general distributional Bellman equations and study existence and uniqueness of their solutions as well as tail properties of return distributions. We give necessary and sufficient conditions for existence and uniqueness of return distributions and identify cases of regular variation. We link distributional Bellman equations to multivariate affine distributional equations. We show that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This makes the general theory of such equations applicable to the distributional reinforcement learning setting.
    
[^67]: 用简洁可解释的加性模型和结构交互预测人口普查调查反应率

    Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions. (arXiv:2108.11328v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.11328](http://arxiv.org/abs/2108.11328)

    本文提出了一种可解释的非参数加性模型，使用少量主要和成对交互效应预测调查反应率。该模型可以生成易于可视化和解释的预测面，并取得了 ROAM 数据集上的最先进性能，可以提供改进美国人口普查局和其他调查的反应率议论。

    

    本文考虑使用一系列灵活且可解释的非参数模型预测调查反应率。本研究受到美国人口普查局著名的 ROAM 应用的启发，该应用使用在美国人口普查规划数据库数据上训练的线性回归模型来识别难以调查的区域。十年前组织的一场众包竞赛表明，基于回归树集成的机器学习方法在预测调查反应率方面表现最佳；然而，由于它们的黑盒特性，相应的模型不能用于拟定的应用。我们考虑使用 $\ell_0$-based 惩罚的非参数加性模型，它具有少数主要和成对交互效应。从方法论的角度来看，我们研究了我们估计器的计算和统计方面，并讨论了将强层次交互合并的变体。我们的算法（在Github 上开源）允许我们生成易于可视化和解释的预测面，从而获得有关调查反应率的可行见解。我们提出的模型在 ROAM 数据集上实现了最先进的性能，并可以提供有关美国人口普查局和其他调查的改进调查反应率的见解。

    In this paper we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition organized around ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study both computational and statistical aspects of our estimator; and discuss variants that incorporate strong hierarchical interactions. Our algorithms (opensourced on gith
    
[^68]: 高斯过程视角下神经网络的双峰下降曲线研究

    Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.07238](http://arxiv.org/abs/2102.07238)

    本文利用随机矩阵理论和高斯过程技术解释了神经网络双峰下降现象，建立了NNGP和随机矩阵理论之间的新联系，揭示该现象受到经验核和NNGP核之间差异的影响。

    

    神经网络中的双峰下降曲线现象描述了当增加参数时，泛化误差起初下降，但在达到一个小于数据点数量的最优参数后增加，然后在过参数化区间再次下降。在本文中，我们使用随机矩阵理论技术来表征经验特征协方差矩阵的谱分布，作为神经网络高斯过程（NNGP）核谱的宽度相关扰动，从而在神经网络领域建立了NNGP文献与随机矩阵理论文献之间的新联系。我们的分析表达式允许我们研究相应核和GP回归的泛化行为，并为双峰下降的现象提供了一个新的解释，即由宽度相关的经验核与宽度无关的NNGP核之间的差异所决定。

    Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. In this paper, we use techniques from random matrix theory to characterize the spectral distribution of the empirical feature covariance matrix as a width-dependent perturbation of the spectrum of the neural network Gaussian process (NNGP) kernel, thus establishing a novel connection between the NNGP literature and the random matrix theory literature in the context of neural networks. Our analytical expression allows us to study the generalisation behavior of the corresponding kernel and GP regression, and provides a new interpretation of the double-descent phenomenon, namely as governed by the discrepancy between the width-dependent empirical kernel and the width-independent NNGP kernel.
    

