# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution.](http://arxiv.org/abs/2310.16834) | 本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。 |
| [^2] | [CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression.](http://arxiv.org/abs/2310.16819) | 本研究提出了一种高维线性回归下一致估计有条件平均处理效应的方法，即使在参数非稀疏的情况下也能获得良好的理论性质。该方法假设潜在结果中的线性模型参数可分为特定于治疗方法和公共参数，并通过一个称为隐式稀疏性的较弱假设实现估计。 |
| [^3] | [Multi-scale Diffusion Denoised Smoothing.](http://arxiv.org/abs/2310.16779) | 本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。 |
| [^4] | [MixerFlow for Image Modelling.](http://arxiv.org/abs/2310.16777) | MixerFlow是一种新型的基于MLP-Mixer架构的正则化流模型，通过提供有效的权重共享机制，实现了更好的图像密度估计性能和更丰富的嵌入表示。 |
| [^5] | [Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference.](http://arxiv.org/abs/2310.16705) | 本文将变分推断重新框架为在变分参数空间上的概率分布优化问题，提出了沃瑟斯坦梯度下降方法来解决优化问题，有效性经过实证实验证实。 |
| [^6] | [Causal Discovery with Generalized Linear Models through Peeling Algorithms.](http://arxiv.org/abs/2310.16698) | 本文提出了一种通过剥离算法和广义线性模型进行因果发现的新方法，适用于分析多种类型的结果。方法通过两种剥离算法来确定因果关系和有效的工具变量，并进行了理论分析。 |
| [^7] | [Adaptive importance sampling for heavy-tailed distributions via $\alpha$-divergence minimization.](http://arxiv.org/abs/2310.16653) | 该论文提出了一种通过匹配目标和建议分布的護航矩，最小化$\alpha$-散度的自适应重要性采样算法，以便在处理重尾分布时获得更准确和更快速的估计结果。 |
| [^8] | [Posterior Consistency for Missing Data in Variational Autoencoders.](http://arxiv.org/abs/2310.16648) | 本论文研究了在包含缺失数据的情况下，从数据中学习变分自动编码器的问题。通过正则化编码器的后验分布，提出了一种改进后验一致性的方法，并在实验证明该方法在缺失值设置下可以提高重建质量和性能。 |
| [^9] | [Covariate Shift Adaptation Robust to Density-Ratio Estimation.](http://arxiv.org/abs/2310.16638) | 该论文研究了在协变量偏移下的密度比估计的罕见问题，提出了一种适应性方法来减轻密度比估计的偏差对模型的影响。 |
| [^10] | [Free-form Flows: Make Any Architecture a Normalizing Flow.](http://arxiv.org/abs/2310.16624) | 本文提出了一种训练过程，通过使用变量转换公式梯度的高效估计器，克服了归一化流设计在解析逆变换方面的限制。这使得任何保持维度的神经网络都可以作为生成模型进行最大似然训练，并在分子生成和反问题基准测试中取得优秀的结果。 |
| [^11] | [Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes.](http://arxiv.org/abs/2310.16597) | 本文扩展了之前的研究，将证明的范围从独立同分布权重扩展到了更大的权重分布类别(PSEUDO-IID)，包括低秩和稀疏设置。作者发现使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。这些结果可以帮助我们识别更广泛的神经网络的边界混沌状态，并进行性能调优。 |
| [^12] | [Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression.](http://arxiv.org/abs/2310.16577) | 本文利用磁力计阵列进行磁场映射，通过新颖方法将磁力计的位置信息纳入，提高了地图质量。 |
| [^13] | [Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression.](http://arxiv.org/abs/2310.16574) | 本论文提出了一种使用结构化核插值的高斯过程回归方法，在室内环境中生成大规模磁场地图。通过将结构化核插值与导数相结合，该方法能够在线性时间复杂度内计算预测均值和协方差，并且在模拟中取得了良好的表现。 |
| [^14] | [Particle-based Variational Inference with Generalized Wasserstein Gradient Flow.](http://arxiv.org/abs/2310.16516) | 本文提出了一种基于广义Wasserstein梯度流的ParVI框架，通过引入凸函数引导的更广泛类别的正则化器，解决了传统基于核函数的方法设计困难和限制性的问题，并展示了其具有强大的收敛性保证和高效性能。 |
| [^15] | [Assessing the overall and partial causal well-specification of nonlinear additive noise models.](http://arxiv.org/abs/2310.16502) | 提出了一种方法来评估非线性因果加性噪声模型的模型规范性问题，并识别出具有因果效应的预测变量。 |
| [^16] | [Symphony of experts: orchestration with adversarial insights in reinforcement learning.](http://arxiv.org/abs/2310.16473) | 这篇论文介绍了一种利用专家策略进行决策指导的编排方法，通过将对抗性设置中的后悔边界结果转移到表格设置下的编排中，推广了自然策略梯度的分析，并提供了关于样本复杂度的洞察。这种方法的关键点在于其透明的证明。在随机匹配玩具模型中进行了模拟实验。 |
| [^17] | [Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding.](http://arxiv.org/abs/2310.16441) | 该论文揭示了在线性网络中的领悟现象，研究了领悟时间与输入输出维度、训练样本量、正则化和网络初始化的关系，并发现泛化准确性的大幅提升并不一定意味着从“记忆”到“理解”的转变。 |
| [^18] | [Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero.](http://arxiv.org/abs/2310.16410) | 本研究提出了一种新的方法，可以从AlphaZero中提取新的国际象棋概念，并发现这些概念可以被顶级国际象棋大师所学习和应用。 |
| [^19] | [Joint Distributional Learning via Cramer-Wold Distance.](http://arxiv.org/abs/2310.16374) | 本文引入了克拉默沃尔德距离正则化，以更好地处理高维数据集和观测变量之间的复杂相关结构，并通过两步学习方法提高了先验建模的灵活性和聚合后验与先验分布之间的对齐度。 |
| [^20] | [SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process.](http://arxiv.org/abs/2310.16336) | 提出了SMURF-THP方法来学习Transformer Hawkes过程并量化预测不确定性。通过学习到的分数函数，可以从预测分布中采样事件到达时间，并计算置信区间来量化不确定性。 |
| [^21] | [Personalized Federated X -armed Bandit.](http://arxiv.org/abs/2310.16323) | 本文对个性化的联邦多臂赌博问题进行了研究，提出了PF-PNE算法，通过双重淘汰策略和有效的本地目标评估方法，实现了同时优化异质本地目标和鼓励联邦合作，该算法在多个基线算法和实验数据集上都表现出较好的性能。 |
| [^22] | [Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo.](http://arxiv.org/abs/2310.16320) | 本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。 |
| [^23] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^24] | [Contextual Bandits for Evaluating and Improving Inventory Control Policies.](http://arxiv.org/abs/2310.16096) | 这项研究引入了均衡策略的概念，并提出了一种基于上下文强盗的算法来评估和改进库存控制策略，这在理论上和实证研究中得到了有利的保证。 |
| [^25] | [Online Robust Mean Estimation.](http://arxiv.org/abs/2310.15932) | 本文研究了在线高维健壮均值估计的问题，提出了两个主要结果。 |
| [^26] | [Optimal Exploration is no harder than Thompson Sampling.](http://arxiv.org/abs/2310.06069) | 这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？ |
| [^27] | [Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data.](http://arxiv.org/abs/2309.09725) | 在无约束特征模型的背景下，我们研究了交叉熵损失函数下不均衡数据的神经塌缩现象。 |
| [^28] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^29] | [Understanding Optimization of Deep Learning.](http://arxiv.org/abs/2306.09338) | 本文全面介绍了深度学习中梯度消失和梯度爆炸等优化挑战，并通过提高梯度流和对网络Lipschitz常数施加约束等措施进行了解决。显式优化和隐式优化是两种解决优化问题的不同方式。 |
| [^30] | [Direct Diffusion Bridge using Data Consistency for Inverse Problems.](http://arxiv.org/abs/2305.19809) | 本文提出了一种用于逆问题的直接扩散链桥算法，提高了逆问题求解器的性能，并通过使用数据一致性解决了当前DDB框架存在的关键限制。 |
| [^31] | [One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration.](http://arxiv.org/abs/2305.18258) | 提出一种在线强化学习方法Maximize to Explore (MEX)，只需优化一个无约束的目标函数，自动平衡探索和利用，实现次线性遗憾。 |
| [^32] | [Most Neural Networks Are Almost Learnable.](http://arxiv.org/abs/2305.16508) | 本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。 |
| [^33] | [Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks.](http://arxiv.org/abs/2305.16475) | 本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。 |
| [^34] | [From Tempered to Benign Overfitting in ReLU Neural Networks.](http://arxiv.org/abs/2305.15141) | 本论文通过对二层ReLU神经网络进行研究，证明了各种假设下过拟合的类型会从一维数据的极端情况下缓和到高维的良性，揭示了输入维度在神经网络过拟合中的关键作用。 |
| [^35] | [Errors-in-variables Fr\'echet Regression with Low-rank Covariate Approximation.](http://arxiv.org/abs/2305.09282) | 本论文提出了一种低秩协变量逼近的误差变量Frechet回归的方法，旨在提高回归估计器的效率和准确性，并实现在高维度和误差变量回归设置中更加有效的建模和估计。 |
| [^36] | [Performative Prediction with Bandit Feedback: Learning through Reparameterization.](http://arxiv.org/abs/2305.01094) | 本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。 |
| [^37] | [Leveraging the two timescale regime to demonstrate convergence of neural networks.](http://arxiv.org/abs/2304.09576) | 研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。 |
| [^38] | [Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness.](http://arxiv.org/abs/2303.09863) | 本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。 |
| [^39] | [Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts.](http://arxiv.org/abs/2302.13875) | 该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。 |
| [^40] | [(S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability.](http://arxiv.org/abs/2302.08982) | 本文研究了在对角线线性网络上，随机性和大步长对梯度下降(GD)和随机梯度下降(SGD)的隐式正则化的影响。实验结果表明大步长对稀疏回归问题中的SGD有益处，但对GD可能有害。这种影响在接近发散阈值的紧密步长下被放大。 |
| [^41] | [Estimating Higher-Order Mixed Memberships via the $\ell_{2,\infty}$ Tensor Perturbation Bound.](http://arxiv.org/abs/2212.08642) | 本文提出了一种用于估计更高阶混合成员关系的方法，该方法基于$\ell_{2,\infty}$张量扰动界限，通过张量混合成员模型对多样化数据的社区结构进行建模，并使用高阶正交迭代算法进行估计过程。通过提供每个节点的误差界限来证明了估计过程的一致性，展示了高阶结构对估计精度的影响。 |
| [^42] | [Bagging in overparameterized learning: Risk characterization and risk monotonization.](http://arxiv.org/abs/2210.11445) | 本文研究了过度参数化学习中Bagging预测器的风险问题，并提出了通用策略来分析Bagging预测器的风险。通过具体化策略，我们得出了Bagging Ridge和Ridgeless预测器的精确渐近风险，并提供了一种交叉验证过程来选择Bagging的最佳子样本大小，以消除风险的非单调行为。 |
| [^43] | [Adaptive novelty detection with false discovery rate guarantee.](http://arxiv.org/abs/2208.06685) | 本研究提出了一种自适应新颖检测方法AdaDetect，能够在有限样本中控制假发现率，而无需分布假设。方法灵活适用于任何概率分类算法，并通过数据自适应学习变换，将功率集中在区分内点和外点的方向上。此外，还提出了适应于空值比例的AdaDetect变体。方法在合成数据集和真实数据集上进行了演示。 |
| [^44] | [Robust Output Analysis with Monte-Carlo Methodology.](http://arxiv.org/abs/2207.13612) | 本论文提出了一个统一的蒙特卡洛抽样输出分析框架，通过对模拟和机器学习输出进行分析，能够非参数化地量化输出的方差和偏差，并提出了一种新的偏差校正估计方法，以提高鲁棒性。 |
| [^45] | [Identifying Peer Influence in Therapeutic Communities.](http://arxiv.org/abs/2203.14223) | 本研究调查了治疗社区中的同伴影响或角色模型效应对于成功毕业的影响。通过分析三个治疗社区的观察数据，我们发现肯定的同伴交流对于居民在自己离开之前成功毕业与否有显著影响。 |
| [^46] | [False membership rate control in mixture models.](http://arxiv.org/abs/2203.02597) | 本文在混合模型框架中重新审视了无监督聚类方法，并开发了一种方法，能够保证虚假成员率不超过给定的阈值α。 |
| [^47] | [Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization.](http://arxiv.org/abs/2006.16205) | 本论文提出了一种组合微调方法，通过冻结预训练的降噪自编码器来保留输出结构，从而显著降低预测器的复杂性并提高泛化性能。 |
| [^48] | [A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM).](http://arxiv.org/abs/1912.13490) | 这个论文提出了一个神经计算框架下的意识理论，称为“目标对齐的内部表示操作”（GARIM）。该理论认为意识支持对目标相关的内部表示进行主动操作，使其与追求的目标更加对齐，从而增加目标导向行为的灵活性。 |
| [^49] | [Community Detection and Stochastic Block Models.](http://arxiv.org/abs/1703.10146) | 本文综述了随机块模型（SBM）中社区探测的最新发展，包括精确、部分和弱恢复等各种恢复要求，并探讨了相变阈值、SNR-互信息权衡以及信息论和计算阈值之间的差距。 |

# 详细

[^1]: 通过估计数据分布比例的离散扩散语言建模

    Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])

    [http://arxiv.org/abs/2310.16834](http://arxiv.org/abs/2310.16834)

    本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。

    

    尽管扩散模型在许多生成建模任务中具有突破性的性能，但在自然语言等离散数据领域中却表现不佳。关键是，标准的扩散模型依赖于成熟的得分匹配理论，但是将其推广到离散结构并没有取得相同的经验收益。在本文中，我们通过提出得分熵，一种新颖的离散得分匹配损失，来弥补这个差距，它比现有方法更稳定，可以形成最大似然训练的ELBO，并且可以通过去噪变体高效优化。我们将我们的得分熵离散扩散模型（SEDD）扩展到GPT-2的实验设置中，实现了极具竞争力的似然度，同时引入了独特的算法优势。特别是，在比较大小相似的SEDD和GPT-2模型时，SEDD达到了可比较的困惑度（通常在基线的+$10\%$内，并且有时超过基线）。此外，SEDD模型学到了...

    Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
    
[^2]: CATE Lasso: 高维线性回归下的有条件平均处理效应估计

    CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression. (arXiv:2310.16819v1 [econ.EM])

    [http://arxiv.org/abs/2310.16819](http://arxiv.org/abs/2310.16819)

    本研究提出了一种高维线性回归下一致估计有条件平均处理效应的方法，即使在参数非稀疏的情况下也能获得良好的理论性质。该方法假设潜在结果中的线性模型参数可分为特定于治疗方法和公共参数，并通过一个称为隐式稀疏性的较弱假设实现估计。

    

    在有关两种治疗方法的因果推断中，有条件的平均处理效应（CATE）作为表示个性化因果效应的重要指标起着重要作用，它被定义为在协变量条件下两种治疗方法的预期结果之间的差异。本研究假设两个线性回归模型，用于描述潜在结果与两种治疗方法的协变量之间的关系，并将CATE定义为两个线性回归模型之间的差异。然后，我们提出了一种方法，可以在高维度和非稀疏参数的情况下一致地估计CATE。在我们的研究中，我们证明了即使不显式假设稀疏性，仍然可以获得如一致性等理论性质，只要我们假设CATE的定义源自一个称为隐式稀疏性的较弱假设。在这个假设中，我们假设潜在结果中线性模型的参数可以分为特定于治疗方法和公共参数，其中特定于治疗方法的参数可被实现的估计。

    In causal inference about two treatments, Conditional Average Treatment Effects (CATEs) play an important role as a quantity representing an individualized causal effect, defined as a difference between the expected outcomes of the two treatments conditioned on covariates. This study assumes two linear regression models between a potential outcome and covariates of the two treatments and defines CATEs as a difference between the linear regression models. Then, we propose a method for consistently estimating CATEs even under high-dimensional and non-sparse parameters. In our study, we demonstrate that desirable theoretical properties, such as consistency, remain attainable even without assuming sparsity explicitly if we assume a weaker assumption called implicit sparsity originating from the definition of CATEs. In this assumption, we suppose that parameters of linear models in potential outcomes can be divided into treatment-specific and common parameters, where the treatment-specific 
    
[^3]: 多尺度扩散去噪平滑

    Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])

    [http://arxiv.org/abs/2310.16779](http://arxiv.org/abs/2310.16779)

    本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。

    

    随着最近的扩散模型，随机平滑已成为少数几个切实可行的方法之一，为大规模预训练模型提供对抗鲁棒性。具体而言，可以通过简单的“去噪和分类”流程，即所谓的去噪平滑，在任何分类器上执行随机平滑，前提是有一个准确的去噪器可用，比如扩散模型。在本文中，我们研究了去噪平滑的准确度和认证鲁棒性之间的权衡：例如，我们质疑哪种扩散模型的表示形式能够最大化去噪平滑的认证鲁棒性。我们考虑了一个新的目标，旨在实现共同噪声水平下平滑分类器的鲁棒性，在共享扩散模型上进行精细调整，同时也为其认证鲁棒性补偿准确度的成本提供了一种新途径。

    Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
    
[^4]: 图像建模的MixerFlow

    MixerFlow for Image Modelling. (arXiv:2310.16777v1 [stat.ML])

    [http://arxiv.org/abs/2310.16777](http://arxiv.org/abs/2310.16777)

    MixerFlow是一种新型的基于MLP-Mixer架构的正则化流模型，通过提供有效的权重共享机制，实现了更好的图像密度估计性能和更丰富的嵌入表示。

    

    正则化流是一种统计模型，通过使用双射变换将复杂密度转换为简单密度，实现了密度估计和从单个模型生成数据的功能。在图像建模的背景下，主要选择的是基于Glow的架构，而其他架构在研究界尚未得到广泛探索。在本研究中，我们提出了一种基于MLP-Mixer架构的新型架构MixerFlow，进一步统一了生成性和判别性建模架构。MixerFlow提供了一种有效的权重共享机制，适用于基于流的模型。我们的结果表明，在固定计算预算下，MixerFlow在图像数据集上具有更好的密度估计性能，并且随着图像分辨率的增加，其性能也得到了良好的扩展，使得MixerFlow成为Glow-based架构的一个强大而简单的替代品。我们还展示了MixerFlow提供了比Glow-based架构更丰富的嵌入表示。

    Normalising flows are statistical models that transform a complex density into a simpler density through the use of bijective transformations enabling both density estimation and data generation from a single model. In the context of image modelling, the predominant choice has been the Glow-based architecture, whereas alternative architectures remain largely unexplored in the research community. In this work, we propose a novel architecture called MixerFlow, based on the MLP-Mixer architecture, further unifying the generative and discriminative modelling architectures. MixerFlow offers an effective mechanism for weight sharing for flow-based models. Our results demonstrate better density estimation on image datasets under a fixed computational budget and scales well as the image resolution increases, making MixeFlow a powerful yet simple alternative to the Glow-based architectures. We also show that MixerFlow provides more informative embeddings than Glow-based architectures.
    
[^5]: 沃瑟斯坦梯度流在变分推断的变分参数空间上的应用

    Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference. (arXiv:2310.16705v1 [cs.LG])

    [http://arxiv.org/abs/2310.16705](http://arxiv.org/abs/2310.16705)

    本文将变分推断重新框架为在变分参数空间上的概率分布优化问题，提出了沃瑟斯坦梯度下降方法来解决优化问题，有效性经过实证实验证实。

    

    变分推断可以被看作是一个优化问题，其中变分参数被调整以使变分分布与真实后验尽可能接近。可以通过黑箱变分推断中的普通梯度下降或自然梯度变分推断中的自然梯度下降来解决优化任务。在本文中，我们将变分推断重新框架为在一个“变分参数空间”中定义的概率分布的目标优化问题。随后，我们提出了沃瑟斯坦梯度下降方法来解决这个优化问题。值得注意的是，这些优化技术，即黑箱变分推断和自然梯度变分推断，可以重新解释为所提出的沃瑟斯坦梯度下降的特定实例。为了提高优化效率，我们开发了实用的方法来数值求解离散梯度流。通过在一个合成数据集上的实证实验，我们验证了所提出方法的有效性。

    Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theore
    
[^6]: 通过剥离算法和广义线性模型进行因果发现

    Causal Discovery with Generalized Linear Models through Peeling Algorithms. (arXiv:2310.16698v1 [stat.ME])

    [http://arxiv.org/abs/2310.16698](http://arxiv.org/abs/2310.16698)

    本文提出了一种通过剥离算法和广义线性模型进行因果发现的新方法，适用于分析多种类型的结果。方法通过两种剥离算法来确定因果关系和有效的工具变量，并进行了理论分析。

    

    本文提出了一种新的方法，利用适用于分析多种类型的结果的广义结构方程模型进行因果发现，包括离散、连续和混合数据。因果发现常常面临无法测量的混淆因素的挑战，这阻碍了因果关系的识别。所提出的方法通过开发两种剥离算法（自下而上和自上而下）来确定因果关系和有效的工具变量，解决了这个问题。该方法首先利用基于节点化GLM回归的剥离算法重构超图，表示变量之间的祖先关系，利用主要和仪器变量之间的关系。然后，利用另一个剥离算法从祖先关系中估计父母-子女效应，同时通过从父母的模型借用信息来解决子女模型的混淆问题。本文对所提出的方法进行了理论分析，建立了相关理论保证。

    This article presents a novel method for causal discovery with generalized structural equation models suited for analyzing diverse types of outcomes, including discrete, continuous, and mixed data. Causal discovery often faces challenges due to unmeasured confounders that hinder the identification of causal relationships. The proposed approach addresses this issue by developing two peeling algorithms (bottom-up and top-down) to ascertain causal relationships and valid instruments. This approach first reconstructs a super-graph to represent ancestral relationships between variables, using a peeling algorithm based on nodewise GLM regressions that exploit relationships between primary and instrumental variables. Then, it estimates parent-child effects from the ancestral relationships using another peeling algorithm while deconfounding a child's model with information borrowed from its parents' models. The article offers a theoretical analysis of the proposed approach, which establishes c
    
[^7]: 通过$\alpha$-散度最小化实现重尾分布的自适应重要性采样

    Adaptive importance sampling for heavy-tailed distributions via $\alpha$-divergence minimization. (arXiv:2310.16653v1 [stat.CO])

    [http://arxiv.org/abs/2310.16653](http://arxiv.org/abs/2310.16653)

    该论文提出了一种通过匹配目标和建议分布的護航矩，最小化$\alpha$-散度的自适应重要性采样算法，以便在处理重尾分布时获得更准确和更快速的估计结果。

    

    自适应重要性采样（AIS）算法被广泛用于逼近复杂目标概率分布的期望。当目标分布具有重尾特性时，现有的AIS算法可能提供不一致的估计或收敛缓慢，因为它们常常忽略目标的尾部行为。为了避免这个问题，我们提出了一种通过使用Student-t建议分布来逼近目标的AIS算法。通过匹配目标和建议分布的護航矩（在重尾分布下也可定义），我们适应了位置和尺度参数。这些更新通过最小化目标和建议分布之间的$\alpha$-散度来与变分推断相联系。然后，我们展示了$\alpha$-散度可以通过广义的有效样本大小的概念来逼近，并利用这个新的视角来通过贝叶斯优化来适应尾部参数。通过应用实例，我们验证了我们方法的有效性。

    Adaptive importance sampling (AIS) algorithms are widely used to approximate expectations with respect to complicated target probability distributions. When the target has heavy tails, existing AIS algorithms can provide inconsistent estimators or exhibit slow convergence, as they often neglect the target's tail behaviour. To avoid this pitfall, we propose an AIS algorithm that approximates the target by Student-t proposal distributions. We adapt location and scale parameters by matching the escort moments - which are defined even for heavy-tailed distributions - of the target and the proposal. These updates minimize the $\alpha$-divergence between the target and the proposal, thereby connecting with variational inference. We then show that the $\alpha$-divergence can be approximated by a generalized notion of effective sample size and leverage this new perspective to adapt the tail parameter with Bayesian optimization. We demonstrate the efficacy of our approach through applications t
    
[^8]: 变分自动编码器中缺失数据的后验一致性

    Posterior Consistency for Missing Data in Variational Autoencoders. (arXiv:2310.16648v1 [cs.LG])

    [http://arxiv.org/abs/2310.16648](http://arxiv.org/abs/2310.16648)

    本论文研究了在包含缺失数据的情况下，从数据中学习变分自动编码器的问题。通过正则化编码器的后验分布，提出了一种改进后验一致性的方法，并在实验证明该方法在缺失值设置下可以提高重建质量和性能。

    

    我们考虑从包含缺失值的数据中学习变分自动编码器（VAE），即一种深度生成模型的问题。由于完整数据通常无法获得或成本太高，这种数据在机器学习的现实应用中普遍存在。我们特别关注改进VAE的摊销后验推断，即在缺失数据情况下可以学习到不一致后验分布的编码器。为此，我们提供了后验一致性的正式定义，并提出一种用于正则化编码器后验分布以促进一致性的方法。我们观察到，所提出的正则化方法在面对缺失值时建议了与文献中通常考虑的训练目标不同的方法。此外，我们在重构质量和d方面的实验证明，我们的正则化方法提高了缺失值设置下的性能。

    We consider the problem of learning Variational Autoencoders (VAEs), i.e., a type of deep generative model, from data with missing values. Such data is omnipresent in real-world applications of machine learning because complete data is often impossible or too costly to obtain. We particularly focus on improving a VAE's amortized posterior inference, i.e., the encoder, which in the case of missing data can be susceptible to learning inconsistent posterior distributions regarding the missingness. To this end, we provide a formal definition of posterior consistency and propose an approach for regularizing an encoder's posterior distribution which promotes this consistency. We observe that the proposed regularization suggests a different training objective than that typically considered in the literature when facing missing values. Furthermore, we empirically demonstrate that our regularization leads to improved performance in missing value settings in terms of reconstruction quality and d
    
[^9]: 适应密度比估计的协变量偏移适应

    Covariate Shift Adaptation Robust to Density-Ratio Estimation. (arXiv:2310.16638v1 [stat.ME])

    [http://arxiv.org/abs/2310.16638](http://arxiv.org/abs/2310.16638)

    该论文研究了在协变量偏移下的密度比估计的罕见问题，提出了一种适应性方法来减轻密度比估计的偏差对模型的影响。

    

    在一种情况下，我们可以访问具有协变量和结果的训练数据，而测试数据只包含协变量。在这种情况下，我们的主要目标是预测测试数据中缺失的结果。为了实现这个目标，我们在协变量偏移下训练参数回归模型，其中训练数据和测试数据之间的协变量分布不同。对于这个问题，现有研究提出了通过使用密度比的重要性加权来进行协变量偏移适应的方法。该方法通过对训练数据损失进行加权平均，每个权重是训练数据和测试数据之间的协变量密度比的估计，以近似测试数据的风险。尽管它允许我们获得一个最小化测试数据风险的模型，但其性能严重依赖于密度比估计的准确性。此外，即使密度比可以一致地估计，密度比的估计误差也会导致回归模型的估计器产生偏差。

    Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's 
    
[^10]: 自由形式流动：使任何架构成为归一化流

    Free-form Flows: Make Any Architecture a Normalizing Flow. (arXiv:2310.16624v1 [cs.LG])

    [http://arxiv.org/abs/2310.16624](http://arxiv.org/abs/2310.16624)

    本文提出了一种训练过程，通过使用变量转换公式梯度的高效估计器，克服了归一化流设计在解析逆变换方面的限制。这使得任何保持维度的神经网络都可以作为生成模型进行最大似然训练，并在分子生成和反问题基准测试中取得优秀的结果。

    

    归一化流是直接最大化可能性的生成模型。以前，归一化流的设计在很大程度上受到对解析逆变换的需要限制。通过使用对变量转换公式的梯度的高效估计器进行训练，我们克服了这个限制。这使得任何保持维度的神经网络都可以通过最大似然训练作为生成模型。我们的方法允许将重点放在精确调整归纳偏见以适应手头的任务上。具体而言，我们在分子生成基准测试中利用$E(n)$-等变网络取得了出色的结果。此外，我们的方法在一个反问题基准测试中也具有竞争力，同时采用现成的ResNet架构。

    Normalizing Flows are generative models that directly maximize the likelihood. Previously, the design of normalizing flows was largely constrained by the need for analytical invertibility. We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula. This enables any dimension-preserving neural network to serve as a generative model through maximum likelihood training. Our approach allows placing the emphasis on tailoring inductive biases precisely to the task at hand. Specifically, we achieve excellent results in molecule generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our method is competitive in an inverse problem benchmark, while employing off-the-shelf ResNet architectures.
    
[^11]: 超越独立同分布权重：稀疏和低秩深度神经网络也是高斯过程

    Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v1 [stat.ML])

    [http://arxiv.org/abs/2310.16597](http://arxiv.org/abs/2310.16597)

    本文扩展了之前的研究，将证明的范围从独立同分布权重扩展到了更大的权重分布类别(PSEUDO-IID)，包括低秩和稀疏设置。作者发现使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。这些结果可以帮助我们识别更广泛的神经网络的边界混沌状态，并进行性能调优。

    

    无限宽神经网络已经被证明是一个有用且可管理的数学模型，使得我们能够理解深度学习中出现的许多现象。其中一个例子是随机深层网络收敛到高斯过程，从而能够对激活函数和网络权重选择对训练动态的影响进行严格分析。在本文中，我们将Matthews等人(2018)的开创性证明扩展到更大的初始权重分布类别(我们称之为PSEUDO-IID)，其中包括独立同分布和正交权重的已有情况，以及因其计算加速优势而受到赞誉的新兴低秩和结构稀疏设置。我们证明，使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。利用我们的结果，可以识别更广泛的神经网络的边界混沌状态，并调整它们的临界性，以增强训练性能。

    The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance the
    
[^12]: 使用具有噪声输入的高斯过程回归的磁力计阵列进行磁场映射

    Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression. (arXiv:2310.16577v1 [stat.ML])

    [http://arxiv.org/abs/2310.16577](http://arxiv.org/abs/2310.16577)

    本文利用磁力计阵列进行磁场映射，通过新颖方法将磁力计的位置信息纳入，提高了地图质量。

    

    室内环境中的铁磁材料会产生环境磁场的扰动。通过使用磁力计测量和有关磁力计位置的信息，可以学习磁场的空间变化幅度。然而，磁力计的位置通常只是大致已知，这对磁场地图的质量产生负面影响。本文研究了如何利用磁力计阵列来提高磁场地图的质量。阵列的位置大致已知，但是阵列上磁力计的相对位置已知。我们在一种新颖的方法中包含了这些信息，以制作环境磁场的地图。我们通过模拟研究了我们的方法的性质，并证明我们的方法可以提高地图质量。我们还通过实验证明了我们方法的有效性。

    Ferromagnetic materials in indoor environments give rise to disturbances in the ambient magnetic field. Maps of these magnetic disturbances can be used for indoor localisation. A Gaussian process can be used to learn the spatially varying magnitude of the magnetic field using magnetometer measurements and information about the position of the magnetometer. The position of the magnetometer, however, is frequently only approximately known. This negatively affects the quality of the magnetic field map. In this paper, we investigate how an array of magnetometers can be used to improve the quality of the magnetic field map. The position of the array is approximately known, but the relative locations of the magnetometers on the array are known. We include this information in a novel method to make a map of the ambient magnetic field. We study the properties of our method in simulation and show that our method improves the map quality. We also demonstrate the efficacy of our method with exper
    
[^13]: 使用结构化核插值的高斯过程回归在大规模环境中生成磁场地图

    Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression. (arXiv:2310.16574v1 [stat.ML])

    [http://arxiv.org/abs/2310.16574](http://arxiv.org/abs/2310.16574)

    本论文提出了一种使用结构化核插值的高斯过程回归方法，在室内环境中生成大规模磁场地图。通过将结构化核插值与导数相结合，该方法能够在线性时间复杂度内计算预测均值和协方差，并且在模拟中取得了良好的表现。

    

    我们提出了一种映射算法，使用近似的高斯过程（GP）回归计算室内环境中的大规模磁场地图。映射环境中环境磁场的空间变化可以用于室内定位算法。为了计算这样的地图，GP回归是一种适合的工具，因为它提供了在新位置的磁场预测以及不确定性量化。由于全GP回归的复杂度随着数据点的数量呈立方增长，因此对于GP的近似方法已被广泛研究。在本文中，我们在结构化核插值（SKI）框架上构建，通过利用高效的Krylov子空间方法加速推断。具体而言，我们将带导数的SKI（D-SKI）纳入了用于磁场建模的标量势模型，并使用线性数据点复杂度计算预测均值和协方差。在我们的模拟中，我们展示了我们的方法。

    We present a mapping algorithm to compute large-scale magnetic field maps in indoor environments with approximate Gaussian process (GP) regression. Mapping the spatial variations in the ambient magnetic field can be used for localization algorithms in indoor areas. To compute such a map, GP regression is a suitable tool because it provides predictions of the magnetic field at new locations along with uncertainty quantification. Because full GP regression has a complexity that grows cubically with the number of data points, approximations for GPs have been extensively studied. In this paper, we build on the structured kernel interpolation (SKI) framework, speeding up inference by exploiting efficient Krylov subspace methods. More specifically, we incorporate SKI with derivatives (D-SKI) into the scalar potential model for magnetic field modeling and compute both predictive mean and covariance with a complexity that is linear in the data points. In our simulations, we show that our metho
    
[^14]: 基于粒子的广义Wasserstein梯度流的变分推理方法

    Particle-based Variational Inference with Generalized Wasserstein Gradient Flow. (arXiv:2310.16516v1 [stat.ML])

    [http://arxiv.org/abs/2310.16516](http://arxiv.org/abs/2310.16516)

    本文提出了一种基于广义Wasserstein梯度流的ParVI框架，通过引入凸函数引导的更广泛类别的正则化器，解决了传统基于核函数的方法设计困难和限制性的问题，并展示了其具有强大的收敛性保证和高效性能。

    

    基于粒子的变分推理方法（ParVIs），如Stein变分梯度下降（SVGD），通过基于核化的Wasserstein梯度流更新粒子，用于Kullback-Leibler（KL）散度。然而，核函数的设计通常是非平凡的，并且可能对方法的灵活性有限制。最近的研究表明，具有二次形式正则化项的功能梯度流逼近可以提高性能。在本文中，我们提出了一种基于广义Wasserstein梯度流的ParVI框架，称为广义Wasserstein梯度下降（GWG），其可以被视为一种具有凸函数引导的更广泛类别的正则化器的功能梯度方法。我们证明了GWG具有强大的收敛性保证。我们还提供了一种自适应版本，可以自动选择Wasserstein度量来加速收敛。在实验证明了所提出框架的有效性和高效性。

    Particle-based variational inference methods (ParVIs) such as Stein variational gradient descent (SVGD) update the particles based on the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. However, the design of kernels is often non-trivial and can be restrictive for the flexibility of the method. Recent works show that functional gradient flow approximations with quadratic form regularization terms can improve performance. In this paper, we propose a ParVI framework, called generalized Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient flow of the KL divergence, which can be viewed as a functional gradient method with a broader class of regularizers induced by convex functions. We show that GWG exhibits strong convergence guarantees. We also provide an adaptive version that automatically chooses Wasserstein metric to accelerate convergence. In experiments, we demonstrate the effectiveness and efficiency of the proposed framework
    
[^15]: 评估非线性加性噪声模型的整体和部分因果良好规范性。

    Assessing the overall and partial causal well-specification of nonlinear additive noise models. (arXiv:2310.16502v1 [stat.ME])

    [http://arxiv.org/abs/2310.16502](http://arxiv.org/abs/2310.16502)

    提出了一种方法来评估非线性因果加性噪声模型的模型规范性问题，并识别出具有因果效应的预测变量。

    

    我们提出了一种方法来检测非线性因果加性噪声模型中的模型规范性问题，可能包括异方差性。我们旨在识别那些即使在这种模型规范问题存在的情况下，我们仍然可以推断出因果效应的预测变量。我们基于对多元观测数据分布的了解开发了一个通用框架，然后针对有限样本数据提出了一种算法，讨论了其渐近性质，并在模拟和真实数据上展示了其表现。

    We propose a method to detect model misspecifications in nonlinear causal additive and potentially heteroscedastic noise models. We aim to identify predictor variables for which we can infer the causal effect even in cases of such misspecification. We develop a general framework based on knowledge of the multivariate observational data distribution and we then propose an algorithm for finite sample data, discuss its asymptotic properties, and illustrate its performance on simulated and real data.
    
[^16]: 专家的交响曲：在强化学习中运用对抗性洞察力的编排

    Symphony of experts: orchestration with adversarial insights in reinforcement learning. (arXiv:2310.16473v1 [cs.LG])

    [http://arxiv.org/abs/2310.16473](http://arxiv.org/abs/2310.16473)

    这篇论文介绍了一种利用专家策略进行决策指导的编排方法，通过将对抗性设置中的后悔边界结果转移到表格设置下的编排中，推广了自然策略梯度的分析，并提供了关于样本复杂度的洞察。这种方法的关键点在于其透明的证明。在随机匹配玩具模型中进行了模拟实验。

    

    结构化强化学习利用具有优势特性的策略以达到更好的性能，特别是在探索具有挑战性的场景中。我们通过编排的概念来探索这一领域，其中一组（少量）专家策略指导决策；我们的第一个贡献是建立了此建模。然后，我们通过从对抗性设置中转移后悔边界结果，在表格设置下建立了编排的价值函数后悔边界。我们将对 Agarwal 等人 [2021, 第5.3节] 中自然策略梯度的分析推广并扩展到任意对抗性聚合策略。我们还将其扩展到估计优势函数的情况，提供了关于期望值和高概率下样本复杂度的洞察。我们方法的一个关键点在于其相对于现有方法而言证明较为透明。最后，我们针对随机匹配玩具模型进行了模拟实验。

    Structured reinforcement learning leverages policies with advantageous properties to reach better performance, particularly in scenarios where exploration poses challenges. We explore this field through the concept of orchestration, where a (small) set of expert policies guides decision-making; the modeling thereof constitutes our first contribution. We then establish value-functions regret bounds for orchestration in the tabular setting by transferring regret-bound results from adversarial settings. We generalize and extend the analysis of natural policy gradient in Agarwal et al. [2021, Section 5.3] to arbitrary adversarial aggregation strategies. We also extend it to the case of estimated advantage functions, providing insights into sample complexity both in expectation and high probability. A key point of our approach lies in its arguably more transparent proofs compared to existing methods. Finally, we present simulations for a stochastic matching toy model.
    
[^17]: 在线性估计器中的领悟——一个可解的模型在不理解的情况下领悟。 （arXiv：2310.16441v1 [stat.ML]）

    Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding. (arXiv:2310.16441v1 [stat.ML])

    [http://arxiv.org/abs/2310.16441](http://arxiv.org/abs/2310.16441)

    该论文揭示了在线性网络中的领悟现象，研究了领悟时间与输入输出维度、训练样本量、正则化和网络初始化的关系，并发现泛化准确性的大幅提升并不一定意味着从“记忆”到“理解”的转变。

    

    领悟是一个有趣的现象，指的是模型在拟合训练数据后仍能泛化。我们通过解析和数值方法表明，即使在简单的师生设置中，具有高斯输入的线性网络执行线性任务时，领悟也会出现。在这种情况下，我们推导出了完整的训练动力学，以训练和泛化数据的协方差矩阵表示。我们提供了关于领悟时间如何取决于输入和输出维度，训练样本量，正则化和网络初始化的精确预测。我们证明了泛化准确性的急剧增加可能并不意味着从“记忆”到“理解”的转变，而只是准确性度量的产物。我们提供了对我们计算的经验证实，并初步的结果表明，某些预测也适用于具有非线性激活函数的更深层次的网络。

    Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with Gaussian inputs. In this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. We present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. We demonstrate that the sharp increase in generalization accuracy may not imply a transition from "memorization" to "understanding", but can simply be an artifact of the accuracy measure. We provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.
    
[^18]: 弥合人工智能与人类知识的差距：在AlphaZero中进行概念发现和传递

    Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero. (arXiv:2310.16410v1 [cs.AI])

    [http://arxiv.org/abs/2310.16410](http://arxiv.org/abs/2310.16410)

    本研究提出了一种新的方法，可以从AlphaZero中提取新的国际象棋概念，并发现这些概念可以被顶级国际象棋大师所学习和应用。

    

    人工智能系统在各个领域取得了超人类水平的表现，为我们提供了一个进一步提升人类知识和提高人类专家表现的机会。然而，这些高效的人工智能系统所包含的知识往往难以提取，也可能难以理解或学习。在这里，我们提出了一种新方法，可以在AlphaZero中提取新的国际象棋概念，AlphaZero是一个通过自我对弈而掌握国际象棋的人工智能系统。我们的分析表明，AlphaZero可能编码了超越现有人类知识的知识，但这些知识最终并不超出人类的理解范围，并且可以成功地学习。在人类研究中，我们展示了这些概念是可以被顶级国际象棋大师所学习的，因为四名顶级国际象棋大师在解决所呈现的概念原型位置时显示出了进步。

    Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. 
    
[^19]: 通过克拉默沃尔德距离进行联合分布学习

    Joint Distributional Learning via Cramer-Wold Distance. (arXiv:2310.16374v1 [cs.LG])

    [http://arxiv.org/abs/2310.16374](http://arxiv.org/abs/2310.16374)

    本文引入了克拉默沃尔德距离正则化，以更好地处理高维数据集和观测变量之间的复杂相关结构，并通过两步学习方法提高了先验建模的灵活性和聚合后验与先验分布之间的对齐度。

    

    在处理高维数据集或观测变量之间复杂相关结构时，基于条件独立性的假设在变分自编码器（VAE）解码器建模中具有局限性。为解决这个问题，我们引入了克拉默沃尔德距离正则化，可以通过闭合形式计算，以促进高维数据集的联合分布学习。此外，我们引入了一个两步学习方法，以实现灵活的先验建模，并提高聚合后验与先验分布之间的对齐度。此外，我们从理论上对该类别中的现有方法进行了区分。为了评估我们提出的方法在合成数据生成方面的性能，我们在具有多个类别变量的高维数据集上进行了实验。考虑到许多现有的数据集和数据科学应用涉及此类数据集，我们的实验有着重要意义。

    The assumption of conditional independence among observed variables, primarily used in the Variational Autoencoder (VAE) decoder modeling, has limitations when dealing with high-dimensional datasets or complex correlation structures among observed variables. To address this issue, we introduced the Cramer-Wold distance regularization, which can be computed in a closed-form, to facilitate joint distributional learning for high-dimensional datasets. Additionally, we introduced a two-step learning method to enable flexible prior modeling and improve the alignment between the aggregated posterior and the prior distribution. Furthermore, we provide theoretical distinctions from existing methods within this category. To evaluate the synthetic data generation performance of our proposed approach, we conducted experiments on high-dimensional datasets with multiple categorical variables. Given that many readily available datasets and data science applications involve such datasets, our experime
    
[^20]: SMURF-THP：基于分数匹配的Transformer Hawkes过程不确定性量化研究

    SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process. (arXiv:2310.16336v1 [cs.LG])

    [http://arxiv.org/abs/2310.16336](http://arxiv.org/abs/2310.16336)

    提出了SMURF-THP方法来学习Transformer Hawkes过程并量化预测不确定性。通过学习到的分数函数，可以从预测分布中采样事件到达时间，并计算置信区间来量化不确定性。

    

    Transformer Hawkes过程模型在建模事件序列数据方面取得了成功。然而，大部分现有的训练方法都依赖于最大化事件序列的似然性，这涉及到一些难以计算的积分。此外，现有方法无法为模型预测提供不确定性量化，例如对预测事件到达时间的置信区间。为了解决这些问题，我们提出了SMURF-THP，一种基于分数匹配的方法，用于学习Transformer Hawkes过程并量化预测的不确定性。具体而言，SMURF-THP通过避免难以计算的积分，学习事件到达时间的分数函数。通过这样一个学习的分数函数，我们可以从预测分布中采样事件到达时间。这自然地通过计算生成的样本上的置信区间，实现了不确定性的量化。我们在事件类型预测方面进行了大量实验证明了该方法的有效性。

    Transformer Hawkes process models have shown to be successful in modeling event sequence data. However, most of the existing training methods rely on maximizing the likelihood of event sequences, which involves calculating some intractable integral. Moreover, the existing methods fail to provide uncertainty quantification for model predictions, e.g., confidence intervals for the predicted event's arrival time. To address these issues, we propose SMURF-THP, a score-based method for learning Transformer Hawkes process and quantifying prediction uncertainty. Specifically, SMURF-THP learns the score function of events' arrival time based on a score-matching objective that avoids the intractable computation. With such a learned score function, we can sample arrival time of events from the predictive distribution. This naturally allows for the quantification of uncertainty by computing confidence intervals over the generated samples. We conduct extensive experiments in both event type predic
    
[^21]: 个性化的联邦多臂赌博问题研究

    Personalized Federated X -armed Bandit. (arXiv:2310.16323v1 [stat.ML])

    [http://arxiv.org/abs/2310.16323](http://arxiv.org/abs/2310.16323)

    本文对个性化的联邦多臂赌博问题进行了研究，提出了PF-PNE算法，通过双重淘汰策略和有效的本地目标评估方法，实现了同时优化异质本地目标和鼓励联邦合作，该算法在多个基线算法和实验数据集上都表现出较好的性能。

    

    本文研究了个性化的联邦多臂赌博问题，其中在联邦学习范式中同时优化了客户端的异质本地目标。我们提出了具有独特双重淘汰策略的PF-PNE算法，通过偏差但有效的本地目标评估，安全地消除非最优区域同时鼓励联邦合作。所提出的PF-PNE算法能够优化具有任意异质性水平的本地目标，并且其有限通信保护了客户端奖励数据的机密性。理论分析表明所提出的算法相对于单客户算法的优势。在实验中，PF-PNE在合成和真实数据集上都优于多个基线算法。

    In this work, we study the personalized federated $\mathcal{X}$-armed bandit problem, where the heterogeneous local objectives of the clients are optimized simultaneously in the federated learning paradigm. We propose the \texttt{PF-PNE} algorithm with a unique double elimination strategy, which safely eliminates the non-optimal regions while encouraging federated collaboration through biased but effective evaluations of the local objectives. The proposed \texttt{PF-PNE} algorithm is able to optimize local objectives with arbitrary levels of heterogeneity, and its limited communications protects the confidentiality of the client-wise reward data. Our theoretical analysis shows the benefit of the proposed algorithm over single-client algorithms. Experimentally, \texttt{PF-PNE} outperforms multiple baselines on both synthetic and real life datasets.
    
[^22]: 增强低精度采样：随机梯度Hamiltonian Monte Carlo

    Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])

    [http://arxiv.org/abs/2310.16320](http://arxiv.org/abs/2310.16320)

    本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。

    

    低精度训练已经成为一种有前景的低成本技术，可以在不牺牲太多准确性的情况下提高深度神经网络的训练效率。其贝叶斯对应物可以进一步提供不确定性量化和改进的泛化准确性。本文研究了在强对数凹和非对数凹分布下，使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)。从理论上讲，我们的结果表明，为了在非对数凹分布下实现2-Wasserstein距离的ε误差，低精度SGHMC相对于低精度采样器（随机梯度Langevin动力学，SGLD）实现了二次改进（$\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$ vs $\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$）。另外，基于真实数据集的实验证明了低精度SGHMC相对于SGLD在非对数凹分布下的优越性。

    Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$). Moreo
    
[^23]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^24]: 上下文强盗用于评估和改进库存控制策略

    Contextual Bandits for Evaluating and Improving Inventory Control Policies. (arXiv:2310.16096v1 [stat.ML])

    [http://arxiv.org/abs/2310.16096](http://arxiv.org/abs/2310.16096)

    这项研究引入了均衡策略的概念，并提出了一种基于上下文强盗的算法来评估和改进库存控制策略，这在理论上和实证研究中得到了有利的保证。

    

    解决库存控制问题的解决方案通常涉及对非平稳随机需求、失去销售和具有随机供应商交货时间的周期性检查的动力学进行强假设的逼近或模拟，并应用优化、动态规划或强化学习等方法。因此，分析和评估任何库存控制策略是重要的，特别是看是否有改进的空间。我们引入了均衡策略的概念，这是一个策略的理想特性，直观地意味着事后只改变少部分操作不会产生实质上更多的回报。我们提供了一个轻量级的基于上下文的强盗算法来评估和偶尔微调策略，并证明该方法在理论上和实证研究中均取得了有利的保证。

    Solutions to address the periodic review inventory control problem with nonstationary random demand, lost sales, and stochastic vendor lead times typically involve making strong assumptions on the dynamics for either approximation or simulation, and applying methods such as optimization, dynamic programming, or reinforcement learning. Therefore, it is important to analyze and evaluate any inventory control policy, in particular to see if there is room for improvement. We introduce the concept of an equilibrium policy, a desirable property of a policy that intuitively means that, in hindsight, changing only a small fraction of actions does not result in materially more reward. We provide a light-weight contextual bandit-based algorithm to evaluate and occasionally tweak policies, and show that this method achieves favorable guarantees, both theoretically and in empirical studies.
    
[^25]: 在线健壮均值估计

    Online Robust Mean Estimation. (arXiv:2310.15932v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.15932](http://arxiv.org/abs/2310.15932)

    本文研究了在线高维健壮均值估计的问题，提出了两个主要结果。

    

    我们研究了在线设置中高维健壮均值估计的问题。具体而言，我们考虑一个场景，在这个场景中，n个传感器正在测量某个共同的持续现象。在每个时间步t=1,2,...,T，第i个传感器报告其在该时间步的读数x^(i)_t。然后，算法必须对该时刻的真实均值μ_t进行估计。我们假设大部分传感器观测到了来自某个公共分布X的独立样本，但是其中一个ε分数的传感器可能表现出恶意行为。算法希望计算出对真实均值μ*:=E[X]的良好近似值μ。我们注意到，如果允许算法等待到时间T才报告其估计值，那么这就变成了一个已经被广泛研究的健壮均值估计问题。然而，我们的算法要求在数据进来时生成部分估计值，这在很大程度上使情况变得复杂。我们证明了关于这个问题的两个主要结果。

    We study the problem of high-dimensional robust mean estimation in an online setting. Specifically, we consider a scenario where $n$ sensors are measuring some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the $i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The algorithm must then commit to its estimate $\mu_t$ for the true mean value of the process at time $t$. We assume that most of the sensors observe independent samples from some common distribution $X$, but an $\epsilon$-fraction of them may instead behave maliciously. The algorithm wishes to compute a good approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that if the algorithm is allowed to wait until time $T$ to report its estimate, this reduces to the well-studied problem of robust mean estimation. However, the requirement that our algorithm produces partial estimates as the data is coming in substantially complicates the situation.  We prove two main results about 
    
[^26]: 最优探索不比汤普森采样更困难

    Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])

    [http://arxiv.org/abs/2310.06069](http://arxiv.org/abs/2310.06069)

    这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？

    

    在给定一组臂$\mathcal{Z}\subset \mathbb{R}^d$和未知参数向量$\theta_\ast\in\mathbb{R}^d$的情况下，纯探索线性臂问题旨在通过对$x^{\top}\theta_{\ast}$的噪声测量，返回$\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$，并以高概率找到正确解。现有的（渐近）最优方法要求要么为每个臂$z\in \mathcal{Z}$进行潜在昂贵的投影，要么在每个时间点明确地维护一部分正在考虑的$\mathcal{Z}$。这种复杂性与流行且简单的汤普森采样算法用于最小化后悔的情况完全相反，后者只需要访问后验采样和argmax oracle，并且在任何时间点都不需要枚举$\mathcal{Z}$。不幸的是，已知汤普森采样对于纯探索是次优的。在这项工作中，我们提出了一个自然的问题：是否存在一种算法能够进行最优探索，而且只需要相同的计算操作？

    Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput
    
[^27]: 无约束特征模型中的交叉熵损失下不受限的神经塌缩机制

    Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data. (arXiv:2309.09725v1 [stat.ML])

    [http://arxiv.org/abs/2309.09725](http://arxiv.org/abs/2309.09725)

    在无约束特征模型的背景下，我们研究了交叉熵损失函数下不均衡数据的神经塌缩现象。

    

    近年来，深度神经网络（DNNs）在计算机视觉和文本处理的各种任务中取得了巨大的成功。有趣的是，这些具有大量参数的DNNs在训练的末期阶段（TPT）的特征表示和末层分类器具有相似的结构特性。具体而言，如果训练数据是平衡的（每个类别具有相同数量的样本），观察到来自同一类别的样本的特征向量收敛到相应的类内均值特征，并且它们的成对角度相同。这一迷人的现象被称为神经塌缩（NC），由Papyan，Han和Donoho在2019年首次提出。最近的许多工作通过采用所谓的无约束特征模型（UFM）在理论上解释了这一现象。在本文中，我们研究了在无约束特征模型的上下文中，弥补了NC现象对不均衡数据在交叉熵损失函数下的拓展。我们的贡献是

    Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is
    
[^28]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^29]: 深度学习的优化理解

    Understanding Optimization of Deep Learning. (arXiv:2306.09338v1 [cs.LG])

    [http://arxiv.org/abs/2306.09338](http://arxiv.org/abs/2306.09338)

    本文全面介绍了深度学习中梯度消失和梯度爆炸等优化挑战，并通过提高梯度流和对网络Lipschitz常数施加约束等措施进行了解决。显式优化和隐式优化是两种解决优化问题的不同方式。

    

    本文全面介绍了深度学习中的优化理论，主要关注梯度消失和梯度爆炸等问题所带来的模型表示能力降低和训练不稳定性等挑战。我们通过提高梯度流和对网络Lipschitz 常数施加约束等措施来分析这两个挑战。为了帮助理解当前的优化方法，我们将其分为显式优化方法和隐式优化方法。显式优化方法涉及直接操作优化器参数，包括权重、梯度、学习率和权重衰减等。相比之下，隐式优化方法侧重于通过增强网络模块（如残差快捷方式、标准化方法、注意机制和激活）来改善网络整体形势。本文提供了深入的分析和实验，以帮助研究人员更好地了解深度学习模型的优化方法。

    This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth a
    
[^30]: 使用数据一致性的直接扩散链桥解决逆问题

    Direct Diffusion Bridge using Data Consistency for Inverse Problems. (arXiv:2305.19809v1 [cs.CV])

    [http://arxiv.org/abs/2305.19809](http://arxiv.org/abs/2305.19809)

    本文提出了一种用于逆问题的直接扩散链桥算法，提高了逆问题求解器的性能，并通过使用数据一致性解决了当前DDB框架存在的关键限制。

    

    基于扩散模型的逆问题求解器表现出令人印象深刻的性能，但速度受限，主要是因为需要从噪声开始进行反向扩散采样。近期的一些工作尝试通过构建扩散过程来直接桥接特定逆问题的清洁和污染数据以减轻这个问题。在本文中，我们首先将这些现有工作统一命名为直接扩散链桥（DDB），证明尽管受不同理论的启发，但由此产生的算法在参数选择上的不同。然后，我们强调当前DDB框架的一个关键限制，即它不能保证数据一致性。为了解决这个问题，我们提出了一种修改的推断程序，它在不需要精细调整的情况下强制数据一致性。我们将得到的方法称为数据一致的DDB（CDDB），它在感知和失真指标方面都优于不一致的对应物，从而有效地推动了逆问题求解器的最新进展。

    Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the
    
[^31]: 一种融合估计和规划实现探索的最大化目标函数的在线强化学习方法

    One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration. (arXiv:2305.18258v1 [cs.LG])

    [http://arxiv.org/abs/2305.18258](http://arxiv.org/abs/2305.18258)

    提出一种在线强化学习方法Maximize to Explore (MEX)，只需优化一个无约束的目标函数，自动平衡探索和利用，实现次线性遗憾。

    

    在在线强化学习中，平衡探索和利用对于以有效的方式找到最优策略至关重要。为了实现这一目标，现有的在线强化学习算法通常包括三个组成部分：估计、规划和探索。然而，为了应对通用函数逼近器，在大多数情况下都需要使用不切实际的算法组件来激励探索，例如数据相关的级别集内优化或繁琐的采样过程。为了解决这一挑战，我们提出了一种易于实现的强化学习框架，称为Maximize to Explore (MEX) ，它只需要无约束地优化一个集成了估计和规划组件的单一目标函数，同时自动平衡探索和利用。理论上，我们证明了对于马尔可夫决策过程的通用函数逼近，MEX实现了一个次线性的遗憾，进一步：

    In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textit{Maximize to Explore} (\texttt{MEX}), which only needs to optimize \emph{unconstrainedly} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \texttt{MEX} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further 
    
[^32]: 大部分神经网络几乎是可学习的

    Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])

    [http://arxiv.org/abs/2305.16508](http://arxiv.org/abs/2305.16508)

    本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。

    

    我们提出了一个PTAS来学习随机常数深度网络。我们证明了对于任何固定的$\epsilon>0$和深度$i$，存在一个多项式时间算法，对于$\sqrt{d} \cdot \mathbb{S}^{d-1}$上的任何分布，学习随机Xavier网络的深度$i$，误差为$\epsilon$。该算法的时间和样本复杂度为$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$，其中$\bar d$是网络的大小。对于某些类似于Sigmoid和ReLU的激活函数，可以将误差界限改进为$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$，从而得到一种几乎多项式时间算法来学习常数深度随机网络。

    We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
    
[^33]: 线性预测器和神经网络的初始化相关样本复杂度

    Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks. (arXiv:2305.16475v1 [cs.LG])

    [http://arxiv.org/abs/2305.16475](http://arxiv.org/abs/2305.16475)

    本文提供了关于线性预测器和神经网络初始化相关样本复杂度的新结果，解决了一些文献中存在的问题，并且证明了新的凸线性预测问题可以被学习。

    

    我们提供了关于向量值线性预测器(由矩阵参数化)、更一般的神经网络的样本复杂性的新结果。专注于大小无关的界限，在这种情况下，仅控制从某个固定参考矩阵$W_0$的参数的Frobenius范数距离，我们展示了样本复杂度行为可以出人意料地不同于我们在研究标量值线性预测器方面所期望的。这还导致了前馈神经网络的新样本复杂度界限，解决了一些文献中存在的问题，并确立了一个新的凸线性预测问题，证明了它可以在没有统一收敛的情况下被学习。

    We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence.
    
[^34]: 从ReLU神经网络的缓和过拟合到良性过拟合

    From Tempered to Benign Overfitting in ReLU Neural Networks. (arXiv:2305.15141v1 [cs.LG])

    [http://arxiv.org/abs/2305.15141](http://arxiv.org/abs/2305.15141)

    本论文通过对二层ReLU神经网络进行研究，证明了各种假设下过拟合的类型会从一维数据的极端情况下缓和到高维的良性，揭示了输入维度在神经网络过拟合中的关键作用。

    

    过参数化神经网络被观察到即使训练模型来完美地适应嘈杂的数据也能很好地推广。这一现象引发了大量关于“良性过拟合”的工作，其中内插预测器实现接近最优性能。最近，有人猜测并经验性地观察到神经网络的行为通常更好地描述为“缓和过拟合”，其中性能既非最优，也非微不足道，并随噪声水平的变化而降低。然而，迄今为止，这一主张尚缺乏关于非线性神经网络理论的证明。在这项工作中，我们提供了几个结果，旨在弥合这些互补的观点。我们研究了一个简单的分类设置，使用二层ReLU神经网络，并证明在各种假设下，过拟合的类型从一维数据的极端情况下缓和到高维的良性。因此，我们证明输入维度在这种情况下有关键作用。

    Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on "benign overfitting", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as "tempered overfitting", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the type of overfitting in thi
    
[^35]: 低秩协变量逼近的误差变量Frechet回归

    Errors-in-variables Fr\'echet Regression with Low-rank Covariate Approximation. (arXiv:2305.09282v1 [stat.ME])

    [http://arxiv.org/abs/2305.09282](http://arxiv.org/abs/2305.09282)

    本论文提出了一种低秩协变量逼近的误差变量Frechet回归的方法，旨在提高回归估计器的效率和准确性，并实现在高维度和误差变量回归设置中更加有效的建模和估计。

    

    Frechet回归已成为处理非欧几里得响应变量的回归分析的一种有前途的方法。然而，它依赖于理想情况下丰富和无噪声的协变量数据，因此其实际应用受到限制。本文提出了一种新的估计方法，通过利用协变量矩阵中固有的低秩结构来解决这些限制。我们的提出的框架结合了全局Frechet回归和主成分回归的概念，旨在提高回归估计器的效率和准确性。通过纳入低秩结构，我们的方法使得在高维度和误差变量回归设置中更加有效的建模和估计成为可能。我们对提议的估计器的大样本性质进行了理论分析，包括偏差、方差和由于测量误差引起的其他变化的全面率分析。此外，我们的数值实验验证了我们提出方法的性能。

    Fr\'echet regression has emerged as a promising approach for regression analysis involving non-Euclidean response variables. However, its practical applicability has been hindered by its reliance on ideal scenarios with abundant and noiseless covariate data. In this paper, we present a novel estimation method that tackles these limitations by leveraging the low-rank structure inherent in the covariate matrix. Our proposed framework combines the concepts of global Fr\'echet regression and principal component regression, aiming to improve the efficiency and accuracy of the regression estimator. By incorporating the low-rank structure, our method enables more effective modeling and estimation, particularly in high-dimensional and errors-in-variables regression settings. We provide a theoretical analysis of the proposed estimator's large-sample properties, including a comprehensive rate analysis of bias, variance, and additional variations due to measurement errors. Furthermore, our numeri
    
[^36]: 通过重新参数化学习实现在线反馈的实现式预测

    Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])

    [http://arxiv.org/abs/2305.01094](http://arxiv.org/abs/2305.01094)

    本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。

    

    本文提出了在数据分布由模型部署自身改变的情形下预测的一个框架——实现式预测。现有研究的重点在于优化准确性，但是其假设往往难以在实践中得到满足。本文针对这类问题，提出了一种两层零阶优化算法，通过重新参数化实现式预测目标，从而将非凸的目标转化为凸的目标。

    Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
    
[^37]: 利用双时间尺度制度证明神经网络的收敛性研究

    Leveraging the two timescale regime to demonstrate convergence of neural networks. (arXiv:2304.09576v1 [math.OC])

    [http://arxiv.org/abs/2304.09576](http://arxiv.org/abs/2304.09576)

    研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。

    

    我们研究了浅层神经网络的训练动态，在内层步长远小于外层步长的双时间尺度制度下。在这个制度下，在简单的单变量环境中，我们证明了梯度流收敛于非凸优化问题的全局最优解。我们的结果不需要神经元数量趋于无限，这使我们的结果不同于最近流行的方法，如神经切向核或平均场制度。我们提供实验说明，显示随机梯度下降按照我们对梯度流的描述进行行为，并因此在双时间尺度制度下收敛于全局最优解，但在此制度之外可能失败。

    We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.
    
[^38]: 通过图表自编码器进行内部数据结构的深度非参数估计：广义误差和鲁棒性。

    Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])

    [http://arxiv.org/abs/2303.09863](http://arxiv.org/abs/2303.09863)

    本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。

    

    自编码器在学习高维数据的低维潜在特征方面已经在各种应用中展现出了显着的成功。假设数据在低维流形附近采样，我们采用图表自编码器，将数据编码为一组图表上的低维潜在特征，从而保留了数据流形的拓扑和几何。我们的论文为图表自编码器的广义误差建立了统计保证，并且通过考虑$d$维流形上$n$个带噪声训练样本及其无噪声对应物来展示它们的去噪能力。通过训练自编码器，我们展示了图表自编码器能够有效地去噪输入数据和正态分布噪声。我们证明，在适当的网络架构下，图表自编码器实现了一个大致为$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$阶的平方广义误差，该误差取决于流形的内在维度，并且仅弱依赖于样本数量$n$。

    Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
    
[^39]: 在结构分布偏移条件下评估图模型的鲁棒性和不确定性

    Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13875](http://arxiv.org/abs/2302.13875)

    该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。

    

    在基于机器学习的可靠决策系统中，模型必须对分布偏移具有鲁棒性或提供其预测的不确定性。在图学习的节点级问题中，分布偏移可能尤为复杂，因为样本是相互依赖的。为了评估图模型的性能，重要的是在各种有意义的分布偏移下对它们进行测试。然而，大多数考虑节点级分布偏移的图基准主要关注节点特征，而结构属性对图问题也很重要。在这项工作中，我们提出了一种基于图结构引出多样化分布偏移的通用方法。我们使用这种方法根据几个节点的结构属性：流行度、局部性和密度来创建数据分割。在我们的实验中，我们全面评估了所提出的分布偏移，并表明它们对于现有的图模型可能非常具有挑战性。我们还修订了一些关于基准测试图模型的先前工作，并提出了一组新的基准测试，考虑了结构分布偏移条件。

    In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
    
[^40]: (S)GD在对角线线性网络上的隐式正则化、大步长和稳定边缘的影响研究

    (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability. (arXiv:2302.08982v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08982](http://arxiv.org/abs/2302.08982)

    本文研究了在对角线线性网络上，随机性和大步长对梯度下降(GD)和随机梯度下降(SGD)的隐式正则化的影响。实验结果表明大步长对稀疏回归问题中的SGD有益处，但对GD可能有害。这种影响在接近发散阈值的紧密步长下被放大。

    

    本文研究了随机性和大步长对梯度下降(GD)和随机梯度下降(SGD)在对角线线性网络上的隐式正则化的影响。我们证明了在过参数化的回归设置中，使用宏观步长的GD和SGD收敛，并通过隐式正则化问题描述它们的解。我们的清晰描述为我们提供了关于随机性和步长对恢复解的影响的定性见解。具体而言，我们表明在稀疏回归问题中，大步长对SGD有稳定的好处，但对GD的稀疏解恢复可能产生阻碍。这些效应在紧密窗口内的步长下被放大，位于“稳定边缘”区域的步长。实验结果支持我们的发现。

    In this paper, we investigate the impact of stochasticity and large stepsizes on the implicit regularisation of gradient descent (GD) and stochastic gradient descent (SGD) over diagonal linear networks. We prove the convergence of GD and SGD with macroscopic stepsizes in an overparametrised regression setting and characterise their solutions through an implicit regularisation problem. Our crisp characterisation leads to qualitative insights about the impact of stochasticity and stepsizes on the recovered solution. Specifically, we show that large stepsizes consistently benefit SGD for sparse regression problems, while they can hinder the recovery of sparse solutions for GD. These effects are magnified for stepsizes in a tight window just below the divergence threshold, in the "edge of stability" regime. Our findings are supported by experimental results.
    
[^41]: 通过$\ell_{2,\infty}$张量扰动界限估计更高阶混合成员关系

    Estimating Higher-Order Mixed Memberships via the $\ell_{2,\infty}$ Tensor Perturbation Bound. (arXiv:2212.08642v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.08642](http://arxiv.org/abs/2212.08642)

    本文提出了一种用于估计更高阶混合成员关系的方法，该方法基于$\ell_{2,\infty}$张量扰动界限，通过张量混合成员模型对多样化数据的社区结构进行建模，并使用高阶正交迭代算法进行估计过程。通过提供每个节点的误差界限来证明了估计过程的一致性，展示了高阶结构对估计精度的影响。

    

    更高阶的多样化数据在机器学习和统计中非常普遍，并经常表现出类似社区的结构，其中每个分量（节点）在每个不同的模式上都有一个与之关联的社区成员资格。本文提出了张量混合成员模型，这是张量块模型的一个推广，其假设成员关系不需要是离散的，而是潜在社区的凸组合。我们证明了我们模型的可辨识性，并提出了一种基于高阶正交迭代算法（HOOI）与单纯形角点寻找算法组合的计算效率估计过程。然后，我们通过提供每个节点的误差界限来证明我们的估计过程的一致性，这展示了高阶结构对估计精度的影响。为了证明我们的一致性结果，我们开发了独立的、可能异方差的$\ell_{2,\infty}$张量扰动界限的HOOI方法。

    Higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. In this paper we propose the tensor mixed-membership blockmodel, a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. We establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm (HOOI) for tensor SVD composed with a simplex corner-finding algorithm. We then demonstrate the consistency of our estimation procedure by providing a per-node error bound, which showcases the effect of higher-order structures on estimation accuracy. To prove our consistency result, we develop the $\ell_{2,\infty}$ tensor perturbation bound for HOOI under independent, possibly heteroskedastic, su
    
[^42]: Bagging在过度参数化学习中的风险刻画和风险单调化

    Bagging in overparameterized learning: Risk characterization and risk monotonization. (arXiv:2210.11445v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2210.11445](http://arxiv.org/abs/2210.11445)

    本文研究了过度参数化学习中Bagging预测器的风险问题，并提出了通用策略来分析Bagging预测器的风险。通过具体化策略，我们得出了Bagging Ridge和Ridgeless预测器的精确渐近风险，并提供了一种交叉验证过程来选择Bagging的最佳子样本大小，以消除风险的非单调行为。

    

    Bagging是统计学和机器学习中常用的集成技术，用于提高预测模型的性能。本文研究了在比例渐近情况下，各种变体的Bagging预测器的预测风险，其中特征数与观测数的比值收敛到常数。具体而言，我们提出了一种分析Bagging预测器在平方误差损失下的预测风险的通用策略，利用简单随机抽样的经典结果。通过特殊化该策略，我们推导了具有任意数量的包的Bagging Ridge和Ridgeless预测器在具有任意特征协方差矩阵和信号向量的良好指定线性模型下的精确渐近风险。此外，我们提供了一种通用的交叉验证过程，用于选择Bagging的最佳子样本大小，并讨论其在消除样本大小的风险的非单调行为方面的实用性。

    Bagging is a commonly used ensemble technique in statistics and machine learning to improve the performance of prediction procedures. In this paper, we study the prediction risk of variants of bagged predictors under the proportional asymptotics regime, in which the ratio of the number of features to the number of observations converges to a constant. Specifically, we propose a general strategy to analyze the prediction risk under squared error loss of bagged predictors using classical results on simple random sampling. Specializing the strategy, we derive the exact asymptotic risk of the bagged ridge and ridgeless predictors with an arbitrary number of bags under a well-specified linear model with arbitrary feature covariance matrices and signal vectors. Furthermore, we prescribe a generic cross-validation procedure to select the optimal subsample size for bagging and discuss its utility to eliminate the non-monotonic behavior of the limiting risk in the sample size (i.e., double or m
    
[^43]: 具有假发现率保证的自适应新颖检测

    Adaptive novelty detection with false discovery rate guarantee. (arXiv:2208.06685v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.06685](http://arxiv.org/abs/2208.06685)

    本研究提出了一种自适应新颖检测方法AdaDetect，能够在有限样本中控制假发现率，而无需分布假设。方法灵活适用于任何概率分类算法，并通过数据自适应学习变换，将功率集中在区分内点和外点的方向上。此外，还提出了适应于空值比例的AdaDetect变体。方法在合成数据集和真实数据集上进行了演示。

    

    本文研究了半监督的新颖检测问题，研究人员可以获得一组“典型”测量数据。受到多重检验和符合推断的最新进展的启发，我们提出了AdaDetect，一种灵活的方法，能够适用于任何概率分类算法，并在有限样本中控制检测到的新颖性的假发现率（FDR），而无需除了可交换性以外的任何分布假设。与通常预先指定p值函数的经典FDR控制方法不同，AdaDetect以数据自适应的方式学习变换，将功率集中在区分内点和外点的方向上。受多重检验文献的启发，我们进一步提出了适应于空值比例的AdaDetect变体，同时保持有限样本的FDR控制。这些方法在合成数据集和真实数据集上进行了演示，包括一个应用程序。

    This paper studies the semi-supervised novelty detection problem where a set of "typical" measurements is available to the researcher. Motivated by recent advances in multiple testing and conformal inference, we propose AdaDetect, a flexible method that is able to wrap around any probabilistic classification algorithm and control the false discovery rate (FDR) on detected novelties in finite samples without any distributional assumption other than exchangeability. In contrast to classical FDR-controlling procedures that are often committed to a pre-specified p-value function, AdaDetect learns the transformation in a data-adaptive manner to focus the power on the directions that distinguish between inliers and outliers. Inspired by the multiple testing literature, we further propose variants of AdaDetect that are adaptive to the proportion of nulls while maintaining the finite-sample FDR control. The methods are illustrated on synthetic datasets and real-world datasets, including an app
    
[^44]: 鲁棒的蒙特卡洛方法输出分析

    Robust Output Analysis with Monte-Carlo Methodology. (arXiv:2207.13612v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2207.13612](http://arxiv.org/abs/2207.13612)

    本论文提出了一个统一的蒙特卡洛抽样输出分析框架，通过对模拟和机器学习输出进行分析，能够非参数化地量化输出的方差和偏差，并提出了一种新的偏差校正估计方法，以提高鲁棒性。

    

    在使用模拟或机器学习进行预测建模时，准确评估估计值的质量是非常重要的，因此需要进行输出分析。近几十年来，输出分析方法已经得到了丰富，并且提出了各种方法来量化输入数据不确定性对模型输出的影响，以增加鲁棒性。然而，大多数方法都是基于假设输入数据符合参数分布族的情况。本文提出了一个统一的输出分析框架，通过蒙特卡洛抽样的方式对模拟和机器学习输出进行分析。该框架可以对输出中引入的方差和偏差进行非参数化的量化，并具有更高阶的准确性。我们通过扩展快速迭代自举抽样和高阶影响函数，提出了一种新的从模型输出中校正偏差的估计方法。为了提高所提方法的可扩展性，我们设计了预算最优规则，并利用控制变量减少方差。

    In predictive modeling with simulation or machine learning, it is critical to accurately assess the quality of estimated values through output analysis. In recent decades output analysis has become enriched with methods that quantify the impact of input data uncertainty in the model outputs to increase robustness. However, most developments are applicable assuming that the input data adheres to a parametric family of distributions. We propose a unified output analysis framework for simulation and machine learning outputs through the lens of Monte Carlo sampling. This framework provides nonparametric quantification of the variance and bias induced in the outputs with higher-order accuracy. Our new bias-corrected estimation from the model outputs leverages the extension of fast iterative bootstrap sampling and higher-order influence functions. For the scalability of the proposed estimation methods, we devise budget-optimal rules and leverage control variates for variance reduction. Our t
    
[^45]: 在治疗社区中识别同伴影响

    Identifying Peer Influence in Therapeutic Communities. (arXiv:2203.14223v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2203.14223](http://arxiv.org/abs/2203.14223)

    本研究调查了治疗社区中的同伴影响或角色模型效应对于成功毕业的影响。通过分析三个治疗社区的观察数据，我们发现肯定的同伴交流对于居民在自己离开之前成功毕业与否有显著影响。

    

    我们研究在治疗社区中是否存在同伴影响或角色模型效应对于成功毕业的影响。我们分析了3个保留了居民之间确认和修正交流记录以及他们入住和离开日期的治疗社区的匿名个体观察数据。这些确认交流使我们能够形成同伴网络，而入住和离开日期使我们能够定义感兴趣的因果效应。我们将因果角色模型效应概念化为居民（自我）观察到他们的某个社交联系人（例如，给予肯定的同伴）在自我离开之前成功毕业与不成功毕业的预期结果之间的差异。由于同伴影响通常与观察数据中未观察到的同质性混淆，我们使用潜变量模型对网络进行建模以估计同质性并将其包含在结果方程中。我们提供了一个理论保证，它可以解决网络中的内生性问题并提供一致的估计。

    We investigate if there is a peer influence or role model effect on successful graduation from Therapeutic Communities (TCs). We analyze anonymized individual-level observational data from 3 TCs that kept records of written exchanges of affirmations and corrections among residents, and their precise entry and exit dates. The affirmations allow us to form peer networks, and the entry and exit dates allow us to define a causal effect of interest. We conceptualize the causal role model effect as measuring the difference in the expected outcome of a resident (ego) who can observe one of their social contacts (e.g., peers who gave affirmations), to be successful in graduating before the ego's exit vs not successfully graduating before the ego's exit. Since peer influence is usually confounded with unobserved homophily in observational data, we model the network with a latent variable model to estimate homophily and include it in the outcome equation. We provide a theoretical guarantee that 
    
[^46]: 混合模型中的虚假成员率控制

    False membership rate control in mixture models. (arXiv:2203.02597v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2203.02597](http://arxiv.org/abs/2203.02597)

    本文在混合模型框架中重新审视了无监督聚类方法，并开发了一种方法，能够保证虚假成员率不超过给定的阈值α。

    

    聚类任务是将样本元素划分为同质群组的任务。大多数数据集包含了模糊且本质上难以归属于某个群组的个体。然而，在实际应用中，将个体错误分类可能是灾难性的，应该尽量避免。为了保持错误分类率较低，可以决定只对部分样本进行分类。在监督的情境中，这种方法被称为具有放弃选择的分类。本文重新审视了该方法在无监督混合模型框架中的应用，并旨在开发一种方法，以保证虚假成员率（FMR）不超过预定的名义水平α。提出了一种插补过程，并通过明确的剩余项量化FMR与目标水平α之间的偏差，提供了一种理论分析。程序还提供了自举版本。

    The clustering task consists in partitioning elements of a sample into homogeneous groups. Most datasets contain individuals that are ambiguous and intrinsically difficult to attribute to one or another cluster. However, in practical applications, misclassifying individuals is potentially disastrous and should be avoided. To keep the misclassification rate small, one can decide to classify only a part of the sample. In the supervised setting, this approach is well known and referred to as classification with an abstention option. In this paper the approach is revisited in an unsupervised mixture model framework and the purpose is to develop a method that comes with the guarantee that the false membership rate (FMR) does not exceed a pre-defined nominal level $\alpha$. A plug-in procedure is proposed, for which a theoretical analysis is provided, by quantifying the FMR deviation with respect to the target level $\alpha$ with explicit remainder terms. Bootstrap versions of the procedure 
    
[^47]: 组合微调：冻结预训练的降噪自编码器以提高泛化性能

    Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. (arXiv:2006.16205v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.16205](http://arxiv.org/abs/2006.16205)

    本论文提出了一种组合微调方法，通过冻结预训练的降噪自编码器来保留输出结构，从而显著降低预测器的复杂性并提高泛化性能。

    

    本文关注受输出有效性约束的结构化输出的预测问题，例如将伪代码翻译为代码时，代码必须能够编译。虽然标记的输入-输出对很难获取，但是“无标签”的输出，即没有对应输入的输出，是免费提供的（例如GitHub上的代码），并且提供了有关输出有效性的信息。我们可以通过预训练降噪器来捕捉输出结构，该降噪器用于去噪无标签输出的损坏版本。我们首先证明了在预训练之后进行标准微调会破坏部分输出结构。然后，我们提出了组合微调方法，该方法将预训练的降噪器与预测器组合进行微调，其中降噪器被冻结以保留输出结构。对于两层ReLU网络，我们证明了组合微调显著降低了预测器的复杂性，从而提高了泛化性能。在实证方面，我们展示了组合微调在两个伪代码到代码翻译任务上优于标准微调。

    We focus on prediction problems with structured outputs that are subject to output validity constraints, e.g. pseudocode-to-code translation where the code must compile. While labeled input-output pairs are expensive to obtain, "unlabeled" outputs, i.e. outputs without corresponding inputs, are freely available (e.g. code on GitHub) and provide information about output validity. We can capture the output structure by pre-training a denoiser to denoise corrupted versions of unlabeled outputs. We first show that standard fine-tuning after pre-training destroys some of this structure. We then propose composed fine-tuning, which fine-tunes a predictor composed with the pre-trained denoiser, which is frozen to preserve output structure. For two-layer ReLU networks, we prove that composed fine-tuning significantly reduces the complexity of the predictor, thus improving generalization. Empirically, we show that composed fine-tuning improves over standard fine-tuning on two pseudocode-to-code 
    
[^48]: 意识的神经计算模型：目标对齐的内部表示操作理论（GARIM）

    A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13490](http://arxiv.org/abs/1912.13490)

    这个论文提出了一个神经计算框架下的意识理论，称为“目标对齐的内部表示操作”（GARIM）。该理论认为意识支持对目标相关的内部表示进行主动操作，使其与追求的目标更加对齐，从而增加目标导向行为的灵活性。

    

    意识作为人类认知的核心要素，已经通过神经科学、心理学、人工智能和机器人技术等多种科学方法进行研究。然而，这些领域之间的不良整合限制了对意识的完整和清晰理解。在这篇论文中，我们通过提出一个神经计算框架下的“目标对齐的内部表示操作”（GARIM）意识理论，为改善这种整合做出了贡献。GARIM理论的核心思想是，意识支持对目标相关的内部表示（如世界状态、对象和行为序列）进行主动操作，使它们与追求的目标更加对齐。这些操作使得意识代理能够在内部产生其所缺乏的知识，以应对新条件和目标，从而增加目标导向行为的灵活性。表示的操作由四个神经功能宏系统（Hierarc...

    Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
    
[^49]: 社区探测和随机块模型

    Community Detection and Stochastic Block Models. (arXiv:1703.10146v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/1703.10146](http://arxiv.org/abs/1703.10146)

    本文综述了随机块模型（SBM）中社区探测的最新发展，包括精确、部分和弱恢复等各种恢复要求，并探讨了相变阈值、SNR-互信息权衡以及信息论和计算阈值之间的差距。

    

    随机块模型（SBM）是一个随机图模型，其中不同组的顶点以不同的方式连接。它被广泛应用作为研究聚类和社区探测的一个典型模型，并提供了一个研究组合统计学和更一般的数据科学中出现的信息理论和计算权衡的肥沃领域。本文概述了近期关于SBM中社区探测的基本限制的发展，包括信息论和计算权衡，以及精确、部分和弱恢复等各种恢复要求。讨论的主要结果包括Chernoff-Hellinger阈值下的精确恢复的相变，Kesten-Stigum阈值下的弱恢复的相变，部分恢复的最佳SNR-互信息权衡，以及信息论和计算阈值之间的差距。

    The stochastic block model (SBM) is a random graph model with different group of vertices connecting differently. It is widely employed as a canonical model to study clustering and community detection, and provides a fertile ground to study the information-theoretic and computational tradeoffs that arise in combinatorial statistics and more generally data science.  This monograph surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational tradeoffs, and for various recovery requirements such as exact, partial and weak recovery. The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal SNR-mutual information tradeoff for partial recovery, and the gap between information-theoretic and computational thresholds.  The monograph gives a principled derivat
    

