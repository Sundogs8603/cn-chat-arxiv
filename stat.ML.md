# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization](https://arxiv.org/abs/2403.04764) | 该论文提出了一种用于批量贝叶斯优化的高效算法，通过最小化Thompson抽样近似的遗憾与不确定性比率，成功协调每个批次的动作选择，同时实现高概率的理论保证，并在非凸测试函数上表现出色. |
| [^2] | [GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks](https://arxiv.org/abs/2403.04747) | 通过提出一种保持方差的聚合函数（VPA），该函数在维持图神经网络（GNNs）的表达能力的基础上，提高了前向和后向动力学，进而导致了增强的预测性能和改善的学习动态。 |
| [^3] | [SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions](https://arxiv.org/abs/2403.04744) | 本研究证明在非高斯成分分析中，对于区分标准多元高斯和在随机隐藏方向上行为类似某一维分布而在正交补上行为类似标准高斯的问题，先前被要求的卡方条件实际上是不必要的。 |
| [^4] | [A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation](https://arxiv.org/abs/2403.04726) | 提出了一种次二次时间算法，用于在存在恶意异常值的情况下进行鲁棒稀疏均值估计 |
| [^5] | [Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration](https://arxiv.org/abs/2403.04629) | 提出了ShapleyBO框架，用Shapley值解释贝叶斯优化提议，量化每个参数对于优化过程的贡献，并能够区分不同类型的不确定性探索贡献。 |
| [^6] | [Repelling-Attracting Hamiltonian Monte Carlo](https://arxiv.org/abs/2403.04607) | 我们提出了一种新的哈密顿蒙特卡洛采样方法 RAHMC，通过引入摩擦系数，采用耗散动力学来击退和吸引采样器，从而高效地从多模态分布中抽样。 |
| [^7] | [Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition](https://arxiv.org/abs/2403.04568) | 通过引入新的最小二乘估计器和self-normalized技术，我们提出了一个新算法，显著改进了线性混合MDPs中的遗憾上界。 |
| [^8] | [What makes an image realistic?](https://arxiv.org/abs/2403.04493) | 论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。 |
| [^9] | [Signature Isolation Forest](https://arxiv.org/abs/2403.04405) | 介绍了一种新颖的异常检测算法"Signature Isolation Forest"，利用粗路径理论的签名变换去除了Functional Isolation Forest的线性内积和词典选择方面的限制。 |
| [^10] | [A Novel Theoretical Framework for Exponential Smoothing](https://arxiv.org/abs/2403.04345) | 简单指数平滑在优化一系列高斯对数似然函数时自然产生出递归方程，该方法可可靠地估计基础趋势，为其鲁棒性提供了新的理论保证。 |
| [^11] | [Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations](https://arxiv.org/abs/2403.04246) | 本研究提出了一种新颖的CNN-LSTM三阶段模型PEnet，通过CNN对数据特征进行浓缩，提供了一种端到端方法，具有高准确性和适应性，以及长序列观测的增强推断速度和高泛化能力，从而在估计SDE方面取得了显著优势。 |
| [^12] | [Regularized DeepIV with Model Selection](https://arxiv.org/abs/2403.04236) | 本文提出了一种名为正则化DeepIV（RDIV）回归的新方法，能够避免IV回归唯一标识、极小极大计算预言和缺乏模型选择过程等限制，同时实现了通用函数逼近。 |
| [^13] | [Fundamental limits of Non-Linear Low-Rank Matrix Estimation](https://arxiv.org/abs/2403.04234) | 证明了贝叶斯最优性能由等效高斯模型和有效先验确定，信号重构需要增长的信噪比条件，并提供了针对非线性低秩矩阵估计问题的渐近误差特征化和近似传递消息算法。 |
| [^14] | [Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference](https://arxiv.org/abs/2403.04082) | 通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断 |
| [^15] | [Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment](https://arxiv.org/abs/2403.04039) | 论文讨论了如何确定足够大的样本规模，以在数据驱动的子组中估计条件反事实期望，通过将原始目标转化为同时推断问题来解决估计误差增加的可能性，并且允许在固定的样本大小预算下逆转问题以确定可行的治疗手臂数量或分区复杂性。 |
| [^16] | [Online Learning with Unknown Constraints](https://arxiv.org/abs/2403.04033) | 在线学习中通过元算法结合在线回归预测器估计未知安全约束，并将在线学习预测转换为符合约束的预测，同时保证在每一轮高概率地满足安全约束。算法的后悔受到在线回归和在线学习预测器的限制，模型类中未知安全约束的逃避维度，以及捕捉安全学习困难的新颖复杂度度量的约束。 |
| [^17] | [Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent](https://arxiv.org/abs/2403.04015) | 通过引入模拟特征指导和强化学习优化的创新框架，提出了一种用于特征选择的方法，以识别最佳有效的特征子集。 |
| [^18] | [On the Efficient Marginalization of Probabilistic Sequence Models](https://arxiv.org/abs/2403.04005) | 论文提出了一系列新颖高效的逼近技术，用于序贯模型的边缘化，这些技术是模型无关的，仅依赖于预训练自回归模型的下一步条件分布的访问和采样。 |
| [^19] | [Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability](https://arxiv.org/abs/2403.03967) | 通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。 |
| [^20] | [Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications](https://arxiv.org/abs/2403.03905) | 我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限 |
| [^21] | [Spectral Algorithms on Manifolds through Diffusion](https://arxiv.org/abs/2403.03669) | 本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。 |
| [^22] | [Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application](https://arxiv.org/abs/2403.03602) | 通过基于数据的方法模拟了气缸压力和循环变化，以解决无法捕捉循环变化的问题，对于燃烧控制设计非常重要 |
| [^23] | [CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver](https://arxiv.org/abs/2403.03391) | CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。 |
| [^24] | [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2402.02746) | 标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。 |
| [^25] | [On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning](https://arxiv.org/abs/2311.12688) | 结合拆分置信预测和贝叶斯深度学习在多类图像分类中的分布外覆盖率基于模型在校准集上的信心水平，可能会影响模型对分布外数据的处理。 |
| [^26] | [Misspecification-robust Sequential Neural Likelihood for Simulation-based Inference](https://arxiv.org/abs/2301.13368) | 本文提出了一种新的顺序神经似然方法，通过纳入额外的调整参数，使其对模型错误规定具有鲁棒性，并能够识别数据的特征。 |
| [^27] | [First-order penalty methods for bilevel optimization](https://arxiv.org/abs/2301.01716) | 本文研究双层优化问题中的一阶惩罚方法，提出了寻找$\varepsilon$-KKT解的方法，并在适当假设下确立了其操作复杂度。 |
| [^28] | [Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks](https://arxiv.org/abs/2208.07581) | 提出了一种新的方法框架，利用神经网络进行极端分位数回归，以更好地理解和量化美国极端森林火灾的风险 |
| [^29] | [On the expressivity of bi-Lipschitz normalizing flows](https://arxiv.org/abs/2107.07232) | 讨论了双Lipschitz正规化流的表达能力，发现了一些难以逼近的目标分布，并通过给出下界刻画了它们之间的距离，最后讨论了使用更复杂的潜在分布等潜在改进方法。 |
| [^30] | [A Retrospective Approximation Approach for Smooth Stochastic Optimization](https://arxiv.org/abs/2103.04392) | 将随机梯度的单步执行方式推广为回顾逼近方法（RA），在每次迭代期间通过可能多步的确定性求解器提高统计效率。 |
| [^31] | [On the consistency of supervised learning with missing values](https://arxiv.org/abs/1902.06931) | 两种方法在带缺失值的监督学习中表现出一致性，当缺失值不具信息性时，使用常数进行插补是一种简单且重要的实践方法。 |
| [^32] | [Understanding black-box models with dependent inputs through a generalization of Hoeffding's decomposition.](http://arxiv.org/abs/2310.06567) | 通过提出一个新的框架，我们可以解释有关依赖输入的黑箱模型。我们证明了在一些合理的假设下，非线性函数可以唯一分解为每个可能子集的函数之和。这个框架有效地推广了Hoeffding分解，并提供了新颖的可解释性指标。 |
| [^33] | [Unsupervised Fact Verification by Language Model Distillation.](http://arxiv.org/abs/2309.16540) | 本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。 |
| [^34] | [A correlation-based fuzzy cluster validity index with secondary options detector.](http://arxiv.org/abs/2308.14785) | 本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。 |
| [^35] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^36] | [Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models.](http://arxiv.org/abs/2306.09251) | 该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。 |
| [^37] | [Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds.](http://arxiv.org/abs/2306.06836) | 本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。 |
| [^38] | [Diffusion Models for Constrained Domains.](http://arxiv.org/abs/2304.05364) | 本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。 |
| [^39] | [Label Alignment Regularization for Distribution Shift.](http://arxiv.org/abs/2211.14960) | 这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。 |

# 详细

[^1]: 将Thompson抽样遗憾与Sigma比率（TS-RSR）最小化：一种用于批量贝叶斯优化的经过证明的高效算法

    Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization

    [https://arxiv.org/abs/2403.04764](https://arxiv.org/abs/2403.04764)

    该论文提出了一种用于批量贝叶斯优化的高效算法，通过最小化Thompson抽样近似的遗憾与不确定性比率，成功协调每个批次的动作选择，同时实现高概率的理论保证，并在非凸测试函数上表现出色.

    

    本文提出了一个新的方法，用于批量贝叶斯优化（BO），其中抽样通过最小化Thompson抽样方法的遗憾与不确定性比率来进行。我们的目标是能够协调每个批次中选择的动作，以最小化点之间的冗余，同时关注具有高预测均值或高不确定性的点。我们对算法的遗憾提供了高概率的理论保证。最后，从数字上看，我们证明了我们的方法在一系列非凸测试函数上达到了最先进的性能，在平均值上比几个竞争对手的基准批量BO算法表现提高了一个数量级。

    arXiv:2403.04764v1 Announce Type: new  Abstract: This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.
    
[^2]: GNN-VPA: 一种用于图神经网络的方差保持聚合策略

    GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks

    [https://arxiv.org/abs/2403.04747](https://arxiv.org/abs/2403.04747)

    通过提出一种保持方差的聚合函数（VPA），该函数在维持图神经网络（GNNs）的表达能力的基础上，提高了前向和后向动力学，进而导致了增强的预测性能和改善的学习动态。

    

    图神经网络（GNNs），特别是消息传递神经网络，在物理学、药物发现和分子建模等各个领域表现出色。GNNs的表达能力，特别是在区分非同构图的能力，关键取决于用于消息聚合和图级读出的函数。通过应用信号传播理论，我们提出了一种保持方差的聚合函数（VPA），该函数保持了表达能力，同时提高了前向和后向动力学。实验证明，VPA导致了流行的GNN架构的预测性能提高，同时改善了学习动态。我们的结果可能为无归一化或自归一化的GNNs铺平道路。

    arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.
    
[^3]: SQ在较弱假设下用于非高斯成分分析的下限

    SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions

    [https://arxiv.org/abs/2403.04744](https://arxiv.org/abs/2403.04744)

    本研究证明在非高斯成分分析中，对于区分标准多元高斯和在随机隐藏方向上行为类似某一维分布而在正交补上行为类似标准高斯的问题，先前被要求的卡方条件实际上是不必要的。

    

    我们研究了统计查询（SQ）模型中非高斯成分分析（NGCA）的复杂性。 先前的工作开发了一种一般方法，用于证明这一任务的SQ下界，并适用于广泛的上下文。 特别地，已知对于满足某些条件的任何一维分布$A$，区分标准多元高斯和在随机隐藏方向上的行为类似$A$而在正交补上行为类似标准高斯的分布是SQ-hard的。 所需条件是（1）$A$与标准一维高斯匹配许多低阶矩，和（2）$A$相对于标准高斯的卡方范数是有限的。 虽然满足矩匹配条件对于难度是必要的，但卡方条件仅出于技术原因而被要求。 在这项工作中，我们证明了后一个条件实际上并非必要。

    arXiv:2403.04744v1 Announce Type: new  Abstract: We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In partic
    
[^4]: 一种用于鲁棒稀疏均值估计的次二次时间算法

    A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation

    [https://arxiv.org/abs/2403.04726](https://arxiv.org/abs/2403.04726)

    提出了一种次二次时间算法，用于在存在恶意异常值的情况下进行鲁棒稀疏均值估计

    

    我们研究了在存在恶意异常值的情况下稀疏均值估计的算法问题。具体来说，算法观测到从$\mathcal{N}(\mu,\mathbf{I}_d)$中的\emph{受损害}样本集，其中未知均值$\mu \in \mathbb{R}^d$被限制为$k$-稀疏。一系列先前的研究工作已经开发出了具有样本复杂度$\mathrm{poly}(k,\log d, 1/\epsilon)$和运行时间$d^2 \mathrm{poly}(k,\log d,1/\epsilon)$的高效算法，其中$\epsilon$是污染分数。特别是，现有算法的最快运行时间是二次的($\Omega(d^2)$)，在高维情况下可能是禁止性的。现有算法运行时间的二次障碍源于这些算法对样本协方差矩阵的依赖，其大小为$d^2$。我们的主要贡献是一种用于鲁棒稀疏均值估计的算法，它在\emph{次二次}时间内运行，使用$\mathrm{poly}

    arXiv:2403.04726v1 Announce Type: cross  Abstract: We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \emph{corrupted} set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean $\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2 \mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \emph{subquadratic} time using $\mathrm{poly
    
[^5]: 用Shapley值解释贝叶斯优化促进人工智能与人类协作

    Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration

    [https://arxiv.org/abs/2403.04629](https://arxiv.org/abs/2403.04629)

    提出了ShapleyBO框架，用Shapley值解释贝叶斯优化提议，量化每个参数对于优化过程的贡献，并能够区分不同类型的不确定性探索贡献。

    

    贝叶斯优化（BO）与高斯过程（GP）已成为解决黑匣子优化问题的不可或缺的算法。然而，BO本身也常常被认为是一个黑匣子，缺乏提供为何提议评估某些参数的理由的方法。我们通过提出ShapleyBO来解决这个问题，这是一个用博弈论Shapley值解释BO提议的框架。它量化了每个参数对BO的收获函数的贡献。利用Shapley值的线性性，我们能够进一步确定每个参数对于像置信边界这样的加法收获函数推动BO的探索和开发的强度。我们还展示了ShapleyBO能够解决探索对于勘探aleatoric和认识epistemic不确定性的贡献。

    arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
    
[^6]: 击退-吸引哈密顿蒙特卡洛

    Repelling-Attracting Hamiltonian Monte Carlo

    [https://arxiv.org/abs/2403.04607](https://arxiv.org/abs/2403.04607)

    我们提出了一种新的哈密顿蒙特卡洛采样方法 RAHMC，通过引入摩擦系数，采用耗散动力学来击退和吸引采样器，从而高效地从多模态分布中抽样。

    

    我们提出了一种哈密顿蒙特卡洛（HMC）的变种，称为击退-吸引哈密顿蒙特卡洛（RAHMC），用于从多模态分布中进行抽样。 RAHMC的关键思想是远离传统HMC保守动力学，转而转向共形哈密顿系统的耗散动力学。具体而言，RAHMC包括两个阶段：一种模击退阶段，鼓励采样器远离高概率密度区域；另一个模吸引阶段，有助于采样器找到并停留在替代模式附近。我们通过引入一个额外的调节参数--摩擦系数来实现这一点。所提出的方法适应目标分布的几何形状，例如模式和密度脊，可以生成越过低概率障碍的提议，且几乎不增加计算开销。

    arXiv:2403.04607v1 Announce Type: cross  Abstract: We propose a variant of Hamiltonian Monte Carlo (HMC), called the Repelling-Attracting Hamiltonian Monte Carlo (RAHMC), for sampling from multimodal distributions. The key idea that underpins RAHMC is a departure from the conservative dynamics of Hamiltonian systems, which form the basis of traditional HMC, and turning instead to the dissipative dynamics of conformal Hamiltonian systems. In particular, RAHMC involves two stages: a mode-repelling stage to encourage the sampler to move away from regions of high probability density; and, a mode-attracting stage, which facilitates the sampler to find and settle near alternative modes. We achieve this by introducing just one additional tuning parameter -- the coefficient of friction. The proposed method adapts to the geometry of the target distribution, e.g., modes and density ridges, and can generate proposals that cross low-probability barriers with little to no computational overhead in 
    
[^7]: 改进的算法用于带有匪夷所思反馈和未知转移的敌对线性混合MDPs

    Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition

    [https://arxiv.org/abs/2403.04568](https://arxiv.org/abs/2403.04568)

    通过引入新的最小二乘估计器和self-normalized技术，我们提出了一个新算法，显著改进了线性混合MDPs中的遗憾上界。

    

    我们研究具有线性函数逼近、未知转移和在匪夷所思反馈设置中的敌对损失的强化学习。具体而言，我们专注于转移核为线性混合模型的线性混合MDPs。我们提出了一种新算法，该算法在高概率下达到了$\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$的遗憾值，其中$d$是特征映射的维度，$S$是状态空间的大小，$A$是动作空间的大小，$H$是每集长度，$K$是集数。我们的结果严格改进了Zhao等人(2023a)中已知的最佳$\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$结果，因为$H \leq S$由层次MDP结构成立。我们的进展主要归因于(i)一种新的最小二乘估计器，用于转移参数，利用了所有状态的访问信息，而不像以前的工作只用一个状态，以及(ii)一种新的self-normalized分。。

    arXiv:2403.04568v1 Announce Type: new  Abstract: We study reinforcement learning with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting. Specifically, we focus on linear mixture MDPs whose transition kernel is a linear mixture model. We propose a new algorithm that attains an $\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability, where $d$ is the dimension of feature mappings, $S$ is the size of state space, $A$ is the size of action space, $H$ is the episode length and $K$ is the number of episodes. Our result strictly improves the previous best-known $\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a) since $H \leq S$ holds by the layered MDP structure. Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized conc
    
[^8]: 使图像真实的因素是什么？

    What makes an image realistic?

    [https://arxiv.org/abs/2403.04493](https://arxiv.org/abs/2403.04493)

    论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。

    

    在过去的十年里，我们在生成看起来真实的数据方面取得了巨大进展，无论是图像、文本、音频还是视频。在这里，我们讨论了与之密切相关的问题，即量化现实主义，即设计能够可靠地区分真实数据和不真实数据的函数。从算法信息理论的观点出发，我们讨论了为什么这个问题很具挑战性，为什么一个好的生成模型单独不能解决它，以及一个好的解决方案应该是什么样的。特别是，我们引入了通用评论者的概念，不像对抗性评论者那样需要对抗性训练。尽管通用评论者并不立即实用，但它们既可以作为引导实际实现的北极星，也可以作为一个工具。

    arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
    
[^9]: Signature Isolation Forest

    Signature Isolation Forest

    [https://arxiv.org/abs/2403.04405](https://arxiv.org/abs/2403.04405)

    介绍了一种新颖的异常检测算法"Signature Isolation Forest"，利用粗路径理论的签名变换去除了Functional Isolation Forest的线性内积和词典选择方面的限制。

    

    Functional Isolation Forest (FIF)是一种针对功能数据设计的最新一流异常检测(AD)算法。它依赖于一种树分区过程，通过将每个曲线观测投影到通过线性内积绘制的词典上来计算异常得分。本文通过引入“Signature Isolation Forest”，一种利用粗路径理论签名变换的新颖AD算法类，来解决这些挑战。我们的目标是通过提出两种算法来消除FIF施加的限制，这两种算法特别针对FIF内积的线性性和词典的选择。

    arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
    
[^10]: 一种新颖的指数平滑理论框架

    A Novel Theoretical Framework for Exponential Smoothing

    [https://arxiv.org/abs/2403.04345](https://arxiv.org/abs/2403.04345)

    简单指数平滑在优化一系列高斯对数似然函数时自然产生出递归方程，该方法可可靠地估计基础趋势，为其鲁棒性提供了新的理论保证。

    

    简单指数平滑是一种经典技术，通过将指数递减的权重分配给过去的观察结果，通过递归方程平滑时间序列数据；有时被提出作为一种经验法则过程。我们提出了一个新颖的理论视角，其中定义简单指数平滑的递归方程自然发生为优化一系列高斯对数似然函数的随机梯度上升方案。在这种分析视角下，我们的主定理表明，-在一般设置下-简单指数平滑收敛到趋势稳态随机过程的趋势邻域。这为指数平滑程序产生可靠的基础趋势估计器提供了新颖的理论保证，阐明了文献中关于简单指数平滑的鲁棒性的长期观察。

    arXiv:2403.04345v1 Announce Type: cross  Abstract: Simple Exponential Smoothing is a classical technique used for smoothing time series data by assigning exponentially decreasing weights to past observations through a recursive equation; it is sometimes presented as a rule of thumb procedure. We introduce a novel theoretical perspective where the recursive equation that defines simple exponential smoothing occurs naturally as a stochastic gradient ascent scheme to optimize a sequence of Gaussian log-likelihood functions. Under this lens of analysis, our main theorem shows that -in a general setting- simple exponential smoothing converges to a neighborhood of the trend of a trend-stationary stochastic process. This offers a novel theoretical assurance that the exponential smoothing procedure yields reliable estimators of the underlying trend shedding light on long-standing observations in the literature regarding the robustness of simple exponential smoothing.
    
[^11]: 基于CNN-LSTM的Levy驱动随机微分方程参数估计的高效性研究

    Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations

    [https://arxiv.org/abs/2403.04246](https://arxiv.org/abs/2403.04246)

    本研究提出了一种新颖的CNN-LSTM三阶段模型PEnet，通过CNN对数据特征进行浓缩，提供了一种端到端方法，具有高准确性和适应性，以及长序列观测的增强推断速度和高泛化能力，从而在估计SDE方面取得了显著优势。

    

    本研究解决了由非高斯噪声驱动的随机微分方程参数估计中的挑战，这对于理解价格波动和传染病传播等动态现象至关重要。先前的研究强调了LSTM网络在估计alpha稳定Levy驱动的SDE参数方面的潜力，但面临高时间复杂度和LSTM链接属性的限制。为了缓解这些问题，我们引入了PEnet，这是一种基于CNN-LSTM的三阶段模型，提供了一种端到端方法，具有优越的准确性和适应不同数据结构的能力，通过CNN对初始数据特征进行浓缩，为长序列观测提供增强的推断速度，并具有高泛化能力，允许其应用于各种复杂的SDE场景。对合成数据集的实验验证了PEnet在估计SDE中的显著优势。

    arXiv:2403.04246v1 Announce Type: cross  Abstract: This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property. To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE
    
[^12]: 具有模型选择的正则化DeepIV

    Regularized DeepIV with Model Selection

    [https://arxiv.org/abs/2403.04236](https://arxiv.org/abs/2403.04236)

    本文提出了一种名为正则化DeepIV（RDIV）回归的新方法，能够避免IV回归唯一标识、极小极大计算预言和缺乏模型选择过程等限制，同时实现了通用函数逼近。

    

    在这篇论文中，我们研究了工具变量（IV）回归的非参数估计。虽然机器学习的最新进展已经引入了灵活的IV估计方法，但它们往往会遇到以下一个或多个限制：（1）将IV回归限制为唯一标识；（2）需要极小极大计算预言，这在实践中非常不稳定；（3）缺乏模型选择过程。在本文中，我们提出了一种可以避免所有三个限制的第一种方法和分析，同时仍然能够实现通用函数逼近。具体而言，我们提出了一种名为正则化DeepIV（RDIV）回归的无极小极大计算预言的方法，可以收敛到最小范数IV解。我们的方法分为两个阶段：首先，我们学习协变量的条件分布，并通过利用所学到的分布，通过最小化Tikhonov正则化损失函数来学习估计值。我们进一步...

    arXiv:2403.04236v1 Announce Type: new  Abstract: In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We furth
    
[^13]: 非线性低秩矩阵估计的基本限制

    Fundamental limits of Non-Linear Low-Rank Matrix Estimation

    [https://arxiv.org/abs/2403.04234](https://arxiv.org/abs/2403.04234)

    证明了贝叶斯最优性能由等效高斯模型和有效先验确定，信号重构需要增长的信噪比条件，并提供了针对非线性低秩矩阵估计问题的渐近误差特征化和近似传递消息算法。

    

    我们考虑从非线性和嘈杂观测中估计低秩矩阵的任务。我们证明了一个强大的普适性结果，表明贝叶斯最优表现由一个等效高斯模型和一个有效先验所决定，其参数完全由非线性函数的展开确定。具体而言，我们表明为了准确重构信号，信噪比需要增长为$N^{\frac 12 (1-1/k_F)}$，其中$k_F$是函数的第一个非零费舍尔信息系数。我们提供了最小可达均方误差（MMSE）的渐近特征化和一种近似传递消息算法，该算法在类似于问题的线性版本的情况下达到了MMSE。我们还提供了通过方法例如主成分分析结合贝叶斯降噪实现的渐近误差，并将它们与贝叶斯最优MMSE进行比较。

    arXiv:2403.04234v1 Announce Type: cross  Abstract: We consider the task of estimating a low-rank matrix from non-linear and noisy observations. We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function. In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as $N^{\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher information coefficient of the function. We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate message-passing algorithm that reaches the MMSE under conditions analogous to the linear version of the problem. We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE.
    
[^14]: 通过插值进行推断：对比表示可证明启用规划和推断

    Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference

    [https://arxiv.org/abs/2403.04082](https://arxiv.org/abs/2403.04082)

    通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断

    

    给定时间序列数据，我们如何回答诸如“未来会发生什么？”和“我们是如何到达这里的？”这类概率推断问题在观测值为高维时具有挑战性。本文展示了这些问题如何通过学习表示的紧凑闭式解决方案。关键思想是将对比学习的变体应用于时间序列数据。之前的工作已经表明，通过对比学习学到的表示编码了概率比。通过将之前的工作扩展以表明表示的边际分布是高斯分布，我们随后证明表示的联合分布也是高斯分布。这些结果共同表明，通过时间对比学习学到的表示遵循高斯马尔可夫链，一种图形模型，其中对表示进行的推断（例如预测、规划）对应于反演低维分布。

    arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
    
[^15]: 用于条件反事实均值估计的样本规模规划: 一个K臂随机实验

    Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment

    [https://arxiv.org/abs/2403.04039](https://arxiv.org/abs/2403.04039)

    论文讨论了如何确定足够大的样本规模，以在数据驱动的子组中估计条件反事实期望，通过将原始目标转化为同时推断问题来解决估计误差增加的可能性，并且允许在固定的样本大小预算下逆转问题以确定可行的治疗手臂数量或分区复杂性。

    

    我们讨论如何确定足够大的样本规模，以估计数据驱动子组中的条件反事实期望，该子组可以由任何特征空间划分算法输出，包括根据预测分数相似的用户进行分组或根据学习的策略树进行分组。在仔细规定推断目标、最小置信水平和最大误差边际后，关键是将原始目标转化为一个同时推断问题，推荐的样本大小以抵消估计误差的增加可能性直接与要进行的推断数量相关。在给定固定样本规模预算的情况下，我们的结果使我们能够将问题反转为关于可行治疗手臂数量或分区复杂性（例如，决策树叶子数量）的问题。使用策略树学习子组，我们评估...

    arXiv:2403.04039v1 Announce Type: new  Abstract: We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups. The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree. After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted. Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves). Using policy trees to learn sub-groups, we evaluate o
    
[^16]: 具有未知约束的在线学习

    Online Learning with Unknown Constraints

    [https://arxiv.org/abs/2403.04033](https://arxiv.org/abs/2403.04033)

    在线学习中通过元算法结合在线回归预测器估计未知安全约束，并将在线学习预测转换为符合约束的预测，同时保证在每一轮高概率地满足安全约束。算法的后悔受到在线回归和在线学习预测器的限制，模型类中未知安全约束的逃避维度，以及捕捉安全学习困难的新颖复杂度度量的约束。

    

    我们考虑在线学习中的问题，其中学习者在每一轮必须遵守一个未知的安全约束。目标是在同时满足安全约束并在后视下最小化对最佳安全动作的后悔。我们提供了一个通用的元算法，利用在线回归预测器来估计未知安全约束，并将在线学习预测转换为符合未知安全约束的预测。在理论方面，我们的算法的后悔可以通过在线回归和在线学习预测器的后悔、包含未知安全约束的模型类的逃避维度，以及一个捕捉安全学习困难程度的新颖复杂度度量来界定。

    arXiv:2403.04033v1 Announce Type: cross  Abstract: We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned
    
[^17]: 通过单个预训练的增强型代理引导的模拟特征选择

    Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent

    [https://arxiv.org/abs/2403.04015](https://arxiv.org/abs/2403.04015)

    通过引入模拟特征指导和强化学习优化的创新框架，提出了一种用于特征选择的方法，以识别最佳有效的特征子集。

    

    特征选择通过消除冗余特征来准备数据的AI可用性。先前的研究主要分为两类：i）监督特征选择，根据特征与目标变量的相关性识别最佳特征子集；ii）无监督特征选择，通过捕获特征集中的基本信息而非使用目标变量来减少特征空间的维度。然而，监督特征选择方法由于依赖目标变量和下游ML任务而导致耗时且泛化能力有限。无监督特征选择方法受限于推导出的特征空间是潜在且不可追踪的。为解决这些挑战，我们引入一种新颖的特征选择框架，通过模拟特征指导并通过强化学习进行优化，以识别最佳有效的特征子集。

    arXiv:2403.04015v1 Announce Type: cross  Abstract: Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves gener
    
[^18]: 关于概率序列模型边缘化的高效性

    On the Efficient Marginalization of Probabilistic Sequence Models

    [https://arxiv.org/abs/2403.04005](https://arxiv.org/abs/2403.04005)

    论文提出了一系列新颖高效的逼近技术，用于序贯模型的边缘化，这些技术是模型无关的，仅依赖于预训练自回归模型的下一步条件分布的访问和采样。

    

    现实世界中的数据经常表现出序列依赖性，涵盖人类行为、医学、金融和气候模拟等各个领域。概率方法捕捉了这些背景下预测相关的固有不确定性，其中自回归模型尤为突出。本论文着重于使用自回归模型回答超出单步预测范围的复杂概率查询，比如未来事件的时间安排或某一事件发生在另一事件之前的可能性。具体来说，我们开发了一系列新颖高效的逼近技术，用于序贯模型的边缘化，这些技术是模型无关的，仅依赖于预训练自回归模型的下一步条件分布的访问和采样，包括传统参数模型以及最新的神经自回归模型。

    arXiv:2403.04005v1 Announce Type: cross  Abstract: Real-world data often exhibits sequential dependence, across diverse domains such as human behavior, medicine, finance, and climate modeling. Probabilistic methods capture the inherent uncertainty associated with prediction in these contexts, with autoregressive models being especially prominent. This dissertation focuses on using autoregressive models to answer complex probabilistic queries that go beyond single-step prediction, such as the timing of future events or the likelihood of a specific event occurring before another. In particular, we develop a broad class of novel and efficient approximation techniques for marginalization in sequential models that are model-agnostic. These techniques rely solely on access to and sampling from next-step conditional distributions of a pre-trained autoregressive model, including both traditional parametric models as well as more recent neural autoregressive models. Specific approaches are pres
    
[^19]: 环境-固有维度差异对对抗脆弱性的影响

    Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability

    [https://arxiv.org/abs/2403.03967](https://arxiv.org/abs/2403.03967)

    通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。

    

    论文介绍了对机器学习模型的对抗攻击存在且对人类来说几乎无法察觉这一事实，在理论上仍然相当神秘。文章引入了两种对抗攻击的概念：自然或在流形上的攻击，这些攻击是可以被人类/神谕感知到的；非自然或脱离流形的攻击，这些攻击则无法被感知到。文章认为脱离流形的攻击存在是数据固有维度与环境维度之间的差异的必然结果。对于2层ReLU网络，我们证明了即使维度差异不影响从观测数据空间中抽取样本的泛化性能，它仍会使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。我们的主要结果提供了在/脱离流形攻击的$\ell_2,\ell_{\infty}$攻击强度与维度差异之间明确的关系。

    arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
    
[^20]: 黑盒$k$-to-$1$-PCA降维：理论与应用

    Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications

    [https://arxiv.org/abs/2403.03905](https://arxiv.org/abs/2403.03905)

    我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限

    

    $k$-主成分分析（$k$-PCA）问题是一种基本的算法原语，在数据分析和降维应用中被广泛使用。在统计环境中，$k$-PCA的目标是识别一个分布的协方差矩阵的顶部特征空间，我们只能通过样本隐式访问这个矩阵。受这些隐式设置的启发，我们分析黑盒缩减方法作为设计$k$-PCA算法的框架，其中我们通过黑盒$1$-PCA预言模拟对未知目标矩阵的访问，该预言返回一个近似的顶部特征向量，根据两个流行的近似概念。尽管这种黑盒方法可能是设计$k$-PCA算法中最自然的基于降维的方法，这种方法，即通过递归调用$1$-PCA预言调用了$k$次，以前很难理解。

    arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
    
[^21]: 通过扩散在流形上的谱算法

    Spectral Algorithms on Manifolds through Diffusion

    [https://arxiv.org/abs/2403.03669](https://arxiv.org/abs/2403.03669)

    本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。

    

    在重现核希尔伯特空间（RKHS）中应用的谱算法的现有研究主要集中在一般核函数上，经常忽略输入特征空间的固有结构。我们的论文引入了一个新的视角，主张输入数据位于一个嵌入到更高维欧几里得空间中的低维流形内。我们研究了RKHS中谱算法的收敛性能，特别是那些由热核生成的，被称为扩散空间的空间。通过结合输入的流形结构，我们采用积分算子技术推导了关于广义范数的紧收敛上界，这表明估计器在强意义下收敛到目标函数，意味着函数本身及其导数同时收敛。这些界提供了两个重要优势：首先，它们是完全连续的。

    arXiv:2403.03669v1 Announce Type: cross  Abstract: The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contin
    
[^22]: 基于数据的具有循环变化的气缸压力模型用于燃烧控制：RCCI发动机应用

    Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application

    [https://arxiv.org/abs/2403.03602](https://arxiv.org/abs/2403.03602)

    通过基于数据的方法模拟了气缸压力和循环变化，以解决无法捕捉循环变化的问题，对于燃烧控制设计非常重要

    

    气缸压力基础控制是先进的预混合燃烧概念的关键推动因素，除了确保稳健且安全的操作外，还可以实现气缸压力和热释放的塑造。这需要快速的面向控制的燃烧模型。多年来，已经提出了平均值模型，可以预测燃烧指标（例如，总的指示平均有效压力，或释放总热量的曲轴旋转角度），或者预测完整的气缸压力。然而，这些模型无法捕捉循环变化，这对于燃烧概念的控制设计至关重要，例如，反应性控制压缩点火，这可能会遭受大幅度的循环变化。在本研究中，使用基于数据的方法对气缸压力和循环变化进行了建模。该模型结合了主成分分解和高斯过程回归。进行了详细研究。

    arXiv:2403.03602v1 Announce Type: cross  Abstract: Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts. Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping. This requires fast control-oriented combustion models. Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure. However, these models are not able to capture cyclic variations. This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations. In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach. The model combines Principle Component Decomposition and Gaussian Process Regression. A detailed study is performed
    
[^23]: CoRMF: 临界有序循环均场伊辛求解器

    CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver

    [https://arxiv.org/abs/2403.03391](https://arxiv.org/abs/2403.03391)

    CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。

    

    我们提出了一种基于RNN的高效伊辛模型求解器，称为Criticality-ordered Recurrent Mean Field (CoRMF)，用于前向伊辛问题。在其核心部分，通过贪婪算法对N个自旋的伊辛模型进行了关键有序自旋序列的引入，从而可以利用自回归均场因子分解，并通过循环神经网络(RNNs)进行优化。我们的方法具有两个显著特点：(i)通过利用底层伊辛图的近似树结构，新获得的关键性顺序使变分均场和RNN之间得以统一，从而允许有效地利用概率推断来探究通常难以处理的伊辛模型;(ii)它具有良好的模块化、独立于模型而又足够表达能力，因此可以完全适用于任何前向伊辛推理问题，而且工作量极小。计算上，通过使用一种方差减少

    arXiv:2403.03391v1 Announce Type: cross  Abstract: We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-redu
    
[^24]: 标准 Gaussian 过程在高维贝叶斯优化中足以应对

    Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization

    [https://arxiv.org/abs/2402.02746](https://arxiv.org/abs/2402.02746)

    标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。

    

    长期以来，人们普遍认为使用标准 Gaussian 过程（GP）进行贝叶斯优化（BO），即标准 BO，在高维优化问题中效果不佳。这种观念可以部分归因于 Gaussian 过程在协方差建模和函数估计中对高维输入的困难。虽然这些担忧看起来合理，但缺乏支持这种观点的经验证据。本文系统地研究了在各种合成和真实世界基准问题上，使用标准 GP 回归进行高维优化的贝叶斯优化。令人惊讶的是，标准 GP 的表现始终位于最佳范围内，往往比专门为高维优化设计的现有 BO 方法表现更好。与刻板印象相反，我们发现标准 GP 可以作为学习高维目标函数的能力强大的代理。在没有强结构假设的情况下，使用标准 GP 进行 BO 可以获得非常好的性能。

    There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
    
[^25]: 关于结合拆分置信预测和贝叶斯深度学习的分布外覆盖率

    On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning

    [https://arxiv.org/abs/2311.12688](https://arxiv.org/abs/2311.12688)

    结合拆分置信预测和贝叶斯深度学习在多类图像分类中的分布外覆盖率基于模型在校准集上的信心水平，可能会影响模型对分布外数据的处理。

    

    贝叶斯深度学习和置信预测是两种用于传达不确定性并增加机器学习系统安全性的方法。我们关注将贝叶斯深度学习与拆分置信预测相结合对分布外覆盖率的影响；特别是在多类图像分类情况下。我们建议，如果模型在校准集上通常缺乏信心，那么得到的置信集可能与简单预测可信集相比，表现出更糟糕的分布外覆盖。相反，如果模型在校准集上过于自信，使用置信预测可能会改善分布外覆盖。我们评估将拆分置信方法与随机梯度下降、深度集合和均场变分推理训练的神经网络相结合的预测集的结果。我们的结果表明，结合B

    arXiv:2311.12688v2 Announce Type: replace  Abstract: Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how this combination effects out-of-distribution coverage; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets. Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. We evaluate prediction sets as a result of combining split conformal methods and neural networks trained with (i) stochastic gradient descent, (ii) deep ensembles, and (iii) mean-field variational inference. Our results suggest that combining B
    
[^26]: 适用于基于模拟推断的误差鲁棒顺序神经似然方法

    Misspecification-robust Sequential Neural Likelihood for Simulation-based Inference

    [https://arxiv.org/abs/2301.13368](https://arxiv.org/abs/2301.13368)

    本文提出了一种新的顺序神经似然方法，通过纳入额外的调整参数，使其对模型错误规定具有鲁棒性，并能够识别数据的特征。

    

    模拟推断技术对于具有难以处理的似然函数的机械和可模拟模型的参数估计至关重要。在传统统计方法(如近似贝叶斯计算和贝叶斯合成似然)的研究中，这些方法通常在规定良好和错误规定的设置下进行研究，但由于浪费模型模拟而导致效率低下。神经网络方法，如顺序神经似然(SNL)，通过利用所有模型模拟来训练神经代理似然函数，避免了这种浪费。然而，SNL在模型错误规定的情况下的性能不可靠，可能导致围绕不准确参数估计的过于自信的后验分布。本文提出了一种新的SNL方法，通过纳入额外的调整参数，使其对模型错误规定具有鲁棒性，并能够识别数据的特征。

    arXiv:2301.13368v2 Announce Type: replace-cross  Abstract: Simulation-based inference techniques are indispensable for parameter estimation of mechanistic and simulable models with intractable likelihoods. While traditional statistical approaches like approximate Bayesian computation and Bayesian synthetic likelihood have been studied under well-specified and misspecified settings, they often suffer from inefficiencies due to wasted model simulations. Neural approaches, such as sequential neural likelihood (SNL) avoid this wastage by utilising all model simulations to train a neural surrogate for the likelihood function. However, the performance of SNL under model misspecification is unreliable and can result in overconfident posteriors centred around an inaccurate parameter estimate. In this paper, we propose a novel SNL method, which through the incorporation of additional adjustment parameters, is robust to model misspecification and capable of identifying features of the data that 
    
[^27]: 双层优化的一阶惩罚方法

    First-order penalty methods for bilevel optimization

    [https://arxiv.org/abs/2301.01716](https://arxiv.org/abs/2301.01716)

    本文研究双层优化问题中的一阶惩罚方法，提出了寻找$\varepsilon$-KKT解的方法，并在适当假设下确立了其操作复杂度。

    

    本文研究了一类无约束和有约束的双层优化问题，其中下层是可能是非光滑凸优化问题，而上层是可能是非凸优化问题。我们引入了$\varepsilon$-KKT解的概念，并展示了一个$\varepsilon$-KKT解在适当假设下会导致一个$O(\sqrt{\varepsilon})$或$O(\varepsilon)$-基于超梯度的稳定点。我们还提出了一阶惩罚方法来寻找它们的$\varepsilon$-KKT解，其子问题实际上是一个结构化的极小极大问题，可以通过作者最近开发的一阶方法适当地解决。在适当假设下，通过它们的基本操作衡量，我们建立了所提出的惩罚方法的“操作复杂度”分别为$O(\varepsilon^{-4}\log\varepsilon^{-1})$和$O(\varepsilon^{-7}\log\varepsilon^{-1})$。

    arXiv:2301.01716v2 Announce Type: replace-cross  Abstract: In this paper we study a class of unconstrained and constrained bilevel optimization problems in which the lower level is a possibly nonsmooth convex optimization problem, while the upper level is a possibly nonconvex optimization problem. We introduce a notion of $\varepsilon$-KKT solution for them and show that an $\varepsilon$-KKT solution leads to an $O(\sqrt{\varepsilon})$- or $O(\varepsilon)$-hypergradient based stionary point under suitable assumptions. We also propose first-order penalty methods for finding an $\varepsilon$-KKT solution of them, whose subproblems turn out to be a structured minimax problem and can be suitably solved by a first-order method recently developed by the authors. Under suitable assumptions, an \emph{operation complexity} of $O(\varepsilon^{-4}\log\varepsilon^{-1})$ and $O(\varepsilon^{-7}\log\varepsilon^{-1})$, measured by their fundamental operations, is established for the proposed penalty 
    
[^28]: 美国极端森林火灾的时空回归建模：部分可解释神经网络

    Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks

    [https://arxiv.org/abs/2208.07581](https://arxiv.org/abs/2208.07581)

    提出了一种新的方法框架，利用神经网络进行极端分位数回归，以更好地理解和量化美国极端森林火灾的风险

    

    许多环境背景下的风险管理需要理解驱动极端事件的机制。用于量化这种风险的有用指标是响应变量的极端分位数，其条件是描述气候、生物圈和环境状态等预测变量。典型情况下，这些分位数位于可观测数据范围之外，因此，为了进行估计，需要在回归框架内指定参数极值模型。在这篇论文中，我们提出了一种新的方法框架，用于利用人工神经网络进行极端分位数回归。

    arXiv:2208.07581v4 Announce Type: replace-cross  Abstract: Risk management in many environmental settings requires an understanding of the mechanisms that drive extreme events. Useful metrics for quantifying such risk are extreme quantiles of response variables conditioned on predictor variables that describe, e.g., climate, biosphere and environmental states. Typically these quantiles lie outside the range of observable data and so, for estimation, require specification of parametric extreme value models within a regression framework. Classical approaches in this context utilise linear or additive relationships between predictor and response variables and suffer in either their predictive capabilities or computational efficiency; moreover, their simplicity is unlikely to capture the truly complex structures that lead to the creation of extreme wildfires. In this paper, we propose a new methodological framework for performing extreme quantile regression using artificial neutral network
    
[^29]: 论双Lipschitz正规化流的表达能力

    On the expressivity of bi-Lipschitz normalizing flows

    [https://arxiv.org/abs/2107.07232](https://arxiv.org/abs/2107.07232)

    讨论了双Lipschitz正规化流的表达能力，发现了一些难以逼近的目标分布，并通过给出下界刻画了它们之间的距离，最后讨论了使用更复杂的潜在分布等潜在改进方法。

    

    一个可逆函数若其自身和其逆函数均具有有界Lipschitz常数，则称之为双Lipschitz函数。 如今，大多数正规化流通过设计或训练而成为双Lipschitz，以限制数值误差 (等等)。 本文讨论了双Lipschitz正规化流的表达能力，并确定了一些难以用这种模型逼近的目标分布。 其次，我们通过给出若干特别不利分布与其最佳逼近之间的总变分距离的若干下界，刻画了双Lipschitz正规化流的表达能力。 最后，我们讨论了一些潜在的解决方法，其中包括使用更复杂的潜在分布。

    arXiv:2107.07232v3 Announce Type: replace  Abstract: An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Nowadays, most Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models. Then, we characterize the expressivity of bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total Variation distance between these particularly unfavorable distributions and their best possible approximation. Finally, we discuss potential remedies which include using more complex latent distributions.
    
[^30]: 一种平滑随机优化的回顾逼近方法

    A Retrospective Approximation Approach for Smooth Stochastic Optimization

    [https://arxiv.org/abs/2103.04392](https://arxiv.org/abs/2103.04392)

    将随机梯度的单步执行方式推广为回顾逼近方法（RA），在每次迭代期间通过可能多步的确定性求解器提高统计效率。

    

    随机梯度（SG）是解决具有平滑（非凸）目标函数$f$和随机一阶oracle的随机优化（SO）问题的事实上的迭代技术。 在每次迭代期间，"确定性求解器"对子采样的确定性问题执行可能多个步骤，并在从统计效率的角度视为进一步求解不必要时停止。 因此，RA系统地将引人注目的内容变得严谨--在每次迭代期间，“插入”一个解算器，例如，L-BFGS线搜索或Newton-CG，并解决。

    arXiv:2103.04392v3 Announce Type: replace-cross  Abstract: Stochastic Gradient (SG) is the defacto iterative technique to solve stochastic optimization (SO) problems with a smooth (non-convex) objective $f$ and a stochastic first-order oracle. SG's attractiveness is due in part to its simplicity of executing a single step along the negative subsampled gradient direction to update the incumbent iterate. In this paper, we question SG's choice of executing a single step as opposed to multiple steps between subsample updates. Our investigation leads naturally to generalizing SG into Retrospective Approximation (RA) where, during each iteration, a "deterministic solver" executes possibly multiple steps on a subsampled deterministic problem and stops when further solving is deemed unnecessary from the standpoint of statistical efficiency. RA thus rigorizes what is appealing for implementation -- during each iteration, "plug in" a solver, e.g., L-BFGS line search or Newton-CG, as is, and solv
    
[^31]: 关于带缺失值的监督学习的一致性

    On the consistency of supervised learning with missing values

    [https://arxiv.org/abs/1902.06931](https://arxiv.org/abs/1902.06931)

    两种方法在带缺失值的监督学习中表现出一致性，当缺失值不具信息性时，使用常数进行插补是一种简单且重要的实践方法。

    

    在许多应用设置中，数据存在缺失值，这使得分析变得具有挑战性。丰富的文献涉及缺失值在推断框架中的处理：从不完整的表中估计参数及其方差。在这里，我们考虑监督学习设置：在训练和测试数据中出现缺失值时预测目标。我们表明了两种方法在预测中的一致性。一个引人注目的结果是，当缺失值不具信息性时，使用常数进行插补，例如在学习之前使用均值，是一致的。这与推断设置形成鲜明对比，推断设置中常用的均值插补方法被指责扭曲数据的分布。这样一个简单的方法在实践中能够保持一致性是很重要的。我们还展示了适用于完整观测的预测器可以通过多重插补在不完整数据上进行最佳预测。最后，为了比较插补

    arXiv:1902.06931v4 Announce Type: replace-cross  Abstract: In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imput
    
[^32]: 通过Hoeffding分解的推广，理解有关依赖输入的黑箱模型

    Understanding black-box models with dependent inputs through a generalization of Hoeffding's decomposition. (arXiv:2310.06567v1 [math.FA])

    [http://arxiv.org/abs/2310.06567](http://arxiv.org/abs/2310.06567)

    通过提出一个新的框架，我们可以解释有关依赖输入的黑箱模型。我们证明了在一些合理的假设下，非线性函数可以唯一分解为每个可能子集的函数之和。这个框架有效地推广了Hoeffding分解，并提供了新颖的可解释性指标。

    

    解释黑箱模型的主要挑战之一是能够将非互不相关随机输入的平方可积函数唯一分解为每个可能子集的函数之和。然而，处理输入之间的依赖关系可能很复杂。我们提出了一个新的框架来研究这个问题，将三个数学领域联系起来：概率论、函数分析和组合数学。我们表明，在输入上的两个合理假设下（非完美的函数依赖性和非退化的随机依赖性），总是可以唯一分解这样一个函数。这种“规范分解”相对直观，揭示了非线性相关输入的非线性函数的线性特性。在这个框架中，我们有效地推广了众所周知的Hoeffding分解，可以看作是一个特殊情况。黑箱模型的斜投影为新颖的可解释性指标提供了可能。

    One of the main challenges for interpreting black-box models is the ability to uniquely decompose square-integrable functions of non-mutually independent random inputs into a sum of functions of every possible subset of variables. However, dealing with dependencies among inputs can be complicated. We propose a novel framework to study this problem, linking three domains of mathematics: probability theory, functional analysis, and combinatorics. We show that, under two reasonable assumptions on the inputs (non-perfect functional dependence and non-degenerate stochastic dependence), it is always possible to decompose uniquely such a function. This ``canonical decomposition'' is relatively intuitive and unveils the linear nature of non-linear functions of non-linearly dependent inputs. In this framework, we effectively generalize the well-known Hoeffding decomposition, which can be seen as a particular case. Oblique projections of the black-box model allow for novel interpretability indic
    
[^33]: 无监督语言模型蒸馏的事实验证

    Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])

    [http://arxiv.org/abs/2309.16540](http://arxiv.org/abs/2309.16540)

    本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。

    

    无监督事实验证旨在通过可靠知识库中的证据来验证主张，而无需任何形式的数据注释。为了解决这个挑战，算法必须为每个主张生成既语义明确又紧凑的特征，以便与源信息进行语义对齐。与之前的工作不同，前者通过学习包含主张及其相应标签的注释语料库来解决对齐问题。我们提出了SFAVEL（通过语言模型蒸馏的自监督事实验证），这是一个新颖的无监督框架，利用预训练的语言模型将自监督特征蒸馏为高质量的主张-事实对齐，而无需注释。这是通过一种新颖的对比损失函数实现的，该函数鼓励特征在保持语料库间的语义关系的同时实现高质量的主张和证据对齐。值得注意的是，我们展示了达到新颖的状态一.

    Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
    
[^34]: 基于相关性的模糊聚类有效性指标与次要选项检测器

    A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])

    [http://arxiv.org/abs/2308.14785](http://arxiv.org/abs/2308.14785)

    本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。

    

    应用聚类分析时，最佳聚类数量是主要关注点之一。已经引入了多个聚类有效性指标来解决这个问题。然而，在某些情况下，有多个选项可以作为最终的聚类数量。大多数现有工作在这个领域忽视了这一方面。在本研究中，我们引入了一种基于相关性的模糊聚类有效性指标，称为Wiroonsri-Preedasawakul（WP）指标。该指标根据一对数据点的实际距离与相应对的调整质心之间的距离之间的相关性来定义。我们评估并比较了我们的指标与Xie-Beni，Pakhira-Bandyopadhyay-Maulik，Tang，Wu-Li，广义C和Kwon2等几个现有指标的性能。我们在四种类型的数据集上进行了评估：人工数据集，现实世界数据集，带有等级的模拟数据集和图像数据集，使用模糊c-mea算法。

    The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
    
[^35]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^36]: 面向扩散式生成模型的非渐进快速收敛方法

    Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models. (arXiv:2306.09251v1 [stat.ML])

    [http://arxiv.org/abs/2306.09251](http://arxiv.org/abs/2306.09251)

    该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。

    

    扩散模型通过学习反转马尔可夫扩散过程将噪音转化为新数据实例，在当代生成建模领域中已成为基石。虽然它们的实用性现在已被广泛认可，但其理论基础仍然不够成熟。在这项工作中，我们开发了一套非渐进理论，以理解离散时间下扩散模型的数据生成过程，假设可以获得（Stein）得分函数的可靠估计。针对一种流行的确定性采样器（基于概率流ODE），我们建立了一个与 $T$（总步数）成比例的收敛速度，改进了过去的结果；对于另一种主流的随机采样器（即一种去噪扩散概率模型（DDPM）），我们导出了一个与 $1/\sqrt{T}$ 成比例的收敛速度，与最先进的理论相匹配。我们的理论对目标数据分布只作出最小的假设（例如，没有平滑）。

    Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoot
    
[^37]: 用函数逼近解决强化学习中重尾奖励问题的极小最大化算法和实例相关遗憾度量

    Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])

    [http://arxiv.org/abs/2306.06836](http://arxiv.org/abs/2306.06836)

    本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。

    

    虽然有许多工作都专注于为有界奖励的强化学习设计有效算法，但当奖励呈现“重尾”分布时——即存在某个 $\epsilon\in(0,1]$ 使得仅有有限的$(1+\epsilon)$-阶矩——是否存在对大状态-动作空间进行采样或时效性算法仍然是一个未解决的问题。 在本文中，我们解决了具有线性函数逼近的 RL 中的这种奖励机制的挑战。我们首先为重尾线性赌臂设计了一种算法——\textsc{Heavy-OFUL}，其实现了一种实例相关的 $T$-round 遗憾度量，为 $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$，这是这种类型的\emph{第一篇}文章。$\nu_t^{1+\epsilon}$是第 $t$ 轮奖励的 $(1+\epsilon)$-阶中心矩。我们进一步证明了在应用于 st 的最坏情况时，上述界是极小值的最优解。

    While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
    
[^38]: 约束域的扩散模型

    Diffusion Models for Constrained Domains. (arXiv:2304.05364v1 [cs.LG])

    [http://arxiv.org/abs/2304.05364](http://arxiv.org/abs/2304.05364)

    本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。

    

    降噪扩散模型是新近涌现的一种生成模型，它在无条件图像生成和语音生成等众多领域实现了最先进的成果。它们由破坏数据的加噪过程和定义为加噪扩散的时间反演的后向阶段组成。以这些成功为基础，扩散模型最近扩展到了黎曼流形设置。然而，这些黎曼扩散模型要求在所有时间上定义测地线。虽然该设置包括许多重要应用，但不包括由不等式约束集定义的流形，这在许多科学领域，如机器人和蛋白设计中是普遍存在的。在本文中，我们介绍了两种方法来弥合这个差距。首先，我们设计了一个基于不等式约束诱导的对数障碍度量的加噪过程。其次，我们介绍了一种基于反射布朗运动的加噪过程。由于现有的扩散模型不能直接应用于约束域，因此本文提出了两种方法来创建约束域的降噪扩散模型。

    Denoising diffusion models are a recent class of generative models which achieve state-of-the-art results in many domains such as unconditional image generation and text-to-speech tasks. They consist of a noising process destroying the data and a backward stage defined as the time-reversal of the noising diffusion. Building on their success, diffusion models have recently been extended to the Riemannian manifold setting. Yet, these Riemannian diffusion models require geodesics to be defined for all times. While this setting encompasses many important applications, it does not include manifolds defined via a set of inequality constraints, which are ubiquitous in many scientific domains such as robotics and protein design. In this work, we introduce two methods to bridge this gap. First, we design a noising process based on the logarithmic barrier metric induced by the inequality constraints. Second, we introduce a noising process based on the reflected Brownian motion. As existing diffu
    
[^39]: 分布偏移的标签对齐正则化

    Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14960](http://arxiv.org/abs/2211.14960)

    这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。

    

    最近的研究强调了监督学习中的标签对齐属性（LAP），即数据集中所有标签的向量大部分在数据矩阵的前几个奇异向量的张成空间内。受到这一观察的启发，我们提出了一种无监督领域自适应的正则化方法，鼓励目标域中的预测与其前几个奇异向量对齐。与传统的领域适应方法专注于正则化表示不同，我们相反，通过在源域和目标域中使用LAP，用正则化分类器与无监督目标数据对齐。理论分析表明，在一定的假设下，我们的解决方案位于目标域数据的前几个右奇异向量的张成空间内，并与最优解对齐。通过消除经典领域适应理论中常见的最优联合风险假设的依赖，我们展示了该方法的有效性。

    Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
    

