# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Lower Complexity Adaptation for Empirical Entropic Optimal Transport.](http://arxiv.org/abs/2306.13580) | 本文研究了经验熵正则化最优输运的统计表现，并证明了它遵循低复杂度适应原则，推导出了其统计界限及参数化速率。 |
| [^2] | [Efficient Model Selection for Predictive Pattern Mining Model by Safe Pattern Pruning.](http://arxiv.org/abs/2306.13561) | 本文提出了“安全模式修剪”方法解决预测模式挖掘中模式数量增长的问题，并展示了其在结构化数据的回归和分类问题中的有效性。 |
| [^3] | [On the Convergence Rate of Gaussianization with Random Rotations.](http://arxiv.org/abs/2306.13520) | 该论文通过理论推导和实验验证，发现高斯化模型的收敛速度对于维数增加呈现出线性关系，原因是模型无法捕捉维之间的相关性。 |
| [^4] | [Two derivations of Principal Component Analysis on datasets of distributions.](http://arxiv.org/abs/2306.13503) | 本篇论文提出了一个新的方法来进行基于分布的主成分分析，通过方差最大化原理和重构误差最小化两种方法推导得闭合解。 |
| [^5] | [Prediction under Latent Subgroup Shifts with High-Dimensional Observations.](http://arxiv.org/abs/2306.13472) | 本研究运用RPM在图像观测数据中识别低维离散隐变量，并且在潜在变量分布不同的情况下适当地预测目标结果。 |
| [^6] | [Understanding quantum machine learning also requires rethinking generalization.](http://arxiv.org/abs/2306.13461) | 本文通过实验认为，传统方法无法解释量子机器学习模型在只使用少量数据训练的情况下表现出成功的泛化性能，该模型可以准确拟合随机状态及随机标记的训练数据，这种记忆随机数据的能力违反了当前小泛化误差的概念，我们通过理论构建补充实证结果，表明量子神经网络可将任意标记拟合到量子状态上，暗示了它们的记忆能力，这些结果排除了单单基于经典复杂度度量的所有可能保证。 |
| [^7] | [On tracking varying bounds when forecasting bounded time series.](http://arxiv.org/abs/2306.13428) | 该论文提出了一种跟踪时间变化边界的新方法，并解决了在线最大似然估计的问题。这种方法有助于处理预测有边界时间序列问题。 |
| [^8] | [Variational Counterfactual Prediction under Runtime Domain Corruption.](http://arxiv.org/abs/2306.13271) | 本论文提出了一种名为 V-CPDC 的新方法，在运行时域失真的情况下，将因果预测子纳入域自适应的概念中，以使其能够在源域和目标域中具有较好的泛化能力。 |
| [^9] | [Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models.](http://arxiv.org/abs/2306.13255) | 本文研究了高斯协变量下的过参数化线性模型在多类分类问题中的泛化能力，成功解决了之前的猜想，并提出的新下界具有信息论中的强对偶定理的性质。 |
| [^10] | [Approximate Causal Effect Identification under Weak Confounding.](http://arxiv.org/abs/2306.13242) | 本文提出了一种有效的方法来在弱混淆下识别因果效应的上限和下限，并证明了这种方法的计算效率优于最先进的多项式程序。 |
| [^11] | [Differentially Private Synthetic Data Using KD-Trees.](http://arxiv.org/abs/2306.13211) | 本文提出了一种使用KD-树的算法，用于生成 $\epsilon $-差分隐私的合成数据，其核密度类似于真实数据集的核密度。该方法克服了维度灾难，是一种可扩展的算法。 |
| [^12] | [Uniform Convergence with Square-Root Lipschitz Loss.](http://arxiv.org/abs/2306.13188) | 该论文通过平方根Lipschitz损失的一致收敛性，对一般的高斯数据建立了保证，允许处理广泛的损失类别，并重新推导和更好地理解“乐观率”的学习保证和插值。 |
| [^13] | [An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression.](http://arxiv.org/abs/2306.13185) | 本文研究了核岭回归中过拟合成本，采用“不可知”的观点，以分析样本量和任务特征结构对成本的影响。通过分析提供了更细致的过度拟合表征。 |
| [^14] | [Adversarial Resilience in Sequential Prediction via Abstention.](http://arxiv.org/abs/2306.13119) | 本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。 |
| [^15] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^16] | [COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks.](http://arxiv.org/abs/2305.06754) | COCKATIEL是一种连续概念排名带归因性解释的技术，基于概念，用于从NLP分类任务的神经网络模型的最后一层中生成有意义的解释，且不会影响准确性或需要新模型，已证明比现有方法产生更有信息量和可靠的解释。 |
| [^17] | [STEEL: Singularity-aware Reinforcement Learning.](http://arxiv.org/abs/2301.13152) | 这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。 |
| [^18] | [Curvature Filtrations for Graph Generative Model Evaluation.](http://arxiv.org/abs/2301.12906) | 该论文使用图形曲率描述符和拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。 |
| [^19] | [Best Arm Identification in Stochastic Bandits: Beyond $\beta-$optimality.](http://arxiv.org/abs/2301.03785) | 本文介绍了一种新的算法来实现随机多臂赌博机中最佳臂识别，它既具有最优性能又计算上高效。 |
| [^20] | [SPRT-based Efficient Best Arm Identification in Stochastic Bandits.](http://arxiv.org/abs/2207.11158) | 本论文提出了一种使用SPRT框架设计的BAI算法并运用于指数族赌博机，该算法具有样本复杂度渐近最优和$\delta-$PAC保证的特点。 |
| [^21] | [Semi-Autoregressive Energy Flows: Exploring Likelihood-Free Training of Normalizing Flows.](http://arxiv.org/abs/2206.06672) | 本文研究了正则化流的无似然训练，提出了能量目标，支持半自回归能量流等灵活的模型架构，并相对于基于似然性的流表现出有竞争力的性能，质疑了机器学习领域中以最大似然作为目标或指标的使用，为生成建模的研究做出了贡献。 |
| [^22] | [Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data.](http://arxiv.org/abs/2203.15797) | 本文提出了一种可用于受限非凸优化中相关数据采样的一阶方法，并通过采用更加温和的混合条件，将复杂度从$\tilde{O}(\varepsilon^{-8})$提升至$\tilde{O}(\varepsilon^{-4})$。 |

# 详细

[^1]: 经验熵正则化最优输运的低复杂度适应性

    Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])

    [http://arxiv.org/abs/2306.13580](http://arxiv.org/abs/2306.13580)

    本文研究了经验熵正则化最优输运的统计表现，并证明了它遵循低复杂度适应原则，推导出了其统计界限及参数化速率。

    

    经验熵正则化最优输运 (EOT) 是优化输运 (OT) 的一种有效且计算可行的替代方案，对大规模数据分析有着广泛的应用。本文推导出了 EOT 成本的新的统计界限，并显示它们在熵正则化参数 $\epsilon$ 和样本大小 $n$ 的统计性能仅取决于两个概率测度之中较简单的那个。例如，在充分平滑的成本下，这会产生具有$\epsilon^{-d/2}$因子的参数化速率$n^{-1/2}$，其中$d$是两个总体测度的最小维度。这确认了经验EOT也遵循了最近才为未规则化OT确认的低复杂度适应原则的标志性特征。根据我们的理论，我们展示了欧几里得空间上的测度的经验熵Gromov-Wasserstein距离及其未规则化版本也遵循此原则。

    Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip
    
[^2]: 安全模式修剪下的预测模式挖掘模型高效选取

    Efficient Model Selection for Predictive Pattern Mining Model by Safe Pattern Pruning. (arXiv:2306.13561v1 [stat.ML])

    [http://arxiv.org/abs/2306.13561](http://arxiv.org/abs/2306.13561)

    本文提出了“安全模式修剪”方法解决预测模式挖掘中模式数量增长的问题，并展示了其在结构化数据的回归和分类问题中的有效性。

    

    预测模式挖掘是一种用于构建预测模型的方法，当输入被表示为结构化数据（例如集合、图和序列）时，该方法适用。预测模式挖掘的主要思想是通过考虑结构化数据中的子结构（例如子集、子图和子序列，称为模式）作为模型的特征来构建预测模型。预测模式挖掘面临的主要挑战在于随着结构化数据的复杂性增加，模式数量呈指数级增长。在本研究中，我们提出了安全模式修剪（SPP）方法来解决预测模式挖掘中模式数爆炸的问题。我们还讨论了如何在实际数据分析的整个模型构建过程中有效地应用它。为了证明所提出的方法的有效性，我们在涉及集合、图和序列的回归和分类问题上进行了数值实验。

    Predictive pattern mining is an approach used to construct prediction models when the input is represented by structured data, such as sets, graphs, and sequences. The main idea behind predictive pattern mining is to build a prediction model by considering substructures, such as subsets, subgraphs, and subsequences (referred to as patterns), present in the structured data as features of the model. The primary challenge in predictive pattern mining lies in the exponential growth of the number of patterns with the complexity of the structured data. In this study, we propose the Safe Pattern Pruning (SPP) method to address the explosion of pattern numbers in predictive pattern mining. We also discuss how it can be effectively employed throughout the entire model building process in practical data analysis. To demonstrate the effectiveness of the proposed method, we conduct numerical experiments on regression and classification problems involving sets, graphs, and sequences.
    
[^3]: 关于随机旋转高斯化的收敛速度。

    On the Convergence Rate of Gaussianization with Random Rotations. (arXiv:2306.13520v1 [cs.LG])

    [http://arxiv.org/abs/2306.13520](http://arxiv.org/abs/2306.13520)

    该论文通过理论推导和实验验证，发现高斯化模型的收敛速度对于维数增加呈现出线性关系，原因是模型无法捕捉维之间的相关性。

    

    高斯化是一种简单的生成模型，可以在没有反向传播的情况下进行训练。它在低维数据上表现出了强大的性能。然而，随着维度的增加，观察到其收敛速度变慢。我们从理论上证明，对于高斯输入，所需的层数与维数成线性关系。我们认为这是因为该模型无法捕捉维之间的相关性。

    Gaussianization is a simple generative model that can be trained without backpropagation. It has shown compelling performance on low dimensional data. As the dimension increases, however, it has been observed that the convergence speed slows down. We show analytically that the number of required layers scales linearly with the dimension for Gaussian input. We argue that this is because the model is unable to capture dependencies between dimensions. Empirically, we find the same linear increase in cost for arbitrary input $p(x)$, but observe favorable scaling for some distributions. We explore potential speed-ups and formulate challenges for further research.
    
[^4]: 基于分布数据集的主成分分析的两种推导方法

    Two derivations of Principal Component Analysis on datasets of distributions. (arXiv:2306.13503v1 [stat.ML])

    [http://arxiv.org/abs/2306.13503](http://arxiv.org/abs/2306.13503)

    本篇论文提出了一个新的方法来进行基于分布的主成分分析，通过方差最大化原理和重构误差最小化两种方法推导得闭合解。

    

    在本短文中，我们将主成分分析（PCA）应用于由其位置和协方差刻画的分布数据集，而非点数据集。与点数据集上的常规PCA可以等效地通过方差最大化原理和重构error最小化两种方法推导出一样，我们从这两个方面推导出了分布PCA的闭合解。

    In this brief note, we formulate Principal Component Analysis (PCA) over datasets consisting not of points but of distributions, characterized by their location and covariance. Just like the usual PCA on points can be equivalently derived via a variance-maximization principle and via a minimization of reconstruction error, we derive a closed-form solution for distributional PCA from both of these perspectives.
    
[^5]: 基于高维观测数据的潜在子群转换下的预测研究

    Prediction under Latent Subgroup Shifts with High-Dimensional Observations. (arXiv:2306.13472v1 [stat.ML])

    [http://arxiv.org/abs/2306.13472](http://arxiv.org/abs/2306.13472)

    本研究运用RPM在图像观测数据中识别低维离散隐变量，并且在潜在变量分布不同的情况下适当地预测目标结果。

    

    本研究提出了一种适用于图形模型中的预测方法，能够适应潜在变量不同分布下的转换，即源环境和目标环境中的潜在变量分布不同。本研究运用识别参数模型（RPM）在图像观测数据中识别低维离散隐变量，并且在具体问题中适当地预测目标结果。

    We introduce a new approach to prediction in graphical models with latent-shift adaptation, i.e., where source and target environments differ in the distribution of an unobserved confounding latent variable. Previous work has shown that as long as "concept" and "proxy" variables with appropriate dependence are observed in the source environment, the latent-associated distributional changes can be identified, and target predictions adapted accurately. However, practical estimation methods do not scale well when the observations are complex and high-dimensional, even if the confounding latent is categorical. Here we build upon a recently proposed probabilistic unsupervised learning framework, the recognition-parametrised model (RPM), to recover low-dimensional, discrete latents from image observations. Applied to the problem of latent shifts, our novel form of RPM identifies causal latent structure in the source environment, and adapts properly to predict in the target. We demonstrate re
    
[^6]: 理解量子机器学习需要重新思考泛化问题

    Understanding quantum machine learning also requires rethinking generalization. (arXiv:2306.13461v1 [quant-ph])

    [http://arxiv.org/abs/2306.13461](http://arxiv.org/abs/2306.13461)

    本文通过实验认为，传统方法无法解释量子机器学习模型在只使用少量数据训练的情况下表现出成功的泛化性能，该模型可以准确拟合随机状态及随机标记的训练数据，这种记忆随机数据的能力违反了当前小泛化误差的概念，我们通过理论构建补充实证结果，表明量子神经网络可将任意标记拟合到量子状态上，暗示了它们的记忆能力，这些结果排除了单单基于经典复杂度度量的所有可能保证。

    

    量子机器学习模型在只用少量数据训练的情况下也能表现出成功的泛化性能。本文通过系统的随机化实验，展示传统的理解泛化的方法无法解释这些量子模型的行为。我们的实验揭示了最先进的量子神经网络能够准确地拟合随机状态和随机训练数据的标记。这种记忆随机数据的能力违反了当前小泛化误差的概念，使得建立在VC维、Rademacher复杂度和所有均匀相关性度量基础上的方法有些棘手。我们还通过理论构建补充了我们的实证结果，表明量子神经网络能够将任意标记拟合到量子状态上，暗示了它们的记忆能力。我们的结果并不排除只用少量训练数据就能获得良好泛化的可能性，但是排除了单单基于经典复杂度度量的所有可能保证。

    Quantum machine learning models have shown successful generalization performance even when trained with few data. In this work, through systematic randomization experiments, we show that traditional approaches to understanding generalization fail to explain the behavior of such quantum models. Our experiments reveal that state-of-the-art quantum neural networks accurately fit random states and random labeling of training data. This ability to memorize random data defies current notions of small generalization error, problematizing approaches that build on complexity measures such as the VC dimension, the Rademacher complexity, and all their uniform relatives. We complement our empirical results with a theoretical construction showing that quantum neural networks can fit arbitrary labels to quantum states, hinting at their memorization ability. Our results do not preclude the possibility of good generalization with few training data but rather rule out any possible guarantees based only
    
[^7]: 关于跟踪预测有边界时间序列时的可变边界问题

    On tracking varying bounds when forecasting bounded time series. (arXiv:2306.13428v1 [stat.ML])

    [http://arxiv.org/abs/2306.13428](http://arxiv.org/abs/2306.13428)

    该论文提出了一种跟踪时间变化边界的新方法，并解决了在线最大似然估计的问题。这种方法有助于处理预测有边界时间序列问题。

    

    我们考虑一个新的框架，其中连续但有界的随机变量具有随时间变化的未观测边界。在单变量时间序列的背景下，我们将这些边界视为有界随机变量分布的参数。我们引入了扩展的对数似然估计，并设计了算法来通过在线最大似然估计来跟踪边界。由于得到的优化问题不是凸的，我们利用了最近关于准凸优化的归一化梯度下降（NGD）的理论结果，最终得到了一种在线归一化梯度下降算法。我们基于模拟研究和一个真实的风力发电预测问题来说明和讨论我们方法的工作原理。

    We consider a new framework where a continuous, though bounded, random variable has unobserved bounds that vary over time. In the context of univariate time series, we look at the bounds as parameters of the distribution of the bounded random variable. We introduce an extended log-likelihood estimation and design algorithms to track the bound through online maximum likelihood estimation. Since the resulting optimization problem is not convex, we make use of recent theoretical results on Normalized Gradient Descent (NGD) for quasiconvex optimization, to eventually derive an Online Normalized Gradient Descent algorithm. We illustrate and discuss the workings of our approach based on both simulation studies and a real-world wind power forecasting problem.
    
[^8]: 运行时域失真下的变分反事实预测

    Variational Counterfactual Prediction under Runtime Domain Corruption. (arXiv:2306.13271v1 [cs.LG])

    [http://arxiv.org/abs/2306.13271](http://arxiv.org/abs/2306.13271)

    本论文提出了一种名为 V-CPDC 的新方法，在运行时域失真的情况下，将因果预测子纳入域自适应的概念中，以使其能够在源域和目标域中具有较好的泛化能力。

    

    目前，已经提出了各种基于观察数据的因果效应估计的神经方法，其中一个默认假设是在训练和推理（即运行时）阶段变量的相同分布和可用性。然而，在运行时可能会发生分布偏移（即域偏移），而变量难以访问带来的挑战更大。这通常是由于不断增加的隐私和伦理问题造成的，这可能使得整个运行时数据中的任意变量不可用且无法填补。我们称域转移和不可访问变量的共同发生为运行时域失真，这严重影响了训练后反事实预测器的泛化能力。为了对抗运行时域失真，我们将反事实预测子纳入域自适应的概念。具体而言，我们通过源域误差和域间差异之和来上限目标域（即运行时协变量）的误差。我们将反事实预测表述为一个函数空间搜索问题。为此，我们提出了一种称为带有域失真的变分反事实预测子的新方法（V-CPDC），该方法直接优化上界，并旨在在源域和目标域中都具有良好的泛化能力。对四个实际数据集的广泛实验表明了我们方法的有效性，特别是在各种运行时域失真情况下。

    To date, various neural methods have been proposed for causal effect estimation based on observational data, where a default assumption is the same distribution and availability of variables at both training and inference (i.e., runtime) stages. However, distribution shift (i.e., domain shift) could happen during runtime, and bigger challenges arise from the impaired accessibility of variables. This is commonly caused by increasing privacy and ethical concerns, which can make arbitrary variables unavailable in the entire runtime data and imputation impractical. We term the co-occurrence of domain shift and inaccessible variables runtime domain corruption, which seriously impairs the generalizability of a trained counterfactual predictor. To counter runtime domain corruption, we subsume counterfactual prediction under the notion of domain adaptation. Specifically, we upper-bound the error w.r.t. the target domain (i.e., runtime covariates) by the sum of source domain error and inter-dom
    
[^9]: 过参数化线性模型下多类分类的渐进泛化精度研究

    Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v1 [cs.LG])

    [http://arxiv.org/abs/2306.13255](http://arxiv.org/abs/2306.13255)

    本文研究了高斯协变量下的过参数化线性模型在多类分类问题中的泛化能力，成功解决了之前的猜想，并提出的新下界具有信息论中的强对偶定理的性质。

    

    本文研究了在具有高斯协变量双层模型下，过参数化线性模型在多类分类中的渐进泛化问题，其中数据点数、特征和类别数都同时增长。我们完全解决了Subramanian等人在'22年所提出的猜想，与预测的泛化区间相匹配。此外，我们的新的下界类似于信息论中的强对偶定理：它们能够确立误分类率逐渐趋近于0或1.我们紧密的结果的一个令人惊讶的结果是，最小范数插值分类器在最小范数插值回归器最优的范围内，可以在渐进上次优。我们分析的关键在于一种新的Hanson-Wright不等式变体，该变体在具有稀疏标签的多类问题中具有广泛的适用性。作为应用，我们展示了相同类型分析在几种不同类型的分类模型上的结果。

    We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al.~'22, where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al.~'22, matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.  The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis 
    
[^10]: 弱混淆下的近似因果效应识别

    Approximate Causal Effect Identification under Weak Confounding. (arXiv:2306.13242v1 [stat.ML])

    [http://arxiv.org/abs/2306.13242](http://arxiv.org/abs/2306.13242)

    本文提出了一种有效的方法来在弱混淆下识别因果效应的上限和下限，并证明了这种方法的计算效率优于最先进的多项式程序。

    

    在只有观测数据可用时，许多研究人员研究了因果效应估计问题。针对可识别因果查询的点估计，已经开发出了正确完备的算法。对于不可识别的因果查询，研究人员开发了多项式程序，以估计因果效应的紧密界限。但对于支持大小较大的变量，优化这些多项式程序在计算上很困难。在本文中，我们分析了“弱混淆”对因果估计的影响。更具体地说，在未观测到的混淆变量的熵很小的假设下，我们提出了一种有效的线性规划方法来导出因果效应的上限和下限。我们证明了我们的界限是一致的，也就是说，当未观测混淆变量的熵趋近于零时，上限和下限之间的差异会消失。最后，我们进行了合成和真实数据模拟，以比较我们的方法与最先进的多项式程序得到的界限，并证明我们的方法在计算上更加高效，性能也可以达到类似的水平。

    Causal effect estimation has been studied by many researchers when only observational data is available. Sound and complete algorithms have been developed for pointwise estimation of identifiable causal queries. For non-identifiable causal queries, researchers developed polynomial programs to estimate tight bounds on causal effect. However, these are computationally difficult to optimize for variables with large support sizes. In this paper, we analyze the effect of "weak confounding" on causal estimands. More specifically, under the assumption that the unobserved confounders that render a query non-identifiable have small entropy, we propose an efficient linear program to derive the upper and lower bounds of the causal effect. We show that our bounds are consistent in the sense that as the entropy of unobserved confounders goes to zero, the gap between the upper and lower bound vanishes. Finally, we conduct synthetic and real data simulations to compare our bounds with the bounds obta
    
[^11]: 使用KD-树的差分隐私合成数据

    Differentially Private Synthetic Data Using KD-Trees. (arXiv:2306.13211v1 [cs.CR])

    [http://arxiv.org/abs/2306.13211](http://arxiv.org/abs/2306.13211)

    本文提出了一种使用KD-树的算法，用于生成 $\epsilon $-差分隐私的合成数据，其核密度类似于真实数据集的核密度。该方法克服了维度灾难，是一种可扩展的算法。

    

    创建一个忠实地代表数据分布并同时保护隐私的合成数据集是一项重要的研究挑战。近年来出现了许多基于空间分割的方法，用于以差分隐私的方式回答统计查询问题。然而，对于合成数据生成问题，最近的研究主要集中在深度生成模型上。相比之下，我们利用了空间分割技术和噪声扰动，从而实现了直观透明的算法。我们提出了数据独立和数据相关的算法，用于生成 $\epsilon $-差分隐私的合成数据，其核密度类似于真实数据集的核密度。此外，我们还提供了有关实用性和隐私权利的理论结果，并展示了我们的数据相关方法如何克服维度灾难并导致可扩展的算法。我们展示了相对于之前的工作的实用改进，并讨论了性能

    Creation of a synthetic dataset that faithfully represents the data distribution and simultaneously preserves privacy is a major research challenge. Many space partitioning based approaches have emerged in recent years for answering statistical queries in a differentially private manner. However, for synthetic data generation problem, recent research has been mainly focused on deep generative models. In contrast, we exploit space partitioning techniques together with noise perturbation and thus achieve intuitive and transparent algorithms. We propose both data independent and data dependent algorithms for $\epsilon$-differentially private synthetic data generation whose kernel density resembles that of the real dataset. Additionally, we provide theoretical results on the utility-privacy trade-offs and show how our data dependent approach overcomes the curse of dimensionality and leads to a scalable algorithm. We show empirical utility improvements over the prior work, and discuss perfo
    
[^12]: 平方根Lipschitz损失的一致收敛性

    Uniform Convergence with Square-Root Lipschitz Loss. (arXiv:2306.13188v1 [stat.ML])

    [http://arxiv.org/abs/2306.13188](http://arxiv.org/abs/2306.13188)

    该论文通过平方根Lipschitz损失的一致收敛性，对一般的高斯数据建立了保证，允许处理广泛的损失类别，并重新推导和更好地理解“乐观率”的学习保证和插值。

    

    我们通过假设类的Rademacher复杂度和标量损失函数的平方根的Lipschitz常数，在高斯数据方面建立了一般的一致收敛性保证。我们展示了这些保证如何大大概括了基于平滑性(导数的Lipschitz常数)的先前结果，并使我们能够处理更广泛的平方根Lipschitz损失类别，其中包括适用于研究相位恢复和ReLU回归的非平滑损失函数，以及重新推导和更好地理解“乐观率”的学习保证和插值。

    We establish generic uniform convergence guarantees for Gaussian data in terms of the Rademacher complexity of the hypothesis class and the Lipschitz constant of the square root of the scalar loss function. We show how these guarantees substantially generalize previous results based on smoothness (Lipschitz constant of the derivative), and allow us to handle the broader class of square-root-Lipschitz losses, which includes also non-smooth loss functions appropriate for studying phase retrieval and ReLU regression, as well as rederive and better understand "optimistic rate" and interpolation learning guarantees.
    
[^13]: (核) 岭回归中过度拟合成本的不可知观察

    An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression. (arXiv:2306.13185v1 [stat.ML])

    [http://arxiv.org/abs/2306.13185](http://arxiv.org/abs/2306.13185)

    本文研究了核岭回归中过拟合成本，采用“不可知”的观点，以分析样本量和任务特征结构对成本的影响。通过分析提供了更细致的过度拟合表征。

    

    本研究研究了有噪声的核岭回归 (KRR) 中过拟合的成本，我们将其定义为插值无岭模型的测试误差与最优调节模型的测试误差之比。我们采用“不可知”的观点，即对于任何目标函数，即使样本量不足以达到一致性或目标函数不在 RKHS 中，我们也将成本看作样本量的函数。使用最近推导出的（非严格的）风险评估，以任务特征结构为基础，利用高斯普适性假设分析过度拟合成本。我们的分析提供了良性、缓和和灾难性过度拟合（参见 Mallinar 等人 2022）的更精细的表征。

    We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an "agnostic" view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).
    
[^14]: 通过弃权实现顺序预测中的对抗鲁棒性

    Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])

    [http://arxiv.org/abs/2306.13119](http://arxiv.org/abs/2306.13119)

    本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。

    

    本文研究了在带有允许注入干净标签对抗性（或超出分布）示例的对抗者的情况下，在随机设置下的顺序预测问题。针对纯随机数据的算法在存在此类对抗性示例的情况下往往失败，从而导致错误的预测。这在许多高风险应用中是不可取的，例如医学建议，这里弃权不进行对抗性示例的预测优于误分类。另一方面，假设完全对抗性数据导致非常悲观的界限，在实践中往往是空洞的。为了实现这一目标，我们提出了一种新的顺序预测模型，它位于纯随机和完全对抗性设置之间，通过允许学习器在对抗样例上无代价地放弃进行预测来实现。假设访问非对抗样例的边际分布，我们设计了一个学习器，其误差随着VC维的变化而变化。

    We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC 
    
[^15]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^16]: COCKATIEL:用可解释元素对NLP任务中的神经网络分类器进行连续概念排名带归因性解释

    COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])

    [http://arxiv.org/abs/2305.06754](http://arxiv.org/abs/2305.06754)

    COCKATIEL是一种连续概念排名带归因性解释的技术，基于概念，用于从NLP分类任务的神经网络模型的最后一层中生成有意义的解释，且不会影响准确性或需要新模型，已证明比现有方法产生更有信息量和可靠的解释。

    

    Transformer结构复杂，其在NLP中的使用虽然取得了许多成功，但其可解释性或可解释性较为棘手。最近的争论表明，注意力图和归因方法不可靠，而我们在本文中介绍了其中一些局限性，同时介绍了COCKATIEL这一新型的模型无关的可解释性技术，它是一种后期方法，基于概念，用于从经过NLP分类任务训练的神经网络模型的最后一层中生成有意义的解释，通过使用非负矩阵分解(NMF)来发现模型利用来进行预测的概念，并利用敏感性分析来准确估计每个概念对模型的重要性，而不会影响底层模型的准确性或需要训练新模型。我们在单一和多方面的情感分析中进行实验，证明COCKATIEL比现有方法产生更有信息量和可靠的解释。

    Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that generates meaningful explanations from the last layer of a neural net model trained on an NLP classification task by using Non-Negative Matrix Factorization (NMF) to discover the concepts the model leverages to make predictions and by exploiting a Sensitivity Analysis to estimate accurately the importance of each of these concepts for the model. It does so without compromising the accuracy of the underlying model or requiring a new one to be trained. We conduct experiments in single and multi-aspect sent
    
[^17]: STEEL: 奇异性感知的强化学习

    STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13152](http://arxiv.org/abs/2301.13152)

    这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。

    

    批量强化学习旨在利用预先收集的数据，在动态环境中找到最优策略，以最大化期望总回报。然而，几乎所有现有算法都依赖于目标策略诱导的分布绝对连续假设，以便通过变换测度使用批量数据来校准目标策略。本文提出了一种新的批量强化学习算法，不需要在具有连续状态和行动的无限时马尔可夫决策过程中绝对连续性假设。我们称这个算法为STEEL：SingulariTy-awarE rEinforcement Learning。我们的算法受到关于离线评估的新误差分析的启发，其中我们使用了最大均值偏差，以及带有分布鲁棒优化的策略定向误差评估方法，以确保异常情况下的性能，并提出了一种用于处理奇异情况的定向算法。

    Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
    
[^18]: 图形生成模型评估的曲率滤波器

    Curvature Filtrations for Graph Generative Model Evaluation. (arXiv:2301.12906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12906](http://arxiv.org/abs/2301.12906)

    该论文使用图形曲率描述符和拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。

    

    图形生成模型评估需要了解分布级别上的图形差异，这需要能够以有效的方式利用图形的显著属性。曲率是图形的一种属性，最近开始证明其在描述图形方面很有用。然而，其表达性质、稳定性和在模型评估中的实际效用仍然很少被探索。我们将图形曲率描述符与拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。

    Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property of graphs, and has recently started to prove useful in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.
    
[^19]: 随机赌博机中的最佳臂识别: 超越$\beta-$最优性

    Best Arm Identification in Stochastic Bandits: Beyond $\beta-$optimality. (arXiv:2301.03785v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.03785](http://arxiv.org/abs/2301.03785)

    本文介绍了一种新的算法来实现随机多臂赌博机中最佳臂识别，它既具有最优性能又计算上高效。

    

    本文研究了在固定置信水平下，随机多臂赌博机中最佳臂识别（BAI）的一个未曾解决的方面。评估赌博算法的两个关键指标是计算效率和性能最优性（例如采样复杂度）。在随机BAI文献中，已经有了设计算法以实现最优性能的进展，但它们通常计算上昂贵（例如基于优化的方法）。也存在计算效率高的方法，但它们与最优性能之间存在可证明的差距（例如，前两种方法中的$\beta$-最优方法）。本文介绍了一种BAI框架和算法，该算法通过一组计算上高效的决策规则实现了最优性能。实现这一点的中心流程是一个按顺序估计最佳分配的例程，直到足够准确地估计为止。具体而言，这些估计是准确的。

    This paper investigates a hitherto unaddressed aspect of best arm identification (BAI) in stochastic multi-armed bandits in the fixed-confidence setting. Two key metrics for assessing bandit algorithms are computational efficiency and performance optimality (e.g., in sample complexity). In stochastic BAI literature, there have been advances in designing algorithms to achieve optimal performance, but they are generally computationally expensive to implement (e.g., optimization-based methods). There also exist approaches with high computational efficiency, but they have provable gaps to the optimal performance (e.g., the $\beta$-optimal approaches in top-two methods). This paper introduces a framework and an algorithm for BAI that achieves optimal performance with a computationally efficient set of decision rules. The central process that facilitates this is a routine for sequentially estimating the optimal allocations up to sufficient fidelity. Specifically, these estimates are accurate
    
[^20]: 基于SPRT的随机赌博机中的最佳臂辨识问题

    SPRT-based Efficient Best Arm Identification in Stochastic Bandits. (arXiv:2207.11158v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11158](http://arxiv.org/abs/2207.11158)

    本论文提出了一种使用SPRT框架设计的BAI算法并运用于指数族赌博机，该算法具有样本复杂度渐近最优和$\delta-$PAC保证的特点。

    

    本文研究随机多臂赌博机在固定置信度场景下的最佳臂辨识问题。考虑到广义指数族赌博机的类别。现有的指数族赌博机算法面临计算挑战。为了缓解这些挑战，最佳臂辨识问题被视为序贯复合假设检验任务进行分析，并提出了一种框架，采用基于似然比的测试，这种测试已经证明对于序列测试是有效的。基于这个检验统计量，设计了一种BAI算法，该算法利用了经典的序贯概率比测试进行臂的选择，并且易于分析指数族赌博机。该算法具有两个关键特点：（1）它的样本复杂度是渐近最优的，（2）它保证是$\delta-$PAC的。现有的有效方法集中在高斯条件下，并要求对被认为是最佳臂的臂使用Thompson采样。

    This paper investigates the best arm identification (BAI) problem in stochastic multi-armed bandits in the fixed confidence setting. The general class of the exponential family of bandits is considered. The existing algorithms for the exponential family of bandits face computational challenges. To mitigate these challenges, the BAI problem is viewed and analyzed as a sequential composite hypothesis testing task, and a framework is proposed that adopts the likelihood ratio-based tests known to be effective for sequential testing. Based on this test statistic, a BAI algorithm is designed that leverages the canonical sequential probability ratio tests for arm selection and is amenable to tractable analysis for the exponential family of bandits. This algorithm has two key features: (1) its sample complexity is asymptotically optimal, and (2) it is guaranteed to be $\delta-$PAC. Existing efficient approaches focus on the Gaussian setting and require Thompson sampling for the arm deemed the 
    
[^21]: 半自回归能量流：探索正则化流的无似然训练。

    Semi-Autoregressive Energy Flows: Exploring Likelihood-Free Training of Normalizing Flows. (arXiv:2206.06672v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06672](http://arxiv.org/abs/2206.06672)

    本文研究了正则化流的无似然训练，提出了能量目标，支持半自回归能量流等灵活的模型架构，并相对于基于似然性的流表现出有竞争力的性能，质疑了机器学习领域中以最大似然作为目标或指标的使用，为生成建模的研究做出了贡献。

    

    训练正则化流生成模型时，由于需要计算Jacobian行列式，因此会面临计算负担较重的挑战。本论文研究了流的无似然训练，并提出了能量目标，一种基于适当得分规则的替代基于样本的损失函数。能量目标是不需要行列式的，并支持灵活的模型架构，这些架构不容易与最大似然训练兼容，包括半自回归能量流，一种新颖的模型族，可以插值为全自回归和非自回归模型之间。相对于基于似然性的流，能量流具有竞争性的样本质量、后验推断和生成速度等性能；该性能与对数似然度量的质量通常非常差的质量无关。我们的发现对最大似然作为目标或指标的使用提出了质疑，并有助于对其在生成建模中所起的作用进行科学研究。

    Training normalizing flow generative models can be challenging due to the need to calculate computationally expensive determinants of Jacobians. This paper studies the likelihood-free training of flows and proposes the energy objective, an alternative sample-based loss based on proper scoring rules. The energy objective is determinant-free and supports flexible model architectures that are not easily compatible with maximum likelihood training, including semi-autoregressive energy flows, a novel model family that interpolates between fully autoregressive and non-autoregressive models. Energy flows feature competitive sample quality, posterior inference, and generation speed relative to likelihood-based flows; this performance is decorrelated from the quality of log-likelihood estimates, which are generally very poor. Our findings question the use of maximum likelihood as an objective or a metric, and contribute to a scientific study of its role in generative modeling.
    
[^22]: 受限非凸优化中具有相关数据的一阶方法的收敛性分析

    Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data. (arXiv:2203.15797v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2203.15797](http://arxiv.org/abs/2203.15797)

    本文提出了一种可用于受限非凸优化中相关数据采样的一阶方法，并通过采用更加温和的混合条件，将复杂度从$\tilde{O}(\varepsilon^{-8})$提升至$\tilde{O}(\varepsilon^{-4})$。

    

    本文针对受限光滑非凸优化问题，分析了在一般的相关数据采样方案下的经典随机投影梯度方法。我们证明了利用Moreau包络和梯度映射范数实现$\varepsilon$-近似稳定点的最坏情况收敛速率为$\tilde{O}(t^{-1/4})$，复杂度为$\tilde{O}(\varepsilon^{-4})$。传统的收敛保证需要从目标分布中进行i.i.d.数据采样，而我们只需要对条件分布进行一种较温和的混合条件即可，该条件适用于广泛的马尔可夫链采样算法。相较于现有的受限光滑非凸优化和相关数据的复杂度为$\tilde{O}(\varepsilon^{-8})$的情况，我们提出的方法经过简化分析后复杂度为$\tilde{O}(\varepsilon^{-4})$。最后，我们演示了相关数据情况下随机近端梯度方法的收敛性。

    We focus on analyzing the classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate of convergence $\tilde{O}(t^{-1/4})$ and complexity $\tilde{O}(\varepsilon^{-4})$ for achieving an $\varepsilon$-near stationary point in terms of the norm of the gradient of Moreau envelope and gradient mapping. While classical convergence guarantee requires i.i.d. data sampling from the target distribution, we only require a mild mixing condition of the conditional distribution, which holds for a wide class of Markov chain sampling algorithms. This improves the existing complexity for the constrained smooth nonconvex optimization with dependent data from $\tilde{O}(\varepsilon^{-8})$ to $\tilde{O}(\varepsilon^{-4})$ with a significantly simpler analysis. We illustrate the generality of our approach by deriving convergence results with dependent data for stochastic proximal gradient methods, 
    

