# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection.](http://arxiv.org/abs/2309.13021) | 该论文提出了一种基于深度学习的混合方法，用于优化基于环境的基因型选择。通过整合不同作物品种的天气数据，特别是在不同气候条件下，准确地预测作物产量对于理解其适应性至关重要。论文中开发了两种新颖的卷积神经网络架构，并利用广义集成方法确定了最优的基因型与环境选择模型。 |
| [^2] | [Scaling Limits of the Wasserstein information matrix on Gaussian Mixture Models.](http://arxiv.org/abs/2309.12997) | 本文研究了高斯混合模型上的Wasserstein度量，并通过缩放极限构建了一类在有界一维齐次晶格上的概率单纯形上的Wasserstein度量。此外，还推广了这种度量到一般的混合模型，并研究了Wasserstein梯度流在不同泛函下的行为。数值实例验证了结果的有效性。 |
| [^3] | [BayesDLL: Bayesian Deep Learning Library.](http://arxiv.org/abs/2309.12928) | BayesDLL是一个用于PyTorch的贝叶斯深度学习库，与其他现有库相比，它可以处理非常大规模的深度网络，无需修改用户代码，并且可以使用预训练模型权重作为先验均值，适用于贝叶斯推断。 |
| [^4] | [Ensemble Differential Evolution with Simulation-Based Hybridization and Self-Adaptation for Inventory Management Under Uncertainty.](http://arxiv.org/abs/2309.12852) | 提出了一种集成差分进化算法，通过模拟混合和自适应机制来解决不确定条件下的库存管理问题。实证研究结果表明该方法可以提高库存管理的财务绩效和优化大搜索空间。 |
| [^5] | [Model-based causal feature selection for general response types.](http://arxiv.org/abs/2309.12833) | 本研究基于模型提出了一种通用响应类型的因果特征选择方法，该方法利用不变性假设从异质环境的数据中输出一部分因果特征的子集，适用于一般的加性噪声模型和非参数设置，解决了非参数条件独立性测试低功率的问题。 |
| [^6] | [On Sparse Modern Hopfield Model.](http://arxiv.org/abs/2309.12673) | 本文介绍了稀疏的现代 Hopfield 模型，通过引入稀疏能量函数和稀疏记忆检索动力学，实现了对稀疏注意机制的一步近似。相比密集模型，稀疏模型的记忆检索误差上界更紧凑，具有明确的稀疏优势条件。同时，稀疏的现代 Hopfield 模型还保持了其密集对应物的稳健理论性质。 |
| [^7] | [Langevin Quasi-Monte Carlo.](http://arxiv.org/abs/2309.12664) | 本文提出了Langevin准蒙特卡洛算法，通过使用低差异准随机样本可以减少其估计误差。 |
| [^8] | [Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes.](http://arxiv.org/abs/2309.12658) | 基于正则化Stein差异的神经算子变分推断用于深度高斯过程，通过使用神经生成器获得取样器以及使用蒙特卡罗估计和子采样随机优化技术解决极小极大问题，提高了深度高斯过程模型的表达能力和推断效果。 |
| [^9] | [Multiply Robust Federated Estimation of Targeted Average Treatment Effects.](http://arxiv.org/abs/2309.12600) | 本研究提出了一种多样性鲁棒性联邦方法，用于通过多中心数据进行合理的因果推断，解决了数据隐私保护和个体协变量分布异质性的挑战，并展示了在效率和鲁棒性方面相对于现有方法的有限样本优势。 |
| [^10] | [Sharpness-Aware Minimization and the Edge of Stability.](http://arxiv.org/abs/2309.12488) | 本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。 |
| [^11] | [A Convex Framework for Confounding Robust Inference.](http://arxiv.org/abs/2309.12450) | 本文提出了一个支撑鲁棒推断的凸框架，通过利用凸规划提供策略价值的精确下界。此外，该方法还可以进行多种扩展，并且具有强理论保证。 |
| [^12] | [Online Infinite-Dimensional Regression: Learning Linear Operators.](http://arxiv.org/abs/2309.06548) | 在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。 |
| [^13] | [Disentanglement via Latent Quantization.](http://arxiv.org/abs/2305.18378) | 本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。 |
| [^14] | [Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation.](http://arxiv.org/abs/2305.17558) | 本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。 |
| [^15] | [Incentivizing Honesty among Competitors in Collaborative Learning and Optimization.](http://arxiv.org/abs/2305.16272) | 这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。 |
| [^16] | [Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions.](http://arxiv.org/abs/2305.14979) | 该论文介绍了一种基于小波域的属性方法WCAM，能够解释神经网络模型对图像损坏的敏感性，确定预测的足够信息，并阐明缩放如何增加准确性。 |
| [^17] | [Synthetic Experience Replay.](http://arxiv.org/abs/2303.06614) | 本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。 |
| [^18] | [Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders.](http://arxiv.org/abs/2302.00662) | 提出了一种在顺序外源未观察到的混淆因素下的强健策略评估和优化方法，使用正交化的强健Fitted-Q迭代，并添加了分位数估计的偏差校正。 |
| [^19] | [A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference.](http://arxiv.org/abs/2212.12393) | 本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。 |
| [^20] | [Quantitative Gaussian Approximation of Randomly Initialized Deep Neural Networks.](http://arxiv.org/abs/2203.07379) | 本论文通过量化高斯逼近的方法，研究了随机初始化的深度神经网络的输出分布与高斯过程的二次Wasserstein距离的上界，揭示了隐藏层和输出层大小对网络高斯行为的影响，并定量地恢复了在宽限制下的分布收敛结果。 |

# 详细

[^1]: 基于深度学习的混合方法用于优化基于环境的基因型选择

    A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection. (arXiv:2309.13021v1 [cs.LG])

    [http://arxiv.org/abs/2309.13021](http://arxiv.org/abs/2309.13021)

    该论文提出了一种基于深度学习的混合方法，用于优化基于环境的基因型选择。通过整合不同作物品种的天气数据，特别是在不同气候条件下，准确地预测作物产量对于理解其适应性至关重要。论文中开发了两种新颖的卷积神经网络架构，并利用广义集成方法确定了最优的基因型与环境选择模型。

    

    准确的作物产量预测对于改善农业实践和确保作物在不同气候条件下的韧性至关重要。在MLCAS2021作物产量预测挑战中，我们利用包含93,028个训练记录的数据集，预测了10,337个测试记录的产量，在13年（2003-2015年）的时间范围内覆盖了美国28个州和加拿大省份的159个地点。该数据集包括5,838个不同基因型的详细信息和为期214天的生长季节的每日天气数据，使得综合分析成为可能。作为获胜团队之一，我们开发了两种新颖的卷积神经网络（CNN）架构：CNN-DNN模型，结合了CNN和全连接网络；以及CNN-LSTM-DNN模型，加入了LSTM层用于天气变量。利用广义集成方法（GEM），我们确定了最优的基因型与环境选择模型。

    Precise crop yield prediction is essential for improving agricultural practices and ensuring crop resilience in varying climates. Integrating weather data across the growing season, especially for different crop varieties, is crucial for understanding their adaptability in the face of climate change. In the MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising 93,028 training records to forecast yields for 10,337 test records, covering 159 locations across 28 U.S. states and Canadian provinces over 13 years (2003-2015). This dataset included details on 5,838 distinct genotypes and daily weather data for a 214-day growing season, enabling comprehensive analysis. As one of the winning teams, we developed two novel convolutional neural network (CNN) architectures: the CNN-DNN model, combining CNN and fully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layer for weather variables. Leveraging the Generalized Ensemble Method (GEM), we determined opt
    
[^2]: 高斯混合模型中Wasserstein信息矩阵的扩展极限

    Scaling Limits of the Wasserstein information matrix on Gaussian Mixture Models. (arXiv:2309.12997v1 [math.PR])

    [http://arxiv.org/abs/2309.12997](http://arxiv.org/abs/2309.12997)

    本文研究了高斯混合模型上的Wasserstein度量，并通过缩放极限构建了一类在有界一维齐次晶格上的概率单纯形上的Wasserstein度量。此外，还推广了这种度量到一般的混合模型，并研究了Wasserstein梯度流在不同泛函下的行为。数值实例验证了结果的有效性。

    

    我们考虑了高斯混合模型(GMMs)上的Wasserstein度量，它被定义为具有有限二阶矩的平滑概率分布空间上Wasserstein度量的推回。通过对GMMs上的Wasserstein度量的缩放极限，它导出了一类在一维有界齐次晶格上的概率单纯形上的Wasserstein度量。具体而言，对于一个方差趋于零的GMMs序列，我们证明了在某种归一化后Wasserstein度量的极限存在。我们还建立了一般GMMs中的这种度量的推广，包括不同的非齐次晶格模型，扩展的GMMs(高斯组件的均值参数也可以改变)，以及包含缩放极限高阶信息的二阶度量。我们进一步研究了GMMs上的Wasserstein梯度流对三个典型的泛函: 势能、内能和相互作用能的情况。数值实例证明了我们的结果。

    We consider the Wasserstein metric on the Gaussian mixture models (GMMs), which is defined as the pullback of the full Wasserstein metric on the space of smooth probability distributions with finite second moment. It derives a class of Wasserstein metrics on probability simplices over one-dimensional bounded homogeneous lattices via a scaling limit of the Wasserstein metric on GMMs. Specifically, for a sequence of GMMs whose variances tend to zero, we prove that the limit of the Wasserstein metric exists after certain renormalization. Generalizations of this metric in general GMMs are established, including inhomogeneous lattice models whose lattice gaps are not the same, extended GMMs whose mean parameters of Gaussian components can also change, and the second-order metric containing high-order information of the scaling limit. We further study the Wasserstein gradient flows on GMMs for three typical functionals: potential, internal, and interaction energies. Numerical examples demons
    
[^3]: BayesDLL: 贝叶斯深度学习库

    BayesDLL: Bayesian Deep Learning Library. (arXiv:2309.12928v1 [cs.LG])

    [http://arxiv.org/abs/2309.12928](http://arxiv.org/abs/2309.12928)

    BayesDLL是一个用于PyTorch的贝叶斯深度学习库，与其他现有库相比，它可以处理非常大规模的深度网络，无需修改用户代码，并且可以使用预训练模型权重作为先验均值，适用于贝叶斯推断。

    

    我们发布了一个新的用于PyTorch的贝叶斯神经网络库，用于大规模深度网络。我们的库实现了主流的近似贝叶斯推断算法：变分推断、MC-dropout、随机梯度MCMC和拉普拉斯近似。与其他现有的贝叶斯神经网络库相比，我们的库有以下主要区别：1）我们的库可以处理包括视觉变换器（ViTs）在内的非常大规模的深度网络。2）用户几乎不需要修改代码（例如，骨干网络定义代码根本不需要修改）。3）我们的库还允许预训练模型权重作为先验均值，这对于使用仅仅依靠下游数据难以从头开始优化的大规模基础模型（如ViTs）进行贝叶斯推断非常有用。我们的代码公开可用于: \url{https://github.com/SamsungLabs/BayesDLL}（备用存储库也可在此处找到：\url{https://github.com/miny})

    We release a new Bayesian neural network library for PyTorch for large-scale deep networks. Our library implements mainstream approximate Bayesian inference algorithms: variational inference, MC-dropout, stochastic-gradient MCMC, and Laplace approximation. The main differences from other existing Bayesian neural network libraries are as follows: 1) Our library can deal with very large-scale deep networks including Vision Transformers (ViTs). 2) We need virtually zero code modifications for users (e.g., the backbone network definition codes do not neet to be modified at all). 3) Our library also allows the pre-trained model weights to serve as a prior mean, which is very useful for performing Bayesian inference with the large-scale foundation models like ViTs that are hard to optimise from scratch with the downstream data alone. Our code is publicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{A mirror repository is also available at: \url{https://github.com/miny
    
[^4]: 基于模拟混合和自适应的集成差分进化算法用于不确定条件下的库存管理

    Ensemble Differential Evolution with Simulation-Based Hybridization and Self-Adaptation for Inventory Management Under Uncertainty. (arXiv:2309.12852v1 [math.OC])

    [http://arxiv.org/abs/2309.12852](http://arxiv.org/abs/2309.12852)

    提出了一种集成差分进化算法，通过模拟混合和自适应机制来解决不确定条件下的库存管理问题。实证研究结果表明该方法可以提高库存管理的财务绩效和优化大搜索空间。

    

    本研究提出了一种基于模拟混合和自适应的集成差分进化算法（EDESH-SA）来解决不确定条件下的库存管理问题。该算法将差分进化算法与模拟混合方法相结合，并通过自适应机制动态地改变变异率和交叉率。由于其适应性，该算法能够处理库存管理中的复杂性和不确定性。通过使用蒙特卡洛模拟（MCS），本研究考虑了连续审查（CR）库存策略，并考虑了随机性和不同的需求情景。这种基于模拟的方法可以真实地评估所提算法在实际环境中解决库存管理挑战的适用性。实证研究的结果显示了所提方法在提高库存管理的财务绩效和优化大搜索空间方面的潜力。

    This study proposes an Ensemble Differential Evolution with Simula-tion-Based Hybridization and Self-Adaptation (EDESH-SA) approach for inven-tory management (IM) under uncertainty. In this study, DE with multiple runs is combined with a simulation-based hybridization method that includes a self-adaptive mechanism that dynamically alters mutation and crossover rates based on the success or failure of each iteration. Due to its adaptability, the algorithm is able to handle the complexity and uncertainty present in IM. Utilizing Monte Carlo Simulation (MCS), the continuous review (CR) inventory strategy is ex-amined while accounting for stochasticity and various demand scenarios. This simulation-based approach enables a realistic assessment of the proposed algo-rithm's applicability in resolving the challenges faced by IM in practical settings. The empirical findings demonstrate the potential of the proposed method to im-prove the financial performance of IM and optimize large search spa
    
[^5]: 基于模型的通用响应类型因果特征选择

    Model-based causal feature selection for general response types. (arXiv:2309.12833v1 [stat.ME])

    [http://arxiv.org/abs/2309.12833](http://arxiv.org/abs/2309.12833)

    本研究基于模型提出了一种通用响应类型的因果特征选择方法，该方法利用不变性假设从异质环境的数据中输出一部分因果特征的子集，适用于一般的加性噪声模型和非参数设置，解决了非参数条件独立性测试低功率的问题。

    

    从观测数据中发现因果关系是一项基本而具有挑战性的任务。在某些应用中，仅学习给定响应变量的因果特征可能已经足够，而不是学习整个潜在的因果结构。不变因果预测（ICP）是一种用于因果特征选择的方法，需要来自异质环境的数据。ICP假设从直接原因生成响应的机制在所有环境中都相同，并利用这种不变性输出一部分因果特征的子集。ICP的框架已经扩展到一般的加性噪声模型和非参数设置，使用条件独立性测试。然而，非参数条件独立性测试经常受到低功率（或较差的类型I错误控制）的困扰，并且上述参数模型不适用于响应不是在连续刻度上测量的应用情况，而是反映了分类信息的情况。

    Discovering causal relationships from observational data is a fundamental yet challenging task. In some applications, it may suffice to learn the causal features of a given response variable, instead of learning the entire underlying causal structure. Invariant causal prediction (ICP, Peters et al., 2016) is a method for causal feature selection which requires data from heterogeneous settings. ICP assumes that the mechanism for generating the response from its direct causes is the same in all settings and exploits this invariance to output a subset of the causal features. The framework of ICP has been extended to general additive noise models and to nonparametric settings using conditional independence testing. However, nonparametric conditional independence testing often suffers from low power (or poor type I error control) and the aforementioned parametric models are not suitable for applications in which the response is not measured on a continuous scale, but rather reflects categor
    
[^6]: 关于稀疏的现代 Hopfield 模型

    On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])

    [http://arxiv.org/abs/2309.12673](http://arxiv.org/abs/2309.12673)

    本文介绍了稀疏的现代 Hopfield 模型，通过引入稀疏能量函数和稀疏记忆检索动力学，实现了对稀疏注意机制的一步近似。相比密集模型，稀疏模型的记忆检索误差上界更紧凑，具有明确的稀疏优势条件。同时，稀疏的现代 Hopfield 模型还保持了其密集对应物的稳健理论性质。

    

    我们介绍了稀疏的现代 Hopfield 模型作为现代 Hopfield 模型的一种扩展。与其密集的对应物一样，稀疏的现代 Hopfield 模型具备一种记忆检索动力学，其一步近似对应于稀疏的注意机制。从理论上讲，我们的关键贡献是通过稀疏熵正则化器的凸共轭导出了封闭形式的稀疏 Hopfield 能量。在此基础上，我们从稀疏能量函数中推导出稀疏记忆检索动力学，并展示了它的一步近似等价于稀疏结构化注意力。重要的是，我们提供了一个依赖于稀疏度的记忆检索误差上界，该上界在证明上要比其密集对应物更紧凑。因此，我们确定并讨论了稀疏优势出现的条件。此外，我们还表明稀疏的现代 Hopfield 模型保持了其密集对应物的稳健理论性质，包括快速的固定点收敛。

    We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point conver
    
[^7]: Langevin准蒙特卡洛算法

    Langevin Quasi-Monte Carlo. (arXiv:2309.12664v1 [stat.CO])

    [http://arxiv.org/abs/2309.12664](http://arxiv.org/abs/2309.12664)

    本文提出了Langevin准蒙特卡洛算法，通过使用低差异准随机样本可以减少其估计误差。

    

    Langevin蒙特卡洛（LMC）及其随机梯度版本是用于从复杂高维分布中抽样的强大算法。为了从密度为$\pi(\theta)\propto \exp(-U(\theta)) $的分布中抽样，LMC通过在梯度方向$\nabla U$上加入高斯扰动来迭代生成下一个样本。对于目标分布$\pi$的期望值通过对LMC样本进行平均估计。在普通蒙特卡洛中，已知通过用低差异序列等准随机样本替代独立随机样本可以显著减少估计误差。在本文中，我们证明LMC的估计误差也可以通过使用准随机样本来减少。具体来说，我们建议使用具有特定低差异性质的完全均匀分布（CUD）序列来生成高斯扰动。在光滑性和凸性条件下，我们证明了带有低差异CUD的LMC算法的收敛性和渐进正态性质。

    Langevin Monte Carlo (LMC) and its stochastic gradient versions are powerful algorithms for sampling from complex high-dimensional distributions. To sample from a distribution with density $\pi(\theta)\propto \exp(-U(\theta)) $, LMC iteratively generates the next sample by taking a step in the gradient direction $\nabla U$ with added Gaussian perturbations. Expectations w.r.t. the target distribution $\pi$ are estimated by averaging over LMC samples. In ordinary Monte Carlo, it is well known that the estimation error can be substantially reduced by replacing independent random samples by quasi-random samples like low-discrepancy sequences. In this work, we show that the estimation error of LMC can also be reduced by using quasi-random samples. Specifically, we propose to use completely uniformly distributed (CUD) sequences with certain low-discrepancy property to generate the Gaussian perturbations. Under smoothness and convexity conditions, we prove that LMC with a low-discrepancy CUD
    
[^8]: 基于正则化Stein差异的神经算子变分推断用于深度高斯过程

    Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes. (arXiv:2309.12658v1 [cs.LG])

    [http://arxiv.org/abs/2309.12658](http://arxiv.org/abs/2309.12658)

    基于正则化Stein差异的神经算子变分推断用于深度高斯过程，通过使用神经生成器获得取样器以及使用蒙特卡罗估计和子采样随机优化技术解决极小极大问题，提高了深度高斯过程模型的表达能力和推断效果。

    

    深度高斯过程（DGP）模型提供了一种强大的非参数贝叶斯推断方法，但精确推断通常是难以求解的，这促使我们使用各种近似方法。然而，现有的方法，如均值场高斯假设，限制了DGP模型的表达能力和效果，而随机逼近可能计算代价高昂。为解决这些挑战，我们引入了基于神经算子的变分推断（NOVI）用于深度高斯过程。NOVI使用神经生成器获得取样器，并在L2空间中最小化生成分布和真实后验之间的正则化Stein差异。我们使用蒙特卡罗估计和子采样随机优化技术解决了极小极大问题。我们证明了通过将Fisher散度与常数相乘来控制方法引入的偏差，从而实现了鲁棒的误差控制，确保了算法的稳定性和精确性。

    Deep Gaussian Process (DGP) models offer a powerful nonparametric approach for Bayesian inference, but exact inference is typically intractable, motivating the use of various approximations. However, existing approaches, such as mean-field Gaussian assumptions, limit the expressiveness and efficacy of DGP models, while stochastic approximation can be computationally expensive. To tackle these challenges, we introduce Neural Operator Variational Inference (NOVI) for Deep Gaussian Processes. NOVI uses a neural generator to obtain a sampler and minimizes the Regularized Stein Discrepancy in L2 space between the generated distribution and true posterior. We solve the minimax problem using Monte Carlo estimation and subsampling stochastic optimization techniques. We demonstrate that the bias introduced by our method can be controlled by multiplying the Fisher divergence with a constant, which leads to robust error control and ensures the stability and precision of the algorithm. Our experim
    
[^9]: 多样性鲁棒性联邦估计的目标平均处理效应

    Multiply Robust Federated Estimation of Targeted Average Treatment Effects. (arXiv:2309.12600v1 [stat.ML])

    [http://arxiv.org/abs/2309.12600](http://arxiv.org/abs/2309.12600)

    本研究提出了一种多样性鲁棒性联邦方法，用于通过多中心数据进行合理的因果推断，解决了数据隐私保护和个体协变量分布异质性的挑战，并展示了在效率和鲁棒性方面相对于现有方法的有限样本优势。

    

    联邦或多中心研究相比单中心研究具有明显优势，包括增加普适性、能够研究少数群体和研究稀有暴露与结果的机会。然而，这些研究存在数据隐私保护和个体协变量分布异质性的挑战。我们提出了一种新颖的联邦方法，通过开发多样性鲁棒性和隐私保护的辅助函数估计，为目标人群提供有效的因果推断。我们通过集成学习建立了转移学习的方法来估计源站点的组合权重以组合信息。我们证明了这些学习到的权重在不同场景下是高效和最优的。我们展示了我们的方法在效率和鲁棒性方面相对于现有方法的有限样本优势。

    Federated or multi-site studies have distinct advantages over single-site studies, including increased generalizability, the ability to study underrepresented populations, and the opportunity to study rare exposures and outcomes. However, these studies are challenging due to the need to preserve the privacy of each individual's data and the heterogeneity in their covariate distributions. We propose a novel federated approach to derive valid causal inferences for a target population using multi-site data. We adjust for covariate shift and covariate mismatch between sites by developing multiply-robust and privacy-preserving nuisance function estimation. Our methodology incorporates transfer learning to estimate ensemble weights to combine information from source sites. We show that these learned weights are efficient and optimal under different scenarios. We showcase the finite sample advantages of our approach in terms of efficiency and robustness compared to existing approaches.
    
[^10]: 锐度感知最小化和稳定性边界。

    Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])

    [http://arxiv.org/abs/2309.12488](http://arxiv.org/abs/2309.12488)

    本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。

    

    最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。

    Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
    
[^11]: 支撑鲁棒推断的凸框架

    A Convex Framework for Confounding Robust Inference. (arXiv:2309.12450v1 [stat.ML])

    [http://arxiv.org/abs/2309.12450](http://arxiv.org/abs/2309.12450)

    本文提出了一个支撑鲁棒推断的凸框架，通过利用凸规划提供策略价值的精确下界。此外，该方法还可以进行多种扩展，并且具有强理论保证。

    

    我们研究了受未观察到的混淆因素影响的离线上下文强化学习中的策略评估问题。传统的敏感性分析方法常被用来在给定的不确定性集合上估计在最坏混淆情况下的策略价值。然而，现有的工作通常为了可行性而采用一些粗糙的松弛不确定性集合的方法，导致对策略价值的估计过于保守。在本文中，我们提出了一种通用估计器，利用凸规划提供了策略价值的一个较为精确的下界。我们的估计器的广泛适用性使得其能够进行多种扩展，例如基于f-分歧的敏感性分析、基于交叉验证和信息准则的模型选择以及利用上界进行鲁棒策略学习等。此外，我们的估计方法可以通过强对偶性重新表述为经验风险最小化问题，从而利用M技术提供了对所提出估计器的强理论保证。

    We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value using convex programming. The generality of our estimator enables various extensions such as sensitivity analysis with f-divergence, model selection with cross validation and information criterion, and robust policy learning with the sharp lower bound. Furthermore, our estimation method can be reformulated as an empirical risk minimization problem thanks to the strong duality, which enables us to provide strong theoretical guarantees of the proposed estimator using techniques of the M-
    
[^12]: 在在线设置下学习线性算子的无限维回归

    Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])

    [http://arxiv.org/abs/2309.06548](http://arxiv.org/abs/2309.06548)

    在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。

    

    我们考虑在线设置下学习两个无限维希尔伯特空间之间的线性算子问题，通过最小二乘损失函数进行学习。我们证明了在$p \in [1, \infty)$范围内，具有均匀有界$p$-Schatten范数的线性算子类是可以在线学习的。另一方面，我们证明了具有均匀有界算子范数的线性算子类\textit{不}是可以在线学习的。此外，我们通过找到一类有界线性算子，证明了在线均一收敛和学习能力之间的分离。最后，我们证明了不可能性结果和均一收敛与学习能力之间的分离在PAC设置下同样成立。

    We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
    
[^13]: 通过潜在量化进行解缠

    Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])

    [http://arxiv.org/abs/2305.18378](http://arxiv.org/abs/2305.18378)

    本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。

    

    在解缠表示学习中，模型需要将数据集的基础变化因素分开并独立地表示出来，而模型并没有提供有关这些因素的真实信息，归纳偏见在实现解缠方面发挥着重要作用。在本文中，我们通过施加严格的交流瓶颈和强大的模型规范化，构建了一种朝着组合编码和解码数据的归纳偏见。具体来说，我们对潜在维度进行可学习的离散编码，并为每个维度应用一个单独的标量码书。潜在量化迫使编码器在许多数据点上使用少量潜在值，从而使解码器能够为每个值分配一致的含义。规范化有助于将模型引向这种简明策略。我们在多个基准数据集上展示了该方法的广泛应用性，并且展示了我们的方法显著提高了一系列标准VAE模型学习的表征的可解释性。

    In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
    
[^14]: 基于虚拟粒子随机逼近的可证速限制变种的SVGD算法。

    Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])

    [http://arxiv.org/abs/2305.17558](http://arxiv.org/abs/2305.17558)

    本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。

    

    Stein变分梯度下降（SVGD）是一种流行的变分推断算法，它模拟相互作用的粒子系统以近似从目标分布中采样，具有各种领域的令人印象深刻的经验性能。在理论上，它的群体（即，无限粒子）极限动力学已经得到了很好的研究，但是SVGD在有限粒子体制下的行为则不太清楚。在这项工作中，我们设计了两种计算效率高的SVGD变体，即VP-SVGD（从概念上讲很优雅）和GB-SVGD（从经验上看很有效），具有可证速的有限粒子收敛率。我们引入了“虚拟粒子”的概念，并在概率测度空间中开发了人口极限SVGD动力学的新型随机逼近方法，它们可以使用有限数量的粒子精确实现。我们的算法可以看作是SVGD的特定随机批处理逼近，比普通方法更具计算效率。

    Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
    
[^15]: 在协同学习和优化中激励竞争对手诚实行为的研究

    Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])

    [http://arxiv.org/abs/2305.16272](http://arxiv.org/abs/2305.16272)

    这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。

    

    协同学习技术能够让机器学习模型的训练比仅利用单一数据源的模型效果更好。然而，在许多情况下，潜在的参与者是下游任务中的竞争对手，如每个都希望通过提供最佳推荐来吸引客户的公司。这可能会激励不诚实的更新，损害其他参与者的模型，从而可能破坏协作的好处。在这项工作中，我们制定了一个模型来描述这种交互，并在该框架内研究了两个学习任务：单轮均值估计和强凸目标的多轮 SGD。对于一类自然的参与者行为，我们发现理性的客户会被激励强烈地操纵他们的更新，从而防止学习。然后，我们提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。最后，我们通过实验证明了这一点。

    Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
    
[^16]: 尺度很重要：基于小波域的属性方法解释模型对图像损坏的敏感性

    Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])

    [http://arxiv.org/abs/2305.14979](http://arxiv.org/abs/2305.14979)

    该论文介绍了一种基于小波域的属性方法WCAM，能够解释神经网络模型对图像损坏的敏感性，确定预测的足够信息，并阐明缩放如何增加准确性。

    

    神经网络在计算机视觉方面表现出了出色的性能，但它们在实际应用中的部署由于对图像损坏的敏感性而具有挑战性。现有的属性方法对于解释对图像损坏的敏感性是无效的，而强健性领域的文献仅提供基于模型的解释。然而，在图像损坏的情况下，审查模型的行为能力对于提高用户信任至关重要。为此，我们介绍了Wavelet sCale Attribution Method (WCAM)，它是从像素域到空间尺度域的属性方法的概括。在空间尺度域中进行属性揭示了模型的关注点和尺度。我们展示WCAM解释了模型在图像破坏下的失效，确定了预测的足够信息，并解释了如何通过缩放增加准确性。

    Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
    
[^17]: 合成经验回放：旨在用扩充数据来提高深度强化学习的效果

    Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06614](http://arxiv.org/abs/2303.06614)

    本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。

    

    过去十年的一个关键主题是，当大型神经网络和大型数据集相结合时，它们可以产生令人惊异的结果。在深度强化学习中，这种范式通常通过经验回放实现，其中过去的经验数据集用于训练策略或值函数。然而，与监督学习或自监督学习不同，强化学习代理必须收集自己的数据，这通常是有限的。因此，利用深度学习的好处是具有挑战性的，即使是小型神经网络在训练开始时也可能出现过拟合现象。在这项工作中，我们利用了生成建模的巨大进步，并提出了合成经验回放（SynthER），一种基于扩散的方法来灵活地上采样代理收集的经验。我们证明了SynthER是一种有效的方法，可以在离线和在线设置下训练强化学习代理，无论是在感知环境还是在像素环境中。在离线设置中，我们观察到了显着的改进。

    A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
    
[^18]: 强健的Fitted-Q评估和迭代在顺序外源未观察到的混淆因素下

    Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders. (arXiv:2302.00662v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00662](http://arxiv.org/abs/2302.00662)

    提出了一种在顺序外源未观察到的混淆因素下的强健策略评估和优化方法，使用正交化的强健Fitted-Q迭代，并添加了分位数估计的偏差校正。

    

    在医学、经济和电子商务等领域，离线强化学习非常重要，因为在线实验可能成本高昂、危险或不道德，并且真实模型未知。然而，大多数方法假设行为策略的所有协变量都是已观察到的。尽管这个假设"顺序可忽略性"在观察数据中不太可能成立，但大部分考虑进入治疗因素的数据可能是观察到的，这激励了敏感性分析。我们研究了在敏感性模型下顺序外源未观察到的混淆因素下的强健策略评估和策略优化。我们提出并分析了正交化的强健Fitted-Q迭代，该方法使用强健贝尔曼算子的封闭形式解来导出强健Q函数的损失最小化问题，并对分位数估计加入偏差校正。我们的算法兼具Fitted-Q迭代的计算简便性和统计优势。

    Offline reinforcement learning is important in domains such as medicine, economics, and e-commerce where online experimentation is costly, dangerous or unethical, and where the true model is unknown. However, most methods assume all covariates used in the behavior policy's action decisions are observed. Though this assumption, sequential ignorability/unconfoundedness, likely does not hold in observational data, most of the data that accounts for selection into treatment may be observed, motivating sensitivity analysis. We study robust policy evaluation and policy optimization in the presence of sequentially-exogenous unobserved confounders under a sensitivity model. We propose and analyze orthogonalized robust fitted-Q-iteration that uses closed-form solutions of the robust Bellman operator to derive a loss minimization problem for the robust Q function, and adds a bias-correction to quantile estimation. Our algorithm enjoys the computational ease of fitted-Q-iteration and statistical 
    
[^19]: A-NeSI: 一种可扩展的近似方法用于概率神经符号推理。

    A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12393](http://arxiv.org/abs/2212.12393)

    本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。

    

    本文研究了将神经网络与符号推理相结合的问题。最近引入的概率神经符号学习（PNL）框架，如DeepProbLog，执行指数时间的精确推理，限制了PNL解决方案的可扩展性。我们介绍了近似神经符号推理（A-NeSI）：一种新的PNL框架，它使用神经网络进行可扩展的近似推理。A-NeSI 1) 在不改变概率逻辑语义的情况下，以多项式时间执行近似推理；2) 使用由背景知识生成的数据进行训练；3) 可以生成有关预测的符号解释；4) 可以在测试时间保证逻辑约束的满足，这在安全关键应用中非常重要。我们的实验表明，A-NeSI是第一个能够解决具有指数组合扩展的三种神经符号任务的端到端方法。最后，我们的实验表明，A-NeSI实现了可解释性和安全性，而没有惩罚。

    We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
    
[^20]: 随机初始化的深度神经网络的量化高斯逼近

    Quantitative Gaussian Approximation of Randomly Initialized Deep Neural Networks. (arXiv:2203.07379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07379](http://arxiv.org/abs/2203.07379)

    本论文通过量化高斯逼近的方法，研究了随机初始化的深度神经网络的输出分布与高斯过程的二次Wasserstein距离的上界，揭示了隐藏层和输出层大小对网络高斯行为的影响，并定量地恢复了在宽限制下的分布收敛结果。

    

    对于任意一个使用随机高斯参数初始化的深度全连接神经网络，我们对其输出分布与适当的高斯过程之间的二次Wasserstein距离进行了上界限制。我们的明确不等式表明隐藏层和输出层的大小如何影响网络的高斯行为，并且定量地恢复了宽限制下的分布收敛结果，即如果所有隐藏层的大小变得很大。

    Given any deep fully connected neural network, initialized with random Gaussian parameters, we bound from above the quadratic Wasserstein distance between its output distribution and a suitable Gaussian process. Our explicit inequalities indicate how the hidden and output layers sizes affect the Gaussian behaviour of the network and quantitatively recover the distributional convergence results in the wide limit, i.e., if all the hidden layers sizes become large.
    

