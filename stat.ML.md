# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.](http://arxiv.org/abs/2309.15111) | 本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。 |
| [^2] | [Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs.](http://arxiv.org/abs/2309.15096) | 本文从两个方向对深度神经网络进行理论分析，提供了通过神经切线核（NTK）和通过凸重塑ReLU网络的全局优化训练目标的方法。此外，我们还提出了一种与NTK相连的多核学习模型，称为门控ReLU网络，通过加权数据屏蔽特征映射来实现全局优化。 |
| [^3] | [On Excess Risk Convergence Rates of Neural Network Classifiers.](http://arxiv.org/abs/2309.15075) | 本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过超额风险来衡量。研究考虑了更一般的场景，使得神经网络可以轻松应用数值优化方法。虽然函数类很大，但无维度速率是可能的。 |
| [^4] | [Reparameterized Variational Rejection Sampling.](http://arxiv.org/abs/2309.14612) | 本文提出了一种重参数化变分拒绝采样（VRS）方法，通过将参数化的提议分布与拒绝采样结合，定义了一个丰富的非参数分布族，明确利用已知的目标分布，为具有连续潜变量的模型提供了一种吸引人的推断策略。 |
| [^5] | [Decoding trust: A reinforcement learning perspective.](http://arxiv.org/abs/2309.14598) | 这项研究通过使用强化学习算法揭示了在成对场景中高水平的信任和可信度是通过同时重视历史经验和未来回报来形成的。 |
| [^6] | [Towards a statistical theory of data selection under weak supervision.](http://arxiv.org/abs/2309.14563) | 本研究针对弱监督下的数据选择进行了统计理论研究，通过实验证明数据选择可以非常有效，有时甚至可以战胜对整个样本的训练。并分析了在不同情况下的数据选择选择方法的有效性。 |
| [^7] | [Cluster-based Method for Eavesdropping Identification and Localization in Optical Links.](http://arxiv.org/abs/2309.14541) | 本文提出了一种基于聚类的方法，用于在光线系统中检测和定位小功率损失的窃听事件。研究结果表明，通过光性能监测数据可以检测这种微小的窃听损失，同时通过在线数据可以有效地定位这类事件。 |
| [^8] | [Byzantine-Resilient Federated PCA and Low Rank Matrix Recovery.](http://arxiv.org/abs/2309.14512) | 这项工作提出了一个拜占庭鲁棒、通信高效和私密的算法(Subspace-Median)来解决在联邦环境中估计对称矩阵主子空间的问题，同时还研究了联邦主成分分析（PCA）和水平联邦低秩列感知（LRCCS）的特殊情况，并展示了Subspace-Median算法的优势。 |
| [^9] | [On the expressivity of embedding quantum kernels.](http://arxiv.org/abs/2309.14419) | 量子核方法是量子和经典机器学习之间最自然的联系之一。本文探讨了嵌入式量子核的表达能力，并得出结论：通过引入计算普适性，任何核函数都可以表示为量子特征映射和嵌入式量子核。 |
| [^10] | [Pseudo Label Selection is a Decision Problem.](http://arxiv.org/abs/2309.13926) | 伪标签选择是半监督学习中的一种方法，通过嵌入决策理论，提出了BPLS框架来解决伪标签选择中的确认偏差问题。 |
| [^11] | [Resilient Constrained Learning.](http://arxiv.org/abs/2306.02426) | 本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。 |
| [^12] | [Borda Regret Minimization for Generalized Linear Dueling Bandits.](http://arxiv.org/abs/2303.08816) | 本文解决了通用广义线性对抗性排名问题中的博尔达后悔最小化问题，提出了高度表达力的模型，并使用一种新的“先探索再执行”算法避免了困难的后悔下限。 |
| [^13] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^14] | [ddml: Double/debiased machine learning in Stata.](http://arxiv.org/abs/2301.09397) | ddml是Stata中的双重/无偏机器学习包，支持五种不同计量模型的因果参数估计，可以灵活估计内生变量的因果效应，在许多现有监督机器学习程序中兼容。推荐与堆叠估计结合使用，提供了蒙特卡洛证据支持。 |
| [^15] | [Online Kernel CUSUM for Change-Point Detection.](http://arxiv.org/abs/2211.15070) | 本研究提出了一种在线变点检测的核CUSUM方法，相比于现有方法更敏感，提供了准确的关键性能指标分析，并建立了最优窗口长度，引入了递归计算程序来确保计算和内存复杂度恒定。 |
| [^16] | [Finite-time analysis of single-timescale actor-critic.](http://arxiv.org/abs/2210.09921) | 这项研究提出了一种在线单时间尺度演员-评论家方法，通过线性函数逼近和马尔可夫样本更新，在连续状态空间中找到了一个$\epsilon$-近似的稳定点，并且在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下证明了其收敛性。 |
| [^17] | [Testing predictions of representation cost theory with CNNs.](http://arxiv.org/abs/2210.01257) | 通过理论和实验证明，训练的卷积神经网络（CNNs）对低频信号具有敏感性，这是因为自然图像的频率分布使大部分能量集中在低到中频。 |
| [^18] | [Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks.](http://arxiv.org/abs/2210.00822) | 本研究通过组合和代数视角探讨了贝叶斯网络的边际独立结构问题，并提出了一个基于 Gröbner 基础的 MCMC 方法 GrUES，该方法在恢复真实结构和估计后验上具有优势。 |
| [^19] | [NN2Poly: A polynomial representation for deep feed-forward artificial neural networks.](http://arxiv.org/abs/2112.11397) | 本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。 |
| [^20] | [Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks.](http://arxiv.org/abs/2110.09548) | 路径正则化为并行ReLU网络提供了一种简化的凸优化问题，通过群稀疏性引导实现了凸模型，并提出了一个近似算法，在所有数据维度上具备完全多项式时间复杂度。 |
| [^21] | [Continuous Treatment Recommendation with Deep Survival Dose Response Function.](http://arxiv.org/abs/2108.10453) | 本论文提出了一个通用公式，称为深度生存剂量反应函数（DeepSDRF），用于解决临床生存数据中的连续治疗推荐问题。通过校正选择偏差，DeepSDRF估计的治疗效果可以用于开发推荐算法。在模拟研究和实际医学数据库上的测试中，DeepSDRF表现出良好的性能。 |
| [^22] | [Optimal Experimental Design for Staggered Rollouts.](http://arxiv.org/abs/1911.03764) | 本文研究了隔开式试验的最优设计问题。对于非自适应实验，提出了一个近似最优解；对于自适应实验，提出了一种新算法——精度导向的自适应实验（PGAE）算法，它使用贝叶斯决策理论来最大化估计治疗效果的预期精度。 |

# 详细

[^1]: SGD在具有接近最优样本复杂度的双层神经网络中寻找并调整特征：以XOR问题为案例研究

    SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])

    [http://arxiv.org/abs/2309.15111](http://arxiv.org/abs/2309.15111)

    本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。

    

    本文研究了小批量随机梯度下降（SGD）在具有二次真实函数分隔数据的双层神经网络上的优化过程。我们证明，对于从$d$维布尔超立方体中由二次“XOR”函数$y = -x_ix_j$标记的数据，可以通过标准小批量SGD在逻辑损失上同时训练两层ReLU激活的双层神经网络，用$d \:\text{polylog}(d)$个样本将其训练到人口误差为$o(1)$的程度。据我们所知，这是首次给出了在标准神经网络上以及标准训练下，对于在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。我们的主要技术是展示网络演化有两个阶段：一个”信号发现“阶段，在此网络规模较小且许多神经元独立演化以寻找特征，以及一个”信号密集“阶段，其中许多神经元相互作用以优化预测。

    In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
    
[^2]: 修复NTK：从神经网络线性化到精确的凸程序

    Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs. (arXiv:2309.15096v1 [cs.LG])

    [http://arxiv.org/abs/2309.15096](http://arxiv.org/abs/2309.15096)

    本文从两个方向对深度神经网络进行理论分析，提供了通过神经切线核（NTK）和通过凸重塑ReLU网络的全局优化训练目标的方法。此外，我们还提出了一种与NTK相连的多核学习模型，称为门控ReLU网络，通过加权数据屏蔽特征映射来实现全局优化。

    

    最近，深度神经网络的理论分析主要集中在两个方向上：1）通过在隐藏层宽度无限大和学习率无穷小的情况下进行的SGD训练的理论洞察力（也称为梯度流）通过神经切线核（NTK）；2）通过锥约束凸重塑ReLU网络的全局优化训练目标。后一种研究方向还提供了ReLU网络的另一种公式，称为门控ReLU网络，可通过高效的无约束凸程序进行全局优化。在这项工作中，我们将门控ReLU网络的凸问题解释为具有加权数据屏蔽特征映射的多核学习（MKL）模型，并与NTK建立了连接。具体而言，我们证明了对于那些与学习目标无关的特定选择的掩码权重，该核等效于门控ReLU网络在训练样本上的NTK。

    Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the tra
    
[^3]: 关于神经网络分类器超额风险收敛速率的研究

    On Excess Risk Convergence Rates of Neural Network Classifiers. (arXiv:2309.15075v1 [stat.ML])

    [http://arxiv.org/abs/2309.15075](http://arxiv.org/abs/2309.15075)

    本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过超额风险来衡量。研究考虑了更一般的场景，使得神经网络可以轻松应用数值优化方法。虽然函数类很大，但无维度速率是可能的。

    

    最近神经网络在模式识别和分类问题上的成功表明，与其他更经典的分类器（如SVM或boosting分类器）相比，神经网络具有独特的特点。本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过其超额风险来衡量。与文献中所规定的典型条件相比，我们考虑了一个更一般的场景，它在两个方面与实际应用类似：首先，要近似的函数类包括了Barron函数作为正子集；其次，构建的神经网络分类器是通过最小化一个替代损失函数而不是0-1损失函数来实现的，从而可以轻松应用基于梯度下降的数值优化方法。虽然我们考虑的函数类非常大，最优速率不能超过$n^{-\frac{1}{3}}$，但在这种情况下，无维度速率是可能的。

    The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\frac{1}{3}}$, it is a regime in which dimension-free rates are possible and approximat
    
[^4]: 重参数化变分拒绝采样

    Reparameterized Variational Rejection Sampling. (arXiv:2309.14612v1 [stat.ML])

    [http://arxiv.org/abs/2309.14612](http://arxiv.org/abs/2309.14612)

    本文提出了一种重参数化变分拒绝采样（VRS）方法，通过将参数化的提议分布与拒绝采样结合，定义了一个丰富的非参数分布族，明确利用已知的目标分布，为具有连续潜变量的模型提供了一种吸引人的推断策略。

    

    传统的变分推断方法依赖于参数化的变分分布族，选择的分布族在确定后验近似的准确性方面起着关键作用。简单的mean-field分布族通常导致较差的近似，而像归一化流这样的丰富分布族往往难以优化，并且通常不包含已知目标分布的结构，因为其是黑箱的。为了扩展灵活的变分分布族空间，我们重新考虑变分拒绝采样（VRS）[Grover et al., 2018]，它将参数化提议分布与拒绝采样结合起来，定义了一个丰富的非参数分布族，明确利用已知的目标分布。通过引入对提议分布参数的低方差重参数化梯度估计器，我们使VRS成为具有连续潜变量的吸引人的推断策略。

    Traditional approaches to variational inference rely on parametric families of variational distributions, with the choice of family playing a critical role in determining the accuracy of the resulting posterior approximation. Simple mean-field families often lead to poor approximations, while rich families of distributions like normalizing flows can be difficult to optimize and usually do not incorporate the known structure of the target distribution due to their black-box nature. To expand the space of flexible variational families, we revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which combines a parametric proposal distribution with rejection sampling to define a rich non-parametric family of distributions that explicitly utilizes the known target distribution. By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables.
    
[^5]: 解读信任:强化学习视角

    Decoding trust: A reinforcement learning perspective. (arXiv:2309.14598v1 [q-bio.PE])

    [http://arxiv.org/abs/2309.14598](http://arxiv.org/abs/2309.14598)

    这项研究通过使用强化学习算法揭示了在成对场景中高水平的信任和可信度是通过同时重视历史经验和未来回报来形成的。

    

    对信任游戏的行为实验表明，信任和可信度在人类中普遍存在，这与正统经济学中假设的经济人的预测相矛盾。这意味着一定存在某种机制促使他们的出现。然而，大多数先前的解释都需要依赖于一些基于模仿学习的因素，即简单版本的社会学习。在这里，我们转向强化学习的范式，个体通过累积经验评估长期回报来更新他们的策略。具体而言，我们使用Q-learning算法研究信任游戏，每个参与者分别与两个不断演化的Q表关联，指导他们作为信任者和托管方的决策。在成对的场景中，我们发现当个体同时重视历史经验和未来回报时，信任和可信度水平较高。从机制上看，Q的演化...

    Behavioral experiments on the trust game have shown that trust and trustworthiness are universal among human beings, contradicting the prediction by assuming \emph{Homo economicus} in orthodox Economics. This means some mechanism must be at work that favors their emergence. Most previous explanations however need to resort to some factors based upon imitative learning, a simple version of social learning. Here, we turn to the paradigm of reinforcement learning, where individuals update their strategies by evaluating the long-term return through accumulated experience. Specifically, we investigate the trust game with the Q-learning algorithm, where each participant is associated with two evolving Q-tables that guide one's decision making as trustor and trustee respectively. In the pairwise scenario, we reveal that high levels of trust and trustworthiness emerge when individuals appreciate both their historical experience and returns in the future. Mechanistically, the evolution of the Q
    
[^6]: 面向弱监督下的数据选择统计理论

    Towards a statistical theory of data selection under weak supervision. (arXiv:2309.14563v1 [stat.ML])

    [http://arxiv.org/abs/2309.14563](http://arxiv.org/abs/2309.14563)

    本研究针对弱监督下的数据选择进行了统计理论研究，通过实验证明数据选择可以非常有效，有时甚至可以战胜对整个样本的训练。并分析了在不同情况下的数据选择选择方法的有效性。

    

    对于一个大小为N的样本，选择一个更小的大小n<N的子样本用于统计估计或学习通常是有用的。这样的数据选择步骤有助于减少数据标记的要求和学习的计算复杂性。我们假设给定了N个未标记的样本{x_i}，并且可以访问一个“替代模型”，它可以比随机猜测更好地预测标签y_i。我们的目标是选择一个子样本集{𝐱_i}，其大小为|G|=n<N。然后我们为这个集合获取标签，并使用它们通过正则化经验风险最小化来训练模型。通过在真实和合成数据上进行混合的数值实验，并在低维和高维渐近情况下进行数学推导，我们证明：(i) 数据选择可以非常有效，特别是在某些情况下可以击败对整个样本的训练；(ii) 在数据选择方面，某些流行的选择在一些情况下是有效的，而在其他情况下则不是。

    Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\{{\boldsymbol x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.  By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selecti
    
[^7]: 光链路中的窃听识别和定位的基于聚类的方法

    Cluster-based Method for Eavesdropping Identification and Localization in Optical Links. (arXiv:2309.14541v1 [stat.ML])

    [http://arxiv.org/abs/2309.14541](http://arxiv.org/abs/2309.14541)

    本文提出了一种基于聚类的方法，用于在光线系统中检测和定位小功率损失的窃听事件。研究结果表明，通过光性能监测数据可以检测这种微小的窃听损失，同时通过在线数据可以有效地定位这类事件。

    

    我们提出了一种基于聚类的方法，用于检测和定位光线系统中小功率损失的窃听事件。我们的研究结果表明，光性能监测（OPM）数据仅通过接收器收集就可以检测到这种微小的窃听损失。另一方面，通过利用在线OPM数据可以有效地实现对这类事件的定位。

    We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.
    
[^8]: 拜占庭鲁棒的联邦PCA和低秩矩阵恢复

    Byzantine-Resilient Federated PCA and Low Rank Matrix Recovery. (arXiv:2309.14512v1 [cs.IT])

    [http://arxiv.org/abs/2309.14512](http://arxiv.org/abs/2309.14512)

    这项工作提出了一个拜占庭鲁棒、通信高效和私密的算法(Subspace-Median)来解决在联邦环境中估计对称矩阵主子空间的问题，同时还研究了联邦主成分分析（PCA）和水平联邦低秩列感知（LRCCS）的特殊情况，并展示了Subspace-Median算法的优势。

    

    在这项工作中，我们考虑了在联邦环境中估计对称矩阵的主子空间（前r个奇异向量的张成）的问题，当每个节点都可以访问对这个矩阵的估计时。我们研究如何使这个问题具有拜占庭鲁棒性。我们引入了一种新颖的可证明的拜占庭鲁棒、通信高效和私密的算法，称为子空间中值算法（Subspace-Median），用于解决这个问题。我们还研究了这个问题的最自然的解法，基于几何中值的修改的联邦幂方法，并解释为什么它是无用的。在这项工作中，我们考虑了鲁棒子空间估计元问题的两个特殊情况 - 联邦主成分分析（PCA）和水平联邦低秩列感知（LRCCS）的谱初始化步骤。对于这两个问题，我们展示了子空间中值算法提供了既具有鲁棒性又具有高通信效率的解决方案。均值的中位数扩展也被开发出来了。

    In this work we consider the problem of estimating the principal subspace (span of the top r singular vectors) of a symmetric matrix in a federated setting, when each node has access to estimates of this matrix. We study how to make this problem Byzantine resilient. We introduce a novel provably Byzantine-resilient, communication-efficient, and private algorithm, called Subspace-Median, to solve it. We also study the most natural solution for this problem, a geometric median based modification of the federated power method, and explain why it is not useful. We consider two special cases of the resilient subspace estimation meta-problem - federated principal components analysis (PCA) and the spectral initialization step of horizontally federated low rank column-wise sensing (LRCCS) in this work. For both these problems we show how Subspace Median provides a resilient solution that is also communication-efficient. Median of Means extensions are developed for both problems. Extensive simu
    
[^9]: 关于嵌入式量子核的表达能力

    On the expressivity of embedding quantum kernels. (arXiv:2309.14419v1 [quant-ph])

    [http://arxiv.org/abs/2309.14419](http://arxiv.org/abs/2309.14419)

    量子核方法是量子和经典机器学习之间最自然的联系之一。本文探讨了嵌入式量子核的表达能力，并得出结论：通过引入计算普适性，任何核函数都可以表示为量子特征映射和嵌入式量子核。

    

    在核方法的背景下，量子核与经典机器学习之间建立了最自然的联系。核方法依赖于内积特征向量，这些特征向量存在于大型特征空间中。量子核通常通过显式构造量子特征态并计算它们的内积来评估，这里称为嵌入式量子核。由于经典核通常在不使用特征向量的情况下进行评估，我们想知道嵌入式量子核的表达能力如何。在这项工作中，我们提出了一个基本问题：是否所有的量子核都可以表达为量子特征态的内积？我们的第一个结果是肯定的：通过调用计算普适性，我们发现对于任何核函数，总是存在对应的量子特征映射和嵌入式量子核。然而，问题更关注的是有效的构造方式。在第二部分中

    One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part
    
[^10]: 伪标签选择是一个决策问题

    Pseudo Label Selection is a Decision Problem. (arXiv:2309.13926v1 [cs.LG])

    [http://arxiv.org/abs/2309.13926](http://arxiv.org/abs/2309.13926)

    伪标签选择是半监督学习中的一种方法，通过嵌入决策理论，提出了BPLS框架来解决伪标签选择中的确认偏差问题。

    

    伪标签选择是半监督学习中一种简单而有效的方法，它需要一些准则来指导伪标签数据的选择。这些准则被证明可以在实践中工作得相当好。然而，它们的性能往往取决于标记数据上初始模型的拟合情况。早期过拟合可能通过选择具有自信但错误预测的实例（通常被称为确认偏差）而传播到最终模型。在两项最近的工作中，我们证明了伪标签选择（PLS）可以自然地嵌入到决策理论中。这为BPLS铺平了道路，它是一种用于PLS的贝叶斯框架，可以缓解确认偏差的问题。其核心是一种新的选择准则：伪样本和标记数据的后验预测的解析近似。我们通过证明这个“伪POS”的贝叶斯最优性来推导出这个选择准则。

    Pseudo-Labeling is a simple and effective approach to semi-supervised learning. It requires criteria that guide the selection of pseudo-labeled data. The latter have been shown to crucially affect pseudo-labeling's generalization performance. Several such criteria exist and were proven to work reasonably well in practice. However, their performance often depends on the initial model fit on labeled data. Early overfitting can be propagated to the final model by choosing instances with overconfident but wrong predictions, often called confirmation bias. In two recent works, we demonstrate that pseudo-label selection (PLS) can be naturally embedded into decision theory. This paves the way for BPLS, a Bayesian framework for PLS that mitigates the issue of confirmation bias. At its heart is a novel selection criterion: an analytical approximation of the posterior predictive of pseudo-samples and labeled data. We derive this selection criterion by proving Bayes-optimality of this "pseudo pos
    
[^11]: 抗干扰约束学习

    Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])

    [http://arxiv.org/abs/2306.02426](http://arxiv.org/abs/2306.02426)

    本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。

    

    在部署机器学习解决方案时，除了准确性之外，它们必须满足多个要求，如公平性、鲁棒性或安全性。这些要求可以通过使用惩罚来隐式地施加，或者通过基于Lagrangian对偶的约束优化方法来显式地施加。无论哪种方式，指定要求都受到妥协和有限的有关数据的先前知识的影响。此外，它们对性能的影响通常只能通过实际解决学习问题来评估。本文提出了一种约束学习方法，该方法在同时解决学习任务的同时调整要求。为此，它以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松了学习约束。我们将此方法称为具有弹性的约束学习，这是对用于描述生态系统的术语的一种借鉴。

    When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
    
[^12]: Borda Regret Minimization for Generalized Linear Dueling Bandits (通用广义线性对抗性排名问题的博尔达后悔最小化算法)

    Borda Regret Minimization for Generalized Linear Dueling Bandits. (arXiv:2303.08816v1 [cs.LG])

    [http://arxiv.org/abs/2303.08816](http://arxiv.org/abs/2303.08816)

    本文解决了通用广义线性对抗性排名问题中的博尔达后悔最小化问题，提出了高度表达力的模型，并使用一种新的“先探索再执行”算法避免了困难的后悔下限。

    

    对抗性排名问题(Dueling bandits)常被用于机器学习应用，如推荐系统和排名问题。本文研究对抗性排名问题中博尔达后悔最小化问题，旨在确定具有最高博尔达得分的项目，并同时最小化累计的后悔。我们提出了一个新的、高度表达力的通用广义线性对抗性排名模型，它包括许多现有模型。 令人惊讶的是，博尔达后悔最小化问题是困难的。 我们证明了渐近时间复杂度的后悔下限是$\Omega(d^{2/3} T^{2/3})$，其中$d$是上下文向量的维数，$T$是时间跨度。为了达到下限，我们提出了一种"先探索再执行"的算法，它具有几乎匹配的上限回归误差$\tilde{O}(d^{2/3} T^{2/3})$。当项目数量$K$很小时，我们的算法可以通过适当选择超参数以达到更小的后悔$\tilde{O}((d\log K)^{1/3}T^{2/3})$。

    Dueling bandits are widely used to model preferential feedback that is prevalent in machine learning applications such as recommendation systems and ranking. In this paper, we study the Borda regret minimization problem for dueling bandits, which aims to identify the item with the highest Borda score while minimizing the cumulative regret. We propose a new and highly expressive generalized linear dueling bandits model, which covers many existing models. Surprisingly, the Borda regret minimization problem turns out to be difficult, as we prove a regret lower bound of order $\Omega(d^{2/3} T^{2/3})$, where $d$ is the dimension of contextual vectors and $T$ is the time horizon. To attain the lower bound, we propose an explore-then-commit type algorithm, which has a nearly matching regret upper bound $\tilde{O}(d^{2/3} T^{2/3})$. When the number of items/arms $K$ is small, our algorithm can achieve a smaller regret $\tilde{O}( (d \log K)^{1/3} T^{2/3})$ with proper choices of hyperparamete
    
[^13]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^14]: ddml: Stata中的双重/无偏机器学习

    ddml: Double/debiased machine learning in Stata. (arXiv:2301.09397v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2301.09397](http://arxiv.org/abs/2301.09397)

    ddml是Stata中的双重/无偏机器学习包，支持五种不同计量模型的因果参数估计，可以灵活估计内生变量的因果效应，在许多现有监督机器学习程序中兼容。推荐与堆叠估计结合使用，提供了蒙特卡洛证据支持。

    

    我们在Stata中引入了一个名为ddml的包，用于双重/无偏机器学习（DDML）。支持五种不同计量模型的因果参数估计，允许在未知函数形式和/或许多外生变量的设置中灵活估计内生变量的因果效应。ddml与Stata中的许多现有监督机器学习程序兼容。我们推荐将DDML与堆叠估计结合使用，将多个机器学习器组合成最终预测器。我们提供了蒙特卡洛证据来支持我们的建议。

    We introduce the package ddml for Double/Debiased Machine Learning (DDML) in Stata. Estimators of causal parameters for five different econometric models are supported, allowing for flexible estimation of causal effects of endogenous variables in settings with unknown functional forms and/or many exogenous variables. ddml is compatible with many existing supervised machine learning programs in Stata. We recommend using DDML in combination with stacking estimation which combines multiple machine learners into a final predictor. We provide Monte Carlo evidence to support our recommendation.
    
[^15]: 在线核CUSUM方法进行变点检测

    Online Kernel CUSUM for Change-Point Detection. (arXiv:2211.15070v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2211.15070](http://arxiv.org/abs/2211.15070)

    本研究提出了一种在线变点检测的核CUSUM方法，相比于现有方法更敏感，提供了准确的关键性能指标分析，并建立了最优窗口长度，引入了递归计算程序来确保计算和内存复杂度恒定。

    

    我们提出了一种高效的在线核Cumulative Sum (CUSUM)方法，用于变点检测，利用核统计量集合中的最大值来考虑未知的变点位置。相比于现有方法，如Scan-B统计量，即对应于非参数Shewhart图过程的方法，我们的方法对于小变化具有更高的敏感性。我们提供了两个关键性能指标的准确分析近似值：平均运行长度（ARL）和预期检测延迟（EDD），这使我们能够建立一个与ARL对数同阶的最优窗口长度，以确保相对于具有无限内存的理论模型能够保持最小功率损失。这类似于参数变点检测文献中的窗口限制广义似然比（GLR）过程的经典结果。此外，我们引入了一种递归计算程序，用于检测统计量，以确保计算和内存复杂度恒定。

    We propose an efficient online kernel Cumulative Sum (CUSUM) method for change-point detection that utilizes the maximum over a set of kernel statistics to account for the unknown change-point location. Our approach exhibits increased sensitivity to small changes compared to existing methods, such as the Scan-B statistic, which corresponds to a non-parametric Shewhart chart-type procedure. We provide accurate analytic approximations for two key performance metrics: the Average Run Length (ARL) and Expected Detection Delay (EDD), which enable us to establish an optimal window length on the order of the logarithm of ARL to ensure minimal power loss relative to an oracle procedure with infinite memory. Such a finding parallels the classic result for window-limited Generalized Likelihood Ratio (GLR) procedure in parametric change-point detection literature. Moreover, we introduce a recursive calculation procedure for detection statistics to ensure constant computational and memory complexi
    
[^16]: 单时间尺度演员-评论家法的有限时间分析

    Finite-time analysis of single-timescale actor-critic. (arXiv:2210.09921v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09921](http://arxiv.org/abs/2210.09921)

    这项研究提出了一种在线单时间尺度演员-评论家方法，通过线性函数逼近和马尔可夫样本更新，在连续状态空间中找到了一个$\epsilon$-近似的稳定点，并且在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下证明了其收敛性。

    

    在许多具有挑战性的应用中，演员-评论家方法取得了显着的成功。然而，在最实际的单时间尺度形式下，其有限时间收敛性仍然不够理解。现有的单时间尺度演员-评论家分析工作仅限于简化的i.i.d.采样或表格设置。我们研究了更实际的在线单时间尺度演员-评论家算法，该算法在连续状态空间中，评论家采用线性函数逼近，并在每个演员步骤中使用单个马尔可夫样本进行更新。先前的分析无法在这种具有挑战性的场景中实现收敛。我们证明，在标准假设下，在线单时间尺度演员-评论家方法能够在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下找到一个$\epsilon$-近似的稳定点，而在i.i.d.采样下，这个复杂度可以进一步改进为$\mathcal{O}(\epsilon^{-2})$。我们的新框架系统地评估了一个

    Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{O}(\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates an
    
[^17]: 使用CNN来测试表示成本理论的预测

    Testing predictions of representation cost theory with CNNs. (arXiv:2210.01257v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01257](http://arxiv.org/abs/2210.01257)

    通过理论和实验证明，训练的卷积神经网络（CNNs）对低频信号具有敏感性，这是因为自然图像的频率分布使大部分能量集中在低到中频。

    

    众所周知，经过训练的卷积神经网络（CNNs）对不同频率的信号具有不同的敏感性。特别是，许多实证研究已经记录了CNNs对低频信号的敏感性。在这项工作中，我们通过理论和实验证明，这种观察到的敏感性是自然图像频率分布的结果，已知大部分能量集中在低到中频。我们的理论分析依赖于CNN的层次在频率空间中的表示，这个想法之前曾被用来加速计算和研究网络训练算法的隐式偏差，但据我们所知，尚未在模型鲁棒性领域应用过。

    It is widely acknowledged that trained convolutional neural networks (CNNs) have different levels of sensitivity to signals of different frequency. In particular, a number of empirical studies have documented CNNs sensitivity to low-frequency signals. In this work we show with theory and experiments that this observed sensitivity is a consequence of the frequency distribution of natural images, which is known to have most of its power concentrated in low-to-mid frequencies. Our theoretical analysis relies on representations of the layers of a CNN in frequency space, an idea that has previously been used to accelerate computations and study implicit bias of network training algorithms, but to the best of our knowledge has not been applied in the domain of model robustness.
    
[^18]: 关于贝叶斯网络边际独立结构的组合和代数视角

    Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks. (arXiv:2210.00822v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.00822](http://arxiv.org/abs/2210.00822)

    本研究通过组合和代数视角探讨了贝叶斯网络的边际独立结构问题，并提出了一个基于 Gröbner 基础的 MCMC 方法 GrUES，该方法在恢复真实结构和估计后验上具有优势。

    

    我们考虑从观测数据中估计贝叶斯网络的边际独立结构的问题，这些数据以一个无向图的形式呈现，被称为无条件依赖图。我们证明了贝叶斯网络的无条件依赖图对应于具有相等独立性和交集数的图。基于这一观察结果，我们给出了与贝叶斯网络的无条件依赖图相关的一个拓扑理想的 Gröbner 基础，然后通过额外的二项式关系将其扩展以连接所有这些图的空间。我们实现了一种名为 GrUES (Gröbner-based Unconditional Equivalence Search) 的 MCMC 方法，该方法基于所得的移动并应用于合成高斯数据。GrUES 以比简单的独立性测试更高的速率恢复真实的边际独立结构，同时还产生了一个包括真实结构的后验估计，其中 $20\%$ 的 HPD 置信区间包含真实结构。

    We consider the problem of estimating the marginal independence structure of a Bayesian network from observational data in the form of an undirected graph called the unconditional dependence graph. We show that unconditional dependence graphs of Bayesian networks correspond to the graphs having equal independence and intersection numbers. Using this observation, a Gr\"obner basis for a toric ideal associated to unconditional dependence graphs of Bayesian networks is given and then extended by additional binomial relations to connect the space of all such graphs. An MCMC method, called GrUES (Gr\"obner-based Unconditional Equivalence Search), is implemented based on the resulting moves and applied to synthetic Gaussian data. GrUES recovers the true marginal independence structure via a penalized maximum likelihood or MAP estimate at a higher rate than simple independence tests while also yielding an estimate of the posterior, for which the $20\%$ HPD credible sets include the true struc
    
[^19]: NN2Poly：用于深度前馈人工神经网络的多项式表示方法

    NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.11397](http://arxiv.org/abs/2112.11397)

    本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。

    

    尽管深度学习应用非常成功，但神经网络的可解释性和理论行为仍然是一个开放的研究领域。本文提出NN2Poly：一种理论方法，用于获取一个显式多项式模型，以提供已经训练好的全连接前馈人工神经网络（多层感知器或MLP）的精确表示。这种方法扩展了文献中提出的先前想法，该想法仅限于单隐藏层的网络，并且适用于回归和分类任务的任意深度MLP。本文的目标是通过在每层上使用激活函数的泰勒展开式，然后使用几个组合性质来计算所需多项式的系数，从而实现此目标。作者讨论了此方法的主要计算挑战以及通过引入一些逼近来克服这些挑战的方法，而不会影响其准确性。通过实验验证表明，尽管NN2Poly方法简单且计算成本低，但对于合成和真实数据集，提供非常准确的多项式逼近。

    Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
    
[^20]: 路径正则化：一种对并行ReLU网络进行凸性和稀疏性引导的正则化方法

    Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.09548](http://arxiv.org/abs/2110.09548)

    路径正则化为并行ReLU网络提供了一种简化的凸优化问题，通过群稀疏性引导实现了凸模型，并提出了一个近似算法，在所有数据维度上具备完全多项式时间复杂度。

    

    理解深度神经网络成功背后的基本原理是当前文献中最重要的开放问题之一。为此，我们研究了深度神经网络的训练问题，并引入了一种分析方法来揭示优化景观中隐藏的凸性。我们考虑了深度并行ReLU网络架构，其也包括标准的深度网络和ResNet作为其特例。然后我们表明，基于路径正则化的训练问题可以表示为一个精确的凸优化问题。我们进一步证明等价的凸问题是通过一种群稀疏性引导的规范进行正则化的。因此，路径正则化的并行ReLU网络可以被视为高维中一种简化的凸模型。更重要的是，由于原始的训练问题可能无法在多项式时间内训练，我们提出了一个在所有数据维度上具有完全多项式时间复杂度的近似算法。然后，我们证明了强全局收敛性。

    Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob
    
[^21]: 深度生存剂量反应函数的连续治疗推荐

    Continuous Treatment Recommendation with Deep Survival Dose Response Function. (arXiv:2108.10453v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.10453](http://arxiv.org/abs/2108.10453)

    本论文提出了一个通用公式，称为深度生存剂量反应函数（DeepSDRF），用于解决临床生存数据中的连续治疗推荐问题。通过校正选择偏差，DeepSDRF估计的治疗效果可以用于开发推荐算法。在模拟研究和实际医学数据库上的测试中，DeepSDRF表现出良好的性能。

    

    我们提出了一个在临床生存数据设置中的连续治疗推荐问题的通用公式，称为深度生存剂量反应函数（DeepSDRF）。也就是说，我们考虑从历史数据中仅仅通过观察到的因素（混杂因子）对观察到的治疗和事件发生时间结果都有影响的条件平均剂量反应（CADR）函数的学习问题。从DeepSDRF中估计的治疗效果使我们能够开发具有选择偏差校正的推荐算法。我们比较了基于随机搜索和强化学习的两种推荐方法，并发现在患者结果方面表现相似。我们在大量的模拟研究和eICU研究机构（eRI）数据库上测试了DeepSDRF和相应的推荐器。据我们所知，这是首次在医学背景下使用因果模型来解决观察数据中的连续治疗效应问题。

    We propose a general formulation for continuous treatment recommendation problems in settings with clinical survival data, which we call the Deep Survival Dose Response Function (DeepSDRF). That is, we consider the problem of learning the conditional average dose response (CADR) function solely from historical data in which observed factors (confounders) affect both observed treatment and time-to-event outcomes. The estimated treatment effect from DeepSDRF enables us to develop recommender algorithms with the correction for selection bias. We compared two recommender approaches based on random search and reinforcement learning and found similar performance in terms of patient outcome. We tested the DeepSDRF and the corresponding recommender on extensive simulation studies and the eICU Research Institute (eRI) database. To the best of our knowledge, this is the first time that causal models are used to address the continuous treatment effect with observational data in a medical context.
    
[^22]: 隔开式试验的最优设计

    Optimal Experimental Design for Staggered Rollouts. (arXiv:1911.03764v5 [econ.EM] UPDATED)

    [http://arxiv.org/abs/1911.03764](http://arxiv.org/abs/1911.03764)

    本文研究了隔开式试验的最优设计问题。对于非自适应实验，提出了一个近似最优解；对于自适应实验，提出了一种新算法——精度导向的自适应实验（PGAE）算法，它使用贝叶斯决策理论来最大化估计治疗效果的预期精度。

    

    本文研究了在不同时期内某组数据单元的治疗开始时间存在差异时，对实验进行设计和分析的问题。设计问题涉及选择每个数据单元的初始治疗时间以便最精确地估计治疗的瞬时效应和累积效应。我们首先考虑非自适应实验，其中所有的治疗分配决策都在实验开始之前做出。针对这种情况，我们证明了优化问题通常是NP难的，并提出了一种近似最优解。在该解决方案下，每个时期进入治疗的分数最初较低，然后变高，最后再次降低。接下来，我们研究了自适应实验设计问题，其中在收集每个时期的数据后更新继续实验和治疗分配决策。对于自适应情况，我们提出了一种新算法——精度导向的自适应实验（PGAE）算法，它使用贝叶斯决策理论来最大化估计治疗效果的预期精度。我们证明了PGAE算法达到了悔恨的下限，悔恨定义为期望累计平方标准误差和任意治疗分配策略所能实现的最佳误差之间的差异。

    In this paper, we study the design and analysis of experiments conducted on a set of units over multiple time periods where the starting time of the treatment may vary by unit. The design problem involves selecting an initial treatment time for each unit in order to most precisely estimate both the instantaneous and cumulative effects of the treatment. We first consider non-adaptive experiments, where all treatment assignment decisions are made prior to the start of the experiment. For this case, we show that the optimization problem is generally NP-hard, and we propose a near-optimal solution. Under this solution, the fraction entering treatment each period is initially low, then high, and finally low again. Next, we study an adaptive experimental design problem, where both the decision to continue the experiment and treatment assignment decisions are updated after each period's data is collected. For the adaptive case, we propose a new algorithm, the Precision-Guided Adaptive Experim
    

