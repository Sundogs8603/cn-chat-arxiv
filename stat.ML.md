# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Graph Kalman Filters.](http://arxiv.org/abs/2303.12021) | 本文首次将卡尔曼和扩展卡尔曼滤波器推广到图形上，使得它可以适用于输出是向量或标量的情况，并且可以学习未知的状态转移和读取函数。 |
| [^2] | [Formulation of Weighted Average Smoothing as a Projection of the Origin onto a Convex Polytope.](http://arxiv.org/abs/2303.11958) | 本文研究了加权移动平均平滑器的最佳加权窗口，将问题转化为原点在凸多面体上的投影，并提供了一些解决方案。 |
| [^3] | [Doubly Regularized Entropic Wasserstein Barycenters.](http://arxiv.org/abs/2303.11844) | 本文提出了一种双重正则化熵Wasserstein重心公式，具有好的正则化、逼近、稳定性和（无网格）优化特性; 其中，只有在$\tau=\lambda/2$的情况下是无偏差的。 |
| [^4] | [Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian.](http://arxiv.org/abs/2303.11835) | 本文提出了一个逐层参数化方法，用于实现内置鲁棒性保证的1D卷积神经网络。该方法基于CNN特征的Lipschitz常数作为鲁棒性度量，并使用Cayley变换和可控性Gram矩来实现CNN的Lipschitz连续性和无约束训练，最后在心律失常数据分类任务中取得了改进的鲁棒性。 |
| [^5] | [Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure.](http://arxiv.org/abs/2303.11786) | 这是一个处理低维流形数据的回归框架，首先通过构建图形骨架来捕捉潜在的流形几何结构，然后在其上运用非参数回归技术来估计回归函数，除了具有非参数优点之外，在处理多个流形数据，嘈杂观察时也表现出较好的鲁棒性。 |
| [^6] | [Exact Non-Oblivious Performance of Rademacher Random Embeddings.](http://arxiv.org/abs/2303.11774) | 本文证实了Rademacher随机投影的非遗忘性能，并建立了Schur-凹性质。这项成果为随机投影的性能提供了新的几何视角，具有更好的量化界限，填补了理论和实践之间的差距。 |
| [^7] | [Universal Smoothed Score Functions for Generative Modeling.](http://arxiv.org/abs/2303.11669) | 本文提出了通用平滑得分函数用于生成模型，通过导出其参数化的通用形式，详细描述了学习平滑后的M-密度的时间复杂度。同时，使用M个独立高斯通道的因子核对未知的兴趣密度进行平滑，并评估了其在CIFAR-10数据集上的表现。 |
| [^8] | [Uniform Risk Bounds for Learning with Dependent Data Sequences.](http://arxiv.org/abs/2303.11650) | 本文针对相关数据序列的学习，不依赖混合参数或者连续带参照的复杂度度量，展示了统一的风险界限，并提出了可应用于场景优化的相关约束随机程序的样本复杂度计算方法。 |
| [^9] | [Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical Thresholds.](http://arxiv.org/abs/2303.11619) | 本文研究了和积多项式的炸裂算法和其 RLCT。 |
| [^10] | [Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches.](http://arxiv.org/abs/2303.11582) | 本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。 |
| [^11] | [How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer.](http://arxiv.org/abs/2303.11454) | 本文提出了具有ReLU激活的随机浅层神经网络的特征描述，对于回归问题，它们类似于无限广义加性模型（IGAM） |
| [^12] | [Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations.](http://arxiv.org/abs/2303.11453) | 本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\ell_2$范数列的解可以泛化到新样本上。 |
| [^13] | [Geometrical aspects of lattice gauge equivariant convolutional neural networks.](http://arxiv.org/abs/2303.11448) | 本文研究了栅格规范等变卷积神经网络的全局群等变性，并提供了一个几何公式，表明L-CNN中的卷积是SU($N$)主丛上规范等变神经网络的一个特例。 |
| [^14] | [Solving High-Dimensional Inverse Problems with Auxiliary Uncertainty via Operator Learning with Limited Data.](http://arxiv.org/abs/2303.11379) | 本文提出了一个基于深度神经网络代理模型进行流图校准的框架， 并结合先前辅助过程的知识来解决高维逆问题，成功识别了高维系统中的源。 |
| [^15] | [Generalized partitioned local depth.](http://arxiv.org/abs/2303.10167) | 本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。 |
| [^16] | [Long-tailed Classification from a Bayesian-decision-theory Perspective.](http://arxiv.org/abs/2303.06075) | 本文从贝叶斯决策理论的角度提出了一个通用且有原则的框架，为长尾分类提供了理论上的支持，并采用综合风险和贝叶斯深度集成方法以提高所有类别的准确性，特别是“长尾”类别。 |
| [^17] | [Statistical Analysis of Karcher Means for Random Restricted PSD Matrices.](http://arxiv.org/abs/2302.12426) | 本文通过研究限制半正定矩阵流形上的本质均值模型及其Karcher均值的统计分析，提出了一般外部信号加噪声模型下的确定性误差界，并表明LRC-dPCA算法与全样本PCA算法性能相同。 |
| [^18] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^19] | [Environmental Sensor Placement with Convolutional Gaussian Neural Processes.](http://arxiv.org/abs/2211.10381) | 本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。 |
| [^20] | [Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression.](http://arxiv.org/abs/2211.07484) | 该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。 |
| [^21] | [Valid Inference after Causal Discovery.](http://arxiv.org/abs/2208.05949) | 本研究开发了工具以实现因果发现后的有效推断，解决了使用相同数据运行因果发现算法后估计因果效应导致经典置信区间的覆盖保证无效问题。 |
| [^22] | [Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review.](http://arxiv.org/abs/2201.04207) | 本文介绍了银行反洗钱的统计和机器学习方法，并提出客户风险评估和可疑行为标识两个核心要素。未来的研究方向包括生成合成数据、半监督和深度学习、可解释性以及结果的公平性。 |
| [^23] | [Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates.](http://arxiv.org/abs/2201.01652) | 本文提出了一个SMM的扩展，该扩展支持代理函数的弱凸性和块多凸性，并在解决非凸约束问题上具有显著优越性能。 |
| [^24] | [Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles.](http://arxiv.org/abs/1703.01347) | 本论文研究了具有噪声特征的上下文线性Bandit问题。我们提出了一个算法，通过观察信息，实现了贝叶斯神谕并得到了$\tilde{O}(d\sqrt{T})$的遗憾界。 |

# 详细

[^1]: 图卡尔曼滤波器

    Graph Kalman Filters. (arXiv:2303.12021v1 [cs.LG])

    [http://arxiv.org/abs/2303.12021](http://arxiv.org/abs/2303.12021)

    本文首次将卡尔曼和扩展卡尔曼滤波器推广到图形上，使得它可以适用于输出是向量或标量的情况，并且可以学习未知的状态转移和读取函数。

    

    众所周知，卡尔曼滤波器通过使用状态空间表示来模拟动态系统，下一个状态的更新以及与新观察到的系统输出相关的信息来控制其不确定性。本文首次将卡尔曼和扩展卡尔曼滤波器推广到离散时间的设置下，其中输入、状态和输出均表示为带属性的图形，其拓扑和属性可以随时间变化。此设置使得我们可以将框架适应于输出是向量或标量的情况（节点/图级任务）。在所提出的理论框架内，未知的状态转移和读取函数与下游预测任务一起端到端学习。

    The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
    
[^2]: 加权平滑在凸多面体上的投影式

    Formulation of Weighted Average Smoothing as a Projection of the Origin onto a Convex Polytope. (arXiv:2303.11958v1 [cs.LG])

    [http://arxiv.org/abs/2303.11958](http://arxiv.org/abs/2303.11958)

    本文研究了加权移动平均平滑器的最佳加权窗口，将问题转化为原点在凸多面体上的投影，并提供了一些解决方案。

    

    本文研究了在平方损失下加权移动平均平滑器的最佳加权窗口。我们证明存在一个最优的对称加权窗口。我们研究了逐步减弱的加权窗口，它们的权重随着远离中心而减少。我们将相应的最小二乘问题公式化为一个二次规划问题，最终将其转化为原点在凸多面体上的投影。此外，当输入数据满足某些条件时，我们还提供了一些最佳窗口的分析解决方案。

    Our study focuses on determining the best weight windows for a weighted moving average smoother under squared loss. We show that there exists an optimal weight window that is symmetrical around its center. We study the class of tapered weight windows, which decrease in weight as they move away from the center. We formulate the corresponding least squares problem as a quadratic program and finally as a projection of the origin onto a convex polytope. Additionally, we provide some analytical solutions to the best window when some conditions are met on the input data.
    
[^3]: 双重正则化熵 Wasserstein 重心

    Doubly Regularized Entropic Wasserstein Barycenters. (arXiv:2303.11844v1 [math.OC])

    [http://arxiv.org/abs/2303.11844](http://arxiv.org/abs/2303.11844)

    本文提出了一种双重正则化熵Wasserstein重心公式，具有好的正则化、逼近、稳定性和（无网格）优化特性; 其中，只有在$\tau=\lambda/2$的情况下是无偏差的。

    

    我们研究了一种常规的正则化Wasserstein重心的公式，这个公式具有良好的正则化、逼近、稳定性和（无网格）优化特性。这个重心被定义为唯一一种最小化关于一族给定概率测度的熵最优输运（EOT）成本之和及熵项的概率测度。我们称之为$(\lambda,\tau)$-重心，其中，$\lambda$ 是内部正则化强度，$\tau$ 是外部正则化强度。这种公式恢复了已经提出的多种EOT重心，适合于不同的 $\lambda, \tau \geq 0$ 选择，并对它们进行了泛化。首先，尽管具有双重正则化，但在$\tau=\lambda/2$ 的情况下，我们证明了我们的公式是无偏的: 对于光滑密度，（未正则化的）Wasserstein 重心目标函数中的次优性是熵正则化强度$\lambda^2$的，而不是一般情况下的$\max \{\lambda, \tau\}$。

    We study a general formulation of regularized Wasserstein barycenters that enjoys favorable regularity, approximation, stability and (grid-free) optimization properties. This barycenter is defined as the unique probability measure that minimizes the sum of entropic optimal transport (EOT) costs with respect to a family of given probability measures, plus an entropy term. We denote it $(\lambda,\tau)$-barycenter, where $\lambda$ is the inner regularization strength and $\tau$ the outer one. This formulation recovers several previously proposed EOT barycenters for various choices of $\lambda,\tau \geq 0$ and generalizes them. First, in spite of -- and in fact owing to -- being \emph{doubly} regularized, we show that our formulation is debiased for $\tau=\lambda/2$: the suboptimality in the (unregularized) Wasserstein barycenter objective is, for smooth densities, of the order of the strength $\lambda^2$ of entropic regularization, instead of $\max\{\lambda,\tau\}$ in general. We discuss 
    
[^4]: 利用Cayley变换和可控性Gram矩的Lipschitz-bounded 1D卷积神经网络(arXiv:2303.11835v1 [cs.LG])

    Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian. (arXiv:2303.11835v1 [cs.LG])

    [http://arxiv.org/abs/2303.11835](http://arxiv.org/abs/2303.11835)

    本文提出了一个逐层参数化方法，用于实现内置鲁棒性保证的1D卷积神经网络。该方法基于CNN特征的Lipschitz常数作为鲁棒性度量，并使用Cayley变换和可控性Gram矩来实现CNN的Lipschitz连续性和无约束训练，最后在心律失常数据分类任务中取得了改进的鲁棒性。

    

    我们建立了一种用于1D卷积神经网络（CNN）的逐层参数化，具有内置的端到端鲁棒性保证。我们使用CNN特征的Lipschitz常数作为鲁棒性度量。我们基于Cayley变换对正交矩阵进行参数化以及对卷积层的状态空间表征的可控性Gram矩进行参数化。所提出的参数化设计满足线性矩阵不等式，从而实现CNN的Lipschitz连续性，进一步实现Lipschitz-bounded 1D CNNs的无约束训练。最后，我们对心律失常数据进行Lipschitz-bounded 1D CNNs的分类训练，并展示了其改进的鲁棒性。

    We establish a layer-wise parameterization for 1D convolutional neural networks (CNNs) with built-in end-to-end robustness guarantees. Herein, we use the Lipschitz constant of the input-output mapping characterized by a CNN as a robustness measure. We base our parameterization on the Cayley transform that parameterizes orthogonal matrices and the controllability Gramian for the state space representation of the convolutional layers. The proposed parameterization by design fulfills linear matrix inequalities that are sufficient for Lipschitz continuity of the CNN, which further enables unconstrained training of Lipschitz-bounded 1D CNNs. Finally, we train Lipschitz-bounded 1D CNNs for the classification of heart arrythmia data and show their improved robustness.
    
[^5]: Skeleton Regression：一种基于流形结构估计的基于图形的方法。

    Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure. (arXiv:2303.11786v1 [cs.LG])

    [http://arxiv.org/abs/2303.11786](http://arxiv.org/abs/2303.11786)

    这是一个处理低维流形数据的回归框架，首先通过构建图形骨架来捕捉潜在的流形几何结构，然后在其上运用非参数回归技术来估计回归函数，除了具有非参数优点之外，在处理多个流形数据，嘈杂观察时也表现出较好的鲁棒性。

    

    我们引入了一个新的回归框架，旨在处理围绕低维流形的复杂数据。我们的方法首先构建一个图形表示，称为骨架，以捕获潜在的几何结构。然后，我们在骨架图上定义指标，应用非参数回归技术，以及基于图形的特征转换来估计回归函数。除了包括的非参数方法外，我们还讨论了一些非参数回归器在骨架图等一般度量空间方面的限制。所提出的回归框架使我们能够避开维度灾难，具有可以处理多个流形的并集并且鲁棒性能应对加性噪声和嘈杂观察的额外优势。我们为所提出的方法提供了统计保证，并通过模拟和实际数据示例证明了其有效性。

    We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. In addition to the included nonparametric methods, we also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework allows us to bypass the curse of dimensionality and provides additional advantages that it can handle the union of multiple manifolds and is robust to additive noise and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.
    
[^6]: Rademacher随机嵌入的精确非遗忘性能

    Exact Non-Oblivious Performance of Rademacher Random Embeddings. (arXiv:2303.11774v1 [cs.LG])

    [http://arxiv.org/abs/2303.11774](http://arxiv.org/abs/2303.11774)

    本文证实了Rademacher随机投影的非遗忘性能，并建立了Schur-凹性质。这项成果为随机投影的性能提供了新的几何视角，具有更好的量化界限，填补了理论和实践之间的差距。

    

    本文重新审视了Rademacher随机投影的性能，建立了新颖的统计保证，与输入数据相比具有数值上的精度和非遗忘性。具体而言，中心结果是Rademacher随机投影与输入的Schur-凹性质。这提供了随机投影性能的新颖几何视角，同时在比以前的研究更好地提高了量化界限。作为这一更广泛结果的推论，我们在稀疏数据或分布具有小范围的数据上获得了改进的表现。这种非遗忘性分析与以前的技术相比是一种新奇之处，并弥合了理论和实践之间经常观察到的差距。主要结果使用代数框架来证明Schur-凹性质，这是一个独立于兴趣的贡献，并且是导数基础标准的一种优雅的替代方法。

    This paper revisits the performance of Rademacher random projections, establishing novel statistical guarantees that are numerically sharp and non-oblivious with respect to the input data. More specifically, the central result is the Schur-concavity property of Rademacher random projections with respect to the inputs. This offers a novel geometric perspective on the performance of random projections, while improving quantitatively on bounds from previous works. As a corollary of this broader result, we obtained the improved performance on data which is sparse or is distributed with small spread. This non-oblivious analysis is a novelty compared to techniques from previous work, and bridges the frequently observed gap between theory and practise. The main result uses an algebraic framework for proving Schur-concavity properties, which is a contribution of independent interest and an elegant alternative to derivative-based criteria.
    
[^7]: 通用平滑得分函数用于生成模型

    Universal Smoothed Score Functions for Generative Modeling. (arXiv:2303.11669v1 [stat.ML])

    [http://arxiv.org/abs/2303.11669](http://arxiv.org/abs/2303.11669)

    本文提出了通用平滑得分函数用于生成模型，通过导出其参数化的通用形式，详细描述了学习平滑后的M-密度的时间复杂度。同时，使用M个独立高斯通道的因子核对未知的兴趣密度进行平滑，并评估了其在CIFAR-10数据集上的表现。

    

    本文探讨基于用具有等噪声级的M个独立高斯通道的因子核平滑未知$\mathbb{R}^d$兴趣密度的生成建模问题，首先通过导出其参数化的通用形式，完整描述了在$\mathbb{R}^{Md}$中学习平滑后的密度的时间复杂度（称为M-密度），并因为其构造方式而具有排列不变性；接着通过分析该类高斯分布的条件数，研究了M-密度抽样的时间复杂度，得到了随着$M$的增加而“形状”变化的几何观察结论；最后，在CIFAR-10数据集上针对这类生成模型的样本质量进行了呈现自由创造距离（14.15）结果的实验，尤其在长时间混合的MCMC链上仅使用单个噪声程序的情况下获得了值得注意的结果。

    We consider the problem of generative modeling based on smoothing an unknown density of interest in $\mathbb{R}^d$ using factorial kernels with $M$ independent Gaussian channels with equal noise levels introduced by Saremi and Srivastava (2022). First, we fully characterize the time complexity of learning the resulting smoothed density in $\mathbb{R}^{Md}$, called M-density, by deriving a universal form for its parametrization in which the score function is by construction permutation equivariant. Next, we study the time complexity of sampling an M-density by analyzing its condition number for Gaussian distributions. This spectral analysis gives a geometric insight on the "shape" of M-densities as one increases $M$. Finally, we present results on the sample quality in this class of generative models on the CIFAR-10 dataset where we report Fr\'echet inception distances (14.15), notably obtained with a single noise level on long-run fast-mixing MCMC chains.
    
[^8]: 带有相关数据序列的学习的统一风险界限

    Uniform Risk Bounds for Learning with Dependent Data Sequences. (arXiv:2303.11650v1 [cs.LG])

    [http://arxiv.org/abs/2303.11650](http://arxiv.org/abs/2303.11650)

    本文针对相关数据序列的学习，不依赖混合参数或者连续带参照的复杂度度量，展示了统一的风险界限，并提出了可应用于场景优化的相关约束随机程序的样本复杂度计算方法。

    

    本文将独立数据的学习理论推广到了相关数据序列的情况。与大多数文献不同的是，我们不依赖于混合参数或者连续带参照的复杂度度量，而是采用了经典的证明模式和容量度量来推导统一的风险界限。特别地，我们展示了标准分类风险最小化理论在相关数据情况下的VC维界限。此外，我们还提供了基于R算子的复杂度界限，其与独立同分布情况下标准的界限没有变化。最后，我们展示了如何将这些结果应用于场景优化的情形，从而计算出具有相关约束的随机程序的样本复杂度。

    This paper extends standard results from learning theory with independent data to sequences of dependent data. Contrary to most of the literature, we do not rely on mixing arguments or sequential measures of complexity and derive uniform risk bounds with classical proof patterns and capacity measures. In particular, we show that the standard classification risk bounds based on the VC-dimension hold in the exact same form for dependent data, and further provide Rademacher complexity-based bounds, that remain unchanged compared to the standard results for the identically and independently distributed case. Finally, we show how to apply these results in the context of scenario-based optimization in order to compute the sample complexity of random programs with dependent constraints.
    
[^9]: 求解和积多项式和实对数规范阈值的炸裂算法

    Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical Thresholds. (arXiv:2303.11619v1 [math.ST])

    [http://arxiv.org/abs/2303.11619](http://arxiv.org/abs/2303.11619)

    本文研究了和积多项式的炸裂算法和其 RLCT。

    

    在考虑给出贝叶斯广义误差的实对数规范阈值时，论文用稍微简单的多项式替换平均误差函数，其 RLCT 对应于平均误差函数的 RLCT，并通过称为炸裂的代数操作解决其奇点来获得其 RLCT。虽然众所周知，任何多项式的奇点都可以通过有限次的炸裂迭代来解决，但并没有明确是否可以通过应用特定的炸裂算法来解决特定多项式的奇点。因此，本文考虑了称为和积多项式的多项式的炸裂算法及其 RLCT。

    When considering a real log canonical threshold (RLCT) that gives a Bayesian generalization error, in general, papers replace a mean error function with a relatively simple polynomial whose RLCT corresponds to that of the mean error function, and obtain its RLCT by resolving its singularities through an algebraic operation called blow-up. Though it is known that the singularities of any polynomial can be resolved by a finite number of blow-up iterations, it is not clarified whether or not it is possible to resolve singularities of a specific polynomial by applying a specific blow-up algorithm. Therefore this paper considers the blow-up algorithm for the polynomials called sum-of-products (sop) polynomials and its RLCT.
    
[^10]: 大规模适应性实验：灵活批处理的贝叶斯算法

    Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])

    [http://arxiv.org/abs/2303.11582](http://arxiv.org/abs/2303.11582)

    本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。

    

    标准的贝叶斯算法假定持续重新分配测量工作，这在实现过程中存在延迟反馈和基础设施/组织难题等挑战。本文针对仅有少数重新分配阶段的实际情况，其中测量结果是以批处理形式测量的，提出了一种新的适应性实验框架，可灵活处理任何批处理大小。我们的主要观察是，在统计推断中普遍使用的正态近似也可以指导可扩展自适应设计。通过推导渐进顺序实验，我们制定了一种动态规划，可以利用平均回报的先验信息。动态规划的状态转移相对于采样分配是可微的，允许使用基于梯度的方法进行规划和策略优化。我们提出了一种简单的迭代规划方法，即残余时限优化，通过优化平衡探索和利用的规划目标来选择采样分配。在合成和真实世界基准测试问题上的实验结果表明，我们的框架实现了最先进的性能，同时具有模块化和易用性。

    Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
    
[^11]: 如何描述ReLU神经网络的隐式正则化特性——第二部分：具有随机第一层的两层多维情况

    How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer. (arXiv:2303.11454v1 [cs.LG])

    [http://arxiv.org/abs/2303.11454](http://arxiv.org/abs/2303.11454)

    本文提出了具有ReLU激活的随机浅层神经网络的特征描述，对于回归问题，它们类似于无限广义加性模型（IGAM）

    

    随机化神经网络是一种强大的模型，其中仅优化了最终层的权重，可降低神经网络模型的计算时间。同时，这些模型在各种回归和分类任务中的泛化能力惊人。本文对于具有ReLU激活的随机、浅层神经网络提出了一个宏观精确的特征描述，即广义加性模型（GAM）类型的回归问题，其中考虑了无限多个方向：无限广义加性模型（IGAM）。 IGAM被形式化为函数空间中特定正则化泛函和相当一般的损失函数的优化问题的解。本文是在先前研究的基础上对多元神经网络进行的扩展，我们在先前研究中展示了在某些条件下具有ReLU激活的宽式RSNs的行为类似于样条回归。

    Randomized neural networks (randomized NNs), where only the terminal layer's weights are optimized constitute a powerful model class to reduce computational time in training the neural network model. At the same time, these models generalize surprisingly well in various regression and classification tasks. In this paper, we give an exact macroscopic characterization (i.e., a characterization in function space) of the generalization behavior of randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs correspond to a generalized additive model (GAM)-typed regression in which infinitely many directions are considered: the infinite generalized additive model (IGAM). The IGAM is formalized as solution to an optimization problem in function space for a specific regularization functional and a fairly general loss. This work is an extension to multivariate NNs of prior work, where we showed how wide RSNs with ReLU activation behave like spline regression under certain conditions 
    
[^12]: 基于Group Lasso的贪婪剪枝在矩阵感知和二次激活神经网络上可证地泛化

    Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])

    [http://arxiv.org/abs/2303.11453](http://arxiv.org/abs/2303.11453)

    本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\ell_2$范数列的解可以泛化到新样本上。

    

    剪枝方案广泛用于降低具有大量参数的模型的复杂性。实践研究表明，修剪过度参数化模型并微调可很好地泛化到新样本上。虽然以上被称为剪枝+微调的流程在降低训练模型的复杂性方面非常成功，但其背后的理论仍然不甚了解。本文通过研究超参数化矩阵感知问题上的剪枝+微调框架来解决这个问题，其中真实结果表示为$U_\star \in \mathbb{R}^{d \times r}$，而超参数化模型表示为$U \in \mathbb{R}^{d \times k}$，其中$k \gg r$。我们研究加上Group Lasso正则化器的平滑版本$\sum_{i=1}^k \| U e_i \|_2$的平均误差的近似局部极小值，证明修剪低$\ell_2$范数列的解$U_{

    Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \| U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a solution $U_{\
    
[^13]: 栅格规范等变卷积神经网络的几何方面

    Geometrical aspects of lattice gauge equivariant convolutional neural networks. (arXiv:2303.11448v1 [hep-lat])

    [http://arxiv.org/abs/2303.11448](http://arxiv.org/abs/2303.11448)

    本文研究了栅格规范等变卷积神经网络的全局群等变性，并提供了一个几何公式，表明L-CNN中的卷积是SU($N$)主丛上规范等变神经网络的一个特例。

    

    栅格规范等变卷积神经网络(L-CNNs)是卷积神经网络的框架，可应用于非阿贝尔栅格规范理论，而不违反规范对称性。我们演示了如何将L-CNNs配备全局群等变性。这使我们能够将公式扩展为既等变于平移，又等变于全局纹格对称性，例如旋转和反射。此外，我们提供了一个L-CNN的几何公式，并显示L-CNN中的卷积是SU($N$)主丛上规范等变神经网络的特例。

    Lattice gauge equivariant convolutional neural networks (L-CNNs) are a framework for convolutional neural networks that can be applied to non-Abelian lattice gauge theories without violating gauge symmetry. We demonstrate how L-CNNs can be equipped with global group equivariance. This allows us to extend the formulation to be equivariant not just under translations but under global lattice symmetries such as rotations and reflections. Additionally, we provide a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as a special case of gauge equivariant neural networks on SU($N$) principal bundles.
    
[^14]: 利用辅助不确定性进行的运算器学习来解决高维逆问题

    Solving High-Dimensional Inverse Problems with Auxiliary Uncertainty via Operator Learning with Limited Data. (arXiv:2303.11379v1 [stat.ML])

    [http://arxiv.org/abs/2303.11379](http://arxiv.org/abs/2303.11379)

    本文提出了一个基于深度神经网络代理模型进行流图校准的框架， 并结合先前辅助过程的知识来解决高维逆问题，成功识别了高维系统中的源。

    

    在复杂的大规模系统（如气候）中，重要的影响是由多种未被完全观察到的混淆过程相互作用引起的。根据系统状态的观察结果来识别出这些影响因素对于归因和预测至关重要，这些结果又为决策制定提供了重要的信息。这些逆问题的难点在于无法隔离影响因素和计算模型模拟的成本。我们引入了一个基于深度神经网络代理模型进行流图校准，并结合先前辅助过程的知识来解决有限数据下的高维逆问题的框架。我们的方法成功地在观察数据稀疏噪声较大的高维系统中确定出源，表现优于现有有限数据源识别方法。

    In complex large-scale systems such as climate, important effects are caused by a combination of confounding processes that are not fully observable. The identification of sources from observations of system state is vital for attribution and prediction, which inform critical policy decisions. The difficulty of these types of inverse problems lies in the inability to isolate sources and the cost of simulating computational models. Surrogate models may enable the many-query algorithms required for source identification, but data challenges arise from high dimensionality of the state and source, limited ensembles of costly model simulations to train a surrogate model, and few and potentially noisy state observations for inversion due to measurement limitations. The influence of auxiliary processes adds an additional layer of uncertainty that further confounds source identification. We introduce a framework based on (1) calibrating deep neural network surrogates to the flow maps provided 
    
[^15]: 广义划分局部深度

    Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])

    [http://arxiv.org/abs/2303.10167](http://arxiv.org/abs/2303.10167)

    本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。

    

    本文提供了一个最近由Berenhaut、Moore和Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]提出的凝聚概念的概括。所提出的表述基于分区局部深度的技术并提炼了两个关键概率概念：局部相关性和支持分割。早期结果在新的背景下得到扩展，并包括在具有不确定性的数据中揭示社区的应用示例。

    In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
    
[^16]: 从贝叶斯决策理论的角度看待长尾分类

    Long-tailed Classification from a Bayesian-decision-theory Perspective. (arXiv:2303.06075v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06075](http://arxiv.org/abs/2303.06075)

    本文从贝叶斯决策理论的角度提出了一个通用且有原则的框架，为长尾分类提供了理论上的支持，并采用综合风险和贝叶斯深度集成方法以提高所有类别的准确性，特别是“长尾”类别。

    

    长尾分类由于类别概率的严重不平衡和对称错误预测成本存在尾部敏感风险而面临挑战。最近的尝试采用重新平衡损失和集成方法，但它们很大程度上是启发式的，并且严重依赖经验结果，缺乏理论解释。此外，现有方法忽略了决策损失，它刻画了与尾部类别相关的不同成本。本文提出了一个从贝叶斯决策理论的角度看待长尾分类的通用且有原则的框架，它统一了包括重新平衡和集成方法在内的现有技术，并为它们的有效性提供了理论上的证明。从这个角度看，我们基于综合风险导出了一个新的目标和一个贝叶斯深度集成方法，以提高所有类别的准确性，特别是“长尾”类别的准确性。此外，我们的框架允许任务自适应决策损失，从而提供了在不同任务中可证明的最优决策。

    Long-tailed classification poses a challenge due to its heavy imbalance in class probabilities and tail-sensitivity risks with asymmetric misprediction costs. Recent attempts have used re-balancing loss and ensemble methods, but they are largely heuristic and depend heavily on empirical results, lacking theoretical explanation. Furthermore, existing methods overlook the decision loss, which characterizes different costs associated with tailed classes. This paper presents a general and principled framework from a Bayesian-decision-theory perspective, which unifies existing techniques including re-balancing and ensemble methods, and provides theoretical justifications for their effectiveness. From this perspective, we derive a novel objective based on the integrated risk and a Bayesian deep-ensemble approach to improve the accuracy of all classes, especially the "tail". Besides, our framework allows for task-adaptive decision loss which provides provably optimal decisions in varying task
    
[^17]: 限制半正定矩阵的Karcher均值的统计分析

    Statistical Analysis of Karcher Means for Random Restricted PSD Matrices. (arXiv:2302.12426v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.12426](http://arxiv.org/abs/2302.12426)

    本文通过研究限制半正定矩阵流形上的本质均值模型及其Karcher均值的统计分析，提出了一般外部信号加噪声模型下的确定性误差界，并表明LRC-dPCA算法与全样本PCA算法性能相同。

    

    现代几何感知机器学习算法通常由于可能复杂的非线性流形结构而缺乏非渐进统计分析。本文在限制半正定矩阵流形上研究本质均值模型，并提供Karcher均值的非渐进统计分析。我们还考虑了一般的外部信号加噪声模型，在此模型下提供了Karcher均值的确定性误差界。作为一个应用，我们展示分布式主成分分析算法LRC-dPCA的性能与全样本PCA算法相同。数值实验充分支持我们的理论。

    Non-asymptotic statistical analysis is often missing for modern geometry-aware machine learning algorithms due to the possibly intricate non-linear manifold structure. This paper studies an intrinsic mean model on the manifold of restricted positive semi-definite matrices and provides a non-asymptotic statistical analysis of the Karcher mean. We also consider a general extrinsic signal-plus-noise model, under which a deterministic error bound of the Karcher mean is provided. As an application, we show that the distributed principal component analysis algorithm, LRC-dPCA, achieves the same performance as the full sample PCA algorithm. Numerical experiments lend strong support to our theories.
    
[^18]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^19]: 带有卷积高斯神经过程的环境传感器放置

    Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10381](http://arxiv.org/abs/2211.10381)

    本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。

    

    环境传感器对于监测天气和气候变化的影响至关重要。然而，在像南极这样的偏远地区，最大化测量信息和有效放置传感器是具有挑战性的。概率机器学习模型可以通过预测新传感器提供的不确定性减少来评估放置信息。高斯过程模型广泛用于此目的，但难以捕捉复杂的非平稳行为并缩放到大型数据集。本文提出使用卷积高斯神经过程（ConvGNP）来解决这些问题。ConvGNP使用神经网络来参数化任意目标位置的联合高斯分布，实现了灵活性和可扩展性。使用模拟的南极地区地面温度异常作为真实数据，ConvGNP学习了空间和季节性非平稳性，并优于非平稳GP基线。在模拟的s中，

    Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
    
[^20]: 带装载和覆盖约束的上下文幸存者问题：基于回归的模块化Lagrangian方法

    Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07484](http://arxiv.org/abs/2211.07484)

    该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。

    

    我们考虑一种上下文幸存者问题的变种，其中算法在总消费的线性约束下使用多个资源。这个问题推广了带背包的上下文幸存者问题(CBwK)，允许装载和覆盖约束，以及正负资源消耗。我们提出了一种新算法，简单、计算效率高，能够实现退化的后悔。当某些约束被违反时，对于CBwK，它在统计上是最优的。我们的算法基于LagrangianBwK(Immorlica等人，FOCS 2019)，这是一种面向CBwK的Lagrangian技术，以及SquareCB(Foster和Rakhlin，ICML 2020)，这是一种面向上下文幸存者的回归技术。我们的分析利用了两种技术本质上的模块化。

    We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
    
[^21]: 因果发现后的有效推断

    Valid Inference after Causal Discovery. (arXiv:2208.05949v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.05949](http://arxiv.org/abs/2208.05949)

    本研究开发了工具以实现因果发现后的有效推断，解决了使用相同数据运行因果发现算法后估计因果效应导致经典置信区间的覆盖保证无效问题。

    

    因果发现和因果效应估计是因果推断中的两个基本任务。虽然已经针对每个任务单独开发了许多方法，但是同时应用这些方法时会出现统计上的挑战：在对相同数据运行因果发现算法后估计因果效应会导致"双重挑选"，从而使经典置信区间的覆盖保证无效。为此，我们开发了针对因果发现后有效的推断工具。通过实证研究，我们发现，天真组合因果发现算法和随后推断算法会导致高度膨胀的误覆盖率，而应用我们的方法则提供可靠的覆盖并实现比数据分割更准确的因果发现。

    Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to "double dipping," invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while achieving more accurate causal discovery than data splitting.
    
[^22]: 用统计和机器学习打击洗钱：综述与介绍

    Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.04207](http://arxiv.org/abs/2201.04207)

    本文介绍了银行反洗钱的统计和机器学习方法，并提出客户风险评估和可疑行为标识两个核心要素。未来的研究方向包括生成合成数据、半监督和深度学习、可解释性以及结果的公平性。

    

    洗钱是一个严重的全球性问题，但是针对反洗钱的统计和机器学习方法的科学文献却很少。本文着重于银行反洗钱，并提供了文献综述和介绍。我们提出了一个统一的术语，其中包括客户风险评估和可疑行为标识两个核心要素。我们发现，客户风险评估是通过诊断来寻找和解释风险因素，而可疑行为标识则是通过未公开的特征和手工风险指数来实现的。最后，我们讨论了未来研究的方向，其中主要挑战之一是需要更多的公共数据集，这可能可以通过生成合成数据来解决，其他可能的研究方向包括半监督和深度学习、可解释性以及结果的公平性。

    Money laundering is a profound global problem. Nonetheless, there is little scientific literature on statistical and machine learning methods for anti-money laundering. In this paper, we focus on anti-money laundering in banks and provide an introduction and review of the literature. We propose a unifying terminology with two central elements: (i) client risk profiling and (ii) suspicious behavior flagging. We find that client risk profiling is characterized by diagnostics, i.e., efforts to find and explain risk factors. On the other hand, suspicious behavior flagging is characterized by non-disclosed features and hand-crafted risk indices. Finally, we discuss directions for future research. One major challenge is the need for more public data sets. This may potentially be addressed by synthetic data generation. Other possible research directions include semi-supervised and deep learning, interpretability, and fairness of the results.
    
[^23]: 弱凸和多凸代理函数的随机正则化主导极小化算法。

    Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates. (arXiv:2201.01652v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2201.01652](http://arxiv.org/abs/2201.01652)

    本文提出了一个SMM的扩展，该扩展支持代理函数的弱凸性和块多凸性，并在解决非凸约束问题上具有显著优越性能。

    

    随机主导极小化（SMM）是一类采样新数据点并最小化目标函数的代理函数的递归平均数的优化算法。要求代理函数是强凸的，非凸情况下的收敛率分析并不可行。该论文提出了一个SMM的扩展，该扩展允许代理函数仅为弱凸或块多凸，并且平均代理函数在近端正则化或块最小化的减小半径中近似最小化。实验证明，该算法在凸约束下处理非i.i.d.数据样本的一阶最优性差距以速率$O((\log n)^{1+\epsilon}/n^{1/2})$下降，期望损失为$O((\log n)^{1+\epsilon}/n^{1/4})$，其中$n$表示处理的数据样本数量。在一些额外的假设下，还提供了目标函数的后期收敛速率。该方法在多个真实数据集上得到展示，并与最先进的方法进行了比较。

    Stochastic majorization-minimization (SMM) is a class of stochastic optimization algorithms that proceed by sampling new data points and minimizing a recursive average of surrogate functions of an objective function. The surrogates are required to be strongly convex and convergence rate analysis for the general non-convex setting was not available. In this paper, we propose an extension of SMM where surrogates are allowed to be only weakly convex or block multi-convex, and the averaged surrogates are approximately minimized with proximal regularization or block-minimized within diminishing radii, respectively. For the general nonconvex constrained setting with non-i.i.d. data samples, we show that the first-order optimality gap of the proposed algorithm decays at the rate $O((\log n)^{1+\epsilon}/n^{1/2})$ for the empirical loss and $O((\log n)^{1+\epsilon}/n^{1/4})$ for the expected loss, where $n$ denotes the number of data samples processed. Under some additional assumption, the lat
    
[^24]: 带噪声特征的上下文线性Bandit：朝向贝叶斯神谕前进

    Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1703.01347](http://arxiv.org/abs/1703.01347)

    本论文研究了具有噪声特征的上下文线性Bandit问题。我们提出了一个算法，通过观察信息，实现了贝叶斯神谕并得到了$\tilde{O}(d\sqrt{T})$的遗憾界。

    

    我们研究了带有噪声和缺失项的上下文线性Bandit问题。为了解决噪声的挑战，我们分析了在观测噪声特征的情况下给出的贝叶斯神谕。我们的贝叶斯分析发现，最优假设可能会远离潜在的可实现函数，这取决于噪声特征，这是高度非直观的，并且在经典的无噪声设置下不会发生。这意味着经典方法不能保证非平凡的遗憾界（regret bound）。因此，我们提出了一个算法，旨在从这个模型下的观察信息中实现贝叶斯神谕，当有大量手臂时，可以实现$\tilde{O}(d\sqrt{T})$遗憾界。我们使用合成和实际数据集演示了所提出的算法。

    We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
    

