# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss](https://arxiv.org/abs/2402.05928) | 本文研究了依赖学习理论中的尖锐率，主要是为了避免样本大小缩减对方差产生影响。当假设类别的拓扑结构符合某些条件时，经验风险最小化者的性能与类别的复杂性和二阶统计量有关。 |
| [^2] | [Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits](https://arxiv.org/abs/2402.05878) | 本论文研究了在结构化赌博机中的贝叶斯固定预算最佳臂识别问题，提出了一种基于先验信息的固定分配算法，并引入了新的证明方法，以得到更紧密的多臂BAI界限。该方法在各种情况下展现出一致且稳健的性能，加深了我们对于该问题的理解。 |
| [^3] | [Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices](https://arxiv.org/abs/2402.05876) | 本研究探索了联邦学习在离线强化学习中的应用，设计了一种适用于联邦离线强化学习的无模型Q-learning算法FedLCB-Q。通过合作利用多个代理的离线数据集，并使用特定的学习速率调度和聚合方法，FedLCB-Q实现了线性加速。 |
| [^4] | [Let Your Graph Do the Talking: Encoding Structured Data for LLMs](https://arxiv.org/abs/2402.05862) | 本论文介绍了一种参数高效的编码方法，可以使大型语言模型（LLMs）能够显式地表示结构化数据，并在图推理任务中取得了显著改进。 |
| [^5] | [How Much is Unseen Depends Chiefly on Information About the Seen](https://arxiv.org/abs/2402.05835) | 该论文发现，在未知种群中属于未在训练数据中出现的类的数据点的比例几乎完全取决于训练数据中出现相同次数的类的数量。论文提出了一个遗传算法，能够根据样本找到一个具有最小均方误差的估计量。 |
| [^6] | [On Calibration and Conformal Prediction of Deep Classifiers](https://arxiv.org/abs/2402.05806) | 本文研究了温度缩放对符合预测方法的影响，通过实证研究发现，校准对自适应C方法产生了有害的影响。 |
| [^7] | [Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence](https://arxiv.org/abs/2402.05802) | 通过无监督机器学习和概率独立性方法，我们发现了2000个潜在源的临床特征，这些特征在肺癌预测中的判别能力优于原始变量，并且能够识别未被诊断的癌症的特征。 |
| [^8] | [How do Transformers perform In-Context Autoregressive Learning?](https://arxiv.org/abs/2402.05787) | 本文研究了Transformers在上下文自回归学习中的表现，并通过训练模型发现了其预测下一个标记的过程。针对不同情况，我们证明了单层线性Transformer实现了梯度下降以及正交性之间的关系。 |
| [^9] | [Latent variable model for high-dimensional point process with structured missingness](https://arxiv.org/abs/2402.05758) | 本文提出了一种针对高维点过程的带有结构缺失的灵活高效的潜变量模型，利用高斯过程捕获时间相关性，并开发了可扩展的变分推理方法进行训练。 |
| [^10] | [Implicit Bias and Fast Convergence Rates for Self-attention](https://arxiv.org/abs/2402.05738) | 该论文研究了在自注意力网络中使用梯度下降训练的隐性偏差以及其收敛速率，通过证明在特定数据设置下收敛性是全局的，并提供了W_t到W_mm的有限时间收敛率。 |
| [^11] | [Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL](https://arxiv.org/abs/2402.05724) | 本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。 |
| [^12] | [Exact capacity of the \emph{wide} hidden layer treelike neural networks with generic activations](https://arxiv.org/abs/2402.05719) | 该论文研究了宽隐藏层树状神经网络的容量，采用全面提升的随机二重性理论(fl RDT)来对宽(TCM)网络的容量进行刻画，得到了一类通用激活函数的显式、闭式容量计算公式。 |
| [^13] | [REMEDI: Corrective Transformations for Improved Neural Entropy Estimation](https://arxiv.org/abs/2402.05718) | REMEDI是一种用于改进神经熵估计的校正转换方法，通过交叉熵最小化和相对熵估计基模型的偏差，提高了估计任务的准确性和效率。 |
| [^14] | [Collaborative non-parametric two-sample testing](https://arxiv.org/abs/2402.05715) | 本文提出了协同非参数的两样本检验（CTST）框架，该框架有效利用了图结构和最小化了对概率密度函数的假设，通过集成f-分布估计、核方法和多任务学习的要点，将传统的在每个节点独立应用的方法优化，对图结构中的多个两样本检验问题进行了更好的处理和分析。 |
| [^15] | [Fixed width treelike neural networks capacity analysis -- generic activations](https://arxiv.org/abs/2402.05696) | 这篇论文研究了树状神经网络的容量，基于Random Duality Theory提出了通用的容量分析框架，并证明该框架适用于其他类型的激活函数，如二次和ReLU函数。 |
| [^16] | [A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs](https://arxiv.org/abs/2402.05674) | 本论文研究了高维模型中的对抗训练，引入了一个可处理的数学模型，并给出了对抗性经验风险最小化器的充分统计的精确渐近描述。研究结果表明存在可以防御而不惩罚准确性的方向，揭示了防御非鲁棒特征的优势。 |
| [^17] | [Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients](https://arxiv.org/abs/2402.05639) | 本文提出了一种通过随机近似梯度最小化投影群体风险的新型非参数仪器变量回归框架，并通过理论和实证实验证明了其竞争性能。此外，本文还处理了二元结果的情况，取得了有希望的结果。 |
| [^18] | [Hypergraph Node Classification With Graph Neural Networks](https://arxiv.org/abs/2402.05569) | 本研究提出了一种简单高效的框架，利用加权子图扩展的图神经网络(WCE-GNN)实现了超图节点分类。实验证明，WCE-GNN具有优秀的预测效果和较低的计算复杂度。 |
| [^19] | [Differentially Private Model-Based Offline Reinforcement Learning](https://arxiv.org/abs/2402.05525) | 本研究提出了一种差分隐私的基于模型的离线强化学习方法，通过学习离线数据中的隐私模型以及基于模型的策略优化，实现了从离线数据中训练具有隐私保护的强化学习代理。同时，研究还总结了在这种设置下隐私的代价。 |
| [^20] | [Low-degree phase transitions for detecting a planted clique in sublinear time](https://arxiv.org/abs/2402.05451) | 在高信号区域中，研究了子线性时间内检测种植团体的算法。通过非适应性低度算法，证明了在一定尺度下可以检测到团体，并且显示了该类算法的计算相变。 |
| [^21] | [Learning Uncertainty-Aware Temporally-Extended Actions](https://arxiv.org/abs/2402.05439) | 我们提出了一种名为不确定性感知时间扩展（UTE）的算法，在强化学习中解决了动作重复可能降低性能的问题，通过测量不确定性从而让策略根据需求进行选择，实验证明UTE优于现有算法。 |
| [^22] | [Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data](https://arxiv.org/abs/2402.05401) | 本研究通过研究自适应激活函数在稀疏实验数据预测建模中的应用，填补了当前对其影响了解不足的重要差距，并揭示出个体可调参数的自适应激活函数在预测模型中具有较高的准确性。 |
| [^23] | [Tradeoffs of Diagonal Fisher Information Matrix Estimators](https://arxiv.org/abs/2402.05379) | 本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。 |
| [^24] | [Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference](https://arxiv.org/abs/2402.05330) | 该论文提出了一种在广义标签转移和干扰参数下进行分类的新方法。通过将分类问题视为带有干扰参数的假设检验问题，我们能够有效地估计分类器在整个干扰参数空间中的接收者操作特性，并设计出在广义标签转移下不变的截断点，从而实现了对事件的可靠分类和不确定性度量。 |
| [^25] | [Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks](https://arxiv.org/abs/2402.05271) | 了解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。前人的研究表明，在训练过程中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这被称为神经特征分析（NFA）。本研究解释了这种相关性的出现，并发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。在早期训练阶段，可以通过解析的方式预测NFA的发展速度。 |
| [^26] | [On Parameter Estimation in Deviated Gaussian Mixture of Experts](https://arxiv.org/abs/2402.05220) | 在偏离高斯混合专家模型中，本文通过构造Voronoi-based损失函数来解决参数估计问题。 |
| [^27] | [Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models](https://arxiv.org/abs/2402.05210) | 这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。 |
| [^28] | [Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series](https://arxiv.org/abs/2402.05203) | 贝尔曼符合推断（BCI）是一个框架，通过解决一维随机控制问题，利用多步预测来提供校准的时间序列预测区间。BCI在任意分布转换和时间依赖性下实现了长期覆盖，且在波动率预测问题上生成更短的预测区间。 |
| [^29] | [Meta-learning the mirror map in policy mirror descent](https://arxiv.org/abs/2402.05187) | 该论文通过实证研究发现，传统的镜像映射选择（NPG）在标准基准环境中常常导致不理想的结果。通过元学习方法，找到了更高效的镜像映射，提升了性能。 |
| [^30] | [Towards Understanding Inductive Bias in Transformers: A View From Infinity](https://arxiv.org/abs/2402.05173) | 本文研究了Transformer模型的归纳偏差，并发现它们倾向于对称排列函数，对称群的表示理论可以用于分析预测，同时提出了一个简化模型来解决学习曲线和网络输出，并在常见设置中得出学习能力的紧密边界，最后还证明了WikiText数据集具有排列对称性。 |
| [^31] | [Online Learning Approach for Survival Analysis](https://arxiv.org/abs/2402.05145) | 本论文介绍了一种在线数学框架，用于实时适应动态环境和有审查数据的生存分析，通过在线牛顿步骤(ONS)算法估计事件时间分布，并提出了保证ONS具有对数随机遗憾界的随机方法和自适应聚合方法。 |
| [^32] | [Continuous Multidimensional Scaling](https://arxiv.org/abs/2402.04436) | 连续多维标度是关于将距离信息嵌入欧几里得空间的过程，并探讨了在对象集不断增加的情况下，将整个嵌入问题序列视为一个固定空间中的一系列优化问题的方法和结论。 |
| [^33] | [A General Theory for Kernel Packets: from state space model to compactly supported basis](https://arxiv.org/abs/2402.04022) | 该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。 |
| [^34] | [Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking](https://arxiv.org/abs/2402.02152) | 这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。 |
| [^35] | [When accurate prediction models yield harmful self-fulfilling prophecies](https://arxiv.org/abs/2312.01210) | 本研究通过调查预测模型的部署对决策产生有害影响的情况，发现这些模型可能会成为有害的自我实现预言。这些模型不会因为对某些患者造成更糟糕的结果而使其预测能力变无效。 |
| [^36] | [Training Overparametrized Neural Networks in Sublinear Time](https://arxiv.org/abs/2208.04508) | 这项研究提出了一种新的训练方法，可以在次线性时间内训练过参数化的神经网络，提高了训练的效率。 |
| [^37] | [Personalized PCA: Decoupling Shared and Unique Features](https://arxiv.org/abs/2207.08041) | 本文介绍了个性化PCA（PerPCA），通过使用全局和局部主成分来编码独特和共享特征，解决了PCA面临的异质性挑战。我们证明，在温和的条件下，我们可以通过受约束优化问题识别和恢复出独特和共享的特征。我们还设计了一个联邦算法来解决这个问题，并证明了算法的线性收敛性。 |
| [^38] | [The Fairness of Credit Scoring Models](https://arxiv.org/abs/2205.10200) | 本论文研究了信用评分模型的公平性问题，提出了一种形式化测试算法公平性的方法，并探索了影响公平性的变量。研究结果可以指导信贷商监测算法公平性、监管机构控制公平性，同时提高受保护群体的利益，同时保持高水平的预测准确性。 |
| [^39] | [Spectral Clustering with Variance Information for Group Structure Estimation in Panel Data](https://arxiv.org/abs/2201.01793) | 该论文提出了使用方差信息进行面板数据中群组结构估计的光谱聚类方法。通过局部分析发现，个体系数估计的方差包含了估计群组结构所需的有用信息。该方法不仅适用于面板数据模型，还适用于只提供参数估计和估计不确定度的情况。通过模拟研究证明，该方法表现出更好的性能。 |
| [^40] | [Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation](https://arxiv.org/abs/2106.04096) | 本文研究了具有线性函数逼近的熵正则化自然策略梯度的收敛性质，证明了其在正则化马尔可夫决策过程中具有线性收敛性以及快速的收敛速率。 |
| [^41] | [Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons.](http://arxiv.org/abs/2401.16571) | 我们提出了一种使用共享神经元的RBF网络的非参数化治疗效应估计方法，适用于多治疗设置。该方法能够建模治疗结果的共同性，并在贝叶斯框架下实现估计和推断，通过模拟实验证明了其数值性能，应用于真实临床数据后也得到了有趣的发现。 |
| [^42] | [Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning.](http://arxiv.org/abs/2401.03756) | 该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。 |
| [^43] | [DiffEnc: Variational Diffusion with a Learned Encoder.](http://arxiv.org/abs/2310.19789) | DiffEnc是一种使用学习的编码器的变分扩散模型，通过引入数据和深度相关的均值函数和可调节的噪声方差比率，实现了最先进的可能性。 |
| [^44] | [Discovering Mixtures of Structural Causal Models from Time Series Data.](http://arxiv.org/abs/2310.06312) | 这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。 |
| [^45] | [On Wasserstein distances for affine transformations of random vectors.](http://arxiv.org/abs/2310.03945) | 本研究阐述了关于随机向量之间的Wasserstein距离的下界，重点是仿射变换，并应用于流型学习和手写数据集模拟。 |
| [^46] | [Optimal partitioning of directed acyclic graphs with dependent costs between clusters.](http://arxiv.org/abs/2308.03970) | 本论文提出了一种名为DCMAP的算法，用于对具有依赖成本的有向无环图进行最优分区。该算法通过优化基于DAG和集群映射的成本函数来寻找所有最优集群，并在途中返回接近最优解。实验证明在复杂系统的DBN模型中，该算法具有时间效率性。 |
| [^47] | [Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques.](http://arxiv.org/abs/2307.12971) | 本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。 |
| [^48] | [A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models.](http://arxiv.org/abs/2307.05251) | 本研究提出了一种随机优化方法，用于解决稳健密度功率分歧（DPD）在一般参数密度模型中的计算复杂性问题，并通过应用传统的随机优化理论来验证其有效性。 |
| [^49] | [On the sample complexity of estimation in logistic regression.](http://arxiv.org/abs/2307.04191) | 本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。 |
| [^50] | [Incentive-Theoretic Bayesian Inference for Collaborative Science.](http://arxiv.org/abs/2307.03748) | 本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。 |
| [^51] | [Multi-Task Learning with Summary Statistics.](http://arxiv.org/abs/2307.02388) | 提出了一种利用汇总统计数据的灵活多任务学习框架，可解决在真实世界设置中数据共享限制的问题。通过自适应参数选择方法和系统非渐近分析，提高了模型性能。通过大量模拟实验证明了方法的有效性。 |
| [^52] | [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection.](http://arxiv.org/abs/2306.01710) | 本文提出了一种基于数据驱动的相对不确定性度量，用于误分类检测。该度量可以通过学习软预测的分布模式，识别出被误分类的样本，并展示了在多个图像分类任务中的实证改进，优于现有的误分类检测方法。 |
| [^53] | [Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape.](http://arxiv.org/abs/2305.19510) | 本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。 |
| [^54] | [Convergence of Alternating Gradient Descent for Matrix Factorization.](http://arxiv.org/abs/2305.06927) | 本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。 |
| [^55] | [Differential geometry with extreme eigenvalues in the positive semidefinite cone.](http://arxiv.org/abs/2304.07347) | 本文提出了一种基于半定锥中的汤普森几何学的可扩展几何框架，利用极广义特征值有效地分析和处理对称正定矩阵数据。同时，基于此几何方法，定义了一种新型 SPD 矩阵迭代平均算法，证明了其存在性和唯一性。 |
| [^56] | [Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons.](http://arxiv.org/abs/2301.11270) | 该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。 |
| [^57] | [Instance-dependent uniform tail bounds for empirical processes.](http://arxiv.org/abs/2209.10053) | 该论文提出了一个经验过程的统一尾部界，该尾部界以函数的个体偏差而不是在考虑的类中的最坏情况偏差为基础。 |
| [^58] | [The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information.](http://arxiv.org/abs/2102.10019) | 本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。 |

# 详细

[^1]: 依赖学习理论中的尖锐率：避免样本大小缩减的平方损失

    Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss

    [https://arxiv.org/abs/2402.05928](https://arxiv.org/abs/2402.05928)

    本文研究了依赖学习理论中的尖锐率，主要是为了避免样本大小缩减对方差产生影响。当假设类别的拓扑结构符合某些条件时，经验风险最小化者的性能与类别的复杂性和二阶统计量有关。

    

    本文研究了具有依赖性（β-混合）数据和平方损失的统计学习，在一个假设类别Φ_p的子集F中，其中Φ_p是范数∥f∥_Φ_p≡sup_m≥1 m^{-1/p}∥f∥_L^m，其中p∈[2，∞]。我们的研究动机是在具有依赖性数据的学习中寻找尖锐的噪声交互项或方差代理。在没有任何可实现性假设的情况下，典型的非渐近结果显示出方差代理通过底层协变量过程的混合时间进行了乘积缩减。我们证明，只要在我们的假设类别F上，L^2和Φ_p的拓扑是可比较的，即Φ_p是一个弱亚高斯类别：∥f∥_Φ_p≲∥f∥_L^2^η，其中η∈(0，1]，经验风险最小化者在其主导项中只实现了一种只依赖于类别复杂性和二阶统计量的速率。我们的结果适用于许多依赖性数据模型。

    In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether t
    
[^2]: 基于先验依赖分配的结构化赌博机中贝叶斯固定预算最佳臂识别

    Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits

    [https://arxiv.org/abs/2402.05878](https://arxiv.org/abs/2402.05878)

    本论文研究了在结构化赌博机中的贝叶斯固定预算最佳臂识别问题，提出了一种基于先验信息的固定分配算法，并引入了新的证明方法，以得到更紧密的多臂BAI界限。该方法在各种情况下展现出一致且稳健的性能，加深了我们对于该问题的理解。

    

    我们研究了在结构化赌博机中的贝叶斯固定预算最佳臂识别（BAI）问题。我们提出了一种算法，该算法基于先验信息和环境结构使用固定分配。我们在多个模型中提供了它在性能上的理论界限，包括线性和分层BAI的首个先验依赖上界。我们的主要贡献是引入了新的证明方法，相比现有方法，它能得到更紧密的多臂BAI界限。我们广泛比较了我们的方法与其他固定预算BAI方法，在各种设置中展示了其一致且稳健的性能。我们的工作改进了对于结构化赌博机中贝叶斯固定预算BAI的理解，并突出了我们的方法在实际场景中的有效性。

    We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios.
    
[^3]: 联邦离线强化学习：合作单一策略即可覆盖

    Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices

    [https://arxiv.org/abs/2402.05876](https://arxiv.org/abs/2402.05876)

    本研究探索了联邦学习在离线强化学习中的应用，设计了一种适用于联邦离线强化学习的无模型Q-learning算法FedLCB-Q。通过合作利用多个代理的离线数据集，并使用特定的学习速率调度和聚合方法，FedLCB-Q实现了线性加速。

    

    离线强化学习通过使用离线数据来学习最优策略，在无法在线收集数据或成本高昂的关键应用中引起了广泛关注。本研究探讨了联邦学习在离线强化学习中的好处，并旨在合作利用多个代理的离线数据集。针对有限时段的表格化马尔可夫决策过程(MDP)，我们设计了FedLCB-Q，这是一种针对联邦离线强化学习量身定制的流行的无模型Q-learning算法的变种。FedLCB-Q使用新颖的学习速率调度在代理处更新本地Q函数，并使用重要性平均和精心设计的悲观惩罚项在中央服务器上聚合它们。我们的样本复杂度分析表明，在适当选择的参数和同步时间表下，FedLCB-Q在代理数量上实现了线性加速，而不需要个别代理拥有高质量的数据集。

    Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual age
    
[^4]: 让你的图来说话：为LLMs编码结构化数据

    Let Your Graph Do the Talking: Encoding Structured Data for LLMs

    [https://arxiv.org/abs/2402.05862](https://arxiv.org/abs/2402.05862)

    本论文介绍了一种参数高效的编码方法，可以使大型语言模型（LLMs）能够显式地表示结构化数据，并在图推理任务中取得了显著改进。

    

    我们如何最有效地将结构化数据编码成序列形式，以供大型语言模型（LLMs）使用？在这项工作中，我们介绍了一种参数高效的方法，可以明确表示LLMs的结构化数据。我们的方法，GraphToken，学习了一种编码函数，以显式结构化信息扩展提示语。与其他专注于有限领域（例如知识图表示）的工作不同，我们的工作是首次针对一般结构化数据编码进行研究，用于各种推理任务。我们展示了明确表示图结构可以显著改进图推理任务。具体来说，我们在GraphQA基准测试中看到了整体的改进 - 在节点、边和图级任务上高达73%的改进。

    How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.
    
[^5]: 不可见数据取决于已知信息的多少

    How Much is Unseen Depends Chiefly on Information About the Seen

    [https://arxiv.org/abs/2402.05835](https://arxiv.org/abs/2402.05835)

    该论文发现，在未知种群中属于未在训练数据中出现的类的数据点的比例几乎完全取决于训练数据中出现相同次数的类的数量。论文提出了一个遗传算法，能够根据样本找到一个具有最小均方误差的估计量。

    

    乍一看可能有些违反直觉：我们发现，在预期中，未知种群中属于在训练数据中没有出现的类的数据点的比例几乎完全由训练数据中出现相同次数的类的数量$f_k$确定。虽然在理论上我们证明了由该估计量引起的偏差在样本大小指数级衰减，但在实践中，高方差阻止我们直接使用它作为样本覆盖估计量。但是，我们对$f_k$之间的依赖关系进行了精确的描述，从而产生了多个不同期望值表示的搜索空间，可以确定地实例化为估计量。因此，我们转向优化，并开发了一种遗传算法，仅根据样本搜索平均均方误差（MSE）最小的估计量。在我们的实验证明，我们的遗传算法发现了具有明显较小方差的估计量。

    It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times. While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. However, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE). In our experiments, our genetic algorithm discovers estimators that have a substantially smalle
    
[^6]: 关于深度分类器的校准和符合预测研究

    On Calibration and Conformal Prediction of Deep Classifiers

    [https://arxiv.org/abs/2402.05806](https://arxiv.org/abs/2402.05806)

    本文研究了温度缩放对符合预测方法的影响，通过实证研究发现，校准对自适应C方法产生了有害的影响。

    

    在许多分类应用中，深度神经网络（DNN）基于分类器的预测需要伴随一些置信度指示。针对这个目标，有两种流行的后处理方法：1）校准：修改分类器的softmax值，使其最大值（与预测相关）更好地估计正确概率；和2）符合预测（CP）：设计一个基于softmax值的分数，从中产生一组预测，具有理论上保证正确类别边际覆盖的特性。尽管在实践中两种指示都可能是需要的，但到目前为止它们之间的相互作用尚未得到研究。为了填补这一空白，在本文中，我们研究了温度缩放，这是最常见的校准技术，对重要的CP方法的影响。我们首先进行了一项广泛的实证研究，其中显示了一些重要的洞察，其中包括令人惊讶的发现，即校准对流行的自适应C方法产生了有害的影响。

    In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive C
    
[^7]: 无监督发现临床疾病特征的概率独立性方法

    Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence

    [https://arxiv.org/abs/2402.05802](https://arxiv.org/abs/2402.05802)

    通过无监督机器学习和概率独立性方法，我们发现了2000个潜在源的临床特征，这些特征在肺癌预测中的判别能力优于原始变量，并且能够识别未被诊断的癌症的特征。

    

    临床疾病的诊断不够准确可能导致很多治疗失败的情况，即使是常见疾病和治疗。通过使用足够大的数据集，可以使用无监督机器学习来更精确地定义临床疾病模式。我们提出了一种利用概率独立性来解开疾病潜在源因对医疗记录的影响的方法。我们从269,099份电子健康记录中的9195个变量中推断出了2000个潜在源的临床特征。这些学习到的特征在一个肺癌预测任务中比原始变量有更好的判别能力，该任务对推断算法来说是未知的，能够预测出无癌症历史的患者在发现孤立的肺结节之前的3年内的恶性疾病。更重要的是，这些特征的解释能力更强，能够识别出许多患者中明显未被诊断的癌症的结节前特征。

    Insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments. With a large enough dataset, it may be possible to use unsupervised machine learning to define clinical disease patterns more precisely. We present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease. We inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 Electronic Health Records. The learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered. More importantly, the signatures' greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients.
    
[^8]: Transformers在上下文自回归学习中的表现如何？

    How do Transformers perform In-Context Autoregressive Learning?

    [https://arxiv.org/abs/2402.05787](https://arxiv.org/abs/2402.05787)

    本文研究了Transformers在上下文自回归学习中的表现，并通过训练模型发现了其预测下一个标记的过程。针对不同情况，我们证明了单层线性Transformer实现了梯度下降以及正交性之间的关系。

    

    Transformers在语言建模任务中取得了最先进的性能。然而，它们取得巨大成功的原因还不清楚。本文通过在简单的下一个标记预测任务上训练Transformer模型，为了更好地理解这一问题。我们展示了训练后的Transformer如何通过首先在上下文中学习W，然后应用预测映射来预测下一个标记。我们称这个结果为上下文自回归学习。具体来说，我们针对W是交换正交矩阵的情况，首先证明了一个训练后的单层线性Transformer在考虑扩展标记的情况下实现一步梯度下降来最小化内部目标函数。当标记没有扩展时，我们对于一个单层对角线线性多头Transformer的全局最小值进行了表征。重要的是，我们展示了头部之间的正交性。

    Transformers have achieved state-of-the-art performance in language modeling tasks. However, the reasons behind their tremendous success are still unclear. In this paper, towards a better understanding, we train a Transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained Transformer predicts the next token by first learning $W$ in-context, then applying a prediction mapping. We call the resulting procedure in-context autoregressive learning. More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer. Importantly, we exhibit orthogonality between heads and show that posi
    
[^9]: 高维点过程的带结构缺失的潜变量模型

    Latent variable model for high-dimensional point process with structured missingness

    [https://arxiv.org/abs/2402.05758](https://arxiv.org/abs/2402.05758)

    本文提出了一种针对高维点过程的带有结构缺失的灵活高效的潜变量模型，利用高斯过程捕获时间相关性，并开发了可扩展的变分推理方法进行训练。

    

    纵向数据在医疗保健、社会学和地震学等许多领域中具有重要意义，但是真实世界的数据集对从业人员来说存在明显的挑战，因为它们可能是高维的，包含有结构化的缺失模式，并且测量时间点可能受到未知随机过程的控制。尽管已经提出了各种解决方案，但其中大多数仅考虑了这些挑战中的一个。在这项工作中，我们提出了一种灵活高效的潜变量模型，能够应对所有这些限制。我们的方法利用高斯过程来捕获样本与其关联的缺失模式之间的时间相关性，同时也用于建模底层的点过程。我们将我们的模型构建为一个变分自动编码器，同时使用深度神经网络参数化的编码器和解码器模型，并开发了一个可扩展的变分推理方法来进行高效的模型训练。我们展示了这个模型在各个领域的竞争性能。

    Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. We demonstrate compe
    
[^10]: 隐性偏差与自注意力的快速收敛速率

    Implicit Bias and Fast Convergence Rates for Self-attention

    [https://arxiv.org/abs/2402.05738](https://arxiv.org/abs/2402.05738)

    该论文研究了在自注意力网络中使用梯度下降训练的隐性偏差以及其收敛速率，通过证明在特定数据设置下收敛性是全局的，并提供了W_t到W_mm的有限时间收敛率。

    

    自注意力是transformer的核心机制，它使其与传统神经网络有所区别，并驱动其出色的性能。为了开发自注意力的基本优化原则，我们研究了用梯度下降（GD）训练具有固定线性解码器的自注意力层在二元分类中的隐性偏差。受到在可分离数据上线性逻辑回归中GD的研究启发，最近的工作表明，随着迭代次数t无限接近于无穷大，键-查询矩阵W_t在局部上（相对于初始化方向）收敛到一个硬边界支持向量机解W_mm。我们的工作在四个方面增强了这个结果。首先，我们确定了非平凡的数据设置，对于这些设置，收敛性是全局的，并揭示了优化空间的特性。其次，我们首次提供了W_t到W_mm的有限时间收敛率，并量化了稀疏化的速率。

    Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in t
    
[^11]: 基于模型的强化学习在平均场博弈中并不比单个智能体强化学习更加困难

    Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL

    [https://arxiv.org/abs/2402.05724](https://arxiv.org/abs/2402.05724)

    本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。

    

    我们研究了在平均场博弈中基于模型的函数逼近下强化学习样本复杂度，该方法需要策略性探索以找到纳什均衡策略。我们引入了部分基于模型的Eluder维度（P-MBED），这是一种更有效的概念来描述模型类复杂度。值得注意的是，P-MBED可以衡量从给定的平均场模型类转换而来的单个智能体模型类的复杂度，并且潜在上可能比\citet{huang2023statistical}提出的MBED指数级低。我们提出了一种模型消除算法，具有新颖的探索策略，并建立了与P-MBED相关的样本复杂度结果，这些结果表明，在基本可实现性和Lipschitz连续性假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。我们进一步将我们的结果推广到多类型平均场博弈。

    We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
    
[^12]: 宽隐藏层树状神经网络中具有通用激活函数的容量准确性

    Exact capacity of the \emph{wide} hidden layer treelike neural networks with generic activations

    [https://arxiv.org/abs/2402.05719](https://arxiv.org/abs/2402.05719)

    该论文研究了宽隐藏层树状神经网络的容量，采用全面提升的随机二重性理论(fl RDT)来对宽(TCM)网络的容量进行刻画，得到了一类通用激活函数的显式、闭式容量计算公式。

    

    最近对于树状委员会机器(TCM)神经网络的研究表明，随机二重性理论(RDT)及其部分提升变种(pl RDT)是能够用于非常精确的网络容量分析的强大工具。在本文中，我们考虑了宽隐藏层网络，并发现\cite{Stojnictcmspnncapdiffactrdt23}中面临的某些数值困难奇迹般地消失了。特别是，我们采用最近发展的全面提升(fl) RDT来表征宽($d\rightarrow \infty$) TCM网络的容量。我们获得了一类非常通用的隐藏层激活函数的显式、闭式容量刻画。尽管所使用的方法显著降低了所需的数值评估量，但最终的fl RDT的实用性和成功仍然需要可靠的数值计算。

    Recent progress in studying \emph{treelike committee machines} (TCM) neural networks (NN) in \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \emph{fully lifted} (fl) RDT to characterize the \emph{wide} ($d\rightarrow \infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical 
    
[^13]: REMEDI: 改进神经熵估计的校正转换

    REMEDI: Corrective Transformations for Improved Neural Entropy Estimation

    [https://arxiv.org/abs/2402.05718](https://arxiv.org/abs/2402.05718)

    REMEDI是一种用于改进神经熵估计的校正转换方法，通过交叉熵最小化和相对熵估计基模型的偏差，提高了估计任务的准确性和效率。

    

    信息论量在机器学习中起着核心作用。数据和模型复杂性的增加使得准确估计这些量的需求增加。然而，随着维度的增加，估计存在重大挑战，现有方法在相对较低的维度中已经困难重重。为了解决这个问题，在这项工作中，我们引入了REMEDI，用于高效准确地估计微分熵，一种基本的信息论量。该方法结合了简单自适应基模型的交叉熵最小化和其相对熵从数据密度中估计的偏差。我们的方法在各种估计任务中得到了改进，包括对合成数据和自然数据的熵估计。此外，我们将重要的理论一致性结果扩展到我们方法所需的更广义的设置中。我们展示了我们的方法如何提高熵估计的准确性和效率。

    Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how
    
[^14]: 协同非参数的两样本检验

    Collaborative non-parametric two-sample testing

    [https://arxiv.org/abs/2402.05715](https://arxiv.org/abs/2402.05715)

    本文提出了协同非参数的两样本检验（CTST）框架，该框架有效利用了图结构和最小化了对概率密度函数的假设，通过集成f-分布估计、核方法和多任务学习的要点，将传统的在每个节点独立应用的方法优化，对图结构中的多个两样本检验问题进行了更好的处理和分析。

    

    本文针对图结构设置中的多个两样本检验问题进行了研究，这是空间统计和神经科学等领域中的常见情景。在固定图中的每个节点v都涉及到两个特定节点概率密度函数（pdfs）p_v和q_v之间的两样本检验问题。我们的目标是在假设连接的节点会产生类似的检验结果的条件下，确定应拒绝零假设p_v = q_v的节点。我们提出了非参数的协同两样本检验（CTST）框架，该框架有效利用了图结构并最小化了对p_v和q_v的假设。我们的方法集成了f-分布估计、核方法和多任务学习的要点。我们使用合成实验和检测地震活动的真实传感器网络来证明CTST优于最先进的在每个节点独立应用的非参数统计检验方法，因此忽视了图结构的重要性。

    This paper addresses the multiple two-sample test problem in a graph-structured setting, which is a common scenario in fields such as Spatial Statistics and Neuroscience. Each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes. We propose the non-parametric collaborative two-sample testing (CTST) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our methodology integrates elements from f-divergence estimation, Kernel Methods, and Multitask Learning. We use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that CTST outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the g
    
[^15]: 固定宽度的树状神经网络容量分析-通用激活函数

    Fixed width treelike neural networks capacity analysis -- generic activations

    [https://arxiv.org/abs/2402.05696](https://arxiv.org/abs/2402.05696)

    这篇论文研究了树状神经网络的容量，基于Random Duality Theory提出了通用的容量分析框架，并证明该框架适用于其他类型的激活函数，如二次和ReLU函数。

    

    我们考虑了树状委员会机（TCM）神经网络的容量。基于随机对偶理论（RDT），\cite{Stojnictcmspnncaprdt23}最近提出了一个通用的框架用于它们的容量分析。然后，在\cite{Stojnictcmspnncapliftedrdt23}中提出了一种基于所谓的“部分提升”RDT（pl RDT）的升级版本。这两个工作方向都着重于具有最典型的“符号”激活函数的网络。然而，在这里，我们专注于具有其他更一般类型激活函数的网络，并且证明了\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23}的框架足够强大，可以处理这些情况。除了标准的“线性”激活函数外，我们发现两个广泛使用的激活函数，即“二次”和“修正线性单元（ReLU）”，可以得到特别方便的结果。更具体地说，对于每个激活函数，我们获得了

    We consider the capacity of \emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \emph{partially lifted} RDT (pl RDT) was then presented in \cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \emph{quadratic} and \emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtai
    
[^16]: 高维模型的对抗训练：几何和权衡

    A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs

    [https://arxiv.org/abs/2402.05674](https://arxiv.org/abs/2402.05674)

    本论文研究了高维模型中的对抗训练，引入了一个可处理的数学模型，并给出了对抗性经验风险最小化器的充分统计的精确渐近描述。研究结果表明存在可以防御而不惩罚准确性的方向，揭示了防御非鲁棒特征的优势。

    

    本研究在高维情况下，即维度$d$和数据点数$n$与固定比例$\alpha = n / d$发散的上下文中，研究了基于边际的线性分类器中的对抗训练。我们引入了一个可处理的数学模型，可以研究数据和对抗攻击者几何之间的相互作用，同时捕捉到对抗鲁棒性文献中观察到的核心现象。我们的主要理论贡献是在通用的凸且非递增损失函数下，对于对抗性经验风险最小化器的充分统计的精确渐近描述。我们的结果使我们能够精确地刻画数据中与更高的泛化/鲁棒性权衡相关的方向，由一个鲁棒性度量和一个有用性度量定义。特别地，我们揭示了存在一些方向，可以进行防御而不惩罚准确性。最后，我们展示了防御非鲁棒特征的优势。

    This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu
    
[^17]: 非参数仪器变量回归通过随机近似梯度

    Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients

    [https://arxiv.org/abs/2402.05639](https://arxiv.org/abs/2402.05639)

    本文提出了一种通过随机近似梯度最小化投影群体风险的新型非参数仪器变量回归框架，并通过理论和实证实验证明了其竞争性能。此外，本文还处理了二元结果的情况，取得了有希望的结果。

    

    本文提出了SAGD-IV，这是一种通过使用随机近似梯度来最小化投影群体风险的新型非参数仪器变量（NPIV）回归框架。仪器变量（IV）被广泛应用于计量经济学中，以解决在存在不可观测混淆因素的情况下的估计问题，并且机器学习社区致力于改进现有方法并在NPIV设置下设计新方法，该设置被认为是一个不适定的线性逆问题。我们提供了对我们算法的理论支持，并通过实证实验进一步证明了其竞争性能。此外，我们还处理了二元结果的情况，并取得了有希望的结果，而该情况在社区中没有得到与其连续对应物的同样关注。

    This paper proposes SAGD-IV, a novel framework for conducting nonparametric instrumental variable (NPIV) regression by employing stochastic approximate gradients to minimize the projected populational risk. Instrumental Variables (IVs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the Machine Learning community has devoted significant effort to improving existing methods and devising new ones in the NPIV setting, which is known to be an ill-posed linear inverse problem. We provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments. Furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart.
    
[^18]: 使用图神经网络进行超图节点分类

    Hypergraph Node Classification With Graph Neural Networks

    [https://arxiv.org/abs/2402.05569](https://arxiv.org/abs/2402.05569)

    本研究提出了一种简单高效的框架，利用加权子图扩展的图神经网络(WCE-GNN)实现了超图节点分类。实验证明，WCE-GNN具有优秀的预测效果和较低的计算复杂度。

    

    超图是用来模拟现实世界数据中的高阶相互作用的关键。图神经网络（GNNs）的成功揭示了神经网络处理具有成对交互的数据的能力。这激发了使用神经网络处理具有高阶相互作用的数据的想法，从而导致了超图神经网络（HyperGNNs）的发展。GNNs和HyperGNNs通常被认为是不同的，因为它们被设计用于处理不同几何拓扑的数据。然而，在本文中，我们在理论上证明，在节点分类的上下文中，大多数HyperGNNs可以使用带有超图的加权子图扩展的GNN来近似。这导致了WCE-GNN，一种简单高效的框架，包括一个GNN和一个加权子图扩展（WCE），用于超图节点分类。对于九个真实世界的超图节点分类数据集的实验表明，WCE-GNN不仅具有优秀的预测效果，而且具有较低的计算复杂度。

    Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
    
[^19]: 差分隐私的基于模型的离线强化学习

    Differentially Private Model-Based Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.05525](https://arxiv.org/abs/2402.05525)

    本研究提出了一种差分隐私的基于模型的离线强化学习方法，通过学习离线数据中的隐私模型以及基于模型的策略优化，实现了从离线数据中训练具有隐私保护的强化学习代理。同时，研究还总结了在这种设置下隐私的代价。

    

    我们解决了具有隐私保证的离线强化学习问题，目标是训练一个相对于数据集中每个轨迹具有差分隐私的策略。为了实现这一目标，我们引入了DP-MORL，一种带有差分隐私保证的MBRL算法。首先，使用DP-FedAvg从离线数据中学习环境的隐私模型，DP-FedAvg是一种为神经网络提供轨迹级差分隐私保证的训练方法。然后，我们使用基于模型的策略优化从（受罚的）隐私模型中推导出策略，无需进一步与系统交互或访问输入数据。我们经验证明，DP-MORL能够从离线数据中训练出具有隐私保护的RL代理，并进一步概述了在这种情况下隐私的代价。

    We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
    
[^20]: 检测子线性时间内种植的团体的低度相变

    Low-degree phase transitions for detecting a planted clique in sublinear time

    [https://arxiv.org/abs/2402.05451](https://arxiv.org/abs/2402.05451)

    在高信号区域中，研究了子线性时间内检测种植团体的算法。通过非适应性低度算法，证明了在一定尺度下可以检测到团体，并且显示了该类算法的计算相变。

    

    我们考虑在由n个顶点组成的随机图中检测大小为k的种植团体的问题。当团体的大小超过Θ(√n)时，多项式时间算法便会增多。在高信号区域，即k = Θ(n^(1/2 + δ))，我们研究更快速的——也就是子线性时间的算法。为此，我们考虑非适应性地查询邻接矩阵的子集M的算法，然后计算所揭示的条目的低度多项式函数。我们证明了这类非适应性低度算法的计算相变：在尺度Θ(n^γ)下，当γ > 3(1/2 - δ)时，可以检测到团体，但当γ < 3(1/2 - δ)时，则无法检测到团体。因此，在不超出非适应性低度类别的范围之外，无法提升检测种植团体的最佳已知运行时间，即Ω(n^(3(1/2-δ)))。我们研究了这个问题，并给出了下界证明。

    We consider the problem of detecting a planted clique of size $k$ in a random graph on $n$ vertices. When the size of the clique exceeds $\Theta(\sqrt{n})$, polynomial-time algorithms for detection proliferate. We study faster -- namely, sublinear time -- algorithms in the high-signal regime when $k = \Theta(n^{1/2 + \delta})$, for some $\delta > 0$. To this end, we consider algorithms that non-adaptively query a subset $M$ of entries of the adjacency matrix and then compute a low-degree polynomial function of the revealed entries. We prove a computational phase transition for this class of non-adaptive low-degree algorithms: under the scaling $\lvert M \rvert = \Theta(n^{\gamma})$, the clique can be detected when $\gamma > 3(1/2 - \delta)$ but not when $\gamma < 3(1/2 - \delta)$. As a result, the best known runtime for detecting a planted clique, $\widetilde{O}(n^{3(1/2-\delta)})$, cannot be improved without looking beyond the non-adaptive low-degree class.   Our proof of the lower bo
    
[^21]: 学习不确定性感知的时间扩展动作

    Learning Uncertainty-Aware Temporally-Extended Actions

    [https://arxiv.org/abs/2402.05439](https://arxiv.org/abs/2402.05439)

    我们提出了一种名为不确定性感知时间扩展（UTE）的算法，在强化学习中解决了动作重复可能降低性能的问题，通过测量不确定性从而让策略根据需求进行选择，实验证明UTE优于现有算法。

    

    在强化学习中，动作空间中的时间抽象，例如动作重复，是一种通过扩展动作促进策略学习的技术。然而，以前的动作重复研究存在一个主要限制，即当重复次优动作时可能降低性能。这个问题经常抵消了动作重复的优势。为了解决这个问题，我们提出了一种名为不确定性感知时间扩展（UTE）的新算法。UTE使用集成方法在动作扩展期间准确地测量不确定性。这个特性允许策略根据其特定需求，在强调探索或采取不确定性-抵制方法之间进行选择。我们通过在Gridworld和Atari 2600环境中进行实验展示了UTE的有效性。我们的研究结果表明，UTE优于现有的动作重复算法，有效地缓解了它们固有的局限性，并显著提高了效果。

    In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhan
    
[^22]: 自适应激活函数在稀疏实验数据预测建模中的应用

    Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data

    [https://arxiv.org/abs/2402.05401](https://arxiv.org/abs/2402.05401)

    本研究通过研究自适应激活函数在稀疏实验数据预测建模中的应用，填补了当前对其影响了解不足的重要差距，并揭示出个体可调参数的自适应激活函数在预测模型中具有较高的准确性。

    

    神经网络设计的关键在于选择激活函数，用于引入能够捕捉复杂输入-输出模式的非线性结构。虽然在具有充足数据的领域（例如图像分类问题）中研究了自适应或可调激活函数的有效性，但在数据有限的情况下，对其对分类准确性和预测不确定性的影响仍然存在重大差距。本研究旨在通过研究两种类型的自适应激活函数来填补这些差距。这些函数在每个隐藏层中引入了共享和个体可调参数，并在包含少于一百个训练实例的三个测试平台中进行了探究。我们的研究揭示了个体可调参数的自适应激活函数（例如指数线性单位（ELU）和软加）在预测模型中具有较高的准确性。

    A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. This research aims to address these gaps by investigating the use of two types of adaptive activation functions. These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in acc
    
[^23]: 对角费舍尔信息矩阵估计器的权衡

    Tradeoffs of Diagonal Fisher Information Matrix Estimators

    [https://arxiv.org/abs/2402.05379](https://arxiv.org/abs/2402.05379)

    本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。

    

    费舍尔信息矩阵描述了神经网络参数空间中的局部几何性质，它提供了理论和工具来理解和优化神经网络。鉴于其计算成本高，实践者通常使用随机估计器，并仅评估对角线条目。我们研究了两种这样的估计器，其准确性和样本复杂性取决于它们关联的方差。我们推导了方差的界限，并在回归和分类网络中实例化它们。我们通过分析和数值研究来权衡这两个估计器。我们发现方差量取决于关于不同参数组的非线性，当估计费舍尔信息时不能忽视它们。

    The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
    
[^24]: 在无拟样似推断的干扰参数和广义标签转移下的分类问题

    Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference

    [https://arxiv.org/abs/2402.05330](https://arxiv.org/abs/2402.05330)

    该论文提出了一种在广义标签转移和干扰参数下进行分类的新方法。通过将分类问题视为带有干扰参数的假设检验问题，我们能够有效地估计分类器在整个干扰参数空间中的接收者操作特性，并设计出在广义标签转移下不变的截断点，从而实现了对事件的可靠分类和不确定性度量。

    

    在我们的训练数据和目标数据之间，标签和潜在干扰参数的分布不同的情况下，如何以可靠的不确定性度量对事件进行分类是一个科学挑战。我们将这种分布转移称为广义标签转移 (GLS)。直接使用观测数据 $\mathbf{X}$ 进行分类会导致预测结果偏差和标签 $Y$ 的无效不确定性估计。我们通过将分类问题视为带有干扰参数的假设检验问题来克服这些偏差。关键思想是在整个干扰参数空间中估计分类器的接收者操作特性 (ROC)，这使我们能够设计在 GLS 下不变的截断点。我们的方法有效地赋予预训练的分类器领域适应能力，并返回有效的预测集合，同时保持有效的不确定性估计。

    An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maint
    
[^25]: 梯度下降引发了深度非线性网络权重与经验NTK之间的对齐

    Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks

    [https://arxiv.org/abs/2402.05271](https://arxiv.org/abs/2402.05271)

    了解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。前人的研究表明，在训练过程中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这被称为神经特征分析（NFA）。本研究解释了这种相关性的出现，并发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。在早期训练阶段，可以通过解析的方式预测NFA的发展速度。

    

    理解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。先前的研究已经确定，在一般结构的训练神经网络中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这个说法被称为神经特征分析（NFA）。然而，这些数量在训练过程中如何相关尚不清楚。在这项工作中，我们解释了这种相关性的出现。我们发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。我们证明了先前研究中引入的NFA是由隔离这种对齐的中心化NFA驱动的。我们还展示了在早期训练阶段，可以通过解析的方式预测NFA的发展速度。

    Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
    
[^26]: 关于偏离高斯混合专家模型参数估计的研究

    On Parameter Estimation in Deviated Gaussian Mixture of Experts

    [https://arxiv.org/abs/2402.05220](https://arxiv.org/abs/2402.05220)

    在偏离高斯混合专家模型中，本文通过构造Voronoi-based损失函数来解决参数估计问题。

    

    本文考虑了在偏离高斯混合专家模型中的参数估计问题，其中数据由$(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$生成，其中$X, Y$分别是协变量向量和响应变量，$g_{0}(Y|X)$是已知函数，$\lambda^{\ast} \in [0, 1]$是真实但未知的混合比例，$(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$对于$1 \leq i \leq k^{\ast}$是高斯混合专家模型的未知参数。该问题源自于拟合优度检验，当我们希望检验数据是否由$g_{0}(Y|X)$（零假设）生成，还是由整个混合（备择假设）生成。基于专家函数的代数结构和$g_0$与混合部分的可区分性，我们构造了新的基于Voronoi的损失函数来捕捉c

    We consider the parameter estimation problem in the deviated Gaussian mixture of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$, where $X, Y$ are respectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$ for $1 \leq i \leq k^{\ast}$ are unknown parameters of the Gaussian mixture of experts. This problem arises from the goodness-of-fit test when we would like to test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or they are generated from the whole mixture (alternative hypothesis). Based on the algebraic structure of the expert functions and the distinguishability between $g_0$ and the mixture part, we construct novel Voronoi-based loss functions to capture the c
    
[^27]: 采用分割引导扩散模型的解剖可控医学图像生成

    Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models

    [https://arxiv.org/abs/2402.05210](https://arxiv.org/abs/2402.05210)

    这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。

    

    扩散模型已经实现了非常高质量的医学图像生成，可以通过为小型或不平衡的数据集提供补充，从而帮助减轻获取和注释新图像的费用，同时还可以应用于其他方面。然而，这些模型在生成图像时面临着全局解剖真实性的挑战。因此，我们提出了一种解剖可控的医学图像生成模型。我们的模型在每个采样步骤中遵循多类解剖分割掩模，并采用随机掩模消融训练算法，以实现对所选解剖约束的条件化，同时允许其他解剖区域的灵活性。这也改善了网络在完全无条件（无约束生成）情况下对解剖真实性的学习。通过对乳腺MRI和腹部/颈部到盆腔CT数据集的比较评估，证明了我们模型在解剖真实性和输入掩模保真度方面具有优越性。

    Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
    
[^28]: 贝尔曼符合推断：时间序列预测中预测区间的校准

    Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series

    [https://arxiv.org/abs/2402.05203](https://arxiv.org/abs/2402.05203)

    贝尔曼符合推断（BCI）是一个框架，通过解决一维随机控制问题，利用多步预测来提供校准的时间序列预测区间。BCI在任意分布转换和时间依赖性下实现了长期覆盖，且在波动率预测问题上生成更短的预测区间。

    

    我们引入了贝尔曼符合推断（BCI），这是一个围绕任何时间序列预测模型的框架，可以提供校准的预测区间。与现有方法不同，BCI能够利用多步预测，并通过在每个时间步骤上解决一维随机控制问题（SCP）来显式优化平均区间长度。特别地，我们使用动态规划算法来找到SCP的最优策略。我们证明了在任意分布转换和时间依赖性下，BCI能够实现长期覆盖，即使多步预测较差。我们在实证中发现，与现有方法相比，BCI避免了无信息区间（长度无限）的生成，并在波动率预测问题上生成了明显更短的预测区间。

    We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods.
    
[^29]: 在策略镜像下降中元学习镜像映射

    Meta-learning the mirror map in policy mirror descent

    [https://arxiv.org/abs/2402.05187](https://arxiv.org/abs/2402.05187)

    该论文通过实证研究发现，传统的镜像映射选择（NPG）在标准基准环境中常常导致不理想的结果。通过元学习方法，找到了更高效的镜像映射，提升了性能。

    

    策略镜像下降（PMD）是强化学习中的一种流行框架，作为一种统一视角，它包含了许多算法。这些算法是通过选择一个镜像映射而导出的，并且具有有限时间的收敛保证。尽管它很受欢迎，但对PMD的全面潜力的探索是有限的，大部分研究集中在一个特定的镜像映射上，即负熵，从而产生了著名的自然策略梯度（NPG）方法。目前的理论研究还不确定镜像映射的选择是否会对PMD的有效性产生重大影响。在我们的工作中，我们进行了实证研究，证明了传统的镜像映射选择（NPG）在几个标准基准环境中经常产生不理想的结果。通过应用元学习方法，我们确定了更高效的镜像映射，提高了性能，无论是平均性能还是最佳性能。

    Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along th
    
[^30]: 探索Transformer模型的归纳偏差: 一个来自无穷的视角

    Towards Understanding Inductive Bias in Transformers: A View From Infinity

    [https://arxiv.org/abs/2402.05173](https://arxiv.org/abs/2402.05173)

    本文研究了Transformer模型的归纳偏差，并发现它们倾向于对称排列函数，对称群的表示理论可以用于分析预测，同时提出了一个简化模型来解决学习曲线和网络输出，并在常见设置中得出学习能力的紧密边界，最后还证明了WikiText数据集具有排列对称性。

    

    我们研究了Transformer模型在无穷的过参数化高斯过程极限中的归纳偏差，并指出Transformer模型在序列空间中更倾向于对称排列函数。我们证明了对称群的表示理论可以用于在数据集对token之间的排列具有对称性时给出定量的分析预测。我们提出了一个简化的Transformer模型，并在极限条件下求解模型，包括对学习曲线和网络输出的准确预测。我们展示了在常见的设置中，可以推导出学习能力的紧密边界，以上下文长度作为函数的缩放定律。最后，我们认为WikiText数据集确实具有一定程度的排列对称性。

    We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.
    
[^31]: 生存分析的在线学习方法

    Online Learning Approach for Survival Analysis

    [https://arxiv.org/abs/2402.05145](https://arxiv.org/abs/2402.05145)

    本论文介绍了一种在线数学框架，用于实时适应动态环境和有审查数据的生存分析，通过在线牛顿步骤(ONS)算法估计事件时间分布，并提出了保证ONS具有对数随机遗憾界的随机方法和自适应聚合方法。

    

    我们引入了一种在线数学框架用于生存分析，可以实时适应动态环境和有审查数据。该框架通过最优二阶在线凸优化算法-在线牛顿步骤(ONS)估计事件时间分布。这种以前未曾探索的方法具有重大优势，包括具有非渐近收敛保证的明确算法。此外，我们分析了ONS超参数的选择，这取决于指数-凸性质并且对遗憾界有显著影响。我们提出了一种保证ONS具有对数随机遗憾界的随机方法。此外，我们引入了一种自适应聚合方法，确保在保持快速遗憾界的同时，在超参数选择方面具有鲁棒性。本文的发现可以超出生存分析领域，并且对于任何具有差的指数-凸性和不稳定ONS的情况都是相关的。

    We introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data. This framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-Online Newton Step (ONS). This approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees. Moreover, we analyze the selection of ONS hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound. We propose a stochastic approach that guarantees logarithmic stochastic regret for ONS. Additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds. The findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ONS. Fi
    
[^32]: 连续多维标度

    Continuous Multidimensional Scaling

    [https://arxiv.org/abs/2402.04436](https://arxiv.org/abs/2402.04436)

    连续多维标度是关于将距离信息嵌入欧几里得空间的过程，并探讨了在对象集不断增加的情况下，将整个嵌入问题序列视为一个固定空间中的一系列优化问题的方法和结论。

    

    多维标度(MDS)是将关于一组$n$个对象的距离信息嵌入到$d$维欧几里得空间中的过程。最初由心理测量学界构思，MDS关注的是嵌入到一组固定对象上的一组固定距离。现代关注的问题更常涉及到研究与一组不断增加的对象相关联的一系列距离的极限行为，如在随机图的统计推断的渐近理论中出现的问题。点到集合映射理论中的标准结果表明，若$n$固定，则嵌入结构的极限是极限距离的嵌入结构。但如果$n$增加怎么办呢？那么就需要重新制定MDS，以便将整个嵌入问题序列视为一个固定空间中的一系列优化问题。我们提出了这样一种重新制定，并推导出一些结论。

    Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space. As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects. Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities. But what if $n$ increases? It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. We present such a reformulation and derive some consequences.
    
[^33]: 一种从状态空间模型到紧支持基的核分组的通用理论

    A General Theory for Kernel Packets: from state space model to compactly supported basis

    [https://arxiv.org/abs/2402.04022](https://arxiv.org/abs/2402.04022)

    该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。

    

    众所周知，高斯过程（GP）的状态空间（SS）模型公式可以将其训练和预测时间降低到O（n）（n为数据点个数）。我们证明了一个m维的GP的SS模型公式等价于我们引入的一个概念，称为通用右核分组（KP）：一种用于GP协方差函数K的变换，使得对于任意$t \leq t_1$，$0 \leq j \leq m-1$和$m+1$个连续点$t_i$，都满足$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$，其中${D}_t^{(j)}f(t)$表示在$t$上作用的第j阶导数。我们将这个思想扩展到了GP的向后SS模型公式，得到了下一个$m$个连续点的左核分组的概念：$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$，对于任意$t\geq t_{2m}$。通过结合左右核分组，可以证明这些协方差函数的适当线性组合产生了$m$个紧支持的核分组函数：对于任意$t\not\in(t_0,t_{2m})$和$j=0,\cdots,m-1$，$\phi^{(j)}(t)=0$。

    It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
    
[^34]: 论文题目：为什么“摸着石头过河”方法主导推荐系统实践；呼吁摒弃反乌托邦思维

    Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking

    [https://arxiv.org/abs/2402.02152](https://arxiv.org/abs/2402.02152)

    这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。

    

    应用推荐系统研究处于一种奇特的境地。尽管在通过A/B测试来衡量性能方面有一个非常严格的协议，但找到要测试的“B”的最佳方法并没有明确地针对性能，而是针对一个代理指标。因此，一个A/B测试的成功或失败完全取决于所提出的代理指标是否与性能相关性更好。没有原则可以在离线情况下确定一个代理指标是否比另一个更好，这使得从业者们摸不着头脑。本论文的目的是质疑这种反乌托邦思维，并主张深度学习堆栈的非标准用法实际上有潜力解锁优化奖励的推荐系统。

    Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
    
[^35]: 当准确的预测模型导致有害的自我实现预言

    When accurate prediction models yield harmful self-fulfilling prophecies

    [https://arxiv.org/abs/2312.01210](https://arxiv.org/abs/2312.01210)

    本研究通过调查预测模型的部署对决策产生有害影响的情况，发现这些模型可能会成为有害的自我实现预言。这些模型不会因为对某些患者造成更糟糕的结果而使其预测能力变无效。

    

    目标：预测模型在医学研究和实践中非常受欢迎。通过为特定患者预测感兴趣的结果，这些模型可以帮助决策困难的治疗决策，并且通常被誉为个性化的、数据驱动的医疗保健的杰出代表。许多预测模型在验证研究中基于其预测准确性而部署用于决策支持。我们调查这是否是一种安全和有效的方法。材料和方法：我们展示了使用预测模型进行决策可以导致有害的决策，即使在部署后这些预测表现出良好的区分度。这些模型是有害的自我实现预言：它们的部署损害了一群患者，但这些患者的更糟糕的结果并不使模型的预测能力无效。结果：我们的主要结果是对这些预测模型集合的形式化描述。接下来，我们展示了在部署前后都进行了良好校准的模型

    Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach.   Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model.   Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment 
    
[^36]: 在次线性时间内训练过参数化的神经网络

    Training Overparametrized Neural Networks in Sublinear Time

    [https://arxiv.org/abs/2208.04508](https://arxiv.org/abs/2208.04508)

    这项研究提出了一种新的训练方法，可以在次线性时间内训练过参数化的神经网络，提高了训练的效率。

    

    深度学习的成功付出了巨大的计算和能源成本，训练过参数化的神经网络的可扩展性正在成为人工智能进展的真正障碍。尽管传统的反向传播通过梯度下降的成本每次迭代很低，但随机梯度下降在非凸设置中具有禁止的收敛速度，无论是在理论上还是实践中。为了缓解这种成本，最近的研究提出了采用具有更快收敛速度但每次迭代成本更高的替代（牛顿类型）训练方法。对于具有$m=\mathrm{poly}(n)$个参数和输入批次$n$个数据点在$\mathbb{R}^d$中的典型神经网络，[Brand, Peng, Song, and Weinstein, ITCS'2021]的先前工作每次迭代需要$\sim mnd+n^3$的时间。在本文中，我们提出了一种新的训练方法，只需要$m^{1-\alpha}nd+n^3$的摊销时间，与先前的方法相比具有更高的效率。

    The success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of artificial intelligence (AI). Despite the popularity and low cost-per-iteration of traditional backpropagation via gradient decent, stochastic gradient descent (SGD) has prohibitive convergence rate in non-convex settings, both in theory and practice.   To mitigate this cost, recent works have proposed to employ alternative (Newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. For a typical neural network with $m=\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\mathbb{R}^d$, the previous work of [Brand, Peng, Song, and Weinstein, ITCS'2021] requires $\sim mnd + n^3$ time per iteration. In this paper, we present a novel training method that requires only $m^{1-\alpha} n d + n^3$ amortized time in the same overparametri
    
[^37]: 个性化PCA:解耦共享和独特特征

    Personalized PCA: Decoupling Shared and Unique Features

    [https://arxiv.org/abs/2207.08041](https://arxiv.org/abs/2207.08041)

    本文介绍了个性化PCA（PerPCA），通过使用全局和局部主成分来编码独特和共享特征，解决了PCA面临的异质性挑战。我们证明，在温和的条件下，我们可以通过受约束优化问题识别和恢复出独特和共享的特征。我们还设计了一个联邦算法来解决这个问题，并证明了算法的线性收敛性。

    

    在这篇论文中，我们解决了PCA面临的一个重要挑战：异质性。当数据来自不同的来源，具有异质的趋势，同时仍然共享某些一致性时，关键是提取共享的知识，同时保留每个来源的独特特征。为此，我们提出了个性化PCA（PerPCA），它使用互相正交的全局和局部主成分来编码独特和共享的特征。我们证明，在温和的条件下，即使协方差矩阵极其不同，也可以通过受约束优化问题识别和恢复出独特和共享的特征。此外，我们设计了一个完全的联邦算法，灵感来自分布式Stiefel梯度下降，用于解决这个问题。该算法引入了一种称为广义回缩的新操作组来处理正交约束，只需要在来源之间共享全局主成分。我们证明了算法在适当的假设下具有线性收敛性。

    In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumpt
    
[^38]: 信用评分模型的公平性

    The Fairness of Credit Scoring Models

    [https://arxiv.org/abs/2205.10200](https://arxiv.org/abs/2205.10200)

    本论文研究了信用评分模型的公平性问题，提出了一种形式化测试算法公平性的方法，并探索了影响公平性的变量。研究结果可以指导信贷商监测算法公平性、监管机构控制公平性，同时提高受保护群体的利益，同时保持高水平的预测准确性。

    

    在信用市场中，筛选算法的目标是区分好类型和坏类型的借款人。然而，在这样做的过程中，他们还可能在具有受保护属性的个体（例如性别、年龄、种族起源）和整个人群之间进行歧视。这可能是无意识的，来源于训练数据集或模型本身。我们展示了如何形式化测试评分模型的算法公平性，以及如何确定影响公平性不足的变量。然后，我们利用这些变量来优化公平性和性能之间的权衡。我们的框架提供了关于如何监测信贷商的算法公平性、如何由监管机构控制、如何改善受保护群体的利益，同时仍保持高水平的预测准确性的指导。

    In credit markets, screening algorithms aim to discriminate between good-type and bad-type borrowers. However, when doing so, they can also discriminate between individuals sharing a protected attribute (e.g. gender, age, racial origin) and the rest of the population. This can be unintentional and originate from the training dataset or from the model itself. We show how to formally test the algorithmic fairness of scoring models and how to identify the variables responsible for any lack of fairness. We then use these variables to optimize the fairness-performance trade-off. Our framework provides guidance on how algorithmic fairness can be monitored by lenders, controlled by their regulators, improved for the benefit of protected groups, while still maintaining a high level of forecasting accuracy.
    
[^39]: 使用方差信息进行面板数据中的群组结构估计的光谱聚类

    Spectral Clustering with Variance Information for Group Structure Estimation in Panel Data

    [https://arxiv.org/abs/2201.01793](https://arxiv.org/abs/2201.01793)

    该论文提出了使用方差信息进行面板数据中群组结构估计的光谱聚类方法。通过局部分析发现，个体系数估计的方差包含了估计群组结构所需的有用信息。该方法不仅适用于面板数据模型，还适用于只提供参数估计和估计不确定度的情况。通过模拟研究证明，该方法表现出更好的性能。

    

    在面板数据设置中，我们可以重复对个体进行观察。通常可以合理地假设存在着共享观察特征效应的个体群组，但是群组通常事先是未知的。我们首先进行局部分析，揭示了个体系数估计的方差包含了估计群组结构的有用信息。然后我们提出了一种方法来估计一般面板数据模型中的未观测到的群组，该方法明确考虑了方差信息。我们提出的方法在个体数量较多和/或每个个体有重复测量的情况下仍然具有可计算性。这些方法还可以应用于只提供参数估计和一些估计不确定度量给研究人员而没有个体层面数据的情况。通过彻底的模拟研究，我们展示了我们的方法的卓越性能。

    Consider a panel data setting where repeated observations on individuals are available. Often it is reasonable to assume that there exist groups of individuals that share similar effects of observed characteristics, but the grouping is typically unknown in advance. We first conduct a local analysis which reveals that the variances of the individual coefficient estimates contain useful information for the estimation of group structure. We then propose a method to estimate unobserved groupings for general panel data models that explicitly account for the variance information. Our proposed method remains computationally feasible with a large number of individuals and/or repeated measurements on each individual. The developed ideas can also be applied even when individual-level data are not available and only parameter estimates together with some quantification of estimation uncertainty are given to the researcher. A thorough simulation study demonstrates superior performance of our metho
    
[^40]: 熵正则化的自然策略梯度与线性函数逼近的线性收敛性

    Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation

    [https://arxiv.org/abs/2106.04096](https://arxiv.org/abs/2106.04096)

    本文研究了具有线性函数逼近的熵正则化自然策略梯度的收敛性质，证明了其在正则化马尔可夫决策过程中具有线性收敛性以及快速的收敛速率。

    

    自然策略梯度（NPG）方法在具有大状态-动作空间的强化学习问题中通过熵正则化取得了令人瞩目的实证成功。然而，在函数逼近的情况下，它们的收敛性质和熵正则化的影响仍然不明确。本文在softmax参数化下，建立了熵正则化的NPG与线性函数逼近的有限时间收敛分析。特别地，我们证明了熵正则化的NPG通过平均满足“激励持久性”条件，并在正则化马尔可夫决策过程中以$\tilde{O}(1/T)$的快速收敛速率达到函数逼近误差。这个收敛结果不需要对策略进行任何先验假设。此外，在浓度系数和基向量的轻微正则性条件下，我们证明了熵正则化的NPG在函数逼近误差范围内呈现“线性收敛”。

    Natural policy gradient (NPG) methods with entropy regularization achieve impressive empirical success in reinforcement learning problems with large state-action spaces. However, their convergence properties and the impact of entropy regularization remain elusive in the function approximation regime. In this paper, we establish finite-time convergence analyses of entropy-regularized NPG with linear function approximation under softmax parameterization. In particular, we prove that entropy-regularized NPG with averaging satisfies the \emph{persistence of excitation} condition, and achieves a fast convergence rate of $\tilde{O}(1/T)$ up to a function approximation error in regularized Markov decision processes. This convergence result does not require any a priori assumptions on the policies. Furthermore, under mild regularity conditions on the concentrability coefficient and basis vectors, we prove that entropy-regularized NPG exhibits \emph{linear convergence} up to a function approxim
    
[^41]: 使用共享神经元的RBF网络估计个体化多治疗反应曲线

    Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons. (arXiv:2401.16571v1 [stat.ME])

    [http://arxiv.org/abs/2401.16571](http://arxiv.org/abs/2401.16571)

    我们提出了一种使用共享神经元的RBF网络的非参数化治疗效应估计方法，适用于多治疗设置。该方法能够建模治疗结果的共同性，并在贝叶斯框架下实现估计和推断，通过模拟实验证明了其数值性能，应用于真实临床数据后也得到了有趣的发现。

    

    异质治疗效应估计是精确医学中的一个重要问题。我们的研究兴趣在于基于一些外部协变量，确定不同治疗方式的差异效应。我们提出了一种新颖的非参数化治疗效应估计方法，适用于多治疗设置。我们对响应曲线的非参数建模依赖于带有共享隐藏神经元的径向基函数（RBF）网络。因此，我们的模型有助于建模治疗结果的共同性。我们在贝叶斯框架下开发了估计和推断方案，并通过高效的马尔科夫链蒙特卡罗算法进行实现，适当地处理了分析各个方面的不确定性。通过模拟实验，展示了该方法的数值性能。将我们提出的方法应用于MIMIC数据后，我们得到了关于不同治疗策略对ICU住院时间和12小时SOFA评分的影响的一些有趣发现。

    Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function (RBF)-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of ICU stay and 12-hour SOFA sc
    
[^42]: 上下文固定预算的最佳臂识别：适应性实验设计与策略学习

    Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])

    [http://arxiv.org/abs/2401.03756](http://arxiv.org/abs/2401.03756)

    该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。

    

    个性化治疗推荐是基于证据的决策中的关键任务。在这项研究中，我们将这个任务作为一个带有上下文信息的固定预算最佳臂识别（Best Arm Identification, BAI）问题来进行建模。在这个设置中，我们考虑了一个给定多个治疗臂的自适应试验。在每一轮中，决策者观察一个刻画实验单位的上下文（协变量），并将该单位分配给其中一个治疗臂。在实验结束时，决策者推荐一个在给定上下文条件下预计产生最高期望结果的治疗臂（最佳治疗臂）。该决策的有效性通过最坏情况下的期望简单遗憾（策略遗憾）来衡量，该遗憾表示在给定上下文条件下，最佳治疗臂和推荐治疗臂的条件期望结果之间的最大差异。我们的初始步骤是推导最坏情况下期望简单遗憾的渐近下界，该下界还暗示着解决该问题的一些思路。

    Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
    
[^43]: DiffEnc: 使用学习的编码器的变分扩散模型

    DiffEnc: Variational Diffusion with a Learned Encoder. (arXiv:2310.19789v1 [cs.LG])

    [http://arxiv.org/abs/2310.19789](http://arxiv.org/abs/2310.19789)

    DiffEnc是一种使用学习的编码器的变分扩散模型，通过引入数据和深度相关的均值函数和可调节的噪声方差比率，实现了最先进的可能性。

    

    扩散模型可以看作是具有两种改进的分层变分自编码器（VAEs）：在生成过程中参数共享的条件分布和在层次结构上独立计算损失。我们对扩散模型进行了两个变化，保留了这些优势的同时增加了模型的灵活性。首先，我们在扩散过程中引入了一个与数据和深度相关的均值函数，从而导致了修改后的扩散损失。我们提出的框架DiffEnc在CIFAR-10上实现了最先进的可能性。其次，我们让反向编码过程的噪声方差与生成过程的比率成为一个自由的权重参数，而不是固定为1。这带来了理论上的洞察力：对于有限深度层次，证据下界（ELBO）可以用作加权扩散损失方法的目标，并用于专门为推理而优化噪声调度。

    Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite
    
[^44]: 从时间序列数据中发现混合结构因果模型

    Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])

    [http://arxiv.org/abs/2310.06312](http://arxiv.org/abs/2310.06312)

    这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。

    

    在金融、气候科学和神经科学等领域，从时间序列数据中推断因果关系是一个巨大的挑战。尽管现代技术可以处理变量之间的非线性关系和灵活的噪声分布，但它们依赖于简化假设，即数据来自相同的潜在因果模型。在这项工作中，我们放松了这个假设，从来源于不同因果模型混合的时间序列数据中进行因果发现。我们推断了潜在的结构性因果模型，以及每个样本属于特定混合成分的后验概率。我们的方法采用了一个端对端的训练过程，最大化了数据似然的证据下界。通过对合成和真实世界数据集的广泛实验，我们证明了我们的方法在因果发现任务中超越了最先进的基准方法，尤其是当数据来自不同的潜在因果模型时。

    In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
    
[^45]: 关于随机向量的仿射变换的Wasserstein距离研究

    On Wasserstein distances for affine transformations of random vectors. (arXiv:2310.03945v1 [stat.ML])

    [http://arxiv.org/abs/2310.03945](http://arxiv.org/abs/2310.03945)

    本研究阐述了关于随机向量之间的Wasserstein距离的下界，重点是仿射变换，并应用于流型学习和手写数据集模拟。

    

    我们阐述了关于在Wasserstein空间中用于数据流型学习的随机向量之间的二次Wasserstein距离的一些已知下界，重点是仿射变换。特别地，我们通过计算协方差矩阵之间的Bures距离，给出了旋转的随机向量在具有不相关分量的$\mathbb{R}^2$空间中的具体下界。我们还得到了仿射变换的组合的上界，从而产生了应用于初始数据测度的丰富的微分同胚。我们将这些界应用于包括在$\mathbb{R}^2$中的一维流型上的各种分布，并说明了这些界的质量。最后，我们提供了一个在流型学习框架中可以应用于模拟手写数字或字母数据集的框架。

    We expound on some known lower bounds of the quadratic Wasserstein distance between random vectors in $\mathbb{R}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in Wasserstein space. In particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices. We also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. We apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{R}^2$ and illustrate the quality of the bounds. Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.
    
[^46]: 对具有依赖成本的有向无环图进行最优分区

    Optimal partitioning of directed acyclic graphs with dependent costs between clusters. (arXiv:2308.03970v1 [cs.DS])

    [http://arxiv.org/abs/2308.03970](http://arxiv.org/abs/2308.03970)

    本论文提出了一种名为DCMAP的算法，用于对具有依赖成本的有向无环图进行最优分区。该算法通过优化基于DAG和集群映射的成本函数来寻找所有最优集群，并在途中返回接近最优解。实验证明在复杂系统的DBN模型中，该算法具有时间效率性。

    

    许多统计推断场景，包括贝叶斯网络、马尔可夫过程和隐马尔可夫模型，可以通过将基础的有向无环图（DAG）划分成集群来支持。然而，在统计推断中，最优划分是具有挑战性的，因为要优化的成本取决于集群内的节点以及通过父节点和/或子节点连接的集群之间的映射，我们将其称为依赖集群。我们提出了一种名为DCMAP的新算法，用于具有依赖集群的最优集群映射。在基于DAG和集群映射的任意定义的正成本函数的基础上，我们证明DCMAP收敛于找到所有最优集群，并在途中返回接近最优解。通过实验证明，该算法对使用计算成本函数的一个海草复杂系统的DBN模型具有时间效率性。对于一个25个和50个节点的DBN，搜索空间大小分别为$9.91\times 10^9$和$1.5$

    Many statistical inference contexts, including Bayesian Networks (BNs), Markov processes and Hidden Markov Models (HMMS) could be supported by partitioning (i.e.~mapping) the underlying Directed Acyclic Graph (DAG) into clusters. However, optimal partitioning is challenging, especially in statistical inference as the cost to be optimised is dependent on both nodes within a cluster, and the mapping of clusters connected via parent and/or child nodes, which we call dependent clusters. We propose a novel algorithm called DCMAP for optimal cluster mapping with dependent clusters. Given an arbitrarily defined, positive cost function based on the DAG and cluster mappings, we show that DCMAP converges to find all optimal clusters, and returns near-optimal solutions along the way. Empirically, we find that the algorithm is time-efficient for a DBN model of a seagrass complex system using a computation cost function. For a 25 and 50-node DBN, the search space size was $9.91\times 10^9$ and $1.5
    
[^47]: 大数据-供应链管理框架的预测：数据预处理和机器学习技术

    Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])

    [http://arxiv.org/abs/2307.12971](http://arxiv.org/abs/2307.12971)

    本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。

    

    本文旨在系统地识别和比较分析最先进的供应链预测策略和技术。提出了一个新的框架，将大数据分析应用于供应链管理中，包括问题识别、数据来源、探索性数据分析、机器学习模型训练、超参数调优、性能评估和优化，以及预测对人力、库存和整个供应链的影响。首先讨论了根据供应链策略收集数据的需求以及如何收集数据。文章讨论了根据周期或供应链目标需要不同类型的预测。推荐使用供应链绩效指标和误差测量系统来优化表现最佳的模型。还讨论了幻影库存对预测的不利影响以及管理决策依赖供应链绩效指标来确定模型性能参数和改进运营管理、透明度的问题。

    This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
    
[^48]: 用于一般参数密度模型的最小化稳健密度功率分歧的随机优化方法

    A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models. (arXiv:2307.05251v1 [stat.ME])

    [http://arxiv.org/abs/2307.05251](http://arxiv.org/abs/2307.05251)

    本研究提出了一种随机优化方法，用于解决稳健密度功率分歧（DPD）在一般参数密度模型中的计算复杂性问题，并通过应用传统的随机优化理论来验证其有效性。

    

    密度功率分歧（DPD）是一种用于稳健地估计观测数据潜在分布的方法，它包括一个要估计的参数密度模型的幂的积分项。虽然对于一些特定的密度（如正态密度和指数密度）可以得到积分项的显式形式，但DPD的计算复杂性使得其无法应用于更一般的参数密度模型，这已经超过了DPD提出的25年。本研究提出了一种用于一般参数密度模型最小化DPD的随机优化方法，并通过参考随机优化的传统理论说明了其适用性。所提出的方法还可以通过使用未归一化模型来最小化另一个基于密度功率的γ-离差[Kanamori和Fujisawa（2015），Biometrika]。

    Density power divergence (DPD) [Basu et al. (1998), Biometrika], designed to estimate the underlying distribution of the observations robustly, comprises an integral term of the power of the parametric density models to be estimated. While the explicit form of the integral term can be obtained for some specific densities (such as normal density and exponential density), its computational intractability has prohibited the application of DPD-based estimation to more general parametric densities, over a quarter of a century since the proposal of DPD. This study proposes a stochastic optimization approach to minimize DPD for general parametric density models and explains its adequacy by referring to conventional theories on stochastic optimization. The proposed approach also can be applied to the minimization of another density power-based $\gamma$-divergence with the aid of unnormalized models [Kanamori and Fujisawa (2015), Biometrika].
    
[^49]: 关于逻辑回归中参数估计的样本复杂度研究

    On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])

    [http://arxiv.org/abs/2307.04191](http://arxiv.org/abs/2307.04191)

    本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。

    

    逻辑回归模型是噪声二元分类问题中最常见的数据生成模型之一。本文研究了在标准正态协变量下，以$\ell_2$误差为限，估计逻辑回归模型参数的样本复杂度，考虑了维度和逆温度的影响。逆温度控制了数据生成过程中的信噪比。虽然逻辑回归的广义界限和渐近性能已经有了深入研究，但关于参数估计的非渐近样本复杂度在之前的分析中没有讨论其与误差和逆温度的依赖关系。我们展示了样本复杂度曲线在逆温度方面具有两个转折点（或临界点），明确划分了低、中和高温度区域。

    The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
    
[^50]: 激励理论的贝叶斯推理在协作科学中的应用

    Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])

    [http://arxiv.org/abs/2307.03748](http://arxiv.org/abs/2307.03748)

    本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。

    

    当代科学研究是一项分布式的、协作的工作，由研究团队、监管机构、资助机构、商业合作伙伴和科学机构组成，彼此互动并面对不同的激励。为了保持科学严谨性，统计方法应该认识到这种情况。为此，我们研究了假设检验的情况，其中有一个代理人（例如研究人员或制药公司）对未知参数拥有私人先验知识，还有一个委托人（如政策制定者或监管机构）希望根据参数值做出决策。代理人根据他们的私人先验选择是否进行统计试验，然后试验的结果由委托人用来做出决策。我们展示了委托人如何进行统计推断，利用代理人的战略行为所透露的信息，也就是他们选择是否进行试验。具体来说，我们展示了如何计算p值，从而综合利用代理人的行为和试验的结果进行推理。

    Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the p
    
[^51]: 使用汇总统计数据的多任务学习

    Multi-Task Learning with Summary Statistics. (arXiv:2307.02388v1 [stat.ME])

    [http://arxiv.org/abs/2307.02388](http://arxiv.org/abs/2307.02388)

    提出了一种利用汇总统计数据的灵活多任务学习框架，可解决在真实世界设置中数据共享限制的问题。通过自适应参数选择方法和系统非渐近分析，提高了模型性能。通过大量模拟实验证明了方法的有效性。

    

    多任务学习已经成为一个强大的机器学习范式，可以整合来自多个来源的数据，利用任务之间的相似性提高整体模型性能。然而，在真实世界的设置中，多任务学习的应用受到数据共享限制的影响，特别是在医疗领域。为了解决这个挑战，我们提出了一个灵活的多任务学习框架，利用来自各种来源的汇总统计数据。此外，我们提出了一种自适应参数选择方法，基于Lepski方法的一种变体，在仅有汇总统计数据时允许数据驱动的调参选择。我们的系统非渐近分析描述了所提方法在样本复杂度和重叠度的各种情况下的性能。我们通过大量模拟实验证明了我们的理论发现和方法的性能。这项工作为跨分析纵向数据提供了一种更灵活的训练相关模型的工具。

    Multi-task learning has emerged as a powerful machine learning paradigm for integrating data from multiple sources, leveraging similarities between tasks to improve overall model performance. However, the application of multi-task learning to real-world settings is hindered by data-sharing constraints, especially in healthcare settings. To address this challenge, we propose a flexible multi-task learning framework utilizing summary statistics from various sources. Additionally, we present an adaptive parameter selection approach based on a variant of Lepski's method, allowing for data-driven tuning parameter selection when only summary statistics are available. Our systematic non-asymptotic analysis characterizes the performance of the proposed methods under various regimes of the sample complexity and overlap. We demonstrate our theoretical findings and the performance of the method through extensive simulations. This work offers a more flexible tool for training related models across
    
[^52]: 基于数据驱动的相对不确定性测度用于误分类检测

    A Data-Driven Measure of Relative Uncertainty for Misclassification Detection. (arXiv:2306.01710v1 [stat.ML])

    [http://arxiv.org/abs/2306.01710](http://arxiv.org/abs/2306.01710)

    本文提出了一种基于数据驱动的相对不确定性度量，用于误分类检测。该度量可以通过学习软预测的分布模式，识别出被误分类的样本，并展示了在多个图像分类任务中的实证改进，优于现有的误分类检测方法。

    

    误分类检测是机器学习中的一个重要问题，它可以识别模型预测不可靠的实例。然而，传统的不确定性测度如香农熵并不能提供一种有效的方式来推断模型预测的实际不确定性。本文提出了一种新颖的数据驱动相对不确定性度量，用于误分类检测。通过学习软预测的分布模式，我们的不确定性度量可以基于预测的类概率标识被误分类的样本。有趣的是，根据所提出的度量，与误分类实例对应的软预测可能具有很大的不确定性，即使它们的香农熵可能很低。我们展示了多个图像分类任务中的实证改进，优于现有的误分类检测方法。

    Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. In this paper, we introduce a novel data-driven measure of relative uncertainty to an observer for misclassification detection. By learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. Interestingly, according to the proposed measure, soft-predictions that correspond to misclassified instances can carry a large amount of uncertainty, even though they may have low Shannon entropy. We demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.
    
[^53]: 略微超参数化的ReLU网络具有有利的损失景观

    Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])

    [http://arxiv.org/abs/2305.19510](http://arxiv.org/abs/2305.19510)

    本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。

    

    本文研究了有限输入数据集上，二层略微超参数化ReLU神经网络的损失景观，使用了参数映射的Jacobian矩阵的秩来估计局部和全局极小值集的维度。使用随机二进制矩阵的结果，我们证明大多数激活模式对应的参数区域没有坏的可微局部极小值。此外，对于一维输入数据，我们证明了网络可以通过大多数的激活模式实现高维全局极小值集合而不具有坏的局部极小值。我们通过发现大多数区域具有完整的秩或缺乏秩，以实验的方式证实了这些结果，这取决于超参数的数量。

    We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
    
[^54]: 矩阵分解中交替梯度下降的收敛性分析

    Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])

    [http://arxiv.org/abs/2305.06927](http://arxiv.org/abs/2305.06927)

    本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。

    

    本文考虑了应用于不对称矩阵分解目标的具有固定步长$\eta>0$的交替梯度下降（AGD）。我们证明了，对于秩为$r$的矩阵$\mathbf {A}\in \mathbb {R} ^ {m \times n}$，$T=\left(\left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2\log(1/\epsilon)\right)$次交替梯度下降即可从非典型随机初始化高概率地达到$\epsilon$-最优分解$\|\mathbf {A}\mathbf {X}_T^{\vphantom{\intercal}}\mathbf {Y}_T^{\intercal}\|_{\rm F}^2\le\epsilon\|\mathbf {A}\|_{\rm F}^2$。分解中因子的秩为$d>r$，因此$\mathbf{X}_T\in\mathbb{R}^{m \times d}$且$\mathbf{Y}_T\in\mathbb{R}^{n \times d}$。实验表明，我们提出的初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。我们的证明概念上很简单：一致的PL不等式和一致的Lipschitz平滑性。

    We consider alternating gradient descent (AGD) with fixed step size $\eta > 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, $T = \left( \left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2 \log(1/\epsilon)\right)$ iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization $\| \mathbf{A} \mathbf{X}_T^{\vphantom{\intercal}} \mathbf{Y}_T^{\intercal} \|_{\rm F}^2 \leq \epsilon \| \mathbf{A} \|_{\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d>r$ so that $\mathbf{X}_T\in\mathbb{R}^{m \times d}$ and $\mathbf{Y}_T \in\mathbb{R}^{n \times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne
    
[^55]: 正半定矩阵的极值特征值差分几何

    Differential geometry with extreme eigenvalues in the positive semidefinite cone. (arXiv:2304.07347v1 [math.DG])

    [http://arxiv.org/abs/2304.07347](http://arxiv.org/abs/2304.07347)

    本文提出了一种基于半定锥中的汤普森几何学的可扩展几何框架，利用极广义特征值有效地分析和处理对称正定矩阵数据。同时，基于此几何方法，定义了一种新型 SPD 矩阵迭代平均算法，证明了其存在性和唯一性。

    

    对称正定矩阵 (SPD) 数据的微分几何方法已被成功应用于计算机视觉、医学成像和机器学习等多个领域。然而，现有几何范式的谱计算成本高昂，难以在高维度和大规模数据下实现。本文基于半定锥的希尔伯特和汤普森几何学提出了计算极广义特征值的可扩展几何框架，构建了基于汤普森几何学的测地空间结构，并证明了这一结构的多个属性。此外，定义了一个基于该几何方法的新型 SPD 矩阵迭代平均，并证明了在给定有限个 SPD 矩阵的情况下其存在性和唯一性。

    Differential geometric approaches to the analysis and processing of data in the form of symmetric positive definite (SPD) matrices have had notable successful applications to numerous fields including computer vision, medical imaging, and machine learning. The dominant geometric paradigm for such applications has consisted of a few Riemannian geometries associated with spectral computations that are costly at high scale and in high dimensions. We present a route to a scalable geometric framework for the analysis and processing of SPD-valued data based on the efficient computation of extreme generalized eigenvalues through the Hilbert and Thompson geometries of the semidefinite cone. We explore a particular geodesic space structure based on Thompson geometry in detail and establish several properties associated with this structure. Furthermore, we define a novel iterative mean of SPD matrices based on this geometry and prove its existence and uniqueness for a given finite collection of 
    
[^56]: 使用来自成对或$K$元比较的人类反馈的规范强化学习

    Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270)

    该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。

    

    我们为带有人类反馈的强化学习问题提供了一个理论框架。我们的分析表明，当真实奖励函数为线性函数时，最大似然估计（MLE）在Bradley-Terry-Luce（BTL）模型和Plackett-Luce（PL）模型下均收敛。然而，我们发现当基于学得的奖励模型训练策略时，MLE会失败，而基于悲观估计的MLE在一定的覆盖假设下提供性能更好的策略。此外，我们证明在PL模型下，真实MLE和将$k$元比较拆分为成对比较的备选MLE都收敛。而真实MLE是渐近更为高效的。我们的结果验证了现有RLHF算法（如InstructGPT）的实验成功，并为算法设计提供了新的见解。此外，我们的结果统一了RLHF问题和最大熵反向强化学习(IRL)问题，并为其提供了第一个样本复杂度界。

    We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
    
[^57]: 经验过程的实例相关的一致尾部界

    Instance-dependent uniform tail bounds for empirical processes. (arXiv:2209.10053v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2209.10053](http://arxiv.org/abs/2209.10053)

    该论文提出了一个经验过程的统一尾部界，该尾部界以函数的个体偏差而不是在考虑的类中的最坏情况偏差为基础。

    

    我们提出了一个经验过程的统一尾部界，该尾部界以函数类为指标，以函数的个体偏差而不是在考虑的类中的最坏情况偏差为基础。通过将标准通用链接论证引入一个最初的“泄气”步骤来建立尾部界。生成的尾部界有一个主要的复杂度组成部分，即一个关于泄气函数类的 Talagrand $\gamma$ 函数的变体，以及一个实例相关的偏差项，通过一个适当缩放的适当范数的版本来衡量。这些项都使用基于相关的母函数的某些系数来表达。当函数类在给定的（指数型）Orlicz空间中时，我们还提供了更明确的近似值来描述所提到的系数。

    We formulate a uniform tail bound for empirical processes indexed by a class of functions, in terms of the individual deviations of the functions rather than the worst-case deviation in the considered class. The tail bound is established by introducing an initial "deflation" step to the standard generic chaining argument. The resulting tail bound has a main complexity component, a variant of Talagrand's $\gamma$ functional for the deflated function class, as well as an instance-dependent deviation term, measured by an appropriately scaled version of a suitable norm. Both of these terms are expressed using certain coefficients formulated based on the relevant cumulant generating functions. We also provide more explicit approximations for the mentioned coefficients, when the function class lies in a given (exponential type) Orlicz space.
    
[^58]: 不确定性的不平等影响：平权行动与平权信息

    The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.10019](http://arxiv.org/abs/2102.10019)

    本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。

    This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.

    像贷款批准、医疗干预和大学录取这样的关键决策是在存在不确定性的情况下进行预测的。在本文中，我们证明了不确定性具有不平等的影响。虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化：平均结果较高的群体通常被分配更高的假阳性率，而平均结果较低的群体则被分配更高的假阴性率。我们展示了额外的数据获取可以消除这种差异并扩大机会的获取。我们称之为平权信息的策略可以作为平权行动的替代方案。

    Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
    

