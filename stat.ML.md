# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Diffusion Self-Guidance for Controllable Image Generation.](http://arxiv.org/abs/2306.00986) | 本论文提出了一种扩散自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制，可以用于执行具有挑战性的图像操作，同时不需要额外模型或训练。 |
| [^2] | [Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions.](http://arxiv.org/abs/2306.00904) | 本文提出了一种用于高阶相互作用的测量方法和基于核的测试方法，并与格理论建立了数学联系，为增强相互作用模型提供了方法。 |
| [^3] | [Non-stationary Reinforcement Learning under General Function Approximation.](http://arxiv.org/abs/2306.00861) | 本文提出了一种新的置信集模型自由算法 SW-OPEA，其具有滑动窗口机制和新的置信集设计来解决非平稳 MDP 问题 |
| [^4] | [Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case.](http://arxiv.org/abs/2306.00857) | 本论文为损失最小化分类树的解决提供了一个通用框架，基于逻辑斯蒂回归的解决方式具有具有解释性特征和竞争泛化能力。 |
| [^5] | [When Does Bottom-up Beat Top-down in Hierarchical Community Detection?.](http://arxiv.org/abs/2306.00833) | 本文研究了使用自下而上算法恢复Hierarchical Stochastic Block Model的树形结构和社区结构的理论保证，并确定了其在中间层次上达到了确切恢复信息理论阈值。 |
| [^6] | [Initial Guessing Bias: How Untrained Networks Favor Some Classes.](http://arxiv.org/abs/2306.00809) | 本文提出了“初始猜测偏差”现象，即在未经过训练的神经网络中，由于架构选择的影响，模型往往会将所有预测指向同一个类别。该现象对架构选择和初始化有实际指导意义，并具有理论后果，例如节点置换对称性的崩溃和深度带来的非平凡差异。 |
| [^7] | [Birth of a Transformer: A Memory Viewpoint.](http://arxiv.org/abs/2306.00802) | 本文研究了transformers如何平衡全局分布和上下文特定分布的两种知识类型，并提供了有关权值矩阵作为联想记忆的作用及梯度如何实现权重学习的理论见解。 |
| [^8] | [Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation.](http://arxiv.org/abs/2306.00788) | 本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。 |
| [^9] | [Data Interpolants -- That's What Discriminators in Higher-order Gradient-regularized GANs Are.](http://arxiv.org/abs/2306.00785) | 本文讨论了在高阶梯度正则化生成对抗网络中优化辨别器的问题，发现最优辨别器问题是$n$维内插问题，并使用多项RBF内插闭式求解，实现更优的性能。 |
| [^10] | [An End-to-End Time Series Model for Simultaneous Imputation and Forecast.](http://arxiv.org/abs/2306.00778) | 本文提出了一种端到端的时间序列模型，可以同时进行缺失值插补和预测，实验结果表明，该模型在这两个任务上表现良好。 |
| [^11] | [Inference and Sampling of Point Processes from Diffusion Excursions.](http://arxiv.org/abs/2306.00762) | 该论文提出了一种从潜在扩散过程中推断和采样点过程的方法，该方法通过将连续路径空间中的扩散返回时间与点过程的新到达相关联，为许多学科中的点过程提供了基础。 |
| [^12] | [Going Deeper with Spectral Embeddings.](http://arxiv.org/abs/2306.00742) | 本文提出两种新的谱嵌入方法，一种基于函数分析原理和核方法，另一种基于深度网络优化损失，提供理论保证和实际有效的算法，并提供新的采样算法。 |
| [^13] | [A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration.](http://arxiv.org/abs/2306.00740) | 深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。 |
| [^14] | [Sharper Bounds for $\ell_p$ Sensitivity Sampling.](http://arxiv.org/abs/2306.00732) | 该论文研究了$\ell_p$子空间嵌入的灵敏度采样界限，取得了比通用界限更好的结果，对于$1\leq p<2$的情况下，界限达到了$\mathfrak{S}^{2/p}$，对于$2<p<\infty$的情况下，界限达到了$\mathfrak{S}^{2-2/p}$。 |
| [^15] | [Balanced Training of Energy-Based Models with Adaptive Flow Sampling.](http://arxiv.org/abs/2306.00684) | 本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。 |
| [^16] | [Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise.](http://arxiv.org/abs/2306.00673) | 本文提出一种新算法，可以属性高效的学习低次多项式阈值函数，并能够在噪声下进行PAC学习。 |
| [^17] | [Learning Prescriptive ReLU Networks.](http://arxiv.org/abs/2306.00651) | 本文提出了一种名为P-ReLU的神经网络模型，用于从观测数据中学习最佳策略。该模型可平衡指导性能和可解释性，具有灵活性和有效性，在多个基准测试中均达到最佳精度。 |
| [^18] | [Byzantine-Robust Clustered Federated Learning.](http://arxiv.org/abs/2306.00638) | 本文研究了在联邦学习环境下，非拜占庭机器可以被分成不相交的聚类，而拜占庭机器可能会对任何聚类实行攻击，破坏训练过程的问题，提出了鲁棒性聚类联邦学习算法。 |
| [^19] | [Unfair Utilities and First Steps Towards Improving Them.](http://arxiv.org/abs/2306.00636) | 该论文提出了一个新的公平框架——考虑政策优化哪个效用，定义了信息价值公平，提出不应使用不满足这一标准的实用程序，并探讨了修改实用程序以满足此公平标准可能对最优政策产生的影响。 |
| [^20] | [From Temporal to Contemporaneous Iterative Causal Discovery in the Presence of Latent Confounders.](http://arxiv.org/abs/2306.00624) | 在潜在混淆因素存在的情况下，我们提出了一种逐步完善因果图的算法，先学习长期的时态关系而不是短期的关系，最后才学习同时性关系。这种方法减少了统计检验次数，提高了因果图的准确性。 |
| [^21] | [On the Effectiveness of Hybrid Mutual Information Estimation.](http://arxiv.org/abs/2306.00608) | 本文研究了混合互信息估计的有效性，提出了一种混合方法以应对判别式和生成式方法各自缺点，同时提出了一种名为预测量化的生成方法，与判别式估计器结合可获得更精确的互信息估计结果。 |
| [^22] | [Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification.](http://arxiv.org/abs/2306.00560) | 该论文提出了一种基于Wasserstein距离的损失函数hinge-Wasserstein，用于缓解回归任务中由于过度自信导致的不确定性问题。这种损失函数有效提高了aleatoric和epistemic不确定性的质量。 |
| [^23] | [Decomposing Global Feature Effects Based on Feature Interactions.](http://arxiv.org/abs/2306.00541) | 提出了全局效应广义可加分解（GADGET）框架，能够最小化特征交互作用的本地特征效应的交互异质性。同时适用于偏依赖、积累局部效应和Shapley可加解释（SHAP）依赖的边际特征效应可视化方法，并提出了一种新的基于置换的交互测试来检测显着的特征交互作用。 |
| [^24] | [A New PHO-rmula for Improved Performance of Semi-Structured Networks.](http://arxiv.org/abs/2306.00522) | 我们提出了一种非侵入式的后处理正交化（PHO）方法，旨在保证模型组成部分的识别性，提供更好的估计和预测质量。 |
| [^25] | [On Masked Pre-training and the Marginal Likelihood.](http://arxiv.org/abs/2306.00520) | 遮蔽预训练利用累积打分函数实现了对模型的边缘似然极大化，提出了设计适当的自监督方法来训练贝叶斯模型的理论。实验证实了这一理论，并探索了遮蔽预训练的主要学习原理。 |
| [^26] | [The Risks of Recourse in Binary Classification.](http://arxiv.org/abs/2306.00497) | 研究发现，在二分分类中提供追索权会增加错误率，导致更多错误的发生。提供算法追索权可能也会在系统级别上给予不利。 |
| [^27] | [Sharded Bayesian Additive Regression Trees.](http://arxiv.org/abs/2306.00361) | 本文提出了一种基于分片的贝叶斯加性回归树模型，通过引入随机化辅助变量和分片树结构，采用贝叶斯加性回归树适配每个分区组件到一个子模型中进行数据分区，引入交集树结构来完全使用树结构指定分片和建模。研究中还推导了理论最优权重和证明了模型复杂度。 |
| [^28] | [Efficient and Robust Bayesian Selection of Hyperparameters in Dimension Reduction for Visualization.](http://arxiv.org/abs/2306.00357) | 本文提出了一种利用有代理模型的贝叶斯优化进行数据驱动敏感性分析和权衡多目标的维数约减参数选择方法，并在多个合成和现实数据集上进行评估，提供了一种稳健且高效的解决方案。 |
| [^29] | [Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective.](http://arxiv.org/abs/2306.00353) | 本研究提出了一个基于概率视角的对抗样本构建方法，可以生成语义感知的对抗性样本，并可以有效规避传统对抗性攻击的强化对抗训练方法。 |
| [^30] | [Improving Energy Conserving Descent for Machine Learning: Theory and Practice.](http://arxiv.org/abs/2306.00352) | 这篇论文介绍了能量守恒下降（ECD）的理论，提出了基于梯度的优化算法ECDSep，能够处理凸优化问题和非凸优化问题，通过改进动态和混沌诱导元素，提高性能；在各种机器学习问题上与流行的优化方法进行了实证比较，发现在每个任务中具有竞争力或改进的性能。 |
| [^31] | [BOtied: Multi-objective Bayesian optimization with tied multivariate ranks.](http://arxiv.org/abs/2306.00344) | BOtied 是一种带有相关多元等级的多目标贝叶斯优化算法，相较于现有的方法具有更高的样本效率和较好的近似质量。 |
| [^32] | [Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks.](http://arxiv.org/abs/2306.00342) | 本文提出了一种显式正则化方法，与隐式正则化结合，可以使单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。 |
| [^33] | [(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy.](http://arxiv.org/abs/2306.00312) | 本论文提出了一种基于不一致性偏差的方法，通过一个简单、直观的条件推导出深度神经网络在分布转移下的(几乎)可证明误差界限，相比于$\mathcal{H}\Delta\mathcal{H}$-divergence更紧密、更易评价。 |
| [^34] | [Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications.](http://arxiv.org/abs/2306.00280) | 本文面向非均匀和时变通信故障问题，提出了一种名为FedPBC的联邦学习算法来修正FedAvg的偏差，该算法构建二叉树来减轻非均匀通信故障的影响，并在非凸任务上实验结果表现优于FedAvg。 |
| [^35] | [Provable Benefit of Mixup for Finding Optimal Decision Boundaries.](http://arxiv.org/abs/2306.00267) | 本研究证明了使用Mixup训练具有可证实的益处，可以显著降低在更可分离数据分布中寻找最佳决策边界的样本复杂度。 |
| [^36] | [A polynomial-time iterative algorithm for random graph matching with non-vanishing correlation.](http://arxiv.org/abs/2306.00266) | 本论文提出了一种具有非零相关性的随机图匹配的多项式迭代算法，并且在边缘相关性非零时成功恢复潜在匹配。 |
| [^37] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^38] | [Combinatorial Neural Bandits.](http://arxiv.org/abs/2306.00242) | 该论文提出了两个组合神经臂算法，通过使用深度神经网络来近似未知得分函数，这是处理类似问题的第一个算法框架。 |
| [^39] | [Generalized Implicit Follow-The-Regularized-Leader.](http://arxiv.org/abs/2306.00201) | 提出了一种新的在线学习算法——广义隐式Follow-The-Regularized-Leader (FTRL)，它可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则。该算法的关键思想是用Fenchel-Young不等式代替损失的线性化，可以直接改进最坏情况下的后悔上限。 |
| [^40] | [Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption.](http://arxiv.org/abs/2306.00196) | 本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。 |
| [^41] | [Conditionally Strongly Log-Concave Generative Models.](http://arxiv.org/abs/2306.00181) | 本论文介绍了条件强对数凹模型(CSLC)。我们将数据分布分解成一组满足强对数凹条件的条件概率分布的乘积形式。通过适应于数据分布的正交投影器得到分解，提供了有效的参数估计和采样算法，同时提供了理论保证。 |
| [^42] | [On the Expressive Power of Neural Networks.](http://arxiv.org/abs/2306.00145) | 本文研究了神经网络的表达能力，证明了某些问题在之前的研究中未得到解决，并提出了新问题的回答，包括广而浅的ReLU网络不能被深而窄的ReLU网络很好地逼近等。 |
| [^43] | [On Mixing Rates for Bayesian CART.](http://arxiv.org/abs/2306.00126) | 本文研究了贝叶斯CART算法中的混合速率，并提供了一些充分条件。作者指出，基于生长和修剪步骤的贝叶斯CART无法快速到达深度孤立信号。作者提出了Twiggy贝叶斯CART以解决这个问题。 |
| [^44] | [Pareto Front Identification with Regret Minimization.](http://arxiv.org/abs/2306.00096) | 本文提出了适用于线性Bandit的Pareto前沿识别算法，该算法使用所有动作的上下文信息，具有较低的样本复杂度，并同时保证了Pareto前沿识别和Pareto遗憾最小化。 |
| [^45] | [A General Framework for Equivariant Neural Networks on Reductive Lie Groups.](http://arxiv.org/abs/2306.00091) | 本文提出了一个通用的等变神经网络架构，能够尊重任何一个约化李群G的有限维表示的对称性，并通过在多个领域的任务上进行实验来证明其性能。 |
| [^46] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |
| [^47] | [Shadows of quantum machine learning.](http://arxiv.org/abs/2306.00061) | 量子机器学习模型需要使用量子计算机进行评估，但我们提出在训练完后，使用量子计算机生成一个经典阴影模型来计算函数的经典计算近似，避免了对量子计算机的需求。 |
| [^48] | [Label Embedding by Johnson-Lindenstrauss Matrices.](http://arxiv.org/abs/2305.19470) | 这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。 |
| [^49] | [Conformal Prediction with Large Language Models for Multi-Choice Question Answering.](http://arxiv.org/abs/2305.18404) | 本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。 |
| [^50] | [Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching.](http://arxiv.org/abs/2305.17884) | 本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。 |
| [^51] | [Counterfactual Formulation of Patient-Specific Root Causes of Disease.](http://arxiv.org/abs/2305.17574) | 本文提出了一种针对疾病患者个体的根本原因的新公式，可以用于自动从数据中检测根本原因，并考虑了噪声标签和疾病流行率等因素，同时具有快速计算的优势。 |
| [^52] | [The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent.](http://arxiv.org/abs/2305.17490) | 本文研究了随机梯度下降的动态稳定性隐式正则化，证明了其具有良好的泛化性。 |
| [^53] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | 本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。 |
| [^54] | [Optimal Learning via Moderate Deviations Theory.](http://arxiv.org/abs/2305.14496) | 本文提出了一种能够在广泛模型中进行最优学习的方法，利用中度偏差原理构建高度准确的置信区间，满足指数精度、一致性和最大精度等标准，为该方法提供了理论依据。 |
| [^55] | [Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach.](http://arxiv.org/abs/2305.04560) | 本文通过将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，提出了在这些流形上构建神经网络的新模型和新层，并在人类动作识别和知识图谱完成两个应用中展示了其有效性。 |
| [^56] | [Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value.](http://arxiv.org/abs/2304.07718) | Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。 |
| [^57] | [A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models.](http://arxiv.org/abs/2304.04916) | 本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。 |
| [^58] | [Extrapolation to complete basis-set limit in density-functional theory by quantile random-forest models.](http://arxiv.org/abs/2303.14760) | 本文利用分位随机森林模型外推完备基组极限，并提供了预测区间以量化模型的不确定性。 |
| [^59] | [Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss.](http://arxiv.org/abs/2303.03027) | 本文使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型，并在有限秩矩阵空间内表征关键点和最小化问题，最终确定了梯度下降算法的收敛性。 |
| [^60] | [Near-optimal learning with average H\"older smoothness.](http://arxiv.org/abs/2302.06005) | 通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。 |
| [^61] | [The SSL Interplay: Augmentations, Inductive Bias, and Generalization.](http://arxiv.org/abs/2302.02774) | 本论文通过分析数据增强、网络架构和训练算法的相互作用，研究了预训练和下游任务的泛化性能，并为SSL从业人员提供了一些见解。 |
| [^62] | [Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback.](http://arxiv.org/abs/2301.13791) | 本文提出了一个改进的算法来解决多时期多分类装载问题，其中使用bandit反馈，提出的算法具有更快的收敛速率和更低的遗憾，同时解决了一个相关问题。 |
| [^63] | [Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates.](http://arxiv.org/abs/2301.11294) | 本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。 |
| [^64] | [Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons.](http://arxiv.org/abs/2301.11270) | 该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。 |
| [^65] | [Graph Neural Tangent Kernel: Convergence on Large Graphs.](http://arxiv.org/abs/2301.10808) | 本文利用GNTK和图核函数探究大规模图神经网络的训练动态，证明了GNTK在收敛于图核函数时的确切性质。这意味着在大规模图的情况下，可以在中等大小的图上进行拟合并在整个图上使用。 |
| [^66] | [Optimal randomized multilevel Monte Carlo for repeatedly nested expectations.](http://arxiv.org/abs/2301.04095) | 本论文提出了一种名为$\mathsf{READ}$的新的蒙特卡罗估计器，具有每个固定$D$的最优计算成本$\mathcal{O}(\varepsilon^{-2})$以及在更一般的假设下，任意$0 < \delta < \frac{1}{2}$几乎最优的计算成本$\mathcal{O}(\varepsilon^{-2(1 + \delta)})$。 |
| [^67] | [Privately Estimating a Gaussian: Efficient, Robust and Optimal.](http://arxiv.org/abs/2212.08018) | 本论文提出了在纯和近似差分隐私模型下，对于未知的 $d$ 维高斯分布进行任意微小差异总变差误差的高效算法，并容忍恶意离群值的存在， 样本复杂度与维度的依赖是最优的；我们还证明了样本上界与目标协方差矩阵的条件数 $\kappa$ 的依赖关系也是最紧密的。 |
| [^68] | [Bipartite Mixed Membership Distribution-Free Model. A novel model for community detection in overlapping bipartite weighted networks.](http://arxiv.org/abs/2211.00912) | 提出了一种无分布双分块混合成员模型（BiMMDF），可用于重叠双分块加权网络的社区发现，并可以模拟重叠双分块符号网络。该模型的估计具有一致性保证和理论分离条件，并可提高在合成网络和现实网络应用中的性能。 |
| [^69] | [Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models.](http://arxiv.org/abs/2210.15458) | 本文章提出了一种算术采样框架，该方法可兼容常见的采样变化，具有可证明的束多样性和令人尴尬的并行性，从原始模型提供无偏和一致的期望。在WMT机器翻译中表现出良好的效果。 |
| [^70] | [Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup.](http://arxiv.org/abs/2210.13512) | 本文提出了一种基于中点 Mixup 的多视角数据学习方法，相比于传统经验风险最小化方法能够更好地学习每个类别的所有特征，具有更好的泛化和鲁棒性。 |
| [^71] | [On the Identifiability and Estimation of Causal Location-Scale Noise Models.](http://arxiv.org/abs/2210.09054) | 本文研究了一类异方差噪声模型，发现除特殊情况外因果方向是可识别的。提出了两个估计器，能够准确识别因果效应。 |
| [^72] | [Near-optimal fitting of ellipsoids to random points.](http://arxiv.org/abs/2208.09493) | 本文解决了椭圆拟合问题的一个猜想，对于给定高斯点$v_1,\ldots,v_n$，我们可以构造出一个对称于原点的拟合椭圆，当$n\geq \Omega(\,d^2/\mathrm{polylog}(d)\,)$时，该椭圆实现了近似最优匹配。 |
| [^73] | [A Theoretical Analysis of the Learning Dynamics under Class Imbalance.](http://arxiv.org/abs/2207.00391) | 本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。 |
| [^74] | [On the Identifiability of Nonlinear ICA: Sparsity and Beyond.](http://arxiv.org/abs/2206.07751) | 本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。 |
| [^75] | [Improving adversarial robustness by putting more regularizations on less robust samples.](http://arxiv.org/abs/2206.03353) | 本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。 |
| [^76] | [Static Scheduling with Predictions Learned through Efficient Exploration.](http://arxiv.org/abs/2205.15695) | 本文研究了单机作业调度的问题，提出了一种基于学习预测的静态调度算法，在类型未知的情况下实现了次线性的过剩成本，尤其在抢占式问题中表现出色，可以在不同作业类型持续时间相差很大时优于非抢占匹配。 |
| [^77] | [MixFlows: principled variational inference via mixed flows.](http://arxiv.org/abs/2205.07475) | 本文提出了混合变分流（MixFlows），是由对初始参考分布的映射重复应用的混合组成的一种新的变分家族。MixFlows具有类似于MCMC的收敛保证，并可以提供比几种黑盒归一化流更可靠的后验逼近，与最先进的MCMC方法所获得的样本质量相当。 |
| [^78] | [A Law of Robustness beyond Isoperimetry.](http://arxiv.org/abs/2202.11592) | 本文提出了一种两层次的稳健性律法，研究了在任意数据分布下的稳健插值问题，并证明其Lipschitz下界分别为$\Omega(\sqrt{n/p})$和$\Omega(n^{1/d})$。 |
| [^79] | [NN2Poly: A polynomial representation for deep feed-forward artificial neural networks.](http://arxiv.org/abs/2112.11397) | 本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。 |
| [^80] | [Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities.](http://arxiv.org/abs/2111.08851) | 本文提出新的一种排除限制的排序一致性序回归方法，基于深度神经网络和条件概率的建模，可以在保持输出概率排序一致性的同时，提高了模型的表现。 |
| [^81] | [Rating transitions forecasting: a filtering approach.](http://arxiv.org/abs/2109.10567) | 本文提出了一种基于滤波算法的评级迁移预测方法，通过点过程滤波框架高效地推断出评级迁移的隐藏因素，并根据商业周期进行预测，从而实时揭示和检测影响评级迁移动态的经济变化。 |
| [^82] | [On Tilted Losses in Machine Learning: Theory and Applications.](http://arxiv.org/abs/2109.06141) | 本文研究了一种在机器学习中不常见但在统计、概率与优化等领域常用的技术——指数倾斜，并将其应用到风险最小化中。所提出的方法可以修正单个损失的影响，增加或减少异常值的作用，可以提高泛化性能，并可以被视为损失的尾部概率的平滑近似。 |
| [^83] | [Model Transferability With Responsive Decision Subjects.](http://arxiv.org/abs/2107.05911) | 本论文研究在响应式和交互式数据分布下，算法预测器的可迁移性问题，提供了性能差距的上界和分类器必须满足的权衡的下界，并刻画了预测器、策略空间选择和偏移性质对可迁移性程度的影响。 |
| [^84] | [Near Optimal Adversarial Attack on UCB Bandits.](http://arxiv.org/abs/2008.09312) | 本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。 |
| [^85] | [Graph Clustering with Graph Neural Networks.](http://arxiv.org/abs/2006.16904) | 本文探讨了图神经网络在图聚类这一无监督问题上的缺陷，并提出了一种名为DMoN的无监督汇聚方法，可以有效地解决聚类恢复问题。 |

# 详细

[^1]: 可控图像生成的扩散自导方法

    Diffusion Self-Guidance for Controllable Image Generation. (arXiv:2306.00986v1 [cs.CV])

    [http://arxiv.org/abs/2306.00986](http://arxiv.org/abs/2306.00986)

    本论文提出了一种扩散自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制，可以用于执行具有挑战性的图像操作，同时不需要额外模型或训练。

    

    大规模生成模型能够从详细文本描述中生成高质量的图像。然而，图像的许多方面很难或不可能通过文本来传达。我们引入了自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制。我们展示了可以从这些表示中提取出对象的形状、位置和外观等属性并用于指导采样。自导类似于分类器引导，但是使用预训练模型本身中存在的信号，不需要额外的模型或训练。我们展示了如何组合一组简单的属性来执行具有挑战性的图像操作，例如修改对象的位置或大小，将一个图像中的对象外观与另一个图像的布局相结合，将多个图像的对象组合成一个，等等。我们还展示了自导可以用于编辑真实图像。

    Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images
    
[^2]: 相互作用测量，分区格和核测试用于高阶相互作用

    Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions. (arXiv:2306.00904v1 [stat.ML])

    [http://arxiv.org/abs/2306.00904](http://arxiv.org/abs/2306.00904)

    本文提出了一种用于高阶相互作用的测量方法和基于核的测试方法，并与格理论建立了数学联系，为增强相互作用模型提供了方法。

    

    仅依赖于成对关系的模型往往无法捕捉到各种领域（如社会经济、生态或生物医学系统）中找到的复杂多变量数据的完整统计结构。两个以上变量组之间的非平凡依赖关系在这些系统的分析和建模中可以发挥重要作用，但从数据中提取这样的高阶相互作用仍然具有挑战性。本文引入了一系列$d$-order ($d \geq 2$)相互作用测量，依次包括可能的联合概率分布分解，并定义了非参数、基于核的测试，以系统地确定$d$-order相互作用的统计显着性。同时，我们建立了与格理论的数学联系，阐明了相互作用度量的导出及其复合排列测试的涵义；澄清了单纯复合体与核矩阵中心化的联系；并提供了一种增强相互作用模型的方法。

    Models that rely solely on pairwise relationships often fail to capture the complete statistical structure of the complex multivariate data found in diverse domains, such as socio-economic, ecological, or biomedical systems. Non-trivial dependencies between groups of more than two variables can play a significant role in the analysis and modelling of such systems, yet extracting such high-order interactions from data remains challenging. Here, we introduce a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly inclusive of possible factorisations of the joint probability distribution, and define non-parametric, kernel-based tests to establish systematically the statistical significance of $d$-order interactions. We also establish mathematical links with lattice theory, which elucidate the derivation of the interaction measures and their composite permutation tests; clarify the connection of simplicial complexes with kernel matrix centring; and provide a means to enhan
    
[^3]: 一般函数逼近下的非平稳强化学习

    Non-stationary Reinforcement Learning under General Function Approximation. (arXiv:2306.00861v1 [cs.LG])

    [http://arxiv.org/abs/2306.00861](http://arxiv.org/abs/2306.00861)

    本文提出了一种新的置信集模型自由算法 SW-OPEA，其具有滑动窗口机制和新的置信集设计来解决非平稳 MDP 问题

    

    一般函数逼近是处理广泛强化学习场景中的大状态和动作空间的一种强有力的工具。然而，对于具有一般函数逼近的非平稳MDP的理论理解仍然有限。本文首次尝试解决此问题。我们首先为非平稳MDP提出了一个称为动态贝尔曼难度维数（DBE）的新复杂度度量，该度量包含静态MDP中大多数现有易处理的RL问题以及非平稳MDP。基于所提出的复杂度度量，我们提出了一种新的置信集模型自由算法SW-OPEA, 其具有滑动窗口机制和新的置信集设计来解决非平稳MDP问题。然后，我们建立了所提出算法的动态后悔上界，并表明，只要变化预算不是显著大，SW-OPEA可以证明是有效的。我们还通过非平稳线性和表格化环境的示例证明，SW-OPEA可以比现有方法更有效地学习接近最优策略的策略。

    General function approximation is a powerful tool to handle large state and action spaces in a broad range of reinforcement learning (RL) scenarios. However, theoretical understanding of non-stationary MDPs with general function approximation is still limited. In this paper, we make the first such an attempt. We first propose a new complexity metric called dynamic Bellman Eluder (DBE) dimension for non-stationary MDPs, which subsumes majority of existing tractable RL problems in static MDPs as well as non-stationary MDPs. Based on the proposed complexity metric, we propose a novel confidence-set based model-free algorithm called SW-OPEA, which features a sliding window mechanism and a new confidence set design for non-stationary MDPs. We then establish an upper bound on the dynamic regret for the proposed algorithm, and show that SW-OPEA is provably efficient as long as the variation budget is not significantly large. We further demonstrate via examples of non-stationary linear and tab
    
[^4]: 损失最小化分类树：一个通用的框架和逻辑斯蒂回归情况研究

    Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case. (arXiv:2306.00857v1 [stat.ML])

    [http://arxiv.org/abs/2306.00857](http://arxiv.org/abs/2306.00857)

    本论文为损失最小化分类树的解决提供了一个通用框架，基于逻辑斯蒂回归的解决方式具有具有解释性特征和竞争泛化能力。

    

    分类树是可解释机器学习中最常见的模型之一，尽管这样的模型通常采用贪婪策略构建，但是近年来，由于混合整数规划（MIP）求解器的显著进展，已经开发了几种精确的学习问题的表述。本文认为其中一些最相关的训练模型可以封装在一个通用框架内，其实例由损失函数和正则化器的规定塑造。接下来，我们介绍了这个框架的新颖实现：具体来说，我们考虑逻辑损失，它在MIP设置中通过线性分段逼近处理，并与$\ell_1$-规则化项相结合。最终的最优逻辑树模型在数值上被证明能够诱导具有增强可解释性特征和竞争性泛化能力的树，与最先进的基于MIP的方法相比。

    The Classification Tree (CT) is one of the most common models in interpretable machine learning. Although such models are usually built with greedy strategies, in recent years, thanks to remarkable advances in Mixer-Integer Programming (MIP) solvers, several exact formulations of the learning problem have been developed. In this paper, we argue that some of the most relevant ones among these training models can be encapsulated within a general framework, whose instances are shaped by the specification of loss functions and regularizers. Next, we introduce a novel realization of this framework: specifically, we consider the logistic loss, handled in the MIP setting by a linear piece-wise approximation, and couple it with $\ell_1$-regularization terms. The resulting Optimal Logistic Tree model numerically proves to be able to induce trees with enhanced interpretability features and competitive generalization capabilities, compared to the state-of-the-art MIP-based approaches.
    
[^5]: 自下而上何时击败自上而下进行分层社区检测？

    When Does Bottom-up Beat Top-down in Hierarchical Community Detection?. (arXiv:2306.00833v1 [cs.SI])

    [http://arxiv.org/abs/2306.00833](http://arxiv.org/abs/2306.00833)

    本文研究了使用自下而上算法恢复Hierarchical Stochastic Block Model的树形结构和社区结构的理论保证，并确定了其在中间层次上达到了确切恢复信息理论阈值。

    

    网络的分层聚类是指查找一组社区的树形结构，其中层次结构的较低级别显示更细粒度的社区结构。解决这一问题的算法有两个主要类别：自上而下的算法和自下而上的算法。本文研究了使用自下而上算法恢复分层随机块模型的树形结构和社区结构的理论保证。我们还确定了这种自下而上算法在层次结构的中间层次上达到了确切恢复信息理论阈值。值得注意的是，这些恢复条件相对于现有的自上而下算法的条件来说，限制更少。

    Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for to
    
[^6]: 初始猜测偏差：未经过训练的神经网络倾向于某些类别

    Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])

    [http://arxiv.org/abs/2306.00809](http://arxiv.org/abs/2306.00809)

    本文提出了“初始猜测偏差”现象，即在未经过训练的神经网络中，由于架构选择的影响，模型往往会将所有预测指向同一个类别。该现象对架构选择和初始化有实际指导意义，并具有理论后果，例如节点置换对称性的崩溃和深度带来的非平凡差异。

    

    神经网络的初始状态在调节后续的训练过程中扮演重要角色。在分类问题的背景下，我们提供了理论分析，证明神经网络的结构可以在训练之前，甚至在不存在显式偏差的情况下，使模型将所有预测都指向同一个类别。我们展示了这种现象的存在，称为“初始猜测偏差”（Initial Guessing Bias，IGB），这取决于架构选择，例如激活函数、最大池化层和网络深度。我们对IGB进行的分析具有实际意义，可以指导架构的选择和初始化。我们还强调理论后果，例如节点置换对称性的崩溃、自平均的破坏、某些均场近似的有效性以及深度带来的非平凡差异。

    The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
    
[^7]: 一种记忆视角下的Transformer生成模型

    Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v1 [stat.ML])

    [http://arxiv.org/abs/2306.00802](http://arxiv.org/abs/2306.00802)

    本文研究了transformers如何平衡全局分布和上下文特定分布的两种知识类型，并提供了有关权值矩阵作为联想记忆的作用及梯度如何实现权重学习的理论见解。

    

    基于Transformer的大型语言模型取得了巨大的实证成功。然而，随着它们被广泛部署，越来越需要更好地理解它们的内部机制以使它们更加可靠。我们研究了transformers如何通过考虑一个合成的设置来平衡存储于它们之中的两种知识类型——全局分布和上下文特定的二元分布。通过对简化的两层Transformer的训练过程进行仔细的实证分析，我们阐述了对全局二元分布的快速学习以及对上下文中的二元分布的"归纳头"机制的较慢发展。我们强调了权值矩阵作为联想记忆的作用，提供了理论上的见解，说明了梯度如何在训练过程中实现权重的学习，并研究了数据分布的作用。

    Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributio
    
[^8]: 通过RKHS逼近理解基于增广的自监督表示学习

    Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation. (arXiv:2306.00788v1 [cs.LG])

    [http://arxiv.org/abs/2306.00788](http://arxiv.org/abs/2306.00788)

    本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。

    

    好的数据增强是自监督表示学习（如对比学习和掩码语言建模）实现经验成功的关键因素之一，但其在学习好的表示方面的理论理解仍然有限。最近的工作建立了自监督学习和逼近图拉普拉斯算子的顶部特征空间之间的联系。在这项工作中，我们利用这一洞察力对基于增广的预训练进行统计分析。我们从保持等距的属性出发，这是由增强给出的目标函数的关键几何特征。我们的第一主要定理为任意编码器提供了接近紧密的上限，用于估计通过在编码器之上拟合线性探测器而产生的估计误差和编码器学习的RKHS的逼近误差。我们的第二个主要定理表明，在温和条件下，可以通过RKHS函数任意精确地逼近增广函数。这个结果意味着，随着增广函数质量的提高，学习的编码器的表示能力也会提高。我们的分析还揭示了自监督学习中批量归一化和数据增强的作用。

    Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main
    
[^9]: 数据内插器——高阶梯度正则化生成对抗网络中的辨别器

    Data Interpolants -- That's What Discriminators in Higher-order Gradient-regularized GANs Are. (arXiv:2306.00785v1 [stat.ML])

    [http://arxiv.org/abs/2306.00785](http://arxiv.org/abs/2306.00785)

    本文讨论了在高阶梯度正则化生成对抗网络中优化辨别器的问题，发现最优辨别器问题是$n$维内插问题，并使用多项RBF内插闭式求解，实现更优的性能。

    

    本文考虑了在生成对抗网络（GAN）中对辨别器进行高阶梯度正则化优化的问题。通过最小二乘（LSGAN）和Wasserstein（WGAN） GAN变体的分析，我们证明了辨别器优化问题是n维内插问题。通过变分微积分导出的最优辨别器，实际上成为涉及迭代的Laplacian或polyharmonic算子的偏微分方程的解。通过polyharmonic算子的联系，我们称对应的GAN为Poly-LSGAN和Poly-WGAN，并通过在多元高斯上的实验验证表明，使用闭式RBF内插实现最优辨别器，惩罚阶数$m \approx\lceil \frac{n}{2} \rceil$，可以获得更优异的性能，相比于用任意选择的辨别器进行训练的GAN。

    We consider the problem of optimizing the discriminator in generative adversarial networks (GANs) subject to higher-order gradient regularization. We show analytically, via the least-squares (LSGAN) and Wasserstein (WGAN) GAN variants, that the discriminator optimization problem is one of interpolation in $n$-dimensions. The optimal discriminator, derived using variational Calculus, turns out to be the solution to a partial differential equation involving the iterated Laplacian or the polyharmonic operator. The solution is implementable in closed-form via polyharmonic radial basis function (RBF) interpolation. In view of the polyharmonic connection, we refer to the corresponding GANs as Poly-LSGAN and Poly-WGAN. Through experimental validation on multivariate Gaussians, we show that implementing the optimal RBF discriminator in closed-form, with penalty orders $m \approx\lceil \frac{n}{2} \rceil $, results in superior performance, compared to training GAN with arbitrarily chosen discri
    
[^10]: 一种端到端的时间序列模型，同时进行缺失值插补和预测

    An End-to-End Time Series Model for Simultaneous Imputation and Forecast. (arXiv:2306.00778v1 [cs.LG])

    [http://arxiv.org/abs/2306.00778](http://arxiv.org/abs/2306.00778)

    本文提出了一种端到端的时间序列模型，可以同时进行缺失值插补和预测，实验结果表明，该模型在这两个任务上表现良好。

    

    使用历史数据进行时间序列预测一直是一个有趣且具有挑战性的课题，尤其是当数据被缺失值损坏时。在许多工业问题中，学习辅助观测和目标变量之间的推理函数非常重要，因为当数据未完全观测到时，它提供了额外的知识。我们开发了一个端到端的时间序列模型，旨在学习这种推理关系并进行多步预测。我们的框架同时训练两个神经网络，一个用于学习特征相关性，另一个用于建模时间行为。我们的模型能够同时插补缺失条目并进行多步预测。实验表明，我们的框架在插补和预测任务的整体表现都很好，优于现有方法。

    Time series forecasting using historical data has been an interesting and challenging topic, especially when the data is corrupted by missing values. In many industrial problem, it is important to learn the inference function between the auxiliary observations and target variables as it provides additional knowledge when the data is not fully observed. We develop an end-to-end time series model that aims to learn the such inference relation and make a multiple-step ahead forecast. Our framework trains jointly two neural networks, one to learn the feature-wise correlations and the other for the modeling of temporal behaviors. Our model is capable of simultaneously imputing the missing entries and making a multiple-step ahead prediction. The experiments show good overall performance of our framework over existing methods in both imputation and forecasting tasks.
    
[^11]: 从扩散过程中的漂移推断和采样点过程

    Inference and Sampling of Point Processes from Diffusion Excursions. (arXiv:2306.00762v1 [stat.CO])

    [http://arxiv.org/abs/2306.00762](http://arxiv.org/abs/2306.00762)

    该论文提出了一种从潜在扩散过程中推断和采样点过程的方法，该方法通过将连续路径空间中的扩散返回时间与点过程的新到达相关联，为许多学科中的点过程提供了基础。

    

    点过程通常可以通过与连续过程的关系进行自然解释。我们提出了一种描述到达时间观测的点过程构建方法，该方法使用潜在扩散过程的状态来描述。在此框架中，我们将扩散的返回时间与点过程的新到达相关联。这导致了一个连续样本路径，用于描述生成到达分布的基础机制。这些模型在许多学科中出现，例如金融领域，在该领域中，市场上的行动由隐藏的连续价格决定，或者在神经科学中，潜在的刺激生成尖峰列。基于伊藤漂移理论的发展，我们提出了从潜在扩散过程派生的点过程的推断和采样方法。我们使用模拟和实际数据的数字示例说明了该方法的应用。所提出的方法和框架为解释许多点过程提供了基础。

    Point processes often have a natural interpretation with respect to a continuous process. We propose a point process construction that describes arrival time observations in terms of the state of a latent diffusion process. In this framework, we relate the return times of a diffusion in a continuous path space to new arrivals of the point process. This leads to a continuous sample path that is used to describe the underlying mechanism generating the arrival distribution. These models arise in many disciplines, such as financial settings where actions in a market are determined by a hidden continuous price or in neuroscience where a latent stimulus generates spike trains. Based on the developments in It\^o's excursion theory, we propose methods for inferring and sampling from the point process derived from the latent diffusion process. We illustrate the approach with numerical examples using both simulated and real data. The proposed methods and framework provide a basis for interpretin
    
[^12]: 基于谱嵌入的深度学习研究

    Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])

    [http://arxiv.org/abs/2306.00742](http://arxiv.org/abs/2306.00742)

    本文提出两种新的谱嵌入方法，一种基于函数分析原理和核方法，另一种基于深度网络优化损失，提供理论保证和实际有效的算法，并提供新的采样算法。

    

    为了有效地处理海量的数据，从而更好地对其进行表征，科学家们采用表示学习。最近，这些方法与一些底层运算的谱分解之间展现出明显的联系。在历史上，是通过在数据的顶部构建图形来建立明确的谱嵌入，而我们提出了两种新的方法：一种基于函数分析原理和核方法构建的，这将导致具有理论保证的算法，另一种基于深度网络训练以优化基本变分损失的算法，它们产生了实际有效的算法。此外，我们提供了一种新的采样算法，利用学习到的表征来在一步中生成新的样本。

    To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
    
[^13]: 深度学习中的一致置信现象及其对校准的影响

    A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])

    [http://arxiv.org/abs/2306.00740](http://arxiv.org/abs/2306.00740)

    深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。

    

    尽管深度神经网络具有惊人的泛化能力，但它们屡次表现出在预测不确定性方面估计不佳的情况——换句话说，它们在错误时经常过度自信。解决这个问题被称为模型校准，并以修改训练方案和训练后校准程序的形式受到了广泛关注。在本文中，我们提出了一个现代模型校准的重要障碍：深度神经网络在它们的训练点周围有大的几乎确定的置信邻域。我们在实验中证明了这种现象在很多模型和数据集对中都会出现（在图像分类的背景下）。此外，我们证明了当这种现象出现时，在类别之间存在重叠的大类数据分布中，即使在应用校准后也不能获得比随机更好的渐近校准模型（在渐近意义下）。

    Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
    
[^14]: $\ell_p$灵敏度采样的更严格界限

    Sharper Bounds for $\ell_p$ Sensitivity Sampling. (arXiv:2306.00732v1 [cs.DS])

    [http://arxiv.org/abs/2306.00732](http://arxiv.org/abs/2306.00732)

    该论文研究了$\ell_p$子空间嵌入的灵敏度采样界限，取得了比通用界限更好的结果，对于$1\leq p<2$的情况下，界限达到了$\mathfrak{S}^{2/p}$，对于$2<p<\infty$的情况下，界限达到了$\mathfrak{S}^{2-2/p}$。

    

    在大规模机器学习中，随机采样是一种近似数据集的流行方式，这种方式可以通过一小部分具有代表性的示例来进行。特别地，灵敏度采样是一种强烈研究的技术，它在极其普遍的情况下提供可证明的近似质量保证，同时将示例的数量减少到VC维$d$和总灵敏度$\mathfrak{S}$的乘积。然而，除了$\ell_2$子空间嵌入以外，很少有保证超过这个$\mathfrak{S}d$通用界限的知识，尽管以前的工作非常强调灵敏度采样。在这项工作中，我们首次展示了对于$ p\neq2$的$\ell_p$子空间嵌入的灵敏度采样界限，这些界限超过了一般的$\mathfrak{S}d$界限，对于$1\leq p<2$，我们取得了大约$\mathfrak{S}^{2/p}$的界限，并且对于$2<p<\infty$，取得了$\mathfrak{S}^{2-2/p}$的界限。在$1\leq p<2$的情况下，我们表明这个边界是密切相关的。

    In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the VC dimension $d$ and the total sensitivity $\mathfrak S$ in remarkably general settings. However, guarantees going beyond this general bound of $\mathfrak S d$ are known in perhaps only one setting, for $\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\ell_p$ subspace embeddings for $p\neq 2$ that improve over the general $\mathfrak S d$ bound, achieving a bound of roughly $\mathfrak S^{2/p}$ for $1\leq p<2$ and $\mathfrak S^{2-2/p}$ for $2<p<\infty$. For $1\leq p<2$, we show that this bound is tight, in the sense that there exist matrices for which
    
[^15]: 使用自适应流采样平衡训练能量基模型

    Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])

    [http://arxiv.org/abs/2306.00684](http://arxiv.org/abs/2306.00684)

    本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。

    

    能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。

    Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
    
[^16]: 属性高效的低次多项式阈值函数带噪声PAC学习

    Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise. (arXiv:2306.00673v1 [cs.DS])

    [http://arxiv.org/abs/2306.00673](http://arxiv.org/abs/2306.00673)

    本文提出一种新算法，可以属性高效的学习低次多项式阈值函数，并能够在噪声下进行PAC学习。

    

    低次多项式阈值函数（PTFs）的概念类在机器学习中起着基础作用。本文研究了$\mathbb{R}^n$上$K$稀疏度-$d$ PTFs的属性高效PAC学习，其中任何这样的概念仅依赖于输入的$K$个属性。我们的主要贡献是：提出一种新算法，在高斯边缘分布下，即使有$O(\epsilon^d)$的$\eta$被恶意噪声Bshouty et al. (2002)破坏，也可以在错误率$\epsilon$下以$O(\frac{K^{{4d}}}{\epsilon^{2d}}\cdot \log^{5d} n)$的样本PAC学习该类，算法运行时间为$({nd}/{\epsilon})^{O(d)}$。在此之前，仅为稀疏齐次超平面的特殊情况建立了属性高效的鲁棒算法。我们的关键因素是：1）将属性稀疏性转化为Hermite多项式基下chow向量的稀疏模式的结构结果；2）一种新的规范化方法，以及利用多项式近似的阈值函数的直接判别。

    The concept class of low-degree polynomial threshold functions (PTFs) plays a fundamental role in machine learning. In this paper, we study PAC learning of $K$-sparse degree-$d$ PTFs on $\mathbb{R}^n$, where any such concept depends only on $K$ out of $n$ attributes of the input. Our main contribution is a new algorithm that runs in time $({nd}/{\epsilon})^{O(d)}$ and under the Gaussian marginal distribution, PAC learns the class up to error rate $\epsilon$ with $O(\frac{K^{4d}}{\epsilon^{2d}} \cdot \log^{5d} n)$ samples even when an $\eta \leq O(\epsilon^d)$ fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces. Our key ingredients are: 1) a structural result that translates the attribute sparsity to a sparsity pattern of the Chow vector under the basis of Hermite polynomials, and 2) a novel 
    
[^17]: 学习指导型ReLU神经网络

    Learning Prescriptive ReLU Networks. (arXiv:2306.00651v1 [cs.LG])

    [http://arxiv.org/abs/2306.00651](http://arxiv.org/abs/2306.00651)

    本文提出了一种名为P-ReLU的神经网络模型，用于从观测数据中学习最佳策略。该模型可平衡指导性能和可解释性，具有灵活性和有效性，在多个基准测试中均达到最佳精度。

    

    本文研究使用观测数据从一组离散治疗选择中学习最佳策略的问题。我们提出了一种分段线性神经网络模型，可以平衡强大的指导性能和可解释性，我们称之为指导型ReLU网络或P-ReLU。我们在理论上展示了这个模型: (i) 将输入空间分成不相交的多面体，属于同一分区的所有实例接受相同的治疗方法；(ii) 可以转化为具有可解释性的具有超平面分裂的等效指导树。我们证明了P-ReLU网络的灵活性，因为可以通过对体系结构进行小的修改来轻松地合并约束条件。通过实验证明，P-ReLU相对于竞争基准模型具有更好的指导精度。最后，我们在使用真实世界数据集训练的P-ReLU中提取可解释的指导树的示例中，展示了无约束和约束情况下可解释性。

    We study the problem of learning optimal policy from a set of discrete treatment options using observational data. We propose a piecewise linear neural network model that can balance strong prescriptive performance and interpretability, which we refer to as the prescriptive ReLU network, or P-ReLU. We show analytically that this model (i) partitions the input space into disjoint polyhedra, where all instances that belong to the same partition receive the same treatment, and (ii) can be converted into an equivalent prescriptive tree with hyperplane splits for interpretability. We demonstrate the flexibility of the P-ReLU network as constraints can be easily incorporated with minor modifications to the architecture. Through experiments, we validate the superior prescriptive accuracy of P-ReLU against competing benchmarks. Lastly, we present examples of interpretable prescriptive trees extracted from trained P-ReLUs using a real-world dataset, for both the unconstrained and constrained sc
    
[^18]: 拜占庭-鲁棒性聚类联邦学习

    Byzantine-Robust Clustered Federated Learning. (arXiv:2306.00638v1 [stat.ML])

    [http://arxiv.org/abs/2306.00638](http://arxiv.org/abs/2306.00638)

    本文研究了在联邦学习环境下，非拜占庭机器可以被分成不相交的聚类，而拜占庭机器可能会对任何聚类实行攻击，破坏训练过程的问题，提出了鲁棒性聚类联邦学习算法。

    

    本文研究了在联邦学习环境下，非拜占庭机器可以被分成不相交的聚类，而拜占庭机器可能会对任何聚类实行攻击，破坏训练过程的问题。我们的目标是识别非拜占庭机器的聚类成员身份，并优化每个聚类所学的模型。我们采用 Ghosh 等人（2020）的迭代联邦聚类算法(IFCA) 框架，交替估计聚类成员身份和优化模型。为了使这个框架对来自拜占庭机器的攻击具有鲁棒性，我们使用了 Yin 等人使用的坐标轴裁剪平均值和坐标轴裁剪中位数聚合方法。

    This paper focuses on the problem of adversarial attacks from Byzantine machines in a Federated Learning setting where non-Byzantine machines can be partitioned into disjoint clusters. In this setting, non-Byzantine machines in the same cluster have the same underlying data distribution, and different clusters of non-Byzantine machines have different learning tasks. Byzantine machines can adversarially attack any cluster and disturb the training process on clusters they attack. In the presence of Byzantine machines, the goal of our work is to identify cluster membership of non-Byzantine machines and optimize the models learned by each cluster. We adopt the Iterative Federated Clustering Algorithm (IFCA) framework of Ghosh et al. (2020) to alternatively estimate cluster membership and optimize models. In order to make this framework robust against adversarial attacks from Byzantine machines, we use coordinate-wise trimmed mean and coordinate-wise median aggregation methods used by Yin e
    
[^19]: 不公平的实用程序及其改进的第一步

    Unfair Utilities and First Steps Towards Improving Them. (arXiv:2306.00636v1 [stat.ML])

    [http://arxiv.org/abs/2306.00636](http://arxiv.org/abs/2306.00636)

    该论文提出了一个新的公平框架——考虑政策优化哪个效用，定义了信息价值公平，提出不应使用不满足这一标准的实用程序，并探讨了修改实用程序以满足此公平标准可能对最优政策产生的影响。

    

    许多公平标准对政策或预测器的选择进行限制。在这项工作中，我们提出了一个不同的思考公平的框架：我们考虑政策正在优化哪个效用，而不是限制政策或预测器的选择。我们定义了信息价值公平，并建议不使用不满足此标准的实用程序。我们描述了如何修改实用程序以满足这种公平标准，并讨论了这可能对相应的最优政策产生的影响。

    Many fairness criteria constrain the policy or choice of predictors. In this work, we propose a different framework for thinking about fairness: Instead of constraining the policy or choice of predictors, we consider which utility a policy is optimizing for. We define value of information fairness and propose to not use utilities that do not satisfy this criterion. We describe how to modify a utility to satisfy this fairness criterion and discuss the consequences this might have on the corresponding optimal policies.
    
[^20]: 在潜在混淆因素存在的情况下，从时态到同时性的迭代因果发现

    From Temporal to Contemporaneous Iterative Causal Discovery in the Presence of Latent Confounders. (arXiv:2306.00624v1 [cs.AI])

    [http://arxiv.org/abs/2306.00624](http://arxiv.org/abs/2306.00624)

    在潜在混淆因素存在的情况下，我们提出了一种逐步完善因果图的算法，先学习长期的时态关系而不是短期的关系，最后才学习同时性关系。这种方法减少了统计检验次数，提高了因果图的准确性。

    

    我们提出了一种基于约束的算法，用于从观测时间序列数据中学习因果结构，在存在潜在混淆因素的情况下。我们假设存在离散时间、稳态结构向量自回归过程，具有时态和同时性因果关系。我们提出的算法会逐步通过先学习长期的时态关系而不是短期的关系（同时性关系最后学习），逐步完善因果图。这种关系排序的学习方法导致所需的统计检验次数减少。我们在实验中通过合成数据验证了这种减少，同时证明与最先进的算法相比，这种减少会导致真实世界数据的更高准确性和更合理的因果图。

    We present a constraint-based algorithm for learning causal structures from observational time-series data, in the presence of latent confounders. We assume a discrete-time, stationary structural vector autoregressive process, with both temporal and contemporaneous causal relations. One may ask if temporal and contemporaneous relations should be treated differently. The presented algorithm gradually refines a causal graph by learning long-term temporal relations before short-term ones, where contemporaneous relations are learned last. This ordering of causal relations to be learnt leads to a reduction in the required number of statistical tests. We validate this reduction empirically and demonstrate that it leads to higher accuracy for synthetic data and more plausible causal graphs for real-world data compared to state-of-the-art algorithms.
    
[^21]: 关于混合互信息估计的有效性研究

    On the Effectiveness of Hybrid Mutual Information Estimation. (arXiv:2306.00608v1 [stat.ML])

    [http://arxiv.org/abs/2306.00608](http://arxiv.org/abs/2306.00608)

    本文研究了混合互信息估计的有效性，提出了一种混合方法以应对判别式和生成式方法各自缺点，同时提出了一种名为预测量化的生成方法，与判别式估计器结合可获得更精确的互信息估计结果。

    

    从联合分布的样本中估计互信息是科学和工程中的一个难题。本文研究了一个概括了判别式和生成式方法的变分界约束，并提出了一种混合方法来减少它们各自的缺点。此外，我们提出了一种称为预测量化 (PQ) 的简单生成方法，它可以与判别式估计器轻松结合以实现最小的计算开销。我们的提议通过降低估计器的方差而产生更紧的信息界约束。我们将这些方法应用于相关的高维高斯分布和涉及受固定能量景观约束的自由粒子系统的随机过程的挑战性任务上。实证结果表明，与相应的判别式估计方法相比，混合方法可以持续提高互信息估计精度。

    Estimating the mutual information from samples from a joint distribution is a challenging problem in both science and engineering. In this work, we realize a variational bound that generalizes both discriminative and generative approaches. Using this bound, we propose a hybrid method to mitigate their respective shortcomings. Further, we propose Predictive Quantization (PQ): a simple generative method that can be easily combined with discriminative estimators for minimal computational overhead. Our propositions yield a tighter bound on the information thanks to the reduced variance of the estimator. We test our methods on a challenging task of correlated high-dimensional Gaussian distributions and a stochastic process involving a system of free particles subjected to a fixed energy landscape. Empirical results show that hybrid methods consistently improved mutual information estimates when compared to the corresponding discriminative counterpart.
    
[^22]: Hinge-Wasserstein: 通过分类避免回归中的过度自信

    Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification. (arXiv:2306.00560v1 [cs.LG])

    [http://arxiv.org/abs/2306.00560](http://arxiv.org/abs/2306.00560)

    该论文提出了一种基于Wasserstein距离的损失函数hinge-Wasserstein，用于缓解回归任务中由于过度自信导致的不确定性问题。这种损失函数有效提高了aleatoric和epistemic不确定性的质量。

    

    现代深度神经网络在性能方面得到了巨大的提高，但它们容易产生过度自信。在模糊甚至不可预测的现实世界场景中，这种过度自信可能对应用程序的安全性构成重大风险。针对回归任务，采用回归-分类方法有潜力缓解这些歧义，因为它可以预测所需输出的离散概率密度。然而，密度估计仍然倾向于过度自信，尤其是在使用常见的NLL损失函数训练时。为了缓解这种过度自信的问题，我们提出了一种基于Wasserstein距离的损失函数，即hinge-Wasserstein。与以前的工作相比，此损失显着提高了两种不确定性的质量： aleatoric不确定性和epistemic不确定性。我们在合成数据集上展示了新损失的能力，其中两种类型的不确定性可以分别控制。此外，作为现实世界场景的演示，我们在基准数据集上评估了我们的方法。

    Modern deep neural networks are prone to being overconfident despite their drastically improved performance. In ambiguous or even unpredictable real-world scenarios, this overconfidence can pose a major risk to the safety of applications. For regression tasks, the regression-by-classification approach has the potential to alleviate these ambiguities by instead predicting a discrete probability density over the desired output. However, a density estimator still tends to be overconfident when trained with the common NLL loss. To mitigate the overconfidence problem, we propose a loss function, hinge-Wasserstein, based on the Wasserstein Distance. This loss significantly improves the quality of both aleatoric and epistemic uncertainty, compared to previous work. We demonstrate the capabilities of the new loss on a synthetic dataset, where both types of uncertainty are controlled separately. Moreover, as a demonstration for real-world scenarios, we evaluate our approach on the benchmark dat
    
[^23]: 基于特征交互作用进行全局特征效应分解

    Decomposing Global Feature Effects Based on Feature Interactions. (arXiv:2306.00541v1 [stat.ML])

    [http://arxiv.org/abs/2306.00541](http://arxiv.org/abs/2306.00541)

    提出了全局效应广义可加分解（GADGET）框架，能够最小化特征交互作用的本地特征效应的交互异质性。同时适用于偏依赖、积累局部效应和Shapley可加解释（SHAP）依赖的边际特征效应可视化方法，并提出了一种新的基于置换的交互测试来检测显着的特征交互作用。

    

    全局特征效应方法，如偏依赖图，提供了预期边际特征效应的可理解的可视化。但是，当存在特征交互作用时，这种全局特征效应方法可能会误导，因为它们不能很好地表示单个观测的局部特征效应。我们正式介绍了基于递归分区的全局效应广义可加分解（GADGET）框架，以找到解释性特征空间中的可解释区域，从而最小化本地特征效应的交互异质性。我们为该框架提供了数学基础，并展示它适用于最流行的方法来可视化边际特征效应，即偏依赖，积累局部效应和Shapley可加解释（SHAP）依赖。此外，我们引入了一种新的基于置换的交互测试来检测显着的特征交互作用，该方法适用于任何特征。

    Global feature effect methods, such as partial dependence plots, provide an intelligible visualization of the expected marginal feature effect. However, such global feature effect methods can be misleading, as they do not represent local feature effects of single observations well when feature interactions are present. We formally introduce generalized additive decomposition of global effects (GADGET), which is a new framework based on recursive partitioning to find interpretable regions in the feature space such that the interaction-related heterogeneity of local feature effects is minimized. We provide a mathematical foundation of the framework and show that it is applicable to the most popular methods to visualize marginal feature effects, namely partial dependence, accumulated local effects, and Shapley additive explanations (SHAP) dependence. Furthermore, we introduce a new permutation-based interaction test to detect significant feature interactions that is applicable to any feat
    
[^24]: 半结构化网络性能的新PHO公式改进。

    A New PHO-rmula for Improved Performance of Semi-Structured Networks. (arXiv:2306.00522v1 [cs.LG])

    [http://arxiv.org/abs/2306.00522](http://arxiv.org/abs/2306.00522)

    我们提出了一种非侵入式的后处理正交化（PHO）方法，旨在保证模型组成部分的识别性，提供更好的估计和预测质量。

    

    最近，将结构回归模型与深度神经网络相结合，以获得更好的可解释性、更强的表达力和统计上有效的不确定性量化，展示了半结构化神经网络的多功能性。然而，我们发现在半结构化神经网络中，正确识别不同模型组成部分的技术会导致次优的网络估计，收敛速度变慢，甚至会出现预测失误。为了解决这些问题，同时保持有利的模型特性，我们提出了一种非侵入式的后处理正交化（PHO）方法，旨在保证模型组成部分的识别性，提供更好的估计和预测质量。我们的理论发现得到了数值实验、基准比较以及COVID-19感染的实际应用的支持。

    Recent advances to combine structured regression models and deep neural networks for better interpretability, more expressiveness, and statistically valid uncertainty quantification demonstrate the versatility of semi-structured neural networks (SSNs). We show that techniques to properly identify the contributions of the different model components in SSNs, however, lead to suboptimal network estimation, slower convergence, and degenerated or erroneous predictions. In order to solve these problems while preserving favorable model properties, we propose a non-invasive post-hoc orthogonalization (PHO) that guarantees identifiability of model components and provides better estimation and prediction quality. Our theoretical findings are supported by numerical experiments, a benchmark comparison as well as a real-world application to COVID-19 infections.
    
[^25]: 关于遮蔽预训练和边缘似然的研究

    On Masked Pre-training and the Marginal Likelihood. (arXiv:2306.00520v1 [stat.ML])

    [http://arxiv.org/abs/2306.00520](http://arxiv.org/abs/2306.00520)

    遮蔽预训练利用累积打分函数实现了对模型的边缘似然极大化，提出了设计适当的自监督方法来训练贝叶斯模型的理论。实验证实了这一理论，并探索了遮蔽预训练的主要学习原理。

    

    遮蔽预训练是一种移除随机输入维度并学习可以预测缺失值的模型的自我监督学习方法。实证结果表明，这种直观的自我监督学习方法可以生成具有很好泛化性能的模型。然而，还缺乏对其理论的深入理解。本文证明了通过适当的积分打分函数进行的遮蔽预训练可以对模型的边缘似然极大化。边缘似然实际上是广义化模型选择的贝叶斯度量。除了揭示遮蔽预训练成功的原因，本文的理论还建议可以设计适当的自监督方法来训练贝叶斯模型。我们在大型语言模型中实验验证了所开发的理论并探索了遮蔽预训练的主要学习原理。

    Masked pre-training removes random input dimensions and learns a model that can predict the missing values. Empirical results indicate that this intuitive form of self-supervised learning yields models that generalize very well to new domains. A theoretical understanding is, however, lacking. This paper shows that masked pre-training with a suitable cumulative scoring function corresponds to maximizing the model's marginal likelihood, which is de facto the Bayesian model selection measure of generalization. Beyond shedding light on the success of masked pre-training, this insight also suggests that Bayesian models can be trained with appropriately designed self-supervision. Empirically, we confirm the developed theory and explore the main learning principles of masked pre-training in large language models.
    
[^26]: 二分分类中追索权的风险

    The Risks of Recourse in Binary Classification. (arXiv:2306.00497v1 [cs.LG])

    [http://arxiv.org/abs/2306.00497](http://arxiv.org/abs/2306.00497)

    研究发现，在二分分类中提供追索权会增加错误率，导致更多错误的发生。提供算法追索权可能也会在系统级别上给予不利。

    

    算法追索权提供解释，以帮助用户推翻机器学习系统的不利决策。但到目前为止，很少有人关注提供追索权是否有益。我们引入了一个抽象的学习理论框架，比较了具有和没有算法追索权的分类的风险（即期望损失）。这使我们能够回答在整个人群水平上提供追索权何时有益或有害的问题。令人惊讶的是，我们发现在许多可信的情况下，提供追索权反而会有害，因为它将用户推向更高类别不确定性的区域，因此会导致更多的错误。我们进一步研究了部署分类器的一方是否有动机针对提供追索权的情况进行策略规划，我们发现有时候确实存在这种现象，这对他们的用户不利。因此，提供算法追索权在系统级别上可能也是有害的。

    Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e. expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level
    
[^27]: 基于分片的贝叶斯加性回归树

    Sharded Bayesian Additive Regression Trees. (arXiv:2306.00361v1 [stat.ML])

    [http://arxiv.org/abs/2306.00361](http://arxiv.org/abs/2306.00361)

    本文提出了一种基于分片的贝叶斯加性回归树模型，通过引入随机化辅助变量和分片树结构，采用贝叶斯加性回归树适配每个分区组件到一个子模型中进行数据分区，引入交集树结构来完全使用树结构指定分片和建模。研究中还推导了理论最优权重和证明了模型复杂度。

    

    本文提出了一种随机分片的贝叶斯加性回归树（SBT）模型。我们引入了一个随机化辅助变量和一个分片树来决定数据的分区，并使用贝叶斯加性回归树（BART）适配每个分区组件到一个子模型中。我们观察到，分片树的最优设计可以确定产品空间上子模型的最优分片，我们引入了一个交集树结构来完全使用树结构指定分片和建模。除了实验外，我们还推导了最小化后验收缩的理论最优权重，并证明了SBT的最坏情况复杂度。

    In this paper we develop the randomized Sharded Bayesian Additive Regression Trees (SBT) model. We introduce a randomization auxiliary variable and a sharding tree to decide partitioning of data, and fit each partition component to a sub-model using Bayesian Additive Regression Tree (BART). By observing that the optimal design of a sharding tree can determine optimal sharding for sub-models on a product space, we introduce an intersection tree structure to completely specify both the sharding and modeling using only tree structures. In addition to experiments, we also derive the theoretical optimal weights for minimizing posterior contractions and prove the worst-case complexity of SBT.
    
[^28]: 高效稳健的贝叶斯维数约减参数选择方法

    Efficient and Robust Bayesian Selection of Hyperparameters in Dimension Reduction for Visualization. (arXiv:2306.00357v1 [stat.ML])

    [http://arxiv.org/abs/2306.00357](http://arxiv.org/abs/2306.00357)

    本文提出了一种利用有代理模型的贝叶斯优化进行数据驱动敏感性分析和权衡多目标的维数约减参数选择方法，并在多个合成和现实数据集上进行评估，提供了一种稳健且高效的解决方案。

    

    我们提出了一种高效稳健的自动调参框架，在维数约减算法中选择超参数，注重大规模数据集和任意性能指标。通过利用有代理模型的贝叶斯优化，我们的方法使得超参数选择具有多目标权衡，并且允许我们进行数据驱动的敏感性分析。通过结合归一化和子采样，所提出的框架在可视化技术如t-SNE和UMAP中展现出了通用性和高效性。我们使用多个质量度量在各种合成和现实数据集上评估了我们的结果，为维数约减算法的超参数选择提供了一种稳健且高效的解决方案。

    We introduce an efficient and robust auto-tuning framework for hyperparameter selection in dimension reduction (DR) algorithms, focusing on large-scale datasets and arbitrary performance metrics. By leveraging Bayesian optimization (BO) with a surrogate model, our approach enables efficient hyperparameter selection with multi-objective trade-offs and allows us to perform data-driven sensitivity analysis. By incorporating normalization and subsampling, the proposed framework demonstrates versatility and efficiency, as shown in applications to visualization techniques such as t-SNE and UMAP. We evaluate our results on various synthetic and real-world datasets using multiple quality metrics, providing a robust and efficient solution for hyperparameter selection in DR algorithms.
    
[^29]: 从概率角度构建语义感知的对抗样本

    Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (arXiv:2306.00353v1 [stat.ML])

    [http://arxiv.org/abs/2306.00353](http://arxiv.org/abs/2306.00353)

    本研究提出了一个基于概率视角的对抗样本构建方法，可以生成语义感知的对抗性样本，并可以有效规避传统对抗性攻击的强化对抗训练方法。

    

    本研究提出了一种新颖的概率视角对抗样本构建方法——箱约束 Langevin Monte Carlo（LMC）。从这个角度出发，我们开发了一种创新性的方法，以原则性的方式生成语义感知的对抗性样本。这种方法超越了几何距离所施加的限制，选择了语义约束。我们的方法赋予了个体将其对语义的理解融入到模型中的能力。通过人类评估，我们验证了我们的语义感知的对抗样本保持其固有的含义。在 MNIST 和 SVHN 数据集上的实验结果表明，我们的语义感知的对抗样本可以有效地规避针对传统对抗性攻击的强健性对抗训练方法。

    In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.
    
[^30]: 改进机器学习的能量守恒下降法：理论与实践

    Improving Energy Conserving Descent for Machine Learning: Theory and Practice. (arXiv:2306.00352v1 [cs.LG])

    [http://arxiv.org/abs/2306.00352](http://arxiv.org/abs/2306.00352)

    这篇论文介绍了能量守恒下降（ECD）的理论，提出了基于梯度的优化算法ECDSep，能够处理凸优化问题和非凸优化问题，通过改进动态和混沌诱导元素，提高性能；在各种机器学习问题上与流行的优化方法进行了实证比较，发现在每个任务中具有竞争力或改进的性能。

    

    我们发展了能量守恒下降（ECD）的理论，并介绍了ECDSep，这是一种基于梯度的优化算法，能够处理凸优化问题和非凸优化问题。该方法基于新颖的ECD优化框架，通过适当混沌的能量守恒动力系统的物理演化来实现，使得即使对于无对称性的通用高维问题的结果分布也能进行分析控制，从而主导低损失。与以往的实现相比，我们利用理论控制来改进动态和诱导混沌的元素，提高性能，同时简化面向不同类别问题的优化算法的超参数调整。我们在各种机器学习问题上与流行的优化方法（如SGD，Adam和AdamW等）进行了实证比较，发现在每个任务中与它们中的最佳方法相比具有竞争力或改进的性能。我们确定了我们方法的局限性，并提出了未来研究的方向。

    We develop the theory of Energy Conserving Descent (ECD) and introduce ECDSep, a gradient-based optimization algorithm able to tackle convex and non-convex optimization problems. The method is based on the novel ECD framework of optimization as physical evolution of a suitable chaotic energy-conserving dynamical system, enabling analytic control of the distribution of results - dominated at low loss - even for generic high-dimensional problems with no symmetries. Compared to previous realizations of this idea, we exploit the theoretical control to improve both the dynamics and chaos-inducing elements, enhancing performance while simplifying the hyper-parameter tuning of the optimization algorithm targeted to different classes of problems. We empirically compare with popular optimization methods such as SGD, Adam and AdamW on a wide range of machine learning problems, finding competitive or improved performance compared to the best among them on each task. We identify limitations in our
    
[^31]: BOtied: 带有相关多元等级的多目标贝叶斯优化

    BOtied: Multi-objective Bayesian optimization with tied multivariate ranks. (arXiv:2306.00344v1 [cs.LG])

    [http://arxiv.org/abs/2306.00344](http://arxiv.org/abs/2306.00344)

    BOtied 是一种带有相关多元等级的多目标贝叶斯优化算法，相较于现有的方法具有更高的样本效率和较好的近似质量。

    

    许多科学和工业应用需要同时优化多个潜在的相互冲突的目标。多目标贝叶斯优化 (MOBO) 是一种高效地识别 Pareto 最优解的框架。我们展示了无支配解和最高多元等级之间的自然联系，它与联合累积分布函数的最外层等高线重合。我们提出了 CDF indicator，这是一种 Pareto 合规的度量，用于评估近似 Pareto 集合的质量，它补充了流行的 hypervolume indicator。MOBO 的核心是采集函数，它通过导航目标之间的最佳折中来确定下一个要评估的候选项。 基于盒子分解目标空间的多目标采集函数（例如期望的 hypervolume 改进（EHVI）和熵搜索）在存在大量目标时的性能缩放很差。我们提出一种采集函数，称为 BOtied，它利用相关多元等级来高效搜索 Pareto frontier。我们的实验表明，BOtied 在样本效率和近似质量方面优于现有方法。

    Many scientific and industrial applications require joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. We show a natural connection between non-dominated solutions and the highest multivariate rank, which coincides with the outermost level line of the joint cumulative distribution function (CDF). We propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets that complements the popular hypervolume indicator. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Multi-objective acquisition functions that rely on box decomposition of the objective space, such as the expected hypervolume improvement (EHVI) and entropy search, scale poorly to a large number of objectives. We propose an acquisition function, call
    
[^32]: 结合显式和隐式正则化的深度网络高效学习

    Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks. (arXiv:2306.00342v1 [cs.LG])

    [http://arxiv.org/abs/2306.00342](http://arxiv.org/abs/2306.00342)

    本文提出了一种显式正则化方法，与隐式正则化结合，可以使单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。

    

    隐式正则化研究优化过程中的梯度轨迹，以解释为什么深度网络更倾向于某些解决方案。在深度线性网络中，已经证明梯度下降隐式地朝向矩阵补全/因式分解任务上的低秩解决方案进行正则化。添加层数不仅可以提高这些任务的性能，而且作为一种加速的预处理方法进一步增强了这种低秩偏向。受此启发，我们提出一种显式惩罚来反映这种隐式偏差，只在某些自适应梯度优化器（例如Adam）起作用。这种组合可以使退化的单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。单层网络还在一系列参数和数据集上能够表现优异，甚至超过了各种矩阵补全方法的表现。

    Works on implicit regularization have studied gradient trajectories during the optimization process to explain why deep networks favor certain kinds of solutions over others. In deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions on matrix completion/factorization tasks. Adding depth not only improves performance on these tasks but also acts as an accelerative pre-conditioning that further enhances this bias towards low-rankedness. Inspired by this, we propose an explicit penalty to mirror this implicit bias which only takes effect with certain adaptive gradient optimizers (e.g. Adam). This combination can enable a degenerate single-layer network to achieve low-rank approximations with generalization error comparable to deep linear networks, making depth no longer necessary for learning. The single-layer network also performs competitively or out-performs various approaches for matrix completion over a range of parameter and da
    
[^33]: 通过不一致性偏差推导出分布转移下的(几乎)可证明误差界限

    (Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy. (arXiv:2306.00312v1 [stat.ML])

    [http://arxiv.org/abs/2306.00312](http://arxiv.org/abs/2306.00312)

    本论文提出了一种基于不一致性偏差的方法，通过一个简单、直观的条件推导出深度神经网络在分布转移下的(几乎)可证明误差界限，相比于$\mathcal{H}\Delta\mathcal{H}$-divergence更紧密、更易评价。

    

    本文使用未标记的测试数据，推导出深度神经网络在分布转换下误差的(几乎)保证上限。我们的方法需要一个简单、直观的条件，由先前的经验研究很好地证明且在实际操作中有效率地满足，而且相比于$\mathcal{H}\Delta\mathcal{H}$-divergence更容易评价且保证更紧密、更不虚伪。

    We derive an (almost) guaranteed upper bound on the error of deep neural networks under distribution shift using unlabeled test data. Prior methods either give bounds that are vacuous in practice or give estimates that are accurate on average but heavily underestimate error for a sizeable fraction of shifts. In particular, the latter only give guarantees based on complex continuous measures such as test calibration -- which cannot be identified without labels -- and are therefore unreliable. Instead, our bound requires a simple, intuitive condition which is well justified by prior empirical works and holds in practice effectively 100% of the time. The bound is inspired by $\mathcal{H}\Delta\mathcal{H}$-divergence but is easier to evaluate and substantially tighter, consistently providing non-vacuous guarantees. Estimating the bound requires optimizing one multiclass classifier to disagree with another, for which some prior works have used sub-optimal proxy losses; we devise a "disagree
    
[^34]: 面向非均匀和时变通信的FedAvg偏差修正研究

    Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications. (arXiv:2306.00280v1 [cs.LG])

    [http://arxiv.org/abs/2306.00280](http://arxiv.org/abs/2306.00280)

    本文面向非均匀和时变通信故障问题，提出了一种名为FedPBC的联邦学习算法来修正FedAvg的偏差，该算法构建二叉树来减轻非均匀通信故障的影响，并在非凸任务上实验结果表现优于FedAvg。

    

    联邦学习是一种分散的学习框架，在这个框架下，参数服务器（PS）和多个客户端合作通过最小化全局目标来训练模型。通信带宽是一种稀缺资源；在每轮迭代中，PS仅从客户端的子集聚合更新。本文关注的是非凸优化问题，这种问题容易受到PS和客户端之间不均匀、时变的通信故障的影响。具体而言，在每轮$t$中，PS和客户端$i$之间的连接只有概率$p_i^t$是活跃的，而这个概率是不知道的。当信道条件在客户端之间异构并且随时间变化时，这种情况会发生。我们发现当$p_i^t$非均匀时，被广泛采用的FL算法FedAvg无法最小化全局目标。鉴于此，我们提出了一种名为FedPBC的简单有效的方法来减轻非均匀通信故障的影响。FedPBC在客户端之间构建二叉树，在建立拓扑结构之前推迟更新的传输，并捕捉本地更新的重要性。我们在温和假设下理论证明了FedPBC收敛于非凸问题的全局最优解。在非凸任务（如图像分类和语言建模）上的实验结果表明，FedPBC在非均匀和时变的通信设置下优于FedAvg。

    Federated learning (FL) is a decentralized learning framework wherein a parameter server (PS) and a collection of clients collaboratively train a model via minimizing a global objective. Communication bandwidth is a scarce resource; in each round, the PS aggregates the updates from a subset of clients only. In this paper, we focus on non-convex minimization that is vulnerable to non-uniform and time-varying communication failures between the PS and the clients. Specifically, in each round $t$, the link between the PS and client $i$ is active with probability $p_i^t$, which is $\textit{unknown}$ to both the PS and the clients. This arises when the channel conditions are heterogeneous across clients and are changing over time.  We show that when the $p_i^t$'s are not uniform, $\textit{Federated Average}$ (FedAvg) -- the most widely adopted FL algorithm -- fails to minimize the global objective. Observing this, we propose $\textit{Federated Postponed Broadcast}$ (FedPBC) which is a simple
    
[^35]: Mixup在寻找最佳决策边界中的可证实益处

    Provable Benefit of Mixup for Finding Optimal Decision Boundaries. (arXiv:2306.00267v1 [cs.LG])

    [http://arxiv.org/abs/2306.00267](http://arxiv.org/abs/2306.00267)

    本研究证明了使用Mixup训练具有可证实的益处，可以显著降低在更可分离数据分布中寻找最佳决策边界的样本复杂度。

    

    本文研究了像Mixup这样的成对数据增强技术如何影响在二元线性分类问题中寻找最佳决策边界的样本复杂度。针对一类具有可分离常数$\kappa$的数据分布，我们分析了训练损失最优分类器与测试准确率最优分类器（即贝叶斯最优分类器）之间的对齐程度。对于没有增强的普通训练，我们发现了一种有趣的现象，称为可分离性的诅咒。随着我们增加$\kappa$使数据分布更加可分离，普通训练的样本复杂度会在$\kappa$中呈指数增长。也许更令人惊讶的是，对于更可分离的数据分布而言，寻找最佳决策边界的任务变得更加困难。针对Mixup训练，我们展示了Mixup减轻了这个问题，通过显著降低样本复杂度。为此，我们开发了适用于Mixup考虑的$n^2$成对增强数据点的新的集中结果。我们的结果提供了关于Mixup的泛化益处的可证保证，并提供了理解Mixup为什么在实践中表现良好的见解。

    We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points cons
    
[^36]: 一种具有非零相关性的随机图匹配的多项式迭代算法

    A polynomial-time iterative algorithm for random graph matching with non-vanishing correlation. (arXiv:2306.00266v1 [cs.DS])

    [http://arxiv.org/abs/2306.00266](http://arxiv.org/abs/2306.00266)

    本论文提出了一种具有非零相关性的随机图匹配的多项式迭代算法，并且在边缘相关性非零时成功恢复潜在匹配。

    

    我们提出了一种用于匹配两个相关的Erdős-Rényi图表的有效算法，它们具有 $n$ 个顶点，其边缘通过潜在的顶点对应关系相互关联。当边缘密度 $q=n^{-\alpha+o(1)}$，对于一个常数 $\alpha \in [0,1)$ 时，我们展示了我们的算法具有多项式运行时间，并且只要边缘相关性非零，就能成功恢复潜在匹配。这与我们先前关于匹配两个具有非零相关性的高斯Wigner矩阵的多项式时间算法的工作密切相关，并且在边缘相关性低于Otter常数的平方根（约为0.338）时，提供了第一个多项式时间随机图匹配算法（无论 $q$ 的范围如何）。

    We propose an efficient algorithm for matching two correlated Erd\H{o}s--R\'enyi graphs with $n$ vertices whose edges are correlated through a latent vertex correspondence. When the edge density $q= n^{- \alpha+o(1)}$ for a constant $\alpha \in [0,1)$, we show that our algorithm has polynomial running time and succeeds to recover the latent matching as long as the edge correlation is non-vanishing. This is closely related to our previous work on a polynomial-time algorithm that matches two Gaussian Wigner matrices with non-vanishing correlation, and provides the first polynomial-time random graph matching algorithm (regardless of the regime of $q$) when the edge correlation is below the square root of the Otter's constant (which is $\approx 0.338$).
    
[^37]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^38]: 组合神经臂的研究

    Combinatorial Neural Bandits. (arXiv:2306.00242v1 [stat.ML])

    [http://arxiv.org/abs/2306.00242](http://arxiv.org/abs/2306.00242)

    该论文提出了两个组合神经臂算法，通过使用深度神经网络来近似未知得分函数，这是处理类似问题的第一个算法框架。

    

    我们研究了一种上下文组合臂问题，其中学习代理在每一轮选择一组臂并根据其分数接收反馈。臂的得分是臂特征的未知函数。通过使用深度神经网络来近似这个未知的得分函数，我们提出了算法：组合神经UCB（CN-UCB）和组合神经汤普森抽样（CN-TS）。我们证明，CN-UCB可以达到$\tilde{\mathcal{O}}(\tilde{d} \sqrt{T})$或 $\tilde{\mathcal{O}}(\sqrt{\tilde{d} T K})$遗憾值，其中$\tilde{d}$是神经切向核矩阵的有效维度，$K$是一组臂的大小，$T$是时间跨度。对于CN-TS，我们采用一种乐观抽样技术来确保组合动作的乐观性，达到最差情况（频率学派）遗憾为$\tilde{\mathcal{O}}(\tilde{d} \sqrt{TK})$。据我们所知，这是第一个解决此类组合臂问题的算法框架。

    We consider a contextual combinatorial bandit problem where in each round a learning agent selects a subset of arms and receives feedback on the selected arms according to their scores. The score of an arm is an unknown function of the arm's feature. Approximating this unknown score function with deep neural networks, we propose algorithms: Combinatorial Neural UCB ($\texttt{CN-UCB}$) and Combinatorial Neural Thompson Sampling ($\texttt{CN-TS}$). We prove that $\texttt{CN-UCB}$ achieves $\tilde{\mathcal{O}}(\tilde{d} \sqrt{T})$ or $\tilde{\mathcal{O}}(\sqrt{\tilde{d} T K})$ regret, where $\tilde{d}$ is the effective dimension of a neural tangent kernel matrix, $K$ is the size of a subset of arms, and $T$ is the time horizon. For $\texttt{CN-TS}$, we adapt an optimistic sampling technique to ensure the optimism of the sampled combinatorial action, achieving a worst-case (frequentist) regret of $\tilde{\mathcal{O}}(\tilde{d} \sqrt{TK})$. To the best of our knowledge, these are the first 
    
[^39]: 广义隐式Follow-The-Regularized-Leader算法

    Generalized Implicit Follow-The-Regularized-Leader. (arXiv:2306.00201v1 [cs.LG])

    [http://arxiv.org/abs/2306.00201](http://arxiv.org/abs/2306.00201)

    提出了一种新的在线学习算法——广义隐式Follow-The-Regularized-Leader (FTRL)，它可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则。该算法的关键思想是用Fenchel-Young不等式代替损失的线性化，可以直接改进最坏情况下的后悔上限。

    

    我们提出了一类新的在线学习算法，称为广义隐式Follow-The-Regularized-Leader (FTRL)，它扩展了FTRL框架的范围。广义隐式FTRL可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则，例如aProx和Mirror-Prox的扩展到FTRL。我们的理论是建设性的，因为它提供了一个简单的统一框架，以设计直接改进最坏情况后悔的上限的更新。关键思想是用Fenchel-Young不等式代替损失的线性化。我们通过证明一些已知的算法，如Mirror-Prox更新，是广义隐式FTRL的实例，展示了框架的灵活性。最后，新框架使我们能够恢复隐式OMD的时间变化界，同时具有相同的计算复杂度。

    We propose a new class of online learning algorithms, generalized implicit Follow-The-Regularized-Leader (FTRL), that expands the scope of FTRL framework. Generalized implicit FTRL can recover known algorithms, as FTRL with linearized losses and implicit FTRL, and it allows the design of new update rules, as extensions of aProx and Mirror-Prox to FTRL. Our theory is constructive in the sense that it provides a simple unifying framework to design updates that directly improve the worst-case upper bound on the regret. The key idea is substituting the linearization of the losses with a Fenchel-Young inequality. We show the flexibility of the framework by proving that some known algorithms, like the Mirror-Prox updates, are instantiations of the generalized implicit FTRL. Finally, the new framework allows us to recover the temporal variation bound of implicit OMD, with the same computational complexity.
    
[^40]: 具有平均奖励的不安定赌徒问题：打破统一全局引子假设

    Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])

    [http://arxiv.org/abs/2306.00196](http://arxiv.org/abs/2306.00196)

    本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。

    

    我们研究了具有平均奖励标准下的无限时不安定赌徒问题，包括离散时间和连续时间设置。一个基本问题是如何设计计算有效的策略，使得优化差距随着臂的数量$N$的增加而减小。现有的渐近最优性结果都依赖于统一全局引子性质(UGAP)，这是一个复杂且难以验证的假设。在本文中，我们提出了一个通用的、基于模拟的框架，将任何单臂策略转化为原始的$N$臂问题的策略。这是通过在每个臂上模拟单臂策略，并仔细地将真实状态引导向模拟状态来实现的。我们的框架可以实例化，产生一个具有$O(1/\sqrt{N})$的最优解差距的策略。在离散时间设置中，我们的结果在更简单的同步假设下成立，涵盖了一些不满足UGAP的问题实例。更值得注意的是，我们的框架可以处理比现有方法更大的问题类，而不需对问题实例做任何特定的结构假设。

    We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
    
[^41]: 条件强对数凹的生成模型

    Conditionally Strongly Log-Concave Generative Models. (arXiv:2306.00181v1 [stat.ML])

    [http://arxiv.org/abs/2306.00181](http://arxiv.org/abs/2306.00181)

    本论文介绍了条件强对数凹模型(CSLC)。我们将数据分布分解成一组满足强对数凹条件的条件概率分布的乘积形式。通过适应于数据分布的正交投影器得到分解，提供了有效的参数估计和采样算法，同时提供了理论保证。

    

    深度生成模型在生成图像方面已经具有很好的效果，但由于存在模式崩溃和记忆问题，其应用受到限制。经典算法在理论上提供严格的保证，但对于高纬度数据需要满足诸如对数凹性等限制条件来避免维度灾难。本文通过介绍条件强对数凹模型(CSLC)，将数据分布分解成一组满足强对数凹条件的条件概率分布的乘积形式，该分解通过适应于数据分布的正交投影器得到。与数据分布不全局对数凹的情况不同，该分解提供了有效的参数估计和采样算法，同时提供了理论保证。我们使用小波包正交投影器证明了一些具有挑战性的多尺度过程满足条件对数凹性。 最后，我们在物理场领域展示了数值结果。

    There is a growing gap between the impressive results of deep image generative models and classical algorithms that offer theoretical guarantees. The former suffer from mode collapse or memorization issues, limiting their application to scientific data. The latter require restrictive assumptions such as log-concavity to escape the curse of dimensionality. We partially bridge this gap by introducing conditionally strongly log-concave (CSLC) models, which factorize the data distribution into a product of conditional probability distributions that are strongly log-concave. This factorization is obtained with orthogonal projectors adapted to the data distribution. It leads to efficient parameter estimation and sampling algorithms, with theoretical guarantees, although the data distribution is not globally log-concave. We show that several challenging multiscale processes are conditionally log-concave using wavelet packet orthogonal projectors. Numerical results are shown for physical field
    
[^42]: 关于神经网络表达能力的研究

    On the Expressive Power of Neural Networks. (arXiv:2306.00145v1 [math.CA])

    [http://arxiv.org/abs/2306.00145](http://arxiv.org/abs/2306.00145)

    本文研究了神经网络的表达能力，证明了某些问题在之前的研究中未得到解决，并提出了新问题的回答，包括广而浅的ReLU网络不能被深而窄的ReLU网络很好地逼近等。

    

    1989年，George Cybenko在一篇里程碑式的论文中证明了宽而浅的神经网络可以在紧致集上逼近任意连续函数，这个通用逼近定理引发了很多后续研究。本文将通过一个框架回答“有没有一些广而浅的ReLU网络无法被深而窄的ReLU网络很好地逼近？”“普遍逼近定理是否仍适用于Sobolev空间范数W 1,1？”“这些结果是否适用于除ReLU之外的激活函数？”等问题。

    In 1989 George Cybenko proved in a landmark paper that wide shallow neural networks can approximate arbitrary continuous functions on a compact set. This universal approximation theorem sparked a lot of follow-up research.  Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks in $L^p$-norms with $p \in [1,\infty)$. Kidger and Lyons proved a universal approximation theorem for deep narrow ReLU-networks. Telgarsky gave an example of a deep narrow ReLU-network that cannot be approximated by a wide shallow ReLU-network unless it has exponentially many neurons.  However, there are even more questions that still remain unresolved. Are there any wide shallow ReLU-networks that cannot be approximated well by deep narrow ReLU-networks? Is the universal approximation theorem still true for other norms like the Sobolev norm $W^{1,1}$? Do these results hold for activation functions other than ReLU?  We will answer all of those questions and more with a framework of two exp
    
[^43]: 关于贝叶斯CART的混合速率

    On Mixing Rates for Bayesian CART. (arXiv:2306.00126v1 [math.ST])

    [http://arxiv.org/abs/2306.00126](http://arxiv.org/abs/2306.00126)

    本文研究了贝叶斯CART算法中的混合速率，并提供了一些充分条件。作者指出，基于生长和修剪步骤的贝叶斯CART无法快速到达深度孤立信号。作者提出了Twiggy贝叶斯CART以解决这个问题。

    

    MCMC的贝叶斯推断的成功与否在很大程度上取决于马尔科夫链是否能迅速达到后验分布。尽管对于贝叶斯非参数后验的推理理论颇丰，但从理想的后验目标模拟MCMC算法的收敛性质并不完全了解。本文重点研究了贝叶斯CART算法，该算法是贝叶斯添加回归树(BART)的构建块。我们推导了在各种提议分布下典型后验的混合时间的上界。利用树的小波表示，我们在信号的某些分层连接限制下提供了贝叶斯CART良好混合（多项式混合）的充分条件。我们还得出了一个负面结果，表明基于简单的生长和修剪步骤的贝叶斯CART不能在指数混合时间内快速到达深度孤立信号。为了纠正近视树的探测，我们提出了Twiggy贝叶斯CART。

    The success of Bayesian inference with MCMC depends critically on Markov chains rapidly reaching the posterior distribution. Despite the plentitude of inferential theory for posteriors in Bayesian non-parametrics, convergence properties of MCMC algorithms that simulate from such ideal inferential targets are not thoroughly understood. This work focuses on the Bayesian CART algorithm which forms a building block of Bayesian Additive Regression Trees (BART). We derive upper bounds on mixing times for typical posteriors under various proposal distributions. Exploiting the wavelet representation of trees, we provide sufficient conditions for Bayesian CART to mix well (polynomially) under certain hierarchical connectivity restrictions on the signal. We also derive a negative result showing that Bayesian CART (based on simple grow and prune steps) cannot reach deep isolated signals in faster than exponential mixing time. To remediate myopic tree exploration, we propose Twiggy Bayesian CART w
    
[^44]: 通过遗憾最小化方法进行Pareto前沿识别

    Pareto Front Identification with Regret Minimization. (arXiv:2306.00096v1 [stat.ML])

    [http://arxiv.org/abs/2306.00096](http://arxiv.org/abs/2306.00096)

    本文提出了适用于线性Bandit的Pareto前沿识别算法，该算法使用所有动作的上下文信息，具有较低的样本复杂度，并同时保证了Pareto前沿识别和Pareto遗憾最小化。

    

    本文考虑线性Bandit情况下的Pareto前沿识别（PFILin），其目标是在平均奖励向量作为环境的线性函数的情况下，识别一组奖励向量不被其他任何向量所占优。PFILin包括最佳动作识别和多目标主动学习等特殊情况。我们提出的算法的样本复杂度为$\tilde{O}(d/\Delta^2)$，其中$d$是上下文的维数，$\Delta$是问题复杂性的一种度量。我们的样本复杂度在对数因子上是最优的。本算法的一个新特点是使用所有动作的上下文信息。除了有效地识别Pareto前沿之外，我们的算法还保证，在样本数大于$\Omega(d\log dL)$时，对于$L$维矢量奖励，瞬时Pareto遗憾的$\tilde{O}(\sqrt{d/t})$界限。通过使用所有动作的上下文信息，我们提出的算法同时为线性Bandit提供了有效的Pareto前沿识别和Pareto遗憾最小化。

    We consider Pareto front identification for linear bandits (PFILin) where the goal is to identify a set of arms whose reward vectors are not dominated by any of the others when the mean reward vector is a linear function of the context. PFILin includes the best arm identification problem and multi-objective active learning as special cases. The sample complexity of our proposed algorithm is $\tilde{O}(d/\Delta^2)$, where $d$ is the dimension of contexts and $\Delta$ is a measure of problem complexity. Our sample complexity is optimal up to a logarithmic factor. A novel feature of our algorithm is that it uses the contexts of all actions. In addition to efficiently identifying the Pareto front, our algorithm also guarantees $\tilde{O}(\sqrt{d/t})$ bound for instantaneous Pareto regret when the number of samples is larger than $\Omega(d\log dL)$ for $L$ dimensional vector rewards. By using the contexts of all arms, our proposed algorithm simultaneously provides efficient Pareto front ide
    
[^45]: 一个在约化李群上等变的神经网络通用框架

    A General Framework for Equivariant Neural Networks on Reductive Lie Groups. (arXiv:2306.00091v1 [stat.ML])

    [http://arxiv.org/abs/2306.00091](http://arxiv.org/abs/2306.00091)

    本文提出了一个通用的等变神经网络架构，能够尊重任何一个约化李群G的有限维表示的对称性，并通过在多个领域的任务上进行实验来证明其性能。

    

    约化李群，如正交群、洛伦兹群或幺正群，在高能物理、量子力学、量子色动力学、分子动力学、计算机视觉和成像等各个科学领域中扮演着重要角色。本文提出了一个通用的等变神经网络架构，能够尊重任何一个约化李群G的有限维表示的对称性。我们的方法推广了成功的ACE和MACE架构对于点云的原子级数据等变到任何一个对约化李群作用等变的数据。我们还引入了lie-nn软件库，提供了开发和实现这种通用G-等变神经网络所需的所有工具。它实现了将表示的通用张量积减少到不可约表示的例行程序，使得我们的架构可以应用到各种问题和群体。通过在各种任务上的实验，包括分子动力学模拟、扩散MRI和计算机视觉，展示了我们框架的普适性和性能。

    Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group G. Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general G-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our 
    
[^46]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    
[^47]: 量子机器学习的阴影

    Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])

    [http://arxiv.org/abs/2306.00061](http://arxiv.org/abs/2306.00061)

    量子机器学习模型需要使用量子计算机进行评估，但我们提出在训练完后，使用量子计算机生成一个经典阴影模型来计算函数的经典计算近似，避免了对量子计算机的需求。

    

    量子机器学习经常被认为是利用量子计算机解决实际问题的最有前途的应用之一。然而，阻碍其在实践中广泛使用的主要障碍是这些模型即使在训练过程后，仍需要访问量子计算机才能对新数据进行评估。为解决这个问题，我们建议在量子模型的训练阶段之后，量子计算机可以用来生成我们所谓的该模型的“经典阴影”，即已学习函数的经典计算近似。虽然最近的研究已经探讨了这个想法并提出了构建这种影子模型的方法，但它们也提出了一个完全经典模型可能代替的可能性，从而首先回避了量子计算机的需要。本文采用新的方法，基于量子线性模型和经典阴影重构的框架来定义阴影模型。

    Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
    
[^48]: 用Johnson-Lindenstrauss矩阵进行标签嵌入

    Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])

    [http://arxiv.org/abs/2305.19470](http://arxiv.org/abs/2305.19470)

    这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。

    

    我们提出了一个基于Johnson-Lindenstrauss矩阵（JLMs）的简单且可扩展的极端多元分类框架。利用JLM的列来嵌入标签，将一个C类分类问题转化为具有$\cO(\log C)$输出维度的回归问题。我们得出了一个超量风险限制，阐明了计算效率和预测准确性之间的权衡，并进一步表明，在Massart噪声条件下，降维的惩罚会消失。我们的方法易于并行化，并且实验结果展示了在大规模应用中其有效性和可扩展性。

    We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
    
[^49]: 基于大语言模型的多项选择题答案确认预测研究

    Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])

    [http://arxiv.org/abs/2305.18404](http://arxiv.org/abs/2305.18404)

    本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。

    

    随着大型语言模型的广泛开发，对它们进行健壮的不确定性量化技术将成为它们在高风险场景下安全部署的关键。本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。这种观察对于下游应用，如选择性分类和过滤低质量预测，可能会有用。我们还研究了符合性预测对于超出主题的问题的交换性假设，这可能是许多实际应用的更为现实的场景。本研究为在需要可靠保证错误率的安全关键情况下更加值得信赖和可靠地使用大型语言模型做出了贡献。

    As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
    
[^50]: 通过草图技术，将粒子方法和张量网络方法结合用于偏微分方程求解

    Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching. (arXiv:2305.17884v1 [math.NA])

    [http://arxiv.org/abs/2305.17884](http://arxiv.org/abs/2305.17884)

    本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。

    

    本文提出了一种解决高维偏微分方程的张量网络框架，其中我们采用粒子模拟更新解决方案，并使用最近提出的张量列车草图技术将新解决方案重新估计为张量网络。我们的方法还可以被解释为通过假设粒子来自底层张量网络来执行粒子数控制的可替代方法。我们通过将其应用于两种特定的情景来展示我们方法的通用性和灵活性：通过Langevin动力学模拟Fokker-Planck方程和通过辅助场量子蒙特卡罗模拟量子虚时间演化。

    In this paper, we propose a general framework for solving high-dimensional partial differential equations with tensor networks. Our approach offers a comprehensive solution methodology, wherein we employ a combination of particle simulations to update the solution and re-estimations of the new solution as a tensor-network using a recently proposed tensor train sketching technique. Our method can also be interpreted as an alternative approach for performing particle number control by assuming the particles originate from an underlying tensor network. We demonstrate the versatility and flexibility of our approach by applying it to two specific scenarios: simulating the Fokker-Planck equation through Langevin dynamics and quantum imaginary time evolution via auxiliary-field quantum Monte Carlo.
    
[^51]: 疾病患者个体根本原因的反事实公式化

    Counterfactual Formulation of Patient-Specific Root Causes of Disease. (arXiv:2305.17574v1 [cs.AI])

    [http://arxiv.org/abs/2305.17574](http://arxiv.org/abs/2305.17574)

    本文提出了一种针对疾病患者个体的根本原因的新公式，可以用于自动从数据中检测根本原因，并考虑了噪声标签和疾病流行率等因素，同时具有快速计算的优势。

    

    疾病的根本原因直观地对应于增加诊断可能性的根本顶点。然而，这种根本原因的描述缺乏计算机算法发展所需的严格数学公式。在以前的工作中，使用干预主义者帐户定义了疾病的病人特定根本原因，该帐户仅攀升到珍珠的因果Ladder的第二层。在这个理论性的文章中，我们通过提出反事实的定义来攀升到第三层，以匹配基于固定事实数据的临床直觉。然后，我们展示了如何使用可解释的人工智能的Shapley值为每个变量分配根因贡献得分。提出的疾病患者个体根本原因的反事实公式化考虑了噪声标签，适应了疾病的流行率，并允许快速计算，无需反事实模拟。

    Root causes of disease intuitively correspond to root vertices that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. Prior work defined patient-specific root causes of disease using an interventionalist account that only climbs to the second rung of Pearl's Ladder of Causation. In this theoretical piece, we climb to the third rung by proposing a counterfactual definition matching clinical intuition based on fixed factual data alone. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence. The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.
    
[^52]: 随机梯度下降中动态稳定性的隐式正则化

    The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent. (arXiv:2305.17490v1 [stat.ML])

    [http://arxiv.org/abs/2305.17490](http://arxiv.org/abs/2305.17490)

    本文研究了随机梯度下降的动态稳定性隐式正则化，证明了其具有良好的泛化性。

    

    本文从“动态稳定性”的角度研究了随机梯度下降（SGD）的隐式正则化。我们首先修正了现有SGD稳定性分析的问题，展示了Hessian矩阵的Frobenius范数和迹与不同稳定性概念的关系。特别地，如果全局最小值在SGD中是线性稳定的，则Hessian矩阵的迹必须小于或等于$2/\eta$，其中$\eta$表示学习率。然而，对于梯度下降（GD），稳定性只对Hessian矩阵的最大特征值施加类似的约束。我们接着分析了这些稳定极值的泛化性质，着重关注了两层ReLU网络和对角线性网络。特别地，我们建立了这两个模型的尖锐度度量和某些参数规范之间的“等价性”，从而证明了SGD的稳定极值具有良好的泛化性。然而，GD的稳定性正则化只在特定情况下产生泛化效益。最后，我们将我们的理论应用于深度线性网络问题，结果表明它对某些模型的表现优于Lasso或岭正则化。

    In this paper, we study the implicit regularization of stochastic gradient descent (SGD) through the lens of {\em dynamical stability} (Wu et al., 2018). We start by revising existing stability analyses of SGD, showing how the Frobenius norm and trace of Hessian relate to different notions of stability. Notably, if a global minimum is linearly stable for SGD, then the trace of Hessian must be less than or equal to $2/\eta$, where $\eta$ denotes the learning rate. By contrast, for gradient descent (GD), the stability imposes a similar constraint but only on the largest eigenvalue of Hessian. We then turn to analyze the generalization properties of these stable minima, focusing specifically on two-layer ReLU networks and diagonal linear networks. Notably, we establish the {\em equivalence} between these metrics of sharpness and certain parameter norms for the two models, which allows us to show that the stable minima of SGD provably generalize well. By contrast, the stability-induced reg
    
[^53]: 从理论角度揭示“思维链”背后的奥秘

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。

    

    最近的研究发现，"思维链"提示能够显著提高大型语言模型（LLMs）的性能，特别是在涉及数学或推理的复杂任务中。尽管获得了巨大的实证成功，但“思维链”背后的机制以及它如何释放LLMs的潜力仍然是神秘的。本文首次从理论上回答了这些问题。具体而言，我们研究了LLMs带有“思维链”在解决基本数学和决策问题中的能力。我们首先给出一个不可能的结果，表明任何有限深度的Transformer都不能直接输出正确的基本算术/方程任务的答案，除非模型大小随着输入长度的增加呈超多项式增长。相反，我们通过构造证明，大小恒定的自回归Transformer足以通过使用常用的数学语言形式生成“思维链”推导来解决这两个任务。

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^54]: 通过中度偏差理论进行最优学习

    Optimal Learning via Moderate Deviations Theory. (arXiv:2305.14496v1 [stat.ML])

    [http://arxiv.org/abs/2305.14496](http://arxiv.org/abs/2305.14496)

    本文提出了一种能够在广泛模型中进行最优学习的方法，利用中度偏差原理构建高度准确的置信区间，满足指数精度、一致性和最大精度等标准，为该方法提供了理论依据。

    

    本文提出了一种在广泛模型中使用置信区间学习函数值的统计最优方法，包括描述为随机规划问题或各种SDE模型的期望损失的一般非参数估计。更准确地说，我们通过采用基于中度偏差原理的方法系统地构建高度准确的置信区间。研究表明，所提出的置信区间在统计意义上是最优的，因为它们满足以指数精度、最小性、一致性、误判概率以及最终的一致最大精度为标准的要求。该方法提出的置信区间是通过强化优化问题的解来表达的，其中不确定性通过数据生成过程引发的中度偏差率函数来表示。我们演示了对于许多模型，这些优化问题具有易于解的结果。

    This paper proposes a statistically optimal approach for learning a function value using a confidence interval in a wide range of models, including general non-parametric estimation of an expected loss described as a stochastic programming problem or various SDE models. More precisely, we develop a systematic construction of highly accurate confidence intervals by using a moderate deviation principle-based approach. It is shown that the proposed confidence intervals are statistically optimal in the sense that they satisfy criteria regarding exponential accuracy, minimality, consistency, mischaracterization probability, and eventual uniformly most accurate (UMA) property. The confidence intervals suggested by this approach are expressed as solutions to robust optimization problems, where the uncertainty is expressed via the underlying moderate deviation rate function induced by the data-generating process. We demonstrate that for many models these optimization problems admit tractable r
    
[^55]: 基于矩阵流形的神经网络构建：陀螺矢量空间方法

    Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach. (arXiv:2305.04560v1 [stat.ML])

    [http://arxiv.org/abs/2305.04560](http://arxiv.org/abs/2305.04560)

    本文通过将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，提出了在这些流形上构建神经网络的新模型和新层，并在人类动作识别和知识图谱完成两个应用中展示了其有效性。

    

    矩阵流形，如对称正定（SPD）矩阵和Grassmann流形，出现在许多应用中。最近，通过应用陀螺群和陀螺矢量空间的理论——这是一个研究双曲几何的强大框架——一些工作尝试在矩阵流形上构建欧几里德神经网络的原则性推广。然而，由于缺乏考虑流形的内积和陀螺角等概念的陀螺矢量空间，相比于用于研究双曲几何的那些概念，这些工作提供的技术和数学工具仍然有限。在本文中，我们将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，并提出了在这些流形上构建神经网络的新模型和新层。我们展示了我们的方法在人类动作识别和知识图谱完成两个应用中的有效性。

    Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.
    
[^56]: Data-OOB:以无需额外计算的Out-of-bag估计为准的数据价值估计方法

    Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])

    [http://arxiv.org/abs/2304.07718](http://arxiv.org/abs/2304.07718)

    Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。

    

    数据评估是一个强大的框架，可以为模型训练提供统计洞察力，以区分哪些数据对于模型训练是有益的，哪些是有害的。纵观各种下游任务，许多以Shapley为基础的数据价值评估方法均显示出了很有前途的结果。然而，由于这需要训练大量的模型，因此众所周知，这是具有挑战性的。因此，将此应用于大型数据集是不可行的。为了解决这个问题，我们提出了Data-OOB，这是一种新的数据价值估计方法，针对bagging模型，它利用了out-of-bag估计。所提出的方法在计算上是高效的，可以通过重复使用训练好的弱学习器来扩展到数百万个数据。具体而言，当评估100个输入维度且存在$10^6$个样本时，Data-OOB仅需要在单个CPU处理器上执行不到2.25个小时。此外，Data-OOB在理论上有坚实的解释，当两个离差值函数相同时，其识别具有相同重要性的数据点。

    Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
    
[^57]: 一种基于数据驱动的状态聚合方法用于动态离散选择模型

    A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])

    [http://arxiv.org/abs/2304.04916](http://arxiv.org/abs/2304.04916)

    本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。

    

    我们研究了动态离散选择模型，其中一个常见的问题是使用代理行为数据估计代理奖励函数（也称为“结构参数”）的参数。这种模型的最大似然估计需要动态规划，这受到维度灾难的限制。在本文中，我们提出了一种新颖的算法，提供了一种数据驱动的方法来选择和聚合状态，降低了估计的计算和样本复杂度。我们的方法分两个阶段。在第一阶段中，我们使用灵活的反向强化学习方法来估计代理Q函数。我们使用这些估计的Q函数，以及一个聚类算法，选择了一些最为重要的状态，这些状态对于驱动Q函数的变化最为关键。在第二阶段，利用这些被选择的“聚合”状态，我们使用常用的嵌套固定点算法进行最大似然估计。所提出的二阶段方法实现了...

    We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
    
[^58]: 用分位随机森林模型在密度泛函理论中外推完备基组极限

    Extrapolation to complete basis-set limit in density-functional theory by quantile random-forest models. (arXiv:2303.14760v2 [physics.comp-ph] CROSS LISTED)

    [http://arxiv.org/abs/2303.14760](http://arxiv.org/abs/2303.14760)

    本文利用分位随机森林模型外推完备基组极限，并提供了预测区间以量化模型的不确定性。

    

    密度泛函理论（DFT）计算的数值精度取决于各种计算参数，其中最关键的之一是基组大小。理论上，使用无限大的基组，即完备基组集，可以达到最高精度。我们的目标是找到一种机器学习模型，将有限基组大小计算外推到完备基组极限。我们从63个二元固体的数据集开始，这些固体使用两种全电子DFT代码，即exciting和FHI-aims，这两种代码使用非常不同类型的基组。使用分位随机森林模型来估计基组大小关于全收敛计算的总能量修正。该随机森林模型对于两种代码都实现了小于25%的对称平均绝对百分比误差，而且胜过了文献中之前的方法。我们的方法还提供了预测区间，可以量化模型的不确定性。

    The numerical precision of density-functional-theory (DFT) calculations depends on a variety of computational parameters, one of the most critical being the basis-set size. The ultimate precision is reached with an infinitely large basis set, i.e., in the limit of a complete basis set (CBS). Our aim in this work is to find a machine-learning model that extrapolates finite basis-size calculations to the CBS limit. We start with a data set of 63 binary solids investigated with two all-electron DFT codes, exciting and FHI-aims, which employ very different types of basis sets. A quantile-random-forest model is used to estimate the total-energy correction with respect to a fully converged calculation as a function of the basis-set size. The random-forest model achieves a symmetric mean absolute percentage error of lower than 25% for both codes and outperforms previous approaches in the literature. Our approach also provides prediction intervals, which quantify the uncertainty of the models'
    
[^59]: 布雷-瓦瑟斯坦距离训练下的生成式深度线性网络的关键点和收敛性分析

    Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss. (arXiv:2303.03027v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.03027](http://arxiv.org/abs/2303.03027)

    本文使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型，并在有限秩矩阵空间内表征关键点和最小化问题，最终确定了梯度下降算法的收敛性。

    

    本文探讨了一种使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型。相较于以往的研究，我们所提出的模型在损失函数和生成式设置上有所不同。我们在有限秩矩阵空间内表征了该方法的关键点和最小化问题。针对低秩矩阵而言，该方法的海森矩阵理论上可能会爆炸，这为优化方法的收敛性分析带来了挑战。我们确定了梯度下降算法中使用损失的平滑微扰版本时的收敛性，并在初始权重的一定假设条件下证明了有限步长梯度下降的收敛性。

    We consider a deep matrix factorization model of covariance matrices trained with the Bures-Wasserstein distance. While recent works have made important advances in the study of the optimization problem for overparametrized low-rank matrix approximation, much emphasis has been placed on discriminative settings and the square loss. In contrast, our model considers another interesting type of loss and connects with the generative setting. We characterize the critical points and minimizers of the Bures-Wasserstein distance over the space of rank-bounded matrices. For low-rank matrices the Hessian of this loss can theoretically blow up, which creates challenges to analyze convergence of optimizaton methods. We establish convergence results for gradient flow using a smooth perturbative version of the loss and convergence results for finite step size gradient descent under certain assumptions on the initial weights.
    
[^60]: 平均Hölder平滑度下的近似最优学习

    Near-optimal learning with average H\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06005](http://arxiv.org/abs/2302.06005)

    通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。

    

    我们将Ashlagi等人（COLT 2021）提出的平均Lipschitz平滑性概念推广到Hölder平滑性，并证明了关于平均Hölder平滑性的上下风险界，这些界的速率甚至在平均Lipschitz平滑性的特殊情况下也优于之前已知界。此外，我们的下界在可实现情况下是最优的，最多差一个对数因子，从而建立了极小值率。从算法的角度来看，由于我们对平均平滑度的定义是针对未知的基础分布的，因此学习者没有函数类的显式表示，无法执行ERM。尽管如此，我们提供了独立的学习算法。

    We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\"older smoothness. This measure of the "effective smoothness" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic "worst-case H\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms
    
[^61]: SSL的相互作用：增强、归纳偏差和泛化

    The SSL Interplay: Augmentations, Inductive Bias, and Generalization. (arXiv:2302.02774v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02774](http://arxiv.org/abs/2302.02774)

    本论文通过分析数据增强、网络架构和训练算法的相互作用，研究了预训练和下游任务的泛化性能，并为SSL从业人员提供了一些见解。

    

    自监督学习（SSL）已成为一种无需监督即可从原始数据中学习表示的强大框架。然而，在实践中，工程师面临着调整优化器和训练过程中表示塌陷等问题。这些挑战促使我们需要一种理论来阐明数据增强、网络架构和训练算法的选择之间的复杂相互作用。我们在一个理论友好的设置下，通过对预训练和下游任务的泛化性能进行精确分析，研究了这种相互作用，并强调了我们理论得出的SSL从业者的一些见解。

    Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.
    
[^62]: 改进的多时期多分类装载问题算法，同时带有bandit反馈

    Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback. (arXiv:2301.13791v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13791](http://arxiv.org/abs/2301.13791)

    本文提出了一个改进的算法来解决多时期多分类装载问题，其中使用bandit反馈，提出的算法具有更快的收敛速率和更低的遗憾，同时解决了一个相关问题。

    

    本文考虑了线性背景下的多类多期装载问题（LMMP），它的目标是将物品装载到预算向量下，并使总价值尽可能大。我们考虑的情况是，与每个操作相关联的奖励和消耗向量是上下文相关的线性函数，并且决策者得到了bandit反馈。当预算至少增长为$ \sqrt{T}$时，我们提出了一种闭合形式的bandit策略，这样可以在上下文维度，分类数和时间范围$T$下保持次线性的遗憾。我们还解决了Agrawal＆Goyal（2018）在LCBK问题中提出的一个开放问题，并提出了一个可以获得更快收敛速率的新的估计器。

    We consider the linear contextual multi-class multi-period packing problem (LMMP) where the goal is to pack items such that the total vector of consumption is below a given budget vector and the total value is as large as possible. We consider the setting where the reward and the consumption vector associated with each action is a class-dependent linear function of the context, and the decision-maker receives bandit feedback. LMMP includes linear contextual bandits with knapsacks and online revenue management as special cases. We establish a new estimator which guarantees a faster convergence rate, and consequently, a lower regret in such problems. We propose a bandit policy that is a closed-form function of said estimated parameters. When the contexts are non-degenerate, the regret of the proposed policy is sublinear in the context dimension, the number of classes, and the time horizon $T$ when the budget grows at least as $\sqrt{T}$. We also resolve an open problem posed by Agrawal &
    
[^63]: 基于硬币采样的无需学习速率的基于梯度的贝叶斯推断方法

    Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates. (arXiv:2301.11294v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11294](http://arxiv.org/abs/2301.11294)

    本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。

    

    近年来，基于粒子的变分推断（ParVI）方法如Stein变分梯度下降（SVGD）因可扩展性在贝叶斯推理中越来越受欢迎。然而，这些方法的性质不可避免地取决于超参数（如学习速率），必须由从业者仔细调整，以确保以合适的速率收敛到目标测度。在本文中，我们引入了一组新的基于硬币投注的可扩展贝叶斯推断方法，这些方法完全不需要学习速率。我们在一系列数值例子中演示了我们方法的性能，包括几个高维模型和数据集，证明了与其他ParVI算法相当的性能，而无需调整学习速率。

    In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.
    
[^64]: 使用来自成对或$K$元比较的人类反馈的规范强化学习

    Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270)

    该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。

    

    我们为带有人类反馈的强化学习问题提供了一个理论框架。我们的分析表明，当真实奖励函数为线性函数时，最大似然估计（MLE）在Bradley-Terry-Luce（BTL）模型和Plackett-Luce（PL）模型下均收敛。然而，我们发现当基于学得的奖励模型训练策略时，MLE会失败，而基于悲观估计的MLE在一定的覆盖假设下提供性能更好的策略。此外，我们证明在PL模型下，真实MLE和将$k$元比较拆分为成对比较的备选MLE都收敛。而真实MLE是渐近更为高效的。我们的结果验证了现有RLHF算法（如InstructGPT）的实验成功，并为算法设计提供了新的见解。此外，我们的结果统一了RLHF问题和最大熵反向强化学习(IRL)问题，并为其提供了第一个样本复杂度界。

    We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
    
[^65]: 图神经切比雪夫核：大规模图上的收敛分析

    Graph Neural Tangent Kernel: Convergence on Large Graphs. (arXiv:2301.10808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10808](http://arxiv.org/abs/2301.10808)

    本文利用GNTK和图核函数探究大规模图神经网络的训练动态，证明了GNTK在收敛于图核函数时的确切性质。这意味着在大规模图的情况下，可以在中等大小的图上进行拟合并在整个图上使用。

    

    图神经网络在图机器学习任务中表现出色，但在大规模图数据上训练时往往比较困难，因为它们的学习动态难以理解。本文利用图神经切比雪夫核（GNTK）和图核函数探究大规模图神经网络的训练动态。在宽度趋于无穷大时，过参数化神经网络的优化等价于在NTK上进行核回归。我们研究了GNTK在另一个独立的维度（图大小）变化时的演化情况，并使用图核函数来定义极限对象——GNN的图核函数和GNTK的图核函数，并证明，在一系列图上，GNTK收敛于图核函数。进一步证明了GNTK的谱依赖于最快学习的方向，这在早期停止训练时变得特别重要，谱也收敛于图核函数的谱。这意味着在大规模图的限制下，对中等大小的图进行拟合的GNTK可以用于在图上进行拟合。

    Graph neural networks (GNNs) achieve remarkable performance in graph machine learning tasks but can be hard to train on large-graph data, where their learning dynamics are not well understood. We investigate the training dynamics of large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. In the limit of large width, optimization of an overparametrized NN is equivalent to kernel regression on the NTK. Here, we investigate how the GNTK evolves as another independent dimension is varied: the graph size. We use graphons to define limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs -- , and prove that, on a sequence of graphs, the GNTKs converge to the graphon NTK. We further prove that the spectrum of the GNTK, which is related to the directions of fastest learning which becomes relevant during early stopping, converges to the spectrum of the graphon NTK. This implies that in the large-graph limit, the GNTK fitted on a graph of moderate size can be used to s
    
[^66]: 重复嵌套期望的最优随机多层蒙特卡罗方法

    Optimal randomized multilevel Monte Carlo for repeatedly nested expectations. (arXiv:2301.04095v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2301.04095](http://arxiv.org/abs/2301.04095)

    本论文提出了一种名为$\mathsf{READ}$的新的蒙特卡罗估计器，具有每个固定$D$的最优计算成本$\mathcal{O}(\varepsilon^{-2})$以及在更一般的假设下，任意$0 < \delta < \frac{1}{2}$几乎最优的计算成本$\mathcal{O}(\varepsilon^{-2(1 + \delta)})$。

    

    在许多现实世界的系统中，重复嵌套期望的估计是一个具有挑战性的任务。然而，当嵌套数目变得很大时，现有的方法通常会遭受高计算成本的困扰。本文提出了一种新的蒙特卡罗估计器$\mathsf{READ}$，它具有每个固定$D$的最优计算成本$\mathcal{O}(\varepsilon^{-2})$以及在更一般的假设下，任意$0 < \delta < \frac{1}{2}$几乎最优的计算成本$\mathcal{O}(\varepsilon^{-2(1 + \delta)})$。

    The estimation of repeatedly nested expectations is a challenging task that arises in many real-world systems. However, existing methods generally suffer from high computational costs when the number of nestings becomes large. Fix any non-negative integer $D$ for the total number of nestings. Standard Monte Carlo methods typically cost at least $\mathcal{O}(\varepsilon^{-(2+D)})$ and sometimes $\mathcal{O}(\varepsilon^{-2(1+D)})$ to obtain an estimator up to $\varepsilon$-error. More advanced methods, such as multilevel Monte Carlo, currently only exist for $D = 1$. In this paper, we propose a novel Monte Carlo estimator called $\mathsf{READ}$, which stands for "Recursive Estimator for Arbitrary Depth.'' Our estimator has an optimal computational cost of $\mathcal{O}(\varepsilon^{-2})$ for every fixed $D$ under suitable assumptions, and a nearly optimal computational cost of $\mathcal{O}(\varepsilon^{-2(1 + \delta)})$ for any $0 < \delta < \frac12$ under much more general assumptions. 
    
[^67]: 私下高效、鲁棒且最优地估计高斯分布

    Privately Estimating a Gaussian: Efficient, Robust and Optimal. (arXiv:2212.08018v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2212.08018](http://arxiv.org/abs/2212.08018)

    本论文提出了在纯和近似差分隐私模型下，对于未知的 $d$ 维高斯分布进行任意微小差异总变差误差的高效算法，并容忍恶意离群值的存在， 样本复杂度与维度的依赖是最优的；我们还证明了样本上界与目标协方差矩阵的条件数 $\kappa$ 的依赖关系也是最紧密的。

    

    本文提出了在纯和近似差分隐私模型下，针对高斯分布的高效算法，并且样本复杂度与维度的依赖是最优的。在纯差分隐私设置下，我们提供了一种有效的算法，使用 $\widetilde{O}(d^2 \log \kappa)$ 个样本，可以对未知的 $d$ 维高斯分布进行任意微小差异总变差误差的估计，同时容忍恶意离群值的存在。这里，$\kappa$ 是目标协方差矩阵的条件数。该样本上界在维度（多对数因子）的依赖上与最佳非隐私估计器相匹配, 我们证明了隐私协方差矩阵估计的一个新的下界，为了表明上文样本上界与目标协方差矩阵的条件数 $\kappa$ 的依赖关系也是最紧密的。在本工作之前，我們只知道该问题的可识别性结果（导致非常低效的超多项式时间算法）。

    In this work, we give efficient algorithms for privately estimating a Gaussian distribution in both pure and approximate differential privacy (DP) models with optimal dependence on the dimension in the sample complexity. In the pure DP setting, we give an efficient algorithm that estimates an unknown $d$-dimensional Gaussian distribution up to an arbitrary tiny total variation error using $\widetilde{O}(d^2 \log \kappa)$ samples while tolerating a constant fraction of adversarial outliers. Here, $\kappa$ is the condition number of the target covariance matrix. The sample bound matches best non-private estimators in the dependence on the dimension (up to a polylogarithmic factor). We prove a new lower bound on differentially private covariance estimation to show that the dependence on the condition number $\kappa$ in the above sample bound is also tight. Prior to our work, only identifiability results (yielding inefficient super-polynomial time algorithms) were known for the problem. In
    
[^68]: 无分布双分块混合成员模型：一种新的用于重叠双分块加权网络社区发现的模型

    Bipartite Mixed Membership Distribution-Free Model. A novel model for community detection in overlapping bipartite weighted networks. (arXiv:2211.00912v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2211.00912](http://arxiv.org/abs/2211.00912)

    提出了一种无分布双分块混合成员模型（BiMMDF），可用于重叠双分块加权网络的社区发现，并可以模拟重叠双分块符号网络。该模型的估计具有一致性保证和理论分离条件，并可提高在合成网络和现实网络应用中的性能。

    

    近年来，对于重叠单分块无权重网络的混合成员建模和估计已经得到了广泛的研究。然而，据我们所知，没有模型适用于更一般的情况，即重叠双分块加权网络。为了填补这一空白，我们引入了一种新的模型，即无分布双分块混合成员模型（BiMMDF）。我们的模型允许邻接矩阵遵循任何分布，只要其期望具有与节点成员有关的块结构即可。特别地，BiMMDF可以模拟重叠双分块符号网络，并且是许多先前模型的扩展，包括流行的混合成员随机块模型。我们应用具有一致估计理论保证的高效算法来拟合BiMMDF。我们进一步探讨了不同分布的BiMMDF的分离条件。此外，我们还考虑了稀疏网络的缺失边缘。BiMMDF的优势在广泛的合成网络和现实网络应用中得到了展示。

    Modeling and estimating mixed memberships for overlapping unipartite un-weighted networks has been well studied in recent years. However, to our knowledge, there is no model for a more general case, the overlapping bipartite weighted networks. To close this gap, we introduce a novel model, the Bipartite Mixed Membership Distribution-Free (BiMMDF) model. Our model allows an adjacency matrix to follow any distribution as long as its expectation has a block structure related to node membership. In particular, BiMMDF can model overlapping bipartite signed networks and it is an extension of many previous models, including the popular mixed membership stochastic blcokmodels. An efficient algorithm with a theoretical guarantee of consistent estimation is applied to fit BiMMDF. We then obtain the separation conditions of BiMMDF for different distributions. Furthermore, we also consider missing edges for sparse networks. The advantage of BiMMDF is demonstrated in extensive synthetic networks an
    
[^69]: 算术采样：用于大型语言模型的并行多样化解码

    Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15458](http://arxiv.org/abs/2210.15458)

    本文章提出了一种算术采样框架，该方法可兼容常见的采样变化，具有可证明的束多样性和令人尴尬的并行性，从原始模型提供无偏和一致的期望。在WMT机器翻译中表现出良好的效果。

    

    大型语言模型的解码方法通常在输出多样性和计算并行性之间进行权衡。本文提出了一种框架，根据由大型语言模型隐式定义的算术代码书进行采样，兼容常见的采样变化，满足一定条件下的可证明的束多样性，同时具有令人尴尬的并行性，并从原始模型提供无偏和一致的期望。我们在WMT机器翻译上展示了我们方法的有效性，将预期的BLEU分数奖励的标准差减少了一半以上，同时与先前的最新方法有了相当的表现。

    Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the 
    
[^70]: 用中点 Mixup 在多视角数据中可证明学习多元特征

    Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13512](http://arxiv.org/abs/2210.13512)

    本文提出了一种基于中点 Mixup 的多视角数据学习方法，相比于传统经验风险最小化方法能够更好地学习每个类别的所有特征，具有更好的泛化和鲁棒性。

    

    Mixup 是一种数据增强技术，依赖于使用数据点和标签的随机凸组合进行训练。近年来，Mixup 已成为训练最先进的图像分类模型的标准基元，因为它在泛化和鲁棒性方面比经验风险最小化有明显的优势。在这项工作中，我们试图从特征学习的角度解释一些这种成功。我们关注的分类问题是，每个类别可能具有多个相关特征（或视图），可用于正确预测类别。我们的主要理论结果表明，在具有每类两个特征的一类非平凡数据分布中，使用经验风险最小化训练 2 层卷积网络可能会导致几乎所有类别只学习一个特征，而使用 Mixup 的特定实例进行训练可以成功地学习每个类别的两个特征。

    Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
    
[^71]: 关于因果位置-尺度噪声模型的可识别性和估计

    On the Identifiability and Estimation of Causal Location-Scale Noise Models. (arXiv:2210.09054v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09054](http://arxiv.org/abs/2210.09054)

    本文研究了一类异方差噪声模型，发现除特殊情况外因果方向是可识别的。提出了两个估计器，能够准确识别因果效应。

    

    本文研究了一类位置-尺度或异方差噪声模型（LSNMs），在其中，效应$ Y $可以被写成是因果$ X $和与$ X $无关的噪声源$ N $的函数，但可能被因果$ X $缩放为一个正函数$ g（X）$，即$ Y = f（X）+ g（X）N $。尽管模型类别非常广泛，但我们发现除一些特殊情况外，因果方向是可识别的。为了在实证上验证这些理论发现，我们提出了两个LSNMs的估计器：一个基于（非线性）特征映射的估计器和一个基于神经网络的估计器。两者将$ Y $给定$ X $的条件分布建模为由其自然参数参数化的高斯分布。当特征映射被正确规定时，我们证明我们的估计器是联合凸的，并且是因果效应识别任务的一致估计器。尽管神经网络没有继承这些保证，但它可以拟合任意复杂性的函数，并达到最先进的性能。

    We study the class of location-scale or heteroscedastic noise models (LSNMs), in which the effect $Y$ can be written as a function of the cause $X$ and a noise source $N$ independent of $X$, which may be scaled by a positive function $g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of the model class, we show the causal direction is identifiable up to some pathological cases. To empirically validate these theoretical findings, we propose two estimators for LSNMs: an estimator based on (non-linear) feature maps, and one based on neural networks. Both model the conditional distribution of $Y$ given $X$ as a Gaussian parameterized by its natural parameters. When the feature maps are correctly specified, we prove that our estimator is jointly concave, and a consistent estimator for the cause-effect identification task. Although the the neural network does not inherit those guarantees, it can fit functions of arbitrary complexity, and reaches state-of-the-art performance
    
[^72]: 随机点到椭圆的近似最优匹配

    Near-optimal fitting of ellipsoids to random points. (arXiv:2208.09493v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2208.09493](http://arxiv.org/abs/2208.09493)

    本文解决了椭圆拟合问题的一个猜想，对于给定高斯点$v_1,\ldots,v_n$，我们可以构造出一个对称于原点的拟合椭圆，当$n\geq \Omega(\,d^2/\mathrm{polylog}(d)\,)$时，该椭圆实现了近似最优匹配。

    

    给定维度为$d$的独立标准高斯点$v_1,\ldots,v_n$，对于哪些$(n,d)$值范围内，高概率存在一个对称于原点的椭圆，可以同时通过所有这些点？将椭圆逼近随机点的这个基本问题与低秩矩阵分解、独立成分分析和主成分分析有关。Saunderson，Parrilo和Willsky [决策与控制会议论文集，6031-6036页，2013]根据强有力的数值证据进行猜测，当点数$n$增加时，椭圆拟合问题从可行状态变为不可行状态，此时$n\sim d^2/4$有一个明显的分界点。我们通过构建一种拟合椭圆来解决这个猜想的正确性，当$n=\Omega(\,d^2/\mathrm{polylog}(d)\,)$时，可以实现近似最优匹配，这比之前Ghosh等人[计算机科学基础研讨会论文集，954-965页，2020]的工作需要更少的点数$n=o(d^{3/2})$。

    Given independent standard Gaussian points $v_1, \ldots, v_n$ in dimension $d$, for what values of $(n, d)$ does there exist with high probability an origin-symmetric ellipsoid that simultaneously passes through all of the points? This basic problem of fitting an ellipsoid to random points has connections to low-rank matrix decompositions, independent component analysis, and principal component analysis. Based on strong numerical evidence, Saunderson, Parrilo, and Willsky [Proc. of Conference on Decision and Control, pp. 6031-6036, 2013] conjecture that the ellipsoid fitting problem transitions from feasible to infeasible as the number of points $n$ increases, with a sharp threshold at $n \sim d^2/4$. We resolve this conjecture up to logarithmic factors by constructing a fitting ellipsoid for some $n = \Omega( \, d^2/\mathrm{polylog}(d) \,)$, improving prior work of Ghosh et al. [Proc. of Symposium on Foundations of Computer Science, pp. 954-965, 2020] that requires $n = o(d^{3/2})$. O
    
[^73]: 类别不平衡下的学习动态理论分析

    A Theoretical Analysis of the Learning Dynamics under Class Imbalance. (arXiv:2207.00391v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.00391](http://arxiv.org/abs/2207.00391)

    本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。

    

    数据不平衡是机器学习中常见的问题，会严重影响模型性能。虽然有各种解决方案，但它们对学习动态的收敛影响尚未被理解。本文阐明了数据不平衡对学习的显著负面影响，当使用梯度优化器进行训练时，少数类和多数类的学习曲线会遵循次优轨迹。这种放缓与不平衡比相关，可以追溯到优化不同类别之间的竞争。我们的主要贡献在于分析了全批次（GD）和随机梯度下降（SGD）的收敛和各种对每种类别梯度做出贡献的归一化变体。我们发现GD不能保证降低每个类别的损失，但可以通过执行各自归一化梯度来解决这个问题。使用SGD时, 类别不平衡会对算法产生额外的影响。

    Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on th
    
[^74]: 非线性独立分量分析的可辨识性：稀疏性及其它

    On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07751](http://arxiv.org/abs/2206.07751)

    本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。

    

    非线性独立分量分析旨在从其可观测的非线性混合中恢复出潜在独立分量。如何使非线性ICA模型可辨识直到某些平凡不确定性是无监督学习中的一个长期问题。最近的突破是将源的标准独立性假设重新定义为在某些辅助变量（例如类标签和/或域/时间索引）给定的条件独立性，作为弱监督或归纳偏置。然而，具有无条件先验的非线性ICA无法从这些发展中受益。我们探索了一条替代路径，并仅考虑混合过程的假设，例如结构稀疏性。我们展示了在这些约束的具体实例下，独立的潜在分量可以从其非线性混合中辨识出来，达到非平凡的非线性ICA可识别性，而无需辅助变量。

    Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
    
[^75]: 在不稳健样本上施加更多正则化以提高对抗性鲁棒性

    Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03353](http://arxiv.org/abs/2206.03353)

    本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。

    

    对抗性训练是提高对抗攻击鲁棒性的一种方法，在人类视觉无法察觉的数据扰动下，使给定的深度神经网络产生误判。本文提出了一种新的对抗训练算法，它在理论上得到很好的证明，并且在实践中表现优于其他现有的算法。该算法的一个新的特点是：对于容易受到对抗攻击的数据，比其他现有的正则化算法更多地应用正则化。理论上，我们证明了我们的算法可以被理解为一个最小化经验风险的正则化算法，它来自一个新的鲁棒风险上界的动机。数值实验表明，我们提出的算法同时提高了泛化性能(在例子上的准确性)和鲁棒性(在对抗攻击上的准确性)，达到了最先进的性能水平。

    Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
    
[^76]: 通过高效探索学习预测的静态调度

    Static Scheduling with Predictions Learned through Efficient Exploration. (arXiv:2205.15695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15695](http://arxiv.org/abs/2205.15695)

    本文研究了单机作业调度的问题，提出了一种基于学习预测的静态调度算法，在类型未知的情况下实现了次线性的过剩成本，尤其在抢占式问题中表现出色，可以在不同作业类型持续时间相差很大时优于非抢占匹配。

    

    我们研究了单机作业调度，每个作业都属于决定其持续时间分布的作业类型。我们首先分析了类型特征已知的情况，然后转向两种学习情景，其中类型未知：非抢占式问题，它要求完成已启动的作业，然后才能移动到另一个作业；和抢占式问题，这里作业执行可以暂停以优先转移到另一个作业。在两种情况下，我们设计的算法相对于已知类型的性能实现了次线性的过剩成本，并证明了非抢占式情况的下限。值得注意的是，我们展示了抢占算法在不同作业类型持续时间相差很大时，理论上和通过模拟的方式可以优于非抢占匹配，在类型持续时间已知时并不存在这种现象。

    We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known.
    
[^77]: 混合流：基于混合流的原则变分推理

    MixFlows: principled variational inference via mixed flows. (arXiv:2205.07475v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.07475](http://arxiv.org/abs/2205.07475)

    本文提出了混合变分流（MixFlows），是由对初始参考分布的映射重复应用的混合组成的一种新的变分家族。MixFlows具有类似于MCMC的收敛保证，并可以提供比几种黑盒归一化流更可靠的后验逼近，与最先进的MCMC方法所获得的样本质量相当。

    

    本文提出了混合变分流（MixFlows），这是一种新的变分家族，由对初始参考分布的映射重复应用的混合组成。首先，我们提供了有效的算法，用于i.i.d.采样、密度评估和无偏ELBO估计。然后，我们证明了当流映射是遍历和保度量的时，MixFlows具有类似于MCMC的收敛保证，并为流映射近似实现提供了误差积累的界限。最后，我们基于未纠正的离散哈密顿动力学和确定性动量恢复开发了 MixFlows 的实现。模拟和实际数据实验表明，MixFlows可以提供比几种黑盒归一化流更可靠的后验逼近，以及与最先进的MCMC方法所获得的样本质量相当的样本。

    This work presents mixed variational flows (MixFlows), a new variational family that consists of a mixture of repeated applications of a map to an initial reference distribution. First, we provide efficient algorithms for i.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then show that MixFlows have MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving, and provide bounds on the accumulation of error for practical implementations where the flow map is approximated. Finally, we develop an implementation of MixFlows based on uncorrected discretized Hamiltonian dynamics combined with deterministic momentum refreshment. Simulated and real data experiments show that MixFlows can provide more reliable posterior approximations than several black-box normalizing flows, as well as samples of comparable quality to those obtained from state-of-the-art MCMC methods.
    
[^78]: 超出等周性的稳健性律法

    A Law of Robustness beyond Isoperimetry. (arXiv:2202.11592v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11592](http://arxiv.org/abs/2202.11592)

    本文提出了一种两层次的稳健性律法，研究了在任意数据分布下的稳健插值问题，并证明其Lipschitz下界分别为$\Omega(\sqrt{n/p})$和$\Omega(n^{1/d})$。

    

    本文研究在一个有界空间上支持任意数据分布的稳健插值问题，并提出了一个两层次的稳健性律法。稳健插值是指通过Lipschitz函数插值$\mathbb{R}^d$中的$n$个嘈杂训练数据点。尽管当样本来自等周性分布时，已经很好地理解了这个问题，但在一般或最坏情况分布下，其性能仍然不清楚。我们证明了对于任意数据分布，插值神经网络的Lipschitz下界为$\Omega(\sqrt{n/p})$，其中$p$表示参数数量。通过这个结果，我们验证了Bubeck，Li和Nagaraj在二层神经网络中使用多项式权重提出的稳健性律法猜想。然后，我们将结果扩展到任意插值逼近器，并证明了稳健插值的Lipschitz下界为$\Omega(n^{1/d})$。我们的结果证明了一个两层次的稳健性律法：

    We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating $n$ noisy training data points in $\mathbb{R}^d$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetry distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound $\Omega(\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer neural networks with polynomial weights. We then extend our result to arbitrary interpolating approximators and prove a Lipschitzness lower bound $\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold law of robustness: i) w
    
[^79]: NN2Poly：用于深度前馈人工神经网络的多项式表示方法

    NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.11397](http://arxiv.org/abs/2112.11397)

    本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。

    

    尽管深度学习应用非常成功，但神经网络的可解释性和理论行为仍然是一个开放的研究领域。本文提出NN2Poly：一种理论方法，用于获取一个显式多项式模型，以提供已经训练好的全连接前馈人工神经网络（多层感知器或MLP）的精确表示。这种方法扩展了文献中提出的先前想法，该想法仅限于单隐藏层的网络，并且适用于回归和分类任务的任意深度MLP。本文的目标是通过在每层上使用激活函数的泰勒展开式，然后使用几个组合性质来计算所需多项式的系数，从而实现此目标。作者讨论了此方法的主要计算挑战以及通过引入一些逼近来克服这些挑战的方法，而不会影响其准确性。通过实验验证表明，尽管NN2Poly方法简单且计算成本低，但对于合成和真实数据集，提供非常准确的多项式逼近。

    Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
    
[^80]: 基于条件概率的深度神经网络在排序一致序回归中的应用

    Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08851](http://arxiv.org/abs/2111.08851)

    本文提出新的一种排除限制的排序一致性序回归方法，基于深度神经网络和条件概率的建模，可以在保持输出概率排序一致性的同时，提高了模型的表现。

    

    近年来，深度神经网络在各种分类和模式识别任务中取得了出色的预测性能。然而，许多实际的预测问题具有序响应变量，并且传统的分类损失函数忽略了这种排序信息。基于深度神经网络的序回归方法解决了这个问题。本文提出了一种新的方法，该方法基于建立每个序分类的条件概率的深度神经结构，从而保证模型的输出概率在预测标签类别中保持其排序一致性。

    In recent times, deep neural networks achieved outstanding predictive performance on various classification and pattern recognition tasks. However, many real-world prediction problems have ordinal response variables, and this ordering information is ignored by conventional classification losses such as the multi-category cross-entropy. Ordinal regression methods for deep neural networks address this. One such method is the CORAL method, which is based on an earlier binary label extension framework and achieves rank consistency among its output layer tasks by imposing a weight-sharing constraint. However, while earlier experiments showed that CORAL's rank consistency is beneficial for performance, it is limited by a weight-sharing constraint in a neural network's fully connected output layer, which may restrict the expressiveness and capacity of a network trained using CORAL. We propose a new method for rank-consistent ordinal regression without this limitation. Our rank-consistent ordi
    
[^81]: 评级迁移预测：一种滤波方法

    Rating transitions forecasting: a filtering approach. (arXiv:2109.10567v4 [math.PR] UPDATED)

    [http://arxiv.org/abs/2109.10567](http://arxiv.org/abs/2109.10567)

    本文提出了一种基于滤波算法的评级迁移预测方法，通过点过程滤波框架高效地推断出评级迁移的隐藏因素，并根据商业周期进行预测，从而实时揭示和检测影响评级迁移动态的经济变化。

    

    分析商业周期对评级迁移的影响是近十五年来引起极大关注的研究课题，特别是在监管机构进行压力测试的情况下。本文认为评级迁移的动态由未观察到的潜在因素控制。在一个点过程滤波框架下，我们解释了如何从评级历史观测中高效地推断出隐藏因素的当前状态。然后我们将经典的 Baum-Welsh 算法调整到了我们的语境中，并说明了如何估计隐藏因素的参数。一旦校准完成，我们就可以实时地揭示和检测影响评级迁移动态的经济变化，并使用无需使用任何外部协变量的滤波公式预测未来的转换概率。我们提出了两种滤波框架：离散版本和连续版本。我们进行了演示和比较。

    Analyzing the effect of business cycle on rating transitions has been a subject of great interest these last fifteen years, particularly due to the increasing pressure coming from regulators for stress testing. In this paper, we consider that the dynamics of rating migrations is governed by an unobserved latent factor. Under a point process filtering framework, we explain how the current state of the hidden factor can be efficiently inferred from observations of rating histories. We then adapt the classical Baum-Welsh algorithm to our setting and show how to estimate the latent factor parameters. Once calibrated, we may reveal and detect economic changes affecting the dynamics of rating migration, in real-time. To this end we adapt a filtering formula which can then be used for predicting future transition probabilities according to economic regimes without using any external covariates. We propose two filtering frameworks: a discrete and a continuous version. We demonstrate and compar
    
[^82]: 关于倾斜损失的机器学习理论与应用研究

    On Tilted Losses in Machine Learning: Theory and Applications. (arXiv:2109.06141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.06141](http://arxiv.org/abs/2109.06141)

    本文研究了一种在机器学习中不常见但在统计、概率与优化等领域常用的技术——指数倾斜，并将其应用到风险最小化中。所提出的方法可以修正单个损失的影响，增加或减少异常值的作用，可以提高泛化性能，并可以被视为损失的尾部概率的平滑近似。

    

    指数倾斜是统计学、概率论、信息论和优化等领域常用的一种技术，用于创建参数化分布转移。尽管在相关领域中非常常见，但倾斜在机器学习中的应用尚不普及。本文旨在通过研究倾斜在风险最小化中的应用来填补这一空白。我们研究了验算风险最小化（TERM）的简单扩展，它使用指数倾斜来灵活调整个别损失的影响。所得到的框架具有多种有用的性质：我们展示了TERM可以分别增加或减少异常值的影响，从而实现公平或鲁棒性。TERM也具有降低方差从而提高泛化性能的优势，并且可以被看作是对损失的尾部概率的平滑近似。我们的工作在TERM和其他相关目标之间建立了严格的联系，例如风险价值、条件风险价值和分布鲁棒优化等。

    Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust op
    
[^83]: 响应式决策主体下的模型可迁移性研究

    Model Transferability With Responsive Decision Subjects. (arXiv:2107.05911v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.05911](http://arxiv.org/abs/2107.05911)

    本论文研究在响应式和交互式数据分布下，算法预测器的可迁移性问题，提供了性能差距的上界和分类器必须满足的权衡的下界，并刻画了预测器、策略空间选择和偏移性质对可迁移性程度的影响。

    

    在人类智能决策参与的一些数据集上，如果存在一个准确的算法预测器，那么它在这些数据集上的准确率是否会被保持呢？在我们的背景设置中，一个代理人或用户对应于从分布$D$中抽样得到的一个样本$(X,Y)$，并面临着模型$h$及其分类结果$h(X)$。代理人可以修改$X$以适应$h$，这将对$(X,Y)$产生分布偏移。我们的设置是受到了机器学习模型被人类代理人使用并最终面对响应式和交互式数据分布的应用案例的启发。我们通过研究模型在可用源分布（数据）上训练的性能如何转化为其诱导域中性能来系统化地讨论模型的可迁移性。我们提供了由于诱导域偏移而导致的性能差距的上界，以及分类器必须满足的权衡的下界。我们的理论刻画了预测器、策略空间选择和偏移性质对可迁移性程度的影响。

    Given an algorithmic predictor that is accurate on some source population consisting of strategic human decision subjects, will it remain accurate if the population respond to it? In our setting, an agent or a user corresponds to a sample $(X,Y)$ drawn from a distribution $\cal{D}$ and will face a model $h$ and its classification result $h(X)$. Agents can modify $X$ to adapt to $h$, which will incur a distribution shift on $(X,Y)$. Our formulation is motivated by applications where the deployed machine learning models are subjected to human agents, and will ultimately face responsive and interactive data distributions. We formalize the discussions of the transferability of a model by studying how the performance of the model trained on the available source distribution (data) would translate to the performance on its induced domain. We provide both upper bounds for the performance gap due to the induced domain shift, as well as lower bounds for the trade-offs that a classifier has to s
    
[^84]: UCB Bandits在对抗攻击中的近乎最优攻击策略

    Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.09312](http://arxiv.org/abs/2008.09312)

    本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。

    

    本文考虑了一种随机多臂赌博问题，其中奖励受到对抗性破坏。我们提出了一种新颖的攻击策略，通过操作UCB原则来拉动一些非最优目标臂$T-o(T)$次，累积成本的标度为$\sqrt{\log T}$，其中$T$为回合数。我们还证明了累积攻击成本的第一个下界。我们的下界与我们的上界匹配，除了$\log\log T$因子，表明我们的攻击策略近乎是最优的。

    We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
    
[^85]: 用图神经网络进行图聚类

    Graph Clustering with Graph Neural Networks. (arXiv:2006.16904v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.16904](http://arxiv.org/abs/2006.16904)

    本文探讨了图神经网络在图聚类这一无监督问题上的缺陷，并提出了一种名为DMoN的无监督汇聚方法，可以有效地解决聚类恢复问题。

    

    图神经网络已经在诸如节点分类和链路预测之类的许多图分析任务中取得了最先进的结果。然而，在图上的一些重要无监督问题，比如图聚类，却对GNN的进展更加有抵抗力。我们通过精心设计一组实验，研究了不同信噪比情况下的表征数据对于图结构的影响。为了解决这些方法在聚类方面的性能问题，我们引入了一种无监督汇聚方法：深度模块网络(DMoN)。这种方法受聚类质量的模块度度量的启发，能够处理聚类恢复问题。

    Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. Graph clustering has the same overall goal as node pooling in GNNs - does this mean that GNN pooling methods do a good job at clustering graphs?  Surprisingly, the answer is no - current GNN pooling methods often fail to recover the cluster structure in cases where simple baselines, such as k-means applied on learned representations, work well. We investigate further by carefully designing a set of experiments to study different signal-to-noise scenarios both in graph structure and attribute data. To address these methods' poor performance in clustering, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery
    

