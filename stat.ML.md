# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On Consistency of Signatures Using Lasso.](http://arxiv.org/abs/2305.10413) | 本文重新审视了Lasso回归对于签名变换的一致性问题，并发现对于不同的过程和时间序列，选择适当的签名定义和随机模型可以提高Lasso回归的一致性。 |
| [^2] | [Optimality of Message-Passing Architectures for Sparse Graphs.](http://arxiv.org/abs/2305.10391) | 本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。 |
| [^3] | [Active Learning in Symbolic Regression Performance with Physical Constraints.](http://arxiv.org/abs/2305.10379) | 本文探讨了利用进化符号回归作为主动学习中的方法来提出哪些数据应该被采集，通过“委员会查询”来减少所需数据，并在重新发现已知方程所需的数据方面实现最新的结果。 |
| [^4] | [Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning.](http://arxiv.org/abs/2305.10282) | 本文提出一种新的三阶段混合RL算法，不需要奖励信息，有效地利用在线和离线数据，从而实现细调以获得更好的结果。 |
| [^5] | [Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions.](http://arxiv.org/abs/2305.10227) | 我们提出了第一个多项式时间算法，在小的破坏节点比例下即可达到Kesten-Stigum临界点的弱恢复。 |
| [^6] | [Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection.](http://arxiv.org/abs/2305.10219) | 该文通过分析数据的可分性和离散度，提出了一种基于S&S比的有效SVM正则化参数、核函数和核参数选择方法，表现较传统方法更优。 |
| [^7] | [Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing.](http://arxiv.org/abs/2305.10187) | 该论文提出了一种评估代步共享平台中动态条件分位治疗效果的框架，通过个体CQTE之和来简化动态CQTE的评估。 |
| [^8] | [Algorithms for Boolean Matrix Factorization using Integer Programming.](http://arxiv.org/abs/2305.10185) | 本文提出了一种基于整数规划的交替优化策略，解决了布尔矩阵分解的NP难题，并且使用另一个整数规划将多个解组合成最优解，实验表明算法在中等规模问题上优于现有技术。 |
| [^9] | [A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling.](http://arxiv.org/abs/2305.10158) | 本文提出了一种名为TwinGP的新的全局-局部近似框架，使用子集数据方法，并将相关函数建模为全局和局部核的组合。TwinGP在计算成本的一小部分下表现与最先进的GP建模方法相当或更好。 |
| [^10] | [Automatic Hyperparameter Tuning in Sparse Matrix Factorization.](http://arxiv.org/abs/2305.10114) | 稀疏矩阵分解中使用贝叶斯框架，提出了一种通过评估稀疏矩阵先验中归一化因子的零点来进行超参数调整的新型数值方法，并在地面真实稀疏矩阵重建中表现出优异性能。 |
| [^11] | [A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.10089) | 本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。 |
| [^12] | [Optimal Weighted Random Forests.](http://arxiv.org/abs/2305.10042) | 本论文提出了针对RF算法的1步和2步最优加权随机森林算法来处理预测性能差异问题，证明渐近最优，并进行了数据研究。 |
| [^13] | [Utility Theory of Synthetic Data Generation.](http://arxiv.org/abs/2305.10015) | 本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用，效用指标的分析界限揭示了指标收敛的关键条件，令人惊讶的是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，效用指标会收敛。 |
| [^14] | [Deep quantum neural networks form Gaussian processes.](http://arxiv.org/abs/2305.09957) | 本文证明了基于Haar随机酉或正交深量子神经网络的某些模型的输出会收敛于高斯过程。然而，这种高斯过程不能用于通过贝叶斯统计学来有效预测QNN的输出。 |
| [^15] | [Model-based Validation as Probabilistic Inference.](http://arxiv.org/abs/2305.09930) | 本文提出了模型验证作为概率推断的方法，在自主系统中估计故障分布。该方法使用基于模型的方法表示故障轨迹分布，并利用自动微分计算轨迹梯度。在多个场景下进行了演示，结果表明在样本效率和参数空间覆盖范围方面取得了改善。 |
| [^16] | [A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction.](http://arxiv.org/abs/2305.09869) | 本文提出了一种名为SELO的链路符号预测模型，使用子图编码方法学习有向网络中的边嵌入。通过引入有符号子图编码方法，并使用线性优化方法将每个子图嵌入到似然矩阵中而非邻接矩阵中，该模型优于其他最先进的方法。 |
| [^17] | [Mimetic Initialization of Self-Attention Layers.](http://arxiv.org/abs/2305.09828) | 本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。 |
| [^18] | [Outage Performance and Novel Loss Function for an ML-Assisted Resource Allocation: An Exact Analytical Framework.](http://arxiv.org/abs/2305.09739) | 本文关注在无法获得未来信道状态信息时，利用机器学习解决不同资源分配对应的失误概率问题。提出了一种新型理论最优、可解释的损失函数，经过模拟验证其在失误概率、学习速度和收敛等方面的表现优于一些常见的损失函数。 |
| [^19] | [Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free.](http://arxiv.org/abs/2305.09668) | 本文研究了在不同隐私要求下的均值估计问题，提出的算法在两组具有不同隐私级别的用户时是极小化的最优的，并揭示了一个有趣的饱和现象。 |
| [^20] | [Orthogonal polynomial approximation and Extended Dynamic Mode Decomposition in chaos.](http://arxiv.org/abs/2305.08074) | 本文在简单的混沌映射上证明了扩展动态模态分解（EDMD）对于多项式可观测字典有指数效率，从而有效处理了混沌动力学中的正则函数问题，并展示了在这种情况下使用EDMD产生的预测和Koopman谱数据收敛至物理上有意义的极限。 |
| [^21] | [A proof of convergence of inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.06137) | 本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。 |
| [^22] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^23] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^24] | [Sample Average Approximation for Black-Box VI.](http://arxiv.org/abs/2304.06803) | 该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。 |
| [^25] | [An inexact linearized proximal algorithm for a class of DC composite optimization problems and applications.](http://arxiv.org/abs/2303.16822) | 本文提出了一种解决非凸非光滑问题的非精确线性化近端算法，并应用于鲁棒分解中的两个问题，得到了有效的数值结果。 |
| [^26] | [Learning curves for deep structured Gaussian feature models.](http://arxiv.org/abs/2303.00564) | 该论文针对深度高斯模型的特征各向异性展开研究。研究结果表明，在第一层特征行之间允许存在相关性可以促进泛化，而后续层的结构通常是不利的。 |
| [^27] | [Gaussian processes at the Helm(holtz): A more fluid model for ocean currents.](http://arxiv.org/abs/2302.10364) | 该论文提出了一种更符合已知电流物理特性的模型，在通过Helmholtz分解获得的向量场的发散和无旋分量上使用高斯过程来预测海流。证明了这种方法在模拟数据和真实浮标数据方面都比之前的方法更有效。 |
| [^28] | [Domain Generalization by Functional Regression.](http://arxiv.org/abs/2302.04724) | 通过函数回归实现领域泛化，构建了线性算子将输入边缘分布与输出条件分布联系起来，并提出了一种基于源分布依赖性的再生核希尔伯特空间预测算法。 |
| [^29] | [Leveraging Demonstrations to Improve Online Learning: Quality Matters.](http://arxiv.org/abs/2302.03319) | 本篇论文探讨了离线演示数据如何改进在线学习的问题，提出了一种利用演示数据的TS算法，并给出了依赖于先验知识的贝叶斯遗憾界；研究发现，预训练可以大幅提高在线性能，改进程度随专家能力水平的提高而增加。 |
| [^30] | [Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs.](http://arxiv.org/abs/2302.02865) | 本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。 |
| [^31] | [Direct Uncertainty Quantification.](http://arxiv.org/abs/2302.02420) | 本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。 |
| [^32] | [Neural networks learn to magnify areas near decision boundaries.](http://arxiv.org/abs/2301.11375) | 神经网络训练能够放大决策边界附近的局部区域，改善整个系统的泛化能力。 |
| [^33] | [A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference.](http://arxiv.org/abs/2212.12393) | 本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。 |
| [^34] | [Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control.](http://arxiv.org/abs/2205.11956) | 本文提出了一种基于雅可比控制的带宽选择启发式方法，该方法具有闭式、计算非常轻的特点，并且在关注带宽的同时可以获得更好的模型泛化性能。 |
| [^35] | [Asymptotics of Network Embeddings Learned via Subsampling.](http://arxiv.org/abs/2107.02363) | 本研究将网络嵌入方法封装为一个统一框架，并从理论上证明了使用子采样学习的网络嵌入的渐近分布，同时提供了潜在参数的收敛速率和算法选择与统计效率之间的权衡。 |
| [^36] | [Stratified Learning: A General-Purpose Statistical Method for Improved Learning under Covariate Shift.](http://arxiv.org/abs/2106.11211) | 该论文提出了一种用于处理训练集不具代表性的协变量漂移情况下改进监督式学习的分层学习方法，并在宇宙学领域的两个问题中证明了其有效性，大幅提升了目标预测结果。 |
| [^37] | [Selecting the Number of Clusters $K$ with a Stability Trade-off: an Internal Validation Criterion.](http://arxiv.org/abs/2006.08530) | 提出了一种新的聚类验证标准，基于聚类稳定性的内部验证原则，在聚类稳定性和聚类质量方面胜过现有的方法。 |
| [^38] | [Solving for multi-class using orthogonal coding matrices.](http://arxiv.org/abs/1801.09055) | 本文研究了使用正交编码矩阵进行多类分类问题的实现方法。实验结果表明，代码中不包含零元素的正交编码矩阵可以通过简单的方法解决概率问题，同时比随机编码更准确。然而与1对1相比，正交编码的准确性仍有待提高。 |

# 详细

[^1]: 使用Lasso的签名一致性研究

    On Consistency of Signatures Using Lasso. (arXiv:2305.10413v1 [stat.ML])

    [http://arxiv.org/abs/2305.10413](http://arxiv.org/abs/2305.10413)

    本文重新审视了Lasso回归对于签名变换的一致性问题，并发现对于不同的过程和时间序列，选择适当的签名定义和随机模型可以提高Lasso回归的一致性。

    

    签名变换是连续和离散时间序列数据的迭代路径积分，它们的普遍非线性通过线性化特征选择问题。本文在理论和数值上重新审视了Lasso回归对于签名变换的一致性问题。我们的研究表明，对于更接近布朗运动或具有较弱跨维度相关性的过程和时间序列，签名定义为It\^o积分的Lasso回归更具一致性；对于均值回归过程和时间序列，其签名定义为Stratonovich积分在Lasso回归中具有更高的一致性。我们的发现强调了在统计推断和机器学习中选择适当的签名和随机模型的重要性。

    Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits the consistency issue of Lasso regression for the signature transform, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\^o integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning.
    
[^2]: 稀疏图的消息传递架构的最优性

    Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])

    [http://arxiv.org/abs/2305.10391](http://arxiv.org/abs/2305.10391)

    本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。

    

    我们研究了特征装饰图上的节点分类问题，在稀疏设置下，即节点的预期度数为节点数的O(1)时。这样的图通常被称为本地树状图。我们引入了一种叫做渐近本地贝叶斯最优性的节点分类任务的贝叶斯最优性概念，并根据这个标准计算了具有任意节点特征和边连接分布的相当一般的统计数据模型的最优分类器。该最优分类器可以使用消息传递图神经网络架构实现。然后我们计算了该分类器的泛化误差，并在一个已经研究充分的统计模型上从理论上与现有的学习方法进行比较。我们发现，在低图信号的情况下，最佳消息传递架构插值于标准MLP和一种典型的c架构之间。

    We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
    
[^3]: 基于物理约束的符号回归中主动学习的表现

    Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])

    [http://arxiv.org/abs/2305.10379](http://arxiv.org/abs/2305.10379)

    本文探讨了利用进化符号回归作为主动学习中的方法来提出哪些数据应该被采集，通过“委员会查询”来减少所需数据，并在重新发现已知方程所需的数据方面实现最新的结果。

    

    进化符号回归（SR）是一种将符号方程拟合到数据中的方法，可以得到简洁易懂的模型。本文探讨使用SR作为主动学习中的方法来提出哪些数据应该被采集，在此过程中考虑物理约束。基于主动学习的SR通过“委员会查询”来提出下一步实验。物理约束可以在非常低的数据情况下改善所建议的方程。这些方法可以减少SR所需的数据，并在重新发现已知方程所需的数据方面实现最新的结果。

    Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
    
[^4]: “无任何奖励信息的细调 Fine-tuning:基于混合增强学习的可证明统计优势”

    Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning. (arXiv:2305.10282v1 [cs.LG])

    [http://arxiv.org/abs/2305.10282](http://arxiv.org/abs/2305.10282)

    本文提出一种新的三阶段混合RL算法，不需要奖励信息，有效地利用在线和离线数据，从而实现细调以获得更好的结果。

    

    本论文研究了在混合环境中进行表格强化学习(RL)，该环境假设可以访问离线数据集并在未知环境中进行在线交互。其中一个核心问题在于如何利用在线数据收集来加强和补充离线数据集，从而实现有效的策略细调。本文借鉴了最近的无奖励探索和基于模型的离线RL 的进展，设计了一个三阶段的混合RL算法，其在样本复杂度方面优于仅使用离线RL 和仅使用在线RL 的最佳结果。所提出的算法在数据收集过程中不需要任何奖励信息。我们的理论是基于一个新概念——单策略局部集中性的，该概念捕捉了分布不匹配和覆盖错误之间的权衡，并指导离线和在线数据之间的相互作用。

    This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data collection to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and model-based offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called single-policy partial concentrability, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.
    
[^5]: 在节点破坏下的随机块模型达到Kesten-Stigum临界点

    Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions. (arXiv:2305.10227v1 [cs.SI])

    [http://arxiv.org/abs/2305.10227](http://arxiv.org/abs/2305.10227)

    我们提出了第一个多项式时间算法，在小的破坏节点比例下即可达到Kesten-Stigum临界点的弱恢复。

    

    我们研究了在节点破坏随机块模型下的强健社区检测，其中对于$n$个顶点中的一个部分具有任意修改边缘权重的敌手。我们提出了第一个多项式时间算法，在小的破坏节点比例下即可达到Kesten-Stigum临界点的弱恢复。在这项工作之前，即使是最先进的强健性算法，在接近Kesten-Stigum临界点时也会被节点破坏敌手攻破。我们进一步将我们的技术扩展到$Z_2$同步问题，其中我们的算法在存在类似的强破坏敌手波动时也能达到最优恢复阈值。我们算法的关键因素是一种新型的可识别性证明，利用主子矩阵的Grothendieck范数推出的推出　作用。

    We study robust community detection in the context of node-corrupted stochastic block model, where an adversary can arbitrarily modify all the edges incident to a fraction of the $n$ vertices. We present the first polynomial-time algorithm that achieves weak recovery at the Kesten-Stigum threshold even in the presence of a small constant fraction of corrupted nodes. Prior to this work, even state-of-the-art robust algorithms were known to break under such node corruption adversaries, when close to the Kesten-Stigum threshold.  We further extend our techniques to the $Z_2$ synchronization problem, where our algorithm reaches the optimal recovery threshold in the presence of similar strong adversarial perturbations.  The key ingredient of our algorithm is a novel identifiability proof that leverages the push-out effect of the Grothendieck norm of principal submatrices.
    
[^6]: 基于可分性和离散度比的SVM正则化参数、核函数和核参数选择方法

    Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection. (arXiv:2305.10219v1 [stat.ML])

    [http://arxiv.org/abs/2305.10219](http://arxiv.org/abs/2305.10219)

    该文通过分析数据的可分性和离散度，提出了一种基于S&S比的有效SVM正则化参数、核函数和核参数选择方法，表现较传统方法更优。

    

    支持向量机（SVM）是一种具有广泛应用的鲁棒机器学习算法，可用于分类、回归和异常值检测。SVM需要调整正则化参数（RP）来控制模型容量和泛化性能。传统上，通过交叉验证（CV）过程对一系列备选RP进行比较以找到最佳RP。此外，对于非线性可分数据，SVM使用核函数，在核函数的网格中选择一组具有一组参数的核函数。RP和核网格的最佳选择是通过CV的网格搜索获得的。通过随机分析正则化参数的行为，本文展示了SVM性能可以建模为数据的可分性和离散度（S&S）的函数。可分性是类别之间距离的度量，离散度是数据点的传播比率。特别地，对于铰链损失成本函数，S&S比可以有效地估计最优RP。此外，本文提出了一种基于S&S比的高效选择核函数及其参数方法。在各种基准数据集上比较了所提出方法与传统方法的性能，结果表明，所提出方法具有更少的需要调整的超参数且性能优异或可比。

    Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning the regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels where a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through the grid-search of CV. By stochastically analyzing the behavior of the regularization parameter, this work shows that the SVM performance can be modeled as a function of separability and scatteredness (S&S) of the data. Separability is a measure of the distance between classes, and scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost fun
    
[^7]: 评估具有应用于代步共享的动态条件分位治疗效果

    Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing. (arXiv:2305.10187v1 [stat.ME])

    [http://arxiv.org/abs/2305.10187](http://arxiv.org/abs/2305.10187)

    该论文提出了一种评估代步共享平台中动态条件分位治疗效果的框架，通过个体CQTE之和来简化动态CQTE的评估。

    

    许多现代科技公司，如Google、Uber和Didi，利用在线实验（也称为A / B测试）评估新政策与现有政策之间的差异。虽然大多数研究集中在平均治疗效应上，但偏斜和重尾的结果分布情况可能会受益于其他标准，例如分位数。然而，在处理来自代步共享平台的数据时，评估动态分位治疗效果（QTE）仍然是一个挑战，因为涉及跨时间和空间的顺序决策。在本文中，我们建立了一个正式框架来计算在治疗独立于特征的条件下的QTE。在特定的模型假设下，我们证明了动态条件QTE（CQTE）等于时间上的个体CQTE之和，尽管累积奖励的条件分位数不一定等于个体奖励的条件分位数之和。这一关键洞察力显著简化了动态CQTE的评估。

    Many modern tech companies, such as Google, Uber, and Didi, utilize online experiments (also known as A/B testing) to evaluate new policies against existing ones. While most studies concentrate on average treatment effects, situations with skewed and heavy-tailed outcome distributions may benefit from alternative criteria, such as quantiles. However, assessing dynamic quantile treatment effects (QTE) remains a challenge, particularly when dealing with data from ride-sourcing platforms that involve sequential decision-making across time and space. In this paper, we establish a formal framework to calculate QTE conditional on characteristics independent of the treatment. Under specific model assumptions, we demonstrate that the dynamic conditional QTE (CQTE) equals the sum of individual CQTEs across time, even though the conditional quantile of cumulative rewards may not necessarily equate to the sum of conditional quantiles of individual rewards. This crucial insight significantly strea
    
[^8]: 布尔矩阵分解的整数规划算法。

    Algorithms for Boolean Matrix Factorization using Integer Programming. (arXiv:2305.10185v1 [math.OC])

    [http://arxiv.org/abs/2305.10185](http://arxiv.org/abs/2305.10185)

    本文提出了一种基于整数规划的交替优化策略，解决了布尔矩阵分解的NP难题，并且使用另一个整数规划将多个解组合成最优解，实验表明算法在中等规模问题上优于现有技术。

    

    布尔矩阵分解（BMF）将一个给定的二进制输入矩阵近似表示为两个更小的二进制因子的乘积。相对于使用标准算术的二进制矩阵分解，BMF使用布尔OR和布尔AND操作进行矩阵乘积运算，从而导致更低的重构误差。BMF是一个NP难题。在本文中，我们首先提出了一种交替优化（AO）策略，使用整数规划（IP）解决BMF中一个因子矩阵的子问题。我们还提供了两种初始化因子的方法。然后，我们展示了如何使用另一个IP将BMF的多个解组合到最优解。这使我们能够提出一种新算法：使用AO生成多个解，然后将它们以最优的方式组合起来。实验表明，我们的算法（可在GitLab上获得）在中等规模问题上优于现有技术的状态。

    Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. As opposed to binary matrix factorization which uses standard arithmetic, BMF uses the Boolean OR and Boolean AND operations to perform matrix products, which leads to lower reconstruction errors. BMF is an NP-hard problem. In this paper, we first propose an alternating optimization (AO) strategy that solves the subproblem in one factor matrix in BMF using an integer program (IP). We also provide two ways to initialize the factors within AO. Then, we show how several solutions of BMF can be combined optimally using another IP. This allows us to come up with a new algorithm: it generates several solutions using AO and then combines them in an optimal way. Experiments show that our algorithms (available on gitlab) outperform the state of the art on medium-scale problems.
    
[^9]: 一种用于大规模高斯过程建模的全局-局部近似框架

    A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling. (arXiv:2305.10158v1 [stat.ML])

    [http://arxiv.org/abs/2305.10158](http://arxiv.org/abs/2305.10158)

    本文提出了一种名为TwinGP的新的全局-局部近似框架，使用子集数据方法，并将相关函数建模为全局和局部核的组合。TwinGP在计算成本的一小部分下表现与最先进的GP建模方法相当或更好。

    

    本文提出了一种新的大规模高斯过程（GP）建模框架。与文献中提出的解决精确GP建模的全局和局部近似相反，我们在构建近似时采用了全局和局部相结合的方法。我们的框架使用子集数据方法，其中子集是一个旨在捕捉数据全局趋势的全局点集的并集，以及一个旨在捕捉给定测试位置周围局部趋势的局部点集。相关函数也被建模为全局和局部核的组合。我们的框架性能，即TwinGP，在计算成本的一小部分下与最先进的GP建模方法相当或更好。

    In this work, we propose a novel framework for large-scale Gaussian process (GP) modeling. Contrary to the global, and local approximations proposed in the literature to address the computational bottleneck with exact GP modeling, we employ a combined global-local approach in building the approximation. Our framework uses a subset-of-data approach where the subset is a union of a set of global points designed to capture the global trend in the data, and a set of local points specific to a given testing location to capture the local trend around the testing location. The correlation function is also modeled as a combination of a global, and a local kernel. The performance of our framework, which we refer to as TwinGP, is on par or better than the state-of-the-art GP modeling methods at a fraction of their computational cost.
    
[^10]: 稀疏矩阵分解中的自动超参数调整

    Automatic Hyperparameter Tuning in Sparse Matrix Factorization. (arXiv:2305.10114v1 [stat.ML])

    [http://arxiv.org/abs/2305.10114](http://arxiv.org/abs/2305.10114)

    稀疏矩阵分解中使用贝叶斯框架，提出了一种通过评估稀疏矩阵先验中归一化因子的零点来进行超参数调整的新型数值方法，并在地面真实稀疏矩阵重建中表现出优异性能。

    

    我们研究了贝叶斯框架下稀疏矩阵分解中的超参数调整问题。在先前的工作中，基于几个近似，通过变分贝叶斯方法得到了具有拉普拉斯先验的稀疏矩阵分解的分析解。基于此解，我们提出了一种通过评估稀疏矩阵先验中归一化因子的零点来进行超参数调整的新型数值方法。我们还通过与广泛使用的稀疏主成分分析算法进行比较，验证了我们的方法在地面真实稀疏矩阵重建中表现出的优异性能。

    We study the problem of hyperparameter tuning in sparse matrix factorization under Bayesian framework. In the prior work, an analytical solution of sparse matrix factorization with Laplace prior was obtained by variational Bayes method under several approximations. Based on this solution, we propose a novel numerical method of hyperparameter tuning by evaluating the zero point of normalization factor in sparse matrix prior. We also verify that our method shows excellent performance for ground-truth sparse matrix reconstruction by comparing it with the widely-used algorithm of sparse principal component analysis.
    
[^11]: 一种多目标优化的Wasserstein反向强化学习模型的证明

    A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])

    [http://arxiv.org/abs/2305.10089](http://arxiv.org/abs/2305.10089)

    本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。

    

    本文证明了Wasserstein反向强化学习模型可以在有限次迭代中让学习者的奖励值模仿专家的奖励值，并证明了在词典序的多目标优化中，Wasserstein反向强化学习模型可以让学习者的最优解模仿专家的最优解。

    We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
    
[^12]: 最优加权随机森林

    Optimal Weighted Random Forests. (arXiv:2305.10042v1 [stat.ML])

    [http://arxiv.org/abs/2305.10042](http://arxiv.org/abs/2305.10042)

    本论文提出了针对RF算法的1步和2步最优加权随机森林算法来处理预测性能差异问题，证明渐近最优，并进行了数据研究。

    

    该论文介绍了一种针对随机森林 (RF) 算法的优化加权方法，针对回归问题，提出了一步最优加权随机森林 (1step-WRF$_\mathrm{opt}$) 和两步最优加权随机森林 (2steps-WRF$_\mathrm{opt}$)，通过加权选择准则来组合基本学习器的预测结果。作者证明了这些算法是渐近最优的，即得到的平方损失和风险与不可行但最佳模型平均估计量的相对差异渐近等同。最后，作者使用数据研究了算法的性能表现。

    The random forest (RF) algorithm has become a very popular prediction method for its great flexibility and promising accuracy. In RF, it is conventional to put equal weights on all the base learners (trees) to aggregate their predictions. However, the predictive performances of different trees within the forest can be very different due to the randomization of the embedded bootstrap sampling and feature selection. In this paper, we focus on RF for regression and propose two optimal weighting algorithms, namely the 1 Step Optimal Weighted RF (1step-WRF$_\mathrm{opt}$) and 2 Steps Optimal Weighted RF (2steps-WRF$_\mathrm{opt}$), that combine the base learners through the weights determined by weight choice criteria. Under some regularity conditions, we show that these algorithms are asymptotically optimal in the sense that the resulting squared loss and risk are asymptotically identical to those of the infeasible but best possible model averaging estimator. Numerical studies conducted on
    
[^13]: 合成数据生成的效用理论

    Utility Theory of Synthetic Data Generation. (arXiv:2305.10015v1 [stat.ML])

    [http://arxiv.org/abs/2305.10015](http://arxiv.org/abs/2305.10015)

    本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用，效用指标的分析界限揭示了指标收敛的关键条件，令人惊讶的是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，效用指标会收敛。

    

    评估合成数据的效用对于衡量合成算法的有效性和效率至关重要。现有的结果侧重于对合成数据效用的经验评估，而针对合成数据算法如何影响效用的理论理解仍然未被充分探索。本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用。该指标定义为在合成和原始数据集上训练的模型之间泛化的绝对差异。我们建立了该效用指标的分析界限来研究指标收敛的关键条件。一个有趣的结果是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，则该效用指标会收敛。另一个重要的效用指标基于合成和原始数据之间潜在的因果机制一致性。该理论使用几种合成算法进行说明，并分析了它们的效用属性。

    Evaluating the utility of synthetic data is critical for measuring the effectiveness and efficiency of synthetic algorithms. Existing results focus on empirical evaluations of the utility of synthetic data, whereas the theoretical understanding of how utility is affected by synthetic data algorithms remains largely unexplored. This paper establishes utility theory from a statistical perspective, aiming to quantitatively assess the utility of synthetic algorithms based on a general metric. The metric is defined as the absolute difference in generalization between models trained on synthetic and original datasets. We establish analytical bounds for this utility metric to investigate critical conditions for the metric to converge. An intriguing result is that the synthetic feature distribution is not necessarily identical to the original one for the convergence of the utility metric as long as the model specification in downstream learning tasks is correct. Another important utility metri
    
[^14]: 深度量子神经网络对应高斯过程

    Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v1 [quant-ph])

    [http://arxiv.org/abs/2305.09957](http://arxiv.org/abs/2305.09957)

    本文证明了基于Haar随机酉或正交深量子神经网络的某些模型的输出会收敛于高斯过程。然而，这种高斯过程不能用于通过贝叶斯统计学来有效预测QNN的输出。

    

    众所周知，从独立同分布的先验条件开始初始化的人工神经网络在隐藏层神经元数目足够大的极限下收敛到高斯过程。本文证明了量子神经网络（QNNs）也存在类似的结果。特别地，我们证明了基于Haar随机酉或正交深QNNs的某些模型的输出在希尔伯特空间维度$d$足够大时会收敛于高斯过程。由于输入状态、测量的可观测量以及酉矩阵的元素不独立等因素的作用，本文对这一结果的推导比经典情形更加微妙。我们分析的一个重要后果是，这个结果得到的高斯过程不能通过贝叶斯统计学来有效地预测QNN的输出。此外，我们的定理表明，Haar随机QNNs中的测量现象比以前认为的要更严重，我们证明了演员的集中现象。

    It is well known that artificial neural networks initialized from independent and identically distributed priors converge to Gaussian processes in the limit of large number of neurons per hidden layer. In this work we prove an analogous result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of certain models based on Haar random unitary or orthogonal deep QNNs converge to Gaussian processes in the limit of large Hilbert space dimension $d$. The derivation of this result is more nuanced than in the classical case due the role played by the input states, the measurement observable, and the fact that the entries of unitary matrices are not independent. An important consequence of our analysis is that the ensuing Gaussian processes cannot be used to efficiently predict the outputs of the QNN via Bayesian statistics. Furthermore, our theorems imply that the concentration of measure phenomenon in Haar random QNNs is much worse than previously thought, as we prove that ex
    
[^15]: 模型验证作为概率推断的方法

    Model-based Validation as Probabilistic Inference. (arXiv:2305.09930v1 [cs.RO])

    [http://arxiv.org/abs/2305.09930](http://arxiv.org/abs/2305.09930)

    本文提出了模型验证作为概率推断的方法，在自主系统中估计故障分布。该方法使用基于模型的方法表示故障轨迹分布，并利用自动微分计算轨迹梯度。在多个场景下进行了演示，结果表明在样本效率和参数空间覆盖范围方面取得了改善。

    

    评估失败分布是验证自主系统的关键步骤。现有的方法侧重于寻找一小范围初始条件下的故障或对测试系统的属性做出限制性假设。我们将顺序系统的故障轨迹分布估计视为贝叶斯推断问题，并采用基于模型的方法利用系统动态的模拟结果表示故障轨迹的分布。在计算轨迹梯度时采用自动微分。我们在倒立摆控制系统、自主驾驶汽车场景和部分可观测月球着陆器中演示了我们的方法。使用开箱即用的Hamiltonian Monte Carlo进行采样，同时使用多链以捕获多模态，并采用梯度平滑以实现安全轨迹。在所有实验中，与黑盒基线相比，我们观察到了样本效率和参数空间覆盖范围的改善。

    Estimating the distribution over failures is a key step in validating autonomous systems. Existing approaches focus on finding failures for a small range of initial conditions or make restrictive assumptions about the properties of the system under test. We frame estimating the distribution over failure trajectories for sequential systems as Bayesian inference. Our model-based approach represents the distribution over failure trajectories using rollouts of system dynamics and computes trajectory gradients using automatic differentiation. Our approach is demonstrated in an inverted pendulum control system, an autonomous vehicle driving scenario, and a partially observable lunar lander. Sampling is performed using an off-the-shelf implementation of Hamiltonian Monte Carlo with multiple chains to capture multimodality and gradient smoothing for safe trajectories. In all experiments, we observed improvements in sample efficiency and parameter space coverage compared to black-box baseline a
    
[^16]: 基于线性优化的有符号子图编码方法用于链路符号预测

    A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction. (arXiv:2305.09869v1 [cs.LG])

    [http://arxiv.org/abs/2305.09869](http://arxiv.org/abs/2305.09869)

    本文提出了一种名为SELO的链路符号预测模型，使用子图编码方法学习有向网络中的边嵌入。通过引入有符号子图编码方法，并使用线性优化方法将每个子图嵌入到似然矩阵中而非邻接矩阵中，该模型优于其他最先进的方法。

    

    本文研究了如何在有限的有符号数据中有效地推断链路的符号问题。我们提出了一个名为SELO的链路符号预测模型，该模型使用子图编码方法来学习有向网络中的边嵌入。特别地，我们通过线性优化方法引入了有符号子图编码方法，将每个子图嵌入到似然矩阵中而不是邻接矩阵中。在六个真实的有符号网络上进行了广泛的实验，并使用AUC、F1、micro-F1和Macro-F1作为评估指标。实验结果表明，所提出的SELO模型在所有四个评估指标上均优于其他最先进的方法。

    In this paper, we consider the problem of inferring the sign of a link based on limited sign data in signed networks. Regarding this link sign prediction problem, SDGNN (Signed Directed Graph Neural Networks) provides the best prediction performance currently to the best of our knowledge. In this paper, we propose a different link sign prediction architecture call SELO (Subgraph Encoding via Linear Optimization), which obtains overall leading prediction performances compared the state-of-the-art algorithm SDGNN. The proposed model utilizes a subgraph encoding approach to learn edge embeddings for signed directed networks. In particular, a signed subgraph encoding approach is introduced to embed each subgraph into a likelihood matrix instead of the adjacency matrix through a linear optimization method. Comprehensive experiments are conducted on six real-world signed networks with AUC, F1, micro-F1, and Macro-F1 as the evaluation metrics. The experiment results show that the proposed SEL
    
[^17]: 自注意力层的拟态初始化

    Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])

    [http://arxiv.org/abs/2305.09828](http://arxiv.org/abs/2305.09828)

    本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。

    

    在小规模数据集上训练Transformer十分困难。通常需要以大规模预训练模型作为起点。我们探索了这些预训练Transformer的权重（尤其是用于视觉任务），试图找到造成这种差异的原因。惊讶的是，我们发现仅仅通过初始化自注意力层的权重，使其“看起来”更像预训练模型，就能够更快且更高精度地训练普通Transformer，尤其是在像CIFAR-10和ImageNet分类这样的视觉任务上，我们的精度提高超过5％和4％。我们的初始化方案是闭式的、无需学习的、非常简单：我们将查询和键权重的乘积设置为近似于标识，将值和投影权重的乘积近似于负标识。由于这类似于我们在预训练Transformer中看到的模式，所以我们称为“拟态初始化”技术。

    It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".
    
[^18]: ML辅助资源分配的失误性能和新型损失函数: 一种精确分析框架

    Outage Performance and Novel Loss Function for an ML-Assisted Resource Allocation: An Exact Analytical Framework. (arXiv:2305.09739v1 [eess.SP])

    [http://arxiv.org/abs/2305.09739](http://arxiv.org/abs/2305.09739)

    本文关注在无法获得未来信道状态信息时，利用机器学习解决不同资源分配对应的失误概率问题。提出了一种新型理论最优、可解释的损失函数，经过模拟验证其在失误概率、学习速度和收敛等方面的表现优于一些常见的损失函数。

    

    机器学习是使6G及以上通信成为可能的重要工具。本文致力于将机器学习应用于解决这些系统普遍遇到的失误概率问题。具体来说，我们考虑单用户多资源贪婪分配策略，其中一个ML二分类预测器在选择充足资源时进行辅助。当预测器遇到相信会满意的资源时，它将其分配给用户。我们的主要结果是建立了该系统失误概率的精确和渐进表达式。在此基础上，我们制定了一种理论最优的、可微的损失函数来训练预测器。我们通过大量模拟比较使用此损失函数训练的预测器和使用其他常用损失函数训练的预测器之间的性能差异。我们还探讨了所提出的损失函数对预测器透明度和无线信道结构的影响。我们的数值结果表明，所提出的损失函数在失误概率、端到端学习速度和收敛方面优于现有的基于二元交叉熵的损失。此外，我们的损失函数制定赋予了可解释的预测器训练，能够稳健地处理信道的时变性。

    Machine Learning (ML) is a popular tool that will be pivotal in enabling 6G and beyond communications. This paper focuses on applying ML solutions to address outage probability issues commonly encountered in these systems. In particular, we consider a single-user multi-resource greedy allocation strategy, where an ML binary classification predictor assists in seizing an adequate resource. With no access to future channel state information, this predictor foresees each resource's likely future outage status. When the predictor encounters a resource it believes will be satisfactory, it allocates it to the user. Critically, the goal of the predictor is to ensure that a user avoids an unsatisfactory resource since this is likely to cause an outage. Our main result establishes exact and asymptotic expressions for this system's outage probability. With this, we formulate a theoretically optimal, differentiable loss function to train our predictor. We then compare predictors trained using thi
    
[^19]: 异构隐私下的均值估计: 部分隐私是可以免费的。

    Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free. (arXiv:2305.09668v1 [cs.CR])

    [http://arxiv.org/abs/2305.09668](http://arxiv.org/abs/2305.09668)

    本文研究了在不同隐私要求下的均值估计问题，提出的算法在两组具有不同隐私级别的用户时是极小化的最优的，并揭示了一个有趣的饱和现象。

    

    差分隐私 (DP) 是一种被广泛运用用于衡量算法隐私损失的框架。传统的DP形式对所有用户强制施加一致的隐私要求，这与现实场景通常不一致，因为用户个体决定他们的隐私偏好。本文探讨了在异构DP约束下的平均数估计问题，其中每个用户可以施加自己独特的隐私水平。我们提出的算法在两组具有不同隐私级别的用户时被证明是极小化的最优的。我们的结果揭示了一个有趣的饱和现象，即在一组用户的隐私水平被放宽而另一组用户的隐私水平保持不变时发生。也就是说，在某个特定情形下，进一步放宽前一组的隐私要求并不会改善最小二乘平均数估计器的性能。因此，中央服务器可以提供一定程度的隐私而不会牺牲性能。

    Differential Privacy (DP) is a well-established framework to quantify privacy loss incurred by any algorithm. Traditional DP formulations impose a uniform privacy requirement for all users, which is often inconsistent with real-world scenarios in which users dictate their privacy preferences individually. This work considers the problem of mean estimation under heterogeneous DP constraints, where each user can impose their own distinct privacy level. The algorithm we propose is shown to be minimax optimal when there are two groups of users with distinct privacy levels. Our results elicit an interesting saturation phenomenon that occurs as one group's privacy level is relaxed, while the other group's privacy level remains constant. Namely, after a certain point, further relaxing the privacy requirement of the former group does not improve the performance of the minimax optimal mean estimator. Thus, the central server can offer a certain degree of privacy without any sacrifice in perform
    
[^20]: 正交多项式逼近和扩展动态模态分解在混沌中的应用

    Orthogonal polynomial approximation and Extended Dynamic Mode Decomposition in chaos. (arXiv:2305.08074v1 [math.NA])

    [http://arxiv.org/abs/2305.08074](http://arxiv.org/abs/2305.08074)

    本文在简单的混沌映射上证明了扩展动态模态分解（EDMD）对于多项式可观测字典有指数效率，从而有效处理了混沌动力学中的正则函数问题，并展示了在这种情况下使用EDMD产生的预测和Koopman谱数据收敛至物理上有意义的极限。

    

    扩展动态模态分解（EDMD）是一种数据驱动的工具，用于动态的预测和模型简化，在物理科学领域得到广泛应用。虽然这种方法在概念上很简单，但在确定性混沌中，它的性质或者它的收敛性还不清楚。特别是，EDMD的最小二乘逼近如何处理需要描绘混沌动力学含义的正则函数的类别，这也是不清楚的。本文在分析上简单的一个圆环展开映射的最简单例子上，发展了关于EDMD的一般的、严格的理论。证明了一个新的关于在单位圆上的正交多项式（OPUC）的理论结果，我们证明在无限数据极限时，针对多项式的可观测字典的最小二乘投影具有指数效率。因此，我们展示了在这种情况下使用EDMD产生的预测和Koopman谱数据收敛到物理上有意义的极限的指数速率。

    Extended Dynamic Mode Decomposition (EDMD) is a data-driven tool for forecasting and model reduction of dynamics, which has been extensively taken up in the physical sciences. While the method is conceptually simple, in deterministic chaos it is unclear what its properties are or even what it converges to. In particular, it is not clear how EDMD's least-squares approximation treats the classes of regular functions needed to make sense of chaotic dynamics.  In this paper we develop a general, rigorous theory of EDMD on the simplest examples of chaotic maps: analytic expanding maps of the circle. Proving a new result in the theory of orthogonal polynomials on the unit circle (OPUC), we show that in the infinite-data limit, the least-squares projection is exponentially efficient for polynomial observable dictionaries. As a result, we show that the forecasts and Koopman spectral data produced using EDMD in this setting converge to the physically meaningful limits, at an exponential rate.  
    
[^21]: 多目标优化的逆强化学习的收敛性证明研究

    A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])

    [http://arxiv.org/abs/2305.06137](http://arxiv.org/abs/2305.06137)

    本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。

    

    本文通过将等效于多目标优化的WIRL问题的逆问题与投影次梯度法相结合，证明了Wasserstein逆强化学习（WIRL）在多目标优化中的收敛性。此外，我们还证明了逆强化学习（最大熵逆强化学习，导引成本学习）在多目标优化中的收敛性。

    We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
    
[^22]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^23]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^24]: 用于黑盒变分推断的样本平均估计方法

    Sample Average Approximation for Black-Box VI. (arXiv:2304.06803v1 [cs.LG])

    [http://arxiv.org/abs/2304.06803](http://arxiv.org/abs/2304.06803)

    该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。

    

    我们提出了一种新的方法，用于解决随机梯度上升的困难，包括选择步长的任务。我们的方法涉及使用一系列样本平均估计问题（SAA）。通过将随机优化问题转化为确定性问题，SAA逼近了随机优化问题的解。我们使用拟牛顿方法和线性搜索来解决每个确定性优化问题，并提出了一种启发式策略来自动选择超参数。我们的实验表明，我们的方法简化了变分推断问题，并实现了比现有方法更快的性能。

    We present a novel approach for black-box VI that bypasses the difficulties of stochastic gradient ascent, including the task of selecting step-sizes. Our approach involves using a sequence of sample average approximation (SAA) problems. SAA approximates the solution of stochastic optimization problems by transforming them into deterministic ones. We use quasi-Newton methods and line search to solve each deterministic optimization problem and present a heuristic policy to automate hyperparameter selection. Our experiments show that our method simplifies the VI problem and achieves faster performance than existing methods.
    
[^25]: 一类DC复合优化问题的非精确线性近似近端算法及应用

    An inexact linearized proximal algorithm for a class of DC composite optimization problems and applications. (arXiv:2303.16822v1 [math.OC])

    [http://arxiv.org/abs/2303.16822](http://arxiv.org/abs/2303.16822)

    本文提出了一种解决非凸非光滑问题的非精确线性化近端算法，并应用于鲁棒分解中的两个问题，得到了有效的数值结果。

    

    本文研究了一类DC复合优化问题。这类问题通常由低秩矩阵恢复的鲁棒分解模型推导而来，是凸复合优化问题和具有非光滑分量的DC规划的扩展。针对这类非凸和非光滑问题，我们提出了一种非精确线性化近端算法（iLPA）。算法中，我们利用目标函数的部分线性化，计算强凸主导的非精确最小化值。迭代序列的生成收敛于潜在函数的Kurdyka-{\L}ojasiewicz（KL）性质，如果潜在函数在极限点处具有KL指数$1/2$的KL性质，则收敛具有局部R线性速率。对于后一种假设，我们利用复合结构提供了一个可验证的条件，并阐明了与凸复合优化所使用的正则性的关系。最后，我们将所提出的非精确线性近端算法应用于解决鲁棒分解中的两个重要问题：张量鲁棒主成分分析（TRPCA）和张量鲁棒低秩张量完成（TRLRTC）。对合成和真实数据的数值结果证明了我们的算法相对于现有最新算法的有效性。

    This paper is concerned with a class of DC composite optimization problems which, as an extension of the convex composite optimization problem and the DC program with nonsmooth components, often arises from robust factorization models of low-rank matrix recovery. For this class of nonconvex and nonsmooth problems, we propose an inexact linearized proximal algorithm (iLPA) which in each step computes an inexact minimizer of a strongly convex majorization constructed by the partial linearization of their objective functions. The generated iterate sequence is shown to be convergent under the Kurdyka-{\L}ojasiewicz (KL) property of a potential function, and the convergence admits a local R-linear rate if the potential function has the KL property of exponent $1/2$ at the limit point. For the latter assumption, we provide a verifiable condition by leveraging the composite structure, and clarify its relation with the regularity used for the convex composite optimization. Finally, the propose
    
[^26]: 深度结构高斯特征模型的学习曲线

    Learning curves for deep structured Gaussian feature models. (arXiv:2303.00564v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.00564](http://arxiv.org/abs/2303.00564)

    该论文针对深度高斯模型的特征各向异性展开研究。研究结果表明，在第一层特征行之间允许存在相关性可以促进泛化，而后续层的结构通常是不利的。

    

    近年来，深度学习理论中对于多层高斯随机特征模型的泛化性能分析引起了广泛关注。然而，很少有研究考虑特征各向异性的影响；大多数模型都假设特征是使用独立同分布的高斯权重生成的。在这篇论文中，我们为具有许多层结构高斯特征的模型导出了学习曲线。我们表明，允许第一层特征的行之间存在相关性可促进泛化，而后续层的结构通常是不利的。我们的结果揭示了权重结构如何影响可解模型的泛化性能。

    In recent years, significant attention in deep learning theory has been devoted to analyzing the generalization performance of models with multiple layers of Gaussian random features. However, few works have considered the effect of feature anisotropy; most assume that features are generated using independent and identically distributed Gaussian weights. Here, we derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.
    
[^27]: 高斯过程在赫赫尔姆霍兹分解中的应用：一种更流体的海洋气流模型

    Gaussian processes at the Helm(holtz): A more fluid model for ocean currents. (arXiv:2302.10364v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.10364](http://arxiv.org/abs/2302.10364)

    该论文提出了一种更符合已知电流物理特性的模型，在通过Helmholtz分解获得的向量场的发散和无旋分量上使用高斯过程来预测海流。证明了这种方法在模拟数据和真实浮标数据方面都比之前的方法更有效。

    

    海洋学家有兴趣预测海流和基于浮标速度的稀疏观测数据来识别当前矢量场中的发散性。高斯过程(GPs)在空间位置上充当连续但高度非线性功能的速度提供了一种吸引人的模型。但我们表明，将具有标准平稳核的GP直接应用于浮标数据可能在当前预测和发散性识别方面遇到困难—由于一些物理上不切实际的先验假设。为了更好地反映已知的电流物理特性，我们建议将标准平稳核放在通过Helmholtz分解获得的向量场的发散和无旋分量上。我们表明，由于该分解仅通过混合偏导数与原始向量场相关，因此我们仍然可以在原始数据给定的情况下进行推理，并且只需要额外进行少数计算。我们通过模拟数据和真实浮标数据证明了这种螺旋GP的有效性，从而在均方预测误差方面优于先前的方法。

    Oceanographers are interested in predicting ocean currents and identifying divergences in a current vector field based on sparse observations of buoy velocities. Since we expect current velocity to be a continuous but highly non-linear function of spatial location, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current prediction and divergence identification -- due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illust
    
[^28]: 函数回归的领域泛化

    Domain Generalization by Functional Regression. (arXiv:2302.04724v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04724](http://arxiv.org/abs/2302.04724)

    通过函数回归实现领域泛化，构建了线性算子将输入边缘分布与输出条件分布联系起来，并提出了一种基于源分布依赖性的再生核希尔伯特空间预测算法。

    

    领域泛化问题是学习如何在给定多种不同源分布的数据时获得良好的泛化能力，以适用于只通过未标记样本观察到的新目标分布的模型。本文以函数回归的形式研究领域泛化问题。我们提出了一种新算法来学习从输入的边缘分布到给定输入的条件下输出的条件分布的线性算子。该算法允许基于源分布依赖性构建预测的再生核希尔伯特空间，并满足理想风险的有限样本误差界。数值实现和源代码已经提供。

    The problem of domain generalization is to learn, given data from different source distributions, a model that can be expected to generalize well on new target distributions which are only seen through unlabeled samples. In this paper, we study domain generalization as a problem of functional regression. Our concept leads to a new algorithm for learning a linear operator from marginal distributions of inputs to the corresponding conditional distributions of outputs given inputs. Our algorithm allows a source distribution-dependent construction of reproducing kernel Hilbert spaces for prediction, and, satisfies finite sample error bounds for the idealized risk. Numerical implementations and source code are available.
    
[^29]: 利用演示数据改进在线学习:质量至关重要

    Leveraging Demonstrations to Improve Online Learning: Quality Matters. (arXiv:2302.03319v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03319](http://arxiv.org/abs/2302.03319)

    本篇论文探讨了离线演示数据如何改进在线学习的问题，提出了一种利用演示数据的TS算法，并给出了依赖于先验知识的贝叶斯遗憾界；研究发现，预训练可以大幅提高在线性能，改进程度随专家能力水平的提高而增加。

    

    我们研究了离线演示数据可以如何改进在线学习，自然而然地期望会有一定的改进，但问题在于如何改进以及可以改进多少？我们表明，改进的程度必须取决于演示数据的质量。为了生成可移植的见解，我们将重点放在了作为典型在线学习算法和模型的多臂赌博机上应用汤普森抽样（TS）。演示数据是由具有给定能力水平的专家生成的，这是我们引入的一个概念。我们提出了一种知情TS算法，通过贝叶斯定理以一致的方式利用演示数据并导出依赖于先验的贝叶斯遗憾界。这提供了洞见，即预训练如何极大地提高在线性能，以及改进程度随专家能力水平的提高而增加。我们还通过贝叶斯引导实现了实用的、近似的知情TS算法，并通过实验证明了实现了实质性的遗憾减少。

    We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level. We also develop a practical, approximate informed TS algorithm through Bayesian bootstrapping and show substantial empirical regret reduction through exp
    
[^30]: 概率对比学习恢复了不确定性输入的正确估计

    Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02865](http://arxiv.org/abs/2302.02865)

    本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。

    

    最近，对比学习编码器被证明可以翻转数据生成过程：它们可以将每个输入（如图像）编码成生成该图像的真实潜变量（Zimmermann等人，2021）。然而，现实世界的观察结果通常存在内在的模糊性。例如，图像可能模糊或只显示3D物体的2D视图，因此可能有多个潜变量生成它们。这使得潜变量的真实后验概率具有异方差不确定性。在这种设置下，我们扩展了常见的InfoNCE目标和编码器，以预测潜变量分布而不是点。我们证明这些分布恢复了数据生成过程的正确后验分布，包括其不确定性水平的估计，该估计存在潜变量空间的旋转。除了提供校准的不确定性估计之外，这些后验分布还允许在图像检索中计算可信区间。它们包括具有与给定查询相同的潜变量的图像。

    Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
    
[^31]: 直接不确定量化

    Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02420](http://arxiv.org/abs/2302.02420)

    本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。

    

    传统神经网络易于训练，但会产生过于自信的预测；而贝叶斯神经网络提供了良好的不确定量化，但优化它们需要耗费时间。本文介绍了一种新的方法——“直接不确定量化”（DirectUQ），它结合了它们的优点，其中神经网络直接输出最后一层的均值和方差。DirectUQ可以导出为一个替代的变分下界，因此从落单变分推理中获益，提供了改进的正则化器。另一方面，像非概率模型一样，DirectUQ具有简单的训练方法，可以使用Rademacher复杂性为模型提供风险边界。实验表明，DirectUQ和DirectUQ集成提供了时间和不确定性量化方面的良好平衡，特别是对于分布之外的数据。

    Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
    
[^32]: 神经网络学习放大决策边界附近的区域

    Neural networks learn to magnify areas near decision boundaries. (arXiv:2301.11375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11375](http://arxiv.org/abs/2301.11375)

    神经网络训练能够放大决策边界附近的局部区域，改善整个系统的泛化能力。

    

    我们研究了训练如何塑造神经网络特征图诱导的黎曼几何。在宽度为无限的情况下，具有随机参数的神经网络在输入空间上引导高度对称的度量。训练分类任务的网络中的特征学习放大了沿决策边界的局部区域。这些变化与先前提出的用于手动调整核方法以改善泛化的几何方法一致。

    We study how training molds the Riemannian geometry induced by neural network feature maps. At infinite width, neural networks with random parameters induce highly symmetric metrics on input space. Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries. These changes are consistent with previously proposed geometric approaches for hand-tuning of kernel methods to improve generalization.
    
[^33]: A-NeSI: 一种可扩展的近似方法用于概率神经符号推理。

    A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12393](http://arxiv.org/abs/2212.12393)

    本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。

    

    本文研究了将神经网络与符号推理相结合的问题。最近引入的概率神经符号学习（PNL）框架，如DeepProbLog，执行指数时间的精确推理，限制了PNL解决方案的可扩展性。我们介绍了近似神经符号推理（A-NeSI）：一种新的PNL框架，它使用神经网络进行可扩展的近似推理。A-NeSI 1) 在不改变概率逻辑语义的情况下，以多项式时间执行近似推理；2) 使用由背景知识生成的数据进行训练；3) 可以生成有关预测的符号解释；4) 可以在测试时间保证逻辑约束的满足，这在安全关键应用中非常重要。我们的实验表明，A-NeSI是第一个能够解决具有指数组合扩展的三种神经符号任务的端到端方法。最后，我们的实验表明，A-NeSI实现了可解释性和安全性，而没有惩罚。

    We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
    
[^34]: 通过雅可比控制选择高斯核岭回归带宽

    Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control. (arXiv:2205.11956v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.11956](http://arxiv.org/abs/2205.11956)

    本文提出了一种基于雅可比控制的带宽选择启发式方法，该方法具有闭式、计算非常轻的特点，并且在关注带宽的同时可以获得更好的模型泛化性能。

    

    大多数机器学习方法需要调整超参数。对于高斯核岭回归，超参数是带宽。带宽指定核函数的长度尺度，必须小心选择才能获得具有良好泛化性能模型。带宽选择的默认方法是交叉验证和边缘似然最大化，这通常会产生良好的结果，尽管计算成本高。此外，这些方法提供的估计往往具有非常高的方差，特别是在训练数据不足时。受雅可比正则化的启发，我们制定了一个近似表达式，用于描述高斯核岭回归推断函数的导数如何取决于核带宽。然后，我们使用这个表达式来提出一种基于雅可比控制的闭式、计算非常轻的带宽选择启发式方法。此外，这个雅可比表达式表明了在检查带宽选择的质量时应关注什么。

    Most machine learning methods require tuning of hyper-parameters. For kernel ridge regression with the Gaussian kernel, the hyper-parameter is the bandwidth. The bandwidth specifies the length-scale of the kernel and has to be carefully selected in order to obtain a model with good generalization. The default methods for bandwidth selection is cross-validation and marginal likelihood maximization, which often yields good results, albeit at high computational costs. Furthermore, the estimates provided by these methods tend to have very high variance, especially when training data are scarce. Inspired by Jacobian regularization, we formulate an approximate expression for how the derivatives of the functions inferred by kernel ridge regression with the Gaussian kernel depend on the kernel bandwidth. We then use this expression to propose a closed-form, computationally feather-light, bandwidth selection heuristic based on controlling the Jacobian. In addition, the Jacobian expression illum
    
[^35]: 子采样学习的网络嵌入的渐近分析

    Asymptotics of Network Embeddings Learned via Subsampling. (arXiv:2107.02363v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.02363](http://arxiv.org/abs/2107.02363)

    本研究将网络嵌入方法封装为一个统一框架，并从理论上证明了使用子采样学习的网络嵌入的渐近分布，同时提供了潜在参数的收敛速率和算法选择与统计效率之间的权衡。

    

    网络数据在现代机器学习中无处不在，相关任务包括节点分类、节点聚类和链接预测。一种常用的方法是首先学习网络的欧几里得嵌入，然后应用于向量值数据开发的算法。对于大型网络，可以使用随机梯度方法学习嵌入，其中子采样方案可以自由选择。尽管这种方法具有强大的实证性能，但它们的理论理解还不够充分。我们的工作将诸如node2vec之类的表示方法封装到一个统一的框架中。在假设图是可交换的情况下，我们证明了学习到的嵌入向量的分布在渐近意义下分解。此外，我们根据潜在参数，包括损失函数和嵌入维数的选择，表征了渐近分布并提供了收敛速率。这为使用子采样学习的网络嵌入提供了基本见解，并阐明了算法选择和统计效率之间的权衡。

    Network data are ubiquitous in modern machine learning, with tasks of interest including node classification, node clustering and link prediction. A frequent approach begins by learning an Euclidean embedding of the network, to which algorithms developed for vector-valued data are applied. For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen. Despite the strong empirical performance of such methods, they are not well understood theoretically. Our work encapsulates representation methods using a subsampling approach, such as node2vec, into a single unifying framework. We prove, under the assumption that the graph is exchangeable, that the distribution of the learned embedding vectors asymptotically decouples. Moreover, we characterize the asymptotic distribution and provided rates of convergence, in terms of the latent parameters, which includes the choice of loss function and the embedding dimension. This provid
    
[^36]: 分层学习：一种用于改善协变量漂移下学习的一般性统计方法

    Stratified Learning: A General-Purpose Statistical Method for Improved Learning under Covariate Shift. (arXiv:2106.11211v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.11211](http://arxiv.org/abs/2106.11211)

    该论文提出了一种用于处理训练集不具代表性的协变量漂移情况下改进监督式学习的分层学习方法，并在宇宙学领域的两个问题中证明了其有效性，大幅提升了目标预测结果。

    

    我们提出了一种简单、具有统计学原理和理论基础的方法，用于在训练集不具代表性的情况下改进监督学习，这种情况被称为协变量漂移。我们建立在因果推断中一种成熟的方法基础之上，表明协变量漂移的影响可以通过在因次分数上进行条件约束来减少或消除。在实践中，通过在估计的因次分数的基础上对数据进行分层构造，从而实现平衡协变量，显著提高目标预测结果。我们在两个现代宇宙学研究问题上证明了我们这种一般性方法的有效性，超越了最先进的重要性加权方法。我们在更新的“超新星光度分类挑战”中获得了最好的AUC值（0.958），并改进了现有的SDSS数据中的星系红移条件密度估计。

    We propose a simple, statistically principled, and theoretically justified method to improve supervised learning when the training set is not representative, a situation known as covariate shift. We build upon a well-established methodology in causal inference, and show that the effects of covariate shift can be reduced or eliminated by conditioning on propensity scores. In practice, this is achieved by fitting learners within strata constructed by partitioning the data based on the estimated propensity scores, leading to approximately balanced covariates and much-improved target prediction. We demonstrate the effectiveness of our general-purpose method on two contemporary research questions in cosmology, outperforming state-of-the-art importance weighting methods. We obtain the best reported AUC (0.958) on the updated "Supernovae photometric classification challenge", and we improve upon existing conditional density estimation of galaxy redshift from Sloan Data Sky Survey (SDSS) data.
    
[^37]: 带有稳定性折衷的选择聚类数目 $K$：一种内部验证标准

    Selecting the Number of Clusters $K$ with a Stability Trade-off: an Internal Validation Criterion. (arXiv:2006.08530v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.08530](http://arxiv.org/abs/2006.08530)

    提出了一种新的聚类验证标准，基于聚类稳定性的内部验证原则，在聚类稳定性和聚类质量方面胜过现有的方法。

    

    模型选择是非参数聚类中的主要挑战之一。毫无疑问，没有可以作为标准答案的真实数据存在，因此评价聚类结果的通用方法尚未出现。聚类目标的不确定性导致了普遍接受的评价标准难以确定。在这方面，聚类稳定性作为一种自然且无需模型的原则而出现：聚类算法应发现数据中稳定的结构。如果数据集从相同的基础分布中重复采样，则算法应找到相似的分区。然而，单纯的稳定性并不适合确定聚类数目。例如，它无法检测聚类数目是否太小。我们提出了一个新的原则：一种好的聚类应该是稳定的，且在每个聚类内部，不存在稳定的子分区。这个原则带来了一种基于聚类稳定性的新型聚类验证标准，克服了传统基于稳定性标准的局限性。我们的框架计算效率高且易于实现。我们在合成和真实世界数据集上展示了我们的标准能够以高精度恢复真实的聚类数目，并且在聚类稳定性和聚类质量方面胜过现有的方法。

    Model selection is a major challenge in non-parametric clustering. There is no universally admitted way to evaluate clustering results for the obvious reason that no ground truth is available. The difficulty to find a universal evaluation criterion is a consequence of the ill-defined objective of clustering. In this perspective, clustering stability has emerged as a natural and model-agnostic principle: an algorithm should find stable structures in the data. If data sets are repeatedly sampled from the same underlying distribution, an algorithm should find similar partitions. However, stability alone is not well-suited to determine the number of clusters. For instance, it is unable to detect if the number of clusters is too small. We propose a new principle: a good clustering should be stable, and within each cluster, there should exist no stable partition. This principle leads to a novel clustering validation criterion based on between-cluster and within-cluster stability, overcoming 
    
[^38]: 使用正交编码矩阵解决多类分类问题

    Solving for multi-class using orthogonal coding matrices. (arXiv:1801.09055v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1801.09055](http://arxiv.org/abs/1801.09055)

    本文研究了使用正交编码矩阵进行多类分类问题的实现方法。实验结果表明，代码中不包含零元素的正交编码矩阵可以通过简单的方法解决概率问题，同时比随机编码更准确。然而与1对1相比，正交编码的准确性仍有待提高。

    

    将二元分类推广到多类分类的常用方法是误差纠正码（ECC）。ECC可以通过多种方式进行优化，例如通过使它们正交化。本文在七个不同的数据集上使用三种二元分类器测试了两种正交ECC，并将它们与其他三种多类别方法：1对1、一对其余和随机ECC进行比较。代码中不包含零元素的第一种正交ECC允许使用快速简单的方法来解决概率问题。最近的文献预测，与随机ECC相比，正交ECC始终更准确。不确定性系数（U.C.）的提高范围在0.4-17.5％（绝对值为0.004-0.139），而Brier分数的提高范围在0.7-10.7％。不幸的是，正交ECC很少比1对1更准确。当将方法与逻辑回归配对时，差异最大，正交ECC从未击败过1对1。

    A common method of generalizing binary to multi-class classification is the error correcting code (ECC). ECCs may be optimized in a number of ways, for instance by making them orthogonal. Here we test two types of orthogonal ECCs on seven different datasets using three types of binary classifier and compare them with three other multi-class methods: 1 vs. 1, one-versus-the-rest and random ECCs. The first type of orthogonal ECC, in which the codes contain no zeros, admits a fast and simple method of solving for the probabilities. Orthogonal ECCs are always more accurate than random ECCs as predicted by recent literature. Improvments in uncertainty coefficient (U.C.) range between 0.4--17.5% (0.004--0.139, absolute), while improvements in Brier score between 0.7--10.7%. Unfortunately, orthogonal ECCs are rarely more accurate than 1 vs. 1. Disparities are worst when the methods are paired with logistic regression, with orthogonal ECCs never beating 1 vs. 1. When the methods are paired wit
    

