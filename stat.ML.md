# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Differentiating Metropolis-Hastings to Optimize Intractable Densities.](http://arxiv.org/abs/2306.07961) | 本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。 |
| [^2] | [Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators.](http://arxiv.org/abs/2306.07918) | 该论文介绍了一种能够处理复杂和间接观测的中介因素的因果中介分析框架。 |
| [^3] | [Identification of Nonlinear Latent Hierarchical Models.](http://arxiv.org/abs/2306.07916) | 本文提出了一种方法，可以在观测变量由因果相关的潜变量生成的非线性潜变量层次因果模型中实现因果结构和潜变量的可识别性。 |
| [^4] | [Omega: Optimistic EMA Gradients.](http://arxiv.org/abs/2306.07905) | Omega是一种优化算法，通过加入历史梯度EMA来减轻噪声的影响并在随机游戏上表现更佳。 |
| [^5] | [Robustly Learning a Single Neuron via Sharpness.](http://arxiv.org/abs/2306.07892) | 该论文提出了一种可以对广泛激活函数族中的神经元的最优$L_2^2$误差进行近似的有效算法。 |
| [^6] | [Symmetry & Critical Points for Symmetric Tensor Decompositions Problems.](http://arxiv.org/abs/2306.07886) | 本文研究了将一个实对称张量分解成秩为1项之和的非凸优化问题，得到了精确的分析估计，并发现了各种阻碍局部优化方法的几何障碍和由于对称性导致的丰富的临界点集合。 |
| [^7] | [Additive Causal Bandits with Unknown Graph.](http://arxiv.org/abs/2306.07858) | 该论文讨论了因果赌博机中选择行动的算法问题，提出了一种基于加性组合线性赌博问题的解决方法以解决未知图谱问题。 |
| [^8] | [A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning.](http://arxiv.org/abs/2306.07818) | PDCA是一种用于离线约束强化学习的算法，它可以通过在Lagrangian函数上运行原始-对偶算法来找到近似鞍点，而无需集中性和强Bellman完备性假设。 |
| [^9] | [The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions.](http://arxiv.org/abs/2306.07774) | 该论文提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播，通过将Lyapunov方程投影到低秩矩阵的流形上，使用数值稳定的动态低秩积分器求解，能够有效地处理高维数据。 |
| [^10] | [Simulation-Based Frequentist Inference with Tractable and Intractable Likelihoods.](http://arxiv.org/abs/2306.07769) | 本文介绍了一种基于模拟的可处理和不可处理似然函数的频率派推断方法，并在宇宙学、高能物理、天文学和流行病学领域进行了演示。 |
| [^11] | [Multi-Fidelity Multi-Armed Bandits Revisited.](http://arxiv.org/abs/2306.07761) | 该论文研究了多保真度多臂赌博机问题，提出了算法框架，并进一步研究了保真度选择的代价复杂度上下界，还提出了一种新的遗憾定义并证明了相应的问题无关和问题相关下界。 |
| [^12] | [Kernelized Reinforcement Learning with Order Optimal Regret Bounds.](http://arxiv.org/abs/2306.07745) | 该论文提出了一种称为$\pi$-KRVI的乐观修改方法，并使用核岭回归进行强化学习中的非线性函数逼近。论文证明了在一般设置下第一个最优遗憾保证，并相对于现有最优结果实现了显着的多项式低差距。 |
| [^13] | [Theoretical Foundations of Adversarially Robust Learning.](http://arxiv.org/abs/2306.07723) | 本论文从理论角度探讨了对抗鲁棒性学习的问题，提出了新的学习算法，并分析了其鲁棒性和泛化性能。 |
| [^14] | [Differentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling.](http://arxiv.org/abs/2306.07674) | 本研究提出了一种在差分隐私条件下的一次排列哈希方法和基于 Bin 的一致加权采样，为大规模搜索和学习应用程序提供了更高效、更方便的工具。 |
| [^15] | [Practice with Graph-based ANN Algorithms on Sparse Data: Chi-square Two-tower model, HNSW, Sign Cauchy Projections.](http://arxiv.org/abs/2306.07607) | 本文探索了在稀疏数据中使用基于图的ANN算法进行高效搜索，特别是HNSW算法在搜索稀疏嵌入方面特别有效。提出了适用于实际中常见的稀疏嵌入的SGN变体，实验结果表明我们提出的算法可以达到最先进的性能水平。 |
| [^16] | [Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach.](http://arxiv.org/abs/2306.07566) | 本文提出了一种处理选择性标记数据的学习问题的方法。通过利用历史决策由一组异质决策者做出的事实，我们建立了一种有原理的工具变量框架，并提出了一种加权学习方法，用于学习预测规则。 |
| [^17] | [Fixed-Budget Best-Arm Identification with Heterogeneous Reward Variances.](http://arxiv.org/abs/2306.07549) | 本文提出了两种改进BAI算法，SHVar适用于已知奖励方差情况，SHAdaVar适用于未知奖励方差情况，算法通过在不同臂之间分配不同比例的预算，更多地选择方差更高的臂，SHAdaVar通过过度估计未知奖励方差以贪心地分配预算。创新之处在于无需关闭预算分配问题的解决方案的臂拉动次数下界。 |
| [^18] | [On Achieving Optimal Adversarial Test Error.](http://arxiv.org/abs/2306.07544) | 本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。 |
| [^19] | [Incentivizing High-Quality Content in Online Recommender Systems.](http://arxiv.org/abs/2306.07479) | 本文研究了在线推荐系统中激励高质量内容的算法问题，经典的在线学习算法会激励生产者创建低质量的内容，但本文提出的一种算法通过惩罚低质量内容的创建者，成功地激励了生产者创造高质量的内容。 |
| [^20] | [Von Mises Mixture Distributions for Molecular Conformation Generation.](http://arxiv.org/abs/2306.07472) | 传统的分子几何结构采样方法计算成本很高，而机器学习方法多数专注于分布中的模式识别。本文提出的Von Mises混合分布用于生成更准确的样本。 |
| [^21] | [A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2306.07465) | 本文提出了一种通用的黑盒方法，适用于多种多智能体强化学习问题，可以在非平稳环境下实现低遗憾率的学习。 |
| [^22] | [Unlocking Sales Growth: Account Prioritization Engine with Explainable AI.](http://arxiv.org/abs/2306.07464) | 论文开发了一款名为 Account Prioritizer 的智能销售账户优先级引擎，使用机器学习和解释算法自动化销售簿优化，在 LinkedIn Business 中成功带来了 +8.08% 的续订订阅增长。 |
| [^23] | [On the Robustness of Removal-Based Feature Attributions.](http://arxiv.org/abs/2306.07462) | 本文研究了Removal-Based特征归因的鲁棒性，提供了全面的理论和实验分析，并证明了所提方法的实际有效性。 |
| [^24] | [FIRE: An Optimization Approach for Fast Interpretable Rule Extraction.](http://arxiv.org/abs/2306.07432) | FIRE是一种用于从树集合中提取易于审查的稀疏规则子集的优化方法，可以鼓励在选择时融合规则，从而增强模型的可解释性，并且在实验中表现出更高的准确性和可解释性。 |
| [^25] | [Multi-Platform Budget Management in Ad Markets with Non-IC Auctions.](http://arxiv.org/abs/2306.07352) | 本论文提出了一种针对广告市场上预算限制和拍卖非激励兼容问题的优化竞标策略，在满足广告主预期预算限制的同时最大化预期总效用；并研究了跨平台提交竞标的在线设置。 |
| [^26] | [Additive Multi-Index Gaussian process modeling, with application to multi-physics surrogate modeling of the quark-gluon plasma.](http://arxiv.org/abs/2306.07299) | 本文针对多物理量代理建模中高维预测训练数据有限、现有代理模型预测不确定性高的问题，提出了一种新的加性多指标高斯过程模型(AdMIn-GP)。该模型利用参数空间内低维嵌入的灵活加性结构，充分利用科学先前知识指导。 |
| [^27] | [Kernel Random Projection Depth for Outlier Detection.](http://arxiv.org/abs/2306.07056) | 本文提出了一种内核随机投影深度方法，用于处理数据云中的多模式和非凸性，实验结果表明在基准数据集上表现优异。 |
| [^28] | [Differential Privacy with Random Projections and Sign Random Projections.](http://arxiv.org/abs/2306.01751) | 本文提出了一系列差分隐私算法，其中iDP-SignRP算法在个体差分隐私设置下效果显著，DP-SignOPORP算法改进了现有算法，DP-OPORP算法表现最优，iDP提供了一种适用于特定数据集的隐私保护解决方案。 |
| [^29] | [The minimax risk in testing the histogram of discrete distributions for uniformity under missing ball alternatives.](http://arxiv.org/abs/2305.18111) | 研究了离散分布样本对于类别间的均匀分布拟合问题下的极小极大风险，在缺少球形替代方案的情况下进行了讨论，通过离散直方图进行检验，获得了一种具有精确刻画的检验方法，并在实证研究中表现出了显著性。 |
| [^30] | [DIVA: A Dirichlet Process Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder.](http://arxiv.org/abs/2305.14067) | 本文提出了DIVA算法，一个基于狄利克雷过程的增量深度聚类框架，利用无限混合高斯作为先验，并利用一种记忆化的在线变分推理方法实现簇的动态适应移动，而不需要先知道特征的数量。该算法表现优越，特别是在增量特征的情况下。 |
| [^31] | [Estimation Beyond Data Reweighting: Kernel Method of Moments.](http://arxiv.org/abs/2305.10898) | 本论文提出了一种新的核矩法估计器，称为KMM，其用于超越数据重新加权的矩方法模型，解除了关于使用 $\varphi$-散度相关的限制。 |
| [^32] | [Fixed points of arbitrarily deep 1-dimensional neural networks.](http://arxiv.org/abs/2303.12814) | 本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。 |
| [^33] | [Concentration Bounds for Discrete Distribution Estimation in KL Divergence.](http://arxiv.org/abs/2302.06869) | 本文提供了拉普拉斯估计器的集中界限，讨论了在KL散度中离散分布估计的问题，并实现了更优的结果。 |
| [^34] | [Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts.](http://arxiv.org/abs/2302.06495) | 本文提出了一种称为Density-Softmax的快速确定性方法，通过将密度函数与softmax结合来提高分布变化下的校准不确定性估计，具有较高的效率和可行性 |
| [^35] | [How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control.](http://arxiv.org/abs/2302.03791) | 本文提出了一种风险控制预测集（RCPS）程序的推广，称为$K$-RCPS，它允许为任何扩散模型提供逐个校准的未来样本间隔，并控制相对于基准真实图像的某种风险概念，同时保持最小平均区间长度。 |
| [^36] | [Homophily modulates double descent generalization in graph convolution networks.](http://arxiv.org/abs/2212.13069) | 本文通过使用统计物理和随机矩阵理论的分析工具，精确地表征了简单图卷积网络在背景随机块模型上的泛化，提出了同质性在图卷积网络的泛化中的调制作用。 |
| [^37] | [Offline Policy Evaluation and Optimization under Confounding.](http://arxiv.org/abs/2211.16583) | 该论文致力于解决离线强化学习中混淆变量导致策略评估和优化存在挑战的问题，包括无法获得一致价值估计和样本复杂度的保证，作者提出了具有保证的下限算法和局部收敛的改进算法。 |
| [^38] | [Distribution Free Prediction Sets for Node Classification.](http://arxiv.org/abs/2211.14555) | 本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类，证明了提供了比简单的符合预测应用更加紧致和良好校准的预测集。 |
| [^39] | [A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System.](http://arxiv.org/abs/2211.03933) | 该论文提出了一种基于超图的机器学习集成网络入侵检测系统，使用超图捕捉端口扫描攻击的演化模式，并使用派生的度量来训练NIDS，从而允许在高精度、高准确率、高召回率性能下实时监测和检测端口扫描活动、其他类型的攻击和敌对入侵，解决了传统NIDS面临的挑战。 |
| [^40] | [Implicit models, latent compression, intrinsic biases, and cheap lunches in community detection.](http://arxiv.org/abs/2210.09186) | 本文提出了一种将社区检测目标与其对应的隐式网络生成模型相联系的解决方案，可以计算网络在任意目标下的描述长度，比较不同算法的性能，同时还可以访问隐式模型。 |
| [^41] | [Decentralized Hyper-Gradient Computation over Time-Varying Directed Networks.](http://arxiv.org/abs/2210.02129) | 本文提出了一种基于时变有向网络的分散式超梯度计算方法，避免了静态无向网络通信 Hessian 矩阵导致的高通信成本和无法使用时变有向网络优势的问题。 |
| [^42] | [Conjugate Natural Selection.](http://arxiv.org/abs/2208.13898) | 本文证明了Fisher-Rao自然梯度下降最佳逼近连续时间复制子方程，这一对应关系称为“共轭自然选择”，为进化计算提供了替代方法，同时提供了连续贝叶斯推理的最佳近似。 |
| [^43] | [Partial Identification of Dose Responses with Hidden Confounders.](http://arxiv.org/abs/2204.11206) | 本论文提出了一种新方法，用于界定无法通过点估计进行识别的连续值处理效应估计范围，以解决存在隐藏混淆因素的问题。 |
| [^44] | [Nonparametric extensions of randomized response for private confidence sets.](http://arxiv.org/abs/2202.08728) | 本文提出了一种随机响应机制的扩展，可在局部差分隐私约束下计算非参数非渐进统计推断，由此得到的结果可用于生成效率高的置信区间和时间均匀置信序列。利用这些方法可以进行实证研究并产生私有模拟。 |
| [^45] | [Exact Solutions of a Deep Linear Network.](http://arxiv.org/abs/2202.04777) | 本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，结果表明权重衰减与模型架构的强烈交互作用会在多于1个隐藏层的网络中创建不良极小值，并表明常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。 |
| [^46] | [Factor-augmented tree ensembles.](http://arxiv.org/abs/2111.14000) | 本文提出了一种因子增强的树集合方法，能够处理多种不规则预测变量，为处理宏观金融问题提供一种可靠的方法。 |
| [^47] | [MARS via LASSO.](http://arxiv.org/abs/2111.11694) | 本文提出了一种自然lasso变体的MARS方法，通过减少对维度的依赖来获得收敛率，并与使用平滑性约束的非参数估计技术联系在一起。 |
| [^48] | [Solving the Dirichlet problem for the Monge-Amp\`ere equation using neural networks.](http://arxiv.org/abs/2110.03310) | 本文应用神经网络解决 Monge-Amp\`ere 方程的迪利克雷问题，使用深度凸输入神经网络的假设可以用来找到唯一的凸解，方法对奇异点、不连续点和源函数中的噪声具有鲁棒性，在高维情况下也能表现良好。 |
| [^49] | [Stochastic coordinate transformations with applications to robust machine learning.](http://arxiv.org/abs/2110.01729) | 本文提出了一种利用随机坐标变换进行异常检测的新方法，该方法通过层级张量积展开来逼近随机过程，并通过训练机器学习分类器对投影系数进行检测。在基准数据集上的实验表明，该方法胜过现有的最先进方法。 |
| [^50] | [WildWood: a new Random Forest algorithm.](http://arxiv.org/abs/2109.08010) | WildWood是一种新的随机森林算法，使用指数权重聚合包外样本以改进预测，并通过使用直方图策略加速分裂查找，具有比标准RF和极限梯度提升算法更快和更具竞争力的性能。 |
| [^51] | [Bandit Quickest Changepoint Detection.](http://arxiv.org/abs/2107.10492) | 基于赌博机的方法可以有效平衡探索和利用，实现对一组传感器的最快变点检测，从而节省资源和成本。 |
| [^52] | [Kernel Thinning.](http://arxiv.org/abs/2105.05842) | 核细化是一种更有效的压缩分布的方法，它可以将$n$点近似的分布压缩到具有可比较最坏积分误差的$\sqrt{n}$点近似，其亚指数保证类似于在$[0,1]^d$上均匀$\mathbb{P}$的经典准蒙特卡罗误差率，但适用于$\mathbb{R}^d$上的一般分布。 |
| [^53] | [Additive interaction modelling using I-priors.](http://arxiv.org/abs/2007.15766) | 本文提出了扩展I-prior方法以解决加性回归模型中的交互作用的挑战，该方法理论和实践上都比其他方法更优，可以使用EM算法估计尺度参数，并提出了交互模型的简明规范。 |
| [^54] | [Adaptive Stopping Rule for Kernel-based Gradient Descent Algorithms.](http://arxiv.org/abs/2001.02879) | 本文提出了一种自适应的内核梯度下降算法的停止准则，使用经验有效维度来量化增量并推导出可执行的提前停止策略。通过使用积分算子方法得出证明，规则具有优化学习速率的最优性，并且提出的停止策略具有计算上的优势。 |
| [^55] | [A Trio Neural Model for Dynamic Entity Relatedness Ranking.](http://arxiv.org/abs/1808.08316) | 这篇论文提出了一种基于神经网络的方法，通过动态评估实体相关性，利用集体注意作为监督，能学习到丰富而不同的实体表示，能在大规模数据集上比竞争基线获得更好的结果。 |
| [^56] | [Fischer-Schultz Lecture: Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments, with an Application to Immunization in India.](http://arxiv.org/abs/1712.04802) | 该论文提出了一种通用的机器学习方法，用于在随机实验中估算和推断异质性处理效应的关键特征，并将其应用于印度免疫计划的数据中，获得有效推断结果。 |
| [^57] | [Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example.](http://arxiv.org/abs/1307.6616) | 该研究表明，在特定的核函数类中，$l^{q}$ 正则化学习在不同阶数 $q$ 下都具有相似的泛化误差界限。 |

# 详细

[^1]: 通过不偏微分对抗复杂密度生成，基于互联马尔科夫链不偏微分优化 MH 采样方法

    Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])

    [http://arxiv.org/abs/2306.07961](http://arxiv.org/abs/2306.07961)

    本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。

    

    在概率模型推理中，目标密度函数通常变得难以计算，需要使用 Monte Carlo 计算。本文开发了一种不偏微分 Metropolis-Hastings 采样器的方法，使我们可以通过概率推理来进行微分。通过将随机微分的最新进展与 Markov 链耦合方法相结合，可以实现无偏，低方差和自动的程序。这使我们能够将基于梯度的优化应用于由于繁琐的目标密度导致期望的情况下。我们通过在高斯混合模型中找到一个模棱两可的观察和在 Ising 模型中最大化比热来演示了我们的方法。

    When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
    
[^2]: 多维和间接观测中介因素的因果中介分析

    Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. (arXiv:2306.07918v1 [cs.LG])

    [http://arxiv.org/abs/2306.07918](http://arxiv.org/abs/2306.07918)

    该论文介绍了一种能够处理复杂和间接观测的中介因素的因果中介分析框架。

    

    因果中介分析是一种强大的方法，可以在潜在结果框架内将处理的总效应分解为直接和介导效应。这在许多科学应用中很重要，可以识别出处理效应的潜在机制。然而，在许多科学应用中，中介因素未被观察到，但可能存在相关测量。为了解决这个限制，我们介绍了一种CMA框架，它可以处理基于可辨识变分自编码器（iVAE）体系结构的复杂和间接观测的中介因素。

    Causal mediation analysis (CMA) is a powerful method to dissect the total effect of a treatment into direct and mediated effects within the potential outcome framework. This is important in many scientific applications to identify the underlying mechanisms of a treatment effect. However, in many scientific applications the mediator is unobserved, but there may exist related measurements. For example, we may want to identify how changes in brain activity or structure mediate an antidepressant's effect on behavior, but we may only have access to electrophysiological or imaging brain measurements. To date, most CMA methods assume that the mediator is one-dimensional and observable, which oversimplifies such real-world scenarios. To overcome this limitation, we introduce a CMA framework that can handle complex and indirectly observed mediators based on the identifiable variational autoencoder (iVAE) architecture. We prove that the true joint distribution over observed and latent variables 
    
[^3]: 非线性潜变量层次模型的识别

    Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])

    [http://arxiv.org/abs/2306.07916](http://arxiv.org/abs/2306.07916)

    本文提出了一种方法，可以在观测变量由因果相关的潜变量生成的非线性潜变量层次因果模型中实现因果结构和潜变量的可识别性。

    

    从观测数据中识别潜变量和因果结构对于许多涉及生物数据、医学数据和非结构化数据（如图像和语言）的实际应用至关重要。然而，当观测变量由因果相关的潜变量生成，并且关系是非线性的时，这项任务可能非常具有挑战性。在这项工作中，我们研究了非线性潜变量层次因果模型的识别问题，在这种模型中，观测变量由一组因果相关的潜变量生成，有些潜变量可能没有观察到的后代。我们证明，在温和的假设下可以实现因果结构和潜变量的可识别性：对于因果结构，我们允许图中任意两个变量之间存在多条路径，这放宽了先前工作中的潜变量树假设；对于结构函数，我们没有进行参数假设，因此可以允许基因

    Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
    
[^4]: Omega: 乐观EMA Gradients

    Omega: Optimistic EMA Gradients. (arXiv:2306.07905v1 [cs.LG])

    [http://arxiv.org/abs/2306.07905](http://arxiv.org/abs/2306.07905)

    Omega是一种优化算法，通过加入历史梯度EMA来减轻噪声的影响并在随机游戏上表现更佳。

    

    随着GAN和对抗性训练的进步，随机min-max优化受到了机器学习界的关注。尽管确定性状态下的博弈优化已经相当好地理解了，但在随机状态下仍存在一些问题。最近的研究表明，像乐观梯度这样的随机梯度下降-上升方法对噪声非常敏感或者会导致失败。虽然存在替代策略，但这些策略可能成本过高。我们引入了Omega，一种具有类似于乐观更新的方法，通过在其更新规则中合并历史梯度的EMA来减轻噪声的影响。我们还探讨了一种包含动量的该算法的变体。虽然我们没有提供收敛性保证，但我们在随机游戏上的实验表明，当应用于线性玩家时，Omega优于乐观梯度方法。

    Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.
    
[^5]: 通过Sharpness强健地学习单个神经元

    Robustly Learning a Single Neuron via Sharpness. (arXiv:2306.07892v1 [cs.LG])

    [http://arxiv.org/abs/2306.07892](http://arxiv.org/abs/2306.07892)

    该论文提出了一种可以对广泛激活函数族中的神经元的最优$L_2^2$误差进行近似的有效算法。

    

    我们研究了在存在对抗性标签噪声的情况下，学习单个神经元对$L_2^2$损失的问题。我们提出了一种有效的算法，对包括ReLU在内的广泛激活函数族中，近似于最优$L_2^2$误差。相较于以前的工作，我们的算法可以应用于更温和的分布假设。使我们结果可行的关键因素是与优化理论的局部误差界的新颖联系。

    We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor. Our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.
    
[^6]: 对称张量分解问题的对称性与临界点

    Symmetry & Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])

    [http://arxiv.org/abs/2306.07886](http://arxiv.org/abs/2306.07886)

    本文研究了将一个实对称张量分解成秩为1项之和的非凸优化问题，得到了精确的分析估计，并发现了各种阻碍局部优化方法的几何障碍和由于对称性导致的丰富的临界点集合。

    

    本文考虑了将一个实对称张量分解成秩为1项之和的非凸优化问题。利用其丰富的对称结构，导出Puiseux级数表示的一系列临界点，并获得了关于临界值和Hessian谱的精确分析估计。这些结果揭示了各种几何障碍，阻碍了局部优化方法的使用，最后，利用一个牛顿多面体论证了固定对称性的所有临界点的完全枚举，并证明了与全局最小值的集合相比，由于对称性的存在，临界点的集合可能会显示出组合的丰富性。

    We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
    
[^7]: 带未知图谱的加性因果赌博机

    Additive Causal Bandits with Unknown Graph. (arXiv:2306.07858v1 [cs.LG])

    [http://arxiv.org/abs/2306.07858](http://arxiv.org/abs/2306.07858)

    该论文讨论了因果赌博机中选择行动的算法问题，提出了一种基于加性组合线性赌博问题的解决方法以解决未知图谱问题。

    

    我们探讨了选择行动的算法，在因果赌博设置下，学习者可以选择干预一组由因果图相关的随机变量，学习者顺序选择干预，并从干预分布中观察样本。学习者的目标是快速找到最大化结果变量期望的，所有可观察变量干预中的干预。我们假设没有关于因果图的任何知识，除了结果和其祖先之间的潜在混淆因素不存在。我们首先展示了未知图问题在结果的父母中可以是指数级难以解决。为了解决这个问题，我们对结果采取额外的加性假设，通过将问题建模为具有全赌博反馈的加性组合线性赌博问题来解决。我们提出了一种新的行动消除算法，展示了如何将此算法应用于这个设置。

    We explore algorithms to select actions in the causal bandit setting where the learner can choose to intervene on a set of random variables related by a causal graph, and the learner sequentially chooses interventions and observes a sample from the interventional distribution. The learner's goal is to quickly find the intervention, among all interventions on observable variables, that maximizes the expectation of an outcome variable. We depart from previous literature by assuming no knowledge of the causal graph except that latent confounders between the outcome and its ancestors are not present. We first show that the unknown graph problem can be exponentially hard in the parents of the outcome. To remedy this, we adopt an additional additive assumption on the outcome which allows us to solve the problem by casting it as an additive combinatorial linear bandit problem with full-bandit feedback. We propose a novel action-elimination algorithm for this setting, show how to apply this al
    
[^8]: 一种基于原始-对偶-评论家算法的离线约束强化学习

    A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning. (arXiv:2306.07818v1 [cs.LG])

    [http://arxiv.org/abs/2306.07818](http://arxiv.org/abs/2306.07818)

    PDCA是一种用于离线约束强化学习的算法，它可以通过在Lagrangian函数上运行原始-对偶算法来找到近似鞍点，而无需集中性和强Bellman完备性假设。

    

    离线约束强化学习旨在学习一种策略，以在现有数据集上满足对成本函数期望值的限制条件下最大化预期的累积奖励。本文提出了一种称为原始-对偶-评论家算法（PDCA）的新算法，用于具有一般函数逼近的离线约束强化学习。PDCA在评论家估计的Lagrangian函数上运行原始-对偶算法。原始玩家采用无悔策略优化神谕，在给定任何评论家和对偶玩家的选择的情况下最大化拉格朗日函数的估计。对偶玩家通过采用无悔在线线性优化神谕，在给定评论家和原始玩家的任何选择的情况下最小化拉格朗日函数的估计。我们展示了PDCA可以成功地找到拉格朗日函数的近似鞍点，这对于约束强化学习问题几乎是最优的。与以前需要集中性和强Bellman完备性假设的作品不同，PDCA只需要一致性和自闭性这两个假设。

    Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires co
    
[^9]: 降秩卡尔曼滤波器：在高维中进行近似低秩动态滤波

    The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v1 [stat.ML])

    [http://arxiv.org/abs/2306.07774](http://arxiv.org/abs/2306.07774)

    该论文提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播，通过将Lyapunov方程投影到低秩矩阵的流形上，使用数值稳定的动态低秩积分器求解，能够有效地处理高维数据。

    

    在高维动态系统的推断和模拟中，需要进行某种形式的降维才能使问题具有可处理性。在本文中，我们提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播。这是通过将预测步骤相关的Lyapunov方程投影到低秩矩阵的流形上来实现的，然后通过最近开发的数值稳定、动态低秩积分器求解这些方程。与此同时，通过注意协方差更新仅转换协方差矩阵的列空间，而该空间由构造得到，从而使更新步骤具有可处理性。算法与现有的基于集合的方法不同之处在于，协方差矩阵的低秩近似是确定性的，而不是随机的。关键在于，这使得该方法能够有效地处理高维数据。

    Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to repr
    
[^10]: 基于模拟的可处理和不可处理似然函数的频率派推断

    Simulation-Based Frequentist Inference with Tractable and Intractable Likelihoods. (arXiv:2306.07769v1 [stat.ME])

    [http://arxiv.org/abs/2306.07769](http://arxiv.org/abs/2306.07769)

    本文介绍了一种基于模拟的可处理和不可处理似然函数的频率派推断方法，并在宇宙学、高能物理、天文学和流行病学领域进行了演示。

    

    在许多科学领域中，连接理论模型和观察结果的高保真度模拟器是不可或缺的工具。当与机器学习相结合时，模拟器使得直接从真实和模拟观察结果中推断理论模型的参数成为可能，而不需要明确使用似然函数，特别是在似然函数不可处理的情况下。我们引入了最近提出的无似然频率学推断（LF2I）方法的一个简单修改，这个修改具有一些计算上的优势。我们通过将该算法应用于三个教学上有趣的例子来说明其实用性：第一个例子来自宇宙学，第二个例子来自高能物理和天文学，两者都具有可处理的似然函数，而第三个具有不可处理的似然函数，来自于流行病学。

    High-fidelity simulators that connect theoretical models with observations are indispensable tools in many sciences. When coupled with machine learning, a simulator makes it possible to infer the parameters of a theoretical model directly from real and simulated observations without explicit use of the likelihood function. This is of particular interest when the latter is intractable. We introduce a simple modification of the recently proposed likelihood-free frequentist inference (LF2I) approach that has some computational advantages. The utility of our algorithm is illustrated by applying it to three pedagogically interesting examples: the first is from cosmology, the second from high-energy physics and astronomy, both with tractable likelihoods, while the third, with an intractable likelihood, is from epidemiology.
    
[^11]: 多保真度多臂赌博机的再探讨

    Multi-Fidelity Multi-Armed Bandits Revisited. (arXiv:2306.07761v1 [cs.LG])

    [http://arxiv.org/abs/2306.07761](http://arxiv.org/abs/2306.07761)

    该论文研究了多保真度多臂赌博机问题，提出了算法框架，并进一步研究了保真度选择的代价复杂度上下界，还提出了一种新的遗憾定义并证明了相应的问题无关和问题相关下界。

    

    我们研究了经典多臂赌博机问题的扩展——多保真度多臂赌博机(MF-MAB)。MF-MAB允许每个臂根据不同的代价(保真度)和观测精度来进行拉动。我们研究了最佳臂识别以及遗憾最小化两个目标。对于最佳臂识别，我们提出了以下内容：(a)代价复杂度下界，(b)两种不同保真度选择方法的算法框架，(c)两种方法的代价复杂度上界。由MF-MAB的这两个代价复杂度下界可以恢复经典（单保真度）MAB的标准样本复杂度下界。对于MF-MAB的遗憾最小化问题，我们提出了一种新的遗憾定义，证明了它的问题无关的遗憾下界为$\Omega(K^{1/3}\Lambda^{2/3})​$和问题相关的下界$\Omega(K\log \Lambda)​$，其中$K$是臂数，$\Lambda$是以代价为单位的决策预算，还提出了一种基于淘汰的算法，其权衡了不同的代价组合。

    We study the multi-fidelity multi-armed bandit (MF-MAB), an extension of the canonical multi-armed bandit (MAB) problem. MF-MAB allows each arm to be pulled with different costs (fidelities) and observation accuracy. We study both the best arm identification with fixed confidence (BAI) and the regret minimization objectives. For BAI, we present (a) a cost complexity lower bound, (b) an algorithmic framework with two alternative fidelity selection procedures, and (c) both procedures' cost complexity upper bounds. From both cost complexity bounds of MF-MAB, one can recover the standard sample complexity bounds of the classic (single-fidelity) MAB. For regret minimization of MF-MAB, we propose a new regret definition, prove its problem-independent regret lower bound $\Omega(K^{1/3}\Lambda^{2/3})$ and problem-dependent lower bound $\Omega(K\log \Lambda)$, where $K$ is the number of arms and $\Lambda$ is the decision budget in terms of cost, and devise an elimination-based algorithm whose w
    
[^12]: 核化强化学习及其近似方法的优化

    Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])

    [http://arxiv.org/abs/2306.07745](http://arxiv.org/abs/2306.07745)

    该论文提出了一种称为$\pi$-KRVI的乐观修改方法，并使用核岭回归进行强化学习中的非线性函数逼近。论文证明了在一般设置下第一个最优遗憾保证，并相对于现有最优结果实现了显着的多项式低差距。

    

    强化学习（RL）在各种具有复杂模型和大状态-行为空间的实际场景中显示出了实证的成功。但是，现有的分析结果通常集中于具有少量状态-行为或简单模型（例如线性建模状态-行为值函数）的设置。 为了推导有效处理更广泛值函数的大状态-行为空间的RL策略，一些最新工作考虑使用核岭回归进行非线性函数逼近。 我们提出了称为$\pi$-KRVI的方法，它是最小二乘值迭代的一种乐观修改，当状态-行为值函数由RKHS表示时。我们证明了在一般设置下第一个最优遗憾保证。我们的结果显示，在许多具有高度非光滑内核（例如神经切向内核或某些Mat\'ern内核）的情况下，相对于现有最优结果，存在显着的多项式低差距。

    Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
    
[^13]: 对抗鲁棒性学习的理论基础

    Theoretical Foundations of Adversarially Robust Learning. (arXiv:2306.07723v1 [cs.LG])

    [http://arxiv.org/abs/2306.07723](http://arxiv.org/abs/2306.07723)

    本论文从理论角度探讨了对抗鲁棒性学习的问题，提出了新的学习算法，并分析了其鲁棒性和泛化性能。

    

    尽管机器学习系统取得了非凡的进展，但已经证明它们对于对抗样本是脆弱的：对测试样本进行细微但有意的扰动，就会导致机器学习模型错误分类。本论文旨在从理论角度解决这一具有挑战性的问题，探讨我们可以希望保证什么样的鲁棒性性质，并提供可实现算法保证这些性质的方法。

    Despite extraordinary progress, current machine learning systems have been shown to be brittle against adversarial examples: seemingly innocuous but carefully crafted perturbations of test examples that cause machine learning predictors to misclassify. Can we learn predictors robust to adversarial examples? and how? There has been much empirical interest in this contemporary challenge in machine learning, and in this thesis, we address it from a theoretical perspective.  In this thesis, we explore what robustness properties can we hope to guarantee against adversarial examples and develop an understanding of how to algorithmically guarantee them. We illustrate the need to go beyond traditional approaches and principles such as empirical risk minimization and uniform convergence, and make contributions that can be categorized as follows: (1) introducing problem formulations capturing aspects of emerging practical challenges in robust learning, (2) designing new learning algorithms with 
    
[^14]: 差分隐私下的一次排列哈希和基于 Bin 的一致加权采样

    Differentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling. (arXiv:2306.07674v1 [stat.ML])

    [http://arxiv.org/abs/2306.07674](http://arxiv.org/abs/2306.07674)

    本研究提出了一种在差分隐私条件下的一次排列哈希方法和基于 Bin 的一致加权采样，为大规模搜索和学习应用程序提供了更高效、更方便的工具。

    

    最小哈希（MinHash）是一种标准算法，广泛应用于具有二进制（0/1）Jaccard相似度的大规模搜索和学习应用程序。MinHash 的常见用途是处理大规模 n-gram 文本表示，以便实践者不必实现原始数据（这将是禁止的）。MinHash 的另一个流行用途是构建哈希表，以实现亚线性时间的近似最近邻搜索。MinHash 还用作构建大规模机器学习系统的工具。标准的 MinHash 实现需要应用 K 个随机排列，而一次排列哈希方法（OPH）则是 MinHash 的一种高效替代方法，它将数据矢量划分为 K 个 bin，并在每个 bin 中生成哈希值。OPH 更加高效，更加便利。在本文中，我们将差分隐私（DP）与 OPH（以及 MinHash）相结合，提出了一种差分隐私下的一次排列哈希和基于 Bin 的一致加权采样方法。

    Minwise hashing (MinHash) is a standard algorithm widely used in the industry, for large-scale search and learning applications with the binary (0/1) Jaccard similarity. One common use of MinHash is for processing massive n-gram text representations so that practitioners do not have to materialize the original data (which would be prohibitive). Another popular use of MinHash is for building hash tables to enable sub-linear time approximate near neighbor (ANN) search. MinHash has also been used as a tool for building large-scale machine learning systems. The standard implementation of MinHash requires applying $K$ random permutations. In comparison, the method of one permutation hashing (OPH), is an efficient alternative of MinHash which splits the data vectors into $K$ bins and generates hash values within each bin. OPH is substantially more efficient and also more convenient to use.  In this paper, we combine the differential privacy (DP) with OPH (as well as MinHash), to propose the 
    
[^15]: 基于图的ANN算法在稀疏数据上的应用：卡方双塔模型、HNSW、符号柯西投影

    Practice with Graph-based ANN Algorithms on Sparse Data: Chi-square Two-tower model, HNSW, Sign Cauchy Projections. (arXiv:2306.07607v1 [cs.IR])

    [http://arxiv.org/abs/2306.07607](http://arxiv.org/abs/2306.07607)

    本文探索了在稀疏数据中使用基于图的ANN算法进行高效搜索，特别是HNSW算法在搜索稀疏嵌入方面特别有效。提出了适用于实际中常见的稀疏嵌入的SGN变体，实验结果表明我们提出的算法可以达到最先进的性能水平。

    

    在实际应用中，稀疏数据很常见。传统的“手工制作”的特征通常是稀疏的。通过训练得到的嵌入向量也可能非常稀疏，例如使用“ReLu”激活函数训练的嵌入。本文探讨了使用基于图的ANN算法（例如，HNSW或其GPU版本SONG）进行稀疏数据中的高效搜索，这些算法在工业实践中非常流行，例如搜索和广告（广告投放）。我们进行了专利广告定向应用的实验，并测试了基准公共数据集。对于广告定向，我们使用标准的“余弦双塔”模型和开发的“卡方双塔”模型训练嵌入，这两种模型都能够产生（非常）稀疏的嵌入，当它们与“ReLu”激活函数集成时。在嵌入式检索（EBR）应用中，在训练完嵌入向量后，下一个至关重要的任务是用于服务的近似最近邻（ANN）搜索。虽然有许多ANN算法可用，但基于图的算法已知是高效且可扩展的。我们经验证明，基于图的算法，特别是HNSW在搜索稀疏嵌入（高达97％的稀疏度）方面特别有效。我们还提出了适用于实际中常见的稀疏嵌入的SGN变体（符号柯西投影）。我们将所提出的算法在公共数据集以及来自广告应用的实际数据上进行了测试。实验结果表明，我们提出的算法可以达到最先进的性能水平。

    Sparse data are common. The traditional ``handcrafted'' features are often sparse. Embedding vectors from trained models can also be very sparse, for example, embeddings trained via the ``ReLu'' activation function. In this paper, we report our exploration of efficient search in sparse data with graph-based ANN algorithms (e.g., HNSW, or SONG which is the GPU version of HNSW), which are popular in industrial practice, e.g., search and ads (advertising).  We experiment with the proprietary ads targeting application, as well as benchmark public datasets. For ads targeting, we train embeddings with the standard ``cosine two-tower'' model and we also develop the ``chi-square two-tower'' model. Both models produce (highly) sparse embeddings when they are integrated with the ``ReLu'' activation function. In EBR (embedding-based retrieval) applications, after we the embeddings are trained, the next crucial task is the approximate near neighbor (ANN) search for serving. While there are many AN
    
[^16]: 学习选择标签下的异质决策者：一种工具变量方法

    Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])

    [http://arxiv.org/abs/2306.07566](http://arxiv.org/abs/2306.07566)

    本文提出了一种处理选择性标记数据的学习问题的方法。通过利用历史决策由一组异质决策者做出的事实，我们建立了一种有原理的工具变量框架，并提出了一种加权学习方法，用于学习预测规则。

    

    我们研究了在选择性标记数据下的学习问题。这种问题在历史决策导致结果仅部分标记时出现。标记数据分布可能与整体人群有显著差异，特别是当历史决策和目标结果可以同时受某些未观察到的因素影响时。因此，仅基于标记数据进行学习可能会导致在整体人群中的严重偏差。我们的论文通过利用许多应用中历史决策由一组异质决策者做出的事实来解决此挑战。具体而言，我们在一个有原理的工具变量框架下分析了这种设置。我们建立了满足观察到的数据时任何给定预测规则的全体风险的点识别条件，并在点识别失败时提供了尖锐的风险界限。我们进一步提出了一种加权学习方法，用于学习预测规则。

    We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
    
[^17]: 异构奖励方差下的定长最优臂识别问题研究

    Fixed-Budget Best-Arm Identification with Heterogeneous Reward Variances. (arXiv:2306.07549v1 [cs.LG])

    [http://arxiv.org/abs/2306.07549](http://arxiv.org/abs/2306.07549)

    本文提出了两种改进BAI算法，SHVar适用于已知奖励方差情况，SHAdaVar适用于未知奖励方差情况，算法通过在不同臂之间分配不同比例的预算，更多地选择方差更高的臂，SHAdaVar通过过度估计未知奖励方差以贪心地分配预算。创新之处在于无需关闭预算分配问题的解决方案的臂拉动次数下界。

    

    本文研究了在异构奖励方差下的定长最优臂识别问题。我们提出了两种应用于不同奖励方差情况下的改进BAI算法：SHVar适用于已知奖励方差情况，SHAdaVar适用于未知奖励方差情况。我们的算法通过在不同臂之间分配不同比例的预算，更多地选择方差更高的臂。我们的算法创新在于SHAdaVar的设计，其通过过度估计未知奖励方差以贪心地分配预算。我们分别对SHVar和SHAdaVar中误识别最优臂的概率进行限制。我们的分析依赖于新颖的无需关闭预算分配问题的解决方案的臂拉动次数下界。由于我们的一个预算分配问题类似于未知方差的最优实验设计，因此我们认为我们的结果具有广泛的兴趣。我们的实验验证了我们算法的有效性。

    We study the problem of best-arm identification (BAI) in the fixed-budget setting with heterogeneous reward variances. We propose two variance-adaptive BAI algorithms for this setting: SHVar for known reward variances and SHAdaVar for unknown reward variances. Our algorithms rely on non-uniform budget allocations among the arms where the arms with higher reward variances are pulled more often than those with lower variances. The main algorithmic novelty is in the design of SHAdaVar, which allocates budget greedily based on overestimating the unknown reward variances. We bound probabilities of misidentifying the best arms in both SHVar and SHAdaVar. Our analyses rely on novel lower bounds on the number of pulls of an arm that do not require closed-form solutions to the budget allocation problem. Since one of our budget allocation problems is analogous to the optimal experiment design with unknown variances, we believe that our results are of a broad interest. Our experiments validate ou
    
[^18]: 关于实现最优对抗测试误差的研究

    On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])

    [http://arxiv.org/abs/2306.07544](http://arxiv.org/abs/2306.07544)

    本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。

    

    本文首先阐述了最优对抗预测器的各种基本特性：最优对抗凸预测器的结构、将对抗凸损失与对抗0-1损失相关联的界限以及连续预测器可以在凸和0-1损失下无限接近最优对抗误差。本文还将这些结果与对抗训练在初始化附近的新Rademacher复杂度界限相结合，证明了对于一般的数据分布和扰动集，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。相比之下，先前的理论工作只考虑了特定的数据分布或仅提供了训练误差的保证。

    We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.
    
[^19]: 在在线推荐系统中激励高质量内容

    Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])

    [http://arxiv.org/abs/2306.07479](http://arxiv.org/abs/2306.07479)

    本文研究了在线推荐系统中激励高质量内容的算法问题，经典的在线学习算法会激励生产者创建低质量的内容，但本文提出的一种算法通过惩罚低质量内容的创建者，成功地激励了生产者创造高质量的内容。

    

    对于像TikTok和YouTube这样的内容推荐系统，平台的决策算法塑造了内容生产者的激励，包括生产者在内容质量上投入多少努力。许多平台采用在线学习，这会产生跨时间的激励，因为今天生产的内容会影响未来内容的推荐。在本文中，我们研究了在线学习产生的激励，分析了在纳什均衡下生产的内容质量。我们发现，像Hedge和EXP3这样的经典在线学习算法会激励生产者创建低质量的内容。特别地，内容质量在学习率方面有上限，并且随着典型学习率进展而趋近于零。在这一负面结果的基础上，我们设计了一种不同的学习算法——基于惩罚创建低质量内容的生产者——正确激励生产者创建高质量内容。我们的算法依赖于新颖的策略性赌博机问题，并克服了在组合设置中应用对抗性技术的挑战。在模拟和真实数据的实验中，我们的算法成功地激励生产者创建高质量内容。

    For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
    
[^20]: Von Mises混合分布用于分子构象生成

    Von Mises Mixture Distributions for Molecular Conformation Generation. (arXiv:2306.07472v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.07472](http://arxiv.org/abs/2306.07472)

    传统的分子几何结构采样方法计算成本很高，而机器学习方法多数专注于分布中的模式识别。本文提出的Von Mises混合分布用于生成更准确的样本。

    

    分子经常被表示为图形，但是基础的三维分子几何结构（原子的位置）最终决定了大多数分子性质。然而，大多数分子在常温下都不是静态的，并采取各种各样的几何结构或$\textit{构象}$。由此产生的几何结构分布$p(x)$称为玻尔兹曼分布，许多分子性质都是在该分布下计算的期望。因此，准确地从玻尔兹曼分布中生成样本对于准确计算这些期望至关重要。传统的基于采样的方法计算成本很高，大部分最近的基于机器学习的方法专注于识别该分布中的$\textit{模式}$，而不是生成真正的$\textit{样本}$。生成这样的样本需要捕捉构象变异性，人们广泛认为分子的大多数构象变异性来自于化学键的旋转。

    Molecules are frequently represented as graphs, but the underlying 3D molecular geometry (the locations of the atoms) ultimately determines most molecular properties. However, most molecules are not static and at room temperature adopt a wide variety of geometries or $\textit{conformations}$. The resulting distribution on geometries $p(x)$ is known as the Boltzmann distribution, and many molecular properties are expectations computed under this distribution. Generating accurate samples from the Boltzmann distribution is therefore essential for computing these expectations accurately. Traditional sampling-based methods are computationally expensive, and most recent machine learning-based methods have focused on identifying $\textit{modes}$ in this distribution rather than generating true $\textit{samples}$. Generating such samples requires capturing conformational variability, and it has been widely recognized that the majority of conformational variability in molecules arises from rota
    
[^21]: 面向非平稳多智能体强化学习的黑盒方法

    A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning. (arXiv:2306.07465v1 [cs.LG])

    [http://arxiv.org/abs/2306.07465](http://arxiv.org/abs/2306.07465)

    本文提出了一种通用的黑盒方法，适用于多种多智能体强化学习问题，可以在非平稳环境下实现低遗憾率的学习。

    

    本文研究了在非平稳多智能体系统中学习均衡的方法，并解决了区别于单智能体学习的挑战。我们重点关注带有赌徒反馈的游戏，其中即使待测试的差距很小，测试一个均衡也可能导致大量的遗憾，并且在静态游戏中存在多个最优解（均衡）会带来额外的难题。为了克服这些障碍，我们提出了一种通用的黑盒方法，适用于广泛的问题，如一般和博弈、潜在博弈和马尔可夫博弈，只要在静态环境下配备适当的学习和测试神谕。当非平稳程度（通过总变化量 $\Delta$ 测量）已知时，我们的算法可以实现 $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ 的遗憾，当 $\Delta$ 未知时，可以实现 $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ 的遗憾。

    We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ regret when $\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, 
    
[^22]: 解锁销售增长：具有可解释 AI 的账户优先级引擎

    Unlocking Sales Growth: Account Prioritization Engine with Explainable AI. (arXiv:2306.07464v1 [cs.AI])

    [http://arxiv.org/abs/2306.07464](http://arxiv.org/abs/2306.07464)

    论文开发了一款名为 Account Prioritizer 的智能销售账户优先级引擎，使用机器学习和解释算法自动化销售簿优化，在 LinkedIn Business 中成功带来了 +8.08% 的续订订阅增长。

    

    B2B 销售需要有效预测客户增长，识别升级潜力以及降低流失风险。LinkedIn 的销售代表传统上依赖直觉和碎片化数据信号来评估客户绩效。这导致在数据理解和策略制定方面投入了大量时间，而在积极销售方面投资不足。为了克服这一挑战，我们开发了一种数据产品，称为 Account Prioritizer，它是智能销售账户优先级引擎。它使用机器学习推荐模型和集成的账户级解释算法在销售 CRM 中自动化销售簿优化的手动过程。一次成功的 A/B 测试表明，Account Prioritizer 为 LinkedIn Business 带来了显著的 +8.08% 续订订阅增长。

    B2B sales requires effective prediction of customer growth, identification of upsell potential, and mitigation of churn risks. LinkedIn sales representatives traditionally relied on intuition and fragmented data signals to assess customer performance. This resulted in significant time investment in data understanding as well as strategy formulation and under-investment in active selling. To overcome this challenge, we developed a data product called Account Prioritizer, an intelligent sales account prioritization engine. It uses machine learning recommendation models and integrated account-level explanation algorithms within the sales CRM to automate the manual process of sales book prioritization. A successful A/B test demonstrated that the Account Prioritizer generated a substantial +8.08% increase in renewal bookings for the LinkedIn Business.
    
[^23]: 关于Removal-Based特征归因的鲁棒性研究

    On the Robustness of Removal-Based Feature Attributions. (arXiv:2306.07462v1 [cs.LG])

    [http://arxiv.org/abs/2306.07462](http://arxiv.org/abs/2306.07462)

    本文研究了Removal-Based特征归因的鲁棒性，提供了全面的理论和实验分析，并证明了所提方法的实际有效性。

    

    为了解释基于输入的复杂模型，开发了许多特征归因方法来分配输入特征的重要性分数。然而，最近的一些研究挑战了特征归因的鲁棒性，指出这些方法对输入和模型扰动敏感，而其他研究通过提出鲁棒归因方法和模型修改来解决这个问题。然而，以往的归因鲁棒性研究主要侧重于基于梯度的特征归因。相比之下，Removal-Based归因方法的鲁棒性质尚未全面地得到理解。为了弥补这一差距，我们从理论上对Removal-Based特征归因的鲁棒性进行了全面的阐述。具体而言，我们对这种方法进行了统一的分析，并在输入和模型扰动的情况下证明了完好和受扰动的归因之间的差异的上限。我们在合成和真实数据集上的实验验证了我们的理论结果，并证明了所提出方法的实际有效性。

    To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and re
    
[^24]: FIRE：一种用于快速可解释规则提取的优化方法

    FIRE: An Optimization Approach for Fast Interpretable Rule Extraction. (arXiv:2306.07432v1 [cs.LG])

    [http://arxiv.org/abs/2306.07432](http://arxiv.org/abs/2306.07432)

    FIRE是一种用于从树集合中提取易于审查的稀疏规则子集的优化方法，可以鼓励在选择时融合规则，从而增强模型的可解释性，并且在实验中表现出更高的准确性和可解释性。

    

    我们提出了FIRE，即Fast Interpretable Rule Extraction，这是一个基于优化的框架，用于从树集合中提取少量但有用的决策规则。 FIRE从树集合中选择稀疏的代表性规则子集，这些子集易于由实践者检查。为了进一步增强所提取模型的可解释性，FIRE鼓励在选择时融合规则，以便许多所选决策规则共享相同的前提条件。该优化框架利用融合正则化惩罚来实现这一点，同时采用非凸稀疏引入惩罚以积极选取规则。FIRE中的优化问题由于问题规模和惩罚的非凸性而对现成求解器构成挑战。为了解决这个问题，利用问题结构，我们开发了一个基于块坐标下降原理的专门求解器； 我们的求解器比现有求解器运行速度快40倍。实验证明，与现有的规则提取方法相比，FIRE能够产生更准确且可解释的模型。

    We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperform
    
[^25]: 在非 IC 拍卖广告市场上的多平台预算管理

    Multi-Platform Budget Management in Ad Markets with Non-IC Auctions. (arXiv:2306.07352v1 [cs.GT])

    [http://arxiv.org/abs/2306.07352](http://arxiv.org/abs/2306.07352)

    本论文提出了一种针对广告市场上预算限制和拍卖非激励兼容问题的优化竞标策略，在满足广告主预期预算限制的同时最大化预期总效用；并研究了跨平台提交竞标的在线设置。

    

    在在线广告市场上，有预算限制的广告主通过在各种平台上反复竞标获得广告位置。我们提出了一种策略，用于在可能存在预算限制的激励兼容或非激励兼容情况下，优化竞标一组拍卖品。我们的策略最大化预期在各个拍卖中的总效用，同时满足广告主预期的预算限制。此外，我们研究了广告主必须在学习其他竞标者的出价情况的同时，跨平台提交竞标的在线设置。在全信息设置下，我们的算法具有 $O(T^{3/4})$ 的遗憾值。最后，我们证明了相比现有的自适应步伐算法，我们的算法在广告放置拍卖的合成和真实数据集上具有更优秀的累积遗憾值。

    In online advertising markets, budget-constrained advertisers acquire ad placements through repeated bidding in auctions on various platforms. We present a strategy for bidding optimally in a set of auctions that may or may not be incentive-compatible under the presence of budget constraints. Our strategy maximizes the expected total utility across auctions while satisfying the advertiser's budget constraints in expectation. Additionally, we investigate the online setting where the advertiser must submit bids across platforms while learning about other bidders' bids over time. Our algorithm has $O(T^{3/4})$ regret under the full-information setting. Finally, we demonstrate that our algorithms have superior cumulative regret on both synthetic and real-world datasets of ad placement auctions, compared to existing adaptive pacing algorithms.
    
[^26]: 加性多指标高斯过程建模及其在夸克胶子等离子体多物理量代理建模中的应用

    Additive Multi-Index Gaussian process modeling, with application to multi-physics surrogate modeling of the quark-gluon plasma. (arXiv:2306.07299v1 [nucl-th])

    [http://arxiv.org/abs/2306.07299](http://arxiv.org/abs/2306.07299)

    本文针对多物理量代理建模中高维预测训练数据有限、现有代理模型预测不确定性高的问题，提出了一种新的加性多指标高斯过程模型(AdMIn-GP)。该模型利用参数空间内低维嵌入的灵活加性结构，充分利用科学先前知识指导。

    

    夸克胶子等离子体是一种独特的核物质相，理论上认为在宇宙大爆炸后不久就填满了宇宙。研究夸克胶子等离子体的关键挑战是需要在高维参数空间内运行复杂的物理模型，以将实验可观测量与理论参数进行协调。然而，由于每次运行都需要数千个CPU小时，因此物理学家只能进行几百次运行，从而导致高维预测的训练数据有限，现有的代理模型通常会产生高预测不确定性。针对这个问题，我们提出了一种新的加性多指标高斯过程(AdMIn-GP)模型，它利用参数空间内低维嵌入的灵活加性结构。这是由科学先前知识指导的，即夸克胶子等离子体受多个不同物理现象(即多物理量)的支配。

    The Quark-Gluon Plasma (QGP) is a unique phase of nuclear matter, theorized to have filled the Universe shortly after the Big Bang. A critical challenge in studying the QGP is that, to reconcile experimental observables with theoretical parameters, one requires many simulation runs of a complex physics model over a high-dimensional parameter space. Each run is computationally very expensive, requiring thousands of CPU hours, thus limiting physicists to only several hundred runs. Given limited training data for high-dimensional prediction, existing surrogate models often yield poor predictions with high predictive uncertainties, leading to imprecise scientific findings. To address this, we propose a new Additive Multi-Index Gaussian process (AdMIn-GP) model, which leverages a flexible additive structure on low-dimensional embeddings of the parameter space. This is guided by prior scientific knowledge that the QGP is dominated by multiple distinct physical phenomena (i.e., multiphysics),
    
[^27]: 内核随机投影深度用于离群点检测

    Kernel Random Projection Depth for Outlier Detection. (arXiv:2306.07056v1 [stat.ML])

    [http://arxiv.org/abs/2306.07056](http://arxiv.org/abs/2306.07056)

    本文提出了一种内核随机投影深度方法，用于处理数据云中的多模式和非凸性，实验结果表明在基准数据集上表现优异。

    

    本文提出了一种扩展的随机投影深度（RPD）方法，用于处理数据云中的多模式和非凸性。在所提出的方法的框架中，RPD在再现核希尔伯特空间中计算。借助内核主成分分析，我们期望所提出的方法可以处理上述多种模式和非凸性。实验结果表明，所提出的方法优于RPD，并可与基准数据集上现有的检测模型相媲美，关于接收操作特征曲线（ROC）下的曲线下面积（AUC）。

    This paper proposes an extension of Random Projection Depth (RPD) to cope with multiple modalities and non-convexity on data clouds. In the framework of the proposed method, the RPD is computed in a reproducing kernel Hilbert space. With the help of kernel principal component analysis, we expect that the proposed method can cope with the above multiple modalities and non-convexity. The experimental results demonstrate that the proposed method outperforms RPD and is comparable to other existing detection models on benchmark datasets regarding Area Under the Curves (AUCs) of Receiver Operating Characteristic (ROC).
    
[^28]: 随机投影和符号随机投影的差分隐私算法

    Differential Privacy with Random Projections and Sign Random Projections. (arXiv:2306.01751v1 [cs.CR])

    [http://arxiv.org/abs/2306.01751](http://arxiv.org/abs/2306.01751)

    本文提出了一系列差分隐私算法，其中iDP-SignRP算法在个体差分隐私设置下效果显著，DP-SignOPORP算法改进了现有算法，DP-OPORP算法表现最优，iDP提供了一种适用于特定数据集的隐私保护解决方案。

    

    本文提出了一系列基于随机投影（RP）的差分隐私（DP）算法，适用于机器学习、数据挖掘和信息检索等各种应用。其中，基于符号随机投影（SignRP）的iDP-SignRP算法在个体差分隐私（iDP）设置下非常有效，而DP-SignOPORP算法在标准DP设置下利用“一次排列+一次随机投影”（OPORP）极大地改进了文献中现有的算法。除不考虑符号之外，在DP-RP家族中，DP-OPORP算法表现最佳。iDP（个体差分隐私）的概念仅适用于特定的数据集。虽然iDP不是严格的DP，但在某些应用中（如向小组用户发布包括嵌入信息或个性化推荐等内容的数据集，而不泄露不属于该组的个人的任何私人信息）可能很有用。

    In this paper, we develop a series of differential privacy (DP) algorithms from a family of random projections (RP), for general applications in machine learning, data mining, and information retrieval. Among the presented algorithms, \textbf{iDP-SignRP} is remarkably effective under the setting of ``individual differential privacy'' (iDP), based on sign random projections (SignRP). Also, \textbf{DP-SignOPORP} considerably improves existing algorithms in the literature under the standard DP setting, using ``one permutation + one random projection'' (OPORP), where OPORP is a variant of the celebrated count-sketch method with fixed-length binning and normalization. Without taking signs, among the DP-RP family, \textbf{DP-OPORP} achieves the best performance.  The concept of iDP (individual differential privacy) is defined only on a particular dataset of interest. While iDP is not strictly DP, iDP might be useful in certain applications, such as releasing a dataset (including sharing embe
    
[^29]: 在缺少球形替代方案下测试离散分布直方图均匀性的极小极大风险

    The minimax risk in testing the histogram of discrete distributions for uniformity under missing ball alternatives. (arXiv:2305.18111v1 [math.ST])

    [http://arxiv.org/abs/2305.18111](http://arxiv.org/abs/2305.18111)

    研究了离散分布样本对于类别间的均匀分布拟合问题下的极小极大风险，在缺少球形替代方案的情况下进行了讨论，通过离散直方图进行检验，获得了一种具有精确刻画的检验方法，并在实证研究中表现出了显著性。

    

    我们考虑测试一个来自许多类别的离散样本对于类别间的均匀分布拟合的问题。作为另一类替代假设，我们考虑去除半径为$\epsilon$的$\ell_p$球形替代方案，其中$p\leq 2$。我们给出了基于直方图（缺失类别、单例、碰撞的数量）的检验在样本数和维数趋向无穷大，$\epsilon\to0$时，渐进极小极大风险的一个精确刻画。例如，当$p=1$且期望样本数$n$与类别数$N$的比值很小（也称为“次线性”区域）时，渐进极小极大风险$R^*_\epsilon$趋近于$2\bar{\Phi}\left(n\epsilon^2/\sqrt{8N}\right)$，其中$\bar{\Phi}(x)$是正态残存函数。在一系列问题参数范围内的实证研究表明，这个估计在有限样本中很精确，并且我们的检验显著。

    We consider the problem of testing the fit of a discrete sample of items from many categories to the uniform distribution over the categories. As a class of alternative hypotheses, we consider the removal of an $\ell_p$ ball of radius $\epsilon$ around the uniform rate sequence for $p \leq 2$. We deliver a sharp characterization of the asymptotic minimax risk when $\epsilon \to 0$ as the number of samples and number of dimensions go to infinity, for testing based on the occurrences' histogram (number of absent categories, singletons, collisions, ...). For example, for $p=1$ and in the limit of a small expected number of samples $n$ compared to the number of categories $N$ (aka "sub-linear" regime), the minimax risk $R^*_\epsilon$ asymptotes to $2 \bar{\Phi}\left(n \epsilon^2/\sqrt{8N}\right) $, with $\bar{\Phi}(x)$ the normal survival function. Empirical studies over a range of problem parameters show that this estimate is accurate in finite samples, and that our test is significantly 
    
[^30]: DIVA：基于狄利克雷过程的变分自编码器的增量深度聚类算法

    DIVA: A Dirichlet Process Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder. (arXiv:2305.14067v1 [cs.LG])

    [http://arxiv.org/abs/2305.14067](http://arxiv.org/abs/2305.14067)

    本文提出了DIVA算法，一个基于狄利克雷过程的增量深度聚类框架，利用无限混合高斯作为先验，并利用一种记忆化的在线变分推理方法实现簇的动态适应移动，而不需要先知道特征的数量。该算法表现优越，特别是在增量特征的情况下。

    

    基于生成模型的深度聚类框架在分类复杂数据方面表现出色，但在处理动态和复杂特征方面受到限制，因为它们需要先知道簇的数量。本文提出了一个非参数深度聚类框架，采用无限混合高斯作为先验。我们的框架利用一种记忆化的在线变分推理方法，实现了簇的“出生”和“合并”移动，使我们的框架能够以“动态适应”的方式聚类数据，而不需要先知道特征的数量。我们把该框架命名为DIVA，即基于狄利克雷过程的增量深度聚类框架的变分自编码器。我们的框架在分类具有动态变化特征的复杂数据方面表现优越，特别是在增量特征的情况下，超过了最先进的基准。

    Generative model-based deep clustering frameworks excel in classifying complex data, but are limited in handling dynamic and complex features because they require prior knowledge of the number of clusters. In this paper, we propose a nonparametric deep clustering framework that employs an infinite mixture of Gaussians as a prior. Our framework utilizes a memoized online variational inference method that enables the "birth" and "merge" moves of clusters, allowing our framework to cluster data in a "dynamic-adaptive" manner, without requiring prior knowledge of the number of features. We name the framework as DIVA, a Dirichlet Process-based Incremental deep clustering framework via Variational Auto-Encoder. Our framework, which outperforms state-of-the-art baselines, exhibits superior performance in classifying complex data with dynamically changing features, particularly in the case of incremental features.
    
[^31]: 超越数据重新加权：核矩法估计

    Estimation Beyond Data Reweighting: Kernel Method of Moments. (arXiv:2305.10898v1 [cs.LG])

    [http://arxiv.org/abs/2305.10898](http://arxiv.org/abs/2305.10898)

    本论文提出了一种新的核矩法估计器，称为KMM，其用于超越数据重新加权的矩方法模型，解除了关于使用 $\varphi$-散度相关的限制。

    

    在机器学习与统计学等多个领域中都会出现矩约束和条件对应，其中，广义矩法（GMM）作为一个估计模型已经引起了人们的关注。然而，往往由于使用 $\varphi$-散度的相关限制将候选分布限制为数据样本的重新加权。而本论文提出了一种新的矩估计方法——基于最大均值偏差的经验似然估计器，即核矩法(KMM)，其实现超越了对数据的重新加权。

    Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\varphi$-divergence to an empirical distribution. However, the use of $\varphi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment
    
[^32]: 任意深度的一维神经网络的不动点

    Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])

    [http://arxiv.org/abs/2303.12814](http://arxiv.org/abs/2303.12814)

    本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    

    本文介绍了一个在$\mathbb{R}$上具有合成性且包含对数S型函数的新函数类。我们使用这个类来证明具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点。虽然这样的神经网络远离实际应用，但我们能够完全理解它们的不动点，并为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
    
[^33]: KL散度中离散分布估计的集中界限

    Concentration Bounds for Discrete Distribution Estimation in KL Divergence. (arXiv:2302.06869v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06869](http://arxiv.org/abs/2302.06869)

    本文提供了拉普拉斯估计器的集中界限，讨论了在KL散度中离散分布估计的问题，并实现了更优的结果。

    

    我们研究了在KL散度中离散分布估计的问题，并为拉普拉斯估计器提供了集中界限。当$n \geq k$时，我们展示了从平均值偏差的缩放为$\sqrt{k} / n$，这比先前最好的结果$k/n$有所改进。我们还建立了一个匹配的下界，表明我们的界限在多项式对数因子上是紧密的。

    We study the problem of discrete distribution estimation in KL divergence and provide concentration bounds for the Laplace estimator. We show that the deviation from mean scales as $\sqrt{k}/n$ when $n \ge k$, improving upon the best prior result of $k/n$. We also establish a matching lower bound that shows that our bounds are tight up to polylogarithmic factors.
    
[^34]: Density-Softmax: 在分布变化下提高不确定性估计的快速确定性方法

    Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts. (arXiv:2302.06495v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06495](http://arxiv.org/abs/2302.06495)

    本文提出了一种称为Density-Softmax的快速确定性方法，通过将密度函数与softmax结合来提高分布变化下的校准不确定性估计，具有较高的效率和可行性

    

    常见确定性深度学习模型在分布变化下存在较大的过度自信问题，概率方法虽然能缓解此问题但计算效率不佳。本文提出Density-Softmax方法，通过将密度函数与softmax结合，以快速且轻量级的方式提高校准不确定性估计。该方法利用潜在表示的似然值，在测试时在远离训练样本时增加不确定性。在理论证明和实验上，Density-Softmax证明了在使用神经网络的情况下可以实现高质量的不确定性估计，从而减少了标准softmax的过度自信。

    Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-
    
[^35]: 如何信任您的扩散模型：一种凸优化方法应对符合风险控制的因式分解模型

    How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control. (arXiv:2302.03791v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03791](http://arxiv.org/abs/2302.03791)

    本文提出了一种风险控制预测集（RCPS）程序的推广，称为$K$-RCPS，它允许为任何扩散模型提供逐个校准的未来样本间隔，并控制相对于基准真实图像的某种风险概念，同时保持最小平均区间长度。

    

    基于分数的生成建模方法，简称扩散模型，在多个重要领域和任务中继续增长。尽管它们提供了来自经验分布的高质量和多样化样本，但在其负责任地用于关键场景方面的可靠性和可信度仍存在重要问题。收敛预测是一种现代工具，用于为任何黑盒子预测器构建有限样本、分布自由的不确定性保证。在这项工作中，我们专注于图像到图像回归任务，并提出了一种风险控制预测集（RCPS）程序的推广，我们称之为$K$-RCPS，它允许$(i)$为任何扩散模型提供逐个校准的未来样本间隔，并$(ii)$控制相对于基准真实图像的某种风险概念，同时保持最小平均区间长度。与现有的收敛风险控制过程不同，我们的过程依靠一种新型的凸优化公式，使其具有计算效率和易于实现的特点。我们在几个图像到图像回归任务上使用得分为基础的生成建模方法来说明我们的程序的有效性，展示了高度校准和良好控制的预测间隔。

    Score-based generative modeling, informally referred to as diffusion models, continue to grow in popularity across several important domains and tasks. While they provide high-quality and diverse samples from empirical distributions, important questions remain on the reliability and trustworthiness of these sampling procedures for their responsible use in critical scenarios. Conformal prediction is a modern tool to construct finite-sample, distribution-free uncertainty guarantees for any black-box predictor. In this work, we focus on image-to-image regression tasks and we present a generalization of the Risk-Controlling Prediction Sets (RCPS) procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise calibrated intervals for future samples of any diffusion model, and $(ii)$ control a certain notion of risk with respect to a ground truth image with minimal mean interval length. Differently from existing conformal risk control procedures, ours relies on a novel convex opti
    
[^36]: 同质性在图卷积网络的双下降泛化中的调制作用

    Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13069](http://arxiv.org/abs/2212.13069)

    本文通过使用统计物理和随机矩阵理论的分析工具，精确地表征了简单图卷积网络在背景随机块模型上的泛化，提出了同质性在图卷积网络的泛化中的调制作用。

    

    图神经网络是用于关系数据集（如代谢、交通和社交网络）的最成功的机器学习模型之一。然而，它们对数据中编码的各种交互的强大泛化的决定因素并不为人所知。来自统计学习理论的方法无法解释出现的现象，如双下降或风险取决于交互性质的问题。我们使用统计物理和随机矩阵理论的分析工具来精确地表征简单图卷积网络在背景随机块模型上的泛化。导出的曲线现象学上十分丰富：它们解释了同质性和异质性学习之间的区别，并预测了最近作品所质疑的GNN中双下降现象的存在。我们展示了风险如何取决于图中的噪声、特征中的噪声和用于训练的节点比例之间的相互作用。我们的分析为理解同质性如何调制图神经网络的泛化提供了第一步。

    Graph neural networks are among the most successful machine learning models for relational datasets like metabolic, transportation, and social networks. Yet the determinants of their strong generalization for diverse interactions encoded in the data are not well understood. Methods from statistical learning theory do not explain emergent phenomena such as double descent or the dependence of risk on the nature of interactions. We use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. The derived curves are phenomenologically rich: they explain the distinction between learning on homophilic and heterophilic and they predict double descent whose existence in GNNs has been questioned by recent work. We show how risk depends on the interplay between the noise in the graph, noise in the features, and the proportion of nodes used for training. Our analysis pr
    
[^37]: 在混淆下的离线策略评估和优化。

    Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16583](http://arxiv.org/abs/2211.16583)

    该论文致力于解决离线强化学习中混淆变量导致策略评估和优化存在挑战的问题，包括无法获得一致价值估计和样本复杂度的保证，作者提出了具有保证的下限算法和局部收敛的改进算法。

    

    在离线强化学习中，评估和优化策略在存在未观察到的混淆变量时是一个备受关注的问题。使用传统的离线强化学习方法来处理混淆问题不仅可能导致糟糕的决策和策略，而且在关键应用领域如医疗和教育中可能会产生灾难性的影响。我们勾勒了混淆的 MDP 离线策略评估的面貌，并根据混淆对数据收集策略的时间演变和影响来区分混淆的假设。在一些情况下，我们确定了一些无法获得一致价值估计的情况，并提供和讨论了计算具有保证的下限的算法。当一致的估计可行时，我们提供了样本复杂度的保证。我们还提出了新的离线策略改进算法，并证明了局部收敛的保证。最后，我们在格子世界和模拟医疗场景中对算法进行了实验评估。

    Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but can also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on their time-evolution and effect on the data-collection policies. We determine when consistent value estimates are not achievable, providing and discussing algorithms to estimate lower bounds with guarantees in those cases. When consistent estimates are achievable, we provide sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on gridworld and a simulated healthcare settin
    
[^38]: 无分布假设的节点分类预测集

    Distribution Free Prediction Sets for Node Classification. (arXiv:2211.14555v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14555](http://arxiv.org/abs/2211.14555)

    本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类，证明了提供了比简单的符合预测应用更加紧致和良好校准的预测集。

    

    图神经网络通常可以在许多重要的真实数据集上达到高精度分类的效果，但其无法提供严格的预测不确定性定义。由于图结构引起的数据点依赖性，量化GNN模型的置信度很困难。本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类。我们对现有的换位分类方法进行了改进，通过适当加权符合分数来反映网络结构。通过在常用标准基准数据集上使用流行的GNN模型进行实验，我们证明了我们的方法提供了比简单的符合预测应用更加紧致和良好校准的预测集。

    Graph Neural Networks (GNNs) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. Quantifying the confidence of GNN models is difficult due to the dependence between datapoints induced by the graph structure.  We leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. We do this by taking an existing approach for conformal classification that relies on \textit{exchangeable} data and modifying it by appropriately weighting the conformal scores to reflect the network structure. We show through experiments on standard benchmark datasets using popular GNN models that our approach provides tighter and better calibrated prediction sets than a naive application of conformal prediction.
    
[^39]: 基于超图的机器学习集成网络入侵检测系统

    A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System. (arXiv:2211.03933v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.03933](http://arxiv.org/abs/2211.03933)

    该论文提出了一种基于超图的机器学习集成网络入侵检测系统，使用超图捕捉端口扫描攻击的演化模式，并使用派生的度量来训练NIDS，从而允许在高精度、高准确率、高召回率性能下实时监测和检测端口扫描活动、其他类型的攻击和敌对入侵，解决了传统NIDS面临的挑战。

    

    网络入侵检测系统(NIDS)在检测恶意攻击时仍然面临挑战。NIDS通常在离线状态下开发，但面对自动生成的端口扫描渗透尝试时，会导致从对手适应到NIDS响应的显着时间滞后。为了解决这些问题，我们使用以Internet协议地址和目标端口为重点的超图来捕捉端口扫描攻击的演化模式。然后使用派生的基于超图的度量来训练一个集成机器学习(ML)的NIDS，从而允许在高精度、高准确率、高召回率性能下实时调整，监测和检测端口扫描活动、其他类型的攻击和敌对入侵。这个ML自适应的NIDS是通过以下几个部分的组合开发出来的：(1)入侵示例，(2)NIDS更新规则，(3)触发NIDS重新训练请求的攻击阈值选择，以及(4)在没有先前网络性质知识的情况下的生产环境。

    Network intrusion detection systems (NIDS) to detect malicious attacks continue to meet challenges. NIDS are often developed offline while they face auto-generated port scan infiltration attempts, resulting in a significant time lag from adversarial adaption to NIDS response. To address these challenges, we use hypergraphs focused on internet protocol addresses and destination ports to capture evolving patterns of port scan attacks. The derived set of hypergraph-based metrics are then used to train an ensemble machine learning (ML) based NIDS that allows for real-time adaption in monitoring and detecting port scanning activities, other types of attacks, and adversarial intrusions at high accuracy, precision and recall performances. This ML adapting NIDS was developed through the combination of (1) intrusion examples, (2) NIDS update rules, (3) attack threshold choices to trigger NIDS retraining requests, and (4) a production environment with no prior knowledge of the nature of network 
    
[^40]: 隐式模型、潜在压缩、内在偏差和廉价午餐在社区检测中的应用

    Implicit models, latent compression, intrinsic biases, and cheap lunches in community detection. (arXiv:2210.09186v6 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2210.09186](http://arxiv.org/abs/2210.09186)

    本文提出了一种将社区检测目标与其对应的隐式网络生成模型相联系的解决方案，可以计算网络在任意目标下的描述长度，比较不同算法的性能，同时还可以访问隐式模型。

    

    社区检测的任务旨在将网络划分为节点集群，以总结其大规模结构，已经引出了许多具有不同目标的竞争算法。 一些社区检测方法是推断性的，通过概率生成模型明确地导出聚类目标，而其他方法是描述性的，根据特定应用的目标将网络分成子集，这使得在同一规模下比较这些方法变得具有挑战性。本文提出了将任何社区检测目标（推断性或描述性）与其相应的隐式网络生成模型相联系的解决方案。这使我们能够计算网络及其在任意目标下的分区的描述长度，无需“地面实况”标签即可比较不同算法的性能，同时还可以访问隐式模型，这是其他方法所不具备的。

    The task of community detection, which aims to partition a network into clusters of nodes to summarize its large-scale structure, has spawned the development of many competing algorithms with varying objectives. Some community detection methods are inferential, explicitly deriving the clustering objective through a probabilistic generative model, while other methods are descriptive, dividing a network according to an objective motivated by a particular application, making it challenging to compare these methods on the same scale. Here we present a solution to this problem that associates any community detection objective, inferential or descriptive, with its corresponding implicit network generative model. This allows us to compute the description length of a network and its partition under arbitrary objectives, providing a principled measure to compare the performance of different algorithms without the need for "ground truth" labels. Our approach also gives access to instances of the
    
[^41]: 基于时变有向网络的分散式超梯度计算

    Decentralized Hyper-Gradient Computation over Time-Varying Directed Networks. (arXiv:2210.02129v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.02129](http://arxiv.org/abs/2210.02129)

    本文提出了一种基于时变有向网络的分散式超梯度计算方法，避免了静态无向网络通信 Hessian 矩阵导致的高通信成本和无法使用时变有向网络优势的问题。

    

    本文解决了分散式联邦学习中估计超梯度时的通信问题。在分散式联邦学习中，超梯度量化了全局共享最优模型的性能如何受到客户端超参数扰动的影响。在先前的工作中，客户端通过在静态无向网络上通信 Hess 矩阵来跟踪这种影响，导致了（i）过高的通信成本和（ii）不能利用更高效和更强大的网络，即时变有向网络。为了解决这些问题，我们引入了一个基于模型参数和梯度的平均操作的 FL 替代性优化条件。然后，我们采用 Push-Sum 作为平均操作，在时变有向网络上进行共识优化技术。因此，从我们的最优条件推导出的超梯度估计器具有两个理想特性，（i）它只需要 Push-Sum 通信

    This paper addresses the communication issues when estimating hyper-gradients in decentralized federated learning (FL). Hyper-gradients in decentralized FL quantifies how the performance of globally shared optimal model is influenced by the perturbations in clients' hyper-parameters. In prior work, clients trace this influence through the communication of Hessian matrices over a static undirected network, resulting in (i) excessive communication costs and (ii) inability to make use of more efficient and robust networks, namely, time-varying directed networks. To solve these issues, we introduce an alternative optimality condition for FL using an averaging operation on model parameters and gradients. We then employ Push-Sum as the averaging operation, which is a consensus optimization technique for time-varying directed networks. As a result, the hyper-gradient estimator derived from our optimality condition enjoys two desirable properties; (i) it only requires Push-Sum communication of
    
[^42]: 共轭自然选择

    Conjugate Natural Selection. (arXiv:2208.13898v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13898](http://arxiv.org/abs/2208.13898)

    本文证明了Fisher-Rao自然梯度下降最佳逼近连续时间复制子方程，这一对应关系称为“共轭自然选择”，为进化计算提供了替代方法，同时提供了连续贝叶斯推理的最佳近似。

    

    我们证明了Fisher-Rao自然梯度下降（FR-NGD）最佳逼近了连续时间复制子方程（一种基本的进化动力学模型），并将此对应关系称为“共轭自然选择”。该对应关系为在连续或高维度假设空间上进行进化计算提供了替代方法。作为一个特例，FR-NGD还提供了连续贝叶斯推理的最佳近似，当假设基于预测实际观测结果而相互竞争时。在这种情况下，该方法避免了计算先验概率的需要。我们通过一个非凸优化问题和一个具有时变参数的随机过程的系统识别任务演示了我们的发现。

    We prove that Fisher-Rao natural gradient descent (FR-NGD) optimally approximates the continuous time replicator equation (an essential model of evolutionary dynamics), and term this correspondence "conjugate natural selection". This correspondence promises alternative approaches for evolutionary computation over continuous or high-dimensional hypothesis spaces. As a special case, FR-NGD also provides the optimal approximation of continuous Bayesian inference when hypotheses compete on the basis of predicting actual observations. In this case, the method avoids the need to compute prior probabilities. We demonstrate our findings on a non-convex optimization problem and a system identification task for a stochastic process with time-varying parameters.
    
[^43]: 隐藏混淆因素下剂量响应的部分识别

    Partial Identification of Dose Responses with Hidden Confounders. (arXiv:2204.11206v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2204.11206](http://arxiv.org/abs/2204.11206)

    本论文提出了一种新方法，用于界定无法通过点估计进行识别的连续值处理效应估计范围，以解决存在隐藏混淆因素的问题。

    

    从观测数据中推断连续值处理的因果效应是一项关键任务，有望更好地为政策和决策制定者提供信息。 确定这些效应所需的关键假设是包括所有混淆变量——处理和结果的因果父母——作为协变量。 不幸的是，仅凭观测数据，我们无法确定这个标准是否得到满足。 当混淆变量被隐藏时，敏感性分析提供了一种原则性的方式来为因果估计提供界限。 虽然在离散值处理的灵敏度分析中受到了广泛关注，但对于连续值处理，却付出了更少的关注。 我们提出了新的方法，用于界定无法通过点估计进行识别的平均和条件平均连续值处理效应估计的范围，因为存在隐藏的混淆。 在多个数据集上进行的半合成基准测试显示出，我们的方法提供了更紧密的覆盖范围。

    Inferring causal effects of continuous-valued treatments from observational data is a crucial task promising to better inform policy- and decision-makers. A critical assumption needed to identify these effects is that all confounding variables -- causal parents of both the treatment and the outcome -- are included as covariates. Unfortunately, given observational data alone, we cannot know with certainty that this criterion is satisfied. Sensitivity analyses provide principled ways to give bounds on causal estimates when confounding variables are hidden. While much attention is focused on sensitivity analyses for discrete-valued treatments, much less is paid to continuous-valued treatments. We present novel methodology to bound both average and conditional average continuous-valued treatment-effect estimates when they cannot be point identified due to hidden confounding. A semi-synthetic benchmark on multiple datasets shows our method giving tighter coverage of the true dose-response c
    
[^44]: 随机响应私有置信集的非参数扩展

    Nonparametric extensions of randomized response for private confidence sets. (arXiv:2202.08728v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2202.08728](http://arxiv.org/abs/2202.08728)

    本文提出了一种随机响应机制的扩展，可在局部差分隐私约束下计算非参数非渐进统计推断，由此得到的结果可用于生成效率高的置信区间和时间均匀置信序列。利用这些方法可以进行实证研究并产生私有模拟。

    

    本文提出了一种在局部差分隐私（LDP）约束下执行非参数、非渐进统计推断的方法，用于计算具有均值$\mu^\star$的有界观测$(X_1,\dots,X_n)$的置信区间（CI）和时间均匀置信序列（CS），当只有访问私有数据$(Z_1,\dots,Z_n)$时。为了实现这一点，我们引入了一个非参数的、顺序交互的 Warner 的著名“随机响应”机制的推广，为任意有界随机变量满足 LDP，并提供 CIs 和 CSs，用于访问所得私有化的观测值的均值。例如，我们的结果在固定时间和时间均匀区域都产生了 Hoeffding 不等式的私有模拟。我们将这些 Hoeffding  类型的 CSs 扩展到捕获时间变化（非平稳）的均值，最后说明了如何利用这些方法进行实证。

    This work derives methods for performing nonparametric, nonasymptotic statistical inference for population means under the constraint of local differential privacy (LDP). Given bounded observations $(X_1, \dots, X_n)$ with mean $\mu^\star$ that are privatized into $(Z_1, \dots, Z_n)$, we present confidence intervals (CI) and time-uniform confidence sequences (CS) for $\mu^\star$ when only given access to the privatized data. To achieve this, we introduce a nonparametric and sequentially interactive generalization of Warner's famous ``randomized response'' mechanism, satisfying LDP for arbitrary bounded random variables, and then provide CIs and CSs for their means given access to the resulting privatized observations. For example, our results yield private analogues of Hoeffding's inequality in both fixed-time and time-uniform regimes. We extend these Hoeffding-type CSs to capture time-varying (non-stationary) means, and conclude by illustrating how these methods can be used to conduct
    
[^45]: 深度线性网络的精确解析解

    Exact Solutions of a Deep Linear Network. (arXiv:2202.04777v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.04777](http://arxiv.org/abs/2202.04777)

    本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，结果表明权重衰减与模型架构的强烈交互作用会在多于1个隐藏层的网络中创建不良极小值，并表明常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。

    

    本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，这是理解神经网络理论中的基础模型。我们的结果表明，在深度神经网络架构中，零是一个特殊的点。我们展示了权重衰减与模型架构的强烈交互作用，并能够在具有超过 $1$ 个隐藏层的网络中创建不良极小值，这与仅有 $1$ 个隐藏层的网络有质的不同。实际上，我们的结果意味着常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。

    This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general.
    
[^46]: 因子增强的树集合方法

    Factor-augmented tree ensembles. (arXiv:2111.14000v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.14000](http://arxiv.org/abs/2111.14000)

    本文提出了一种因子增强的树集合方法，能够处理多种不规则预测变量，为处理宏观金融问题提供一种可靠的方法。

    

    本文提出了利用状态空间方法提取潜在稳态因子来扩展时间序列回归树信息集的方法。通过这样做，该方法将时间序列回归树的应用扩展到两个方面。第一，它可以处理测量误差、非平稳趋势、季节性和/或缺失观测等不规则的预测变量。第二，它提供了一种明确的利用领域专业理论来指导时间序列回归树的方法。实证结果表明，这些因子增强的树集合方法在宏观金融问题方面提供了一种可靠的方法。本文重点介绍了美国股票波动率与商业周期之间的先导滞后效应。

    This manuscript proposes to extend the information set of time-series regression trees with latent stationary factors extracted via state-space methods. In doing so, this approach generalises time-series regression trees on two dimensions. First, it allows to handle predictors that exhibit measurement error, non-stationary trends, seasonality and/or irregularities such as missing observations. Second, it gives a transparent way for using domain-specific theory to inform time-series regression trees. Empirically, ensembles of these factor-augmented trees provide a reliable approach for macro-finance problems. This article highlights it focussing on the lead-lag effect between equity volatility and the business cycle in the United States.
    
[^47]: MARS via LASSO.（arXiv:2111.11694v2 [math.ST] 已更新）

    MARS via LASSO. (arXiv:2111.11694v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2111.11694](http://arxiv.org/abs/2111.11694)

    本文提出了一种自然lasso变体的MARS方法，通过减少对维度的依赖来获得收敛率，并与使用平滑性约束的非参数估计技术联系在一起。

    

    多元自适应回归样条（Multivariate Adaptive Regression Splines，MARS）是Friedman在1991年提出的一种非参数回归方法。MARS将简单的非线性和非加性函数拟合到回归数据上。本文提出并研究了MARS方法的一种自然lasso变体。我们的方法是基于最小二乘估计，通过考虑MARS基础函数的无限维线性组合并强加基于变分的复杂度约束条件来获得函数的凸类。虽然我们的估计是定义为无限维优化问题的解，但其可以通过有限维凸优化来计算。在一些标准设计假设下，我们证明了我们的估计器仅在维度上对数收敛，因此在一定程度上避免了通常的维度灾难。我们还表明，我们的方法自然地与基于平滑性约束的非参数估计技术相联系。

    Multivariate adaptive regression splines (MARS) is a popular method for nonparametric regression introduced by Friedman in 1991. MARS fits simple nonlinear and non-additive functions to regression data. We propose and study a natural lasso variant of the MARS method. Our method is based on least squares estimation over a convex class of functions obtained by considering infinite-dimensional linear combinations of functions in the MARS basis and imposing a variation based complexity constraint. Our estimator can be computed via finite-dimensional convex optimization, although it is defined as a solution to an infinite-dimensional optimization problem. Under a few standard design assumptions, we prove that our estimator achieves a rate of convergence that depends only logarithmically on dimension and thus avoids the usual curse of dimensionality to some extent. We also show that our method is naturally connected to nonparametric estimation techniques based on smoothness constraints. We i
    
[^48]: 应用神经网络解决 Monge-Amp\`ere 方程的迪利克雷问题

    Solving the Dirichlet problem for the Monge-Amp\`ere equation using neural networks. (arXiv:2110.03310v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.03310](http://arxiv.org/abs/2110.03310)

    本文应用神经网络解决 Monge-Amp\`ere 方程的迪利克雷问题，使用深度凸输入神经网络的假设可以用来找到唯一的凸解，方法对奇异点、不连续点和源函数中的噪声具有鲁棒性，在高维情况下也能表现良好。

    

    Monge-Amp\`ere 方程是分析、几何和应用科学中非常重要的一个全非线性偏微分方程。本文应用神经网络解决了与 Monge-Amp\`ere 方程相关的迪利克雷问题，并且展示了使用深度凸输入神经网络的假设可以用来找到唯一的凸解。我们分析了奇异点、不连续点和源函数中的噪声对方法效果的影响，考虑了非平凡域，并研究了方法在高维情况下的性能。我们通过数值分析研究了收敛性并基于稳定性结果给出了误差估计。我们还将此方法与使用惩罚缺乏凸性的损失函数以及标准前馈网络结合使用的替代方法进行了比较。

    The Monge-Amp\`ere equation is a fully nonlinear partial differential equation (PDE) of fundamental importance in analysis, geometry and in the applied sciences. In this paper we solve the Dirichlet problem associated with the Monge-Amp\`ere equation using neural networks and we show that an ansatz using deep input convex neural networks can be used to find the unique convex solution. As part of our analysis we study the effect of singularities, discontinuities and noise in the source function, we consider nontrivial domains, and we investigate how the method performs in higher dimensions. We investigate the convergence numerically and present error estimates based on a stability result. We also compare this method to an alternative approach in which standard feed-forward networks are used together with a loss function which penalizes lack of convexity.
    
[^49]: 随机坐标变换及其在鲁棒机器学习中的应用

    Stochastic coordinate transformations with applications to robust machine learning. (arXiv:2110.01729v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.01729](http://arxiv.org/abs/2110.01729)

    本文提出了一种利用随机坐标变换进行异常检测的新方法，该方法通过层级张量积展开来逼近随机过程，并通过训练机器学习分类器对投影系数进行检测。在基准数据集上的实验表明，该方法胜过现有的最先进方法。

    

    本文介绍了一组新的特征，利用Karhunen-Loeve展开法来识别输入数据的潜在随机行为。这些新特征是通过基于最近的函数数据分析理论进行的坐标变换构建的，用于异常检测。相关的信号分解是用已知优化属性的层级张量积展开来逼近具有有限功能空间的随机过程（随机场）。原则上，这些低维空间可以捕捉给定名义类别的'底层信号'的大部分随机变化，并且可以将来自其它类别的信号拒绝为随机异常。通过名义类别的层级有限维展开，构建了一系列用于检测异常信号组件的正交嵌套子空间。然后使用这些子空间中的投影系数来训练用于异常检测的机器学习（ML）分类器。我们在几个基准数据集上评估所提出的方法，结果表明其胜过现有的最先进方法。

    In this paper we introduce a set of novel features for identifying underlying stochastic behavior of input data using the Karhunen-Loeve expansion. These novel features are constructed by applying a coordinate transformation based on the recent Functional Data Analysis theory for anomaly detection. The associated signal decomposition is an exact hierarchical tensor product expansion with known optimality properties for approximating stochastic processes (random fields) with finite dimensional function spaces. In principle these low dimensional spaces can capture most of the stochastic behavior of `underlying signals' in a given nominal class, and can reject signals in alternative classes as stochastic anomalies. Using a hierarchical finite dimensional expansion of the nominal class, a series of orthogonal nested subspaces is constructed for detecting anomalous signal components. Projection coefficients of input data in these subspaces are then used to train a Machine Learning (ML) clas
    
[^50]: WildWood：一种新的随机森林算法

    WildWood: a new Random Forest algorithm. (arXiv:2109.08010v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.08010](http://arxiv.org/abs/2109.08010)

    WildWood是一种新的随机森林算法，使用指数权重聚合包外样本以改进预测，并通过使用直方图策略加速分裂查找，具有比标准RF和极限梯度提升算法更快和更具竞争力的性能。

    

    我们介绍了WildWood（WW）这种新的用于监督学习的集合算法，采用了Random Forest（RF）类型。标准的RF算法使用自助法样本来计算包外（out-of-bag）分数，WW使用这些样本产生改进的预测，给出每个完全生长的树的所有可能子树预测的聚合。这是通过使用在包外样本上计算的指数权重聚合实现的，这些样本由称为上下文树加权（context tree weighting）的算法精确且高效地计算出来。值得注意的是，与其他成熟的集合方法，如标准RF和极限梯度提升（extreme gradient boosting）算法相比，WildWoods快速且具有竞争力，其中包括采用加速分裂查找的直方图策略。

    We introduce WildWood (WW), a new ensemble algorithm for supervised learning of Random Forest (RF) type. While standard RF algorithms use bootstrap out-of-bag samples to compute out-of-bag scores, WW uses these samples to produce improved predictions given by an aggregation of the predictions of all possible subtrees of each fully grown tree in the forest. This is achieved by aggregation with exponential weights computed over out-of-bag samples, that are computed exactly and very efficiently thanks to an algorithm called context tree weighting. This improvement, combined with a histogram strategy to accelerate split finding, makes WW fast and competitive compared with other well-established ensemble methods, such as standard RF and extreme gradient boosting algorithms.
    
[^51]: 异常最快变点检测中的赌博机方法

    Bandit Quickest Changepoint Detection. (arXiv:2107.10492v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.10492](http://arxiv.org/abs/2107.10492)

    基于赌博机的方法可以有效平衡探索和利用，实现对一组传感器的最快变点检测，从而节省资源和成本。

    

    许多工业和安全应用程序使用一组传感器来检测时间行为模式中的突变。这些突变通常在局部表现出来，仅使一小部分传感器有信息。由于资源限制，监控每个传感器可能很昂贵，这是进行赌博机最快变点检测问题的动机，其中选择一系列传感动作（或传感器），并且只观察与所选动作对应的测量。我们推导了有限参数概率分布类别的检测延迟的信息理论下界。我们随后提出了一种计算有效的在线感知方案，它无缝平衡了对不同传感选项的探索需求与询问信息动作的利用。我们推导了所提出方案的预期延迟界限，同时证明了这些界限与我们的信息理论下界在低维空间的匹配性。

    Many industrial and security applications employ a suite of sensors for detecting abrupt changes in temporal behavior patterns. These abrupt changes typically manifest locally, rendering only a small subset of sensors informative. Continuous monitoring of every sensor can be expensive due to resource constraints, and serves as a motivation for the bandit quickest changepoint detection problem, where sensing actions (or sensors) are sequentially chosen, and only measurements corresponding to chosen actions are observed. We derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. We then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. We derive expected delay bounds for the proposed scheme and show that these bounds match our information-theoretic lower bounds at low
    
[^52]: 核细化

    Kernel Thinning. (arXiv:2105.05842v9 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.05842](http://arxiv.org/abs/2105.05842)

    核细化是一种更有效的压缩分布的方法，它可以将$n$点近似的分布压缩到具有可比较最坏积分误差的$\sqrt{n}$点近似，其亚指数保证类似于在$[0,1]^d$上均匀$\mathbb{P}$的经典准蒙特卡罗误差率，但适用于$\mathbb{R}^d$上的一般分布。

    

    我们介绍了核细化，一种比独立同分布采样或标准细化更有效地压缩分布$\mathbb{P}$的新方法。给定一个合适的再生核$\mathbf{k}_{\star}$和$\mathcal{O}(n^2)$时间，核细化将一个$n$点近似的$\mathbb{P}$压缩成一个具有与相关再生核希尔伯特空间中的可比较最坏积分误差的$\sqrt{n}$点近似。在概率上，紧支撑的$\mathbb{P}$的积分误差最大差别为$\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$，在$\mathbb{R}^d$上的亚指数$\mathbb{P}$为$\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$。相比之下，来自$\mathbb{P}$的等大小i.i.d.样本面临$\Omega(n^{-1/4})$的积分误差。我们的亚指数保证类似于在$[0,1]^d$上均匀$\mathbb{P}$的经典准蒙特卡罗误差率，但适用于$\mathbb{R}^d$上的一般分布和一个大

    We introduce kernel thinning, a new procedure for compressing a distribution $\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel $\mathbf{k}_{\star}$ and $\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\mathbb{P}$ into a $\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. The maximum discrepancy in integration error is $\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$ in probability for compactly supported $\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-1/4})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\mathbb{R}^d$ and a wid
    
[^53]: 使用I-priors进行加性交互作用建模

    Additive interaction modelling using I-priors. (arXiv:2007.15766v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2007.15766](http://arxiv.org/abs/2007.15766)

    本文提出了扩展I-prior方法以解决加性回归模型中的交互作用的挑战，该方法理论和实践上都比其他方法更优，可以使用EM算法估计尺度参数，并提出了交互模型的简明规范。

    

    加性回归模型与交互作用在文献中得到广泛研究，采用了样条或高斯过程回归等方法。然而，由于存在许多平滑参数并且缺乏合适的准则，这些方法可能会对估计和模型选择构成挑战。本文提出通过将I-prior方法（Bergsma，2020）扩展到多个可能为多维的协变量来解决这些挑战。I-prior方法在理论和实践上都比其他方法（如高斯过程回归和Tikhonov正则化）具有一些优势。特别地，I-prior是一个适当的先验，基于最少的假设，产生一个可接受的后验均值，并且可以使用具有简单E和M步的EM算法来估计尺度（或平滑）参数。此外，我们介绍了一种交互模型的简明规范，它具有两个好处：（i）它降低了尺度参数的数量

    Additive regression models with interactions are widely studied in the literature, using methods such as splines or Gaussian process regression. However, these methods can pose challenges for estimation and model selection, due to the presence of many smoothing parameters and the lack of suitable criteria. We propose to address these challenges by extending the I-prior methodology (Bergsma, 2020) to multiple covariates, which may be multidimensional. The I-prior methodology has some advantages over other methods, such as Gaussian process regression and Tikhonov regularization, both theoretically and practically. In particular, the I-prior is a proper prior, is based on minimal assumptions, yields an admissible posterior mean, and estimation of the scale (or smoothing) parameters can be done using an EM algorithm with simple E and M steps. Moreover, we introduce a parsimonious specification of models with interactions, which has two benefits: (i) it reduces the number of scale parameter
    
[^54]: 基于内核的梯度下降算法的自适应停止准则

    Adaptive Stopping Rule for Kernel-based Gradient Descent Algorithms. (arXiv:2001.02879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.02879](http://arxiv.org/abs/2001.02879)

    本文提出了一种自适应的内核梯度下降算法的停止准则，使用经验有效维度来量化增量并推导出可执行的提前停止策略。通过使用积分算子方法得出证明，规则具有优化学习速率的最优性，并且提出的停止策略具有计算上的优势。

    

    本文提出了基于内核的梯度下降算法的自适应停止准则。我们引入了经验有效维度来量化KGD迭代的增量并推导出可实施的提前停止策略。我们在学习理论框架下分析了自适应停止准则的性能。使用最近发展的积分算子方法，我们严格证明了配备此规则的KGD的最优学习速率的最优性。此外，我们还给出了配备所述提前停止规则的KGD的迭代次数的尖锐界限，以说明其计算优势。

    In this paper, we propose an adaptive stopping rule for kernel-based gradient descent (KGD) algorithms. We introduce the empirical effective dimension to quantify the increments of iterations in KGD and derive an implementable early stopping strategy. We analyze the performance of the adaptive stopping rule in the framework of learning theory. Using the recently developed integral operator approach, we rigorously prove the optimality of the adaptive stopping rule in terms of showing the optimal learning rates for KGD equipped with this rule. Furthermore, a sharp bound on the number of iterations in KGD equipped with the proposed early stopping rule is also given to demonstrate its computational advantage.
    
[^55]: 一种三元神经模型用于动态实体相关性排名

    A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/1808.08316](http://arxiv.org/abs/1808.08316)

    这篇论文提出了一种基于神经网络的方法，通过动态评估实体相关性，利用集体注意作为监督，能学习到丰富而不同的实体表示，能在大规模数据集上比竞争基线获得更好的结果。

    

    测量实体相关性是许多自然语言处理和信息检索应用的基本任务。之前的研究通常在静态设置和非监督方式下研究实体相关性。然而，现实世界中的实体往往涉及许多不同的关系，因此实体关系随时间变得非常动态。在这项工作中，我们提出了一种基于神经网络的方法来动态评估实体相关性，利用集体注意力作为监督。我们的模型能够在联合框架中学习丰富而不同的实体表示。通过对大规模数据集的广泛实验，我们证明了我们的方法比竞争基线获得了更好的结果。

    Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
    
[^56]: Fischer-Schultz 讲座：随机实验中异质性处理效应的通用机器学习推断，以印度免疫为例

    Fischer-Schultz Lecture: Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments, with an Application to Immunization in India. (arXiv:1712.04802v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1712.04802](http://arxiv.org/abs/1712.04802)

    该论文提出了一种通用的机器学习方法，用于在随机实验中估算和推断异质性处理效应的关键特征，并将其应用于印度免疫计划的数据中，获得有效推断结果。

    

    我们提出了一种策略，用于在随机实验中估算和推断异质性效应的关键特征。这些关键特征包括使用机器学习代理的效应的最佳线性预测器，按影响组排序的平均效应以及最受影响单位的平均特征和最不受影响的单位。该方法在高维设置中有效，其中效应由预测和因果机器学习方法进行代理（但不一定是一致估计的）。我们将这些代理后处理成关键特征的估计值。我们的方法是通用的，可以与有惩罚的方法，神经网络，随机森林，提升树和集成方法一起使用，既可以进行预测也可以进行因果分析。估计和推断基于重复数据分割，以避免过度拟合并实现有效性。我们使用结果的分位聚合来自许多潜在的分割，特别是取p值的中位数和置信区间的中位数和其他分位数。我们将该方法应用于印度一项大型免疫计划的数据中，估计免疫对儿童发病率和死亡率的影响。我们的结果表明，我们的方法在高维度，异质性处理设置中产生有效的推断。

    We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied (but not necessarily consistently estimated) by predictive and causal machine learning methods. We post-process these proxies into estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, neural networks, random forests, boosted trees, and ensemble methods, both predictive and causal. Estimation and inference are based on repeated data splitting to avoid overfitting and achieve validity. We use quantile aggregation of the results across many potential splits, in particular taking medians of p-values and medians and other quantiles of conf
    
[^57]: $l^q$正则化学习的泛化性能是否依赖于$q$？一个否定的例子。

    Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example. (arXiv:1307.6616v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1307.6616](http://arxiv.org/abs/1307.6616)

    该研究表明，在特定的核函数类中，$l^{q}$ 正则化学习在不同阶数 $q$ 下都具有相似的泛化误差界限。

    

    $l^q$-正则化已经被证明是机器学习和统计建模中一种有吸引力的技术。它通过适当缩小系数来提高机器（模型）的泛化（预测）能力。在不同的正则化阶数 $q$ 选择下，$l^q$ 估计器的形状不同。特别地，$l^1$ 导致 LASSO 估计，而 $l^{2}$ 对应于平滑的岭回归。这使得阶数 $q$ 成为应用中的一个潜在调参参数。为了促进 $l^{q}$-正则化的使用，我们打算寻找一种建模策略，可以避免在 $q$ 上进行精细的选择。在这样的精神下，我们将我们的研究置于一个样本相关假设空间（SDHS）下的 $l^{q}$-正则化核学习的一般框架中。对于一类指定的核函数，在 $0<q<\infty$ 的所有 $l^{q}$ 估计值都具有类似的泛化误差界限。这些估计边界是一个...

    $l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. It attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. The shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. In particular, $l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth ridge regression. This makes the order $q$ a potential tuning parameter in applications. To facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. In this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (SDHS). For a designated class of kernel functions, we show that all $l^{q}$ estimators for $0< q < \infty$ attain similar generalization error bounds. These estimated bounds are a
    

