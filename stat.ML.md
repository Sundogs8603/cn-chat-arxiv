# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Transformers are Expressive, But Are They Expressive Enough for Regression?](https://arxiv.org/abs/2402.15478) | Transformer在逼近连续函数方面存在困难，是否真正是通用函数逼近器仍有待考证 |
| [^2] | [GROS: A General Robust Aggregation Strategy](https://arxiv.org/abs/2402.15442) | GROS是一种新的稳健程序，用于在度量空间中组合估计量，具有次高斯特性，通过在样本上进行最小化可在实践中实现。 |
| [^3] | [Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models](https://arxiv.org/abs/2402.15432) | 本文在混合模型中建立了一个通用下界，通过Chernoff散度来表达，将其拓展到具有次指数尾部的混合模型，并证明了迭代算法在这些混合模型中实现了最佳误差率 |
| [^4] | [The Impact of LoRA on the Emergence of Clusters in Transformers](https://arxiv.org/abs/2402.15415) | 本文利用转换器数学框架探讨了LoRA算法对Token聚类结构动态的影响，发现在不同参数下，修改后的注意力矩阵动态的聚类表现出较长时间的显著差异，但仍在短时间内保持密切相似。 |
| [^5] | [Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps](https://arxiv.org/abs/2402.15409) | 在处理拥有潜在变量的稀疏线性回归问题时，通过对协变量进行异质缩放，Lasso方法可以获得强有力的估计保证。 |
| [^6] | [Efficient semi-supervised inference for logistic regression under case-control studies](https://arxiv.org/abs/2402.15365) | 针对病例-对照研究的逻辑回归半监督推断，利用未标记数据可以识别截距参数，解决了截距参数在半监督学习中不可辨识的问题。 |
| [^7] | [Rapid Bayesian identification of sparse nonlinear dynamics from scarce and noisy data](https://arxiv.org/abs/2402.15357) | 提出了一种快速的概率框架，称为贝叶斯-SINDy，用于从有限且嘈杂数据中学习正确的模型方程，并且对参数估计中的不确定性进行量化，特别适用于生物数据和实时系统识别。 |
| [^8] | [Information-Theoretic Safe Bayesian Optimization](https://arxiv.org/abs/2402.15347) | 提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。 |
| [^9] | [Fourier Basis Density Model](https://arxiv.org/abs/2402.15345) | 引入了一种基于受限Fourier基的轻量级、灵活且端到端可训练的概率密度模型，能够有效逼近各种多模态1维密度，表现优于传统的深度因式模型，同时在学习压缩任务中展示了其实用性。 |
| [^10] | [Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates](https://arxiv.org/abs/2402.15344) | 研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与迭代和SFO复杂度之间的关系，并指出使用关键批量大小的SGD可以最小化SFO复杂度 |
| [^11] | [Categorical Deep Learning: An Algebraic Theory of Architectures](https://arxiv.org/abs/2402.15332) | 提出了一种关于深度学习架构的代数理论，应用范畴论构建了一个桥梁，有效地涵盖了神经网络设计的不同风格，同时自然地编码了计算机科学和自动机理论中的许多标准结构。 |
| [^12] | [Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations](https://arxiv.org/abs/2402.15285) | 使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程，提出了一种在生成建模中解决HJB方程的新方法，该方法无需样本，不依赖于归一化常数，并能避免维数灾难。 |
| [^13] | [Classification of compact radio sources in the Galactic plane with supervised machine learning](https://arxiv.org/abs/2402.15232) | 本研究使用射电和红外图像，结合20,000张图像数据集进行监督机器学习，实现对银河平面中紧凑射电源的分类。 |
| [^14] | [Statistical Agnostic Regression: a machine learning method to validate regression models](https://arxiv.org/abs/2402.15213) | 本文提出了一种新的方法，统计无关地评估了线性回归模型，并评估了ML估计在检测方面的表现。 |
| [^15] | [Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control](https://arxiv.org/abs/2402.15194) | 扩散模型的微调方法可以通过最大化奖励函数的价值来以目标导向方式进行微调，但可能会面临奖励崩溃的挑战。 |
| [^16] | [Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2402.15171) | 提出了一种协方差自适应的最小二乘算法，利用在线估计协方差结构，相对于基于代理方差的算法获得改进的遗憾上界，特别在协方差系数全为非负时，能有效地利用半臂反馈，并在各种参数设置下表现优异。 |
| [^17] | [Benchmarking Observational Studies with Experimental Data under Right-Censoring](https://arxiv.org/abs/2402.15137) | 论文通过考虑截尾现象，提出了一种基准测试方法，用于观测研究与随机对照试验中条件平均处理效应的等价性测试，从而验证有效性假设是否成立。 |
| [^18] | [Multi-Armed Bandits with Abstention](https://arxiv.org/abs/2402.15127) | 提出了一个扩展的多臂赌博机问题，引入了弃权选项，并成功设计和分析了算法，实现了渐近和米迷诺下最优。 |
| [^19] | [Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding](https://arxiv.org/abs/2402.15125) | 通过深度展开技术，本文提出的可训练SVGD算法加速了其收敛速度，相比传统SVGD变体表现出更快的收敛速度。 |
| [^20] | [Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification](https://arxiv.org/abs/2402.15115) | 提出一种物理约束的多项式混沌展开方法，将科学机器学习与不确定性量化无缝集成，有效地实现SciML任务中的不确定性量化和在UQ任务中利用SciML提高不确定性评估。 |
| [^21] | [Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities](https://arxiv.org/abs/2402.15053) | 使用对数Sobolev不等式构造的MI下限的贪婪方法在非线性模型优化设计中表现出色 |
| [^22] | [Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration](https://arxiv.org/abs/2402.15019) | 提出了一种新的一致性引导温度缩放（CTS）策略，通过提供源域数据样本之间的相互监督，显著增强了域外（OOD）校准性能。 |
| [^23] | [Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study](https://arxiv.org/abs/2402.15005) | 该研究通过弗莱明汉姆心脏病数据作为案例研究，比较了八种机器学习分类算法在不同训练/测试场景下的预测性能，发现极端梯度提升和支持向量机在训练不平衡数据时存在缺陷。 |
| [^24] | [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992) | 本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。 |
| [^25] | [Verifiable Boosted Tree Ensembles](https://arxiv.org/abs/2402.14988) | 本研究将可验证学习从基本集成方法扩展到高级提升树集成，提出了一个伪多项式时间算法来验证鲁棒性，对基于$L_p$-范数的攻击者具有出色的性能。 |
| [^26] | [On the Performance of Empirical Risk Minimization with Smoothed Data](https://arxiv.org/abs/2402.14987) | 在数据是良好指定和平滑的情况下，对于经验风险最小化（ERM）与平方损失的性能，当类是可从 iid 数据中学习时，ERM能够实现次线性误差。 |
| [^27] | [Nonsmooth Nonparametric Regression via Fractional Laplacian Eigenmaps](https://arxiv.org/abs/2402.14985) | 通过分数拉普拉斯特征映射，开发了针对真实回归函数非光滑情况的非参数回归方法，成功处理了该函数类在$L_2$-分数 Sobolev 空间中的特性，并证明了误差上界为$n^{-\frac{2s}{2s+d}}$。 |
| [^28] | [Comparative Analysis of Data Preprocessing Methods, Feature Selection Techniques and Machine Learning Models for Improved Classification and Regression Performance on Imbalanced Genetic Data](https://arxiv.org/abs/2402.14980) | 研究比较了数据预处理、特征选择和模型选择对训练遗传数据集上模型性能的影响，发现异常值和倾斜会影响模型性能。 |
| [^29] | [Smoothness Adaptive Hypothesis Transfer Learning](https://arxiv.org/abs/2402.14966) | 本文提出了光滑自适应迁移学习（SATL）算法，通过在两个阶段均采用高斯核，使估计器能够适应目标/源及其偏移函数的未知光滑性。 |
| [^30] | [A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems](https://arxiv.org/abs/2402.14959) | 本论文提出了一个多阶段因果框架，融入犯罪行为，用于评估执法系统中的种族偏见，以解决以往研究中存在的限制，对偏见进行量化，并确定主要偏见来源。 |
| [^31] | [In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization](https://arxiv.org/abs/2402.14951) | 该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。 |
| [^32] | [Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket](https://arxiv.org/abs/2402.14029) | 提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。 |
| [^33] | [Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions](https://arxiv.org/abs/2402.08082) | 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。 |
| [^34] | [Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data](https://arxiv.org/abs/2302.00834) | 该论文研究了深度ReLU神经网络在不规则间隔数据上的插值问题，证明了在数据点间距指数级小的情况下需要$\Omega(N)$个参数，同时指出现有的位提取技术无法应用于这种情况。 |
| [^35] | [Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary](https://arxiv.org/abs/2210.16311) | 本文提出了一种名为Group-Nonlinear-Lasso的方法，可以同时估计混合物中的线性系数和特征的非线性参数，并使用证明函数对预测误差提供了高概率界限。 |
| [^36] | [Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees](https://arxiv.org/abs/2210.11327) | 本文提出了一种基于梯度提升决策树的训练动态来评估每个训练实例行为的方法，针对包含大部分表格化或结构化数据的数据集，相较于自信学习、直接启发式和健壮提升算法，取得了最佳结果。 |
| [^37] | [Interventional Causal Representation Learning](https://arxiv.org/abs/2209.11924) | 干预数据有助于因果表示学习，可以通过干预数据中潜在因素支持的几何特征来识别潜在的因果因素。 |
| [^38] | [Efficient error and variance estimation for randomized matrix computations](https://arxiv.org/abs/2207.06342) | 该论文提出了用于随机矩阵计算的高效误差和方差估计方法，可帮助评估输出质量并指导算法参数选择。 |
| [^39] | [On Hypothesis Transfer Learning of Functional Linear Models](https://arxiv.org/abs/2206.04277) | 该研究在函数线性回归下探讨了迁移学习，提出了使用RKHS距离衡量任务相似性，并提出了两种算法来处理迁移，一种需要已知正源，另一种利用聚合技术实现无源信息的稳健传输。同时建立了学习问题的下界，并证明了算法的上界。 |
| [^40] | [Bernstein Flows for Flexible Posteriors in Variational Bayes](https://arxiv.org/abs/2202.05650) | 该论文提出了一种名为伯恩斯块流变分推断（BF-VI）的方法，能够灵活逼近复杂的多元后验，在实验中表现优于其他VI方法。 |
| [^41] | [Causal Discovery from Conditionally Stationary Time Series](https://arxiv.org/abs/2110.06257) | 该论文提出了一种State-Dependent Causal Inference（SDCI）方法，可以处理一类宽泛的非平稳时间序列，成功地回复出潜在的因果依赖关系。 |
| [^42] | [Adversarial Examples Detection with Bayesian Neural Network](https://arxiv.org/abs/2105.08620) | 提出了一种基于贝叶斯神经网络的新框架，利用随机性模拟隐藏层输出分布，从而改善对抗样本检测性能。 |
| [^43] | [User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient](https://arxiv.org/abs/1710.00095) | 该论文分析了具有不准确梯度的 Langevin Monte Carlo 算法的采样问题，并在Wasserstein-2距离中提出了改进的误差保证。 |
| [^44] | [Estimation of partially known Gaussian graphical models with score-based structural priors.](http://arxiv.org/abs/2401.14340) | 本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。 |
| [^45] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | 本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。 |
| [^46] | [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models.](http://arxiv.org/abs/2310.00902) | DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。 |
| [^47] | [Convolutional Deep Kernel Machines.](http://arxiv.org/abs/2309.09814) | 这项研究介绍了一种称为卷积深度核机器的新型核方法，该方法纯粹使用核而不使用特征，通过高效的跨域诱导点近似方案和多种模型变体的设计，达到了在MNIST、CIFAR-10和CIFAR-100上接近甚至超过其他方法的测试准确率。 |
| [^48] | [Simulation-based inference using surjective sequential neural likelihood estimation.](http://arxiv.org/abs/2308.01054) | 我们提出了一种使用全射序列神经似然估计（SSNL）进行基于仿真的推断的新方法，在模型中无法计算似然函数并且只能使用模拟器生成数据的情况下，SSNL通过拟合降维的全射归一化流模型，并将其作为替代似然函数，解决了先前基于似然方法在高维数据集中遇到的问题，并在各种实验中展示了其优越性能。 |
| [^49] | [Learning thermodynamically constrained equations of state with uncertainty.](http://arxiv.org/abs/2306.17004) | 本研究提出了一种数据驱动的机器学习方法，用于学习具有不确定性的热力学约束状态方程，并进行不确定性量化，以提高方程预测的可信度。 |
| [^50] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |

# 详细

[^1]: Transformer是表现力强大的，但是对于回归任务来说表现力足够吗？

    Transformers are Expressive, But Are They Expressive Enough for Regression?

    [https://arxiv.org/abs/2402.15478](https://arxiv.org/abs/2402.15478)

    Transformer在逼近连续函数方面存在困难，是否真正是通用函数逼近器仍有待考证

    

    Transformer已成为自然语言处理中至关重要的技术，在机器翻译和摘要等应用中表现出色。随着它们的广泛应用，一些研究尝试分析Transformer的表现力。神经网络的表现力指的是它能够逼近的函数类。一个神经网络是完全表现力的，如果它可以充当通用函数逼近器。我们尝试分析Transformer的表现力。与现有观点相反，我们的研究结果表明，Transformer在可靠逼近连续函数方面存在困难，依赖于具有可观区间的分段常数逼近。关键问题是：“Transformer是否真正是通用函数逼近器？”为了解决这个问题，我们进行了彻底的调查，通过实验提供理论见解和支持证据。我们的贡献包括了一个理论分析……（摘要未完整）

    arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
    
[^2]: GROS: 一个通用的稳健聚合策略

    GROS: A General Robust Aggregation Strategy

    [https://arxiv.org/abs/2402.15442](https://arxiv.org/abs/2402.15442)

    GROS是一种新的稳健程序，用于在度量空间中组合估计量，具有次高斯特性，通过在样本上进行最小化可在实践中实现。

    

    这篇论文引入了一种新的非常通用的稳健程序，用于在度量空间中组合估计量，称为GROS。该方法类似于众所周知的均值中位数方法，在文献\cite{devroye2016sub}中有描述。首先，样本被分成$K$组。随后，为每个组计算一个估计量。最后，利用稳健程序组合这$K$个估计量。我们证明这个估计量是次高斯的，并得到它的破裂点，即Donoho的意义下。稳健程序涉及一个在一般度量空间上的最小化问题，但我们表明，如果最小化在样本上进行，那么将获得相同（经常数相差）的次高斯性，使得GROS在实践中可行。通过五个模拟研究评估了GROS的性能：第一个研究着重于使用$k$-means进行分类，第二个研究着重于多臂老虎机问题，第三个研究着重于回归问题。第四个是...

    arXiv:2402.15442v1 Announce Type: cross  Abstract: A new, very general, robust procedure for combining estimators in metric spaces is introduced GROS. The method is reminiscent of the well-known median of means, as described in \cite{devroye2016sub}. Initially, the sample is divided into $K$ groups. Subsequently, an estimator is computed for each group. Finally, these $K$ estimators are combined using a robust procedure. We prove that this estimator is sub-Gaussian and we get its break-down point, in the sense of Donoho. The robust procedure involves a minimization problem on a general metric space, but we show that the same (up to a constant) sub-Gaussianity is obtained if the minimization is taken over the sample, making GROS feasible in practice. The performance of GROS is evaluated through five simulation studies: the first one focuses on classification using $k$-means, the second one on the multi-armed bandit problem, the third one on the regression problem. The fourth one is the 
    
[^3]: 在次指数混合模型中实现极小化聚类误差：通用下界和最佳速率

    Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models

    [https://arxiv.org/abs/2402.15432](https://arxiv.org/abs/2402.15432)

    本文在混合模型中建立了一个通用下界，通过Chernoff散度来表达，将其拓展到具有次指数尾部的混合模型，并证明了迭代算法在这些混合模型中实现了最佳误差率

    

    聚类是无监督机器学习中的一个关键挑战，通常通过混合模型的视角来研究。在高斯和次高斯混合模型中恢复聚类标签的最佳误差率涉及到特定的信噪比。简单的迭代算法，如Lloyd算法，可以达到这个最佳误差率。在本文中，我们首先为任何混合模型中的误差率建立了一个通用下界，通过Chernoff散度来表达，这是一个比信噪比更通用的模型信息度量。然后我们证明了迭代算法在混合模型中实现了这个下界，特别强调了具有拉普拉斯分布误差的位置-尺度混合。此外，针对更适合由泊松或负二项混合模型建模的数据集，我们研究了其分布属于指数族的混合模型。

    arXiv:2402.15432v1 Announce Type: cross  Abstract: Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd's algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such m
    
[^4]: LoRA对转换器中聚类的影响

    The Impact of LoRA on the Emergence of Clusters in Transformers

    [https://arxiv.org/abs/2402.15415](https://arxiv.org/abs/2402.15415)

    本文利用转换器数学框架探讨了LoRA算法对Token聚类结构动态的影响，发现在不同参数下，修改后的注意力矩阵动态的聚类表现出较长时间的显著差异，但仍在短时间内保持密切相似。

    

    在本文中，我们利用\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}提出的转换器数学框架，探讨注意力参数和初始标记值的变化如何影响标记聚类的结构动态。我们的分析表明，虽然修改后的注意力矩阵动态中的聚类可能在较长时间内与原始聚类差异显著，但在较短时间间隔内，它们在参数差异的影响下仍保持密切相似。这项工作通过LoRA算法\cite{hu2021lora,peft}的实际应用，为微调领域做出了贡献，增进了我们对LoRA增强的Transformer模型行为的理解。

    arXiv:2402.15415v1 Announce Type: new  Abstract: In this paper, we employ the mathematical framework on Transformers developed by \citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. This work contributes to the fine-tuning field through practical applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced Transformer models.
    
[^5]: 拥有潜在变量的Lasso：高效估计、协变量重新缩放和计算统计差距

    Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps

    [https://arxiv.org/abs/2402.15409](https://arxiv.org/abs/2402.15409)

    在处理拥有潜在变量的稀疏线性回归问题时，通过对协变量进行异质缩放，Lasso方法可以获得强有力的估计保证。

    

    众所周知，当感兴趣的协变量之间存在强相关性时，Lasso的统计性能会显著下降。特别是，与计算效率低下的备选方案如最佳子集选择相比，Lasso的预测误差会变得严重严重。由于在稀疏线性回归问题中存在一个被普遍猜测的计算统计权衡，通常不可能一般性地减小这一差距。在这项工作中，我们提出了一个自然的稀疏线性回归设置，其中协变量之间的强相关性来自未观察到的潜在变量。在这种设定下，我们分析了由强相关性引起的问题，并设计了一个令人惊讶地简单的修复方法。虽然标准化协变量的Lasso失败了，但有一种异质缩放的协变量，Lasso将突然获得对估计的强有力保证。此外，我们设计了一个简单而高效的程序

    arXiv:2402.15409v1 Announce Type: cross  Abstract: It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations. In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection. Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general.   In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation. Moreover, we design a simple, efficient procedure fo
    
[^6]: 针对病例-对照研究的逻辑回归半监督推断的高效方法

    Efficient semi-supervised inference for logistic regression under case-control studies

    [https://arxiv.org/abs/2402.15365](https://arxiv.org/abs/2402.15365)

    针对病例-对照研究的逻辑回归半监督推断，利用未标记数据可以识别截距参数，解决了截距参数在半监督学习中不可辨识的问题。

    

    半监督学习在统计学和机器学习中越来越受到关注。在半监督学习设置中，收集了一个带有结果和协变量的标记数据集，以及一个仅包含协变量的未标记数据集。我们考虑了在半监督设置中的推断问题，在这种设置下，标记数据中的结果是二进制的，经过病例-对照抽样的方式收集标记数据。病例-对照抽样是一种有效的抽样方案，可减轻二进制数据中的不平衡结构。在逻辑模型假设下，病例-对照数据仍然可以为回归模型的斜率参数提供一致的估计量。然而，截距参数是不可辨识的。因此，不能从病例-对照数据中估计边际比例。我们发现，在有未标记数据的情况下，可以在半监督学习设置中识别截距参数。

    arXiv:2402.15365v1 Announce Type: cross  Abstract: Semi-supervised learning has received increasingly attention in statistics and machine learning. In semi-supervised learning settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected. We consider an inference problem in semi-supervised settings where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling. Case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data. Under the logistic model assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model. However, the intercept parameter is not identifiable. Consequently, the marginal case proportion cannot be estimated from case-control data. We find out that with the availability of the unlabeled data, the intercept parameter can be identified in semi-supervised learning setting.
    
[^7]: 从稀疏且嘈杂数据中快速识别稀疏非线性动力学的贝叶斯方法

    Rapid Bayesian identification of sparse nonlinear dynamics from scarce and noisy data

    [https://arxiv.org/abs/2402.15357](https://arxiv.org/abs/2402.15357)

    提出了一种快速的概率框架，称为贝叶斯-SINDy，用于从有限且嘈杂数据中学习正确的模型方程，并且对参数估计中的不确定性进行量化，特别适用于生物数据和实时系统识别。

    

    我们提出了一个快速的概率框架，用于识别控制观测数据动态的微分方程。我们将SINDy方法重新构建到贝叶斯框架中，并使用高斯逼近来加速计算。由此产生的方法，贝叶斯-SINDy，不仅量化了参数估计中的不确定性，而且在从有限且嘈杂数据中学习正确模型时更加稳健。通过使用合成和真实例子，如猞猁-野兔种群动态，我们展示了新框架在学习正确模型方程中的有效性，并比较了其与现有方法的计算和数据效率。由于贝叶斯-SINDy可以快速吸收数据并对噪声具有稳健性，因此特别适用于生物数据和控制中的实时系统识别。其概率框架还使得可以计算信息熵。

    arXiv:2402.15357v1 Announce Type: cross  Abstract: We propose a fast probabilistic framework for identifying differential equations governing the dynamics of observed data. We recast the SINDy method within a Bayesian framework and use Gaussian approximations for the prior and likelihood to speed up computation. The resulting method, Bayesian-SINDy, not only quantifies uncertainty in the parameters estimated but also is more robust when learning the correct model from limited and noisy data. Using both synthetic and real-life examples such as Lynx-Hare population dynamics, we demonstrate the effectiveness of the new framework in learning correct model equations and compare its computational and data efficiency with existing methods. Because Bayesian-SINDy can quickly assimilate data and is robust against noise, it is particularly suitable for biological data and real-time system identification in control. Its probabilistic framework also enables the calculation of information entropy, 
    
[^8]: 信息论安全贝叶斯优化

    Information-Theoretic Safe Bayesian Optimization

    [https://arxiv.org/abs/2402.15347](https://arxiv.org/abs/2402.15347)

    提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。

    

    我们考虑了一个顺序决策任务，其目标是在不评估违反先验未知（安全）约束的参数的情况下优化未知函数。一个常见的方法是在未知函数上放置高斯过程先验，并且仅允许在高概率安全区域内进行评估。大多数当前方法依赖于对域的离散化，并且不能直接扩展到连续情况。此外，它们利用约束的规则假设的方式引入了一个额外的关键超参数。在本文中，我们提出了一个信息论安全探索准则，该准则直接利用GP后验来识别最具信息的安全参数进行评估。将这一探索准则与众所周知的贝叶斯优化收益函数结合起来，产生了一种新颖的安全贝叶斯优化选择准则。

    arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
    
[^9]: Fourier基密度模型

    Fourier Basis Density Model

    [https://arxiv.org/abs/2402.15345](https://arxiv.org/abs/2402.15345)

    引入了一种基于受限Fourier基的轻量级、灵活且端到端可训练的概率密度模型，能够有效逼近各种多模态1维密度，表现优于传统的深度因式模型，同时在学习压缩任务中展示了其实用性。

    

    我们引入了一种轻量级、灵活且端到端可训练的概率密度模型，其由一个受限的Fourier基参数化。我们评估了该模型在逼近一系列多模态1维密度方面的表现，这些密度通常很难拟合。与[1]中引入的深度因式模型相比，我们的模型在类似的计算预算下实现了更低的交叉熵。此外，我们还在一个玩具压缩任务上评估了我们的方法，展示了其在学习压缩中的实用性。

    arXiv:2402.15345v1 Announce Type: new  Abstract: We introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained Fourier basis. We assess its performance at approximating a range of multi-modal 1D densities, which are generally difficult to fit. In comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget. In addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression.
    
[^10]: 使用固定和递减学习率的随机梯度下降的迭代和随机一阶预言者复杂度

    Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates

    [https://arxiv.org/abs/2402.15344](https://arxiv.org/abs/2402.15344)

    研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与迭代和SFO复杂度之间的关系，并指出使用关键批量大小的SGD可以最小化SFO复杂度

    

    随机梯度下降（SGD）的性能取决于学习率和批量大小，影响训练所需的迭代次数和随机一阶预言者（SFO）复杂度。先前的数值结果表明，对于使用固定学习率的SGD，随着批量大小的增加，训练所需的迭代次数减少，并且SFO复杂度在关键批量大小时最小化，一旦批量大小超过该大小后增加。本文研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与所需迭代和SFO复杂度之间的关系，并表明使用关键批量大小的SGD可以最小化SFO复杂度。

    arXiv:2402.15344v1 Announce Type: cross  Abstract: The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity. We also provide numerical compariso
    
[^11]: 分类深度学习：一种关于架构的代数理论

    Categorical Deep Learning: An Algebraic Theory of Architectures

    [https://arxiv.org/abs/2402.15332](https://arxiv.org/abs/2402.15332)

    提出了一种关于深度学习架构的代数理论，应用范畴论构建了一个桥梁，有效地涵盖了神经网络设计的不同风格，同时自然地编码了计算机科学和自动机理论中的许多标准结构。

    

    我们提出了一个关于指定和研究深度学习架构的通用框架的立场。我们认为到目前为止关于这一领域的关键尝试缺乏一种一致的桥梁，能够指定模型必须满足的约束并规定它们的实现方式。专注于构建这样一个桥梁，我们建议应用范畴论——准确地说，单子值于参数映射的二范畴的通用代数——作为一种单一理论，优雅地包含了神经网络设计的这两种风格。为了支持我们的观点，我们展示了这一理论如何恢复由几何深度学习导致的约束，以及从神经网络不同领域的多种架构（如RNNs）的实现。我们还展示了这一理论如何自然地编码了计算机科学和自动机理论中的许多标准结构。

    arXiv:2402.15332v1 Announce Type: cross  Abstract: We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.
    
[^12]: 使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程的生成建模

    Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations

    [https://arxiv.org/abs/2402.15285](https://arxiv.org/abs/2402.15285)

    使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程，提出了一种在生成建模中解决HJB方程的新方法，该方法无需样本，不依赖于归一化常数，并能避免维数灾难。

    

    从概率密度中进行采样在不确定性量化（UQ）和生成建模（GM）等领域中是一项常见挑战。 在GM中，特别流行的采样工具是依赖于Ornstein-Uhlenbeck正向过程的对数密度的逆时间扩散过程。 在Berner等人[2022]中，作者指出这些对数密度可以通过解决源自随机最优控制的哈密尔顿-雅各比-贝尔曼（HJB）方程来获得。 虽然这个HJB方程通常使用间接方法来处理，比如政策迭代和对神经网络这样的黑匣子架构进行无监督训练，我们提出通过直接时间积分来解决HJB方程，使用张量矩阵（TT）格式的压缩多项式进行空间离散化。 这种方法没有样本需求，不依赖于归一化常数，并且可以避免维数灾难。

    arXiv:2402.15285v1 Announce Type: cross  Abstract: Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionalit
    
[^13]: 用监督机器学习对银河平面的紧凑射电源进行分类

    Classification of compact radio sources in the Galactic plane with supervised machine learning

    [https://arxiv.org/abs/2402.15232](https://arxiv.org/abs/2402.15232)

    本研究使用射电和红外图像，结合20,000张图像数据集进行监督机器学习，实现对银河平面中紧凑射电源的分类。

    

    从处理过的数据产品中生成科学准备就绪的数据是未来Square Kilometre Array（SKA）及其前身射电连续波普查面临的主要挑战之一，这是由于预期的数据量和实现高度自动化处理的需求。本文着重于使用射电和红外图像对银河平面中的紧凑射电源进行分类。为此，我们产生了一个包含约20,000张来自过去射电和红外调查以及使用澳大利亚SKA Pathfinder（ASKAP）进行的试验调查的不同天文类别的紧凑源图像的策划数据集。还获得了部分数据的射电谱指数信息。然后在产生的数据集上训练了两个不同的分类器。

    arXiv:2402.15232v1 Announce Type: cross  Abstract: Generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the Square Kilometre Array (SKA) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing. Source extraction, characterization, and classification are the major stages involved in this process. In this work we focus on the classification of compact radio sources in the Galactic plane using both radio and infrared images as inputs. To this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the Australian SKA Pathfinder (ASKAP). Radio spectral index information was also obtained for a subset of the data. We then trained two different classifiers on the produced dataset. The first model 
    
[^14]: 统计无偏回归：一种用于验证回归模型的机器学习方法

    Statistical Agnostic Regression: a machine learning method to validate regression models

    [https://arxiv.org/abs/2402.15213](https://arxiv.org/abs/2402.15213)

    本文提出了一种新的方法，统计无关地评估了线性回归模型，并评估了ML估计在检测方面的表现。

    

    回归分析是统计建模中的一个核心主题，旨在估计因变量（通常称为响应变量）与一个或多个自变量（即解释变量）之间的关系。线性回归是迄今为止在预测、预测或因果推断等多个研究领域执行此任务的最流行方法。除了解决线性回归问题的各种传统方法外，如普通最小二乘法、岭回归或套索回归——这些方法往往是更高级机器学习（ML）技术的基础——后者已成功地应用在这种场景中，但没有对统计显著性进行正式定义。最多，基于经验测量（如残差或准确度）进行置换或基于经典分析，以反映ML估计对检测的更高能力。本文介绍了一种新的方法，该方法统计无关地评估了线性回归模型，并对ML估计在检测方面的表现进行了评估。

    arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
    
[^15]: 连续时间扩散模型的微调作为熵正则化控制

    Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control

    [https://arxiv.org/abs/2402.15194](https://arxiv.org/abs/2402.15194)

    扩散模型的微调方法可以通过最大化奖励函数的价值来以目标导向方式进行微调，但可能会面临奖励崩溃的挑战。

    

    扩散模型在捕捉复杂数据分布方面表现出色，例如自然图像和蛋白质的分布。虽然扩散模型经过训练可代表训练数据集中的分布，但我们通常更关注其他属性，例如生成图像的美学质量或生成蛋白质的功能属性。扩散模型可以通过最大化某些奖励函数的价值（例如图像的美学质量）以目标导向的方式进行微调。然而，这些方法可能会导致样本多样性减少，与训练数据分布出现显著偏差，甚至由于利用不完美的奖励函数而导致样本质量较差。在许多实际应用中奖励函数是用于近似真实“真实”奖励的学习模型时，最后一个问题经常会产生。这些挑战总称为“奖励崩溃”。

    arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
    
[^16]: 用于随机组合半臂老虎机的协方差自适应最小二乘算法

    Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits

    [https://arxiv.org/abs/2402.15171](https://arxiv.org/abs/2402.15171)

    提出了一种协方差自适应的最小二乘算法，利用在线估计协方差结构，相对于基于代理方差的算法获得改进的遗憾上界，特别在协方差系数全为非负时，能有效地利用半臂反馈，并在各种参数设置下表现优异。

    

    我们解决了随机组合半臂老虎机问题，其中玩家可以从包含d个基本项的P个子集中进行选择。大多数现有算法（如CUCB、ESCB、OLS-UCB）需要对奖励分布有先验知识，比如子高斯代理-方差的上界，这很难准确估计。在这项工作中，我们设计了OLS-UCB的方差自适应版本，依赖于协方差结构的在线估计。在实际设置中，估计协方差矩阵的系数要容易得多，并且相对于基于代理方差的算法，导致改进的遗憾上界。当协方差系数全为非负时，我们展示了我们的方法有效地利用了半臂反馈，并且可以明显优于老虎机反馈方法，在指数级别P≫d以及P≤d的情况下，这一点并不来自大多数现有分析。

    arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
    
[^17]: 使用受右截尾影响的实验数据对观测研究进行基准测试

    Benchmarking Observational Studies with Experimental Data under Right-Censoring

    [https://arxiv.org/abs/2402.15137](https://arxiv.org/abs/2402.15137)

    论文通过考虑截尾现象，提出了一种基准测试方法，用于观测研究与随机对照试验中条件平均处理效应的等价性测试，从而验证有效性假设是否成立。

    

    从观测研究中推断因果关系需要进行无法验证的有效性假设；然而，可以通过将观测研究与随机对照试验（RCT）中的实验数据进行基准测试来验证这些假设。现有程序的一个主要限制是未考虑到截尾现象，尽管有大量的RCT和报告右截尾时间间隔事件结果的观测研究。我们考虑两种情况，其中截尾时间（1）独立于时间间隔事件，以及（2）在观测研究和RCT中以相同方式取决于时间至事件。对于前者，我们采用一种截断-双重鲁棒信号，用于条件平均处理效应（CATE）的等价性测试，以促进OS和RCT中CATE的等价性测试，这相当于测试有效性的假设是否成立。对于后者，我们发现尽管可能无法进行无偏CATE估计，但仍然可以使用相同的测试。我们验证了我们的考虑截尾的测试的有效性

    arXiv:2402.15137v1 Announce Type: cross  Abstract: Drawing causal inferences from observational studies (OS) requires unverifiable validity assumptions; however, one can falsify those assumptions by benchmarking the OS with experimental data from a randomized controlled trial (RCT). A major limitation of existing procedures is not accounting for censoring, despite the abundance of RCTs and OSes that report right-censored time-to-event outcomes. We consider two cases where censoring time (1) is independent of time-to-event and (2) depends on time-to-event the same way in OS and RCT. For the former, we adopt a censoring-doubly-robust signal for the conditional average treatment effect (CATE) to facilitate an equivalence test of CATEs in OS and RCT, which serves as a proxy for testing if the validity assumptions hold. For the latter, we show that the same test can still be used even though unbiased CATE estimation may not be possible. We verify the effectiveness of our censoring-aware tes
    
[^18]: 具有弃权选项的多臂赌博机问题

    Multi-Armed Bandits with Abstention

    [https://arxiv.org/abs/2402.15127](https://arxiv.org/abs/2402.15127)

    提出了一个扩展的多臂赌博机问题，引入了弃权选项，并成功设计和分析了算法，实现了渐近和米迷诺下最优。

    

    我们介绍了一个新颖的多臂赌博机问题扩展，其中包含了额外的战略元素：弃权选项。在这个增强框架中，代理不仅需要在每个时间步选择一个臂，还可以选择在观察之前放弃接受随机瞬时奖励。当选择弃权时，代理要么遭受固定的后悔，要么获得一定的奖励保证。鉴于这种额外的复杂性，我们探讨是否可以开发出既渐近又米迷诺下最优的有效算法。我们通过设计和分析算法来回答这个问题，这些算法的后悔满足相应的信息理论下限。我们的研究为弃权选项的好处提供了有价值的数量化见解，为在其他具有这种选项的在线决策问题中进一步探索奠定了基础。

    arXiv:2402.15127v1 Announce Type: new  Abstract: We introduce a novel extension of the canonical multi-armed bandit problem that incorporates an additional strategic element: abstention. In this enhanced framework, the agent is not only tasked with selecting an arm at each time step, but also has the option to abstain from accepting the stochastic instantaneous reward before observing it. When opting for abstention, the agent either suffers a fixed regret or gains a guaranteed reward. Given this added layer of complexity, we ask whether we can develop efficient algorithms that are both asymptotically and minimax optimal. We answer this question affirmatively by designing and analyzing algorithms whose regrets meet their corresponding information-theoretic lower bounds. Our results offer valuable quantitative insights into the benefits of the abstention option, laying the groundwork for further exploration in other online decision-making problems with such an option. Numerical results f
    
[^19]: 通过深度展开加速斯坦变分梯度下降的收敛速度

    Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding

    [https://arxiv.org/abs/2402.15125](https://arxiv.org/abs/2402.15125)

    通过深度展开技术，本文提出的可训练SVGD算法加速了其收敛速度，相比传统SVGD变体表现出更快的收敛速度。

    

    Stein变分梯度下降（SVGD）是一种著名的基于粒子的变分推断方法，用于对目标分布进行采样。SVGD已经引起了人们的兴趣，应用于贝叶斯推理等机器学习技术。本文提出了将一种名为深度展开的深度学习技术融入SVGD的新型可训练算法。这种方法促进了对SVGD的内部参数进行学习，从而加速了其收敛速度。为了评估所提出的可训练SVGD算法，我们对三项任务进行了数值模拟：对一维高斯混合进行采样，进行贝叶斯逻辑回归以及学习贝叶斯神经网络。结果表明，我们提出的算法比SVGD的传统变体表现出更快的收敛速度。

    arXiv:2402.15125v1 Announce Type: new  Abstract: Stein variational gradient descent (SVGD) is a prominent particle-based variational inference method used for sampling a target distribution. SVGD has attracted interest for application in machine-learning techniques such as Bayesian inference. In this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into SVGD. This approach facilitates the learning of the internal parameters of SVGD, thereby accelerating its convergence speed. To evaluate the proposed trainable SVGD algorithms, we conducted numerical simulations of three tasks: sampling a one-dimensional Gaussian mixture, performing Bayesian logistic regression, and learning Bayesian neural networks. The results show that our proposed algorithms exhibit faster convergence than the conventional variants of SVGD.
    
[^20]: 物理约束的多项式混沌展开用于科学机器学习和不确定性量化

    Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification

    [https://arxiv.org/abs/2402.15115](https://arxiv.org/abs/2402.15115)

    提出一种物理约束的多项式混沌展开方法，将科学机器学习与不确定性量化无缝集成，有效地实现SciML任务中的不确定性量化和在UQ任务中利用SciML提高不确定性评估。

    

    我们提出了一种新颖的物理约束的多项式混沌展开作为一种替代建模方法，能够执行科学机器学习（SciML）和不确定性量化（UQ）任务。所提出的方法具有独特的能力：将SciML与UQ无缝集成，从而能够有效地量化SciML任务中的不确定性，并利用SciML来改善UQ相关任务中的不确定性评估。该替代模型可以有效地纳入多种物理约束，如支配偏微分方程（PDEs）及其相关的初始和边界条件约束，不等式型约束（如单调性，凸性，非负性等），以及在训练过程中添加额外先验信息以辅助有限数据。这确保了物理上合理的预测，并显著减少了昂贵计算的需求。

    arXiv:2402.15115v1 Announce Type: cross  Abstract: We present a novel physics-constrained polynomial chaos expansion as a surrogate modeling method capable of performing both scientific machine learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method possesses a unique capability: it seamlessly integrates SciML into UQ and vice versa, which allows it to quantify the uncertainties in SciML tasks effectively and leverage SciML for improved uncertainty assessment during UQ-related tasks. The proposed surrogate model can effectively incorporate a variety of physical constraints, such as governing partial differential equations (PDEs) with associated initial and boundary conditions constraints, inequality-type constraints (e.g., monotonicity, convexity, non-negativity, among others), and additional a priori information in the training process to supplement limited data. This ensures physically realistic predictions and significantly reduces the need for expensive comp
    
[^21]: 使用对数Sobolev不等式的非线性贝叶斯最优实验设计

    Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities

    [https://arxiv.org/abs/2402.15053](https://arxiv.org/abs/2402.15053)

    使用对数Sobolev不等式构造的MI下限的贪婪方法在非线性模型优化设计中表现出色

    

    我们研究了从一个较大的候选池中选择$k$个实验的问题，目标是最大化所选子集与基础参数之间的互信息（MI）。由于组合优化问题的复杂性以及在非线性/非高斯设置中评估MI的困难性，找到确切解决方案在计算上是昂贵的。我们提出了基于通过对数Sobolev不等式构造的新的计算廉价的MI下限的贪婪方法。我们证明，在包括具有非加性噪声的非线性模型的最优设计在内的各种设置中，我们的方法优于随机选择策略、高斯逼近和嵌套蒙特卡洛（NMC）MI估算器。

    arXiv:2402.15053v1 Announce Type: cross  Abstract: We study the problem of selecting $k$ experiments from a larger candidate pool, where the goal is to maximize mutual information (MI) between the selected subset and the underlying parameters. Finding the exact solution is to this combinatorial optimization problem is computationally costly, not only due to the complexity of the combinatorial search but also the difficulty of evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches based on new computationally inexpensive lower bounds for MI, constructed via log-Sobolev inequalities. We demonstrate that our method outperforms random selection strategies, Gaussian approximations, and nested Monte Carlo (NMC) estimators of MI in various settings, including optimal design for nonlinear models with non-additive noise.
    
[^22]: 使用样式和内容信息的一致性引导温度缩放用于域外校准

    Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration

    [https://arxiv.org/abs/2402.15019](https://arxiv.org/abs/2402.15019)

    提出了一种新的一致性引导温度缩放（CTS）策略，通过提供源域数据样本之间的相互监督，显著增强了域外（OOD）校准性能。

    

    近年来，关于深度神经网络对领域转移的鲁棒性越来越受到关注。然而，大多数现有研究都集中在提高模型的准确性上，而不是校准性能，而后者是值得信赖的AI系统的另一个重要要求。温度缩放（TS）作为一种可以保持准确性的事后校准方法，在领域内环境中已被证明是有效的，但在领域外（OOD）却不是，因为事先很难获取未见领域的验证集。在本文中，我们提出了一种新的温度缩放策略，一致性引导温度缩放（CTS），通过提供源域数据样本之间的相互监督，可以显著提高OOD校准性能。受到我们的观察到的发现，由于不一致的样本预测导致的过度自信是OOD校准的主要障碍，我们提出了一种新的校准策略。

    arXiv:2402.15019v1 Announce Type: cross  Abstract: Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to 
    
[^23]: 机器学习分类算法的比较及其在弗莱明汉姆心脏研究中的应用

    Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study

    [https://arxiv.org/abs/2402.15005](https://arxiv.org/abs/2402.15005)

    该研究通过弗莱明汉姆心脏病数据作为案例研究，比较了八种机器学习分类算法在不同训练/测试场景下的预测性能，发现极端梯度提升和支持向量机在训练不平衡数据时存在缺陷。

    

    在医疗保健中使用机器学习算法可能会放大社会不公正和健康不平等。本研究针对机器学习分类算法在开发和部署过程中出现的一些一般化障碍，使用弗莱明汉姆冠心病数据作为案例研究，展示了如何有效地选择概率截断以将回归模型转换为分类器。我们比较了八种机器学习分类算法在四种训练/测试场景下的预测性能的抽样分布，以测试它们的一般化能力和延续偏见的潜力。我们发现，极端梯度提升和支持向量机在训练不平衡数据时存在缺陷。

    arXiv:2402.15005v1 Announce Type: new  Abstract: The use of machine learning algorithms in healthcare can amplify social injustices and health inequities. While the exacerbation of biases can occur and compound during the problem selection, data collection, and outcome definition, this research pertains to some generalizability impediments that occur during the development and the post-deployment of machine learning classification algorithms. Using the Framingham coronary heart disease data as a case study, we show how to effectively select a probability cutoff to convert a regression model for a dichotomous variable into a classifier. We then compare the sampling distribution of the predictive performance of eight machine learning classification algorithms under four training/testing scenarios to test their generalizability and their potential to perpetuate biases. We show that both the Extreme Gradient Boosting, and Support Vector Machine are flawed when trained on an unbalanced data
    
[^24]: 小型基准测试：用更少的示例评估LLM

    tinyBenchmarks: evaluating LLMs with fewer examples

    [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)

    本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。

    

    大型语言模型（LLMs）的多功能性导致创建了多种基准测试，彻底测试各种语言模型的能力。这些基准测试包含成千上万个示例，使得评估LLMs非常昂贵。本文研究了减少评估LLMs性能所需的评估次数的策略。例如，我们展示了要准确估计LLMs在MMLU上的性能（一个包含14K个示例的流行多选问答基准测试），只需要在100个精心挑选的示例上评估这个LLMs。我们发布了评估工具和流行基准测试的微型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和微型基准测试足以可靠且高效地重现原始评估结果。

    arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
    
[^25]: 可验证的提升树集成

    Verifiable Boosted Tree Ensembles

    [https://arxiv.org/abs/2402.14988](https://arxiv.org/abs/2402.14988)

    本研究将可验证学习从基本集成方法扩展到高级提升树集成，提出了一个伪多项式时间算法来验证鲁棒性，对基于$L_p$-范数的攻击者具有出色的性能。

    

    可验证学习倡导训练易于进行高效安全验证的机器学习模型。先前的研究表明，特定类的决策树集成，即称为大广泛集成，可以在多项式时间内针对任何基于范数的攻击者进行鲁棒性验证。本研究将可验证学习从基本集成方法（即硬多数投票）扩展到高级提升树集成，比如那些使用XGBoost或LightGBM训练的集成。我们的正式结果表明，在考虑基于$L_\infty$-范数的攻击者时，鲁棒性验证可以在多项式时间内实现，但对于其他基于范数的攻击者来说仍然是NP难的。尽管如此，我们提出了一个伪多项式时间算法来验证针对基于$L_p$-范数的攻击者的鲁棒性，其中$p \in \mathbb{N} \cup \{0\}$，在实践中具有出色的性能。我们的实验评估表明

    arXiv:2402.14988v1 Announce Type: new  Abstract: Verifiable learning advocates for training machine learning models amenable to efficient security verification. Prior research demonstrated that specific classes of decision tree ensembles -- called large-spread ensembles -- allow for robustness verification in polynomial time against any norm-based attacker. This study expands prior work on verifiable learning from basic ensemble methods (i.e., hard majority voting) to advanced boosted tree ensembles, such as those trained using XGBoost or LightGBM. Our formal results indicate that robustness verification is achievable in polynomial time when considering attackers based on the $L_\infty$-norm, but remains NP-hard for other norm-based attackers. Nevertheless, we present a pseudo-polynomial time algorithm to verify robustness against attackers based on the $L_p$-norm for any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent performance. Our experimental evaluation shows th
    
[^26]: 在平滑数据上的经验风险最小化性能研究

    On the Performance of Empirical Risk Minimization with Smoothed Data

    [https://arxiv.org/abs/2402.14987](https://arxiv.org/abs/2402.14987)

    在数据是良好指定和平滑的情况下，对于经验风险最小化（ERM）与平方损失的性能，当类是可从 iid 数据中学习时，ERM能够实现次线性误差。

    

    为了避开在序贯决策中的统计和计算困难结果，最近的工作考虑了平滑的在线学习，其中假设每个时间点的数据分布在给定历史条件下相对于基础度量具有有界的似然比。虽然先前的工作已经证明了平滑性的好处，但它们要么假设基础度量对学习者是已知的，要么提出的算法在特殊情况下仅适用于计算效率低下。本研究研究了更一般的设置，即基础度量对学习者是\emph{未知}的情况，特别关注在数据明确定和平滑的情况下，当数据是良好指定时经验风险最小化（ERM）与平方损失的性能。我们展示，在这种设置下，只要类是可从iid数据中学习的，ERM就能够实现次线性误差；特别是，当数据是iid时，ERM实现的错误尺度为$\tilde O(

    arXiv:2402.14987v1 Announce Type: cross  Abstract: In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. This work investigates the more general setting where the base measure is \emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with square loss when the data are well-specified and smooth. We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\tilde O(
    
[^27]: 通过分数拉普拉斯特征映射进行非光滑非参数回归

    Nonsmooth Nonparametric Regression via Fractional Laplacian Eigenmaps

    [https://arxiv.org/abs/2402.14985](https://arxiv.org/abs/2402.14985)

    通过分数拉普拉斯特征映射，开发了针对真实回归函数非光滑情况的非参数回归方法，成功处理了该函数类在$L_2$-分数 Sobolev 空间中的特性，并证明了误差上界为$n^{-\frac{2s}{2s+d}}$。

    

    我们为真实回归函数不一定平滑的情况开发了非参数回归方法。具体来说，我们的方法使用了分数拉普拉斯，并旨在处理真实回归函数位于$L_2$-分数 Sobolev 空间（阶数为$s\in (0,1)$）的情况。该函数类是一个 Hilbert 空间，位于平方可积函数空间和一阶 Sobolev 空间之间，包括分数幂函数、分段常数或多项式函数以及尖峰函数作为典型示例。对于我们提出的方法，我们证明了关于样本内均方估计误差的上界，具有$n^{-\frac{2s}{2s+d}}$的阶，其中$d$是维数，$s$是前述顺序参数，$n$是观测数量。我们还提供了初步的实证结果，验证了所开发方法的实际表现。

    arXiv:2402.14985v1 Announce Type: cross  Abstract: We develop nonparametric regression methods for the case when the true regression function is not necessarily smooth. More specifically, our approach is using the fractional Laplacian and is designed to handle the case when the true regression function lies in an $L_2$-fractional Sobolev space with order $s\in (0,1)$. This function class is a Hilbert space lying between the space of square-integrable functions and the first-order Sobolev space consisting of differentiable functions. It contains fractional power functions, piecewise constant or polynomial functions and bump function as canonical examples. For the proposed approach, we prove upper bounds on the in-sample mean-squared estimation error of order $n^{-\frac{2s}{2s+d}}$, where $d$ is the dimension, $s$ is the aforementioned order parameter and $n$ is the number of observations. We also provide preliminary empirical results validating the practical performance of the developed
    
[^28]: 数据预处理方法、特征选择技术和机器学习模型在不平衡遗传数据上提高分类和回归性能的比较分析

    Comparative Analysis of Data Preprocessing Methods, Feature Selection Techniques and Machine Learning Models for Improved Classification and Regression Performance on Imbalanced Genetic Data

    [https://arxiv.org/abs/2402.14980](https://arxiv.org/abs/2402.14980)

    研究比较了数据预处理、特征选择和模型选择对训练遗传数据集上模型性能的影响，发现异常值和倾斜会影响模型性能。

    

    基因组测序技术的快速发展导致了大量基因组数据的收集。研究人员可能有兴趣在这些数据上使用机器学习模型来预测基因突变的致病性或临床意义。然而，许多遗传数据集包含不平衡的目标变量，这给机器学习模型带来挑战：在回归任务中观察结果倾斜/不平衡，在分类任务中类别不平衡。遗传数据集通常具有高基数和倾斜的预测变量，这进一步增加了挑战。我们旨在研究数据预处理、特征选择技术和模型选择对训练在这些数据集上的模型性能的影响。我们使用5折交叉验证测量性能，并比较不同技术组合下的平均r平方和准确率指标。我们发现预测变量或目标变量中的异常值/倾斜会对模型的性能产生影响。

    arXiv:2402.14980v1 Announce Type: cross  Abstract: Rapid advancements in genome sequencing have led to the collection of vast amounts of genomics data. Researchers may be interested in using machine learning models on such data to predict the pathogenicity or clinical significance of a genetic mutation. However, many genetic datasets contain imbalanced target variables that pose challenges to machine learning models: observations are skewed/imbalanced in regression tasks or class-imbalanced in classification tasks. Genetic datasets are also often high-cardinal and contain skewed predictor variables, which poses further challenges. We aimed to investigate the effects of data preprocessing, feature selection techniques, and model selection on the performance of models trained on these datasets. We measured performance with 5-fold cross-validation and compared averaged r-squared and accuracy metrics across different combinations of techniques. We found that outliers/skew in predictor or t
    
[^29]: 光滑自适应假设迁移学习

    Smoothness Adaptive Hypothesis Transfer Learning

    [https://arxiv.org/abs/2402.14966](https://arxiv.org/abs/2402.14966)

    本文提出了光滑自适应迁移学习（SATL）算法，通过在两个阶段均采用高斯核，使估计器能够适应目标/源及其偏移函数的未知光滑性。

    

    许多现有的基于核的两阶段假设迁移学习算法在不同阶段均采用相同的核正则化，并依赖于函数的已知光滑性来实现最优性。因此，在实践中，它们未能适应目标/源及其偏移之间的变化和未知光滑性。本文通过提出光滑自适应迁移学习（SATL），一个基于两阶段核岭回归（KRR）的算法，解决了这些问题。我们首先证明，在目标专用KRR学习中采用错误指定的固定带宽高斯核可以实现极小化最优性，并推导出一种适应未知Sobolev光滑性的自适应过程。利用这些结果，SATL在两阶段均采用高斯核，以使估计量能够适应目标/源及其偏移函数的未知光滑性。我们推导了学习问题在过量风险中的极小值下限，并表明

    arXiv:2402.14966v1 Announce Type: cross  Abstract: Many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. In this paper, we address these problems by proposing Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression(KRR)-based algorithm. We first prove that employing the misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality and derive an adaptive procedure to the unknown Sobolev smoothness. Leveraging these results, SATL employs Gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. We derive the minimax lower bound of the learning problem in excess risk and show that
    
[^30]: 评估执法系统中种族偏见的因果框架

    A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems

    [https://arxiv.org/abs/2402.14959](https://arxiv.org/abs/2402.14959)

    本论文提出了一个多阶段因果框架，融入犯罪行为，用于评估执法系统中的种族偏见，以解决以往研究中存在的限制，对偏见进行量化，并确定主要偏见来源。

    

    我们致力于开发一种数据驱动方法来评估执法系统中种族诱发的偏见。 在最近的研究中，已经讨论了在警民互动背景下使用警察停车数据解决这个问题，但存在两个关键限制。 首先，只有在将真实犯罪行为考虑在内时，偏见才能得到恰当量化，但在以前的研究中缺乏。 第二，执法系统是多阶段的，因此重要的是在“因果交互链”中孤立出偏见的真正来源，而不仅仅关注最终结果； 这有助于引导改革。 在这项工作中，我们通过提出一个包含犯罪行为的多阶段因果框架来解决这些挑战。 我们提供了一个理论特征和一个相关的数据驱动方法来评估(a)任何形式的种族偏见的存在，以及(b)如果是这样，这种偏见的主要来源是种族和...

    arXiv:2402.14959v1 Announce Type: cross  Abstract: We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While the recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the "causal chain of interactions" rather than simply focusing on the end outcome; this can help guide reforms. In this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and
    
[^31]: 一个线性Transformer块的上下文学习：MLP组件和一步GD初始化的优势

    In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization

    [https://arxiv.org/abs/2402.14951](https://arxiv.org/abs/2402.14951)

    该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。

    

    我们研究了结合线性注意力组件和线性多层感知器（MLP）组件的线性Transformer块（LTB）的上下文学习（ICL）能力。对于具有高斯先验和非零均值的线性回归的ICL，我们表明LTB可以实现几乎贝叶斯最优的ICL风险。相比之下，仅使用线性注意力必须产生不可避免的附加近似误差。此外，我们建立了LTB与具有可学习初始化的一步梯度下降估计器（$\mathsf{GD}-\mathbf{\beta}$）之间的对应关系，从每个$\mathsf{GD}-\mathbf{\beta}$估计器可以通过LTB估计器实现，到最小化类内ICL风险的每个最优LTB估计器实际上是一个$\mathsf{GD}-\mathbf{\beta}$估计器。最后，我们表明$\mathsf{GD}-\mathbf{\beta}$估计器可以通过梯度优化高效地优化。

    arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator. Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient f
    
[^32]: 冻结网络中的部分搜索足以找到强大的彩票票证

    Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket

    [https://arxiv.org/abs/2402.14029](https://arxiv.org/abs/2402.14029)

    提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。

    

    arXiv:2402.14029v1 公告类型：跨越 摘要：随机初始化的稠密网络包含可以在不进行权重学习的情况下实现高准确度的子网络--强大的彩票票证（SLTs）。最近，Gadhikar等人（2023年）在理论和实验证明，SLTs也可以在随机修剪的源网络中找到，从而减少SLT的搜索空间。然而，这限制了对甚至比源网络更稀疏的SLTs的搜索，导致由于意外的高稀疏性而准确度较差。本文提出了一种通过独立于所需SLT稀疏性的任意比率减少SLT搜索空间的方法。通过冻结一部分初始权重的随机子集，将其排除在搜索空间之外--即，通过永久修剪它们或将它们锁定为SLT的固定部分。事实上，通过我们与随机冻结变量的子集和逼近，在这种减少的搜索空间中，SLT的存在在理论上是得到保证的。除此之外，还可以减少...

    arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
    
[^33]: 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难

    Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions

    [https://arxiv.org/abs/2402.08082](https://arxiv.org/abs/2402.08082)

    基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。

    

    尽管基于分数的生成模型（SGMs）在巨大的图像生成任务中取得了显著的成功，但它们的数学基础仍然有限。在本文中，我们分析了SGMs在学习一个子高斯概率分布族中的近似和泛化。我们引入了一种关于概率分布复杂性的概念，即相对密度与标准高斯测度的相对密度。我们证明，如果对数相对密度可以通过神经网络进行局部逼近，并且网络参数可以适当地受限，那么通过经验分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。我们通过示例说明了我们的理论，其中包括某些高斯混合分布。我们证明的一个关键点是推导出与正向过程相关的真实得分函数的维度无关的深度神经网络逼近速率。

    While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
    
[^34]: 用于不规则间隔数据的深度ReLU神经网络插值的尖锐下界

    Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data

    [https://arxiv.org/abs/2302.00834](https://arxiv.org/abs/2302.00834)

    该论文研究了深度ReLU神经网络在不规则间隔数据上的插值问题，证明了在数据点间距指数级小的情况下需要$\Omega(N)$个参数，同时指出现有的位提取技术无法应用于这种情况。

    

    我们研究了深度ReLU神经网络的插值能力。具体来说，我们考虑深度ReLU网络如何在单位球中的$N$个数据点上进行值的插值，这些点之间相距$\delta$。我们表明在$\delta$在$N$指数级小的区域中需要$\Omega(N)$个参数，这给出了该区域的尖锐结果，因为$O(N)$个参数总是足够的。 这也表明用于证明VC维度下界的位提取技术无法应用于不规则间隔的数据点。最后，作为应用，我们给出了深度ReLU神经网络在嵌入端点处为Sobolev空间实现的近似速率的下界。

    arXiv:2302.00834v2 Announce Type: replace  Abstract: We study the interpolation power of deep ReLU neural networks. Specifically, we consider the question of how efficiently, in terms of the number of parameters, deep ReLU networks can interpolate values at $N$ datapoints in the unit ball which are separated by a distance $\delta$. We show that $\Omega(N)$ parameters are required in the regime where $\delta$ is exponentially small in $N$, which gives the sharp result in this regime since $O(N)$ parameters are always sufficient. This also shows that the bit-extraction technique used to prove lower bounds on the VC dimension cannot be applied to irregularly spaced datapoints. Finally, as an application we give a lower bound on the approximation rates that deep ReLU neural networks can achieve for Sobolev spaces at the embedding endpoint.
    
[^35]: 来自连续字典的混合物的离散学习

    Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary

    [https://arxiv.org/abs/2210.16311](https://arxiv.org/abs/2210.16311)

    本文提出了一种名为Group-Nonlinear-Lasso的方法，可以同时估计混合物中的线性系数和特征的非线性参数，并使用证明函数对预测误差提供了高概率界限。

    

    在本文中，我们观察了一组信号，可能是一个连续信号，受到噪声的干扰。每个信号是由一个未知数量的特征混合而成，这些特征属于一个连续字典。连续字典由一个实非线性参数进行参数化。我们假设这些信号共享一个基本结构，假定每个信号的活跃特征包含在一个有限稀疏集合中。我们提出了正则化优化问题，同时估计混合物中的线性系数和特征的非线性参数。优化问题由数据保真度项和$(\ell_1,L^p)$-惩罚项组成。我们称其解为Group-Nonlinear-Lasso，并使用证明函数对预测误差提供了高概率界限。借鉴最近关于离散学习方法几何性质的研究，我们表明只要特定参数满足条件，就可以构造这样的函数。

    arXiv:2210.16311v2 Announce Type: replace-cross  Abstract: In this paper we observe a set, possibly a continuum, of signals corrupted by noise. Each signal is a finite mixture of an unknown number of features belonging to a continuous dictionary. The continuous dictionary is parametrized by a real non-linear parameter. We shall assume that the signals share an underlying structure by assuming that each signal has its active features included in a finite and sparse set. We formulate regularized optimization problem to estimate simultaneously the linear coefficients in the mixtures and the non-linear parameters of the features. The optimization problem is composed of a data fidelity term and a $(\ell_1,L^p)$-penalty. We call its solution the Group-Nonlinear-Lasso and provide high probability bounds on the prediction error using certificate functions. Following recent works on the geometry of off-the-grid methods, we show that such functions can be constructed provided the parameters of t
    
[^36]: 利用梯度提升决策树的训练动态提高数据质量

    Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees

    [https://arxiv.org/abs/2210.11327](https://arxiv.org/abs/2210.11327)

    本文提出了一种基于梯度提升决策树的训练动态来评估每个训练实例行为的方法，针对包含大部分表格化或结构化数据的数据集，相较于自信学习、直接启发式和健壮提升算法，取得了最佳结果。

    

    真实世界的数据集中常常包含有错误标记的实例，这会影响模型的性能，尤其是在泛化超出分布范围时。同时，每个示例对学习过程可能有不同的贡献。这促使研究者更好地理解数据实例在模型中对好指标的贡献角色。本文提出了一种基于梯度提升决策树（GBDTs）训练动态计算的度量来评估每个训练实例行为的方法。我们专注于包含大部分表格化或结构化数据的数据集，对于这类数据集，决策树集成在性能方面仍处于领先地位。与自信学习、直接启发式和健壮提升算法相比，我们的方法在整体上取得了最佳结果。我们展示了在检测嘈杂标签以清理数据集、改进模型指标方面的结果。

    arXiv:2210.11327v2 Announce Type: replace  Abstract: Real world datasets contain incorrectly labeled instances that hamper the performance of the model and, in particular, the ability to generalize out of distribution. Also, each example might have different contribution towards learning. This motivates studies to better understanding of the role of data instances with respect to their contribution in good metrics in models. In this paper we propose a method based on metrics computed from training dynamics of Gradient Boosting Decision Trees (GBDTs) to assess the behavior of each training example. We focus on datasets containing mostly tabular or structured data, for which the use of Decision Trees ensembles are still the state-of-the-art in terms of performance. Our methods achieved the best results overall when compared with confident learning, direct heuristics and a robust boosting algorithm. We show results on detecting noisy labels in order clean datasets, improving models' metri
    
[^37]: 干预因果表示学习

    Interventional Causal Representation Learning

    [https://arxiv.org/abs/2209.11924](https://arxiv.org/abs/2209.11924)

    干预数据有助于因果表示学习，可以通过干预数据中潜在因素支持的几何特征来识别潜在的因果因素。

    

    因果表示学习旨在从低级感官数据中提取高级潜在因素。大多数现有方法依赖于观测数据和结构假设（如条件独立性）来识别潜在因素。然而，干预数据在各种应用中普遍存在。干预数据能否促进因果表示学习？本文探讨了这个问题。关键观察是，干预数据通常携带潜在因素支持的几何特征（即每个潜在因素可能采取的值）。举例来说，当潜在因素存在因果联系时，干预可以打破干预潜在因素支持和它们祖先之间的依赖关系。利用这一事实，我们证明在获得完美$do$干预数据后，可以确定潜在的因果因素，而且能够实现区块仿射识别。

    arXiv:2209.11924v4 Announce Type: replace-cross  Abstract: Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect $do$ interventions. Moreover, we can achieve block affine identific
    
[^38]: 针对随机矩阵计算的高效误差和方差估计

    Efficient error and variance estimation for randomized matrix computations

    [https://arxiv.org/abs/2207.06342](https://arxiv.org/abs/2207.06342)

    该论文提出了用于随机矩阵计算的高效误差和方差估计方法，可帮助评估输出质量并指导算法参数选择。

    

    随机矩阵算法已成为科学计算和机器学习中必不可少的工具。为了安全地在应用中使用这些算法，需要结合后验误差估计来评估输出的质量。为满足这一需求，本文提出了两种诊断方法：用于随机低秩逼近的留一法误差估计器和一种杰基刀重采样方法，用于估计随机矩阵计算的输出方差。这两种诊断方法对于随机低秩逼近算法（如随机奇异值分解和随机Nystrom逼近）计算迅速，并提供可用于评估计算输出质量和指导算法参数选择的有用信息。

    arXiv:2207.06342v4 Announce Type: replace-cross  Abstract: Randomized matrix algorithms have become workhorse tools in scientific computing and machine learning. To use these algorithms safely in applications, they should be coupled with posterior error estimates to assess the quality of the output. To meet this need, this paper proposes two diagnostics: a leave-one-out error estimator for randomized low-rank approximations and a jackknife resampling method to estimate the variance of the output of a randomized matrix computation. Both of these diagnostics are rapid to compute for randomized low-rank approximation algorithms such as the randomized SVD and randomized Nystr\"om approximation, and they provide useful information that can be used to assess the quality of the computed output and guide algorithmic parameter choices.
    
[^39]: 关于函数线性模型假设迁移学习的研究

    On Hypothesis Transfer Learning of Functional Linear Models

    [https://arxiv.org/abs/2206.04277](https://arxiv.org/abs/2206.04277)

    该研究在函数线性回归下探讨了迁移学习，提出了使用RKHS距离衡量任务相似性，并提出了两种算法来处理迁移，一种需要已知正源，另一种利用聚合技术实现无源信息的稳健传输。同时建立了学习问题的下界，并证明了算法的上界。

    

    我们研究了在再生核希尔伯特空间（RKHS）框架下的函数线性回归（FLR）的迁移学习（TL），观察到现有高维线性回归中的TL技术与基于截断的FLR方法不兼容，因为函数数据在本质上是无限维的，并由平滑的基础过程生成。我们使用RKHS距离来衡量任务之间的相似性，允许传输的信息类型与所施加的RKHS的属性相关联。基于假设偏移迁移学习范式，提出了两种算法：一种在已知正源时进行传输，另一种利用聚合技术实现无需先验信息的稳健传输。我们为这个学习问题建立了下界，并展示了所提出的算法享有匹配的渐近上界。

    arXiv:2206.04277v4 Announce Type: replace-cross  Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing the TL techniques in existing high-dimensional linear regression is not compatible with the truncation-based FLR methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. These analyses
    
[^40]: 变分贝叶斯中的柔性后验的伯恩斯块流

    Bernstein Flows for Flexible Posteriors in Variational Bayes

    [https://arxiv.org/abs/2202.05650](https://arxiv.org/abs/2202.05650)

    该论文提出了一种名为伯恩斯块流变分推断（BF-VI）的方法，能够灵活逼近复杂的多元后验，在实验中表现优于其他VI方法。

    

    变分推断（VI）是一种通过优化来近似难以计算后验的技术。与MCMC相比，VI可以扩展到许多观测。然而，在复杂后验的情况下，现有的VI方法通常产生令人不满意的后验近似。本文提出了伯恩斯块流变分推断（BF-VI），这是一种稳健且易于使用的方法，足够灵活以逼近复杂的多元后验。BF-VI结合了归一化流和基于伯恩斯多项式的转换模型的思想。在基准实验中，我们将BF-VI解与准确的后验、MCMC解以及基于归一化流的VI等现有VI方法进行了比较。我们发现在低维模型中，BF-VI可以准确逼近真实后验；而在高维模型中，BF-VI优于其他VI方法。此外，我们利用BF-VI针对半结构化Mela开发了一个贝叶斯模型。

    arXiv:2202.05650v2 Announce Type: replace-cross  Abstract: Variational inference (VI) is a technique to approximate difficult to compute posteriors by optimization. In contrast to MCMC, VI scales to many observations. In the case of complex posteriors, however, state-of-the-art VI approaches often yield unsatisfactory posterior approximations. This paper presents Bernstein flow variational inference (BF-VI), a robust and easy-to-use method, flexible enough to approximate complex multivariate posteriors. BF-VI combines ideas from normalizing flows and Bernstein polynomial-based transformation models. In benchmark experiments, we compare BF-VI solutions with exact posteriors, MCMC solutions, and state-of-the-art VI methods including normalizing flow based VI. We show for low-dimensional models that BF-VI accurately approximates the true posterior; in higher-dimensional models, BF-VI outperforms other VI methods. Further, we develop with BF-VI a Bayesian model for the semi-structured Mela
    
[^41]: 从有条件平稳时间序列中进行因果发现

    Causal Discovery from Conditionally Stationary Time Series

    [https://arxiv.org/abs/2110.06257](https://arxiv.org/abs/2110.06257)

    该论文提出了一种State-Dependent Causal Inference（SDCI）方法，可以处理一类宽泛的非平稳时间序列，成功地回复出潜在的因果依赖关系。

    

    因果发现，即从观测数据推断潜在的因果关系，已被证明对AI系统具有极大挑战。在时间序列建模背景下，传统的因果发现方法主要考虑具有完全观测变量和/或来自平稳时间序列的数据的受限场景。我们开发了一种因果发现方法来处理一类宽泛的非平稳时间序列，即在条件上是平稳的条件平稳时间序列，其中非平稳行为被建模为在一组（可能是隐藏的）状态变量上的平稳性。命名为State-Dependent Causal Inference（SDCI），我们的方法能够可证地回复出潜在的因果依赖关系，证明在完全观察到的状态下，并在存在隐藏状态时经验性地实现。后者通过对合成线性系统和非线性粒子相互作用数据的实验进行验证，SDCI实现了优于基线因果发现方法的性能。

    arXiv:2110.06257v2 Announce Type: replace  Abstract: Causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for AI systems. In time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. Named State-Dependent Causal Inference (SDCI), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. The latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where SDCI achieves superior performance over baseline causal discovery methods
    
[^42]: 具有贝叶斯神经网络的对抗样本检测

    Adversarial Examples Detection with Bayesian Neural Network

    [https://arxiv.org/abs/2105.08620](https://arxiv.org/abs/2105.08620)

    提出了一种基于贝叶斯神经网络的新框架，利用随机性模拟隐藏层输出分布，从而改善对抗样本检测性能。

    

    在本文中，我们提出了一个新的框架来检测对抗样本，其灵感来源于这样的观察结果：随机组件可以提高预测器的平滑性，使得更容易模拟深度神经网络的输出分布。基于这些观察结果，我们提出了一种新颖的贝叶斯对抗样本检测器，简称为BATer，以提高对抗样本检测的性能。具体而言，我们研究了自然样本和对抗样本之间隐藏层输出的分布差异，并建议使用贝叶斯神经网络的随机性来模拟隐藏层输出分布，并利用分布的离散性来检测对抗样本。

    arXiv:2105.08620v3 Announce Type: replace-cross  Abstract: In this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate the output distribution of a deep neural network. With these observations, we propose a novel Bayesian adversarial example detector, short for BATer, to improve the performance of adversarial example detection. Specifically, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of the Bayesian neural network to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. The advantage of a Bayesian neural network is that the output is stochastic while a deep neural network without random components does not have such characteristics. Empirical results on several benchmark datasets against popular a
    
[^43]: 具有不准确梯度的 Langevin Monte Carlo 的用户友好保证

    User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient

    [https://arxiv.org/abs/1710.00095](https://arxiv.org/abs/1710.00095)

    该论文分析了具有不准确梯度的 Langevin Monte Carlo 算法的采样问题，并在Wasserstein-2距离中提出了改进的误差保证。

    

    在本文中，我们研究了从已知光滑且强对数凹函数的概率密度函数中采样的问题。我们分析了基于(高度过阻尼) Langevin 扩散的离散化的近似采样方法，并建立了在Wasserstein-2距离中测量的误差保证。我们在三个方向上改进或扩展了最新结果。首先，我们对优化的不定步长一阶 Langevin Monte Carlo(LMC)算法的误差给出了上界。这个结果的优点是不受时间限制(我们无需事先知道目标精度)，并且在对应的常数步长结果基础上提升了对数因子。其次，我们研究了当无法准确评估对数密度梯度，但可以获得前述梯度的近似时的情况。

    arXiv:1710.00095v4 Announce Type: replace-cross  Abstract: In this paper, we study the problem of sampling from a given probability density function that is known to be smooth and strongly log-concave. We analyze several methods of approximate sampling based on discretizations of the (highly overdamped) Langevin diffusion and establish guarantees on its error measured in the Wasserstein-2 distance. Our guarantees improve or extend the state-of-the-art results in three directions. First, we provide an upper bound on the error of the first-order Langevin Monte Carlo (LMC) algorithm with optimized varying step-size. This result has the advantage of being horizon free (we do not need to know in advance the target precision) and to improve by a logarithmic factor the corresponding result for the constant step-size. Second, we study the case where accurate evaluations of the gradient of the log-density are unavailable, but one can have access to approximations of the aforementioned gradient.
    
[^44]: 基于得分结构先验的部分已知高斯图模型估计

    Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])

    [http://arxiv.org/abs/2401.14340](http://arxiv.org/abs/2401.14340)

    本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。

    

    我们提出了一种新的算法，用于支持估计部分已知的高斯图模型，并且结合了关于底层图的先验信息。与传统方法相比，传统方法使用点估计方法基于最大似然或最大后验准则，并使用（简单的）精度矩阵先验来提供点估计。我们考虑对图进行先验，并依赖退火朗格维能扩散从后验分布中生成样本。由于朗格维能采样器需要访问底层图先验的得分函数，因此我们使用图神经网络来有效地从图数据集（事先可用或从已知分布生成）估计得分。数值实验证明了我们方法的优势。

    We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
    
[^45]: 几乎等变性通过李代数卷积

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    

    最近，在机器学习中，模型相对于群作用的等变性已成为一个重要的研究课题。然而，赋予一个架构具体的群等变性对模型所期望看到的数据变换类型施加了强大的先验。严格等变模型强制执行对称性，但真实世界的数据并不总是符合这样的严格等变性，可能是因为数据中的噪声或仅编码了近似或部分对称性的潜在物理定律。在这种情况下，严格等变性的先验实际上可能过于强大，导致模型在真实数据上表现不佳。因此，在这项工作中，我们研究了一个相关的主题，即几乎等变性。我们提供了一个与当前文献中现有定义不同的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^46]: DataInf：在LLMs和扩散模型中高效估计数据影响力

    DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])

    [http://arxiv.org/abs/2310.00902](http://arxiv.org/abs/2310.00902)

    DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。

    

    量化训练数据点的影响力对于理解机器学习模型的输出和提高AI管道的透明度至关重要。影响函数是一种原则性和流行的数据归属方法，但其计算成本使其难以使用。这个问题在大型语言模型和文本到图像模型的设置中更加突出。在这项工作中，我们提出了DataInf，一种高效的影响力近似方法，适用于大规模生成型AI模型。通过利用易于计算的闭式表达式，DataInf在计算和内存效率方面优于现有的影响计算算法。我们的理论分析表明，DataInf特别适用于诸如LoRA的参数有效微调技术。通过系统的实证评估，我们展示了DataInf能够准确地近似影响分数，并且比现有方法快几个数量级。

    Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
    
[^47]: 卷积深度核机器

    Convolutional Deep Kernel Machines. (arXiv:2309.09814v1 [stat.ML])

    [http://arxiv.org/abs/2309.09814](http://arxiv.org/abs/2309.09814)

    这项研究介绍了一种称为卷积深度核机器的新型核方法，该方法纯粹使用核而不使用特征，通过高效的跨域诱导点近似方案和多种模型变体的设计，达到了在MNIST、CIFAR-10和CIFAR-100上接近甚至超过其他方法的测试准确率。

    

    深度核机器(DKMs)是一种最近引入的具有其他深度模型灵活性的核方法，包括深度神经网络和深度高斯过程。DKMs纯粹使用核，而不使用特征，因此与其他方法（从神经网络到深度核学习甚至深度高斯过程）不同，后者都使用特征作为基本组成部分。在这里，我们引入了卷积DKMs，并配以一种高效的跨域诱导点近似方案。此外，我们还开发并实验评估了许多模型变体，包括9种不同类型的为卷积DKMs设计的归一化方法，两种似然函数和两种不同类型的顶层。尽管只在约28个GPU小时内训练（比完全的NNGP / NTK / Myrtle kernel快1-2个数量级），但得到的模型在MNIST上实现了约99％的测试准确性，在CIFAR-10上为92％，在CIFAR-100上为71％，同时达到可比较的性能。

    Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
    
[^48]: 使用全射序列神经似然估计进行基于仿真的推断

    Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])

    [http://arxiv.org/abs/2308.01054](http://arxiv.org/abs/2308.01054)

    我们提出了一种使用全射序列神经似然估计（SSNL）进行基于仿真的推断的新方法，在模型中无法计算似然函数并且只能使用模拟器生成数据的情况下，SSNL通过拟合降维的全射归一化流模型，并将其作为替代似然函数，解决了先前基于似然方法在高维数据集中遇到的问题，并在各种实验中展示了其优越性能。

    

    我们提出了全射序列神经似然（SSNL）估计方法，这是一种在模型中无法计算似然函数并且只能使用可以生成合成数据的模拟器时进行基于仿真的推断的新方法。SSNL拟合一个降维的全射归一化流模型，并将其用作替代似然函数，从而可以使用传统的贝叶斯推断方法，包括马尔科夫链蒙特卡罗方法或变分推断。通过将数据嵌入到低维空间中，SSNL解决了先前基于似然方法在应用于高维数据集时遇到的几个问题，例如包含无信息数据维度或位于较低维流形上的数据。我们对SSNL在各种实验中进行了评估，并表明它通常优于在基于仿真推断中使用的现代方法，例如在一项来自天体物理学的具有挑战性的真实世界例子上对磁场模型的建模。

    We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic fi
    
[^49]: 学习具有不确定性的热力学约束状态方程

    Learning thermodynamically constrained equations of state with uncertainty. (arXiv:2306.17004v1 [physics.data-an])

    [http://arxiv.org/abs/2306.17004](http://arxiv.org/abs/2306.17004)

    本研究提出了一种数据驱动的机器学习方法，用于学习具有不确定性的热力学约束状态方程，并进行不确定性量化，以提高方程预测的可信度。

    

    高能量密度实验的数值模拟需要方程状态模型（EOS），用于关联材料的热力学状态变量 - 即压力、体积/密度、能量和温度。EOS模型通常采用半经验性参数化方法构建，假定具有与物理相关的函数形式，并使用实验/模拟数据进行校准。由于校准数据（参数不确定性）和假定的EOS函数形式（模型不确定性）中存在固有的不确定性，执行不确定性量化（UQ）以提高EOS预测的可信度是至关重要的。模型不确定性对于UQ研究来说是具有挑战性的，因为它需要探索所有可能的物理一致的函数形式空间。因此，通常在不违反热力学定律的情况下忽略模型不确定性而偏向于参数不确定性。本研究提出了一种数据驱动的机器学习方法来学习EOS模型的热力学约束方程。

    Numerical simulations of high energy-density experiments require equation of state (EOS) models that relate a material's thermodynamic state variables -specifically pressure, volume/density, energy, and temperature. EOS models are typically constructed using a semi-empirical parametric methodology, which assumes a physics-informed functional form with many tunable parameters calibrated using experimental/simulation data. Since there are inherent uncertainties in the calibration data (parametric uncertainty) and the assumed functional EOS form (model uncertainty), it is essential to perform uncertainty quantification (UQ) to improve confidence in the EOS predictions. Model uncertainty is challenging for UQ studies since it requires exploring the space of all possible physically consistent functional forms. Thus, it is often neglected in favor of parametric uncertainty, which is easier to quantify without violating thermodynamic laws. This work presents a data-driven machine learning a
    
[^50]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    

