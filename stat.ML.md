# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bayesian Causal Inference in Doubly Gaussian DAG-probit Models.](http://arxiv.org/abs/2304.05976) | 本文讨论在观测数据下，通过构造两个有向无环图，并共享公共参数来对两组的二元响应变量和协变量进行建模。双高斯DAG-probit模型是在此基础上提出的，在模型中我们可以估计每个节点的效应大小。 |
| [^2] | [Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box.](http://arxiv.org/abs/2304.05527) | 本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。 |
| [^3] | [ChemCrow: Augmenting large-language models with chemistry tools.](http://arxiv.org/abs/2304.05376) | 本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。 |
| [^4] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^5] | [Criticality versus uniformity in deep neural networks.](http://arxiv.org/abs/2304.04784) | 本文研究了深度神经网络在混沌边缘初始化时的训练能力，发现饱和的激活函数会妨碍训练效率。结果表明，沿混沌边缘初始化只是获得最佳可训练性所必需但不充分的条件。 |
| [^6] | [Replicability and stability in learning.](http://arxiv.org/abs/2304.03757) | 该论文研究了机器学习中的可复制性和全局稳定性，并证明许多学习任务只能弱化地实现全局稳定性。 |
| [^7] | [Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series.](http://arxiv.org/abs/2304.03069) | 本文提出了一种适用于非平稳时间序列的自适应学生t分布方法，基于方法的一般自适应矩可以使用廉价的指数移动平均值（EMA）来估计参数。 |
| [^8] | [Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer.](http://arxiv.org/abs/2303.08622) | 本文提出了一种适用于文本引导图像风格迁移中的零样本对比损失方法，可以在不需要额外训练的情况下生成具有相同语义内容的图像。 |
| [^9] | [Benchmarking optimality of time series classification methods in distinguishing diffusions.](http://arxiv.org/abs/2301.13112) | 本研究提出使用似然比检验为基准测试TSC算法在区分扩散过程中的最优性。随机森林、ResNet和ROCKET算法在单变量时间序列和多元高斯过程中可以实现LRT最优性，但在分类高维非线性多元时间序列时是次优的。 |
| [^10] | [Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification.](http://arxiv.org/abs/2301.07609) | 该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。 |
| [^11] | [Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors.](http://arxiv.org/abs/2211.12005) | 本文提出自我集成保护技术，通过对训练数据添加不可感知的扰动，通过模型检查点的梯度发现这些样本，可以有效地防止竞争对手在数据上训练高性能模型。 |
| [^12] | [Analytic theory for the dynamics of wide quantum neural networks.](http://arxiv.org/abs/2203.16711) | 本研究分析了训练误差的梯度下降动态，提出了一个简单的分析公式，可以捕捉到宽量子神经网络损失函数的平均行为。我们预测并表征了随机量子电路残余训练误差作为系统参数的指数衰减。 |
| [^13] | [Robust Hybrid Learning With Expert Augmentation.](http://arxiv.org/abs/2202.03881) | 该论文介绍了一种名为“专家增强”的混合数据增强策略，可以将其纳入混合系统以提高泛化性能，该方法可以有效克服混合模型性能仅限于训练分布的限制。作者在三个控制实验中从常微分方程和偏微分方程建模动态系统，并在真实双摆数据集上评估了该方法的潜在应用。 |
| [^14] | [Zero-Truncated Poisson Regression for Sparse Multiway Count Data Corrupted by False Zeros.](http://arxiv.org/abs/2201.10014) | 本文提出了一种零截断泊松回归方法来处理被假零值污染的多维计数数据，无需区分真实和假的零计数，即可准确估计非负参数张量空间，处理效率高。 |
| [^15] | [Causal Inference Despite Limited Global Confounding via Mixture Models.](http://arxiv.org/abs/2112.11602) | 本论文提出了一种基于混合模型的因果推断方法，通过解决混合问题和恢复概率分布，可以确定原本无法确定的因果关系。 |
| [^16] | [Asymptotic bias of inexact Markov Chain Monte Carlo methods in high dimension.](http://arxiv.org/abs/2108.00682) | 本文研究了高维情况下近似MCMC方法的渐近偏差对不精确MCMC方法的不变概率测度和目标分布之间的Wasserstein距离和对于有限相互作用的模型，渐近偏差有类似的依赖关系。 |
| [^17] | [In-Network Learning: Distributed Training and Inference in Networks.](http://arxiv.org/abs/2107.03433) | 提出了一种在移动设备和分布式网络中实现分布式训练和推断的算法和架构，并实现了推断的传播和融合。与现有技术相比，该方法具有更好的性能表现。 |
| [^18] | [Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff.](http://arxiv.org/abs/2102.12736) | 该论文提出了一种基于贝叶斯后验共识分布的插补方法，可以在控制方差和前瞻性偏差权衡的同时进行时间序列插补，适用于金融等领域。 |
| [^19] | [SoK: Certified Robustness for Deep Neural Networks.](http://arxiv.org/abs/2009.04131) | 本文系统地研究了深度神经网络的认证鲁棒性，并提供了全面的基准测试。论文总结了关于凸松弛、混合整数规划和随机平滑等方法的最新研究进展，并讨论了其未来研究方向和应用。 |
| [^20] | [The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization.](http://arxiv.org/abs/2006.01244) | 本研究提出使用阶乘幂作为新的参数设置工具，可以简化或提高动量法和随机优化方法的收敛速度。 |
| [^21] | [Improving Certified Robustness via Statistical Learning with Logical Reasoning.](http://arxiv.org/abs/2003.00120) | 本研究通过将统计 ML 模型与逻辑推理组件集成，提出了方法来进一步提高 ML 模型的认证鲁棒性，同时给出了首个适用于马尔可夫逻辑网络的鲁棒性界限。 |
| [^22] | [Estimating uncertainty of earthquake rupture using Bayesian neural network.](http://arxiv.org/abs/1911.09660) | 本文使用贝叶斯神经网络估计地震断层不确定性，通过解决数据不足问题和确定导致断层的参数组合，并使用两千次的断层模拟来训练和测试模型，最终得分0.83。 |

# 详细

[^1]: 双高斯DAG-probit模型中的贝叶斯因果推断

    Bayesian Causal Inference in Doubly Gaussian DAG-probit Models. (arXiv:2304.05976v1 [stat.ME])

    [http://arxiv.org/abs/2304.05976](http://arxiv.org/abs/2304.05976)

    本文讨论在观测数据下，通过构造两个有向无环图，并共享公共参数来对两组的二元响应变量和协变量进行建模。双高斯DAG-probit模型是在此基础上提出的，在模型中我们可以估计每个节点的效应大小。

    

    我们考虑在观察数据下，对两个组的二元响应变量以及一组协变量进行建模。分组变量可以是混淆变量（治疗和结果的共同原因），性别，病例/对照组，种族等。给定协变量和一个二元潜变量，目标是构造两个有向无环图(DAGs),同时共享一些公共参数。表示变量的节点集对于两组是相同的，但表示变量之间因果关系的有向边可以有潜在的区别。对于每个组，我们还估计了每个节点的效应大小。我们假设每个组在其DAG下符合高斯分布。由于DAG的马尔科夫性质，给定父节点后，DAG的联合分布是条件独立的。我们在两个组下引入了高斯DAG-probit模型的概念，因此是双高斯DAG-probit模型。

    We consider modeling a binary response variable together with a set of covariates for two groups under observational data. The grouping variable can be the confounding variable (the common cause of treatment and outcome), gender, case/control, ethnicity, etc. Given the covariates and a binary latent variable, the goal is to construct two directed acyclic graphs (DAGs), while sharing some common parameters. The set of nodes, which represent the variables, are the same for both groups but the directed edges between nodes, which represent the causal relationships between the variables, can be potentially different. For each group, we also estimate the effect size for each node. We assume that each group follows a Gaussian distribution under its DAG. Given the parent nodes, the joint distribution of DAG is conditionally independent due to the Markov property of DAGs. We introduce the concept of Gaussian DAG-probit model under two groups and hence doubly Gaussian DAG-probit model. To estima
    
[^2]: 一种使用确定性目标的黑匣子变分推断：更快，更精确，更黑。

    Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])

    [http://arxiv.org/abs/2304.05527](http://arxiv.org/abs/2304.05527)

    本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。

    

    自动微分变分推断（ADVI）提供了多种现代概率编程语言中快速易用的后验近似方法。然而它的随机优化器缺乏明确的收敛标准，并且需要调整参数。此外，ADVI继承了均值场变分贝叶斯（MFVB）的较差后验不确定性估计。我们引入了“确定性ADVI”（DADVI）来解决这些问题。DADVI用固定的蒙特卡罗近似替换了MFVB的不可解目标，这一技术在随机优化文献中被称为“样本平均近似”（SAA）。通过优化近似但确定的目标，DADVI可以使用现成的二阶优化，而且与标准均值场ADVI不同的是，可以适用于更准确的后验线性响应（LR）协方差估计。与现有的最坏情况理论相反，我们表明，在某些常见的统计问题类别上，DADVI和SAA可以表现得更好。

    Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
    
[^3]: ChemCrow:用化学工具增强大型语言模型

    ChemCrow: Augmenting large-language models with chemistry tools. (arXiv:2304.05376v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.05376](http://arxiv.org/abs/2304.05376)

    本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。

    

    近期大型语言模型(LLMs)在跨领域的任务表现出一定的优势，但在化学相关问题上却表现不佳。此外，这些模型缺乏访问外部知识源，限制了它们在科学应用中的有用性。在本研究中，我们介绍了ChemCrow，一种LLM化学代理，旨在完成有机合成、药物发现和材料设计等任务。通过整合13个专家设计的工具，ChemCrow提高了LLM在化学中的性能，并产生了新的能力。我们的评估，包括LLM和人类专家评估，证明了ChemCrow在自动化各种化学任务方面的有效性。令人惊讶的是，我们发现GPT-4作为评估器无法区分明显错误的GPT-4完成和GPT-4 + ChemCrow性能。这种工具的滥用有很大的风险，我们讨论了它们的潜在危害。在负责任的情况下，ChemCrow不仅可以帮助专业化学家并降低成本。

    Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers ba
    
[^4]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^5]: 深度神经网络中的临界性与均匀性比较

    Criticality versus uniformity in deep neural networks. (arXiv:2304.04784v1 [cs.LG])

    [http://arxiv.org/abs/2304.04784](http://arxiv.org/abs/2304.04784)

    本文研究了深度神经网络在混沌边缘初始化时的训练能力，发现饱和的激活函数会妨碍训练效率。结果表明，沿混沌边缘初始化只是获得最佳可训练性所必需但不充分的条件。

    

    沿着混沌边缘初始化的深层前馈网络表现出指数级优越的训练能力，其最大可训练深度可以量化。本文探讨了沿混沌边缘饱和tanh激活函数的影响。具体而言，我们确定了相空间中最大熵的后激活分布的均匀性线。该线交叉于混沌边缘，并指示了超过激活函数饱和开始妨碍训练效率的区域。我们的结果表明，沿混沌边缘初始化是获得最佳可训练性所必需但不充分的条件。

    Deep feedforward networks initialized along the edge of chaos exhibit exponentially superior training ability as quantified by maximum trainable depth. In this work, we explore the effect of saturation of the tanh activation function along the edge of chaos. In particular, we determine the line of uniformity in phase space along which the post-activation distribution has maximum entropy. This line intersects the edge of chaos, and indicates the regime beyond which saturation of the activation function begins to impede training efficiency. Our results suggest that initialization along the edge of chaos is a necessary but not sufficient condition for optimal trainability.
    
[^6]: 学习中的可复制性和稳定性

    Replicability and stability in learning. (arXiv:2304.03757v1 [cs.LG])

    [http://arxiv.org/abs/2304.03757](http://arxiv.org/abs/2304.03757)

    该论文研究了机器学习中的可复制性和全局稳定性，并证明许多学习任务只能弱化地实现全局稳定性。

    

    可复制性是科学中的关键，因为它使我们能够验证和验证研究结果。Impagliazzo、Lei、Pitassi和Sorrell（'22）最近开始研究机器学习中的可复制性。如果同一算法在两个独立同分布输入上使用相同的内部随机性时通常产生相同的输出，则学习算法是可复制的。我们研究了一种不涉及固定随机性的可复制性变体。如果一个算法在两个独立同分布的输入上（不固定内部随机性）应用时通常产生相同的输出，则算法满足这种形式的可复制性。这个变种被称为全局稳定性，并在差分隐私的上下文中由Bun、Livni和Moran（'20）介绍。 Impagliazzo等人展示了如何提高任何可复制算法的效果，以使其产生的输出概率无限接近于1。相反，我们证明了对于许多学习任务，只能弱化地实现全局稳定性，这里输出只有相同的部分。

    Replicability is essential in science as it allows us to validate and verify research findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recently initiated the study of replicability in machine learning. A learning algorithm is replicable if it typically produces the same output when applied on two i.i.d. inputs using the same internal randomness. We study a variant of replicability that does not involve fixing the randomness. An algorithm satisfies this form of replicability if it typically produces the same output when applied on two i.i.d. inputs (without fixing the internal randomness). This variant is called global stability and was introduced by Bun, Livni and Moran (`20) in the context of differential privacy.  Impagliazzo et al. showed how to boost any replicable algorithm so that it produces the same output with probability arbitrarily close to 1. In contrast, we demonstrate that for numerous learning tasks, global stability can only be accomplished weakly, where the same o
    
[^7]: 自适应学生t分布与方法矩移动估计器用于非平稳时间序列

    Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series. (arXiv:2304.03069v1 [stat.ME])

    [http://arxiv.org/abs/2304.03069](http://arxiv.org/abs/2304.03069)

    本文提出了一种适用于非平稳时间序列的自适应学生t分布方法，基于方法的一般自适应矩可以使用廉价的指数移动平均值（EMA）来估计参数。

    

    真实的时间序列通常是非平稳的，这带来了模型适应的难题。传统方法如GARCH假定任意类型的依赖性。为了避免这种偏差，我们将着眼于最近提出的不可知的移动估计器哲学：在时间$t$找到优化$F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$移动对数似然的参数，随时间演化。例如，它允许使用廉价的指数移动平均值（EMA）来估计参数，例如绝对中心矩$E[|x-\mu|^p]$随$p\in\mathbb{R}^+$的变化而演化$m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$。这种基于方法的一般自适应矩的应用将呈现在学生t分布上，尤其是在经济应用中流行，这里应用于DJIA公司的对数收益率。

    The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like GARCH assume arbitrary type of dependence. To prevent such bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $E[|x-\mu|^p]$ evolving with $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$ for one or multiple powers $p\in\mathbb{R}^+$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies.
    
[^8]: 零样本对比损失用于文本引导扩散图像风格迁移

    Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])

    [http://arxiv.org/abs/2303.08622](http://arxiv.org/abs/2303.08622)

    本文提出了一种适用于文本引导图像风格迁移中的零样本对比损失方法，可以在不需要额外训练的情况下生成具有相同语义内容的图像。

    

    扩散模型在文本引导图像风格迁移中表现出极大的潜力，但由于其随机性而存在风格转换和内容保护之间的权衡。现有方法需要计算密集的扩散模型微调或附加神经网络。为了解决这个问题，我们在扩散模型中提出了一种零样本对比损失，它不需要额外的微调或辅助网络。通过利用预训练的扩散模型中生成样本和原始图像嵌入之间的图块对比损失，我们的方法可以以零样本的方式生成具有与源图像相同语义内容的图像。我们的方法在保留内容且不需要额外训练的同时，在图像风格迁移、图像到图像的转换和操作中均优于现有方法。我们的实验结果证实了我们提出的方法的有效性。

    Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
    
[^9]: 时间序列分类方法在分辨扩散过程中的最优性基准测试

    Benchmarking optimality of time series classification methods in distinguishing diffusions. (arXiv:2301.13112v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13112](http://arxiv.org/abs/2301.13112)

    本研究提出使用似然比检验为基准测试TSC算法在区分扩散过程中的最优性。随机森林、ResNet和ROCKET算法在单变量时间序列和多元高斯过程中可以实现LRT最优性，但在分类高维非线性多元时间序列时是次优的。

    

    统计最优性基准测试对于分析和设计时间序列分类（TSC）算法至关重要。本研究提出使用似然比检验（LRT）基准测试TSC算法在区分扩散过程中的最优性。LRT是根据Neyman-Pearson引理得出的最优分类器。LRT基准测试计算效率高，因为LRT不需要训练，并且扩散过程可以进行高效模拟，可以灵活反映出真实世界应用的特定特征。我们使用三种常用的TSC算法进行基准测试：随机森林、ResNet和ROCKET。这些算法可以实现单变量时间序列和多元高斯过程的LRT最优性。但是，这些模型无关的算法在分类高维非线性多元时间序列时是次优的。此外，LRT基准测试提供了工具来分析分类准确性与时间依赖性的关系。

    Statistical optimality benchmarking is crucial for analyzing and designing time series classification (TSC) algorithms. This study proposes to benchmark the optimality of TSC algorithms in distinguishing diffusion processes by the likelihood ratio test (LRT). The LRT is an optimal classifier by the Neyman-Pearson lemma. The LRT benchmarks are computationally efficient because the LRT does not need training, and the diffusion processes can be efficiently simulated and are flexible to reflect the specific features of real-world applications. We demonstrate the benchmarking with three widely-used TSC algorithms: random forest, ResNet, and ROCKET. These algorithms can achieve the LRT optimality for univariate time series and multivariate Gaussian processes. However, these model-agnostic algorithms are suboptimal in classifying high-dimensional nonlinear multivariate time series. Additionally, the LRT benchmark provides tools to analyze the dependence of classification accuracy on the time 
    
[^10]: 物理学知识作为不确定性量化模型的信息场理论

    Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07609](http://arxiv.org/abs/2301.07609)

    该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    

    数据驱动的方法结合物理学知识是建模系统的强有力技术。此类模型的目标是通过将测量结果与已知物理定律相结合，高效地求解基本场。由于许多系统包含未知元素，如缺失参数、嘈杂数据或不完整的物理定律，因此这通常被视为一种不确定性量化问题。处理所有变量的常见技术通常取决于用于近似后验的数值方案，并且希望有一种不依赖于任何离散化的方法。信息场理论（IFT）提供了对不一定是高斯场的场进行统计学的工具。我们通过将描述场的物理定律的信息编码为函数先验来扩展IFT到物理信息场理论（PIFT）。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
    
[^11]: 自我集成保护：训练检查点是良好的数据保护者

    Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors. (arXiv:2211.12005v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12005](http://arxiv.org/abs/2211.12005)

    本文提出自我集成保护技术，通过对训练数据添加不可感知的扰动，通过模型检查点的梯度发现这些样本，可以有效地防止竞争对手在数据上训练高性能模型。

    

    随着数据变得越来越重要，公司在发布数据时通常会非常谨慎，因为竞争对手可以使用它来训练高性能模型，从而对公司的商业竞争力造成巨大威胁。为了防止在数据上训练良好的模型，我们可以对其添加不可感知的扰动。由于这样的扰动旨在伤害整个训练过程，因此它们应该反映DNN训练的脆弱性，而不是单个模型的脆弱性。基于这个新想法，我们寻找在训练中始终无法识别（从未被正确分类）的扰动样本。在本文中，我们通过模型检查点的梯度发现这些样本，形成所提出的自我集成保护（SEP）。该方法非常有效，因为（1）在正常训练过程中忽略的示例上进行学习往往会产生不忽略正常示例的DNN；（2）检查点之间跨模型的梯度与正交接近，表示它们与具有不同体系结构的DNN一样多样化。

    As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. Th
    
[^12]: 宽量子神经网络动力学的分析理论

    Analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.16711](http://arxiv.org/abs/2203.16711)

    本研究分析了训练误差的梯度下降动态，提出了一个简单的分析公式，可以捕捉到宽量子神经网络损失函数的平均行为。我们预测并表征了随机量子电路残余训练误差作为系统参数的指数衰减。

    

    参数量子电路可用作量子神经网络，并有潜力在解决学习问题时优于它们的经典对应物。迄今为止，大部分关于它们在实际问题上表现的结果是启发式的。特别是，对于量子神经网络的训练收敛率还没有完全理解。在这里，我们分析梯度下降的动态，研究一类可变量量子机器学习模型的训练误差。我们将宽量子神经网络定义为带有大量量子位和可变参数的参数化量子电路极限。我们然后发现了一个简单的分析公式，可以捕捉到它们损失函数的平均行为，并讨论了我们研究的结果的后果。例如，对于随机量子电路，我们预测并表征了残余训练误差作为系统参数的指数衰减。我们最终通过各种量子电路的数值模拟验证了我们的分析理论，并展示了与我们的预测一致的结果。

    Parameterized quantum circuits can be used as quantum neural networks and have the potential to outperform their classical counterparts when trained for addressing learning problems. To date, much of the results on their performance on practical problems are heuristic in nature. In particular, the convergence rate for the training of quantum neural networks is not fully understood. Here, we analyze the dynamics of gradient descent for the training error of a class of variational quantum machine learning models. We define wide quantum neural networks as parameterized quantum circuits in the limit of a large number of qubits and variational parameters. We then find a simple analytic formula that captures the average behavior of their loss function and discuss the consequences of our findings. For example, for random quantum circuits, we predict and characterize an exponential decay of the residual training error as a function of the parameters of the system. We finally validate our analy
    
[^13]: 强韧的混合学习：专家增强

    Robust Hybrid Learning With Expert Augmentation. (arXiv:2202.03881v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03881](http://arxiv.org/abs/2202.03881)

    该论文介绍了一种名为“专家增强”的混合数据增强策略，可以将其纳入混合系统以提高泛化性能，该方法可以有效克服混合模型性能仅限于训练分布的限制。作者在三个控制实验中从常微分方程和偏微分方程建模动态系统，并在真实双摆数据集上评估了该方法的潜在应用。

    

    混合建模通过将专家模型与从数据中学习的机器学习（ML）组件相结合，减少了专家模型的错误建模。与许多机器学习算法类似，混合模型的性能保证仅限于训练分布。利用专家模型通常在训练域外也适用的见解，我们通过引入一种名为专家增强的混合数据增强策略克服了这个限制。基于混合建模的概率形式化，我们证明了可以将专家增强纳入现有的混合系统以提高泛化性能。我们还在三个控制实验中从常微分方程和偏微分方程建模动态系统。最后，我们评估了专家增强在真实双摆数据集上的潜在现实世界应用。

    Hybrid modelling reduces the misspecification of expert models by combining them with machine learning (ML) components learned from data. Similarly to many ML algorithms, hybrid model performance guarantees are limited to the training distribution. Leveraging the insight that the expert model is usually valid even outside the training domain, we overcome this limitation by introducing a hybrid data augmentation strategy termed \textit{expert augmentation}. Based on a probabilistic formalization of hybrid modelling, we demonstrate that expert augmentation, which can be incorporated into existing hybrid systems, improves generalization. We empirically validate the expert augmentation on three controlled experiments modelling dynamical systems with ordinary and partial differential equations. Finally, we assess the potential real-world applicability of expert augmentation on a dataset of a real double pendulum.
    
[^14]: 针对稀疏多维计数数据中的假零值的零截断泊松回归

    Zero-Truncated Poisson Regression for Sparse Multiway Count Data Corrupted by False Zeros. (arXiv:2201.10014v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2201.10014](http://arxiv.org/abs/2201.10014)

    本文提出了一种零截断泊松回归方法来处理被假零值污染的多维计数数据，无需区分真实和假的零计数，即可准确估计非负参数张量空间，处理效率高。

    

    我们提出了一种新颖的统计推断方法，用于处理被假零值污染的多维计数数据，这些假零值和真实的零计数无法区分。我们的方法包括将泊松分布进行零截断，以消除所有零值。这种简单的截断方法避免了区分真实零计数和假零计数的需求，并减少了需要处理的数据量。推断是通过在泊松参数空间上施加低秩张量结构的张量完成的。我们的主要结果表明，可以根据非负标准分解中的从约IR^2log_2^2(I)个近似非零计数的N重秩为R的参数张量M可以通过零截断泊松回归进行准确估计。当参数从下面统一约束时，我们的结果也量化了零截断泊松分布的误差。

    We propose a novel statistical inference methodology for multiway count data that is corrupted by false zeros that are indistinguishable from true zero counts. Our approach consists of zero-truncating the Poisson distribution to neglect all zero values. This simple truncated approach dispenses with the need to distinguish between true and false zero counts and reduces the amount of data to be processed. Inference is accomplished via tensor completion that imposes low-rank tensor structure on the Poisson parameter space.  Our main result shows that an $N$-way rank-$R$ parametric tensor $\boldsymbol{\mathscr{M}}\in(0,\infty)^{I\times \cdots\times I}$ generating Poisson observations can be accurately estimated by zero-truncated Poisson regression from approximately $IR^2\log_2^2(I)$ non-zero counts under the nonnegative canonical polyadic decomposition. Our result also quantifies the error made by zero-truncating the Poisson distribution when the parameter is uniformly bounded from below.
    
[^15]: 通过混合模型进行有限全局混淆的因果推断

    Causal Inference Despite Limited Global Confounding via Mixture Models. (arXiv:2112.11602v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11602](http://arxiv.org/abs/2112.11602)

    本论文提出了一种基于混合模型的因果推断方法，通过解决混合问题和恢复概率分布，可以确定原本无法确定的因果关系。

    

    贝叶斯网络是一组$n$个随机变量（图的顶点）上的有向无环图（DAG）; 贝叶斯网络分布（BND）是在图上马尔可夫的随机变量的概率分布。这种模型的有限$k$-混合由一个更大的图形式化表示，该图具有一个额外的“隐藏”（或“潜在”）随机变量$U$，其范围为$\{1,\ldots,k\}$，并且$U$到每个其他顶点都有一个有向边。这种类型的模型在因果推断中是基本的，其中$U$模拟了多个群体的未观察到的混淆效应，使得可观察的DAG中的因果关系变得模糊不清。通过解决混合问题并恢复$U$上的联合概率分布，传统上无法确定的因果关系变得可确定。通过将其约化为更为研究的“空”图中的“乘积”情况，我们提出了第一个学习非空DAG的混合算法。

    A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random variables (the vertices); a Bayesian Network Distribution (BND) is a probability distribution on the random variables that is Markovian on the graph. A finite $k$-mixture of such models is graphically represented by a larger graph which has an additional "hidden" (or "latent") random variable $U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other vertex. Models of this type are fundamental to causal inference, where $U$ models an unobserved confounding effect of multiple populations, obscuring the causal relationships in the observable DAG. By solving the mixture problem and recovering the joint probability distribution on $U$, traditionally unidentifiable causal relationships become identifiable. Using a reduction to the more well-studied "product" case on empty graphs, we give the first algorithm to learn mixtures of non-empty DAGs.
    
[^16]: 高维情况下近似马尔可夫链蒙特卡罗方法的渐近偏差

    Asymptotic bias of inexact Markov Chain Monte Carlo methods in high dimension. (arXiv:2108.00682v2 [math.PR] UPDATED)

    [http://arxiv.org/abs/2108.00682](http://arxiv.org/abs/2108.00682)

    本文研究了高维情况下近似MCMC方法的渐近偏差对不精确MCMC方法的不变概率测度和目标分布之间的Wasserstein距离和对于有限相互作用的模型，渐近偏差有类似的依赖关系。

    

    近似马尔可夫链蒙特卡罗(MCMC)方法依赖于不完全保留目标分布的马尔科夫链，例如未调整的Langevin算法(ULA)和未调整的Hamilton蒙特卡罗(uHMC)。本文研究了在维度和离散步长两个方面深入了解了这种渐近偏差对不精确MCMC方法的不变概率测度和目标分布之间的Wasserstein距离的影响。我们假设精确或近似动力学的收敛性到平衡的Wasserstein边界，证明了对于ULA和uHMC，渐近偏差取决于与方案的目标分布或稳态概率测度有关的关键量。作为一个推论，我们得出结论，对于具有有限相互作用的模型，例如均场模型、有限范围图模型和其扰动模型，渐近偏差对这些模型有类似的依赖关系。

    Inexact Markov Chain Monte Carlo methods rely on Markov chains that do not exactly preserve the target distribution. Examples include the unadjusted Langevin algorithm (ULA) and unadjusted Hamiltonian Monte Carlo (uHMC). This paper establishes bounds on Wasserstein distances between the invariant probability measures of inexact MCMC methods and their target distributions with a focus on understanding the precise dependence of this asymptotic bias on both dimension and discretization step size. Assuming Wasserstein bounds on the convergence to equilibrium of either the exact or the approximate dynamics, we show that for both ULA and uHMC, the asymptotic bias depends on key quantities related to the target distribution or the stationary probability measure of the scheme. As a corollary, we conclude that for models with a limited amount of interactions such as mean-field models, finite range graphical models, and perturbations thereof, the asymptotic bias has a similar dependence on the s
    
[^17]: 网络内学习：分布式训练和网络中的推断

    In-Network Learning: Distributed Training and Inference in Networks. (arXiv:2107.03433v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.03433](http://arxiv.org/abs/2107.03433)

    提出了一种在移动设备和分布式网络中实现分布式训练和推断的算法和架构，并实现了推断的传播和融合。与现有技术相比，该方法具有更好的性能表现。

    

    现代机器学习技术的成功使得移动设备和无线网络能够实现重要的新服务，然而，由于数据和处理能力在无线网络中高度分布，这也带来了重大挑战。本文提出了一种学习算法和架构，利用多个数据流和处理单元，不仅在训练阶段而且在推断阶段进行推断传播和融合。我们研究了提出方法的设计准则及其对带宽的要求。同时，我们还讨论了在典型的无线电接入中使用神经网络的实现方面，并提供了实验证明了本方法相比现有技术的优势。

    It is widely perceived that leveraging the success of modern machine learning techniques to mobile devices and wireless networks has the potential of enabling important new services. This, however, poses significant challenges, essentially due to that both data and processing power are highly distributed in a wireless network. In this paper, we develop a learning algorithm and an architecture that make use of multiple data streams and processing units, not only during the training phase but also during the inference phase. In particular, the analysis reveals how inference propagates and fuses across a network. We study the design criterion of our proposed method and its bandwidth requirements. Also, we discuss implementation aspects using neural networks in typical wireless radio access; and provide experiments that illustrate benefits over state-of-the-art techniques.
    
[^18]: 用Wasserstein插值进行时间序列插补以达到最佳远期偏差和方差平衡

    Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff. (arXiv:2102.12736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.12736](http://arxiv.org/abs/2102.12736)

    该论文提出了一种基于贝叶斯后验共识分布的插补方法，可以在控制方差和前瞻性偏差权衡的同时进行时间序列插补，适用于金融等领域。

    

    缺失的时间序列数据是一个普遍存在的实际问题。时间序列数据中的插补方法通常被应用于完整的面板数据，目的是为了训练一个用于下游样本外任务的模型。例如，在金融中，缺失收益的插补可能被应用于在训练组合优化模型之前。然而，这种做法可能会导致未来性能上的前瞻性偏差。使用完整数据集进行插补存在使用只训练数据的插补中较大的方差之间的固有权衡。通过连接时间中显示的信息层次，我们提出了一个贝叶斯后验共识分布，它在插补中最优地控制了方差和前瞻性偏差的权衡。我们展示了我们的方法在合成和真实金融数据中的优点。

    Missing time-series data is a prevalent practical problem. Imputation methods in time-series data often are applied to the full panel data with the purpose of training a model for a downstream out-of-sample task. For example, in finance, imputation of missing returns may be applied prior to training a portfolio optimization model. Unfortunately, this practice may result in a look-ahead-bias in the future performance on the downstream task. There is an inherent trade-off between the look-ahead-bias of using the full data set for imputation and the larger variance in the imputation from using only the training data. By connecting layers of information revealed in time, we propose a Bayesian posterior consensus distribution which optimally controls the variance and look-ahead-bias trade-off in the imputation. We demonstrate the benefit of our methodology both in synthetic and real financial data.
    
[^19]: SoK: 深度神经网络的认证鲁棒性

    SoK: Certified Robustness for Deep Neural Networks. (arXiv:2009.04131v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.04131](http://arxiv.org/abs/2009.04131)

    本文系统地研究了深度神经网络的认证鲁棒性，并提供了全面的基准测试。论文总结了关于凸松弛、混合整数规划和随机平滑等方法的最新研究进展，并讨论了其未来研究方向和应用。

    

    深度神经网络在各种任务上取得了最先进的性能，但最近的研究表明，深度神经网络容易受到对抗攻击，这在将这些模型部署到自动驾驶等安全关键型应用时引起了重大关注。不同的防御方法已被提出来对抗对抗攻击，包括经验性防御和认证鲁棒性防御。本文系统化研究了认证鲁棒性防御方法及相关的实际和理论意义和发现。同时，我们还首次对各种数据集上的现有鲁棒性认证和训练方法进行了全面的基准测试。特别是，我们关注基于凸松弛、混合整数规划和随机平滑的认证鲁棒性方法。此外，我们总结了对更先进的深度神经网络，如图卷积神经网络（GCNN）和生成模型的认证鲁棒性的最新进展。最后，我们讨论了未来的研究方向和认证鲁棒性DNN的潜在应用。

    Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which can usually be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In par
    
[^20]: 阶乘幂的威力: (随机) 优化的新参数设置。

    The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization. (arXiv:2006.01244v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.01244](http://arxiv.org/abs/2006.01244)

    本研究提出使用阶乘幂作为新的参数设置工具，可以简化或提高动量法和随机优化方法的收敛速度。

    

    凸优化和非凸优化方法的收敛速度取决于许多常数的选择，包括步长、Lyapunov函数常数和动量常数。在本文中，我们提出使用阶乘幂作为定义出现在收敛证明中的常数的灵活工具。我们列举了这些数列具有的一些显著性质，并展示了如何将它们应用于加速梯度方法、动量法和随机方差缩减方法（SVRG）的收敛证明中，以简化或改进收敛速度。

    The convergence rates for convex and non-convex optimization methods depend on the choice of a host of constants, including step sizes, Lyapunov function constants and momentum constants. In this work we propose the use of factorial powers as a flexible tool for defining constants that appear in convergence proofs. We list a number of remarkable properties that these sequences enjoy, and show how they can be applied to convergence proofs to simplify or improve the convergence rates of the momentum method, accelerated gradient and the stochastic variance reduced method (SVRG).
    
[^21]: 通过逻辑推理与统计学习提高认证鲁棒性

    Improving Certified Robustness via Statistical Learning with Logical Reasoning. (arXiv:2003.00120v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2003.00120](http://arxiv.org/abs/2003.00120)

    本研究通过将统计 ML 模型与逻辑推理组件集成，提出了方法来进一步提高 ML 模型的认证鲁棒性，同时给出了首个适用于马尔可夫逻辑网络的鲁棒性界限。

    

    最近，针对复杂 ML 模型的认证鲁棒性快速提高需要进行密集的算法工作。然而，目前鲁棒性认证方法只能在有限的扰动半径内进行认证。考虑到现有的纯数据驱动的统计方法已经达到瓶颈，在本文中，我们提出使用马尔可夫逻辑网络（MLN）将统计 ML 模型与知识（通过逻辑规则表示）作为推理组件进行集成，以进一步提高整体的认证鲁棒性。这引发了关于认证这种范式（特别是推理组件，如 MLN）鲁棒性的新的研究问题。作为理解这些问题的第一步，我们首先证明了证明 MLN 鲁棒性计算复杂度是 #P-难的。在这个难度结果的指导下，我们通过仔细分析不同的模型规则，推导出了 MLN 的第一个认证的鲁棒性界限。最后，我们在两个广泛使用的数据集上进行了实验验证。

    Intensive algorithmic efforts have been made to enable the rapid improvements of certificated robustness for complex ML models recently. However, current robustness certification methods are only able to certify under a limited perturbation radius. Given that existing pure data-driven statistical approaches have reached a bottleneck, in this paper, we propose to integrate statistical ML models with knowledge (expressed as logical rules) as a reasoning component using Markov logic networks (MLN, so as to further improve the overall certified robustness. This opens new research questions about certifying the robustness of such a paradigm, especially the reasoning component (e.g., MLN). As the first step towards understanding these questions, we first prove that the computational complexity of certifying the robustness of MLN is #P-hard. Guided by this hardness result, we then derive the first certified robustness bound for MLN by carefully analyzing different model regimes. Finally, we c
    
[^22]: 使用贝叶斯神经网络估计地震断层的不确定性

    Estimating uncertainty of earthquake rupture using Bayesian neural network. (arXiv:1911.09660v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1911.09660](http://arxiv.org/abs/1911.09660)

    本文使用贝叶斯神经网络估计地震断层不确定性，通过解决数据不足问题和确定导致断层的参数组合，并使用两千次的断层模拟来训练和测试模型，最终得分0.83。

    

    贝叶斯神经网络（BNN）是一种结合了神经网络（NN）和随机过程优势的概率模型。因此，BNN可以解决过度拟合问题，并在数据有限的应用中表现良好。地震断层研究就是这样一个数据不足的问题，科学家必须依靠许多试验和错误的数值或物理模型来进行研究。在这项工作中，使用BNN，（1）解决数据问题，（2）找出导致地震断裂的参数组合，（3）估计地震断层的不确定性。使用了两千次的断层模拟来训练和测试模型，在每个模拟中，一个简单的2D断层结构被考虑，在中心处具有高斯几何异质性，8个参数在每次模拟中变化。BNN的测试F1得分为0.83。

    Bayesian neural networks (BNN) are the probabilistic model that combines the strengths of both neural network (NN) and stochastic processes. As a result, BNN can combat overfitting and perform well in applications where data is limited. Earthquake rupture study is such a problem where data is insufficient, and scientists have to rely on many trial and error numerical or physical models. Lack of resources and computational expenses, often, it becomes hard to determine the reasons behind the earthquake rupture. In this work, a BNN has been used (1) to combat the small data problem and (2) to find out the parameter combinations responsible for earthquake rupture and (3) to estimate the uncertainty associated with earthquake rupture. Two thousand rupture simulations are used to train and test the model. A simple 2D rupture geometry is considered where the fault has a Gaussian geometric heterogeneity at the center, and eight parameters vary in each simulation. The test F1-score of BNN (0.83
    

