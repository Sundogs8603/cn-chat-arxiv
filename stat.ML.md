# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) | 展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。 |
| [^2] | [Robustly estimating heterogeneity in factorial data using Rashomon Partitions](https://arxiv.org/abs/2404.02141) | 通过使用拉细孟划分集，我们能够在因子数据中稳健地估计异质性，并将因子空间划分成协变量组合的“池”，以便区分结果的差异。 |
| [^3] | [Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies](https://arxiv.org/abs/2404.01930) | 提供了新的全面近似保证，支持最大增益比率和近似次模函数，包括基数约束下的最大化和最小成本覆盖保证，并且引入了自适应选择策略的新参数“最大增益比率”。 |
| [^4] | [Adversarial Combinatorial Bandits with Switching Costs](https://arxiv.org/abs/2404.01883) | 研究了具有切换成本的对抗性组合赌博机问题，推导了极小后悔的下限并设计了逼近算法。 |
| [^5] | [Supervised Autoencoder MLP for Financial Time Series Forecasting](https://arxiv.org/abs/2404.01866) | 通过监督型自编码器的使用和参数调整，可以显著提升金融时间序列预测的效果，对投资策略性能有重要影响。 |
| [^6] | [When does Subagging Work?](https://arxiv.org/abs/2404.01832) | 研究展示了在机器学习中一种流行的非参数方法——回归树上，子抽样聚合（subagging）的有效性，并发现对于任何给定的分裂数，subagging都可以优于单棵树，并且在较多分裂的情况下改进更大。 |
| [^7] | [Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy](https://arxiv.org/abs/2404.01830) | 提出了一种适用于未知记录策略和价值函数的双重稳健离线评估估计器DRUnknown，实现了最小渐近方差和半参数下界下最佳性能。 |
| [^8] | [Asymptotics of Language Model Alignment](https://arxiv.org/abs/2404.01730) | 本文提供了对最优KL约束的强化学习解的闭合形式刻画，证明了实现KL散度和奖励之间权衡的对齐方法必须近似最优KL约束的RL解。 |
| [^9] | [Preventing Model Collapse in Gaussian Process Latent Variable Models](https://arxiv.org/abs/2404.01697) | 本文通过理论分析投影方差对高斯过程潜变量模型的影响，以及集成了谱混合（SM）核和可微随机傅立叶特征（RFF）核逼近来解决核灵活性不足问题，从而防止模型崩溃。 |
| [^10] | [FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality](https://arxiv.org/abs/2404.01608) | 提出了一种通过不变性原则解决公平和泛化机器学习问题的方法，包括基于训练环境的oracle FAIRM，以及在线性模型中实现FAIRM的高效算法，在实验中表现出极小最优性。 |
| [^11] | [Propensity Score Alignment of Unpaired Multimodal Data](https://arxiv.org/abs/2404.01595) | 本文提出了一种解决多模态表示学习中对齐不配对样本挑战的方法，通过估计倾向得分来定义样本之间的距离。 |
| [^12] | [Fair MP-BOOST: Fair and Interpretable Minipatch Boosting](https://arxiv.org/abs/2404.01521) | Fair MP-Boost是一种旨在平衡公平性和准确性的Boosting方法，通过自适应学习特征和观测来选择小批量，以同时提高预测准确性和公平性。 |
| [^13] | [Can Biases in ImageNet Models Explain Generalization?](https://arxiv.org/abs/2404.01509) | 图像网模型的偏见是否能够解释模型的泛化问题，对此进行了大规模研究。 |
| [^14] | [Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance](https://arxiv.org/abs/2404.01436) | 本文提出了对于RMSProp和Adam在非凸优化中的紧致收敛性分析，首次展示了在最宽松的假设下的收敛性结果，并展示了RMSProp和Adam的迭代复杂度分别为$\mathcal O(\epsilon^{-4})$。 |
| [^15] | [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://arxiv.org/abs/2404.01413) | 本文通过比较数据取代和数据积累两种情况，发现累积数据可以防止模型崩溃。 |
| [^16] | [Learning to Solve Job Shop Scheduling under Uncertainty](https://arxiv.org/abs/2404.01308) | 该论文利用深度强化学习技术解决了具有不确定性的作业车间调度问题，重点在于提出了一种新颖方法来处理具有不确定持续时间的JSSP。 |
| [^17] | [Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning](https://arxiv.org/abs/2404.00015) | 提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。 |
| [^18] | [Sharp bounds for the max-sliced Wasserstein distance](https://arxiv.org/abs/2403.00666) | 对于可分希尔伯特空间上的概率测度与其经验分布之间的最大切片1-Wasserstein距离，得到了尖锐的上下界限。 |
| [^19] | [Learning Memory Kernels in Generalized Langevin Equations](https://arxiv.org/abs/2402.11705) | 提出一种学习广义朗之万方程中记忆核的新方法，通过正则化Prony方法估计相关函数并在Sobolev范数Loss函数和RKHS正则化下实现回归，在指数加权的$L^2$空间内获得改进性能，对比其他回归估计器展示了其优越性。 |
| [^20] | [Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data](https://arxiv.org/abs/2402.04498) | 本论文提出了一种名为路径空间卡尔曼滤波器（PKF）的扩展算法，可以动态跟踪数据和先前知识的不确定性，并用贝叶斯方法量化不同的不确定性来源。通过在合成数据集上进行数值实验，我们证明了PKF优于传统KF方法，并将该方法应用于生物时间序列数据集。 |
| [^21] | [Low-Rank MDPs with Continuous Action Spaces](https://arxiv.org/abs/2311.03564) | 该研究通过探索多种方法，将针对低秩MDPs的现有方法扩展到连续动作空间，同时保持近似正确的学习保证。 |
| [^22] | [From algorithms to action: improving patient care requires causality](https://arxiv.org/abs/2209.07397) | 改进患者护理需要考虑因果关系，建立和验证的预测模型必须对治疗决策的因果关系进行考虑，以避免在决策时造成伤害。 |
| [^23] | [VC dimension of Graph Neural Networks with Pfaffian activation functions.](http://arxiv.org/abs/2401.12362) | 本文分析了图神经网络（GNN）中使用不同常用激活函数（如sigmoid和双曲正切）时的VC维度，采用了Pfaffian函数理论框架，通过架构参数和合作数量提供了界限。 |
| [^24] | [Statistical inference for pairwise comparison models.](http://arxiv.org/abs/2401.08463) | 本论文通过建立极大似然估计量的渐近正态性结果，填补了配对比较模型中统计推断的空白，为各种配对比较模型提供了统一的方法，超越了Bradley-Terry模型，为实践者提供了坚实的理论保证。 |
| [^25] | [On the Stability of Iterative Retraining of Generative Models on their own Data.](http://arxiv.org/abs/2310.00429) | 本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。 |
| [^26] | [The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance.](http://arxiv.org/abs/2309.13775) | 提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。 |
| [^27] | [Samplet basis pursuit.](http://arxiv.org/abs/2306.10180) | 本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。 |
| [^28] | [Bayesian neural networks via MCMC: a Python-based tutorial.](http://arxiv.org/abs/2304.02595) | 本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。 |
| [^29] | [Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time.](http://arxiv.org/abs/2302.11068) | 本论文提出了一种在几乎线性时间内用鲁棒交替最小化方法完成低秩矩阵补全的方法，并证明了观察几乎线性数量的条目即可恢复矩阵$M$，此方法克服了交替最小化方法需要精确计算的限制，更符合实际实现中对效率的要求。 |
| [^30] | [On the Generalized Likelihood Ratio Test and One-Class Classifiers.](http://arxiv.org/abs/2210.12494) | 本文考虑了一类分类器和广义似然比检验的问题，证明了多层感知器神经网络和支持向量机模型在收敛时会表现为广义似然比检验。同时，作者还展示了一类最小二乘SVM在收敛时也能达到广义似然比检验的效果。 |
| [^31] | [CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty.](http://arxiv.org/abs/2208.08626) | 本文提出了一种新的CP-PINNs模型，通过将PINNs与总变差惩罚相结合，实现了准确的变点检测和PDE的发现。我们还开发了一种元学习算法，能够在数据的连续批次上动态改进优化目标。实证结果表明，在存在变点的情况下，该方法能够准确估计参数和模型对齐，在没有变点的情况下能够数值上收敛到原始PINNs模型的解。 |

# 详细

[^1]: 用简单自适应攻击越狱功能对齐的LLM

    Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks

    [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151)

    展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。

    

    我们展示了即使是最新的安全对齐的LLM也不具有抵抗简单自适应越狱攻击的稳健性。首先，我们展示了如何成功利用对logprobs的访问进行越狱：我们最初设计了一个对抗性提示模板（有时会适应目标LLM），然后我们在后缀上应用随机搜索以最大化目标logprob（例如token“Sure”），可能会进行多次重启。通过这种方式，我们实现了对GPT-3.5/4、Llama-2-Chat-7B/13B/70B、Gemma-7B和针对GCG攻击进行对抗训练的HarmBench上的R2D2等几乎100%的攻击成功率--根据GPT-4的评判。我们还展示了如何通过转移或预填充攻击以100%的成功率对所有不暴露logprobs的Claude模型进行越狱。此外，我们展示了如何在受污染的模型中使用对一组受限制的token执行随机搜索以查找木马字符串的方法--这项任务与许多其他任务共享相同的属性。

    arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
    
[^2]: 使用拉细孟划分在因子数据中稳健估计异质性

    Robustly estimating heterogeneity in factorial data using Rashomon Partitions

    [https://arxiv.org/abs/2404.02141](https://arxiv.org/abs/2404.02141)

    通过使用拉细孟划分集，我们能够在因子数据中稳健地估计异质性，并将因子空间划分成协变量组合的“池”，以便区分结果的差异。

    

    许多统计分析，无论是在观测数据还是随机对照试验中，都会问：感兴趣的结果如何随可观察协变量组合变化？不同的药物组合如何影响健康结果，科技采纳如何依赖激励和人口统计学？我们的目标是将这个因子空间划分成协变量组合的“池”，在这些池中结果会发生差异（但池内部不会发生），而现有方法要么寻找一个单一的“最优”分割，要么从可能分割的整个集合中抽样。这两种方法都忽视了这样一个事实：特别是在协变量之间存在相关结构的情况下，可能以许多种方式划分协变量空间，在统计上是无法区分的，尽管对政策或科学有着非常不同的影响。我们提出了一种名为拉细孟划分集的替代视角

    arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
    
[^3]: 自适应组合最大化：超越近似贪心策略

    Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies

    [https://arxiv.org/abs/2404.01930](https://arxiv.org/abs/2404.01930)

    提供了新的全面近似保证，支持最大增益比率和近似次模函数，包括基数约束下的最大化和最小成本覆盖保证，并且引入了自适应选择策略的新参数“最大增益比率”。

    

    我们研究自适应组合最大化，在机器学习中是一个核心挑战，应用于主动学习以及许多其他领域。我们研究贝叶斯设置，并考虑在基数约束和最小成本覆盖下的最大化目标。我们提供了新的全面近似保证，包含先前的结果，并对其进行了明显加强。我们的近似保证同时支持最大增益比率和近似次模函数，并包括基数约束下的最大化和最小成本覆盖保证。此外，我们为修改后的先验提供了一个近似保证，这对于获得不取决于先验中最小概率的主动学习保证至关重要。此外，我们发现了自适应选择策略的一个新参数，我们称之为“最大增益比率”。

    arXiv:2404.01930v1 Announce Type: new  Abstract: We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains. We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage. We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them. Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee. In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior. Moreover, we discover a new parameter of adaptive selection policies, which we term the "maximal gain ratio". We show
    
[^4]: 具有切换成本的对抗性组合赌博机问题

    Adversarial Combinatorial Bandits with Switching Costs

    [https://arxiv.org/abs/2404.01883](https://arxiv.org/abs/2404.01883)

    研究了具有切换成本的对抗性组合赌博机问题，推导了极小后悔的下限并设计了逼近算法。

    

    我们研究了具有切换成本$\lambda$的对抗性组合赌博机问题，考虑了赌博机反馈和半赌博机反馈设置。在忽视对手的情况下，我们推导了$K$个基本臂和时间跨度$T$的极小后悔的下限，并设计了算法来逼近这些下限。为了证明这些下限，我们为两种反馈设置设计了随机损失序列，借鉴了Dekel等人（2014年）之前工作中的一个思想。赌博机反馈的下限为$ \tilde{\Omega}\big( (\lambda K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$，而半赌博机反馈的下限为$ \tilde{\Omega}\big( (\lambda K I)^{\frac{1}{3}} T^{\frac{2}{3}}\big)$，其中$I$是在每一轮中播放的组合臂中的基本臂数量。

    arXiv:2404.01883v1 Announce Type: cross  Abstract: We study the problem of adversarial combinatorial bandit with a switching cost $\lambda$ for a switch of each selected arm in each round, considering both the bandit feedback and semi-bandit feedback settings. In the oblivious adversarial case with $K$ base arms and time horizon $T$, we derive lower bounds for the minimax regret and design algorithms to approach them. To prove these lower bounds, we design stochastic loss sequences for both feedback settings, building on an idea from previous work in Dekel et al. (2014). The lower bound for bandit feedback is $ \tilde{\Omega}\big( (\lambda K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$ while that for semi-bandit feedback is $ \tilde{\Omega}\big( (\lambda K I)^{\frac{1}{3}} T^{\frac{2}{3}}\big)$ where $I$ is the number of base arms in the combinatorial arm played in each round. To approach these lower bounds, we design algorithms that operate in batches by dividing the time horizon into batc
    
[^5]: 监督型自编码器多层感知机用于金融时间序列预测

    Supervised Autoencoder MLP for Financial Time Series Forecasting

    [https://arxiv.org/abs/2404.01866](https://arxiv.org/abs/2404.01866)

    通过监督型自编码器的使用和参数调整，可以显著提升金融时间序列预测的效果，对投资策略性能有重要影响。

    

    本文研究了如何通过使用神经网络中的监督型自编码器来增强金融时间序列预测，旨在改善投资策略表现。具体研究了噪声增强和三重障碍标记对风险调整回报的影响，使用夏普比率和信息比率。研究重点关注了从2010年1月1日至2022年4月30日期间作为交易资产的标普500指数，EUR/USD和BTC/USD。研究结果表明，具有平衡噪声增强和瓶颈大小的监督型自编码器显著提升了策略效果。然而，过多的噪声和大的瓶颈大小可能会损害表现，突出了精确参数调整的重要性。本文还提出了一种新的优化指标的推导，可与三重障碍标记一起使用。这项研究的结果对政策具有重要影响，暗示金融市场预测工具的持续改进。

    arXiv:2404.01866v1 Announce Type: new  Abstract: This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance. It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios. The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022. Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness. However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning. This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling. The results of this study have substantial policy implications, suggesting that financi
    
[^6]: 何时使用Subagging？

    When does Subagging Work?

    [https://arxiv.org/abs/2404.01832](https://arxiv.org/abs/2404.01832)

    研究展示了在机器学习中一种流行的非参数方法——回归树上，子抽样聚合（subagging）的有效性，并发现对于任何给定的分裂数，subagging都可以优于单棵树，并且在较多分裂的情况下改进更大。

    

    我们研究了在机器学习中一种流行的非参数方法——回归树上，子抽样聚合（subagging）的有效性。首先，我们给出了树的逐点一致性的充分条件。我们明确了（i）偏差取决于单元的直径，因此，具有少数分裂的树倾向于存在偏差，以及（ii）方差取决于单元中的观测数量，因此，具有许多分裂的树倾向于具有较大的方差。虽然这些关于偏差和方差的陈述在协变量空间中是全局适用的，我们展示了，在某些约束条件下，它们在局部也是成立的。第二，我们比较了子抽样聚合和具有不同分裂数的树的性能。我们发现，对于任何给定的分裂数，子抽样聚合都优于单棵树，并且这种改进在较多分裂的情况下比较少分裂的情况下更大。然而，一个以最佳大小生长的单棵树可以优于子抽样聚合。

    arXiv:2404.01832v1 Announce Type: cross  Abstract: We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform su
    
[^7]: 估计记录策略的双重稳健离线评估

    Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy

    [https://arxiv.org/abs/2404.01830](https://arxiv.org/abs/2404.01830)

    提出了一种适用于未知记录策略和价值函数的双重稳健离线评估估计器DRUnknown，实现了最小渐近方差和半参数下界下最佳性能。

    

    我们介绍了一种用于马尔可夫决策过程的新颖的双重稳健（DR）离线评估（OPE）估计器DRUnknown，旨在应对记录策略和价值函数均未知的情况。该估计器首先估计记录策略，然后通过最小化估计器的渐近方差来估计价值函数模型，同时考虑记录策略的估计效果。当记录策略模型正确指定时，DRUnknown在现有OPE估计器类中达到最小的渐近方差。当价值函数模型也被正确指定时，DRUnknown是最优的，因为它的渐近方差达到了半参数下界。我们在情境臂和强化学习中进行了实验结果，比较了DRUnknown与现有方法的性能。

    arXiv:2404.01830v1 Announce Type: cross  Abstract: We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.
    
[^8]: 语言模型对齐的渐进研究

    Asymptotics of Language Model Alignment

    [https://arxiv.org/abs/2404.01730](https://arxiv.org/abs/2404.01730)

    本文提供了对最优KL约束的强化学习解的闭合形式刻画，证明了实现KL散度和奖励之间权衡的对齐方法必须近似最优KL约束的RL解。

    

    让$p$表示一个生成式语言模型。让$r$表示一个奖励模型，返回一个标量，捕捉从$p$中抽取的内容被偏好的程度。语言模型对齐的目标是改变$p$为一个新的分布$\phi$，使得期望奖励更高，同时保持$\phi$接近$p$。一种流行的对齐方法是KL约束的强化学习（RL），选择一个分布$\phi_\Delta$，最大化$E_{\phi_{\Delta}} r(y)$，同时满足相对熵约束$KL(\phi_\Delta || p) \leq \Delta$。另一种简单的对齐方法是最佳-$N$，从$p$中抽取$N$个样本，并选择奖励最高的一个。在本文中，我们提供了最优KL约束的RL解的闭合形式刻画。我们证明了任何实现KL散度和奖励之间可比较的权衡的对齐方法，必须近似最优KL约束的RL解。

    arXiv:2404.01730v1 Announce Type: new  Abstract: Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\phi$ that results in a higher expected reward while keeping $\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\phi_\Delta$ that maximizes $E_{\phi_{\Delta}} r(y)$ subject to a relative entropy constraint $KL(\phi_\Delta || p) \leq \Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in t
    
[^9]: 防止高斯过程潜变量模型中的模型崩溃

    Preventing Model Collapse in Gaussian Process Latent Variable Models

    [https://arxiv.org/abs/2404.01697](https://arxiv.org/abs/2404.01697)

    本文通过理论分析投影方差对高斯过程潜变量模型的影响，以及集成了谱混合（SM）核和可微随机傅立叶特征（RFF）核逼近来解决核灵活性不足问题，从而防止模型崩溃。

    

    Gaussian process latent variable models (GPLVMs)是一类多才多艺的无监督学习模型，通常用于降维。然而，用GPLVMs对数据建模时常见的挑战包括核灵活性不足和投影噪声选择不当，导致了一种以模糊潜变量表示为主要特征的模型崩溃，这种表示不反映数据的潜在结构。本文首先从理论上通过线性GPLVM的视角研究了投影方差对模型崩溃的影响。其次，通过集成谱混合（SM）核和可微随机傅立叶特征（RFF）核逼近，解决了由于核灵活性不足导致的模型崩溃问题，从而保证了通过现成的自动微分工具实现学习核参数的计算可扩展性和效率。

    arXiv:2404.01697v1 Announce Type: cross  Abstract: Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hype
    
[^10]: FAIRM: 学习不变表示以实现算法公平性和域泛化的极小最优性

    FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality

    [https://arxiv.org/abs/2404.01608](https://arxiv.org/abs/2404.01608)

    提出了一种通过不变性原则解决公平和泛化机器学习问题的方法，包括基于训练环境的oracle FAIRM，以及在线性模型中实现FAIRM的高效算法，在实验中表现出极小最优性。

    

    机器学习方法通常假设测试数据与训练数据具有相同的分布。然而，由于应用中存在多个层次的异质性，这一假设可能不成立，从而引发算法公平性和域泛化方面的问题。在这项工作中，我们通过不变性原则解决了公平且具有泛化能力的机器学习问题。我们提出了一个基于训练环境的oracle，FAIRM，它在多样性类型条件下具有理想的公平性和域泛化特性。然后，我们在弱分布假设下提供了一个具有有限样本理论保证的经验FAIRM。我们还开发了有效的算法来在线性模型中实现FAIRM，并展示了具有极小最优性的非渐近性能。我们在合成数据和MNIST数据的数值实验中评估了我们的方法，并展示了其优于对应方法的表现。

    arXiv:2404.01608v1 Announce Type: cross  Abstract: Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts.
    
[^11]: 多模态数据无配对倾向得分对齐

    Propensity Score Alignment of Unpaired Multimodal Data

    [https://arxiv.org/abs/2404.01595](https://arxiv.org/abs/2404.01595)

    本文提出了一种解决多模态表示学习中对齐不配对样本挑战的方法，通过估计倾向得分来定义样本之间的距离。

    

    多模态表示学习技术通常依赖于配对样本来学习共同的表示，但在生物学等领域，往往难以收集配对样本，因为测量设备通常会破坏样本。本文介绍了一种解决多模态表示学习中对齐不配对样本的方法。我们将因果推断中的潜在结果与多模态观察中的潜在视图进行类比，这使我们能够使用Rubin的框架来估计一个共同的空间，以匹配样本。我们的方法假设我们收集了经过处理实验干扰的样本，并利用此来从每种模态中估计倾向得分，其中包括潜在状态和处理之间的所有共享信息，并可用于定义样本之间的距离。我们尝试了两种利用这一方法的对齐技术。

    arXiv:2404.01595v1 Announce Type: new  Abstract: Multimodal representation learning techniques typically rely on paired samples to learn common representations, but paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, which allows us to use Rubin's framework to estimate a common space in which to match samples. Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this di
    
[^12]: 公平的MP-BOOST: 公平且可解释的小批量 Boosting

    Fair MP-BOOST: Fair and Interpretable Minipatch Boosting

    [https://arxiv.org/abs/2404.01521](https://arxiv.org/abs/2404.01521)

    Fair MP-Boost是一种旨在平衡公平性和准确性的Boosting方法，通过自适应学习特征和观测来选择小批量，以同时提高预测准确性和公平性。

    

    集成方法，特别是Boosting，在表格数据中已被证明是高效且广泛应用的机器学习技术。在本文中，我们旨在利用传统Boosting方法的稳健预测能力，同时增强公平性和可解释性。为实现这一目标，我们开发了Fair MP-Boost，这是一种平衡公平性和准确性的随机Boosting方案，通过在训练期间自适应学习特征和观察来实现。具体来说，Fair MP-Boost依据自适应学习的特征和观察采样概率，顺序抽取小批量观察和特征，被称为minipatches (MP)。我们通过结合损失函数或特征重要性分数来设计这些概率，以同时解决准确性和公平性问题。因此，Fair MP-Boost优先考虑重要且公平的特征以及具有挑战性的实例，从而选择最相关的小批量。

    arXiv:2404.01521v1 Announce Type: cross  Abstract: Ensemble methods, particularly boosting, have established themselves as highly effective and widely embraced machine learning techniques for tabular data. In this paper, we aim to leverage the robust predictive power of traditional boosting methods while enhancing fairness and interpretability. To achieve this, we develop Fair MP-Boost, a stochastic boosting scheme that balances fairness and accuracy by adaptively learning features and observations during training. Specifically, Fair MP-Boost sequentially samples small subsets of observations and features, termed minipatches (MP), according to adaptively learned feature and observation sampling probabilities. We devise these probabilities by combining loss functions, or by combining feature importance scores to address accuracy and fairness simultaneously. Hence, Fair MP-Boost prioritizes important and fair features along with challenging instances, to select the most relevant minipatc
    
[^13]: 图像网模型中的偏见能解释泛化吗？

    Can Biases in ImageNet Models Explain Generalization?

    [https://arxiv.org/abs/2404.01509](https://arxiv.org/abs/2404.01509)

    图像网模型的偏见是否能够解释模型的泛化问题，对此进行了大规模研究。

    

    深度学习方法面临的主要挑战之一是模型对来自训练分布长尾的稀有内部分布（ID）样本和训练分布之外（OOD）样本的强大泛化能力。对于图像分类，这体现在对扭曲图像的攻击、性能下降以及对概念（如草图）的泛化不足。目前对神经网络泛化的理解非常有限，但发现了一些区别模型与人类视觉的偏见，这些偏见可能导致这些限制。因此，已经尝试了多种不同成功程度的方法来减少这些训练中的偏见以改善泛化性能。我们在已建立的ResNet-50架构上进行大规模研究，在48个通过不同获取途径获得的ImageNet模型上进行了实验。

    arXiv:2404.01509v1 Announce Type: cross  Abstract: The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different 
    
[^14]: RMSProp和Adam在具有仿射噪声方差的广义光滑非凸优化中的收敛性保证

    Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance

    [https://arxiv.org/abs/2404.01436](https://arxiv.org/abs/2404.01436)

    本文提出了对于RMSProp和Adam在非凸优化中的紧致收敛性分析，首次展示了在最宽松的假设下的收敛性结果，并展示了RMSProp和Adam的迭代复杂度分别为$\mathcal O(\epsilon^{-4})$。

    

    本文在坐标级别广义光滑性和仿射噪声方差的最宽松假设下，为非凸优化中的RMSProp和Adam提供了首个收敛性分析。首先分析了RMSProp，它是一种具有自适应学习率但没有一阶动量的Adam的特例。具体地，为了解决自适应更新、无界梯度估计和Lipschitz常数之间的依赖挑战，我们证明了下降引理中的一阶项收敛，并且其分母由梯度范数的函数上界限制。基于这一结果，我们展示了使用适当的超参数的RMSProp收敛到一个$\epsilon$-稳定点，其迭代复杂度为$\mathcal O(\epsilon^{-4})$。然后，将我们的分析推广到Adam，额外的挑战是由于梯度与一阶动量之间的不匹配。我们提出了一个新的上界限制

    arXiv:2404.01436v1 Announce Type: cross  Abstract: This paper provides the first tight convergence analyses for RMSProp and Adam in non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance. We first analyze RMSProp, which is a special case of Adam with adaptive learning rates but without first-order momentum. Specifically, to solve the challenges due to dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm. Based on this result, we show that RMSProp with proper hyperparameters converges to an $\epsilon$-stationary point with an iteration complexity of $\mathcal O(\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and first-order momentum. We develop a new upper bound on
    
[^15]: 模型崩溃是否不可避免？通过累积真实和合成数据打破递归的诅咒

    Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data

    [https://arxiv.org/abs/2404.01413](https://arxiv.org/abs/2404.01413)

    本文通过比较数据取代和数据积累两种情况，发现累积数据可以防止模型崩溃。

    

    随着生成模型的激增，以及在网络规模数据上的预训练，一个及时的问题浮出水面：当这些模型被训练在它们自己生成的输出上时会发生什么？最近对模型数据反馈循环的研究发现，这样的循环可能导致模型崩溃，即性能随着每次模型拟合迭代逐渐下降，直到最新的模型变得无用。然而，最近几篇研究模型崩溃的论文都假设随着时间推移，新数据会取代旧数据，而不是假设数据会随时间累积。在本文中，我们比较了这两种情况，并表明积累数据可以防止模型崩溃。我们首先研究了一个解析可处理的设置，其中一系列线性模型拟合到先前模型的预测。先前的工作表明，如果数据被替换，测试误差会随着模型拟合迭代次数线性增加；我们扩展了这个研究探讨了数据逐渐累积的情况下会发生什么。

    arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
    
[^16]: 学习在不确定性下解决作业车间调度问题

    Learning to Solve Job Shop Scheduling under Uncertainty

    [https://arxiv.org/abs/2404.01308](https://arxiv.org/abs/2404.01308)

    该论文利用深度强化学习技术解决了具有不确定性的作业车间调度问题，重点在于提出了一种新颖方法来处理具有不确定持续时间的JSSP。

    

    作业车间调度问题（JSSP）是一个组合优化问题，其中任务需要在机器上进行调度，以最小化诸如最大完工时间或延迟等标准。为了解决更加现实的场景，我们为每个任务的持续时间关联了一个概率分布。我们的目标是生成一个稳健的调度，即最小化平均完工时间。本文引入了一种新方法，利用深度强化学习（DRL）技术来寻找稳健的解决方案，重点关注具有不确定持续时间的JSSP。本研究的关键贡献包括：（1）DRL在JSSP应用中的进展，增强泛化性和可伸缩性，（2）一种新颖的方法来解决具有不确定持续时间的JSSP。 Wheatley方法，集成了图神经网络（GNNs）和DRL，已公开可用于进一步的研究和应用。

    arXiv:2404.01308v1 Announce Type: new  Abstract: Job-Shop Scheduling Problem (JSSP) is a combinatorial optimization problem where tasks need to be scheduled on machines in order to minimize criteria such as makespan or delay. To address more realistic scenarios, we associate a probability distribution with the duration of each task. Our objective is to generate a robust schedule, i.e. that minimizes the average makespan. This paper introduces a new approach that leverages Deep Reinforcement Learning (DRL) techniques to search for robust solutions, emphasizing JSSPs with uncertain durations. Key contributions of this research include: (1) advancements in DRL applications to JSSPs, enhancing generalization and scalability, (2) a novel method for addressing JSSPs with uncertain durations. The Wheatley approach, which integrates Graph Neural Networks (GNNs) and DRL, is made publicly available for further research and applications.
    
[^17]: 利用量子增强机器学习赋能信用评分系统

    Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning

    [https://arxiv.org/abs/2404.00015](https://arxiv.org/abs/2404.00015)

    提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。

    

    Quantum Kernels被认为在量子机器学习的早期阶段提供了有用性。然而，在利用庞大数据集时，高度复杂的经典模型很难超越，特别是在理解力方面。尽管如此，一旦数据稀缺且倾斜，经典模型就会遇到困难。量子特征空间被预计在这样具有挑战性的情景中能够找到更好的数据特征和目标类别之间的联系，最重要的是增强了泛化能力。在这项工作中，我们提出了一种名为Systemic Quantum Score (SQS)的新方法，并提供了初步结果，表明在金融行业生产级应用案例中，SQS可能比纯经典模型具有优势。我们的具体研究表明，SQS能够从较少的数据点中提取出模式，并且在数据需求量大的算法（如XGBoost）上表现出更好的性能，带来优势。

    arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
    
[^18]: 最大切片Wasserstein距离的尖锐界限

    Sharp bounds for the max-sliced Wasserstein distance

    [https://arxiv.org/abs/2403.00666](https://arxiv.org/abs/2403.00666)

    对于可分希尔伯特空间上的概率测度与其经验分布之间的最大切片1-Wasserstein距离，得到了尖锐的上下界限。

    

    我们得到了关于在可分希尔伯特空间上的概率测度与从$n$个样本中获得的经验分布之间期望的最大切片1-Wasserstein距离的尖锐上下界。我们还得到了一个适用于Banach空间上的概率测度的版本。

    arXiv:2403.00666v1 Announce Type: cross  Abstract: We obtain sharp upper and lower bounds for the expected max-sliced 1-Wasserstein distance between a probability measure on a separable Hilbert space and its empirical distribution from $n$ samples. A version of this result for probability measures on Banach spaces is also obtained.
    
[^19]: 在广义朗之万方程中学习记忆核

    Learning Memory Kernels in Generalized Langevin Equations

    [https://arxiv.org/abs/2402.11705](https://arxiv.org/abs/2402.11705)

    提出一种学习广义朗之万方程中记忆核的新方法，通过正则化Prony方法估计相关函数并在Sobolev范数Loss函数和RKHS正则化下实现回归，在指数加权的$L^2$空间内获得改进性能，对比其他回归估计器展示了其优越性。

    

    我们引入了一种新颖的方法来学习广义朗之万方程中的记忆核。该方法最初利用正则化Prony方法从轨迹数据中估计相关函数，然后通过基于Sobolev范数的回归和RKHS正则化来进行回归。我们的方法保证在指数加权的$L^2$空间内获得了改进的性能，核估计误差受控于估计相关函数的误差。我们通过数值示例展示了我们的估计器相对于依赖于$L^2$损失函数的其他回归估计器以及从逆拉普拉斯变换推导出的估计器的优越性，这些示例突显了我们的估计器在各种权重参数选择上的持续优势。此外，我们提供了包括力和漂移项在方程中的应用示例。

    arXiv:2402.11705v1 Announce Type: cross  Abstract: We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.
    
[^20]: 带有动态过程不确定性的路径空间卡尔曼滤波器用于时间序列数据分析

    Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data

    [https://arxiv.org/abs/2402.04498](https://arxiv.org/abs/2402.04498)

    本论文提出了一种名为路径空间卡尔曼滤波器（PKF）的扩展算法，可以动态跟踪数据和先前知识的不确定性，并用贝叶斯方法量化不同的不确定性来源。通过在合成数据集上进行数值实验，我们证明了PKF优于传统KF方法，并将该方法应用于生物时间序列数据集。

    

    卡尔曼滤波器（KF）是一种最优线性状态预测算法，广泛应用于工程学、经济学、机器人学和太空探索等领域。在本文中，我们发展了KF的扩展，称为路径空间卡尔曼滤波器（PKF），它允许我们动态跟踪与底层数据和先前知识相关的不确定性，并使用贝叶斯方法量化不同的不确定性来源。该算法的一个应用是自动检测内部机制模型与数据在时间上发生变化的时间窗口。首先，我们提供了描述PKF算法收敛性的定理。然后，我们通过数值实验证明PKF在合成数据集上的性能优于传统KF方法，平均均方误差降低了数个数量级。最后，我们将该方法应用于生物时间序列数据集。

    Kalman Filter (KF) is an optimal linear state prediction algorithm, with applications in fields as diverse as engineering, economics, robotics, and space exploration. Here, we develop an extension of the KF, called a Pathspace Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties associated with the underlying data and prior knowledge, and b) take as input an entire trajectory and an underlying mechanistic model, and using a Bayesian methodology quantify the different sources of uncertainty. An application of this algorithm is to automatically detect temporal windows where the internal mechanistic model deviates from the data in a time-dependent manner. First, we provide theorems characterizing the convergence of the PKF algorithm. Then, we numerically demonstrate that the PKF outperforms conventional KF methods on a synthetic dataset lowering the mean-squared-error by several orders of magnitude. Finally, we apply this method to biological time-course dataset i
    
[^21]: 具有连续动作空间的低秩马尔可夫决策过程

    Low-Rank MDPs with Continuous Action Spaces

    [https://arxiv.org/abs/2311.03564](https://arxiv.org/abs/2311.03564)

    该研究通过探索多种方法，将针对低秩MDPs的现有方法扩展到连续动作空间，同时保持近似正确的学习保证。

    

    低秩马尔可夫决策过程（MDPs）最近在强化学习（RL）领域中崭露头角，因为它们可以提供约等于正确（PAC）的学习保证，同时还可以融合ML算法进行表示学习。然而，当前对低秩MDPs的方法存在局限性，它们只考虑有限的动作空间，并且在动作数量$|\mathcal{A}| \to \infty$时给出了空洞的界限，这极大地限制了它们的适用性。在这项工作中，我们研究了将这些方法扩展到具有连续动作的设置的问题，并探讨了多种具体的方法来进行这种扩展。作为案例研究，我们考虑了FLAMBE算法（Agarwal等，2020），这是一种用于低秩MDPs的PAC RL的奖励无关方法。我们展示了，在不对算法进行任何修改的情况下，当允许动作为连续时，我们获得了类似的PAC界限。

    arXiv:2311.03564v2 Announce Type: replace-cross  Abstract: Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain a similar PAC bound when actions are allowed to be continuous. Specifical
    
[^22]: 从算法到行动：改进患者护理需要因果关系

    From algorithms to action: improving patient care requires causality

    [https://arxiv.org/abs/2209.07397](https://arxiv.org/abs/2209.07397)

    改进患者护理需要考虑因果关系，建立和验证的预测模型必须对治疗决策的因果关系进行考虑，以避免在决策时造成伤害。

    

    在癌症研究中，建立和验证预测结果的兴趣很大，以支持治疗决策。然而，由于大多数结果预测模型是在不考虑治疗决策的因果关系的情况下开发和验证的，许多已发表的结果预测模型在用于决策时可能会造成伤害，尽管在验证研究中被发现准确。《美国联合癌症委员会风险模型认可核查单》对预测模型的验证指南和风险模型认可核查表无法防止在开发和验证过程中准确但在用于决策时有害的预测模型的出现。我们解释了为什么会出现这种情况以及如何构建和验证对决策有用的模型。

    arXiv:2209.07397v2 Announce Type: replace  Abstract: In cancer research there is much interest in building and validating outcome predicting outcomes to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.
    
[^23]: 图神经网络中带有Pfaffian激活函数的VC维度

    VC dimension of Graph Neural Networks with Pfaffian activation functions. (arXiv:2401.12362v1 [stat.ML])

    [http://arxiv.org/abs/2401.12362](http://arxiv.org/abs/2401.12362)

    本文分析了图神经网络（GNN）中使用不同常用激活函数（如sigmoid和双曲正切）时的VC维度，采用了Pfaffian函数理论框架，通过架构参数和合作数量提供了界限。

    

    图神经网络（GNN）近年来作为一种强大的工具出现，以数据驱动的方式学习各种图领域的任务；基于消息传递机制，GNN由于其与Weisfeiler-Lehman（WL）图同构测试密切相关的直观表达而越来越受欢迎，它们已被证明等价。从理论角度看，GNN被证明是通用逼近器，并且最近对具有分段多项式激活函数的GNN的泛化能力（即，对Vapnik Cherovenikis（VC）维度的界限）进行了研究。我们的工作目标是将对GNN的VC维度的分析扩展到其他常用激活函数，如sigmoid和双曲正切，使用Pfaffian函数理论框架。提供了与架构参数（深度，神经元数量，输入尺寸）以及与合作数量有关的界限。

    Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion; based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked with the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they have proven equivalent. From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability (namely, bounds on the Vapnik Chervonekis (VC) dimension) has recently been investigated for GNNs with piecewise polynomial activation functions. The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to architecture parameters (depth, number of neurons, input size) as well as with respect to the number of co
    
[^24]: 对于配对比较模型的统计推断

    Statistical inference for pairwise comparison models. (arXiv:2401.08463v1 [math.ST])

    [http://arxiv.org/abs/2401.08463](http://arxiv.org/abs/2401.08463)

    本论文通过建立极大似然估计量的渐近正态性结果，填补了配对比较模型中统计推断的空白，为各种配对比较模型提供了统一的方法，超越了Bradley-Terry模型，为实践者提供了坚实的理论保证。

    

    配对比较模型被用于各个领域的实用性和排名评估。现代问题规模的增加强调了对于当被比较对象数量无限增加时，对于这些模型中的统计推断的理解的需求。目前，文献中对于这些模型中的统计推断的理解还相当有限，除非只是在少数特殊实例中。本文通过在广泛的配对比较模型中建立极大似然估计量的渐近正态性结果来填补这一空白。关键思想在于将费舍尔信息矩阵识别为加权图拉普拉斯矩阵，通过一种细致入微的谱分析方法来进行研究。我们的发现为在各种配对比较模型中进行统计推断提供了第一个统一的方法，超越了Bradley-Terry模型，为实践者提供了坚实的理论保证。通过利用合成数据进行的模拟验证这一渐近正态性结果，然后进行了

    Pairwise comparison models are used for quantitatively evaluating utility and ranking in various fields. The increasing scale of modern problems underscores the need to understand statistical inference in these models when the number of subjects diverges, which is currently lacking in the literature except in a few special instances. This paper addresses this gap by establishing an asymptotic normality result for the maximum likelihood estimator in a broad class of pairwise comparison models. The key idea lies in identifying the Fisher information matrix as a weighted graph Laplacian matrix which can be studied via a meticulous spectral analysis. Our findings provide the first unified theory for performing statistical inference in a wide range of pairwise comparison models beyond the Bradley--Terry model, benefiting practitioners with a solid theoretical guarantee for their use. Simulations utilizing synthetic data are conducted to validate the asymptotic normality result, followed by 
    
[^25]: 关于生成模型在其自己的数据上迭代训练的稳定性研究

    On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])

    [http://arxiv.org/abs/2310.00429](http://arxiv.org/abs/2310.00429)

    本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。

    

    深度生成模型在建模复杂数据方面取得了巨大的进展，往往展现出超过典型人类能力的样本真实性辨别能力。这一成功的关键驱动力无疑是这些模型消耗海量网络规模数据的结果。由于这些模型惊人的性能和易得性，网络上将不可避免地出现越来越多的合成内容。这个事实直接意味着生成模型的未来迭代必须面对一个现实：它们的训练数据由清洁数据和先前模型生成的人工数据组成。在本文中，我们开发了一个框架来对混合数据集（包括真实数据和合成数据）上训练生成模型对稳定性的影响进行严格研究。我们首先证明了在初始生成模型足够好地近似数据分布并且真实数据与合成数据的比例适当的情况下，迭代训练的稳定性。

    Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
    
[^26]: 论文标题：The Rashomon Importance Distribution: 摆脱不稳定的基于单一模型的变量重要性

    The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])

    [http://arxiv.org/abs/2309.13775](http://arxiv.org/abs/2309.13775)

    提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。

    

    量化变量重要性对于回答遗传学、公共政策和医学等领域的重大问题至关重要。当前的方法通常计算给定数据集上训练的给定模型的变量重要性。然而，对于给定数据集，可能有许多模型同样能解释目标结果;如果不考虑所有可能的解释，不同的研究者可能会得出许多冲突但同样有效的结论。此外，即使考虑了给定数据集的所有可能解释，这些洞察力可能不具有普适性，因为并非所有好的解释在合理的数据扰动下都是稳定的。我们提出了一种新的变量重要性框架，该框架量化了在所有好的模型集合中的变量重要性，并且在数据分布上是稳定的。我们的框架非常灵活，可以与大多数现有的模型类和全局变量重要性指标结合使用。

    Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
    
[^27]: 基于Samplet基 Pursuit 的核学习方法

    Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])

    [http://arxiv.org/abs/2306.10180](http://arxiv.org/abs/2306.10180)

    本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。

    

    本文考虑了基于l1正则化的Samplet坐标下的核学习问题。在Samplet基的系数上，应用l1正则化项可以强制增加稀疏性。因此，我们称这种方法为Samplet基 Pursuit。Samplet基是波形类型的有符号测度，专门用于散乱数据。它们具有与小波相似的本地化、多分辨率分析和数据压缩性质。可以在Samplet基上稀疏地表示的信号类比单尺度基上能够表示稀疏的信号类别要大得多。特别地，仅用基函数映射的几个特征叠加即可表示的所有信号也可以在Samplet坐标下实现稀疏表示。我们提出了一种高效解决该问题的方法，将软阈值和半光滑牛顿法相结合，并将该方法与快速迭代收缩阈值算法进行了比较。实验结果表明了该方法在稀疏性和预测精度方面的优势。

    We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
    
[^28]: 基于MCMC的贝叶斯神经网络：基于Python的教程

    Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])

    [http://arxiv.org/abs/2304.02595](http://arxiv.org/abs/2304.02595)

    本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。

    

    贝叶斯推断为机器学习和深度学习提供了参数估计和不确定性量化的方法。变分推断和马尔科夫链蒙特卡罗（MCMC）采样技术用于实现贝叶斯推断。在过去三十年中，MCMC方法在适应更大的模型（如深度学习）和大数据问题方面面临了许多挑战。包括梯度的高级提议（例如Langevin提议分布）提供了一种解决MCMC采样中的一些限制的方法，此外，MCMC方法通常被限制在统计学家的使用范围内，并且仍不是深度学习研究人员的主流方法。我们提供了一个MCMC方法的教程，涵盖了简单的贝叶斯线性和逻辑模型，以及贝叶斯神经网络。这个教程的目的是通过编码来弥合理论和实现之间的差距，鉴于当前MCMC方法的普及程度仍然较低。

    Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
    
[^29]: 用鲁棒交替最小化方法在几乎线性时间内完成低秩矩阵补全

    Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time. (arXiv:2302.11068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11068](http://arxiv.org/abs/2302.11068)

    本论文提出了一种在几乎线性时间内用鲁棒交替最小化方法完成低秩矩阵补全的方法，并证明了观察几乎线性数量的条目即可恢复矩阵$M$，此方法克服了交替最小化方法需要精确计算的限制，更符合实际实现中对效率的要求。

    

    给定一个矩阵$M\in \mathbb{R}^{m\times n}$，低秩矩阵补全问题要求我们通过只观察一组指定的条目$\Omega\subseteq [m]\times [n]$来找到$M$的秩为$k$的近似$UV^\top$，其中$U\in \mathbb{R}^{m\times k}$，$V\in \mathbb{R}^{n\times k}$。本文主要研究了一种被广泛使用的方法--交替最小化框架。Jain、Netrapalli和Sanghavi~\cite{jns13}证明了如果$M$的行和列是不相干的，那么交替最小化方法可以通过观察几乎线性数量的条目可靠地恢复矩阵$M$。虽然样本复杂度之后被改进~\cite{glz17}，但交替最小化步骤要求精确计算。这阻碍了更高效算法的开发，并未描述交替最小化的实际实现，其中更新通常是近似执行，以提高效率。

    Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in \mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a few entries specified by a set of entries $\Omega\subseteq [m]\times [n]$. In particular, we examine an approach that is widely used in practice -- the alternating minimization framework. Jain, Netrapalli and Sanghavi~\cite{jns13} showed that if $M$ has incoherent rows and columns, then alternating minimization provably recovers the matrix $M$ by observing a nearly linear in $n$ number of entries. While the sample complexity has been subsequently improved~\cite{glz17}, alternating minimization steps are required to be computed exactly. This hinders the development of more efficient algorithms and fails to depict the practical implementation of alternating minimization, where the updates are usually performed approximately in favor of efficiency.  In this p
    
[^30]: 关于广义似然比检验和一类分类器

    On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12494](http://arxiv.org/abs/2210.12494)

    本文考虑了一类分类器和广义似然比检验的问题，证明了多层感知器神经网络和支持向量机模型在收敛时会表现为广义似然比检验。同时，作者还展示了一类最小二乘SVM在收敛时也能达到广义似然比检验的效果。

    

    一类分类（OCC）是决定观察样本是否属于目标类的问题。我们考虑在包含目标类样本的数据集上学习一个表现为广义似然比检验（GLRT）的OCC模型的问题。当目标类的统计信息可用时，GLRT解决了相同的问题。GLRT是一个众所周知且在特定条件下可证明最佳的分类器。为此，我们考虑了多层感知器神经网络（NN）和支持向量机（SVM）模型。它们使用人工数据集训练为两类分类器，其中替代类使用在目标类数据集的定义域上均匀生成的随机样本。我们证明，在适当的假设下，模型在大数据集上收敛到了GLRT。此外，我们还展示了具有适当核函数的一类最小二乘SVM（OCLSSVM）在收敛时表现为GLRT。

    One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
    
[^31]: CP-PINNs: 使用物理知识神经网络和总变差惩罚进行PDE中的变点检测

    CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.08626](http://arxiv.org/abs/2208.08626)

    本文提出了一种新的CP-PINNs模型，通过将PINNs与总变差惩罚相结合，实现了准确的变点检测和PDE的发现。我们还开发了一种元学习算法，能够在数据的连续批次上动态改进优化目标。实证结果表明，在存在变点的情况下，该方法能够准确估计参数和模型对齐，在没有变点的情况下能够数值上收敛到原始PINNs模型的解。

    

    本文展示了在参数中存在未知变点的情况下，物理知识神经网络（PINNs）可能无法正确估计偏微分方程（PDE）的动态过程。为了解决这个问题，我们提出了一个新的CP-PINNs模型，将PINNs与总变差惩罚相结合，用于准确的变点检测和PDE的发现。为了在模型拟合、PDE发现和变点检测任务之间进行最优组合，我们开发了一种新的元学习算法，利用批量学习在数据的连续批次上动态改进优化目标。在实证方面，在动态过程中存在变点的情况下，我们的方法能够准确估计参数和模型对齐，在数据中没有变点的情况下，数值上收敛到原始PINNs模型的解。

    The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
    

