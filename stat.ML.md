# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Convergence Analysis of Flow Matching in Latent Space with Transformers](https://arxiv.org/abs/2404.02538) | 该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。 |
| [^2] | [Riemannian Laplace Approximation with the Fisher Metric](https://arxiv.org/abs/2311.02766) | 黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。 |
| [^3] | [Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints](https://arxiv.org/abs/2212.04672) | 提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。 |
| [^4] | [Prepare Non-classical Collective Spin State by Reinforcement Learning.](http://arxiv.org/abs/2401.16320) | 通过强化学习设计控制场的方案成功生成了非经典态，以应用于自旋压缩态的产生。该方法在保持压缩和纠缠的同时提供了不同的控制序列，并观察到控制脉冲密集应用可以提高结果的性能。 |
| [^5] | [Stochastic Gradient Descent for Gaussian Processes Done Right.](http://arxiv.org/abs/2310.20581) | 本文研究了使用随机梯度下降方法优化高斯过程回归问题，并引入了一种特定的随机对偶梯度下降算法，该方法在标准回归基准和贝叶斯优化任务上表现出很高的竞争力。 |
| [^6] | [Convergence analysis of online algorithms for vector-valued kernel regression.](http://arxiv.org/abs/2309.07779) | 本文考虑了在线学习算法在向量值内核回归问题中的收敛性能，证明了在RKHS范数中的期望平方误差可以被一个特定公式所限制。 |
| [^7] | [Bayesian Reasoning for Physics Informed Neural Networks.](http://arxiv.org/abs/2308.13222) | 本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。 |
| [^8] | [Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System.](http://arxiv.org/abs/2308.07523) | 深层操作符网络（DeepONet）作为一种强大的替代建模方法，在核系统数字孪生技术中展示出了显著的预测精度和计算效率。然而，挑战仍然存在，包括最佳传感器放置和模型评估。 |
| [^9] | [Latent Space Representations of Neural Algorithmic Reasoners.](http://arxiv.org/abs/2307.08874) | 这项工作对神经算法推理器中执行算法时产生的潜在空间结构进行了详细分析，并提出了解决两种故障模式的方法。通过使用softmax聚合器解决分辨率丧失问题，以及衰减潜在空间来处理超出范围的值，这些改变在标准CLRS-30基准测试中大多数算法上实现了改进。 |
| [^10] | [Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization.](http://arxiv.org/abs/2307.03571) | 本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。 |
| [^11] | [Understanding Generalization in the Interpolation Regime using the Rate Function.](http://arxiv.org/abs/2306.10947) | 本文利用大偏差理论，提出一种基于函数的平滑模型特征描述方法，解释了为什么一些插值器有很好的泛化能力以及现代学习技术为什么能够找到它们。 |
| [^12] | [On Achieving Optimal Adversarial Test Error.](http://arxiv.org/abs/2306.07544) | 本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。 |
| [^13] | [Robust Bayesian Inference for Measurement Error Models.](http://arxiv.org/abs/2306.01468) | 该文提出了一个Bayesian非参数学习框架，对于测量误差具有强鲁棒性，不需要知道误差分布和协变量可重复测量的假设，并能够吸收先验信念，这能产生两种通过不同损失函数的测量误差强鲁棒方法。 |
| [^14] | [Confidence Intervals for Error Rates in Matching Tasks: Critical Review and Recommendations.](http://arxiv.org/abs/2306.01198) | 本文回顾了构建匹配任务误差率置信区间的方法，研究其统计特性并提供了最佳实践建议。 |
| [^15] | [Subsampling Error in Stochastic Gradient Langevin Diffusions.](http://arxiv.org/abs/2305.13882) | 该研究分析了随机梯度 Langevin 动力学在大型数据环境下使用子采样产生的误差。研究者提出了一种新的连续时间马尔可夫过程，该过程切换数据子集并可用于扩散子采样 MCMC 方法，并证明了该方法的收敛性。 |
| [^16] | [On the Convergence of the ELBO to Entropy Sums.](http://arxiv.org/abs/2209.03077) | 本论文研究了 ELBO 收敛到熵和的问题，证明了对于一类广泛的生成模型，ELBO 在所有学习的稳定点处都等于一系列熵的和，为无监督学习的学习算法的基本属性提供了深入的洞察。 |
| [^17] | [Emergent segmentation from participation dynamics and multi-learner retraining.](http://arxiv.org/abs/2206.02667) | 该论文通过研究基于数据驱动服务的参与和重新训练动态，发现当学习者和用户子群具有风险减少性质时，唯一的稳定均衡是细分的，将子群分配给单个学习者。功利主义社会最优是一个稳定均衡。 |

# 详细

[^1]: 流匹配在潜空间中的收敛性分析与Transformer

    Convergence Analysis of Flow Matching in Latent Space with Transformers

    [https://arxiv.org/abs/2404.02538](https://arxiv.org/abs/2404.02538)

    该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。

    

    我们提出了ODE-based生成模型，特别是流匹配的理论收敛性保证。我们使用预训练的自编码器网络将高维原始输入映射到低维潜空间，其中一个Transformer网络被训练来预测从标准正态分布到目标潜空间分布的变换速度场。我们的误差分析展示了这种方法的有效性，表明通过估计的ODE流生成样本的分布在温斯坦-2距离下收敛到目标分布，这在温和且实际的假设下成立。此外，我们展示了具有利普希茨连续性的Transformer网络可以有效地逼近任意光滑函数，这可能是独立感兴趣的。

    arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.
    
[^2]: 具有Fisher度量的黎曼拉普拉斯逼近

    Riemannian Laplace Approximation with the Fisher Metric

    [https://arxiv.org/abs/2311.02766](https://arxiv.org/abs/2311.02766)

    黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。

    

    Laplace方法用高斯分布在其模式处对目标密度进行近似。基于Bernstein-von Mises定理，它在贝叶斯推断中是计算效率高且渐近准确的，但对于复杂的目标和有限数据后验，它往往是一种过于粗糙的近似。最近对Laplace逼近的一般化是根据选择的黎曼几何对高斯近似进行转换，提供了更丰富的近似族，同时保持计算效率。然而，正如本文所示，其性质严重依赖于所选择的度量，实际上，在先前研究中采用的度量导致的逼近即使在无限数据量的极限下也过于狭窄且存在偏差。我们通过进一步发展逼近族，推导出两种在无限数据极限下精确的替代变种，扩展了理论分析。

    arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
    
[^3]: Primal Dual Alternating Proximal Gradient算法用于具有耦合线性约束的非光滑非凸极小极大问题

    Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints

    [https://arxiv.org/abs/2212.04672](https://arxiv.org/abs/2212.04672)

    提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。

    

    非凸极小极大问题近年来在机器学习、信号处理和许多其他领域引起了广泛关注。本文提出了一种用于解决非光滑非凸（强）凹和非凸线性极小极大问题的原始对偶交替近端梯度（PDAPG）算法和原始对偶近端梯度（PDPG-L）算法，分别用于具有耦合线性约束的情况。这两种算法的迭代复杂度证明为 $\mathcal{O}\left( \varepsilon ^{-2} \right)$ （对应 $\mathcal{O}\left( \varepsilon ^{-4} \right)$）在非凸强凹 （对应非凸凹）情况下，以及 $\mathcal{O}\left( \varepsilon ^{-3} \right)$ 在非凸线性情况下，分别达到 $\varepsilon$-稳态点。据我们所知，它们是用于解决具有耦合线性约束的非凸极小极大问题的第一批具有迭代复杂度保证的算法。

    arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
    
[^4]: 利用强化学习生成非经典集合自旋态的方案

    Prepare Non-classical Collective Spin State by Reinforcement Learning. (arXiv:2401.16320v1 [quant-ph])

    [http://arxiv.org/abs/2401.16320](http://arxiv.org/abs/2401.16320)

    通过强化学习设计控制场的方案成功生成了非经典态，以应用于自旋压缩态的产生。该方法在保持压缩和纠缠的同时提供了不同的控制序列，并观察到控制脉冲密集应用可以提高结果的性能。

    

    我们提出了一种利用强化学习来设计控制场的方案，用于生成非经典态。该方案以应用于开放集体自旋模型中的自旋压缩态为例，其中设计了一个线性控制项来控制动力学。强化学习代理根据以耗散和去相干为特征的环境中的相干自旋态开始，确定了控制脉冲的时间序列。与恒定控制方案相比，这种方法提供了多种控制序列，保持了集体自旋压缩和纠缠。观察到控制脉冲的密集应用可以增强结果的性能。此外，通过添加控制操作，性能得到了轻微增强。所提出的策略在较大系统中展现了更高的效果。对储备热激发对控制结果有不利影响。应该确认这一点。

    We propose a scheme leveraging reinforcement learning to engineer control fields for generating non-classical states. It is exemplified by the application to prepare spin squeezed state for an open collective spin model where a linear control term is designed to govern the dynamics. The reinforcement learning agent determines the temporal sequence of control pulses, commencing from coherent spin state in an environment characterized by dissipation and dephasing. When compared to constant control scenarios, this approach provides various control sequences maintaining collective spin squeezing and entanglement. It is observed that denser application of the control pulses enhances the performance of the outcomes. Furthermore, there is a minor enhancement in the performance by adding control actions. The proposed strategy demonstrates increased effectiveness for larger systems. And thermal excitations of the reservoir are detrimental to the control outcomes. It should be confirmed that thi
    
[^5]: 高斯过程的随机梯度下降的正确实现

    Stochastic Gradient Descent for Gaussian Processes Done Right. (arXiv:2310.20581v1 [cs.LG])

    [http://arxiv.org/abs/2310.20581](http://arxiv.org/abs/2310.20581)

    本文研究了使用随机梯度下降方法优化高斯过程回归问题，并引入了一种特定的随机对偶梯度下降算法，该方法在标准回归基准和贝叶斯优化任务上表现出很高的竞争力。

    

    本文研究了使用平方损失函数的高斯过程回归的优化问题。目前，解决这个问题的最常见方法是应用精确求解器，比如共轭梯度下降，要么直接应用，要么应用于问题的降阶版本。最近，在深度学习的成功推动下，随机梯度下降作为一种替代方法获得了广泛关注。本文展示了当正确使用时（我们指的是利用优化和核函数领域的特定见解），这种方法是非常有效的。因此，我们引入了一种特定的随机对偶梯度下降算法，可以使用任何深度学习框架的几行代码实现。我们通过消融实验解释了我们的设计决策的优势，并表明新方法具有很高的竞争力。我们对标准回归基准和贝叶斯优化任务进行评估，证明了我们的方法的优越性。

    We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apa
    
[^6]: 在向量值内核回归的在线算法的收敛分析

    Convergence analysis of online algorithms for vector-valued kernel regression. (arXiv:2309.07779v1 [stat.ML])

    [http://arxiv.org/abs/2309.07779](http://arxiv.org/abs/2309.07779)

    本文考虑了在线学习算法在向量值内核回归问题中的收敛性能，证明了在RKHS范数中的期望平方误差可以被一个特定公式所限制。

    

    我们考虑使用适当的再生核希尔伯特空间（RKHS）作为先验，通过在线学习算法从噪声向量值数据中逼近回归函数的问题。在在线算法中，独立同分布的样本通过随机过程逐个可用，并依次处理以构建对回归函数的近似。我们关注这种在线逼近算法的渐近性能，并证明了在RKHS范数中的期望平方误差可以被$C^2(m+1)^{-s/(2+s)}$绑定，其中$m$为当下处理的数据数量，参数$0<s\leq 1$表示对回归函数的额外光滑性假设，常数$C$取决于输入噪声的方差、回归函数的光滑性以及算法的其他参数。

    We consider the problem of approximating the regression function from noisy vector-valued data by an online learning algorithm using an appropriate reproducing kernel Hilbert space (RKHS) as prior. In an online algorithm, i.i.d. samples become available one by one by a random process and are successively processed to build approximations to the regression function. We are interested in the asymptotic performance of such online approximation algorithms and show that the expected squared error in the RKHS norm can be bounded by $C^2 (m+1)^{-s/(2+s)}$, where $m$ is the current number of processed data, the parameter $0<s\leq 1$ expresses an additional smoothness assumption on the regression function and the constant $C$ depends on the variance of the input noise, the smoothness of the regression function and further parameters of the algorithm.
    
[^7]: 用于物理信息神经网络的贝叶斯推理

    Bayesian Reasoning for Physics Informed Neural Networks. (arXiv:2308.13222v1 [physics.comp-ph])

    [http://arxiv.org/abs/2308.13222](http://arxiv.org/abs/2308.13222)

    本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。

    

    本文提出了一种基于贝叶斯公式的物理信息神经网络（PINN）方法。我们采用了MacKay在Neural Computation（1992年）中提出的贝叶斯神经网络框架。通过拉普拉斯近似法，得到后验密度。对于每个模型（拟合），计算所谓的证据。它是一种分类假设的度量。最优解具有最大的证据值。贝叶斯框架使我们能够控制边界对总损失的影响。事实上，贝叶斯算法通过微调损失组件的相对权重。我们解决了热力学、波动和Burger方程。所得结果与精确解基本一致。所有解都提供了在贝叶斯框架内计算的不确定性。

    Physics informed neural network (PINN) approach in Bayesian formulation is presented. We adopt the Bayesian neural network framework formulated by MacKay (Neural Computation 4 (3) (1992) 448). The posterior densities are obtained from Laplace approximation. For each model (fit), the so-called evidence is computed. It is a measure that classifies the hypothesis. The most optimal solution has the maximal value of the evidence. The Bayesian framework allows us to control the impact of the boundary contribution to the total loss. Indeed, the relative weights of loss components are fine-tuned by the Bayesian algorithm. We solve heat, wave, and Burger's equations. The obtained results are in good agreement with the exact solutions. All solutions are provided with the uncertainties computed within the Bayesian framework.
    
[^8]: 深层操作符网络在核系统数字孪生技术中的潜力

    Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System. (arXiv:2308.07523v1 [stat.ML])

    [http://arxiv.org/abs/2308.07523](http://arxiv.org/abs/2308.07523)

    深层操作符网络（DeepONet）作为一种强大的替代建模方法，在核系统数字孪生技术中展示出了显著的预测精度和计算效率。然而，挑战仍然存在，包括最佳传感器放置和模型评估。

    

    本研究在核工程的数字孪生系统中引入了深层操作符网络（DeepONet）作为一种强大的替代建模方法。随着核能作为一种碳中和解决方案的重要性不断增加，采用数字孪生技术对于提高核工程应用中的运营效率、安全性和预测能力变得至关重要。DeepONet具有显著的预测精度，优于传统的机器学习方法。通过广泛的基准测试和评估，本研究展示了DeepONet在解决复杂粒子传输问题中的可扩展性和计算效率。通过将函数作为输入数据并使用训练数据构建操作符G，DeepONet能够有效处理多样化和复杂的场景。然而，DeepONet的应用也揭示了与最佳传感器放置和模型评估相关的挑战，这是实际实施中的关键问题。

    This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing 
    
[^9]: 神经算法推理器的潜在空间表示

    Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])

    [http://arxiv.org/abs/2307.08874](http://arxiv.org/abs/2307.08874)

    这项工作对神经算法推理器中执行算法时产生的潜在空间结构进行了详细分析，并提出了解决两种故障模式的方法。通过使用softmax聚合器解决分辨率丧失问题，以及衰减潜在空间来处理超出范围的值，这些改变在标准CLRS-30基准测试中大多数算法上实现了改进。

    

    神经算法推理（NAR）是一个研究领域，专注于设计能够可靠地捕捉经典计算的神经架构，通常通过学习执行算法来实现。典型的方法是依赖于图神经网络（GNN）架构，它们将输入编码为高维潜在空间，在算法执行期间反复转换。在这项工作中，我们对GNN在执行算法时导致的潜在空间结构进行了详细分析。我们发现了两种可能的故障模式：（i）分辨率丧失，使得难以区分相似的值；（ii）无法处理训练期间未观察到的值范围之外的值。我们提出通过依赖softmax聚合器来解决第一个问题，并建议衰减潜在空间以处理超出范围的值。我们展示了这些变化在使用最先进的方法时，在标准CLRS-30基准测试中大多数算法上的改进。

    Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art
    
[^10]: 平滑边缘：利用Hadamard超参数化在稀疏正则化的平滑优化中的一般框架

    Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])

    [http://arxiv.org/abs/2307.03571](http://arxiv.org/abs/2307.03571)

    本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。

    

    本文介绍了一种用于（结构化）稀疏正则化问题中的$\ell_q$和$\ell_{p,q}$正则化的平滑方法。这些非平滑且可能非凸的问题的优化通常依赖于专门的过程。相比之下，我们的一般框架与主流的一阶优化方法（如随机梯度下降和加速变体）兼容，无需任何修改。这是通过平滑优化转移实现的，其中选定模型参数的超参数化使用Hadamard乘积和惩罚的改变。在超参数问题中，通过用替代参数进行平滑和凸性的$\ell_2$正则化，能够在原始参数化中引入非平滑和非凸性的$\ell_q$或$\ell_{p,q}$正则化。我们证明了我们的方法不仅能够得到匹配的全局最小值，还能得到等价的局部最小值。这在非凸稀疏正则化中尤其有用，因为在这种情况下找到全局最小值非常困难。

    This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
    
[^11]: 使用速率函数理解插值区间的泛化

    Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])

    [http://arxiv.org/abs/2306.10947](http://arxiv.org/abs/2306.10947)

    本文利用大偏差理论，提出一种基于函数的平滑模型特征描述方法，解释了为什么一些插值器有很好的泛化能力以及现代学习技术为什么能够找到它们。

    

    本文基于大偏差理论的基本原理，提出了一种模型平滑度的新特征描述方法。与以往的工作不同，以往的工作通常用实数值（如权重范数）来表征模型的平滑度，我们表明可以用简单的实值函数来描述平滑度。基于模型平滑度的这一概念，我们提出了一个统一的理论解释，为什么一些插值器表现出非常好的泛化能力，以及为什么广泛使用的现代学习技术（如随机梯度下降，$\ell_2$-规范化，数据增强，不变的架构和超参数化）能够找到它们。我们得出的结论是，所有这些方法都提供了互补的过程，这些过程使优化器偏向于更平滑的插值器，而根据这种理论分析，更平滑的插值器是具有更好的泛化误差的插值器。

    In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
    
[^12]: 关于实现最优对抗测试误差的研究

    On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])

    [http://arxiv.org/abs/2306.07544](http://arxiv.org/abs/2306.07544)

    本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。

    

    本文首先阐述了最优对抗预测器的各种基本特性：最优对抗凸预测器的结构、将对抗凸损失与对抗0-1损失相关联的界限以及连续预测器可以在凸和0-1损失下无限接近最优对抗误差。本文还将这些结果与对抗训练在初始化附近的新Rademacher复杂度界限相结合，证明了对于一般的数据分布和扰动集，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。相比之下，先前的理论工作只考虑了特定的数据分布或仅提供了训练误差的保证。

    We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.
    
[^13]: 测量误差模型的强鲁棒性Bayesian推断

    Robust Bayesian Inference for Measurement Error Models. (arXiv:2306.01468v1 [stat.ME])

    [http://arxiv.org/abs/2306.01468](http://arxiv.org/abs/2306.01468)

    该文提出了一个Bayesian非参数学习框架，对于测量误差具有强鲁棒性，不需要知道误差分布和协变量可重复测量的假设，并能够吸收先验信念，这能产生两种通过不同损失函数的测量误差强鲁棒方法。

    

    测量误差是指影响响应变量的协变量受到噪声干扰。这可能会导致误导性的推断结果，尤其是在估计协变量和响应变量之间关系的准确性至关重要的问题中，如因果效应估计问题中。现有的处理测量误差的方法通常依赖于强假设，例如对误差分布或其方差的知识和协变量可重复测量的可用性。我们提出了一个Bayesian非参数学习框架，它对于测量误差具有强鲁棒性，不需要上述假设，并能够吸收关于真实误差分布的先验信念。我们的方法产生了两种通过不同损失函数的测量误差强鲁棒方法：一种基于总最小二乘目标，另一种基于最大平均偏差（MMD）。后者允许推广到非高斯分布的情况。

    Measurement error occurs when a set of covariates influencing a response variable are corrupted by noise. This can lead to misleading inference outcomes, particularly in problems where accurately estimating the relationship between covariates and response variables is crucial, such as causal effect estimation. Existing methods for dealing with measurement error often rely on strong assumptions such as knowledge of the error distribution or its variance and availability of replicated measurements of the covariates. We propose a Bayesian Nonparametric Learning framework which is robust to mismeasured covariates, does not require the preceding assumptions, and is able to incorporate prior beliefs about the true error distribution. Our approach gives rise to two methods that are robust to measurement error via different loss functions: one based on the Total Least Squares objective and the other based on Maximum Mean Discrepancy (MMD). The latter allows for generalisation to non-Gaussian d
    
[^14]: 匹配任务的误差率置信区间：关键综述与建议

    Confidence Intervals for Error Rates in Matching Tasks: Critical Review and Recommendations. (arXiv:2306.01198v1 [stat.ME])

    [http://arxiv.org/abs/2306.01198](http://arxiv.org/abs/2306.01198)

    本文回顾了构建匹配任务误差率置信区间的方法，研究其统计特性并提供了最佳实践建议。

    

    匹配算法通常用于预测收集中项目之间的匹配。例如，在1：1的人脸验证中，匹配算法预测两张人脸图像是否描绘同一个人。当数据相关且误差率低时，准确评估此类算法误差率的不确定性可能具有挑战性，这是文献中经常被忽略的两个方面。在本文中，我们回顾了构建1:1人脸验证等匹配任务误差率置信区间的方法。我们推导和检验了这些方法的统计属性，并使用合成和真实世界数据集演示了覆盖率和区间宽度如何随着样本量、误差率和数据相关程度变化。基于我们的发现，我们提供了构建匹配任务误差率置信区间最佳实践的建议。

    Matching algorithms are commonly used to predict matches between items in a collection. For example, in 1:1 face verification, a matching algorithm predicts whether two face images depict the same person. Accurately assessing the uncertainty of the error rates of such algorithms can be challenging when data are dependent and error rates are low, two aspects that have been often overlooked in the literature. In this work, we review methods for constructing confidence intervals for error rates in matching tasks such as 1:1 face verification. We derive and examine the statistical properties of these methods and demonstrate how coverage and interval width vary with sample size, error rates, and degree of data dependence using both synthetic and real-world datasets. Based on our findings, we provide recommendations for best practices for constructing confidence intervals for error rates in matching tasks.
    
[^15]: 随机梯度 Langevin 扩散中的子采样误差

    Subsampling Error in Stochastic Gradient Langevin Diffusions. (arXiv:2305.13882v1 [stat.ML])

    [http://arxiv.org/abs/2305.13882](http://arxiv.org/abs/2305.13882)

    该研究分析了随机梯度 Langevin 动力学在大型数据环境下使用子采样产生的误差。研究者提出了一种新的连续时间马尔可夫过程，该过程切换数据子集并可用于扩散子采样 MCMC 方法，并证明了该方法的收敛性。

    

    随机梯度 Langevin 动力学 (SGLD) 通常用于大规模数据的统计学习中近似贝叶斯后验分布。与许多常规马尔可夫链蒙特卡罗 (MCMC) 算法不同，SGLD 对于后验分布不是稳定的。它有两个错误来源：第一个错误是由 Euler-Maruyama 离散化 Langevin 扩散过程引入的，第二个错误来自于数据子采样，这使得它适用于大规模数据环境。在本文中，我们考虑了 SGLD 的理想化版本，以分析该方法的纯子采样误差，我们可以将其视为基于扩散的子采样 MCMC 方法的最佳情况误差。事实上，我们引入并研究了随机梯度 Langevin 扩散 (SGLDiff)，这是一个连续时间马尔可夫过程，它遵循与数据子集相应的 Langevin 扩散，并在指数等待时间后切换该数据子集。在此，我们证明了瓦瑟斯坦距离 (Was)

    The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler--Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method's pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show that the Was
    
[^16]: 关于ELBO收敛到熵和的研究

    On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.03077](http://arxiv.org/abs/2209.03077)

    本论文研究了 ELBO 收敛到熵和的问题，证明了对于一类广泛的生成模型，ELBO 在所有学习的稳定点处都等于一系列熵的和，为无监督学习的学习算法的基本属性提供了深入的洞察。

    

    变分下界（又称ELBO或自由能）是许多经典和新颖的无监督学习算法的核心目标。学习算法可以改变模型参数，使变分下界增加。通常，学习进行到参数收敛到接近学习动态的稳定点值。在本文的理论贡献中，我们证明了（对于一类非常广泛的生成模型），变分下界在所有学习的稳定点处均等于一系列熵的和。对于具有一组潜在变量和一组观测变量的标准机器学习模型，这个和包括三个熵: (A) 变分分布的熵（平均熵），(B) 模型先验分布的负熵和 (C) 可观测分布的（期望）负熵。所得到的结果适用于包括：有限数量的数据点，在学习的任意阶段和各种不同的生成模型等真实条件。本研究为无监督学习的学习算法的基本属性提供了深入洞察，是对优化推理和学习的理论分析的第一步。

    The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many established as well as many novel algorithms for unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases. Learning usually proceeds until parameters have converged to values close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For standard machine learning models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at an
    
[^17]: 从参与度动态和多学习者重新训练中产生的紧急细分

    Emergent segmentation from participation dynamics and multi-learner retraining. (arXiv:2206.02667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02667](http://arxiv.org/abs/2206.02667)

    该论文通过研究基于数据驱动服务的参与和重新训练动态，发现当学习者和用户子群具有风险减少性质时，唯一的稳定均衡是细分的，将子群分配给单个学习者。功利主义社会最优是一个稳定均衡。

    

    在基于数据驱动的服务中选择参与，往往基于该服务的质量，影响了服务学习和改进的能力。我们研究了当学习者和用户子群都具有风险减少性质时，参与和重新训练的动态生成的情况，其中包括了梯度下降、乘法权重等广泛的更新方法。举个例子，假设个体选择在社交媒体平台上花费时间的比例与每个平台对他们的工作效果成比例。每个平台还会收集其活跃用户的数据，并用梯度步骤更新参数。对于这个例子和我们的一般动态类别，我们展示了唯一的渐近稳定均衡是细分的，将子群分配给单个学习者。在温和的假设下，功利主义社会最优是一个稳定均衡。与先前的工作相反，先前的工作显示重复的风险最小化可能不会对韧性和利益进行任何保证。

    The choice to participate in a data-driven service, often made on the basis of quality of that service, influences the ability of the service to learn and improve. We study the participation and retraining dynamics that arise when both the learners and sub-populations of users are \emph{risk-reducing}, which cover a broad class of updates including gradient descent, multiplicative weights, etc. Suppose, for example, that individuals choose to spend their time amongst social media platforms proportionally to how well each platform works for them. Each platform also gathers data about its active users, which it uses to update parameters with a gradient step. For this example and for our general class of dynamics, we show that the only asymptotically stable equilibria are segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can
    

