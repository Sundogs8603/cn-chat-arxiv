# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning the mechanisms of network growth](https://arxiv.org/abs/2404.00793) | 我们提出了一种新的模型选择方法，通过在大量合成网络数据上训练分类器，设计出易于计算、解析可处理且具有解释性的动态特征类型，实现了几乎完美的网络分类，超过了目前技术水平。 |
| [^2] | [Optimal Flow Matching: Learning Straight Trajectories in Just One Step](https://arxiv.org/abs/2403.13117) | 该论文提出了一种新颖的最优流匹配方法，能够在一步中学习实现二次成本下的直线 OT 位移。 |
| [^3] | [Mean-Field Microcanonical Gradient Descent](https://arxiv.org/abs/2403.08362) | 提出了均场微正则梯度下降方法，通过同时采样多个弱耦合数据点，在控制熵损失的同时在似然拟合方面表现良好。 |
| [^4] | [Mutual Information Estimation via Normalizing Flows](https://arxiv.org/abs/2403.02187) | 通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。 |
| [^5] | [Critical windows: non-asymptotic theory for feature emergence in diffusion models](https://arxiv.org/abs/2403.01633) | 我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。 |
| [^6] | [Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection](https://arxiv.org/abs/2403.01485) | 本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法 |
| [^7] | [Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion](https://arxiv.org/abs/2402.17886) | 本文提出了一种基于去噪扩散过程的零阶扩散蒙特卡洛算法，克服了非对数凹分布采样中的亚稳定性问题，并证明其采样精度具有倒多项式依赖。 |
| [^8] | [Conformalized Selective Regression](https://arxiv.org/abs/2402.16300) | 通过利用一致性预测，提供基于模型特定偏差的置信度量，以解决选择性回归中不确定性测量的方法。 |
| [^9] | [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992) | 本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。 |
| [^10] | [Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering](https://arxiv.org/abs/2402.12302) | 提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。 |
| [^11] | [AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods](https://arxiv.org/abs/2402.11215) | AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。 |
| [^12] | [Accelerating Parallel Sampling of Diffusion Models](https://arxiv.org/abs/2402.09970) | 本文提出了一种并行化自回归过程来加速扩散模型的采样的方法，并引入了ParaTAA，一种通用的并行采样算法，可以显著减少推理步骤。 |
| [^13] | [On the Distance from Calibration in Sequential Prediction](https://arxiv.org/abs/2402.07458) | 本论文研究了顺序预测中的标定距离，证明了存在一种预测算法可以在敌人选择的二进制序列上实现$O(\sqrt{T})$的标定距离，通过较低的标定距离进行准确近似。 |
| [^14] | [HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs](https://arxiv.org/abs/2402.07309) | 本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。 |
| [^15] | [On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling](https://arxiv.org/abs/2402.05098) | 本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。 |
| [^16] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^17] | [SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning](https://arxiv.org/abs/2402.04114) | 本文量化了联邦线性随机逼近算法中异质性偏差的影响，并提出SCAFFLSA作为一种改进方法来消除此偏差。在联邦时间差异学习中，该方法能够显著提高算法的复杂性。 |
| [^18] | [Breaking the Curse of Dimensionality with Distributed Neural Computation](https://arxiv.org/abs/2402.03460) | 通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。 |
| [^19] | [A Framework for Partially Observed Reward-States in RLHF](https://arxiv.org/abs/2402.03282) | 这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。 |
| [^20] | [Diffusive Gibbs Sampling](https://arxiv.org/abs/2402.03008) | 扩散吉布斯采样是一种创新的采样方法，通过集成扩散模型并应用吉布斯采样，有效地从具有远程和断开模态特征的分布中采样，表现出比其他方法更好的混合性能，并在多种任务中取得显著改进的结果。 |
| [^21] | [Glocal Hypergradient Estimation with Koopman Operator](https://arxiv.org/abs/2402.02741) | 本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。 |
| [^22] | [Variational DAG Estimation via State Augmentation With Stochastic Permutations](https://arxiv.org/abs/2402.02644) | 使用状态扩展和随机排列进行变分DAG估计的方法可以超越竞争的贝叶斯和非贝叶斯基准方法，从而在估计贝叶斯网络结构方面取得更好的性能。 |
| [^23] | [Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems](https://arxiv.org/abs/2402.02190) | 本研究提出了连续张量放松方法(CTRA)，用于在组合优化问题中寻找多样化的解决方案。CTRA通过对离散决策变量进行连续放松，解决了寻找多样化解决方案的挑战。 |
| [^24] | [Multitask methods for predicting molecular properties from heterogeneous data](https://arxiv.org/abs/2401.17898) | 多任务方法可以通过利用异质数据源来预测分子属性，大大降低数据生成成本，并且达到与耦合簇数据相当的准确度。 |
| [^25] | [Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?.](http://arxiv.org/abs/2401.13544) | 本文介绍了一种超越概念瓶颈模型的方法，可以使黑盒模型可干预。通过基于概念的干预来影响模型的输出，并利用这种方法对黑盒模型进行微调。实验证明，微调可以提高干预的效果，并产生更好校准的预测。 |
| [^26] | [DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport.](http://arxiv.org/abs/2401.13112) | 本文提出了使用最优传输进行分布式对抗解释的方法DISCOUNT，将对抗解释的概念扩展到整个输入输出分布，并通过统计置信度来支撑这一方法。 |
| [^27] | [Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference.](http://arxiv.org/abs/2310.16705) | 本文将变分推断重新框架为在变分参数空间上的概率分布优化问题，提出了沃瑟斯坦梯度下降方法来解决优化问题，有效性经过实证实验证实。 |
| [^28] | [Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining.](http://arxiv.org/abs/2310.08566) | 通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。 |
| [^29] | [Energy-Guided Continuous Entropic Barycenter Estimation for General Costs.](http://arxiv.org/abs/2310.01105) | 本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。 |
| [^30] | [Controlling Continuous Relaxation for Combinatorial Optimization.](http://arxiv.org/abs/2309.16965) | 本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。 |
| [^31] | [Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach.](http://arxiv.org/abs/2309.16858) | 我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。 |
| [^32] | [A Geometric Framework for Neural Feature Learning.](http://arxiv.org/abs/2309.10140) | 本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。 |
| [^33] | [Calibrated Explanations for Regression.](http://arxiv.org/abs/2308.16245) | 本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。 |
| [^34] | [Learning to solve Bayesian inverse problems: An amortized variational inference approach.](http://arxiv.org/abs/2305.20004) | 本文提出了一种基于深度神经网络的参数化表示后验分布的摊销变分推理方法，以实现实时推理的目的，可应用于流体力学中的参数估计和流场重构等领域。 |
| [^35] | [A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates.](http://arxiv.org/abs/2303.16668) | 本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。 |
| [^36] | [Lost in the Shuffle: Testing Power in the Presence of Errorful Network Vertex Labels.](http://arxiv.org/abs/2208.08638) | 研究了当网络中存在不匹配/标签混乱的顶点时，两个样本图假设检验中的功率损失，并通过多个实验加以验证。 |
| [^37] | [Long Story Short: Omitted Variable Bias in Causal Machine Learning.](http://arxiv.org/abs/2112.13398) | 在因果机器学习中，我们通过推导出遗漏变量偏差的尖锐上界，为广泛的线性泛函因果参数提供了一种简单而通用的方法。这种方法可以应用于许多传统的因果推断研究目标，并且仅取决于潜变量在结果和参数的Riesz表示器中所导致的额外变异。 |

# 详细

[^1]: 学习网络增长机制

    Learning the mechanisms of network growth

    [https://arxiv.org/abs/2404.00793](https://arxiv.org/abs/2404.00793)

    我们提出了一种新的模型选择方法，通过在大量合成网络数据上训练分类器，设计出易于计算、解析可处理且具有解释性的动态特征类型，实现了几乎完美的网络分类，超过了目前技术水平。

    

    我们提出了一种面向动态真实网络的新模型选择方法。我们的方法涉及在大量合成网络数据上训练分类器。数据是通过模拟动态网络的九种最先进的随机图模型生成的，选定参数范围以确保网络规模随时间呈指数增长。我们设计了一种概念上新颖的动态特征类型，它计算在特定时间间隔内一组顶点收到的新链接数。所提出的特征易于计算，解析上可处理，并具有解释性。我们的方法实现了对合成网络的几乎完美分类，超过当前技术水平。将我们的分类方法应用于真实世界的引文网络，对文献中声称的具有优先附着、适应性和老化的模型最好地适应真实世界的引文网络的说法具有可靠性，尽管有时，预测可能。

    arXiv:2404.00793v1 Announce Type: cross  Abstract: We propose a novel model-selection method for dynamic real-life networks. Our approach involves training a classifier on a large body of synthetic network data. The data is generated by simulating nine state-of-the-art random graph models for dynamic networks, with parameter range chosen to ensure exponential growth of the network size in time. We design a conceptually novel type of dynamic features that count new links received by a group of vertices in a particular time interval. The proposed features are easy to compute, analytically tractable, and interpretable. Our approach achieves a near-perfect classification of synthetic networks, exceeding the state-of-the-art by a large margin. Applying our classification method to real-world citation networks gives credibility to the claims in the literature that models with preferential attachment, fitness and aging fit real-world citation networks best, although sometimes, the predicted m
    
[^2]: 最优流匹配：在一步中学习直线轨迹

    Optimal Flow Matching: Learning Straight Trajectories in Just One Step

    [https://arxiv.org/abs/2403.13117](https://arxiv.org/abs/2403.13117)

    该论文提出了一种新颖的最优流匹配方法，能够在一步中学习实现二次成本下的直线 OT 位移。

    

    在过去几年中，流匹配方法在生成建模中得到了蓬勃发展。社区追求的一个引人注目的属性是能够学习具有直线轨迹的流，这些轨迹实现了最优输运（OT）置换。直线性对于快速集成学习流的路径至关重要。不幸的是，大多数现有的流直线化方法都基于非平凡的迭代过程，在训练过程中积累误差或利用启发式小批量OT近似。为解决这一问题，我们开发了一种新颖的最优流匹配方法，仅通过一次流匹配步骤即可为二次成本恢复直线OT置换。

    arXiv:2403.13117v1 Announce Type: cross  Abstract: Over the several recent years, there has been a boom in development of flow matching methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the optimal transport (OT) displacements. Straightness is crucial for fast integration of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative procedures which accumulate the error during training or exploit heuristic minibatch OT approximations. To address this issue, we develop a novel optimal flow matching approach which recovers the straight OT displacement for the quadratic cost in just one flow matching step.
    
[^3]: 均场微正则梯度下降

    Mean-Field Microcanonical Gradient Descent

    [https://arxiv.org/abs/2403.08362](https://arxiv.org/abs/2403.08362)

    提出了均场微正则梯度下降方法，通过同时采样多个弱耦合数据点，在控制熵损失的同时在似然拟合方面表现良好。

    

    微正则梯度下降是一种能量模型的采样过程，可实现高维分布的高效采样，通过梯度下降将样本从高熵分布（如高斯白噪声）转运至低能区域。我们将这一模型置于正则化流的框架中，显示它通常会由于在下降过程中失去不必要的熵而过度拟合。为解决这一问题，我们提出了均场微正则梯度下降，同时采样多个弱耦合数据点，允许更好地控制熵损失，同时在似然拟合方面付出较小代价。我们将这些模型应用于金融时间序列的背景中，展示了在合成和真实数据上的改进。

    arXiv:2403.08362v1 Announce Type: cross  Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.
    
[^4]: 通过正则化流进行互信息估计

    Mutual Information Estimation via Normalizing Flows

    [https://arxiv.org/abs/2403.02187](https://arxiv.org/abs/2403.02187)

    通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。

    

    我们提出了一种新颖的方法来解决互信息（MI）估计问题，即引入基于正则化流的估计器。该估计器将原始数据映射到具有已知互信息闭合形式表达式的目标分布。我们证明了我们的方法产生了原始数据的互信息估计。通过高维数据的实验结果展示了所提出估计器的优势。

    arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
    
[^5]: 批判性窗口：扩散模型中特征出现的非渐进理论

    Critical windows: non-asymptotic theory for feature emergence in diffusion models

    [https://arxiv.org/abs/2403.01633](https://arxiv.org/abs/2403.01633)

    我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。

    

    我们发展理论来理解图像生成扩散模型中一个有趣的属性，我们称之为批判性窗口。实证上观察到在采样过程中存在狭窄的时间间隔，在此期间会出现最终图像的特定特征，例如图像类别或背景颜色。而这种特性对于解释性是有利的，因为意味着可以将生成的特性定位到轨迹的一个小片段，但这似乎与扩散的连续性质相矛盾。我们提出了一个形式化框架来研究这些窗口，并表明对于来自混合强对数凹密度分布的数据，这些窗口可以用一定的跨组和组内分离度量来显式地约束。我们还为诸如良条件G的具体示例实例化了这些界限。

    arXiv:2403.01633v1 Announce Type: new  Abstract: We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned G
    
[^6]: 对深度生成模型的费舍尔信息度量进行近似用于检测离群分布

    Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection

    [https://arxiv.org/abs/2403.01485](https://arxiv.org/abs/2403.01485)

    本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法

    

    基于概率似然的深度生成模型，如基于评分的扩散模型和变分自动编码器，是近年来用于拟合高维数据分布（如图像、文本或音频）的先进机器学习模型之一。它们可以自然地应用于许多下游任务之一，即离群分布（OOD）检测。然而，Nalisnick等人的开创性工作表明，深度生成模型始终为OOD数据推断出比它们训练过的数据更高的对数似然，标志着一个悬而未决的问题。在这项工作中，我们分析了使用数据点对深度生成模型的参数梯度进行OOD检测，基于这样的简单直觉，即OOD数据的梯度范数应该大于训练数据。我们形式化地将梯度大小的度量量化为近似费舍尔信息度量。我们展示了费舍尔信息矩阵（FIM）具有较大的绝对值

    arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di
    
[^7]: 用于非对数凹分布的零阶采样方法：通过去噪扩散缓解亚稳定性

    Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion

    [https://arxiv.org/abs/2402.17886](https://arxiv.org/abs/2402.17886)

    本文提出了一种基于去噪扩散过程的零阶扩散蒙特卡洛算法，克服了非对数凹分布采样中的亚稳定性问题，并证明其采样精度具有倒多项式依赖。

    

    这篇论文考虑了基于其非对数凹分布未归一化密度查询的采样问题。首先描述了一个基于模拟去噪扩散过程的框架，即扩散蒙特卡洛（DMC），其得分函数通过通用蒙特卡洛估计器逼近。DMC是一个基于神谕的元算法，其中神谕是假设可以访问生成蒙特卡洛分数估计器的样本的访问。然后，我们提供了一个基于拒绝采样的这个神谕的实现，这将DMC转化为一个真正的算法，称为零阶扩散蒙特卡洛（ZOD-MC）。我们通过首先构建一个通用框架，即DMC的性能保证，而不假设目标分布为对数凹或满足任何等周不等式，提供了收敛分析。然后我们证明ZOD-MC对所需采样精度具有倒多项式依赖，尽管仍然受到...

    arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
    
[^8]: Conformalized Selective Regression

    Conformalized Selective Regression

    [https://arxiv.org/abs/2402.16300](https://arxiv.org/abs/2402.16300)

    通过利用一致性预测，提供基于模型特定偏差的置信度量，以解决选择性回归中不确定性测量的方法。

    

    预测模型是否总是要提供预测？在追求最大预测性能的过程中，可靠性和公平性往往被忽视，尤其是关于不确定性的作用。选择性回归，也称为“拒绝选项”，允许模型在存在相当大的不确定性情况下放弃预测。尽管7十年前就最初提出了选择性回归的方法，但大多数方法主要集中在用于测量不确定性的基于分布的代理，尤其是条件方差。但这种关注忽视了模型特定偏差对模型性能的显著影响。本文提出了一种新的选择性回归方法，通过利用一致性预测，为基于模型特定偏差的个别预测提供有根据的置信度度量。此外，我们提出了一个标准化的评估框架，以便进行恰当的比较。

    arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
    
[^9]: 小型基准测试：用更少的示例评估LLM

    tinyBenchmarks: evaluating LLMs with fewer examples

    [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)

    本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。

    

    大型语言模型（LLMs）的多功能性导致创建了多种基准测试，彻底测试各种语言模型的能力。这些基准测试包含成千上万个示例，使得评估LLMs非常昂贵。本文研究了减少评估LLMs性能所需的评估次数的策略。例如，我们展示了要准确估计LLMs在MMLU上的性能（一个包含14K个示例的流行多选问答基准测试），只需要在100个精心挑选的示例上评估这个LLMs。我们发布了评估工具和流行基准测试的微型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和微型基准测试足以可靠且高效地重现原始评估结果。

    arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
    
[^10]: 谱聚类中特征向量的渐近高斯波动

    Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering

    [https://arxiv.org/abs/2402.12302](https://arxiv.org/abs/2402.12302)

    提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。

    

    谱聚类的性能依赖于相似矩阵的特征向量的条目波动，该波动直到现在仍未得到描述。本文表明，一般尖峰随机矩阵模型的信号+噪声结构被转移到相应的格拉姆核矩阵的特征向量上，并且它们的条目波动在大维度区域呈高斯分布。这种类似于中心极限定理的结果是准确预测谱聚类的分类性能的最后一块缺失的拼图。提出的证明非常通用，仅依赖于噪声的旋转不变性。对合成和真实数据的数值实验表明了这个现象的普适性。

    arXiv:2402.12302v1 Announce Type: cross  Abstract: The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.
    
[^11]: AdAdaGrad：自适应梯度方法的自适应批大小方案

    AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods

    [https://arxiv.org/abs/2402.11215](https://arxiv.org/abs/2402.11215)

    AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。

    

    随机梯度优化器中批量大小的选择对模型训练至关重要。然而，在训练过程中变化批大小的实践相对其他超参数较少探讨。我们研究了从自适应采样方法中导出的自适应批大小策略，传统上仅应用于随机梯度下降。考虑到学习速率和批大小之间的显著相互作用，以及自适应梯度方法在深度学习中的普及，我们强调在这些情境中需要自适应批大小策略。我们介绍了AdAdaGrad及其标量变体AdAdaGradNorm，它们在训练过程中逐渐增加批大小，同时使用AdaGrad和AdaGradNorm进行模型更新。我们证明了AdaGradNorm以高概率以$O(1/K)$的速度收敛，用于找到光滑非凸函数的一阶稳定点在$K$次迭代内。

    arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
    
[^12]: 加速并行采样扩散模型

    Accelerating Parallel Sampling of Diffusion Models

    [https://arxiv.org/abs/2402.09970](https://arxiv.org/abs/2402.09970)

    本文提出了一种并行化自回归过程来加速扩散模型的采样的方法，并引入了ParaTAA，一种通用的并行采样算法，可以显著减少推理步骤。

    

    扩散模型已经成为图像生成的最先进生成模型。然而，由于其采样过程中固有的自回归性质，从扩散模型中进行采样通常耗时。在本文中，我们提出了一种新的方法，通过并行化自回归过程来加速扩散模型的采样。具体而言，我们将采样过程重新构建为通过固定点迭代解决三角非线性方程组的过程。通过这种创新的公式，我们探索了一些系统化的技术，进一步减少了求解过程所需的迭代步骤。应用这些技术，我们引入了ParaTAA，一种通用的、无需训练的并行采样算法，可以利用额外的计算和内存资源来增加采样速度。我们的实验表明，ParaTAA可以减少常见的顺序采样所需的推理步骤。

    arXiv:2402.09970v1 Announce Type: new  Abstract: Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampli
    
[^13]: 关于顺序预测中的标定距离研究

    On the Distance from Calibration in Sequential Prediction

    [https://arxiv.org/abs/2402.07458](https://arxiv.org/abs/2402.07458)

    本论文研究了顺序预测中的标定距离，证明了存在一种预测算法可以在敌人选择的二进制序列上实现$O(\sqrt{T})$的标定距离，通过较低的标定距离进行准确近似。

    

    我们研究了一种顺序二进制预测场景，在这种场景中，预测器的评估是以标定距离为基准的，标定距离定义为预测值与事后完全标定的预测集之间的$L_1$距离。这类似于最近由B{\l}asiok、Gopalan、Hu和Nakkiran（STOC 2023）提出的离线场景中的标定度量。标定距离是一种自然且直观的偏离完美标定的度量，并且满足不同于许多常见的标定度量（如$L_1$标定误差及其变种）的Lipschitz连续性属性。我们证明了存在一种预测算法，可以在对敌人选择的长度为$T$的二进制序列上，以期望$O(\sqrt{T})$的标定距离实现。在这个上界的核心是一个结构性结果，证明了标定距离可以通过较低的标定距离进行准确近似。

    We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a con
    
[^14]: HyperBERT:将混合超图感知层与语言模型用于文本属性超图上的节点分类

    HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs

    [https://arxiv.org/abs/2402.07309](https://arxiv.org/abs/2402.07309)

    本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。

    

    超图通过复杂的拓扑结构标记，表达多个实体之间的高阶相互作用，其中超边扮演重要角色。最近，基于超图的深度学习方法在学习文本属性超图上的节点分类问题中引起了越来越多的研究关注。然而，现有方法往往难以同时捕捉超图结构信息的全部内容和节点属性中的丰富语言属性，这在很大程度上影响了它们的效果和泛化能力。为了克服这些挑战，我们探索了如何通过为节点分类任务进一步增强预训练的BERT模型，引入专门的超图感知层。这些层将高阶结构归纳偏差引入语言模型中，从而提高模型利用超图结构中的高阶上下文信息和文本中的语义信息的能力。

    Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
    
[^15]: 关于分散推断模型的扩散模型：基准测试和改进随机控制和采样

    On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling

    [https://arxiv.org/abs/2402.05098](https://arxiv.org/abs/2402.05098)

    本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。

    

    我们研究了训练扩散模型以从给定的非标准化密度或能量函数分布中采样的问题。我们对几种扩散结构推断方法进行了基准测试，包括基于模拟的变分方法和离策略方法（连续生成流网络）。我们的结果揭示了现有算法的相对优势，同时对过去的研究提出了一些质疑。我们还提出了一种新颖的离策略方法探索策略，基于目标空间中的局部搜索和回放缓冲区的使用，并证明它可以改善各种目标分布上的样本质量。我们研究的采样方法和基准测试的代码已公开在https://github.com/GFNOrg/gfn-diffusion，作为未来在分散推断模型上工作的基础。

    We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
    
[^16]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^17]: SCAFFLSA：量化和消除联邦线性随机逼近和时间差异学习中的异质性偏差

    SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning

    [https://arxiv.org/abs/2402.04114](https://arxiv.org/abs/2402.04114)

    本文量化了联邦线性随机逼近算法中异质性偏差的影响，并提出SCAFFLSA作为一种改进方法来消除此偏差。在联邦时间差异学习中，该方法能够显著提高算法的复杂性。

    

    本文对联邦线性随机逼近算法（FedLSA）进行了非渐进分析。我们明确量化了异质代理本地训练引入的偏差，并研究了该算法的样本复杂性。我们证明了FedLSA的通信复杂性与所需精度 $\epsilon$ 呈多项式关系，这限制了联邦的好处。为了克服这一问题，我们提出了SCAFFLSA，一种新型的FedLSA变体，它使用控制变量来校正本地训练的偏差，并在不对统计异质性做出任何假设的情况下证明了其收敛性。我们将所提出的方法应用于具有线性函数逼近的联邦时间差异学习，并分析了相应的复杂性改进。

    In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
    
[^18]: 用分布式神经计算突破维度灾难

    Breaking the Curse of Dimensionality with Distributed Neural Computation

    [https://arxiv.org/abs/2402.03460](https://arxiv.org/abs/2402.03460)

    通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。

    

    我们提出了一种理论方法，利用分布式神经计算算法来克服维度灾难。我们的模块化分布式深度学习范式，称为“神经途径”，可以在多台机器上实现任意精度，同时仅加载少量参数到GPU VRAM中。具体地，我们证明了对于每个误差水平$\varepsilon>0$和每个Lipschitz函数$f:[0,1]^n\to \mathbb{R}$，我们可以构建一个神经途径模型，该模型能够在$[0,1]^n$上以$\varepsilon$精度均匀逼近$f$，并且仅需要在内存中加载$\mathcal{O}(\varepsilon^{-1})$个网络参数以及在前向传播期间加载$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$个网络参数。这改进了传统非分布式深度学习模型（即ReLU多层感知机）的最优界限，后者需要$\mathcal{O}(\varepsilon^{-n/2})$个参数来达到相同的精度。目前唯一的其他可用深度学习模型

    We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and $\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model
    
[^19]: 一个部分观察到的奖励状态在RLHF中的框架

    A Framework for Partially Observed Reward-States in RLHF

    [https://arxiv.org/abs/2402.03282](https://arxiv.org/abs/2402.03282)

    这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。

    

    最近几年来，强化学习从人类反馈（RLHF）的研究因其在LLMs的发展中起到的作用而变得重要。神经科学研究表明，人类对刺激的反应已知依赖于部分观察到的“内部状态”。不幸的是，当前的RLHF模型没有考虑到这一点。此外，大多数RLHF模型没有考虑到中间反馈，在实证研究中变得越来越重要，可以帮助提高样本复杂性和对齐性。为了解决这些局限性，我们将RLHF建模为部分观察到的奖励状态的强化学习（PORRL）。我们展示了从RLHF中两种主要形式的人类反馈 - 基数反馈和决斗反馈到PORRL的缩减。对于基数反馈，我们开发了通用的统计高效算法，并将它们实例化为POR-UCRL和POR-UCBVI。对于决斗反馈，我们表明，简单的基数反馈缩减不能达到亚线性的决斗回归。

    The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
    
[^20]: 扩散吉布斯采样

    Diffusive Gibbs Sampling

    [https://arxiv.org/abs/2402.03008](https://arxiv.org/abs/2402.03008)

    扩散吉布斯采样是一种创新的采样方法，通过集成扩散模型并应用吉布斯采样，有效地从具有远程和断开模态特征的分布中采样，表现出比其他方法更好的混合性能，并在多种任务中取得显著改进的结果。

    

    传统马尔可夫链蒙特卡洛（MCMC）方法在多模态分布的混合不足方面存在着挑战，特别是在贝叶斯推断和分子动力学等实际应用中。针对这个问题，我们提出了一种创新的采样方法——扩散吉布斯采样（DiGS），用于有效采样具有远程和断开模态特征的分布。DiGS集成了扩散模型的最新发展，利用高斯卷积创建一个辅助噪声分布，以在原始空间中连接孤立的模态，并应用吉布斯采样从两个空间中交替抽取样本。我们的方法在采样多模态分布方面表现出比并行温度法等最先进方法更好的混合性能。我们证明我们的采样器在各种任务中取得了显著改进的结果，包括高斯混合模型、贝叶斯神经网络和分子动力学。

    The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
    
[^21]: 具有Koopman算子的全局超梯度估计

    Glocal Hypergradient Estimation with Koopman Operator

    [https://arxiv.org/abs/2402.02741](https://arxiv.org/abs/2402.02741)

    本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。

    

    基于梯度的超参数优化方法使用超梯度来更新超参数，即元标准的梯度与超参数的关系。先前的研究使用两种不同的更新策略：一种是使用模型训练完成后得到的全局超梯度来优化超参数，另一种是使用每个模型更新之后得到的局部超梯度。虽然全局超梯度具有可靠性，但计算成本显著；相反，局部超梯度速度快但常常不是最优的。在本文中，我们提出了glocal超梯度估计，将“全局”的质量与“局部”的效率结合起来。为此，我们使用Koopman算子理论来线性化超梯度的动态，以便可以仅通过使用局部超梯度的轨迹来高效地近似全局超梯度。因此，我们可以使用估计的全局超梯度贪婪地优化超参数，同时实现可靠性和效率。

    Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending "global" quality with "local" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo
    
[^22]: 通过状态扩展和随机排列的方法进行变分DAG估计

    Variational DAG Estimation via State Augmentation With Stochastic Permutations

    [https://arxiv.org/abs/2402.02644](https://arxiv.org/abs/2402.02644)

    使用状态扩展和随机排列进行变分DAG估计的方法可以超越竞争的贝叶斯和非贝叶斯基准方法，从而在估计贝叶斯网络结构方面取得更好的性能。

    

    从观测数据中估计贝叶斯网络的结构，即有向无环图（DAG），是一个在统计和计算上都很困难的问题，在因果发现等领域有着重要应用。贝叶斯方法在解决这个任务方面是一个有希望的方向，因为它们允许进行不确定性量化，并处理众所周知的可识别性问题。从概率推断的角度来看，主要的挑战是（i）表示满足DAG约束的图的分布和（ii）估计底层组合空间的后验概率。我们提出了一种方法，通过在DAG和排列的扩展空间上构建联合分布来解决这些挑战。我们通过变分推断进行后验估计，在其中利用了离散分布的连续松弛。我们展示了我们的方法在一系列合成和实际数据上能够超越竞争的贝叶斯和非贝叶斯基准方法。

    Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
    
[^23]: 在组合优化问题中寻找多样化解决方案的连续张量放松方法

    Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems

    [https://arxiv.org/abs/2402.02190](https://arxiv.org/abs/2402.02190)

    本研究提出了连续张量放松方法(CTRA)，用于在组合优化问题中寻找多样化的解决方案。CTRA通过对离散决策变量进行连续放松，解决了寻找多样化解决方案的挑战。

    

    在组合优化问题中，寻找最佳解是最常见的目标。然而，在实际场景中，单一解决方案可能不适用，因为目标函数和约束条件只是原始现实世界情况的近似值。为了解决这个问题，寻找具有不同特征的多样化解决方案和约束严重性的变化成为自然的方向。这种策略提供了在后处理过程中选择合适解决方案的灵活性。然而，发现这些多样化解决方案比确定单一解决方案更具挑战性。为了克服这一挑战，本研究引入了连续张量松弛退火 (CTRA) 方法，用于基于无监督学习的组合优化求解器。CTRA通过扩展连续松弛方法，将离散决策变量转换为连续张量，同时解决了多个问题。该方法找到了不同特征的多样化解决方案和约束严重性的变化。

    Finding the best solution is the most common objective in combinatorial optimization (CO) problems. However, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. To tackle this, finding (i) "heterogeneous solutions", diverse solutions with distinct characteristics, and (ii) "penalty-diversified solutions", variations in constraint severity, are natural directions. This strategy provides the flexibility to select a suitable solution during post-processing. However, discovering these diverse solutions is more challenging than identifying a single solution. To overcome this challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning-based CO solvers. CTRA addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. This method finds heterog
    
[^24]: 从异质数据预测分子属性的多任务方法

    Multitask methods for predicting molecular properties from heterogeneous data

    [https://arxiv.org/abs/2401.17898](https://arxiv.org/abs/2401.17898)

    多任务方法可以通过利用异质数据源来预测分子属性，大大降低数据生成成本，并且达到与耦合簇数据相当的准确度。

    

    数据生成仍然是训练代理模型预测分子属性的瓶颈。我们证明，多任务高斯过程回归通过利用昂贵和廉价的数据源，克服了这个限制。特别是，我们考虑从耦合簇（CC）和密度泛函理论（DFT）数据构建的训练集。我们报告说，多任务代理模型可以以CC级精度进行预测，并且数据生成成本减少了一个数量级。值得注意的是，我们的方法允许训练集包括由异质混合的交换相关泛函生成的DFT数据，而不对泛函精度施加任何人为的层次结构。更一般地，多任务框架可以适应更广泛范围的训练集结构，包括不同保真级别之间的完全差异，而不像现有的基于$\Delta$学习的核方法那样，我们证明这两种方法的准确度可以相似。

    Data generation remains a bottleneck in training surrogate models to predict molecular properties. We demonstrate that multitask Gaussian process regression overcomes this limitation by leveraging both expensive and cheap data sources. In particular, we consider training sets constructed from coupled-cluster (CC) and density function theory (DFT) data. We report that multitask surrogates can predict at CC level accuracy with a reduction to data generation cost by over an order of magnitude. Of note, our approach allows the training set to include DFT data generated by a heterogeneous mix of exchange-correlation functionals without imposing any artificial hierarchy on functional accuracy. More generally, the multitask framework can accommodate a wider range of training set structures -- including full disparity between the different levels of fidelity -- than existing kernel approaches based on $\Delta$-learning, though we show that the accuracy of the two approaches can be similar. Con
    
[^25]: 超越概念瓶颈模型：如何使黑盒模型可干预？

    Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?. (arXiv:2401.13544v1 [cs.LG])

    [http://arxiv.org/abs/2401.13544](http://arxiv.org/abs/2401.13544)

    本文介绍了一种超越概念瓶颈模型的方法，可以使黑盒模型可干预。通过基于概念的干预来影响模型的输出，并利用这种方法对黑盒模型进行微调。实验证明，微调可以提高干预的效果，并产生更好校准的预测。

    

    最近，可解释的机器学习重新探索了概念瓶颈模型（CBM），包括从原始特征中逐步预测高级概念和从预测的概念中预测目标变量。这个模型类别的一个引人注目的优势是用户能够对预测的概念值进行干预，从而影响模型的下游输出。在这项工作中，我们介绍了一种方法，在已经训练好但本质上不可解释的神经网络上进行基于概念的干预，给定一个带有注释的验证集。此外，我们将模型的可干预性定义为基于概念干预的有效性的度量，并利用这个定义来对黑盒模型进行微调。实证上，我们探索了合成表格数据和自然图像基准上黑盒分类器的干预性。我们证明，微调提高了干预的效果，并经常产生更好校准的预测。

    Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the 
    
[^26]: DISCOUNT: 使用最优传输进行分布式对抗解释

    DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])

    [http://arxiv.org/abs/2401.13112](http://arxiv.org/abs/2401.13112)

    本文提出了使用最优传输进行分布式对抗解释的方法DISCOUNT，将对抗解释的概念扩展到整个输入输出分布，并通过统计置信度来支撑这一方法。

    

    对抗解释是在黑盒决策模型中提供洞察力和可解释性的事实方法，通过确定导致不同结果的替代输入实例来实现。本文将对抗解释的概念扩展到分布上下文，从个体数据点扩大到整个输入输出分布，命名为分布式对抗解释。在分布式对抗解释中，我们的重点转向分析事实和对抗的分布属性，类似于评估个体实例及其结果决策的经典方法。我们利用最优传输来构建一个机会约束优化问题，旨在导出与事实对应的对抗分布，以统计置信度做支撑。我们提出的优化方法DISCOUNT在输入和输出分布之间平衡这种置信度。

    Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
    
[^27]: 沃瑟斯坦梯度流在变分推断的变分参数空间上的应用

    Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference. (arXiv:2310.16705v1 [cs.LG])

    [http://arxiv.org/abs/2310.16705](http://arxiv.org/abs/2310.16705)

    本文将变分推断重新框架为在变分参数空间上的概率分布优化问题，提出了沃瑟斯坦梯度下降方法来解决优化问题，有效性经过实证实验证实。

    

    变分推断可以被看作是一个优化问题，其中变分参数被调整以使变分分布与真实后验尽可能接近。可以通过黑箱变分推断中的普通梯度下降或自然梯度变分推断中的自然梯度下降来解决优化任务。在本文中，我们将变分推断重新框架为在一个“变分参数空间”中定义的概率分布的目标优化问题。随后，我们提出了沃瑟斯坦梯度下降方法来解决这个优化问题。值得注意的是，这些优化技术，即黑箱变分推断和自然梯度变分推断，可以重新解释为所提出的沃瑟斯坦梯度下降的特定实例。为了提高优化效率，我们开发了实用的方法来数值求解离散梯度流。通过在一个合成数据集上的实证实验，我们验证了所提出方法的有效性。

    Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theore
    
[^28]: 以Transformer为决策者：通过监督预训练实现可证明的上下文强化学习

    Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])

    [http://arxiv.org/abs/2310.08566](http://arxiv.org/abs/2310.08566)

    通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。

    

    在离线强化学习数据集上预训练的大型Transformer模型展示了令人惊叹的上下文强化学习能力，即当它们面对来自未知环境的交互轨迹时，它们能够做出良好的决策。然而，Transformer模型如何进行训练以执行上下文强化学习，在理论上尚未得到很好的理解。特别是，尚不清楚Transformer模型可以在上下文中执行哪些强化学习算法以及离线训练数据中的分布差异如何影响已学习的算法。本文提供了一个理论框架，分析了对上下文强化学习的监督预训练。这包括了两种最近提出的训练方法：算法蒸馏和决策预训练的Transformer模型。首先，在假设模型可实现的情况下，我们证明了经过监督预训练的Transformer模型将模仿专家算法在观察到的轨迹上的条件期望。广义误差的缩放范围与…

    Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
    
[^29]: 基于能量导向的连续熵巴氏中心估计方法及其在一般成本问题中的应用

    Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])

    [http://arxiv.org/abs/2310.01105](http://arxiv.org/abs/2310.01105)

    本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。

    

    优化输运（OT）巴氏中心是一种在捕捉概率分布几何特性的同时对其进行平均的数学方法。本文提出了一种新颖的算法，用于近似计算任意OT成本函数的连续熵OT巴氏中心。我们的方法基于最近在机器学习社区中受到关注的基于弱OT的连续熵最优输运问题的对偶重构。除了创新性之外，我们的方法还具有以下若干优势特点：（i）我们建立了对恢复解的质量界限；（ii）该方法与基于能量的模型（EBMs）学习过程无缝连接，可以使用经过良好调整的算法解决感兴趣的问题；（iii）它提供了一种直观的优化方案，避免使用极小-极大、强化等复杂技巧。为了验证我们的方法，我们考虑了s

    Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
    
[^30]: 控制组合优化的连续放松

    Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])

    [http://arxiv.org/abs/2309.16965](http://arxiv.org/abs/2309.16965)

    本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。

    

    最近在组合优化（CO）问题中，图神经网络（GNNs）显示出巨大潜力。通过无监督学习找到近似解的受物理启发的GNN（PI-GNN）求解器在大规模CO问题上引起了极大关注。然而，对于相对密集图上的CO问题，贪婪算法的性能恶化，但对于PI-GNN求解器的性能却没有太多讨论。此外，由于PI-GNN求解器采用了放松策略，学习后需要从连续空间人工转换回原始离散空间，可能会破坏解的鲁棒性。本文通过数值实验证明了PI-GNN求解器在密集图上的CO问题的学习早期可能陷入局部解的情况，其中所有变量都为零。然后，我们通过控制连续性和离散性来解决这些问题。

    Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
    
[^31]: Transductive Learning的尖锐泛化：一种Transductive Local Rademacher Complexity方法

    Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])

    [http://arxiv.org/abs/2309.16858](http://arxiv.org/abs/2309.16858)

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。

    

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们的工作将传统的local rademacher complexity (LRC)的思想扩展到了transductive设置中，相对于典型的LRC方法在归纳设置中的分析有了相当大的变化。我们提出了一种基于Rademacher complex的局部化工具，可以应用于各种transductive learning问题，并在适当条件下得到了尖锐的界限。与LRC的发展类似，我们通过从独立变量的方差信息开始构建TLRC，将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。经过精心设计的...

    We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
    
[^32]: 一个基于神经特征学习的几何框架

    A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])

    [http://arxiv.org/abs/2309.10140](http://arxiv.org/abs/2309.10140)

    本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。

    

    我们提出了一个基于神经特征提取器的学习系统设计的新框架，通过利用特征空间中的几何结构。首先，我们引入了特征几何，它将统计依赖和特征统一到同一个具有几何结构的函数空间中。通过应用特征几何，我们将每个学习问题形式化为解决由学习设置指定的依赖组件的最佳特征近似解。我们提出了一种嵌套技术来设计学习算法，从数据样本中学习最佳特征，这可以应用于现有的网络架构和优化器。为了展示嵌套技术的应用，我们进一步讨论了多变量学习问题，包括条件推理和多模态学习，在这些问题中，我们提出了最佳特征并揭示了它们与经典方法的联系。

    We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
    
[^33]: 回归问题的校准解释

    Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])

    [http://arxiv.org/abs/2308.16245](http://arxiv.org/abs/2308.16245)

    本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。

    

    人工智能（AI）通常是现代决策支持系统（DSS）的一部分。在基于AI的DSS中使用的最佳预测模型缺乏透明度。可解释的人工智能（XAI）旨在创建能够向人类用户解释其理由的AI系统。XAI中的局部解释可以提供关于个别预测的原因的信息，即特征重要性。然而，现有局部解释方法的一个关键缺点是无法量化与特征重要性相关的不确定性。本文介绍了特征重要性解释方法Calibrated Explanations（CE）的扩展，之前只支持分类，现在支持标准回归和概率回归，即目标超过任意阈值的概率。回归问题的扩展保留了CE的所有优点，例如将底层模型的预测与置信度校准。

    Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
    
[^34]: 学习解决贝叶斯逆问题：一种摊销变分推理方法

    Learning to solve Bayesian inverse problems: An amortized variational inference approach. (arXiv:2305.20004v1 [stat.ML])

    [http://arxiv.org/abs/2305.20004](http://arxiv.org/abs/2305.20004)

    本文提出了一种基于深度神经网络的参数化表示后验分布的摊销变分推理方法，以实现实时推理的目的，可应用于流体力学中的参数估计和流场重构等领域。

    

    逆问题，即从实验数据中估计物理模型的参数，在科学和工程中普遍存在。贝叶斯公式是黄金标准，因为它可以缓解病态性问题并量化认识不确定性。然而，由于解析后验不通常可用，人们采用马尔可夫链蒙特卡罗采样或近似变分推理。但是，需要重新从头开始进行推理以适应每组新数据。这种缺点限制了贝叶斯公式在实时设置，例如工程系统的健康监测和医疗诊断中的适用性。本文的目标是开发一种方法，通过学习从数据到后验的贝叶斯逆映射，即从数据到后验的映射，实现实时推理的方法。我们的方法如下。我们使用基于深度神经网络的参数化表示后验分布。接下来，我们通过摊销变分推理学习网络参数，其中训练神经网络以预测从数据中预测后验分布，从而实现快速准确的推理。我们在流体力学中的一些逆问题上展示了我们方法的有效性，其中包括参数估计和流场重构。

    Inverse problems, i.e., estimating parameters of physical models from experimental data, are ubiquitous in science and engineering. The Bayesian formulation is the gold standard because it alleviates ill-posedness issues and quantifies epistemic uncertainty. Since analytical posteriors are not typically available, one resorts to Markov chain Monte Carlo sampling or approximate variational inference. However, inference needs to be rerun from scratch for each new set of data. This drawback limits the applicability of the Bayesian formulation to real-time settings, e.g., health monitoring of engineered systems, and medical diagnosis. The objective of this paper is to develop a methodology that enables real-time inference by learning the Bayesian inverse map, i.e., the map from data to posteriors. Our approach is as follows. We represent the posterior distribution using a parameterization based on deep neural networks. Next, we learn the network parameters by amortized variational inferenc
    
[^35]: 基于矩阵自回归的联邦学习拜占庭容错聚合方案

    A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])

    [http://arxiv.org/abs/2303.16668](http://arxiv.org/abs/2303.16668)

    本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。

    

    本文提出了FLANDERS，一种新颖的联邦学习（FL）聚合方案，可以抵御拜占庭攻击。FLANDERS将每个FL轮次中由客户端发送的本地模型更新视为矩阵值时间序列。然后，通过将实际观测与由矩阵自回归预测模型估计的观测进行比较，识别恶意客户端作为这个时间序列的异常值。在不同FL设置下对多个数据集进行的实验证明，FLANDERS在抵御拜占庭攻击方面与最强大的基线相匹配。此外，与现有的防御策略相比， FLANDERS即使在极其严重的攻击场景下仍然非常有效。

    In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
    
[^36]: 在错误标记的网络顶点存在的情况下进行功耗测试

    Lost in the Shuffle: Testing Power in the Presence of Errorful Network Vertex Labels. (arXiv:2208.08638v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.08638](http://arxiv.org/abs/2208.08638)

    研究了当网络中存在不匹配/标签混乱的顶点时，两个样本图假设检验中的功率损失，并通过多个实验加以验证。

    

    许多两个样本的网络假设检验方法都是在顶点对应在网络之间的隐含假设下运行的。在本文中，我们考虑了当网络中存在不匹配/标签混乱的顶点时，两个样本图假设检验中的功率损失。在随机点乘积和随机块模型网络的背景下，我们在理论上探讨了由于混洗对基于估计的边缘概率矩阵或邻接矩阵之间的Frobenius范数差异的一对假设检验的功率损失。功耗测试的损失通过众多模拟和实验进一步加强，在文献中比较了多个最近提出的测试中的功率损失，在随机块模型和随机点乘积图模型中均有体现。最后，我们通过来自神经科学和社交网络分析的两个例子展示了混洗可能对真实数据测试的影响。

    Many two-sample network hypothesis testing methodologies operate under the implicit assumption that the vertex correspondence across networks is a priori known. In this paper, we consider the degradation of power in two-sample graph hypothesis testing when there are misaligned/label-shuffled vertices across networks. In the context of random dot product and stochastic block model networks, we theoretically explore the power loss due to shuffling for a pair of hypothesis tests based on Frobenius norm differences between estimated edge probability matrices or between adjacency matrices. The loss in testing power is further reinforced by numerous simulations and experiments, both in the stochastic block model and in the random dot product graph model, where we compare the power loss across multiple recently proposed tests in the literature. Lastly, we demonstrate the impact that shuffling can have in real-data testing in a pair of examples from neuroscience and from social network analysi
    
[^37]: 《长话短说：因果机器学习中的遗漏变量偏差》

    Long Story Short: Omitted Variable Bias in Causal Machine Learning. (arXiv:2112.13398v4 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2112.13398](http://arxiv.org/abs/2112.13398)

    在因果机器学习中，我们通过推导出遗漏变量偏差的尖锐上界，为广泛的线性泛函因果参数提供了一种简单而通用的方法。这种方法可以应用于许多传统的因果推断研究目标，并且仅取决于潜变量在结果和参数的Riesz表示器中所导致的额外变异。

    

    我们推导了一类广泛的因果参数的遗漏变量偏差的一般但简单的尖锐上界，这些参数可以被认定为结果的条件期望函数的线性泛函。这样的泛函包括许多因果推断研究中的传统调查目标，例如（加权）潜在结果的平均值、平均处理效应（包括子组效应，如对待处理对象的影响）、（加权）平均导数和来自协变量分布变化的策略效应 - 全部适用于一般的非参数因果模型。我们的构造依赖于目标泛函的Riesz-Fréchet表示。具体来说，我们展示了偏差上界仅取决于潜变量在结果和感兴趣参数的Riesz表示器中所创建的附加变化。此外，在许多重要情况下（例如平均处理效应和平均导数）

    We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects and avearage derivative
    

