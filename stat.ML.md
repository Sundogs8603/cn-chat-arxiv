# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Chordal Averaging on Flag Manifolds and Its Applications.](http://arxiv.org/abs/2303.13501) | 本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。 |
| [^2] | [Generalization with quantum geometry for learning unitaries.](http://arxiv.org/abs/2303.13462) | 本文研究了量子机器学习模型的泛化能力，使用数据的量子费舍尔信息度量来评估成功训练和泛化所需的电路参数和训练数据的数量，并展示通过去除对称性来提高泛化能力，同时发现超出分布泛化能力可以比使用相同分布更优。 |
| [^3] | [Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation.](http://arxiv.org/abs/2303.13429) | 本文提出了一种相互作用粒子 Langevin 算法，用于最大边缘似然估计。使用此算法，估计器的优化误差具有非渐近浓度界限。 |
| [^4] | [Logistic Regression Equivalence: A Framework for Comparing Logistic Regression Models Across Populations.](http://arxiv.org/abs/2303.13330) | 本论文提出了一种逻辑回归模型比较的方法，旨在评估不同亚群中拟合模型之间的差异，通过等价性测试来激励推断的准确性。 |
| [^5] | [Deep Generative Multi-Agent Imitation Model as a Computational Benchmark for Evaluating Human Performance in Complex Interactive Tasks: A Case Study in Football.](http://arxiv.org/abs/2303.13323) | 该论文使用基于生成模型的 AI 智能体作为计算基准，以评估人类在困难的涉及多个人类和情境因素的任务中的表现及发现有待改进的领域。该论文以足球表现分析为例，使用大型球员和球位置跟踪数据集训练生成模型，并成功进行足球比赛中的交互模仿。 |
| [^6] | [Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees.](http://arxiv.org/abs/2303.13228) | 本文提出了一种算法来丰富神经网络训练数据集，从而减少最坏情况的违规，并提高其性能保证。 |
| [^7] | [A Simple Explanation for the Phase Transition in Large Language Models with List Decoding.](http://arxiv.org/abs/2303.13112) | 本文提供了一个对大语言模型中相变现象的简单解释，利用列表译码器建模，它能够保证在LLM低于临界阈值时错误候选序列数的期望保持有界，而在高于该阈值时呈指数增长。 |
| [^8] | [Preference-Aware Constrained Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2303.13034) | PAC-MOO是一个偏好感知的约束多目标贝叶斯优化方法，可以有效地解决在黑盒目标函数和从业者指定的目标偏好下，大部分输入空间是不可行的约束多目标优化问题。 |
| [^9] | [Continuous Indeterminate Probability Neural Network.](http://arxiv.org/abs/2303.12964) | 本文提出了CIPNN模型，该模型能够推导出连续潜在随机变量的解析解，同时提出了CIPAE自编码器，并通过可视化潜在随机变量的方法验证了模型的有效性。 |
| [^10] | [Reinforcement Learning with Exogenous States and Rewards.](http://arxiv.org/abs/2303.12957) | 该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。 |
| [^11] | [Generalized Data Thinning Using Sufficient Statistics.](http://arxiv.org/abs/2303.12931) | 本研究发展了一种基于充分统计量的通用策略，通过松弛求和要求并仅要求函数重构随机变量X，进一步推广了数据稀化方法，扩展了可进行稀化的分布族，并统一了样本分裂和数据稀化。 |
| [^12] | [Revisiting the Fragility of Influence Functions.](http://arxiv.org/abs/2303.12922) | 本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。 |
| [^13] | [Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues.](http://arxiv.org/abs/2303.12878) | 该论文介绍了排序数据分析中的鲁棒性共识问题以及该问题相关的统计方法，其中Consensus Ranking问题是重点，旨在通过中位数排名来总结排列的概率分布。 |
| [^14] | [The power and limitations of learning quantum dynamics incoherently.](http://arxiv.org/abs/2303.12834) | 本文证明在不相干框架下，通过浅层测量只能有效地学习低纠缠门。虽然该类框架为我们在不同物理平台之间转移量子过程提供了方法，但也存在一定的局限性。 |
| [^15] | [Fixed points of arbitrarily deep 1-dimensional neural networks.](http://arxiv.org/abs/2303.12814) | 本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。 |
| [^16] | [Non-asymptotic analysis of Langevin-type Monte Carlo algorithms.](http://arxiv.org/abs/2303.12407) | 本文提出了一种新的Langevin型算法并应用于吉布斯分布。通过提出的2-Wasserstein距离上限，我们发现势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性可以保证算法具有接近零的误差。新的Langevin型算法还可以应用于无凸性或连续可微性的势函数。 |
| [^17] | [Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators.](http://arxiv.org/abs/2303.08431) | 本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。 |
| [^18] | [The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference.](http://arxiv.org/abs/2302.09163) | 研究分析因子化高斯逼近在变分推断中的应用，发现该方法低估所逼近分布的不确定性。特别地，当用对角协方差矩阵的高斯逼近具有密集协方差矩阵的高斯时，所推断的高斯总是低估了原始高斯的分量方差和熵。 |
| [^19] | [Sliced Optimal Partial Transport.](http://arxiv.org/abs/2212.08049) | 本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。 |
| [^20] | [Omnigrok: Grokking Beyond Algorithmic Data.](http://arxiv.org/abs/2210.01117) | 本文通过分析神经网络的损失景观，发现训练和测试损失之间的不匹配是grokking的原因，提出了“LU机制”，并成功诱导了算法数据集的grokking和消除了其grokking现象。它们的dramatic grokking依赖于表示学习。 |
| [^21] | [Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes.](http://arxiv.org/abs/2110.15332) | 本文提出了一种方法，可以在离线强化学习中，应用于从医疗保健或教育领域收集的观测数据。我们考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估，以解决观察到的行动可能受到未观察到的因素影响，导致估计值出现偏差的问题。 |
| [^22] | [Nearest neighbor process: weak convergence and non-asymptotic bound.](http://arxiv.org/abs/2110.15083) | 本文介绍了一种中心统计量——最近邻测度，并通过均匀中心极限定理和一种均匀的非渐近界限研究了它。该测度可能为推断提供了一种替代方法。 |
| [^23] | [Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments.](http://arxiv.org/abs/2012.10315) | 该论文提出了一种使用负对照、代理变量和工具变量的核方法，以识别治疗效果并学习非参数治疗效果。 作者证明了算法的一致性和收敛速度，并估计了香烟吸烟的剂量反应曲线。 |
| [^24] | [The Variational Method of Moments.](http://arxiv.org/abs/2012.09422) | 本文提出了一个非常通用的条件矩问题估计器类 - 变分矩方法，使得我们能够控制无限数量的矩，并提供了基于核方法和神经网络的多个VMM估计器的理论分析和证明。 |
| [^25] | [Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity.](http://arxiv.org/abs/2004.12908) | 本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。 |

# 详细

[^1]: 旗型流形上的弦均值及其应用

    Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])

    [http://arxiv.org/abs/2303.13501](http://arxiv.org/abs/2303.13501)

    本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。

    

    本文提出了一种新的、可证明收敛的算法，用于在弦度量下计算旗型流形上一组点的旗形均值和旗形中值。旗型流形是一种数学空间，由嵌套的向量空间子空间序列组成，并且在维度上逐渐增加。旗型流形是已知的许多矩阵群的超集，包括Stiefel和Grassmanians，使其成为在各种计算机视觉问题中非常有用的通用对象。为了解决计算一阶旗帜统计数据的挑战，我们首先将问题转化为涉及辅助变量受Stiefel流形约束的问题。Stiefel流形是一组正交框架的空间，利用Stiefel流形优化的数值稳定性和效率，可以有效地计算旗形均值。通过一系列实验证明了我们的方法在Grassmann和旋转均值以及主成分问题中的有效性。

    This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
    
[^2]: 利用量子几何进行学习幺正变换的泛化

    Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])

    [http://arxiv.org/abs/2303.13462](http://arxiv.org/abs/2303.13462)

    本文研究了量子机器学习模型的泛化能力，使用数据的量子费舍尔信息度量来评估成功训练和泛化所需的电路参数和训练数据的数量，并展示通过去除对称性来提高泛化能力，同时发现超出分布泛化能力可以比使用相同分布更优。

    

    泛化是量子机器学习模型从训练数据学习准确预测新数据的能力。在这里，我们引入数据的量子费舍尔信息度量(DQFIM)来确定模型何时能够泛化。对于幺正变换的可变学习，DQFIM量化了成功训练和泛化所需的电路参数和训练数据的数量。我们应用DQFIM来解释何时恒定数量的训练状态和多项式数量的参数足以实现泛化。此外，通过从训练数据中删除对称性，可以提高泛化能力。最后，我们显示，使用不同数据分布进行训练和测试的超出分布泛化能力可以比使用相同分布的能力更优。我们的研究为提高量子机器学习中的泛化能力开辟了新的方法。

    Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
    
[^3]: 最大边缘似然估计的相互作用粒子 Langevin 算法

    Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation. (arXiv:2303.13429v1 [stat.CO])

    [http://arxiv.org/abs/2303.13429](http://arxiv.org/abs/2303.13429)

    本文提出了一种相互作用粒子 Langevin 算法，用于最大边缘似然估计。使用此算法，估计器的优化误差具有非渐近浓度界限。

    

    本文研究了一类相互作用粒子系统，用于实现潜变量模型参数的最大边缘似然估计过程。为此，我们提出了一种连续时间相互作用粒子系统，它可以被看作是在扩展的状态空间上的 Langevin漂移，其中在经典的优化中，粒子数量作为相反温度参数。使用Langevin漂移，我们证明了最大边缘似然估计器的优化误差的非渐近浓度界限，这些界限与粒子系统中的粒子数量，算法的迭代次数以及时间离散化分析的步长参数有关。

    We study a class of interacting particle systems for implementing a marginal maximum likelihood estimation (MLE) procedure to optimize over the parameters of a latent variable model. To do so, we propose a continuous-time interacting particle system which can be seen as a Langevin diffusion over an extended state space, where the number of particles acts as the inverse temperature parameter in classical settings for optimisation. Using Langevin diffusions, we prove nonasymptotic concentration bounds for the optimisation error of the maximum marginal likelihood estimator in terms of the number of particles in the particle system, the number of iterations of the algorithm, and the step-size parameter for the time discretisation analysis.
    
[^4]: 逻辑回归等价性:一种比较不同族群下逻辑回归模型的框架

    Logistic Regression Equivalence: A Framework for Comparing Logistic Regression Models Across Populations. (arXiv:2303.13330v1 [stat.ME])

    [http://arxiv.org/abs/2303.13330](http://arxiv.org/abs/2303.13330)

    本论文提出了一种逻辑回归模型比较的方法，旨在评估不同亚群中拟合模型之间的差异，通过等价性测试来激励推断的准确性。

    

    本文讨论如何评估不同亚群中拟合逻辑回归模型之间的差异。我们以研究学习障碍的计算机诊断为例，其中以性别为基础的亚群可能需要分别建立模型。在这种情况下，对于零差异假设的显著性检验可能会产生逆向激励，因为较大的方差和较小的样本会增加不拒绝零假设的概率。我们认为，在预先设定的容差水平上进行等价性测试可激励推断的准确性。我们设计了一组级联的等价性测试，每个测试都涉及模型的不同方面：现象在回归系数中的编码方式、每个样本中的单独预测和平均平方预测误差中的总体准确性。针对每个等价性测试，我们提出了一种设置最小效应量阈值的策略。

    In this paper we discuss how to evaluate the differences between fitted logistic regression models across sub-populations. Our motivating example is in studying computerized diagnosis for learning disabilities, where sub-populations based on gender may or may not require separate models. In this context, significance tests for hypotheses of no difference between populations may provide perverse incentives, as larger variances and smaller samples increase the probability of not-rejecting the null. We argue that equivalence testing for a prespecified tolerance level on population differences incentivizes accuracy in the inference. We develop a cascading set of equivalence tests, in which each test addresses a different aspect of the model: the way the phenomenon is coded in the regression coefficients, the individual predictions in the per example log odds ratio and the overall accuracy in the mean square prediction error. For each equivalence test, we propose a strategy for setting the 
    
[^5]: 复杂互动任务中的人类表现评估：一个基于深度生成多智能体模型的计算基准方法案例研究，以足球为例

    Deep Generative Multi-Agent Imitation Model as a Computational Benchmark for Evaluating Human Performance in Complex Interactive Tasks: A Case Study in Football. (arXiv:2303.13323v1 [stat.ML])

    [http://arxiv.org/abs/2303.13323](http://arxiv.org/abs/2303.13323)

    该论文使用基于生成模型的 AI 智能体作为计算基准，以评估人类在困难的涉及多个人类和情境因素的任务中的表现及发现有待改进的领域。该论文以足球表现分析为例，使用大型球员和球位置跟踪数据集训练生成模型，并成功进行足球比赛中的交互模仿。

    

    在许多应用中，如工程和体育中，评估人类的表现是常见的需求。评估人类在完成复杂的互动任务中的表现，最常见的方法是使用已被证明在该情境下有效的度量标准，或使用主观测量技术。然而，这可能是一个容易出错和不可靠的过程，因为静态度量标准无法捕捉到与这些任务相关的所有复杂情境，并且主观测量存在偏差。我们的研究的目标是创建基于数据驱动的 AI 智能体，作为计算基准来评估人类在解决涉及多个人类和情境因素的困难任务中的表现。我们在足球表现分析的背景下进行了演示。我们在大型球员和球位置跟踪数据集上训练了基于 Conditional Variational Recurrent Neural Network（VRNN）模型的生成模型。训练后的模型用于模仿两个团队在足球比赛中的交互。我们的结果表明，生成模型能够模拟比赛中逼真的球员移动和团队之间的交互。这个模型可以用作评估人类足球表现的基准，并确定需要提高的领域。

    Evaluating the performance of human is a common need across many applications, such as in engineering and sports. When evaluating human performance in completing complex and interactive tasks, the most common way is to use a metric having been proved efficient for that context, or to use subjective measurement techniques. However, this can be an error prone and unreliable process since static metrics cannot capture all the complex contexts associated with such tasks and biases exist in subjective measurement. The objective of our research is to create data-driven AI agents as computational benchmarks to evaluate human performance in solving difficult tasks involving multiple humans and contextual factors. We demonstrate this within the context of football performance analysis. We train a generative model based on Conditional Variational Recurrent Neural Network (VRNN) Model on a large player and ball tracking dataset. The trained model is used to imitate the interactions between two te
    
[^6]: 丰富神经网络训练数据集以提高最坏情况性能保证

    Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees. (arXiv:2303.13228v1 [cs.LG])

    [http://arxiv.org/abs/2303.13228](http://arxiv.org/abs/2303.13228)

    本文提出了一种算法来丰富神经网络训练数据集，从而减少最坏情况的违规，并提高其性能保证。

    

    机器学习算法，特别是神经网络（NNs），是用于近似非线性关系（例如AC-OPF）的有价值的工具，并在部署时实现几个数量级的加速。通常在电力系统文献中，神经网络是通过在训练过程之前生成的固定数据集进行训练的。本文证明，在训练过程中调整神经网络训练数据集可以提高神经网络的性能，并大幅减少其最坏情况违规。本文提出了一个算法，用于识别和丰富关键数据点的训练数据集，以减少最坏情况违规，提供具有改进最坏情况性能保证的神经网络。我们在四个测试电力系统中演示了我们算法的性能，范围从39个总线到162个总线。

    Machine learning algorithms, especially Neural Networks (NNs), are a valuable tool used to approximate non-linear relationships, like the AC-Optimal Power Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several orders of magnitude when deployed for use. Often in power systems literature, the NNs are trained with a fixed dataset generated prior to the training process. In this paper, we show that adapting the NN training dataset during training can improve the NN performance and substantially reduce its worst-case violations. This paper proposes an algorithm that identifies and enriches the training dataset with critical datapoints that reduce the worst-case violations and deliver a neural network with improved worst-case performance guarantees. We demonstrate the performance of our algorithm in four test power systems, ranging from 39-buses to 162-buses.
    
[^7]: 利用列表译码解释大语言模型中的相变现象

    A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])

    [http://arxiv.org/abs/2303.13112](http://arxiv.org/abs/2303.13112)

    本文提供了一个对大语言模型中相变现象的简单解释，利用列表译码器建模，它能够保证在LLM低于临界阈值时错误候选序列数的期望保持有界，而在高于该阈值时呈指数增长。

    

    最近的实验结果表明，大语言模型（LLM）呈现出小模型所没有的突出能力。当模型达到一定的规模关键点时，系统性能得到了极大地提高。在这篇文章中，我们提供了一个简单的解释，并将LLM建模为一个序列到序列的随机函数。我们使用列表译码器代替每个步骤的即时生成，该译码器在每个步骤保留一个候选序列列表，并在结束时推迟输出序列的生成。我们表明，存在一个临界阈值，当LLM低于此阈值时，期望的错误候选序列数保持有界，当LLM高于此阈值时，期望错误序列数呈指数增长。这样的阈值与传染病的基本繁殖数有关。

    Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function. Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of erroneous candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.
    
[^8]: 偏好感知的约束多目标贝叶斯优化

    Preference-Aware Constrained Multi-Objective Bayesian Optimization. (arXiv:2303.13034v1 [cs.LG])

    [http://arxiv.org/abs/2303.13034](http://arxiv.org/abs/2303.13034)

    PAC-MOO是一个偏好感知的约束多目标贝叶斯优化方法，可以有效地解决在黑盒目标函数和从业者指定的目标偏好下，大部分输入空间是不可行的约束多目标优化问题。

    

    本文解决了在大部分输入空间是不可行（即违反约束条件）时，基于黑盒目标函数和从业者指定的目标偏好的约束多目标优化问题。这个问题在许多工程设计问题中都存在，包括模拟电路和电力系统设计。我们的总体目标是在可行的输入设计的小部分上近似最优Pareto集合。主要挑战包括设计空间的巨大大小、多个目标和大量的约束条件以及只能在进行昂贵的仿真后才能确认的可行的输入设计的小部分。我们提出了一种新颖而有效的偏好感知的约束多目标贝叶斯优化方法（PAC-MOO）来解决这些挑战。关键思想是学习输出目标和约束的代理模型，并根据从业者预测的偏好选择评估目标的候选输入。PAC-MOO根据预测的目标和约束的联合偏好迭代地选择下一个要模拟的输入设计，并使用新获得的数据更新代理模型。实验结果表明，与现有的最先进方法相比，所提出的方法具有有效性和效率。

    This paper addresses the problem of constrained multi-objective optimization over black-box objective functions with practitioner-specified preferences over the objectives when a large fraction of the input space is infeasible (i.e., violates constraints). This problem arises in many engineering design problems including analog circuits and electric power system design. Our overall goal is to approximate the optimal Pareto set over the small fraction of feasible input designs. The key challenges include the huge size of the design space, multiple objectives and large number of constraints, and the small fraction of feasible input designs which can be identified only after performing expensive simulations. We propose a novel and efficient preference-aware constrained multi-objective Bayesian optimization approach referred to as PAC-MOO to address these challenges. The key idea is to learn surrogate models for both output objectives and constraints, and select the candidate input for eva
    
[^9]: 连续不定概率神经网络

    Continuous Indeterminate Probability Neural Network. (arXiv:2303.12964v1 [cs.LG])

    [http://arxiv.org/abs/2303.12964](http://arxiv.org/abs/2303.12964)

    本文提出了CIPNN模型，该模型能够推导出连续潜在随机变量的解析解，同时提出了CIPAE自编码器，并通过可视化潜在随机变量的方法验证了模型的有效性。

    

    本文介绍了一种称为CIPNN（Continuous Indeterminate Probability Neural Network）的通用模型，该模型基于IPNN，用于离散潜在随机变量。目前，连续潜在变量的后验被认为是不可计算的，但是IPNN提出了新的理论，可以解决这个问题。本文的贡献有四个方面。首先，我们推导了连续潜在随机变量的后验计算的解析解，并提出了一个通用分类模型（CIPNN）。其次，我们提出了一种通用的自编码器——CIPAE（Continuous Indeterminate Probability Auto-Encoder），其中解码器部分不是神经网络，而是第一次使用全概率推理模型。第三，我们提出了一种新的可视化潜在随机变量的方法。我们使用N维潜在变量之一作为解码器来重建输入图像，即使是分类任务也能达到效果，这样，我们可以看到每个潜在变量代表什么。第四，我们通过MNIST和Fashion-MNIST数据集的实验证明了所提出模型的有效性。

    This paper introduces a general model called CIPNN - Continuous Indeterminate Probability Neural Network, and this model is based on IPNN, which is used for discrete latent random variables. Currently, posterior of continuous latent variables is regarded as intractable, with the new theory proposed by IPNN this problem can be solved. Our contributions are Four-fold. First, we derive the analytical solution of the posterior calculation of continuous latent random variables and propose a general classification model (CIPNN). Second, we propose a general auto-encoder called CIPAE - Continuous Indeterminate Probability Auto-Encoder, the decoder part is not a neural network and uses a fully probabilistic inference model for the first time. Third, we propose a new method to visualize the latent random variables, we use one of N dimensional latent variables as a decoder to reconstruct the input image, which can work even for classification tasks, in this way, we can see what each latent varia
    
[^10]: 具有外部状态和奖励的强化学习

    Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])

    [http://arxiv.org/abs/2303.12957](http://arxiv.org/abs/2303.12957)

    该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。

    

    外部状态变量和奖励会通过向奖励信号注入不可控的变化而减慢强化学习的速度。本文对外部状态变量和奖励进行了正式化，并表明如果奖励函数加法分解成内生和外生两个部分，MDP可以分解为一个外生马尔可夫奖励过程（基于外部奖励）和一个内生马尔可夫决策过程（优化内生奖励）。内生MDP的任何最优策略也是原始MDP的最优策略，但由于内生奖励通常具有降低的方差，因此内生MDP更容易求解。我们研究了状态空间分解为内外生状态空间的情况，而这种状态空间分解并没有给出，而是必须发现。本文介绍并证明了在线性组合下发现内生和外生状态空间的算法的正确性。

    Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms
    
[^11]: 基于充分统计量的广义数据稀化

    Generalized Data Thinning Using Sufficient Statistics. (arXiv:2303.12931v1 [stat.ME])

    [http://arxiv.org/abs/2303.12931](http://arxiv.org/abs/2303.12931)

    本研究发展了一种基于充分统计量的通用策略，通过松弛求和要求并仅要求函数重构随机变量X，进一步推广了数据稀化方法，扩展了可进行稀化的分布族，并统一了样本分裂和数据稀化。

    

    本文旨在开发一种将随机变量X分解为多个独立随机变量的通用策略，而不会丢失任何有关未知参数的信息。我们通过松弛求和要求并仅要求一些已知的独立随机变量的函数完全重构X来推广了最近一篇论文的过程。该过程的推广有两个目的。首先，它极大地扩展了可进行稀化的分布族。其次，它统一了样本分裂和数据稀化，它们在表面上似乎非常不同，但应用了同样的原理。这个共同的原理是充分性。我们利用这一认识对各种不同的家族进行广义稀疏化操作。

    Our goal is to develop a general strategy to decompose a random variable $X$ into multiple independent random variables, without sacrificing any information about unknown parameters. A recent paper showed that for some well-known natural exponential families, $X$ can be "thinned" into independent random variables $X^{(1)}, \ldots, X^{(K)}$, such that $X = \sum_{k=1}^K X^{(k)}$. In this paper, we generalize their procedure by relaxing this summation requirement and simply asking that some known function of the independent random variables exactly reconstruct $X$. This generalization of the procedure serves two purposes. First, it greatly expands the families of distributions for which thinning can be performed. Second, it unifies sample splitting and data thinning, which on the surface seem to be very different, as applications of the same principle. This shared principle is sufficiency. We use this insight to perform generalized thinning operations for a diverse set of families.
    
[^12]: 重新审视影响函数的脆弱性

    Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])

    [http://arxiv.org/abs/2303.12922](http://arxiv.org/abs/2303.12922)

    本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。

    

    最近几年有很多论文致力于解释深度学习模型的预测。然而，很少有方法被提出来验证这些解释的准确性或可信度。最近，影响函数被证明是一种评估深度神经网络在单个样本上的灵敏度的方法。但是，先前的研究表明影响函数易受噪声和数据分布不对称性影响，缺乏鲁棒性。本文旨在研究影响函数的脆弱性，通过探究影响函数背后的机理，从而为增强影响函数的鲁棒性提供新思路。

    In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
    
[^13]: 排序数据分析中的鲁棒性共识：定义、属性和计算问题

    Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues. (arXiv:2303.12878v1 [cs.LG])

    [http://arxiv.org/abs/2303.12878](http://arxiv.org/abs/2303.12878)

    该论文介绍了排序数据分析中的鲁棒性共识问题以及该问题相关的统计方法，其中Consensus Ranking问题是重点，旨在通过中位数排名来总结排列的概率分布。

    

    随着人工智能系统中鲁棒性问题日益突出，需要开发可靠的统计学习技术，即使在部分受损数据的情况下也能确保可靠性。偏好数据以 (完整) 排序的形式出现时也不例外，尤其是饱受此类数据支持或产生的技术(例如，搜索引擎，推荐系统)大规模部署之时，需要相应的概念和工具。然而，由于排列的集合 (即对称群 $\mathfrak{S}_n$) 没有向量空间结构，且排序数据分析中考虑的统计量的复杂性，使得在该领域中制定鲁棒性目标具有挑战性。在本文中，我们引入了鲁棒性概念以及专用的统计方法，针对排名数据分析中的旗舰问题：Consensus Ranking，旨在通过中位数排名来总结排列的概率分布。

    As the issue of robustness in AI systems becomes vital, statistical learning techniques that are reliable even in presence of partly contaminated data have to be developed. Preference data, in the form of (complete) rankings in the simplest situations, are no exception and the demand for appropriate concepts and tools is all the more pressing given that technologies fed by or producing this type of data (e.g. search engines, recommending systems) are now massively deployed. However, the lack of vector space structure for the set of rankings (i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of statistics considered in ranking data analysis make the formulation of robustness objectives in this domain challenging. In this paper, we introduce notions of robustness, together with dedicated statistical methods, for Consensus Ranking the flagship problem in ranking data analysis, aiming at summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking. Precise
    
[^14]: 量子动力学的学习的能力与局限性

    The power and limitations of learning quantum dynamics incoherently. (arXiv:2303.12834v1 [quant-ph])

    [http://arxiv.org/abs/2303.12834](http://arxiv.org/abs/2303.12834)

    本文证明在不相干框架下，通过浅层测量只能有效地学习低纠缠门。虽然该类框架为我们在不同物理平台之间转移量子过程提供了方法，但也存在一定的局限性。

    

    量子过程学习是研究量子系统的重要工具之一。然而大部分研究都放在了自旋相干和器件自耦合的波动函数上，研究量子动力学在系统和目标不直接交互的情况下是否可以被学习并没有得到足够的关注。这类不相干的框架实际上非常吸引人，因为它们能够在不需要挑战性的混合纠缠方案中，为我们提供在不同物理平台之间转移量子过程的方法。在本文中，我们通过分析需要仿真的明确的相干学习策略的测量次数，提供了在不相干框架下学习幺正过程样本复杂度的界限。我们证明，如果允许任意测量，则任何有效表示的幺正矩阵都可以在不相干框架内被有效地学习；然而，如果仅限于浅层测量，则只能有效地学习低纠缠门。因此，我们的工作突出了学习量子动力学在不相干框架中的能力与局限性。

    Quantum process learning is emerging as an important tool to study quantum systems. While studied extensively in coherent frameworks, where the target and model system can share quantum information, less attention has been paid to whether the dynamics of quantum systems can be learned without the system and target directly interacting. Such incoherent frameworks are practically appealing since they open up methods of transpiling quantum processes between the different physical platforms without the need for technically challenging hybrid entanglement schemes. Here we provide bounds on the sample complexity of learning unitary processes incoherently by analyzing the number of measurements that are required to emulate well-established coherent learning strategies. We prove that if arbitrary measurements are allowed, then any efficiently representable unitary can be efficiently learned within the incoherent framework; however, when restricted to shallow-depth measurements only low-entangl
    
[^15]: 任意深度的一维神经网络的不动点

    Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])

    [http://arxiv.org/abs/2303.12814](http://arxiv.org/abs/2303.12814)

    本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    

    本文介绍了一个在$\mathbb{R}$上具有合成性且包含对数S型函数的新函数类。我们使用这个类来证明具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点。虽然这样的神经网络远离实际应用，但我们能够完全理解它们的不动点，并为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
    
[^16]: Langevin型Monte Carlo算法的非渐进分析

    Non-asymptotic analysis of Langevin-type Monte Carlo algorithms. (arXiv:2303.12407v1 [math.ST])

    [http://arxiv.org/abs/2303.12407](http://arxiv.org/abs/2303.12407)

    本文提出了一种新的Langevin型算法并应用于吉布斯分布。通过提出的2-Wasserstein距离上限，我们发现势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性可以保证算法具有接近零的误差。新的Langevin型算法还可以应用于无凸性或连续可微性的势函数。

    

    本文研究了Langevin型算法应用于吉布斯分布的情况，其中势函数是耗散的，且其弱梯度具有有限的连续性模量。我们的主要结果是2-Wasserstein距离上限的非渐进性，它衡量了吉布斯分布与基于Liptser-Shiryaev理论和函数不等式的Langevin型算法的一般分布之间的距离。我们应用这个上限来展示势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性是充分的，可以通过适当控制参数来获得Langevin Monte Carlo算法的收敛性。我们还针对无凸性或连续可微性的势函数提出了球形平滑技术的Langevin型算法。

    We study the Langevin-type algorithms for Gibbs distributions such that the potentials are dissipative and their weak gradients have the finite moduli of continuity. Our main result is a non-asymptotic upper bound of the 2-Wasserstein distance between the Gibbs distribution and the law of general Langevin-type algorithms based on the Liptser--Shiryaev theory and functional inequalities. We apply this bound to show that the dissipativity of the potential and the $\alpha$-H\"{o}lder continuity of the gradient with $\alpha>1/3$ are sufficient for the convergence of the Langevin Monte Carlo algorithm with appropriate control of the parameters. We also propose Langevin-type algorithms with spherical smoothing for potentials without convexity or continuous differentiability.
    
[^17]: 政策梯度算法收敛于几乎线性二次型调节器的全局最优策略

    Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])

    [http://arxiv.org/abs/2303.08431](http://arxiv.org/abs/2303.08431)

    本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。

    

    决策者只获得了非完整信息的非线性控制系统在各种应用中普遍存在。本研究探索了强化学习方法，以找到几乎线性二次型调节器系统中最优策略。我们考虑一个动态系统，结合线性和非线性组成部分，并由相同结构的策略进行管理。在假设非线性组成部分包含具有小型Lipschitz系数的内核的情况下，我们对成本函数的优化进行了表征。虽然成本函数通常是非凸的，但我们确立了全局最优解附近局部的强凸性和光滑性。此外，我们提出了一种初始化机制，以利用这些属性。在此基础上，我们设计了一个策略梯度算法，可以保证以线性速率收敛于全局最优解。

    Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
    
[^18]: 收缩-解耦平衡：分析因子化高斯逼近在变分推断中的应用

    The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference. (arXiv:2302.09163v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09163](http://arxiv.org/abs/2302.09163)

    研究分析因子化高斯逼近在变分推断中的应用，发现该方法低估所逼近分布的不确定性。特别地，当用对角协方差矩阵的高斯逼近具有密集协方差矩阵的高斯时，所推断的高斯总是低估了原始高斯的分量方差和熵。

    

    当变分推断（VI）使用因子化逼近时，它们往往会低估它们用来逼近的分布的不确定性，如以各种方式测量。我们考虑两种衡量VI不确定性亏损的流行方法：（i）它低估分量方差的程度，（ii）它低估熵的程度。为了更好地理解这些影响以及它们之间的关系，我们考虑了一个信息丰富的设置，可以在其中明确（和优雅地）分析这些影响：使用对角协方差矩阵的高斯（$q$）逼近具有密集协方差矩阵的高斯（$p$）。我们证明了$q$总是低估了$p$的分量方差和熵，尽管不一定低估的程度相同。此外，我们证明$q$的熵由两个相互竞争的因素的平衡决定：它的分量方差收缩会降低它的熵。

    When factorized approximations are used for variational inference (VI), they tend to underestimate the uncertainty -- as measured in various ways -- of the distributions they are meant to approximate. We consider two popular ways to measure the uncertainty deficit of VI: (i) the degree to which it underestimates the componentwise variance, and (ii) the degree to which it underestimates the entropy. To better understand these effects, and the relationship between them, we examine an informative setting where they can be explicitly (and elegantly) analyzed: the approximation of a Gaussian,~$p$, with a dense covariance matrix, by a Gaussian,~$q$, with a diagonal covariance matrix. We prove that $q$ always underestimates both the componentwise variance and the entropy of $p$, \textit{though not necessarily to the same degree}. Moreover we demonstrate that the entropy of $q$ is determined by the trade-off of two competing forces: it is decreased by the shrinkage of its componentwise varianc
    
[^19]: 切片最优偏转运输

    Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08049](http://arxiv.org/abs/2212.08049)

    本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。

    

    最优传输（OT）已经在机器学习、数据科学和计算机视觉中变得极其流行。OT问题的核心假设是源和目标测度的总质量相等，这限制了它的应用。最优偏转运输（OPT）是最近提出的解决这个限制的方法。与OT问题类似，OPT的计算依赖于解决线性规划问题（通常在高维度中），这可能会变得计算上困难。在本文中，我们提出了一种计算一维非负测度之间OPT问题的有效算法。接下来，遵循切片OT距离的思想，我们利用切片定义了切片OPT距离。最后，我们展示了切片OPT-based方法在各种数值实验中的计算和精度优势。特别是，我们展示了我们提出的Sliced-OPT在噪声点云配准中的应用。

    Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
    
[^20]: Omnigrok：理解超越算法数据的“Grokking”

    Omnigrok: Grokking Beyond Algorithmic Data. (arXiv:2210.01117v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01117](http://arxiv.org/abs/2210.01117)

    本文通过分析神经网络的损失景观，发现训练和测试损失之间的不匹配是grokking的原因，提出了“LU机制”，并成功诱导了算法数据集的grokking和消除了其grokking现象。它们的dramatic grokking依赖于表示学习。

    

    Grokking是一种不寻常的现象，指算法数据集在过拟合训练数据后长时间仍然能进行泛化，一直以来一直难以理解。本文旨在通过分析神经网络的损失景观来理解grokking，并确定训练和测试损失之间的不匹配是grokking的原因。我们将其称为“LU机制”，因为训练和测试损失（对模型权重规范）通常分别类似于“L”和“U”。这个简单的机制可以很好地解释grokking的许多方面：数据大小依赖性、权重衰减依赖性、表示的出现等。在直觉上给定的基础上，我们能够在涉及图像、语言和分子的任务中诱导grokking。反向来看，我们能够消除算法数据集的grokking。我们将算法数据集的dramatic grokking归因于表示学习。

    Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
    
[^21]: 近端强化学习：部分观测马尔可夫决策过程中高效的离线策略评估

    Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.15332v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15332](http://arxiv.org/abs/2110.15332)

    本文提出了一种方法，可以在离线强化学习中，应用于从医疗保健或教育领域收集的观测数据。我们考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估，以解决观察到的行动可能受到未观察到的因素影响，导致估计值出现偏差的问题。

    

    在从医疗保健或教育领域收集的观察数据应用离线强化学习时，一个普遍的关注点是，观察到的行动可能受到未观察到的因素的影响，引起混淆并导致在假设完美马尔可夫决策过程模型的情况下得出的估计值出现偏差。本文考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估。具体来说，我们考虑在给定只由不同且未知的策略生成的具有部分状态观测的轨迹的情况下估计POMDP中给定目标策略的值，该策略可能依赖于未观察到的状态。我们解决了两个问题：什么条件允许我们从观察到的数据中识别目标策略值，并且在识别的情况下，如何最好地估计它。为了回答这些问题，我们将近端因果推断框架扩展到我们的POMDP设置中，提供了许多场景，其中通过所谓的桥接函数的存在实现了识别。

    In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in a POMDP given trajectories with only partial state observations generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functio
    
[^22]: 最近邻过程：弱收敛和非渐近界限

    Nearest neighbor process: weak convergence and non-asymptotic bound. (arXiv:2110.15083v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2110.15083](http://arxiv.org/abs/2110.15083)

    本文介绍了一种中心统计量——最近邻测度，并通过均匀中心极限定理和一种均匀的非渐近界限研究了它。该测度可能为推断提供了一种替代方法。

    

    介绍并研究了由给定点的最近邻所得到的经验测度——最近邻测度作为一种中心统计量。首先，在底层函数类上满足（反映最近邻算法的本地化特性的）（本地）支撑熵条件下，将相关经验过程证明为满足均匀中心极限定理。其次，在统一熵数的著名条件（通常称为Vapnik-Chervonenkis）下建立了一种均匀的非渐近界限。在均匀中心极限定理中所获得的高斯极限的协方差等于条件协方差算子（给出兴趣点）。这提示了一种可能性，即在使用相同的推理方式但仅使用最近邻而不是全部替换标准经验测度的标准方法的情况下，扩展标准方法 - 非局部。

    The empirical measure resulting from the nearest neighbors to a given point \textit{the nearest neighbor measure} - is introduced and studied as a central statistical quantity. First, the associated empirical process is shown to satisfy a uniform central limit theorem under a (local) bracketing entropy condition on the underlying class of functions (reflecting the localizing nature of the nearest neighbor algorithm). Second a uniform non-asymptotic bound is established under a well-known condition, often referred to as Vapnik-Chervonenkis, on the uniform entropy numbers. The covariance of the Gaussian limit obtained in the uniform central limit theorem is equal to the conditional covariance operator (given the point of interest). This suggests the possibility of extending standard approaches - non local - replacing simply the standard empirical measure by the nearest neighbor measure while using the same way of making inference but with the nearest neighbors only instead of the full 
    
[^23]: 无法观测到的混淆变量的核方法：负对照、代理变量和工具变量

    Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.10315](http://arxiv.org/abs/2012.10315)

    该论文提出了一种使用负对照、代理变量和工具变量的核方法，以识别治疗效果并学习非参数治疗效果。 作者证明了算法的一致性和收敛速度，并估计了香烟吸烟的剂量反应曲线。

    

    负对照是一种在存在未测量混淆的情况下学习治疗与结果之间因果关系的策略。如果有两个辅助变量可用：一个负对照治疗（对实际结果没有影响）和一个负对照结果（不受实际治疗影响），则仍然可以识别治疗效果。 这些辅助变量也可以视为传统控制变量集的代理变量，并且它们类似于工具变量。我提出了一族基于核岭回归的算法，在负对照下学习非参数治疗效果。示例包括剂量反应曲线、具有分布偏移的剂量反应曲线和异质性治疗效果。 数据可以是离散、连续和低维、高维或无限维。我证明了均匀一致性并提供了有限样本收敛率。 我估计了香烟吸烟剂量反应曲线。

    Negative control is a strategy for learning the causal relationship between treatment and outcome in the presence of unmeasured confounding. The treatment effect can nonetheless be identified if two auxiliary variables are available: a negative control treatment (which has no effect on the actual outcome), and a negative control outcome (which is not affected by the actual treatment). These auxiliary variables can also be viewed as proxies for a traditional set of control variables, and they bear resemblance to instrumental variables. I propose a family of algorithms based on kernel ridge regression for learning nonparametric treatment effects with negative controls. Examples include dose response curves, dose response curves with distribution shift, and heterogeneous treatment effects. Data may be discrete or continuous, and low, high, or infinite dimensional. I prove uniform consistency and provide finite sample rates of convergence. I estimate the dose response curve of cigarette sm
    
[^24]: 条件矩问题的变分矩方法

    The Variational Method of Moments. (arXiv:2012.09422v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.09422](http://arxiv.org/abs/2012.09422)

    本文提出了一个非常通用的条件矩问题估计器类 - 变分矩方法，使得我们能够控制无限数量的矩，并提供了基于核方法和神经网络的多个VMM估计器的理论分析和证明。

    

    条件矩问题是描述结构性因果参数的有力形式化工具。一个常见的方法是将问题转化为有限组的边际矩条件，并应用最优加权广义矩法（OWGMM）。本文提出了一个称之为变分矩方法（VMM）的非常通用的条件矩问题估计器类，并自然地使我们能够控制无限数量的矩。作者对多个VMM估计器进行了详细的理论分析，包括基于核方法和神经网络的估计器，并提供了这些方法在一定条件下一致估计真实因果参数的证明。

    The conditional moment problem is a powerful formulation for describing structural causal parameters in terms of observables, a prominent example being instrumental variable regression. A standard approach reduces the problem to a finite set of marginal moment conditions and applies the optimally weighted generalized method of moments (OWGMM), but this requires we know a finite set of identifying moments, can still be inefficient even if identifying, or can be theoretically efficient but practically unwieldy if we use a growing sieve of moment conditions. Motivated by a variational minimax reformulation of OWGMM, we define a very general class of estimators for the conditional moment problem, which we term the variational method of moments (VMM) and which naturally enables controlling infinitely-many moments. We provide a detailed theoretical analysis of multiple VMM estimators, including ones based on kernel methods and neural nets, and provide conditions under which these are consist
    
[^25]: 代表性集成在准线性复杂度下实现协同生命周期学习

    Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2004.12908](http://arxiv.org/abs/2004.12908)

    本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。

    

    在终身学习中，数据不仅可以用于改进当前任务的性能，还可以用于之前和尚未遇到的任务。传统的机器学习则从空白状态开始，仅针对单个任务使用数据。虽然传统迁移学习算法可以提高未来任务的性能，但在学习新任务后对旧任务的性能下降（称为遗忘）。近期针对连续或终身学习的许多方法都试图在给定新任务的情况下保持对旧任务的性能。但是，仅努力避免忘记将目标定得过低。终身学习的目标不仅应该是提高未来任务（前向传递）的性能，而且还应该是用任何新数据提高过去任务（反向传递）的性能。我们的关键见解是，我们可以协同集成分别在不同任务上独立学习的表示，以实现准线性复杂度下的前向和后向传递。本文提出了一种新方法，称为“终身学习中的表示集成（RELL）”，它集成了知识蒸馏和知识保持正则化方法，以利用不同表示中包含的互补信息。我们的实验表明，RELL在各种基准数据集上都优于现有最先进方法，尤其是在存在灾难性遗忘的情况下实现了显着更好的反向传递。

    In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
    

