# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NECO: NEural Collapse Based Out-of-distribution detection.](http://arxiv.org/abs/2310.06823) | NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。 |
| [^2] | [Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning.](http://arxiv.org/abs/2310.06793) | 该论文研究了低秩结构下的矩阵估计问题，并提出了基于频谱的方法，这些方法在估计奇异子空间方面表现出色，并且能够实现几乎最小的逐元素误差。这些新结果为充分利用低秩结构的强化学习算法的设计提供了可能性。 |
| [^3] | [Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules.](http://arxiv.org/abs/2310.06746) | 通过因果规则学习，我们可以利用加权因果规则来估计和加强对异质治疗效应的理解。 |
| [^4] | [S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models.](http://arxiv.org/abs/2310.06715) | 本研究解析了基于深度学习的睡眠阶段分类模型的设计空间，找到了适用于不同输入表示的稳健架构，并在睡眠数据集上实现了显著的性能提升。 |
| [^5] | [Implicit Variational Inference for High-Dimensional Posteriors.](http://arxiv.org/abs/2310.06643) | 本文提出了一种隐变分推断的方法，使用神经采样器指定隐含分布，在高维空间中近似复杂的多峰和相关后验分布。通过引入局部线性化的约束，避免了依赖额外的网络和不稳定对抗目标的问题。此外，还提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布。实证分析表明，该方法可以恢复大型贝叶斯神经网络中层之间的相关性，这对于网络的性能至关重要。 |
| [^6] | [XAI for Early Crop Classification.](http://arxiv.org/abs/2310.06574) | 通过使用XAI方法，我们提出了一种早期作物分类的方法，可以识别重要的时间步骤，并确定最短分类时间范围的边界区域。在准确性和早期性方面取得了良好的平衡。 |
| [^7] | [Statistical properties and privacy guarantees of an original distance-based fully synthetic data generation method.](http://arxiv.org/abs/2310.06571) | 本研究提出了一种基于距离的全合成数据生成方法，并通过使用新开发的工具评估了其风险-效用特性。 |
| [^8] | [Understanding black-box models with dependent inputs through a generalization of Hoeffding's decomposition.](http://arxiv.org/abs/2310.06567) | 通过提出一个新的框架，我们可以解释有关依赖输入的黑箱模型。我们证明了在一些合理的假设下，非线性函数可以唯一分解为每个可能子集的函数之和。这个框架有效地推广了Hoeffding分解，并提供了新颖的可解释性指标。 |
| [^9] | [Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN.](http://arxiv.org/abs/2310.06537) | 本文提出了一种基于多变量生成对抗网络的数据级混合策略选择方法，通过平衡磁盘SMART数据集并结合遗传算法，提高磁盘故障分类预测准确性。 |
| [^10] | [Disk failure prediction based on multi-layer domain adaptive learning.](http://arxiv.org/abs/2310.06534) | 本文介绍了一种基于多层域适应学习技术的硬盘故障预测方法，通过源领域和目标领域之间的对比，成功提高了对具有少量故障样本的硬盘数据进行故障预测的能力。 |
| [^11] | [Topological data analysis of human vowels: Persistent homologies across representation spaces.](http://arxiv.org/abs/2310.06508) | 本文研究了人类元音的拓扑数据分析，重点探讨了信号表示的选择对结果的影响。 |
| [^12] | [Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling.](http://arxiv.org/abs/2310.06389) | 本研究提出了乐高积木，通过集成局部特征丰富和全局内容协调，实现了高效且可自适应的迭代细化扩散建模。这些积木可以堆叠在一起，用于在测试时根据需要进行重构，从而减少采样成本并生成高分辨率图像。 |
| [^13] | [Learning bounded-degree polytrees with known skeleton.](http://arxiv.org/abs/2310.06333) | 本论文提出了一种高效学习已知骨架的有界度多树的算法，并给出了在多项式时间和样本复杂度内的有限样本保证。这对于复杂概率分布的学习具有重要意义。 |
| [^14] | [Discovering Mixtures of Structural Causal Models from Time Series Data.](http://arxiv.org/abs/2310.06312) | 这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。 |
| [^15] | [Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing.](http://arxiv.org/abs/2310.06306) | 本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。 |
| [^16] | [Deep Learning: A Tutorial.](http://arxiv.org/abs/2310.06251) | 该论文讲述了深度学习方法对结构化高维数据的洞察和预测规则，通过使用多层半仿射输入转换来提供预测规则并找到特征。 |
| [^17] | [A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data.](http://arxiv.org/abs/2310.06241) | 这项研究提出了一种使用稀疏贝叶斯方法从有限数据中学习可解释的拉格朗日描述物理系统的框架，并通过勒让德变换自动提取哈密顿描述。 |
| [^18] | [Automatic Integration for Spatiotemporal Neural Point Processes.](http://arxiv.org/abs/2310.06179) | 本文提出了一种自动化的时空神经点过程积分方法(AutoSTPP)，扩展了AutoInt方法用于三维时空点过程(STPP)的计算，具有优越性能。 |
| [^19] | [Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness.](http://arxiv.org/abs/2310.06161) | 本论文提出了一个框架，鼓励深度模型利用更多样的特征进行预测，以减轻简单性偏差带来的OOD推广问题，并在各种问题和应用中展示了其有效性。 |
| [^20] | [Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization.](http://arxiv.org/abs/2310.06159) | 本章提出了一种名为缩放梯度下降（ScaledGD）的新算法，能够在恒定速率下收敛，而不受低秩对象条件数的影响，并且在各种任务中具有低迭代成本。 |
| [^21] | [Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds.](http://arxiv.org/abs/2310.06157) | 本研究提出了一种基于模型的参数化方法，利用测地线和流动来描述可微流形上的距离和长度最小化曲线。这为在不同iable流形上进行统计和降阶建模提供了机会。 |
| [^22] | [Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques.](http://arxiv.org/abs/2310.06148) | 本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。 |
| [^23] | [Learning Layer-wise Equivariances Automatically using Gradients.](http://arxiv.org/abs/2310.06131) | 该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。 |
| [^24] | [When is Agnostic Reinforcement Learning Statistically Tractable?.](http://arxiv.org/abs/2310.06113) | 本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。 |
| [^25] | [Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach.](http://arxiv.org/abs/2310.06112) | 本文理论分析了宽深度神经网络的鲁棒过拟合现象，并提出了一种名为Adv-NTK的AT算法来增强神经网络的鲁棒性。 |
| [^26] | [Grokking as the Transition from Lazy to Rich Training Dynamics.](http://arxiv.org/abs/2310.06110) | 研究发现洞察现象可能是由神经网络从懒惰训练动态过渡到丰富的特征学习模式的结果，通过跟踪足够的统计量，发现洞察是在网络首先尝试拟合核回归解决方案后，进行后期特征学习找到通用解决方案之后的结果。 |
| [^27] | [Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making.](http://arxiv.org/abs/2310.06105) | 本文提出了一种数学框架来量化基于风险决策的深度学习分类中离散输入的预测不确定性。我们的研究有助于在决策过程中减轻相关风险，并且可以适用于处理涉及分类和离散特征变量的问题。 |
| [^28] | [High Dimensional Causal Inference with Variational Backdoor Adjustment.](http://arxiv.org/abs/2310.06100) | 本论文介绍了一种用于高维因果推断的变分背门调整技术，能够处理高维治疗和混杂因素，并成功应用于医疗数据中。 |
| [^29] | [Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting.](http://arxiv.org/abs/2310.06081) | 本文研究了一类广泛的马尔可夫链，即Ito链的Ito扩散逼近。与大多数相关论文不同，我们的链具有各向同性和状态相关的噪声，并可以适用于多种应用场景。我们证明了Ito链与对应的随机微分方程之间的W2-距离的上界。这些结果改进了已有的估计方法，并在某些特殊情况下提供了首次的分析。 |
| [^30] | [Optimal Exploration is no harder than Thompson Sampling.](http://arxiv.org/abs/2310.06069) | 这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？ |
| [^31] | [Cost-sensitive probabilistic predictions for support vector machines.](http://arxiv.org/abs/2310.05997) | 提出了一种新方法，将支持向量机转化为成本敏感的概率分类器，并充分利用了正则化参数的信息。 |
| [^32] | [Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions.](http://arxiv.org/abs/2310.05921) | 符合决策理论是一种框架，可以通过不完美的机器学习预测产生安全的自主决策。该理论的创新之处在于可以在没有对世界模型做出任何假设的情况下提供具有低风险的统计保证的决策。 |
| [^33] | [Universal Graph Random Features.](http://arxiv.org/abs/2310.04859) | 本文提出了一种新的准蒙特卡罗机制，称为排斥随机游走，通过改进图的采样，提高了统计估计器的集中度。该机制在估计图内核、PageRank向量和图形浓度等方面展示了有效性。 |
| [^34] | [On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic Gradient Descent.](http://arxiv.org/abs/2310.02402) | 本文提出了一种延迟MLMC梯度估计器，通过重复利用之前步骤中计算过的梯度分量，大大降低了MLMC的并行复杂性，并在数值实验中证明了其在随机梯度下降中具有更好的并行复杂性。 |
| [^35] | [Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models.](http://arxiv.org/abs/2310.00839) | 本文提出了一种使用深度生成模型和集成方法的井下特性表征方法。通过结合WGAN-GP和ES-MDA技术，实现了准确且高效的K场估计。这种方法在几个井下实例中得到了验证，并展示了未知K字段的主要特征。 |
| [^36] | [Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory.](http://arxiv.org/abs/2308.01853) | 这篇论文研究了统计估计中的分布偏移问题，主要关注Wasserstein分布偏移，提出了联合分布偏移概念，并分析了几个统计问题的解决方法。论文发现了最优的极小极大风险和最不利的扰动，并证明了样本均值和最小二乘估计量的优越性。 |
| [^37] | [Kernelised Normalising Flows.](http://arxiv.org/abs/2307.14839) | 本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。 |
| [^38] | [L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference.](http://arxiv.org/abs/2306.03580) | 本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。 |
| [^39] | [What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization.](http://arxiv.org/abs/2305.19420) | 本文对In-Context Learning进行了全面的研究，通过贝叶斯模型平均算法来隐式地实现ICL估计量，并采用在线学习的角度来分析ICL性能，建立后悔界限，并通过关注机制近似参数化。 |
| [^40] | [Self-Correcting Bayesian Optimization through Bayesian Active Learning.](http://arxiv.org/abs/2304.11005) | 该论文提出了SAL和SCoreBO两种方法，用于提高高斯过程模型的超参数选择和贝叶斯优化的表现。 |
| [^41] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^42] | [Eryn : A multi-purpose sampler for Bayesian inference.](http://arxiv.org/abs/2303.02164) | Eryn是一个多功能的贝叶斯推断采样器，通过集成多种概念和算法，提供了一个单独的MCMC包，用于解决物理学中的参数估计和模型选择问题。 |
| [^43] | [Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron.](http://arxiv.org/abs/2302.10034) | 通过研究学习单个神经元的过度参数化设置，本研究发现梯度下降算法的收敛速度会以指数级减慢，是首个给出该问题全局收敛结果的研究。通过证明上下界，我们精确刻画出了收敛速度，并指出了过度参数化对于收敛速度的影响。 |
| [^44] | [Quantum Learning Theory Beyond Batch Binary Classification.](http://arxiv.org/abs/2302.07409) | 这篇论文通过研究量子学习理论拓展了批处理多类学习、在线布尔学习和在线多类学习，并提出了第一个具有量子示例的在线学习模型。 |
| [^45] | [Robust Knowledge Transfer in Tiered Reinforcement Learning.](http://arxiv.org/abs/2302.05534) | 本文研究了层级增强学习中的知识传输，提出了一种新颖的在线学习算法，在没有先验知识的任务相似性的情况下实现强大的知识传输。 |
| [^46] | [Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems.](http://arxiv.org/abs/2208.13305) | 该论文提出了一种通用的方法，用于解决神经网络在高维空间中逼近连续函数的问题，并对理论与实践之间的差距进行了缩小。该方法基于Johnson-Lindenstrauss嵌入的观察，通过将高维集合嵌入到低维空间中，使得较小的神经网络可以有效逼近高维连续函数。 |
| [^47] | [Multiple Descent in the Multiple Random Feature Model.](http://arxiv.org/abs/2208.09897) | 本文研究了过度参数化学习中的双下降现象，并在多组分预测模型中进一步探究了多次下降现象。通过理论计算和实验证实，随机特征模型的风险曲线可以呈现出三次下降。 |
| [^48] | [No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling.](http://arxiv.org/abs/2202.11219) | 本文研究了在线对抗环境中，使用对数池化方法学习专家权重的问题。我们提出了一种基于在线镜像下降算法的方法，以达到无遗憾保证。 |
| [^49] | [Multi-consensus Decentralized Accelerated Gradient Descent.](http://arxiv.org/abs/2005.00797) | 本文提出了一种新颖的多共识分散加速梯度下降算法，实现了最优计算复杂度和接近最优的通信复杂度。算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。实证研究表明该方法在机器学习应用中超越其他方法。 |

# 详细

[^1]: NECO: 基于神经坍塌的超出分布检测

    NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])

    [http://arxiv.org/abs/2310.06823](http://arxiv.org/abs/2310.06823)

    NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。

    

    由于模型过于自信并且没有意识到其认识论限制，检测超出分布（OOD）数据是机器学习中的一个重要挑战。我们假设“神经坍塌”，一种影响超出分布数据的现象，也会影响超出分布数据。为了从这种相互作用中受益，我们引入了NECO，一种用于OOD检测的新颖的事后方法，它利用“神经坍塌”和主成分空间的几何属性来识别OOD数据。我们的大量实验表明，NECO在小规模和大规模OOD检测任务上取得了最先进的结果，同时在不同的网络架构上展示了强大的泛化能力。此外，我们还对我们的方法在OOD检测中的有效性提供了理论解释。我们计划在匿名期结束后发布代码。

    Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
    
[^2]: 低秩强化学习的频谱逐元素矩阵估计

    Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning. (arXiv:2310.06793v1 [cs.LG])

    [http://arxiv.org/abs/2310.06793](http://arxiv.org/abs/2310.06793)

    该论文研究了低秩结构下的矩阵估计问题，并提出了基于频谱的方法，这些方法在估计奇异子空间方面表现出色，并且能够实现几乎最小的逐元素误差。这些新结果为充分利用低秩结构的强化学习算法的设计提供了可能性。

    

    我们研究强化学习中出现的低秩结构的矩阵估计问题。在低秩赌博机中，需要恢复的矩阵指定了预期的臂奖励，而在低秩马尔可夫决策过程(MDP)中，它可以描述MDP的转换核。在这两种情况下，矩阵的每个元素都承载重要信息，我们寻求具有低逐元素误差的估计方法。重要的是，这些方法还需要适应可用数据中的固有相关性（例如，对于MDPs，数据由系统轨迹组成）。我们研究了简单的基于频谱的矩阵估计方法的性能：我们展示了它们有效地恢复了矩阵的奇异子空间，并且具有几乎最小的逐元素误差。这些关于低秩矩阵估计的新结果使得设计完全利用底层低秩结构的强化学习算法成为可能。我们提供了两个这样的算法示例。

    We study matrix estimation problems arising in reinforcement learning (RL) with low-rank structure. In low-rank bandits, the matrix to be recovered specifies the expected arm rewards, and for low-rank Markov Decision Processes (MDPs), it may for example characterize the transition kernel of the MDP. In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these methods further need to accommodate for inherent correlations in the available data (e.g. for MDPs, the data consists of system trajectories). We investigate the performance of simple spectral-based matrix estimation approaches: we show that they efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error. These new results on low-rank matrix estimation make it possible to devise reinforcement learning algorithms that fully exploit the underlying low-rank structure. We provide two examples of such algorit
    
[^3]: 因果规则学习：通过加权因果规则增强对异质治疗效应的理解

    Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules. (arXiv:2310.06746v1 [cs.LG])

    [http://arxiv.org/abs/2310.06746](http://arxiv.org/abs/2310.06746)

    通过因果规则学习，我们可以利用加权因果规则来估计和加强对异质治疗效应的理解。

    

    解释性是利用机器学习方法估计异质治疗效应时的关键问题，特别是对于医疗应用来说，常常需要做出高风险决策。受到解释性的预测性、描述性、相关性框架的启发，我们提出了因果规则学习，该方法通过找到描述潜在子群的精细因果规则集来估计和增强我们对异质治疗效应的理解。因果规则学习包括三个阶段：规则发现、规则选择和规则分析。在规则发现阶段，我们利用因果森林生成一组具有相应子群平均治疗效应的因果规则池。选择阶段使用D-学习方法从这些规则中选择子集，将个体水平的治疗效应作为子群水平效应的线性组合进行解构。这有助于回答之前文献忽视的问题：如果一个个体同时属于多个不同的治疗子群，会怎么样呢？

    Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously bel
    
[^4]: S4Sleep: 解析基于深度学习的睡眠阶段分类模型的设计空间

    S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models. (arXiv:2310.06715v1 [cs.LG])

    [http://arxiv.org/abs/2310.06715](http://arxiv.org/abs/2310.06715)

    本研究解析了基于深度学习的睡眠阶段分类模型的设计空间，找到了适用于不同输入表示的稳健架构，并在睡眠数据集上实现了显著的性能提升。

    

    对于多通道睡眠脑电图记录进行睡眠阶段打分是一项耗时且存在显著的评分人员之间差异的任务。因此，应用机器学习算法可以带来很大的益处。虽然已经为此提出了许多算法，但某些关键的架构决策并未得到系统性的探索。在本研究中，我们详细调查了广泛的编码器-预测器架构范畴内的这些设计选择。我们找到了适用于时间序列和声谱图输入表示的稳健架构。这些架构将结构化状态空间模型作为组成部分，对广泛的SHHS数据集的性能进行了统计显著的提升。这些改进通过统计和系统误差估计进行了评估。我们预计，从本研究中获得的架构洞察不仅对未来的睡眠分期研究有价值，而且对整体睡眠研究都有价值。

    Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hol
    
[^5]: 高维后验推断的隐变分推断

    Implicit Variational Inference for High-Dimensional Posteriors. (arXiv:2310.06643v1 [cs.LG])

    [http://arxiv.org/abs/2310.06643](http://arxiv.org/abs/2310.06643)

    本文提出了一种隐变分推断的方法，使用神经采样器指定隐含分布，在高维空间中近似复杂的多峰和相关后验分布。通过引入局部线性化的约束，避免了依赖额外的网络和不稳定对抗目标的问题。此外，还提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布。实证分析表明，该方法可以恢复大型贝叶斯神经网络中层之间的相关性，这对于网络的性能至关重要。

    

    在变分推断中，贝叶斯模型的好处在于准确捕捉真实的后验分布。我们提出使用指定隐含分布的神经采样器，这对于近似高维空间中复杂多峰和相关后验分布非常适用。我们的方法通过局部线性化神经采样器引入新的约束，这与现有方法不同，现有方法依赖于额外的鉴别器网络和不稳定的对抗目标。此外，我们提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布，通过使用可微分的数值近似来解决计算上的问题。我们的实证分析表明，我们的方法能够在大型贝叶斯神经网络中恢复层之间的相关性，这是网络性能关键但臭名昭著的属性。

    In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notorious
    
[^6]: XAI用于早期作物分类

    XAI for Early Crop Classification. (arXiv:2310.06574v1 [cs.LG])

    [http://arxiv.org/abs/2310.06574](http://arxiv.org/abs/2310.06574)

    通过使用XAI方法，我们提出了一种早期作物分类的方法，可以识别重要的时间步骤，并确定最短分类时间范围的边界区域。在准确性和早期性方面取得了良好的平衡。

    

    我们提出了一种通过使用可解释人工智能（XAI）方法识别重要时间步骤的早期作物分类方法。我们的方法包括训练一个基准的作物分类模型进行层级相关传播（LRP），以便可以确定显著的时间步骤。我们选择了一定数量的重要时间索引来创建最短分类时间范围的边界区域。我们确定了2019年4月21日至2019年8月9日期间在准确性和早期性方面具有最好的权衡。与使用完整时间序列相比，这个时间范围只损失了0.75％的准确性。我们观察到，由LRP导出的重要时间步骤还突出了输入值中区分不同类别的细节。

    We propose an approach for early crop classification through identifying important timesteps with eXplainable AI (XAI) methods. Our approach consists of training a baseline crop classification model to carry out layer-wise relevance propagation (LRP) so that the salient time step can be identified. We chose a selected number of such important time indices to create the bounding region of the shortest possible classification timeframe. We identified the period 21st April 2019 to 9th August 2019 as having the best trade-off in terms of accuracy and earliness. This timeframe only suffers a 0.75% loss in accuracy as compared to using the full timeseries. We observed that the LRP-derived important timesteps also highlight small details in input values that differentiates between different classes and
    
[^7]: 一种基于距离的全合成数据生成方法的统计性质和隐私保证

    Statistical properties and privacy guarantees of an original distance-based fully synthetic data generation method. (arXiv:2310.06571v1 [stat.ML])

    [http://arxiv.org/abs/2310.06571](http://arxiv.org/abs/2310.06571)

    本研究提出了一种基于距离的全合成数据生成方法，并通过使用新开发的工具评估了其风险-效用特性。

    

    引言：原始研究生成的数据量呈指数增长。为了遵守开放科学原则，建议将其公开发布。然而，从人类参与者收集的数据不能直接发布，因为会涉及隐私问题。完全合成数据是对此挑战的一种有希望的解决方案。法国人口流行病学与健康研究中心探索了这种方法，通过基于分类和回归树以及一种原始的基于距离的过滤方法构建了一个合成数据生成框架。本研究的目标是开发这个框架的改进版本，并使用经验和形式工具评估其风险-效用特性，包括为此评估而开发的新工具。

    Introduction: The amount of data generated by original research is growing exponentially. Publicly releasing them is recommended to comply with the Open Science principles. However, data collected from human participants cannot be released as-is without raising privacy concerns. Fully synthetic data represent a promising answer to this challenge. This approach is explored by the French Centre de Recherche en {\'E}pid{\'e}miologie et Sant{\'e} des Populations in the form of a synthetic data generation framework based on Classification and Regression Trees and an original distance-based filtering. The goal of this work was to develop a refined version of this framework and to assess its risk-utility profile with empirical and formal tools, including novel ones developed for the purpose of this evaluation.Materials and Methods: Our synthesis framework consists of four successive steps, each of which is designed to prevent specific risks of disclosure. We assessed its performance by applyi
    
[^8]: 通过Hoeffding分解的推广，理解有关依赖输入的黑箱模型

    Understanding black-box models with dependent inputs through a generalization of Hoeffding's decomposition. (arXiv:2310.06567v1 [math.FA])

    [http://arxiv.org/abs/2310.06567](http://arxiv.org/abs/2310.06567)

    通过提出一个新的框架，我们可以解释有关依赖输入的黑箱模型。我们证明了在一些合理的假设下，非线性函数可以唯一分解为每个可能子集的函数之和。这个框架有效地推广了Hoeffding分解，并提供了新颖的可解释性指标。

    

    解释黑箱模型的主要挑战之一是能够将非互不相关随机输入的平方可积函数唯一分解为每个可能子集的函数之和。然而，处理输入之间的依赖关系可能很复杂。我们提出了一个新的框架来研究这个问题，将三个数学领域联系起来：概率论、函数分析和组合数学。我们表明，在输入上的两个合理假设下（非完美的函数依赖性和非退化的随机依赖性），总是可以唯一分解这样一个函数。这种“规范分解”相对直观，揭示了非线性相关输入的非线性函数的线性特性。在这个框架中，我们有效地推广了众所周知的Hoeffding分解，可以看作是一个特殊情况。黑箱模型的斜投影为新颖的可解释性指标提供了可能。

    One of the main challenges for interpreting black-box models is the ability to uniquely decompose square-integrable functions of non-mutually independent random inputs into a sum of functions of every possible subset of variables. However, dealing with dependencies among inputs can be complicated. We propose a novel framework to study this problem, linking three domains of mathematics: probability theory, functional analysis, and combinatorics. We show that, under two reasonable assumptions on the inputs (non-perfect functional dependence and non-degenerate stochastic dependence), it is always possible to decompose uniquely such a function. This ``canonical decomposition'' is relatively intuitive and unveils the linear nature of non-linear functions of non-linearly dependent inputs. In this framework, we effectively generalize the well-known Hoeffding decomposition, which can be seen as a particular case. Oblique projections of the black-box model allow for novel interpretability indic
    
[^9]: 基于多变量生成对抗网络的磁盘故障预测模型的数据级混合策略选择

    Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN. (arXiv:2310.06537v1 [stat.ML])

    [http://arxiv.org/abs/2310.06537](http://arxiv.org/abs/2310.06537)

    本文提出了一种基于多变量生成对抗网络的数据级混合策略选择方法，通过平衡磁盘SMART数据集并结合遗传算法，提高磁盘故障分类预测准确性。

    

    数据类别不平衡是分类问题中常见的问题，其中少数类样本在分类任务中往往更重要且更容易被误分类。因此，解决数据类别不平衡的分类问题非常重要。SMART数据集是一个明显存在类别不平衡的数据集，其中包含大量健康样本和相对较少的缺陷样本。该数据集可作为磁盘健康状况的可靠指标。本文通过混合并整合多变量生成对抗网络(GAN)合成的数据，以在数据级别上平衡磁盘SMART数据集；再结合遗传算法，提高特定分类模型上的磁盘故障分类预测准确性。

    Data class imbalance is a common problem in classification problems, where minority class samples are often more important and more costly to misclassify in a classification task. Therefore, it is very important to solve the data class imbalance classification problem. The SMART dataset exhibits an evident class imbalance, comprising a substantial quantity of healthy samples and a comparatively limited number of defective samples. This dataset serves as a reliable indicator of the disc's health status. In this paper, we obtain the best balanced disk SMART dataset for a specific classification model by mixing and integrating the data synthesised by multivariate generative adversarial networks (GAN) to balance the disk SMART dataset at the data level; and combine it with genetic algorithms to obtain higher disk fault classification prediction accuracy on a specific classification model.
    
[^10]: 基于多层域适应学习的硬盘故障预测

    Disk failure prediction based on multi-layer domain adaptive learning. (arXiv:2310.06534v1 [stat.ML])

    [http://arxiv.org/abs/2310.06534](http://arxiv.org/abs/2310.06534)

    本文介绍了一种基于多层域适应学习技术的硬盘故障预测方法，通过源领域和目标领域之间的对比，成功提高了对具有少量故障样本的硬盘数据进行故障预测的能力。

    

    大规模数据存储容易发生故障。传统的机器学习模型依赖于历史数据进行预测，在硬盘故障预测方面存在困难。本文提出了一种基于多层域适应学习技术的硬盘故障预测新方法。首先，选择存在大量故障的硬盘数据作为源领域，选择存在较少故障的硬盘数据作为目标领域。通过选择的源领域和目标领域进行特征提取网络的训练。通过源领域和目标领域之间的对比，促进了诊断知识从源领域向目标领域的转移。根据实验结果表明，所提出的技术能够生成可靠的预测模型，并提高对具有少量故障样本的硬盘数据进行故障预测的能力。

    Large scale data storage is susceptible to failure. As disks are damaged and replaced, traditional machine learning models, which rely on historical data to make predictions, struggle to accurately predict disk failures. This paper presents a novel method for predicting disk failures by leveraging multi-layer domain adaptive learning techniques. First, disk data with numerous faults is selected as the source domain, and disk data with fewer faults is selected as the target domain. A training of the feature extraction network is performed with the selected origin and destination domains. The contrast between the two domains facilitates the transfer of diagnostic knowledge from the domain of source and target. According to the experimental findings, it has been demonstrated that the proposed technique can generate a reliable prediction model and improve the ability to predict failures on disk data with few failure samples.
    
[^11]: 人类元音的拓扑数据分析：跨表示空间的持续同调

    Topological data analysis of human vowels: Persistent homologies across representation spaces. (arXiv:2310.06508v1 [cs.SD])

    [http://arxiv.org/abs/2310.06508](http://arxiv.org/abs/2310.06508)

    本文研究了人类元音的拓扑数据分析，重点探讨了信号表示的选择对结果的影响。

    

    拓扑数据分析（TDA）在信号/图像处理中已被成功应用于各种任务，包括可视化和监督/无监督分类。通常，拓扑特征是从持续同调理论中获得的。标准的TDA流程从原始信号数据或其表示开始，然后基于预定义的过滤构建多尺度拓扑结构，最后计算用于进一步利用的拓扑标记。常用的拓扑标记是持续图（或其变换）。当前的研究讨论了利用拓扑标记的多种方式的后果，但很少讨论过过滤的选择，我们所知道的，对信号表示的选择尚未成为研究的对象。本文尝试解决这个问题。为此，我们收集了真实的音频数据并建

    Topological Data Analysis (TDA) has been successfully used for various tasks in signal/image processing, from visualization to supervised/unsupervised classification. Often, topological characteristics are obtained from persistent homology theory. The standard TDA pipeline starts from the raw signal data or a representation of it. Then, it consists in building a multiscale topological structure on the top of the data using a pre-specified filtration, and finally to compute the topological signature to be further exploited. The commonly used topological signature is a persistent diagram (or transformations of it). Current research discusses the consequences of the many ways to exploit topological signatures, much less often the choice of the filtration, but to the best of our knowledge, the choice of the representation of a signal has not been the subject of any study yet. This paper attempts to provide some answers on the latter problem. To this end, we collected real audio data and bu
    
[^12]: 学习可堆叠和可跳过的乐高积木以实现高效、可重构和可变分辨率的扩散建模

    Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling. (arXiv:2310.06389v1 [cs.CV])

    [http://arxiv.org/abs/2310.06389](http://arxiv.org/abs/2310.06389)

    本研究提出了乐高积木，通过集成局部特征丰富和全局内容协调，实现了高效且可自适应的迭代细化扩散建模。这些积木可以堆叠在一起，用于在测试时根据需要进行重构，从而减少采样成本并生成高分辨率图像。

    

    扩散模型在生成真实感图像方面表现出色，但在训练和采样方面具有显著的计算成本。尽管有各种技术来解决这些计算挑战，但一个较少探索的问题是设计一个高效且适应性强的网络骨干，用于迭代细化。当前的选项如U-Net和Vision Transformer通常依赖于资源密集型的深度网络，缺乏在变量分辨率下生成图像或使用比训练中更小的网络所需的灵活性。本研究引入了乐高积木，它们无缝集成了局部特征丰富和全局内容协调。这些积木可以堆叠在一起，创建一个测试时可重构的扩散骨干，允许选择性跳过积木以减少采样成本，并生成比训练数据更高分辨率的图像。乐高积木通过MLP对局部区域进行丰富，并使用Transformer块进行变换，同时保持一致的全分辨率

    Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution i
    
[^13]: 学习具有已知骨架的有界度多树

    Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v1 [cs.LG])

    [http://arxiv.org/abs/2310.06333](http://arxiv.org/abs/2310.06333)

    本论文提出了一种高效学习已知骨架的有界度多树的算法，并给出了在多项式时间和样本复杂度内的有限样本保证。这对于复杂概率分布的学习具有重要意义。

    

    我们为高维概率分布的一类丰富的多树（polytrees）——有界度多树，建立了高效适当学习的有限样本保证。有界度多树是贝叶斯网络的子类，贝叶斯网络是一种广泛研究的图模型类型。最近，Bhattacharyya等人（2021）通过提供一种高效算法，在已知无向图（骨架）的情况下，为1-多树恢复了有限样本保证。我们通过扩展他们的结果，提供了一种高效算法，可以在多项式时间和样本复杂度内学习任何有界度的$d$-多树。我们将算法与信息论样本复杂度的下界结合起来，表明对维度和目标精度参数的依赖几乎是紧致的。

    We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
    
[^14]: 从时间序列数据中发现混合结构因果模型

    Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])

    [http://arxiv.org/abs/2310.06312](http://arxiv.org/abs/2310.06312)

    这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。

    

    在金融、气候科学和神经科学等领域，从时间序列数据中推断因果关系是一个巨大的挑战。尽管现代技术可以处理变量之间的非线性关系和灵活的噪声分布，但它们依赖于简化假设，即数据来自相同的潜在因果模型。在这项工作中，我们放松了这个假设，从来源于不同因果模型混合的时间序列数据中进行因果发现。我们推断了潜在的结构性因果模型，以及每个样本属于特定混合成分的后验概率。我们的方法采用了一个端对端的训练过程，最大化了数据似然的证据下界。通过对合成和真实世界数据集的广泛实验，我们证明了我们的方法在因果发现任务中超越了最先进的基准方法，尤其是当数据来自不同的潜在因果模型时。

    In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
    
[^15]: 面向制造业的AI孵化的上下文强化学习集成活跃学习方法

    Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. (arXiv:2310.06306v1 [cs.LG])

    [http://arxiv.org/abs/2310.06306](http://arxiv.org/abs/2310.06306)

    本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。

    

    工业物联网系统中的在线感知和计算资源促进了基于AI的决策。然而，数据质量问题，如类别不平衡，阻碍了离线训练的AI模型。为了解决这个问题，AI模型会通过流式数据进行在线更新以持续改进。然而，由于注释约束，监督学习模型在选择用于更新的优质流式样本方面面临挑战。文献中的主动学习方法通过关注不足或过度表示的区域来提供解决方案。在不断变化的制造背景下平衡这些策略是具有挑战性的。AI学习到的一些获取准则可以动态适应，但可能无法始终处理频繁的变化。我们引入了一种集成主动学习方法CBEAL，专门利用主动学习代理进行探索或利用。代理的权重根据决策有效性进行调整。CBEAL可以优化地指导数据获取，实现数据效果的最小化。

    Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minim
    
[^16]: 深度学习：一篇教程

    Deep Learning: A Tutorial. (arXiv:2310.06251v1 [stat.ML])

    [http://arxiv.org/abs/2310.06251](http://arxiv.org/abs/2310.06251)

    该论文讲述了深度学习方法对结构化高维数据的洞察和预测规则，通过使用多层半仿射输入转换来提供预测规则并找到特征。

    

    我们的目标是提供对深度学习方法的回顾，以揭示对结构化高维数据的洞察。与大多数统计模型常用的浅层加性结构不同，深度学习使用多层半仿射输入转换来提供预测规则。应用这些转换层导致一组属性（或特征），可以应用概率统计方法。因此，我们可以同时达到可扩展的预测规则和不确定性量化，其中稀疏正则化找到了特征。

    Our goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.
    
[^17]: 从数据中发现可解释的动力学系统的拉格朗日贝叶斯框架

    A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data. (arXiv:2310.06241v1 [stat.ML])

    [http://arxiv.org/abs/2310.06241](http://arxiv.org/abs/2310.06241)

    这项研究提出了一种使用稀疏贝叶斯方法从有限数据中学习可解释的拉格朗日描述物理系统的框架，并通过勒让德变换自动提取哈密顿描述。

    

    学习和预测物理系统的动力学需要对基本物理定律有深入的理解。最近的研究将方程发现的框架推广到物理系统的哈密顿和拉格朗日的发现。虽然现有的方法使用神经网络对拉格朗日进行参数化，但我们提出了一种使用稀疏贝叶斯方法从有限的数据中学习可解释的拉格朗日描述物理系统的替代框架。与现有的神经网络方法不同，所提出的方法(a)得到了可解释的拉格朗日描述，(b)利用贝叶斯学习来量化由于有限数据而导致的认知不确定性，(c)通过勒让德变换自动提取从学习到的拉格朗日得到的哈密顿描述，(d)提供基于常微分方程（ODE）和偏微分方程（PDE）的观测系统描述。涉及六个不同的例子，包括两种情况。

    Learning and predicting the dynamics of physical systems requires a profound understanding of the underlying physical laws. Recent works on learning physical laws involve generalizing the equation discovery frameworks to the discovery of Hamiltonian and Lagrangian of physical systems. While the existing methods parameterize the Lagrangian using neural networks, we propose an alternate framework for learning interpretable Lagrangian descriptions of physical systems from limited data using the sparse Bayesian approach. Unlike existing neural network-based approaches, the proposed approach (a) yields an interpretable description of Lagrangian, (b) exploits Bayesian learning to quantify the epistemic uncertainty due to limited data, (c) automates the distillation of Hamiltonian from the learned Lagrangian using Legendre transformation, and (d) provides ordinary (ODE) and partial differential equation (PDE) based descriptions of the observed systems. Six different examples involving both di
    
[^18]: 自动化的时空神经点过程积分方法

    Automatic Integration for Spatiotemporal Neural Point Processes. (arXiv:2310.06179v1 [cs.LG])

    [http://arxiv.org/abs/2310.06179](http://arxiv.org/abs/2310.06179)

    本文提出了一种自动化的时空神经点过程积分方法(AutoSTPP)，扩展了AutoInt方法用于三维时空点过程(STPP)的计算，具有优越性能。

    

    学习连续时间的点过程对于许多离散事件预测任务至关重要。然而，对于时空点过程（STPPs）的积分问题是一个重要挑战，因为它涉及到对空间和时间进行三重积分计算。现有的STPP积分方法要么假设强度函数具有参数形式，这缺乏灵活性；要么用蒙特卡洛采样来近似强度，这引入了数值误差。Omi等人最近的工作提出了一个自动积分方法AutoInt，用于高效地积分灵活的强度函数，但该方法只关注1D时间点过程。本文将AutoInt方法扩展至3D STPP，提出了一种新的范式：AutoSTPP（自动化的时空神经点过程积分方法）。我们表明，直接扩展之前的工作会过于约束强度函数，导致性能不佳。我们证明了我们的方法在各种实验中的优越性能。

    Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prov
    
[^19]: 减轻深度学习中的简单性偏差以改善OOD推广和鲁棒性

    Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness. (arXiv:2310.06161v1 [cs.LG])

    [http://arxiv.org/abs/2310.06161](http://arxiv.org/abs/2310.06161)

    本论文提出了一个框架，鼓励深度模型利用更多样的特征进行预测，以减轻简单性偏差带来的OOD推广问题，并在各种问题和应用中展示了其有效性。

    

    神经网络已知存在简单性偏差，即它们倾向于学习“简单”特征而不是更“复杂”的特征，即使后者可能更具信息量。简单性偏差可能导致模型做出具有较差的OOD推广的偏见性预测。为了解决这个问题，我们提出了一个框架，鼓励模型使用更多样的特征进行预测。我们首先训练一个简单模型，然后使用条件互信息对其进行正则化，得到最终模型。我们在各种问题设置和真实世界应用中展示了这个框架的有效性，证明它有效地解决了简单性偏差，促使更多特征被使用，增强了OOD推广，并提高了子组鲁棒性和公平性。我们补充这些结果与对正则化效果及其OOD推广特性的理论分析。

    Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning 'simple' features over more 'complex' ones, even when the latter may be more informative. Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization. To address this, we propose a framework that encourages the model to use a more diverse set of features to make predictions. We first train a simple model, and then regularize the conditional mutual information with respect to it to obtain the final model. We demonstrate the effectiveness of this framework in various problem settings and real-world applications, showing that it effectively addresses simplicity bias and leads to more features being used, enhances OOD generalization, and improves subgroup robustness and fairness. We complement these results with theoretical analyses of the effect of the regularization and its OOD generalization properties.
    
[^20]: 经过缩放梯度下降法的，甚至过参数化的可证明加速的病态低秩估计

    Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization. (arXiv:2310.06159v1 [cs.LG])

    [http://arxiv.org/abs/2310.06159](http://arxiv.org/abs/2310.06159)

    本章提出了一种名为缩放梯度下降（ScaledGD）的新算法，能够在恒定速率下收敛，而不受低秩对象条件数的影响，并且在各种任务中具有低迭代成本。

    

    科学和工程中遇到的许多问题可以归纳为从不完整且可能损坏的线性测量中估计低秩对象（例如矩阵和张量）。通过矩阵和张量分解的视角，其中一种最流行的方法是使用简单的迭代算法，如梯度下降（GD）直接恢复低秩因子，这样可以实现小内存和计算开销。然而，GD的收敛速率线性地依赖于低秩对象的条件数，有时甚至是二次的，因此当问题病态时，GD的收敛非常缓慢。本章介绍了一种新的算法方法，称为缩放梯度下降法（ScaledGD），它能够以恒定速率线性地收敛，而不依赖于低秩对象的条件数，同时保持了梯度下降在各种任务中的每次迭代成本较低，包括感知、鲁棒主成分估计等任务。

    Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal c
    
[^21]: 基于流形的Eikonal方程：可微流形上的测地距离和流动

    Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds. (arXiv:2310.06157v1 [cs.CG])

    [http://arxiv.org/abs/2310.06157](http://arxiv.org/abs/2310.06157)

    本研究提出了一种基于模型的参数化方法，利用测地线和流动来描述可微流形上的距离和长度最小化曲线。这为在不同iable流形上进行统计和降阶建模提供了机会。

    

    通过机器学习模型发现的流形提供了底层数据的紧凑表示。这些流形上的测地线定义了局部长度最小化曲线，并提供了距离的概念，这对于降阶建模、统计推断和插值至关重要。在这项工作中，我们提出了一种基于模型的参数化方法来表示流形上的距离场和测地流动，利用扩展的Eikonal方程的解决方案。我们展示了流形的几何特性如何影响距离场，并利用测地流动直接获得全局长度最小化曲线。这项工作为在可微流形上进行统计和降阶建模提供了机会。

    Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
    
[^22]: 理解迁移学习和基于梯度的元学习技术

    Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques. (arXiv:2310.06148v1 [cs.LG])

    [http://arxiv.org/abs/2310.06148](http://arxiv.org/abs/2310.06148)

    本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。

    

    深度神经网络在各种任务上可以取得良好的性能，但通常需要大量的数据来训练。元学习作为一种提高这些网络从有限数据中泛化能力的方法受到广泛关注。虽然元学习技术在各种场景下被证明是成功的，但最近的结果表明，在评估与训练数据分布不同的任务时，与复杂的元学习技术（如MAML）相比，仅对预训练网络进行微调的基准线可能更有效。这一点令人惊讶，因为MAML的学习行为与微调的行为类似：两者都依赖于重复使用已学习的特征。我们调查了微调、MAML和另一种名为Reptile的元学习技术之间观察到的性能差异，并展示了MAML和Reptile在低数据情况下的快速适应能力。

    Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data r
    
[^23]: 使用梯度自动学习层间等变性

    Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])

    [http://arxiv.org/abs/2310.06131](http://arxiv.org/abs/2310.06131)

    该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。

    

    卷积将等变性对称性编码到神经网络中，从而提高了泛化性能。然而，对称性提供了网络可以表示的函数的固定硬约束，需要事先指定，并且不能适应改变。我们的目标是允许灵活的对称性约束，可以通过梯度自动地从数据中学习。从头开始学习对称性和相关的权重连接结构有两个困难。首先，它需要有效灵活的层间等变性参数化。其次，对称性作为约束，因此不会被训练损失函数鼓励。为了克服这些挑战，我们改进了软等变性的参数化，并通过优化边缘似然来学习层间等变性的数量，其中边缘似然是使用可微分的拉普拉斯逼近估计的。这个目标平衡了数据拟合和模型复杂性，使层间对称性在深度学习中被发现。

    Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee
    
[^24]: 什么时候是基于不可知激励强化学习的统计可处理性？

    When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])

    [http://arxiv.org/abs/2310.06113](http://arxiv.org/abs/2310.06113)

    本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。

    

    本文研究了基于不可知的PAC强化学习（RL）的问题：给定一个策略类别Π，需要和一个未知的有可能有大状态和动作空间的MDP进行多少轮互动来学习一个关于Π的ε-次优策略？为此，我们引入了一个新的复杂度度量，称为“跨越容量”，它仅依赖于策略类别Π，并且与MDP的动态无关。通过一个生成模型，我们证明了对于任何策略类别Π，有界的跨越容量可以刻画PAC可学习性。然而，对于在线RL来说，情况更加微妙。我们证明了存在一个具有有界跨越容量的策略类别Π，需要超多项式数量的样本才能学习。这揭示了在不同生成和在线访问模型之间以及在线访问下的确定/随机MDP之间的不确定可学习性之间的出乎意料的差异。在积极的方面，我们识别出一个额外的“太阳”

    We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunf
    
[^25]: 宽深度神经网络的鲁棒过拟合的理论分析：一种NTK方法

    Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach. (arXiv:2310.06112v1 [cs.LG])

    [http://arxiv.org/abs/2310.06112](http://arxiv.org/abs/2310.06112)

    本文理论分析了宽深度神经网络的鲁棒过拟合现象，并提出了一种名为Adv-NTK的AT算法来增强神经网络的鲁棒性。

    

    对抗训练(AT)是增强深度神经网络(DNNs)鲁棒性的经典方法。然而，最近的研究实验证明它存在鲁棒过拟合现象，即长时间的AT可能对DNNs的鲁棒性产生不利影响。本文对DNNs的鲁棒过拟合提出了一个理论解释。具体而言，我们将神经切向核(NTK)理论非平凡地扩展到AT，并证明了通过AT训练的宽DNN可以很好地近似为一个线性化的DNN。此外，对于平方损失，可以推导出线性化DNN的闭式AT动力学，揭示了一种新的AT退化现象：长期的AT将导致宽DNN退化为没有AT的DNN，从而引起鲁棒过拟合。根据我们的理论结果，我们进一步设计了一种名为Adv-NTK的方法，这是第一种针对无限宽的DNNs的AT算法。在实际数据集上的实验结果表明，Adv-NTK可以帮助无限宽的DNNs提升鲁棒性。

    Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparab
    
[^26]: 从懒惰到丰富训练动态的洞察力

    Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v1 [stat.ML])

    [http://arxiv.org/abs/2310.06110](http://arxiv.org/abs/2310.06110)

    研究发现洞察现象可能是由神经网络从懒惰训练动态过渡到丰富的特征学习模式的结果，通过跟踪足够的统计量，发现洞察是在网络首先尝试拟合核回归解决方案后，进行后期特征学习找到通用解决方案之后的结果。

    

    我们提出了洞察现象，即神经网络的训练损失在测试损失之前大幅下降，可能是由于神经网络从懒惰的训练动态转变为丰富的特征学习模式。为了说明这一机制，我们研究了在没有正则化的情况下，使用Vanilla梯度下降方法在多项式回归问题上进行的两层神经网络的训练，该训练展现了无法用现有理论解释的洞察现象。我们确定了该网络测试损失的足够统计量，并通过训练跟踪这些统计量揭示了洞察现象的发生。我们发现，在这种情况下，网络首先尝试使用初始特征拟合核回归解决方案，接着在训练损失已经很低的情况下进行后期特征学习，从而找到了一个能够泛化的解决方案。我们发现，洞察产生的关键因素是特征学习的速率，这可以通过缩放网络参数来精确控制。

    We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the ne
    
[^27]: 用于基于风险决策的深度学习分类中离散输入的不确定性量化

    Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making. (arXiv:2310.06105v1 [stat.ML])

    [http://arxiv.org/abs/2310.06105](http://arxiv.org/abs/2310.06105)

    本文提出了一种数学框架来量化基于风险决策的深度学习分类中离散输入的预测不确定性。我们的研究有助于在决策过程中减轻相关风险，并且可以适用于处理涉及分类和离散特征变量的问题。

    

    深度神经网络（DNN）模型在基于风险决策中的应用引起了广泛关注，在医疗、金融、制造和质量控制等领域具有广泛的应用。为了在决策过程中减轻相关风险，预测的置信度或不确定性应该与算法的整体性能一起进行评估。最近关于贝叶斯深度学习的研究有助于量化源于输入噪声和模型参数的预测不确定性。然而，这些模型中对输入噪声的正态性假设限制了其适用于涉及分类和离散特征变量的问题。在本文中，我们提出了一个数学框架来量化DNN模型的预测不确定性。这种预测不确定性源于遵循已知有限离散分布的预测器误差。然后，我们使用该框架进行了一个案例研究，预测结核病患者治疗结果的过程中的不确定性。

    The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of t
    
[^28]: 使用变分背门调整进行高维因果推断

    High Dimensional Causal Inference with Variational Backdoor Adjustment. (arXiv:2310.06100v1 [cs.AI])

    [http://arxiv.org/abs/2310.06100](http://arxiv.org/abs/2310.06100)

    本论文介绍了一种用于高维因果推断的变分背门调整技术，能够处理高维治疗和混杂因素，并成功应用于医疗数据中。

    

    背门调整是一种因果推断技术，用于从纯观察数据中估计干预数量。在医疗环境中，背门调整可用于控制混杂因素并估计治疗效果。然而，高维治疗和混杂因素可能引发一系列潜在问题：可计算性、可辨识性、优化等。在这项工作中，我们采用生成建模方法来解决高维治疗和混杂因素的背门调整问题。我们将背门调整视为一种变分推断优化问题，无需依赖代理变量和隐藏混杂因素。实验上，我们的方法能够在各种高维环境中估计干预概率，包括半合成X光医疗数据。据我们所知，这是背门调整的首次应用，其中所有相关变量都是高维的。

    Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
    
[^29]: 用于采样、优化和提升的通用Ito链的Ito扩散逼近

    Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. (arXiv:2310.06081v1 [math.OC])

    [http://arxiv.org/abs/2310.06081](http://arxiv.org/abs/2310.06081)

    本文研究了一类广泛的马尔可夫链，即Ito链的Ito扩散逼近。与大多数相关论文不同，我们的链具有各向同性和状态相关的噪声，并可以适用于多种应用场景。我们证明了Ito链与对应的随机微分方程之间的W2-距离的上界。这些结果改进了已有的估计方法，并在某些特殊情况下提供了首次的分析。

    

    本文考虑了一类相当一般和广泛的马尔可夫链，即Ito链，其类似于某些随机微分方程的Euler-Maruyama离散化。我们研究的链是一个统一的理论分析框架。与大多数相关论文中的正态和状态独立噪声不同，我们的链具有几乎任意各向同性和状态相关噪声。此外，我们链的漂移和扩散系数可以是精确的，以涵盖诸如随机梯度Langevin动力学、采样、随机梯度下降或随机梯度提升等广泛的应用。我们证明了Ito链与对应的随机微分方程之间的W2-距离的一个上界。这些结果改进或覆盖了大部分已知的估计。此外，对于某些特殊情况，我们的分析是第一个。

    This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
    
[^30]: 最优探索不比汤普森采样更困难

    Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])

    [http://arxiv.org/abs/2310.06069](http://arxiv.org/abs/2310.06069)

    这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？

    

    在给定一组臂$\mathcal{Z}\subset \mathbb{R}^d$和未知参数向量$\theta_\ast\in\mathbb{R}^d$的情况下，纯探索线性臂问题旨在通过对$x^{\top}\theta_{\ast}$的噪声测量，返回$\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$，并以高概率找到正确解。现有的（渐近）最优方法要求要么为每个臂$z\in \mathcal{Z}$进行潜在昂贵的投影，要么在每个时间点明确地维护一部分正在考虑的$\mathcal{Z}$。这种复杂性与流行且简单的汤普森采样算法用于最小化后悔的情况完全相反，后者只需要访问后验采样和argmax oracle，并且在任何时间点都不需要枚举$\mathcal{Z}$。不幸的是，已知汤普森采样对于纯探索是次优的。在这项工作中，我们提出了一个自然的问题：是否存在一种算法能够进行最优探索，而且只需要相同的计算操作？

    Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput
    
[^31]: 成本敏感的支持向量机的概率预测

    Cost-sensitive probabilistic predictions for support vector machines. (arXiv:2310.05997v1 [stat.ML])

    [http://arxiv.org/abs/2310.05997](http://arxiv.org/abs/2310.05997)

    提出了一种新方法，将支持向量机转化为成本敏感的概率分类器，并充分利用了正则化参数的信息。

    

    支持向量机（SVM）被广泛使用，是最受关注和使用的二元分类机器学习模型之一。SVM的分类是基于得分过程的，得到的是确定性的分类规则，可以转化为概率规则（在现成的SVM库中实现），但本质上并不是概率性的。另一方面，SVM中正则化参数的调优被认为需要很高的计算量，并且生成的信息没有充分利用，没有用于构建概率分类规则。在本文中，我们提出了一种新方法来生成SVM的概率输出。新方法具有以下三个特点。首先，它是设计为成本敏感的，因此可以很容易地适应敏感性（或真正例率，TPR）和特异性（真负例率，TNR）的不同重要性。结果，模型能够处理成本敏感的问题。

    Support vector machines (SVMs) are widely used and constitute one of the best examined and used machine learning models for two-class classification. Classification in SVM is based on a score procedure, yielding a deterministic classification rule, which can be transformed into a probabilistic rule (as implemented in off-the-shelf SVM libraries), but is not probabilistic in nature. On the other hand, the tuning of the regularization parameters in SVM is known to imply a high computational effort and generates pieces of information that are not fully exploited, not being used to build a probabilistic classification rule. In this paper we propose a novel approach to generate probabilistic outputs for the SVM. The new method has the following three properties. First, it is designed to be cost-sensitive, and thus the different importance of sensitivity (or true positive rate, TPR) and specificity (true negative rate, TNR) is readily accommodated in the model. As a result, the model can dea
    
[^32]: 符合决策理论: 通过不完美的预测产生安全的自主决策

    Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions. (arXiv:2310.05921v1 [stat.ML])

    [http://arxiv.org/abs/2310.05921](http://arxiv.org/abs/2310.05921)

    符合决策理论是一种框架，可以通过不完美的机器学习预测产生安全的自主决策。该理论的创新之处在于可以在没有对世界模型做出任何假设的情况下提供具有低风险的统计保证的决策。

    

    我们介绍了一种符合决策理论的框架，可以在机器学习预测不完美的情况下产生安全的自主决策。这种决策的例子是普遍存在的，从依赖于行人预测的机器人规划算法，到校准自动化制造以实现高吞吐量和低错误率，再到在运行时选择信任名义策略还是切换到安全备份策略。我们算法产生的决策在统计保证的情况下是安全的，无需对世界模型作出任何假设；观测数据可以不满足独立同分布(I.I.D.)的条件，甚至可能是对抗性的。该理论将符合预测的结果扩展到直接校准决策，而不需要构建预测集合。实验证明了我们方法在围绕人类进行机器人运动规划、自动股票交易和机器人制造方面的实用性。

    We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturin
    
[^33]: 通用图随机特征

    Universal Graph Random Features. (arXiv:2310.04859v1 [stat.ML])

    [http://arxiv.org/abs/2310.04859](http://arxiv.org/abs/2310.04859)

    本文提出了一种新的准蒙特卡罗机制，称为排斥随机游走，通过改进图的采样，提高了统计估计器的集中度。该机制在估计图内核、PageRank向量和图形浓度等方面展示了有效性。

    

    我们提出了一种新颖的准蒙特卡罗机制，称为排斥随机游走，以改进基于图的采样。通过在相互作用集合的轨迹之间引入相关性，使它们的边际转移概率保持不变，我们能够更高效地探索图形，提高统计估计器的集中度，同时保持它们的无偏性。该机制可以轻松地实现。我们展示了在估计图内核、PageRank向量和图形浓度等各种情况下，排斥随机游走的有效性。我们提供了详细的实验评估和鲁棒的理论保证。据我们所知，排斥随机游走是第一个在图上相关步行者方向进行严格研究的准蒙特卡罗方案，为这个令人兴奋的新兴领域带来了新的研究。

    We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.
    
[^34]: 关于随机梯度下降中多层蒙特卡洛的并行复杂性

    On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic Gradient Descent. (arXiv:2310.02402v1 [cs.LG])

    [http://arxiv.org/abs/2310.02402](http://arxiv.org/abs/2310.02402)

    本文提出了一种延迟MLMC梯度估计器，通过重复利用之前步骤中计算过的梯度分量，大大降低了MLMC的并行复杂性，并在数值实验中证明了其在随机梯度下降中具有更好的并行复杂性。

    

    在用于顺序模拟（如神经随机微分方程）的随机梯度下降（SGD）中，多层蒙特卡洛（MLMC）方法已被证明在理论计算复杂性方面优于朴素的蒙特卡洛方法。然而在实践中，MLMC在现代GPU等大规模并行计算平台上的可扩展性较差，因为其并行复杂性与朴素蒙特卡洛方法相当。为了解决这个问题，我们提出了延迟MLMC梯度估计器，通过重复利用之前步骤中计算过的梯度分量，大大降低MLMC的并行复杂性。所提出的估计器在每次迭代中能够证明降低平均并行复杂性，但代价是稍差的收敛速率。在我们的数值实验中，我们使用深度对冲的示例来证明与标准MLMC在SGD中相比，我们的方法具有更好的并行复杂性。

    In the stochastic gradient descent (SGD) for sequential simulations such as the neural stochastic differential equations, the Multilevel Monte Carlo (MLMC) method is known to offer better theoretical computational complexity compared to the naive Monte Carlo approach. However, in practice, MLMC scales poorly on massively parallel computing platforms such as modern GPUs, because of its large parallel complexity which is equivalent to that of the naive Monte Carlo method. To cope with this issue, we propose the delayed MLMC gradient estimator that drastically reduces the parallel complexity of MLMC by recycling previously computed gradient components from earlier steps of SGD. The proposed estimator provably reduces the average parallel complexity per iteration at the cost of a slightly worse per-iteration convergence rate. In our numerical experiments, we use an example of deep hedging to demonstrate the superior parallel complexity of our method compared to the standard MLMC in SGD.
    
[^35]: 使用深度生成模型的基于集成方法的井下特性表征

    Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models. (arXiv:2310.00839v1 [cs.LG])

    [http://arxiv.org/abs/2310.00839](http://arxiv.org/abs/2310.00839)

    本文提出了一种使用深度生成模型和集成方法的井下特性表征方法。通过结合WGAN-GP和ES-MDA技术，实现了准确且高效的K场估计。这种方法在几个井下实例中得到了验证，并展示了未知K字段的主要特征。

    

    估计如水力传导率（K）等空间分布属性是井下特性表征中的重大挑战。然而，由于计算成本高和稀疏数据集的预测精度低，逆向建模在不适定的高维应用中受限。本文将Wasserstein生成对抗网络与梯度惩罚（WGAN-GP）和基于集成的多元数据同化（ES-MDA）技术相结合，实现了准确且高效的井下特性表征。WGAN-GP通过训练从低维潜变量空间生成高维K场，ES-MDA通过同化可用测量结果来更新潜变量。利用几个井下实例评估了所提出方法的准确性和效率，以及未知K字段的主要特征。

    Estimating spatially distributed properties such as hydraulic conductivity (K) from available sparse measurements is a great challenge in subsurface characterization. However, the use of inverse modeling is limited for ill-posed, high-dimensional applications due to computational costs and poor prediction accuracy with sparse datasets. In this paper, we combine Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), a deep generative model that can accurately capture complex subsurface structure, and Ensemble Smoother with Multiple Data Assimilation (ES-MDA), an ensemble-based inversion method, for accurate and accelerated subsurface characterization. WGAN-GP is trained to generate high-dimensional K fields from a low-dimensional latent space and ES-MDA then updates the latent variables by assimilating available measurements. Several subsurface examples are used to evaluate the accuracy and efficiency of the proposed method and the main features of the unknown K fie
    
[^36]: 统计估计中的分布偏移: Wasserstein扰动与极小极大理论

    Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])

    [http://arxiv.org/abs/2308.01853](http://arxiv.org/abs/2308.01853)

    这篇论文研究了统计估计中的分布偏移问题，主要关注Wasserstein分布偏移，提出了联合分布偏移概念，并分析了几个统计问题的解决方法。论文发现了最优的极小极大风险和最不利的扰动，并证明了样本均值和最小二乘估计量的优越性。

    

    分布偏移是现代统计学习中的一个严重问题，因为它们可以将数据的特性从真实情况中系统地改变。我们专注于Wasserstein分布偏移，其中每个数据点可能会发生轻微扰动，而不是Huber污染模型，其中一部分观测值是异常值。我们提出并研究了超出独立扰动的偏移，探索了联合分布偏移，其中每个观测点的扰动可以协调进行。我们分析了几个重要的统计问题，包括位置估计、线性回归和非参数密度估计。在均值估计和线性回归的预测误差方差下，我们找到了精确的极小极大风险、最不利的扰动，并证明了样本均值和最小二乘估计量分别是最优的。这适用于独立和联合偏移，但最不利的扰动和极小极大风险是不同的。

    Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
    
[^37]: 核化归一化流

    Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])

    [http://arxiv.org/abs/2307.14839](http://arxiv.org/abs/2307.14839)

    本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。

    

    归一化流是以其可逆的架构而被描述的生成模型。然而，可逆性要求对其表达能力施加限制，需要大量的参数和创新的架构设计来达到满意的结果。虽然基于流的模型主要依赖于基于神经网络的转换来实现表达能力，但替代的转换方法却受到了有限的关注。在这项工作中，我们提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到框架中。我们的结果表明，相比于基于神经网络的流，核化流可以产生有竞争力或优越的结果，同时保持参数效率。核化流在低数据环境中表现出色，可以在数据稀缺的应用中进行灵活的非参数密度估计。

    Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
    
[^38]: L-C2ST: 基于本地诊断实现模拟推断中后验近似

    L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])

    [http://arxiv.org/abs/2306.03580](http://arxiv.org/abs/2306.03580)

    本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。

    

    最近许多模拟推断（SBI）的工作都依赖于深度生成模型来近似复杂、高维度的后验分布。然而，评估这些近似是否可信仍是一个挑战。大多数方法仅在观测空间期望下评估后验估计器。这限制了它们的可解释性，并不能足够地确定哪些观测结果可以信任这些近似或应该改进。我们基于著名的分类器两样本检验 (C2ST)，引入 L-C2ST，一个新方法，允许在任何给定的观测下本地评估后验估计器。它提供有理论基础和易于解释的，如图示诊断。与 C2ST 不同的是，L-C2ST 不需要访问真实后验的样本。对于基于归一化流的后验估计器，L-C2ST 可以专门提供更好的统计功率，同时计算效率更高。

    Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
    
[^39]: In-Context Learning学习了什么以及如何学习？贝叶斯模型平均、参数化和泛化。

    What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization. (arXiv:2305.19420v1 [stat.ML])

    [http://arxiv.org/abs/2305.19420](http://arxiv.org/abs/2305.19420)

    本文对In-Context Learning进行了全面的研究，通过贝叶斯模型平均算法来隐式地实现ICL估计量，并采用在线学习的角度来分析ICL性能，建立后悔界限，并通过关注机制近似参数化。

    

    本文通过回答几个开放性问题，对In-Context Learning（ICL）进行了全面的研究：(a)在语言模型中学习的是什么类型的ICL估计量？(b)适合评估ICL的性能度量是什么，并且错误率是多少？(c)Transformer架构如何实现ICL？为了回答(a)，我们采取了贝叶斯观点，并证明ICL隐含实现了贝叶斯模型平均算法。我们证明了这个贝叶斯模型平均算法可以通过关注机制近似参数化。(b)从在线学习的角度分析ICL性能，建立一个后悔界限 $\mathcal{O}(1/T)$，其中$T$是ICL输入序列长度。(c)除了在关注中编码的贝叶斯模型平均算法，我们还表明，在涉及期间，学习模型和名义模型之间的总变分距离被一个近似误差和一个泛化误差之和所界定。

    In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned within language models? (b) What are suitable performance metrics to evaluate ICL accurately and what are the error rates? (c) How does the transformer architecture enable ICL? To answer (a), we take a Bayesian view and demonstrate that ICL implicitly implements the Bayesian model averaging algorithm. This Bayesian model averaging algorithm is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a regret bound $\mathcal{O}(1/T)$, where $T$ is the ICL input sequence length. To address (c), in addition to the encoded Bayesian model averaging algorithm in attention, we show that during pertaining, the total variation distance between the learned model and the nominal model is bounded by a sum of an approximation error and a genera
    
[^40]: 通过贝叶斯主动学习实现自校正贝叶斯优化

    Self-Correcting Bayesian Optimization through Bayesian Active Learning. (arXiv:2304.11005v1 [cs.LG])

    [http://arxiv.org/abs/2304.11005](http://arxiv.org/abs/2304.11005)

    该论文提出了SAL和SCoreBO两种方法，用于提高高斯过程模型的超参数选择和贝叶斯优化的表现。

    

    高斯过程已成为贝叶斯优化和主动学习中的首选模型。然而，高斯过程的完全发挥需要巧妙选择超参数，而在文献中很少有关于找到正确超参数的努力。我们演示了选择好的超参数对于高斯过程的影响，并提出了两个明确优先考虑此目标的收购函数。统计距离主动学习（SAL）考虑后验样本的平均不一致性，由统计距离测量。结果显示，在许多测试函数上，它胜过了贝叶斯主动学习的最新结果。然后，我们引入了自校正贝叶斯优化（SCoreBO），它将SAL扩展到同时执行贝叶斯优化和主动超参数学习。相比传统BO，SCoreBO以改进的速度学习模型超参数，同时在最新的贝叶斯优化搜索中取得更好的表现。

    Gaussian processes are cemented as the model of choice in Bayesian optimization and active learning. Yet, they are severely dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding the right hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize this goal. Statistical distance-based Active Learning (SAL) considers the average disagreement among samples from the posterior, as measured by a statistical distance. It is shown to outperform the state-of-the-art in Bayesian active learning on a number of test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active hyperparameter learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization met
    
[^41]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^42]: Eryn: 一种多功能的贝叶斯推断采样器

    Eryn : A multi-purpose sampler for Bayesian inference. (arXiv:2303.02164v2 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2303.02164](http://arxiv.org/abs/2303.02164)

    Eryn是一个多功能的贝叶斯推断采样器，通过集成多种概念和算法，提供了一个单独的MCMC包，用于解决物理学中的参数估计和模型选择问题。

    

    近年来，贝叶斯推断方法在物理学的许多不同问题中被广泛应用，其中检测和表征是必要的。引力波天文学中的数据分析就是一个典型的例子。贝叶斯推断非常成功，因为该技术提供了参数的后验概率分布表示，不确定性由实验测量的精确度提供信息。在过去的几十年中，许多具体的进展已经被提出并应用于解决各种不同的问题。在这项工作中，我们提出了一个马尔科夫链蒙特卡洛（MCMC）算法，将这些概念集成到一个单独的MCMC包中。为此，我们构建了一个用户友好且多功能的贝叶斯推断工具箱Eryn，可用于解决参数估计和模型选择问题，从简单的推断问题到复杂的模型选择问题。

    In recent years, methods for Bayesian inference have been widely used in many different problems in physics where detection and characterization are necessary. Data analysis in gravitational-wave astronomy is a prime example of such a case. Bayesian inference has been very successful because this technique provides a representation of the parameters as a posterior probability distribution, with uncertainties informed by the precision of the experimental measurements. During the last couple of decades, many specific advances have been proposed and employed in order to solve a large variety of different problems. In this work, we present a Markov Chain Monte Carlo (MCMC) algorithm that integrates many of those concepts into a single MCMC package. For this purpose, we have built {\tt Eryn}, a user-friendly and multipurpose toolbox for Bayesian inference, which can be utilized for solving parameter estimation and model selection problems, ranging from simple inference questions, to those w
    
[^43]: 过度参数化会导致梯度下降在学习单个神经元时的收敛速度指数级减慢

    Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron. (arXiv:2302.10034v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10034](http://arxiv.org/abs/2302.10034)

    通过研究学习单个神经元的过度参数化设置，本研究发现梯度下降算法的收敛速度会以指数级减慢，是首个给出该问题全局收敛结果的研究。通过证明上下界，我们精确刻画出了收敛速度，并指出了过度参数化对于收敛速度的影响。

    

    我们重新审视了在具有ReLU激活函数和方形损失的高斯输入下学习单个神经元的问题。我们特别关注学生网络具有n≥2个神经元的过度参数化设置。我们证明了随机初始化的梯度下降算法以O(T^-3)的速度全局收敛。这是对于该问题的第一个超过精确参数化设置(n=1)的全局收敛结果，其中梯度下降算法呈现出exp(-Ω(T))的速度。令人惊讶的是，我们进一步证明了在过度参数化设置中，随机初始化的梯度流算法的下界是Ω(T^-3)。这两个下界共同给出了收敛速度的精确刻画，并首次暗示了过度参数化会使收敛速度指数级减慢。为了证明全局收敛，我们需要处理梯度下降动力学中学生神经元之间的相互作用，这在精确参数化设置中不存在。

    We revisit the problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\left(T^{-3}\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\exp(-\Omega(T))$ rate. Perhaps surprisingly, we further present an $\Omega\left(T^{-3}\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that over-parameterization can exponentially slow down the convergence rate. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in
    
[^44]: 《超越批处理二元分类的量子学习理论》

    Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07409](http://arxiv.org/abs/2302.07409)

    这篇论文通过研究量子学习理论拓展了批处理多类学习、在线布尔学习和在线多类学习，并提出了第一个具有量子示例的在线学习模型。

    

    Arunachalam和de Wolf（2018）证明了在可实现和糊涂设置下，量子批处理学习布尔函数的样本复杂性与相应的经典样本复杂性具有相同的形式和数量级。在本文中，我们将这个明显令人惊讶的结果推广到了批处理多类学习、在线布尔学习和在线多类学习。对于我们的在线学习结果，我们首先考虑了Dawid和Tewari（2022）经典模型的自适应对手变体。然后，我们引入了第一个（据我们所知）具有量子示例的在线学习模型。

    Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
    
[^45]: 强大的层级增强学习中的知识传输

    Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05534](http://arxiv.org/abs/2302.05534)

    本文研究了层级增强学习中的知识传输，提出了一种新颖的在线学习算法，在没有先验知识的任务相似性的情况下实现强大的知识传输。

    

    本文研究了层级增强学习设置，这是一个并行传输学习框架，在这个框架中，目标是将知识从低层（源）任务传输到高层（目标）任务，以减少后者的探索风险，同时并行解决这两个任务。与先前的工作不同，我们不假设低层和高层任务共享相同的动态或奖励函数，并且专注于在没有先验知识的任务相似性的情况下实现强大的知识传输。我们确定了一个称为“最优值支配”的自然而必要的条件，适用于我们的目标。在这个条件下，我们提出了一种新颖的在线学习算法，使得对于高层任务，在部分状态上可以实现恒定的遗憾，这取决于任务相似性，并在两个任务不相似时保持接近最优遗憾；而对于低层任务，它可以在不做出牺牲的情况下保持接近最优。此外，我们进一步研究了具有多个低层任务的情况。

    In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
    
[^46]: 在高维度中利用神经网络逼近连续函数，并应用于逆问题

    Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems. (arXiv:2208.13305v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.13305](http://arxiv.org/abs/2208.13305)

    该论文提出了一种通用的方法，用于解决神经网络在高维空间中逼近连续函数的问题，并对理论与实践之间的差距进行了缩小。该方法基于Johnson-Lindenstrauss嵌入的观察，通过将高维集合嵌入到低维空间中，使得较小的神经网络可以有效逼近高维连续函数。

    

    在过去的十年中，神经网络在各个领域中的逆问题中取得了显著的成功，这推动了它们在医学影像到地震分析等领域的采用。然而，这些逆问题的高维度使得现有理论无法解释在实践中为什么使用看似较小的网络也能得到良好的效果。为了缩小理论与实践之间的差距，我们提供了一种通用的方法来界定神经网络逼近高维集合上的H\"older（或均匀）连续函数所需的复杂度。这种方法基于一个观察：给定一个高维集合$S\subset\mathbb{R}^D$，如果存在一个Johnson-Lindenstrauss嵌入$A\in\mathbb{R}^{d\times D}$，其中$d$较小，将$S$嵌入到一个低维立方体$[-M,M]^d$中，那么对于任何H\"o

    The remarkable successes of neural networks in a huge variety of inverse problems have fueled their adoption in disciplines ranging from medical imaging to seismic analysis over the past decade. However, the high dimensionality of such inverse problems has simultaneously left current theory, which predicts that networks should scale exponentially in the dimension of the problem, unable to explain why the seemingly small networks used in these settings work as well as they do in practice. To reduce this gap between theory and practice, we provide a general method for bounding the complexity required for a neural network to approximate a H\"older (or uniformly) continuous function defined on a high-dimensional set with a low-complexity structure. The approach is based on the observation that the existence of a Johnson-Lindenstrauss embedding $A\in\mathbb{R}^{d\times D}$ of a given high-dimensional set $S\subset\mathbb{R}^D$ into a low dimensional cube $[-M,M]^d$ implies that for any H\"o
    
[^47]: 多个随机特征模型中的多次下降现象

    Multiple Descent in the Multiple Random Feature Model. (arXiv:2208.09897v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2208.09897](http://arxiv.org/abs/2208.09897)

    本文研究了过度参数化学习中的双下降现象，并在多组分预测模型中进一步探究了多次下降现象。通过理论计算和实验证实，随机特征模型的风险曲线可以呈现出三次下降。

    

    最近的研究表明，过度参数化学习中存在双下降现象。尽管这一现象已经得到了研究，但在理论上尚未完全理解。本文研究了多组分预测模型中的多次下降现象。首先考虑一个“双随机特征模型” (DRFM)，它连接了两种类型的随机特征，并研究了DRFM在岭回归中实现的过度风险。我们计算了高维框架下，当训练样本量、数据维度和随机特征维度趋向于无穷大时，过度风险的精确极限。基于这个计算，我们进一步从理论上证明了DRFM的风险曲线可能呈现出三次下降。然后我们进行了深入的实验研究，以验证我们的理论。最后，我们将研究扩展到“多随机特征模型” (MRFM)，并展示了MRFMs集成K种类型的情况。

    Recent works have demonstrated a double descent phenomenon in over-parameterized learning. Although this phenomenon has been investigated by recent works, it has not been fully understood in theory. In this paper, we investigate the multiple descent phenomenon in a class of multi-component prediction models. We first consider a ''double random feature model'' (DRFM) concatenating two types of random features, and study the excess risk achieved by the DRFM in ridge regression. We calculate the precise limit of the excess risk under the high dimensional framework where the training sample size, the dimension of data, and the dimension of random features tend to infinity proportionally. Based on the calculation, we further theoretically demonstrate that the risk curves of DRFMs can exhibit triple descent. We then provide a thorough experimental study to verify our theory. At last, we extend our study to the ''multiple random feature model'' (MRFM), and show that MRFMs ensembling $K$ types
    
[^48]: 无遗憾学习与无界损失：对数池化的情况

    No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling. (arXiv:2202.11219v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11219](http://arxiv.org/abs/2202.11219)

    本文研究了在线对抗环境中，使用对数池化方法学习专家权重的问题。我们提出了一种基于在线镜像下降算法的方法，以达到无遗憾保证。

    

    在每个时间步长$T$中，$m$个专家报告了关于$n$个结果的概率分布；我们希望学习一种聚合这些预测的方法，以达到无遗憾保证。我们关注一种被称为对数池化的基本和实用的聚合方法——log odds 的加权平均，它在某种意义上是一种最优的池化方法选择，如果我们希望最小化 log 损失（作为我们的损失函数）。我们考虑在线对抗环境中学习最佳参数集（即专家权重）的问题。我们假设（必要条件下），对抗选择的结果和预测是一致的，也就是说专家报告了经过校准的预测。施加这个约束条件创建了一个（据我们所知）新颖的半对抗设置，其中对手保留了大量的灵活性。在这个设置下，我们提出了一种基于在线镜像下降的算法，以一种学习专家权重的方式，实现了$O(\s

    For each of $T$ time steps, $m$ experts report probability distributions over $n$ outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as logarithmic pooling -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains $O(\s
    
[^49]: 多共识分散加速梯度下降

    Multi-consensus Decentralized Accelerated Gradient Descent. (arXiv:2005.00797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.00797](http://arxiv.org/abs/2005.00797)

    本文提出了一种新颖的多共识分散加速梯度下降算法，实现了最优计算复杂度和接近最优的通信复杂度。算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。实证研究表明该方法在机器学习应用中超越其他方法。

    

    本文考虑了分散凸优化问题，在大规模机器学习、传感器网络和控制理论等领域有广泛应用。我们提出了一种新颖的算法，实现了最优计算复杂度和接近最优的通信复杂度。我们的理论结果对于一个开放问题给出了肯定的答案，即是否存在一种算法可以实现与全局条件数相关而不是局部条件数相关的通信复杂度的(lower bound)相匹配。此外，我们的算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。我们的方法设计依赖于Nesterov 加速、多共识和梯度追踪等众所周知的技术的创新整合。实证研究显示了我们的方法在机器学习应用中的优越性能。

    This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does \emph{not} require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov's acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.
    

