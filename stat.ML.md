# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection](https://rss.arxiv.org/abs/2402.01635) | 本文介绍了一种利用kNN算法进行条件均值和方差估计的方法，该方法采用了自动不确定性量化和变量选择技术，提高了估计的准确性和性能。 |
| [^2] | [Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type](https://rss.arxiv.org/abs/2402.01632) | 这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。 |
| [^3] | [Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction](https://rss.arxiv.org/abs/2402.01629) | 这篇论文提出了一个通用框架，其中使用广义语法规则（GGRs）来实现组合一般化，将其视为转导任务中的对称性约束。该框架不仅形式化了语言转导的广义对称性概念，还与强化学习和其他研究领域有关联。 |
| [^4] | [L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders](https://rss.arxiv.org/abs/2402.01614) | L2G2G是一种可扩展的局部到全局网络嵌入方法，通过动态同步潜在节点表示以提高GAE的准确性，并利用解码器计算只有本地图块损失，从而更好地利用了图的信息。 |
| [^5] | [Hyperparameter tuning via trajectory predictions: Stochastic prox-linear methods in matrix sensing](https://rss.arxiv.org/abs/2402.01599) | 本论文分析了一种小批量近似线性算法，在矩阵感知问题中，通过预测误差和确定性递归，展示了如何进行非渐近的超参数调优。分析结果显示，该方法是随机的，但可以从一个本地初始化收敛到一个统计误差边界，并揭示了批量大小、步长和噪声水平对收敛速度和统计估计误差的影响。 |
| [^6] | [Deep Active Learning for Data Mining from Conflict Text Corpora](https://rss.arxiv.org/abs/2402.01577) | 本文提出了一种利用深度主动学习从冲突文本语料库中进行数据挖掘的方法。通过迭代的主动学习过程，结合大型的仅编码器语言模型，可以提取与冲突动态相关的子类事件，达到类似于人类的性能。 |
| [^7] | [Adaptive Optimization for Prediction with Missing Data](https://rss.arxiv.org/abs/2402.01543) | 本文提出了一种针对缺失数据预测的自适应优化方法，通过自适应线性回归模型来适应观测特征集，并将填充规则和回归模型同时学习，相比顺序学习方法，在数据非完全随机缺失情况下，方法实现了2-10%的准确性改进。 |
| [^8] | [Mapping the Multiverse of Latent Representations](https://rss.arxiv.org/abs/2402.01514) | 提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。 |
| [^9] | [Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers](https://rss.arxiv.org/abs/2402.01502) | 本文将树集成解释为自适应的自正则化平滑器，通过量化预测的平滑程度并根据测试和训练输入的差异调节平滑性，提供了对树集成成功驱动因素的新见解。 |
| [^10] | [Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates](https://rss.arxiv.org/abs/2402.01493) | 这种方法提出了一种新的蒙特卡罗方法，使用球谐函数作为控制变量来近似计算切片华瑟斯坦距离。与蒙特卡罗相比，该方法具有更好的收敛速度和理论性质。 |
| [^11] | [Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?](https://rss.arxiv.org/abs/2402.01484) | 通过揭示权重和函数空间之间的关系，我们成功实现了贝叶斯神经网络的可行的基于样本推理，并提出了一种有效的贝叶斯深度集成方法来解决采样和收敛问题。 |
| [^12] | [Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes](https://rss.arxiv.org/abs/2402.01476) | 本论文提出了自核-特征对稀疏变分高斯过程（KEP-SVGP）用于构建具有不确定性感知的自注意力。通过核SVD（KSVD）解决了注意力核的不对称性，并实现了降低的复杂度。 |
| [^13] | [Deep Conditional Generative Learning: Model and Error Analysis](https://rss.arxiv.org/abs/2402.01460) | 提出了一种基于ODE的深度生成方法，通过条件Follmer流来学习条件分布，通过离散化和深度神经网络实现高效转化。同时，通过Wasserstein距离的非渐近收敛速率，提供了第一个端到端误差分析，数值实验证明其在不同场景下的优越性。 |
| [^14] | [Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach](https://rss.arxiv.org/abs/2402.01454) | 本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。 |
| [^15] | [Improving importance estimation in covariate shift for providing accurate prediction error](https://rss.arxiv.org/abs/2402.01450) | 该论文研究了在协变量偏移问题中改进重要性估计以提高预测误差的准确性。 |
| [^16] | [Conditioning non-linear and infinite-dimensional diffusion processes](https://rss.arxiv.org/abs/2402.01434) | 本文探索了在无穷维空间中对非线性过程进行条件约束的方法，并应用于进化生物学中的生物形态时间序列分析。 |
| [^17] | [Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization](https://rss.arxiv.org/abs/2402.01401) | 通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。 |
| [^18] | [Query-Efficient Correlation Clustering with Noisy Oracle](https://rss.arxiv.org/abs/2402.01400) | 本论文提出了一种低查询成本的聚类方法，利用纯在组合多臂赌博机探索范式实现在线学习，并设计了能在NP-hard情况下运行的多项式时间算法。 |
| [^19] | [A Probabilistic Model to explain Self-Supervised Representation Learning](https://rss.arxiv.org/abs/2402.01399) | 该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。 |
| [^20] | [Emergence of heavy tails in homogenized stochastic gradient descent](https://rss.arxiv.org/abs/2402.01382) | 这项研究分析了齐次化随机梯度下降算法，在数值实验中验证了参数尾指数的明确上下界，并量化了优化参数与尾指数之间的相互作用，从而为重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力之间的关系提供了贡献。 |
| [^21] | [Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion](https://rss.arxiv.org/abs/2402.01342) | 本文提出了一个在训练过程中进行神经元对齐的方法，通过置换子空间减少了线性模块连通性的局限性，为模型融合算法的改进提供了可能性。 |
| [^22] | [Fundamental Properties of Causal Entropy and Information Gain](https://rss.arxiv.org/abs/2402.01341) | 本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。 |
| [^23] | [Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum](https://rss.arxiv.org/abs/2402.01297) | 我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。 |
| [^24] | [Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape](https://rss.arxiv.org/abs/2402.01258) | 本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。 |
| [^25] | [MIQCQP reformulation of the ReLU neural networks Lipschitz constant estimation problem](https://rss.arxiv.org/abs/2402.01199) | 本文提出了一种新的方法，通过引入新的约束条件，使用二次约束混合整数规划（MIQCQP）来估计ReLU神经网络的Lipschitz常数。这些问题的解可以给出Lipschitz常数的下界和上界，并且当特定条件满足时，它们与精确的Lipschitz常数重合。 |
| [^26] | [The Optimality of Kernel Classifiers in Sobolev Space](https://rss.arxiv.org/abs/2402.01148) | 本文研究了核分类器在Sobolev空间中的最优性质，并通过对条件概率的假设和核回归理论的应用，导出了核分类器的分类超额风险上界和Sobolev空间的极小极大下界。此外，我们还提出了一种简单方法来估计插值平滑度，并将其应用于实际数据集。 |
| [^27] | [Learning Network Representations with Disentangled Graph Auto-Encoder](https://rss.arxiv.org/abs/2402.01143) | 本文介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。 |
| [^28] | [Online conformal prediction with decaying step sizes](https://rss.arxiv.org/abs/2402.01139) | 本文介绍了一种在线自适应预测方法，通过使用递减步长来改进在任意序列上的覆盖率保证，并且能够同时估计总体分位数。 |
| [^29] | [Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints](https://rss.arxiv.org/abs/2402.01111) | 本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\widetilde{O}(\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\log\log K)$。此外，还给出了所有具有$\widetilde{O}(\sqrt{K})$后悔界算法的批量复杂度下界。 |
| [^30] | [Bayesian Deep Learning for Remaining Useful Life Estimation via Stein Variational Gradient Descent](https://rss.arxiv.org/abs/2402.01098) | 本研究使用斯坦优化梯度下降算法，将标准的频率主义神经网络转化为贝叶斯神经网络，以应对预测性维护中估计剩余寿命的不确定性问题。 |
| [^31] | [How many views does your deep neural network use for prediction?](https://rss.arxiv.org/abs/2402.01095) | 本文提出了最小有效视图（MSVs）的概念，该概念类似于多视图，但适用于实际图像，并且通过实证研究表明，MSV的数量与模型的预测准确性之间存在关系。 |
| [^32] | [A Dynamical Model of Neural Scaling Laws](https://rss.arxiv.org/abs/2402.01092) | 这篇论文提出了一个动力学模型来解释神经缩放定律。通过分析梯度下降训练的随机特征模型，研究发现训练时间和模型大小的缩放具有不同的幂律指数，而计算最优缩放规则要求增加训练步数快于增加模型参数，与实证观察相一致。 |
| [^33] | [Scalable Higher-Order Tensor Product Spline Models](https://rss.arxiv.org/abs/2402.01090) | 我们提出了一种可扩展的高阶张量积样条模型，允许加入所有（高阶）非线性特征效应的相互作用，并具有与没有相互作用的模型成比例的计算成本。 |
| [^34] | [No Free Prune: Information-Theoretic Barriers to Pruning at Initialization](https://rss.arxiv.org/abs/2402.01089) | 本文解释了为什么在初始化时修剪神经网络困难，并提出了一个关于有效参数数量的理论解释。我们指出，在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。为此，我们怀疑在训练过程中和训练后修剪是必要的。 |
| [^35] | [Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures](https://rss.arxiv.org/abs/2402.01055) | 本论文提出了用于从带有噪声标签的数据中学习非可分解性能度量的多类学习算法。这些算法分别适用于单调凸性和线性比率两类性能度量，并基于类条件噪声模型进行噪声校正。 |
| [^36] | [Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation](https://rss.arxiv.org/abs/2402.01052) | 本文提出了一种关于逆问题的弱凸正则化器的收敛性问题的一般化公式，并证明了通过一类弱凸正则化器的实现可以达到收敛，并应用于学习的正则化中实现了对计算机层析成像中学习对抗性正则化器性能的提高。 |
| [^37] | [Distributed MCMC inference for Bayesian Non-Parametric Latent Block Model](https://rss.arxiv.org/abs/2402.01050) | 本文提出了一种基于分布式MCMC推理的贝叶斯非参数潜在分块模型方法，通过将观测值和特征划分为分区，并采用Master/Worker架构来提高聚类标签准确性和执行时间。 |
| [^38] | [Fisher information dissipation for time inhomogeneous stochastic differential equations](https://rss.arxiv.org/abs/2402.01036) | 本文提供了时间不齐次随机微分方程的费舍尔信息耗散方法。通过将Langevin动力学的概率转移方程构造为关于时间依赖的最优传输度量的修正梯度流，我们得到了概率密度函数的收敛性保证，并在几个时间不齐次Langevin动力学中进行了验证。 |
| [^39] | [Multivariate Probabilistic Time Series Forecasting with Correlated Errors](https://rss.arxiv.org/abs/2402.01000) | 本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。 |
| [^40] | [Credal Learning Theory](https://rss.arxiv.org/abs/2402.00957) | 本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。 |
| [^41] | [Geometry of Polynomial Neural Networks](https://rss.arxiv.org/abs/2402.00949) | 本研究利用代数几何工具研究了具有单项式激活函数的多项式神经网络的表达性和学习过程，通过对神经流形的维度和学习度的研究，提供了网络表达能力和训练复杂度的度量，并给出了可学函数数量的上界。 |
| [^42] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^43] | [Online Variational Sequential Monte Carlo](https://rss.arxiv.org/abs/2312.12616) | 本文提出了一种在线学习的算法，名为在线VSMC，它基于变分顺序蒙特卡洛方法，在处理数据流时能够实时进行模型参数估计和粒子提议适应。 |
| [^44] | [Marginal Laplacian Score](https://rss.arxiv.org/abs/2311.17795) | 边际拉普拉斯分数（MLS）是一种针对高维度不平衡数据的改进版拉普拉斯分数（LS），通过保留数据集边缘的局部结构，提供了一种有效的无监督特征选择方法。该方法被成功地集成到不同iable无监督特征选择（DUFS）算法中，在合成和公共数据集上展示了稳健且改进的性能。 |
| [^45] | [Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML](https://rss.arxiv.org/abs/2311.09200) | 通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。 |
| [^46] | [High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft](https://rss.arxiv.org/abs/2311.06130) | 本文介绍了一种创新的降维算法，利用偏最小二乘回归来减少构建混合变量高斯过程所需的超参数数量。这种方法在结构和多学科应用中表现出良好的潜力，适用于绿色飞机的优化等多个领域。 |
| [^47] | [Forward $\chi^2$ Divergence Based Variational Importance Sampling](https://rss.arxiv.org/abs/2311.02516) | 引入了一种基于前向$\chi^2$散度的变分重要抽样方法(VIS)，通过直接估计和最大化对数似然来增强对复杂后验分布的估计性能。实验证明，VIS方法在多种潜变量模型中均优于最先进的基线方法，表现出更高的对数似然和模型参数估计准确性。 |
| [^48] | [Variational Linearized Laplace Approximation for Bayesian Deep Learning](https://rss.arxiv.org/abs/2302.12565) | 本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。 |
| [^49] | [Generative Adversarial Learning of Sinkhorn Algorithm Initializations](https://rss.arxiv.org/abs/2212.00133) | 本文通过生成对抗学习的方法，训练神经网络来学习Sinkhorn算法的初始化，显著加快收敛速度，同时保持算法的可微分性和并行性，并证明了网络的普适性和独立求解能力。 |
| [^50] | [Higher-order accurate two-sample network inference and network hashing](https://rss.arxiv.org/abs/2208.07573) | 本文提出了一种新的方法和算法工具箱，解决了网络比较中的多个挑战，并具有高阶精确度和功率优化的特点。这个方法在速度和准确度方面优于现有工具，并且具有强大的理论保证。 |
| [^51] | [A Statistical Learning View of Simple Kriging](https://rss.arxiv.org/abs/2202.07365) | 本文从统计学习的视角对简单克里金方法进行了分析，解决了在大数据时代中，考虑复杂空间相关结构的大规模数据集预测问题。 |
| [^52] | [Distributional Reinforcement Learning by Sinkhorn Divergence](https://rss.arxiv.org/abs/2202.00769) | 本文提出了SinkhornDRL方法，使用Sinkhorn散度来减小当前和目标Bellman回报分布之间的差异，并通过理论证明和实证实验展示了该方法的优越性。 |
| [^53] | [Simple Imputation Rules for Prediction with Missing Data: Contrasting Theoretical Guarantees with Empirical Performance](https://rss.arxiv.org/abs/2104.03158) | 本研究对缺失数据问题进行了研究，通过对比理论和实证结果，展示了一些简单填补规则在预测任务中的性能。在广泛填补方法家族中，我们发现均值填补是最优的，而众数填补是次优的。实证结果除了支持理论发现外，还强调了理论和实践之间的差距和未来研究的机会。 |
| [^54] | [Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons.](http://arxiv.org/abs/2401.16571) | 我们提出了一种使用共享神经元的RBF网络的非参数化治疗效应估计方法，适用于多治疗设置。该方法能够建模治疗结果的共同性，并在贝叶斯框架下实现估计和推断，通过模拟实验证明了其数值性能，应用于真实临床数据后也得到了有趣的发现。 |
| [^55] | [Bayesian Optimization with Hidden Constraints via Latent Decision Models.](http://arxiv.org/abs/2310.18449) | 本文介绍了一种基于潜在决策模型的贝叶斯优化方法，通过利用变分自编码器学习可行决策的分布，在原始空间和潜在空间之间实现了双向映射，从而解决了公共决策制定中的隐藏约束问题。 |
| [^56] | [Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency.](http://arxiv.org/abs/2310.15351) | 本论文研究了在贝叶斯优化中使用随机探索的方法，并证明了其能够实现最佳的误差率和最优遗憾保证。同时，所提出的算法通过随机探索避免了每次迭代中非凸获取函数的昂贵优化，具有计算上的优势。 |
| [^57] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | 本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。 |
| [^58] | [Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach.](http://arxiv.org/abs/2310.11531) | 本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。 |
| [^59] | [Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs.](http://arxiv.org/abs/2310.10107) | 本文分析了后验采样学习算法在序列化POMDPs中的遗憾性能，并在一定条件下提供了改进的多项式贝叶斯遗憾界。 |
| [^60] | [A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network.](http://arxiv.org/abs/2310.05495) | 本论文介绍了一种基于神经切向核视角的联邦平均方法在深度线性神经网络上的应用，并探讨了该方法面临的挑战。 |
| [^61] | [A Neural Scaling Law from Lottery Ticket Ensembling.](http://arxiv.org/abs/2310.02258) | 《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。 |
| [^62] | [Deep graph kernel point processes.](http://arxiv.org/abs/2306.11313) | 本文提出了一种基于潜在图拓扑的图点过程方法，并开发了一种新颖的深度图核来描述事件之间的触发和抑制效应，该方法在合成和实际数据集上具有优越性。 |
| [^63] | [Variational Gradient Descent using Local Linear Models.](http://arxiv.org/abs/2305.15577) | 本文提出了使用局部线性模型实现目标和粒子分布KL散度降低的新估计器，可以使用样本进行计算而不需要目标得分函数，具有比SVGD更简单有效的计算方法，对于高维度情况下的模型也有优化，提升估计精度。 |
| [^64] | [I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data.](http://arxiv.org/abs/2210.13954) | 该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。 |

# 详细

[^1]: kNN算法用于条件均值和方差估计，具有自动不确定性量化和变量选择

    kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection

    [https://rss.arxiv.org/abs/2402.01635](https://rss.arxiv.org/abs/2402.01635)

    本文介绍了一种利用kNN算法进行条件均值和方差估计的方法，该方法采用了自动不确定性量化和变量选择技术，提高了估计的准确性和性能。

    

    本文介绍了一种基于kNN的回归方法，将传统的非参数kNN模型的可扩展性和适应性与一种新的变量选择技术相结合。该方法主要目标是准确估计随机响应变量的条件均值和方差，从而有效地描述各种情景下的条件分布。我们的方法包含了一个健壮的不确定性量化机制，利用我们之前关于条件均值和方差的估计工作。 kNN的应用确保了在预测区间时可扩展的计算效率和与最优非参数速率相一致的统计准确性。此外，我们引入了一种新的kNN半参数算法来估计考虑协变量的ROC曲线。对于选择平滑参数k，我们提出了一个具有理论保证的算法。变量选择的引入显著提高了该方法相对于传统方法的性能。

    In this paper, we introduce a kNN-based regression method that synergizes the scalability and adaptability of traditional non-parametric kNN models with a novel variable selection technique. This method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.Our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance. The employment of kNN ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates. Additionally, we introduce a new kNN semi-parametric algorithm for estimating ROC curves, accounting for covariates. For selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.Incorporation of variable selection enhances the performance of the method significantly over convention
    
[^2]: 超越尺度：具有任意类型未知超参数的无遗憾贝叶斯优化

    Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type

    [https://rss.arxiv.org/abs/2402.01632](https://rss.arxiv.org/abs/2402.01632)

    这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。

    

    贝叶斯优化需要拟合高斯过程模型，而拟合高斯过程模型需要指定超参数 - 大部分理论文献假设这些超参数是已知的。之前的理论研究通常假设数据在空间中均匀填充，而常用的高斯过程超参数的最大似然估计器只有在这种情况下才是一致的。然而，在贝叶斯优化中，数据不一定满足这种均匀填充的条件。由于无法保证超参数估计的正确性，并且这些超参数可以显著影响高斯过程拟合，因此对具有未知超参数的贝叶斯优化进行理论分析非常具有挑战性。之前提出的具有无遗憾特性的算法仅能处理特殊情况下的未知长度尺度、再生核希尔伯特空间范数，并且仅适用于频率派的情况。我们提出了一种新的算法，命名为HE-GP-UCB，它是第一个具有无遗憾特性的算法，在具有未知超参数的情况下实现了贝叶斯优化。

    Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
    
[^3]: 《论文标题：广义语法规则和基于结构的一般化——对于词汇任务和转导的经典等变性之外的一般化的立场论文》

    Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction

    [https://rss.arxiv.org/abs/2402.01629](https://rss.arxiv.org/abs/2402.01629)

    这篇论文提出了一个通用框架，其中使用广义语法规则（GGRs）来实现组合一般化，将其视为转导任务中的对称性约束。该框架不仅形式化了语言转导的广义对称性概念，还与强化学习和其他研究领域有关联。

    

    组合一般化是人类学习词汇与现有神经网络之间的主要差异之一。我们提出了一个通用框架，用于构建能够使用广义语法规则（GGRs）进行组合一般化的模型，GGRs是一类基于对称性的转导任务的组合约束，我们将其视为受物理学任务中等变性约束启发的转导类比。除了为语言转导形式化广义的对称性概念之外，我们的框架足够通用，可以包含许多现有工作作为特例。我们提出了关于如何实现GGRs的想法，并在此过程中与强化学习和其他研究领域建立了联系。

    Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.
    
[^4]: L2G2G: 一种可扩展的局部到全局网络嵌入方法，基于图自动编码器

    L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders

    [https://rss.arxiv.org/abs/2402.01614](https://rss.arxiv.org/abs/2402.01614)

    L2G2G是一种可扩展的局部到全局网络嵌入方法，通过动态同步潜在节点表示以提高GAE的准确性，并利用解码器计算只有本地图块损失，从而更好地利用了图的信息。

    

    对于分析实际网络，图表示学习是一种常用工具。这些方法，如图自动编码器(GAE)，通常依赖于低维表示，也称为嵌入，通过最小化损失函数获得;这些嵌入与解码器一起用于节点分类和边预测等下游任务。虽然GAE往往相当准确，但存在可扩展性问题。为了改善速度，Local2Global方法通过基于特征向量同步的图块嵌入相结合，显示出快速且准确的效果。在这里，我们提出了L2G2G，一种_Local2Global方法，它在不牺牲可扩展性的情况下提高了GAE的准确性。这种改进是通过在训练GAE期间动态同步潜在节点表示来实现的。它还受益于解码器计算只有本地图块损失。因此，每个时代中的本地嵌入对齐利用了更多来自图的信息。

    For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the gr
    
[^5]: 通过轨迹预测进行超参数调优：矩阵感知中的随机近似线性方法

    Hyperparameter tuning via trajectory predictions: Stochastic prox-linear methods in matrix sensing

    [https://rss.arxiv.org/abs/2402.01599](https://rss.arxiv.org/abs/2402.01599)

    本论文分析了一种小批量近似线性算法，在矩阵感知问题中，通过预测误差和确定性递归，展示了如何进行非渐近的超参数调优。分析结果显示，该方法是随机的，但可以从一个本地初始化收敛到一个统计误差边界，并揭示了批量大小、步长和噪声水平对收敛速度和统计估计误差的影响。

    

    本论文针对从被噪声污染的秩1高斯测量中恢复未知秩1矩阵的问题，分析了一种迭代的小批量近似线性算法，并推导出一种确定性递归来预测该方法的误差。使用非渐近的框架，我们证明了对于任何批量大小和大范围的步长，这个预测是准确的。特别是，我们的分析揭示了这个方法，尽管是随机的，从一个本地初始化通过固定步长线性收敛到一个统计误差边界。我们的分析还揭示了批量大小、步长和噪声水平如何影响（线性）收敛速度和最终的统计估计误差，并展示了如何利用我们的确定性预测来进行超参数调优（如步长和批量大小的选择）而无需运行元算法。

    Motivated by the desire to understand stochastic algorithms for nonconvex optimization that are robust to their hyperparameter choices, we analyze a mini-batched prox-linear iterative algorithm for the problem of recovering an unknown rank-1 matrix from rank-1 Gaussian measurements corrupted by noise. We derive a deterministic recursion that predicts the error of this method and show, using a non-asymptotic framework, that this prediction is accurate for any batch-size and a large range of step-sizes. In particular, our analysis reveals that this method, though stochastic, converges linearly from a local initialization with a fixed step-size to a statistical error floor. Our analysis also exposes how the batch-size, step-size, and noise level affect the (linear) convergence rate and the eventual statistical estimation error, and we demonstrate how to use our deterministic predictions to perform hyperparameter tuning (e.g. step-size and batch-size selection) without ever running the met
    
[^6]: 来自冲突文本语料库的深度主动学习数据挖掘

    Deep Active Learning for Data Mining from Conflict Text Corpora

    [https://rss.arxiv.org/abs/2402.01577](https://rss.arxiv.org/abs/2402.01577)

    本文提出了一种利用深度主动学习从冲突文本语料库中进行数据挖掘的方法。通过迭代的主动学习过程，结合大型的仅编码器语言模型，可以提取与冲突动态相关的子类事件，达到类似于人类的性能。

    

    高分辨率的武装冲突事件数据以及相关过程已经通过UCDP GED、ACLED等数据集彻底改变了政治争论的研究。然而，大部分这些数据集仅限于收集时空（高分辨率）和强度数据。关于目标、战术、目的等动态信息很少被收集，这是因为数据收集的工作量非常大。然而，大部分数据集依赖于丰富的文本数据库，可以进一步挖掘与每个事件相关的更多信息。本文提出了一种廉价且高性能的方法，利用主动学习来改进机器学习模型，通过顺序（有导向的）人工输入的迭代过程。然后，使用主动学习逐步训练（微调）一个大型的仅编码器语言模型，以提取与冲突动态相关的子类事件。该方法表现出与人类（金标准）相似的性能。

    High-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like UCDP GED, ACLED etc. However, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data. Information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data. However, most datasets rely on a rich corpus of textual data allowing further mining of further information connected to each event. This paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input. Active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics. The approach shows performance similar to human (gold-standar
    
[^7]: 针对缺失数据预测的自适应优化方法

    Adaptive Optimization for Prediction with Missing Data

    [https://rss.arxiv.org/abs/2402.01543](https://rss.arxiv.org/abs/2402.01543)

    本文提出了一种针对缺失数据预测的自适应优化方法，通过自适应线性回归模型来适应观测特征集，并将填充规则和回归模型同时学习，相比顺序学习方法，在数据非完全随机缺失情况下，方法实现了2-10%的准确性改进。

    

    在训练具有缺失条目的预测模型时，最常用和多功能的方法是一种流水线技术，首先填充缺失条目，然后计算预测结果。本文将缺失数据预测视为一个两阶段的自适应优化问题，并提出了一种新的模型类别，自适应线性回归模型，其中回归系数能够适应观测特征集。我们表明一些自适应线性回归模型等同于同时学习填充规则和下游线性回归模型而不是顺序学习。我们利用这种联合填充-回归的解释将我们的框架推广到非线性模型。在数据非完全随机缺失的情况下，我们的方法在样外准确性方面实现了2-10%的改进。

    When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
    
[^8]: 映射潜在表示的多元宇宙

    Mapping the Multiverse of Latent Representations

    [https://rss.arxiv.org/abs/2402.01514](https://rss.arxiv.org/abs/2402.01514)

    提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。

    

    响应最近对通过多元宇宙分析来应对机器学习中的可靠性和稳健性问题的呼吁，我们提出了PRESTO，一种系统的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。尽管这些模型得到了广泛的应用，但对它们嵌入的变异性仍然不被充分理解，导致了不必要的复杂性和不可靠的表示。我们的框架使用持续同调来表征不同组合的多样化机器学习方法、(超)参数配置和数据集所产生的潜在空间，从而使我们能够测量它们之间的成对(非)相似性并对其分布进行统计推理。正如我们在理论上和实证上所证明的那样，我们的流程保持了潜在表示集合的理想特性，可以用于进行敏感性分析、检测异常嵌入或高效有效地导航超参。

    Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperpa
    
[^9]: 为什么随机森林有效？将树集成解释为自适应的自正则化平滑器的理解

    Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers

    [https://rss.arxiv.org/abs/2402.01502](https://rss.arxiv.org/abs/2402.01502)

    本文将树集成解释为自适应的自正则化平滑器，通过量化预测的平滑程度并根据测试和训练输入的差异调节平滑性，提供了对树集成成功驱动因素的新见解。

    

    尽管树集成在效果和广泛应用方面表现出卓越的能力，但其成功的驱动因素尚未完全被理解。在本文中，我们强调将树集成解释为自适应的自正则化平滑器可以提供新的直觉和更深入的见解。我们利用这个观点来展示，当将随机化树集成视为平滑器时，它们的预测不仅比它们所包含的单个树的预测更加平滑，而且还根据测试和训练输入之间的差异在测试时进一步调节它们的平滑性。首先，我们利用这个洞察力重新审视、精炼和协调了最近两个对森林成功的解释，通过测量所暗示的平滑程度来客观地量化树集成的猜想行为方式。然后，我们超越了现有的关于树集成改进机制的解释

    Despite their remarkable effectiveness and broad application, the drivers of success underlying ensembles of trees are still not fully understood. In this paper, we highlight how interpreting tree ensembles as adaptive and self-regularizing smoothers can provide new intuition and deeper insight to this topic. We use this perspective to show that, when studied as smoothers, randomized tree ensembles not only make predictions that are quantifiably more smooth than the predictions of the individual trees they consist of, but also further regulate their smoothness at test-time based on the dissimilarity between testing and training inputs. First, we use this insight to revisit, refine and reconcile two recent explanations of forest success by providing a new way of quantifying the conjectured behaviors of tree ensembles objectively by measuring the effective degree of smoothing they imply. Then, we move beyond existing explanations for the mechanisms by which tree ensembles improve upon in
    
[^10]: 使用球谐函数作为控制变量的切片华瑟斯坦估计

    Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates

    [https://rss.arxiv.org/abs/2402.01493](https://rss.arxiv.org/abs/2402.01493)

    这种方法提出了一种新的蒙特卡罗方法，使用球谐函数作为控制变量来近似计算切片华瑟斯坦距离。与蒙特卡罗相比，该方法具有更好的收敛速度和理论性质。

    

    切片华瑟斯坦（SW）距离是概率测度之间的华瑟斯坦距离的平均值，结果为相关的一维投影的华瑟斯坦距离。因此，SW距离可以写成对球面上均匀测度的积分，并且可以使用蒙特卡罗框架来计算SW距离。球谐函数是球面上的多项式，它们构成了球面上可积函数集合的正交基。将这两个事实结合在一起，提出了一种新的蒙特卡罗方法，称为球谐控制变量（SHCV），用于使用球谐函数作为控制变量近似计算SW距离。结果表明，该方法具有良好的理论性质，例如在变量之间存在一定形式的线性依赖时，混合高斯测度的无误差特性。此外，与蒙特卡罗相比，得到了更快的收敛速度。

    The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for g
    
[^11]: 连接点：模式连接是否是贝叶斯神经网络可行的基于样本推理的关键？

    Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?

    [https://rss.arxiv.org/abs/2402.01484](https://rss.arxiv.org/abs/2402.01484)

    通过揭示权重和函数空间之间的关系，我们成功实现了贝叶斯神经网络的可行的基于样本推理，并提出了一种有效的贝叶斯深度集成方法来解决采样和收敛问题。

    

    在贝叶斯神经网络的基于样本推理（SBI）中，网络参数空间的大小和结构是一个主要挑战。我们的研究表明，通过接受权重和函数空间之间的特征关系，成功实现SBI是可能的，揭示了过度参数化和采样问题困难之间的系统联系。通过大量实验，我们建立了采样和收敛诊断的实际指南。因此，我们提出了一种贝叶斯深度集成方法作为一种有效的解决方案，具有竞争性能和不确定性量化能力。

    A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a Bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification.
    
[^12]: 自核-特征对稀疏变分高斯过程中的自注意力

    Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes

    [https://rss.arxiv.org/abs/2402.01476](https://rss.arxiv.org/abs/2402.01476)

    本论文提出了自核-特征对稀疏变分高斯过程（KEP-SVGP）用于构建具有不确定性感知的自注意力。通过核SVD（KSVD）解决了注意力核的不对称性，并实现了降低的复杂度。

    

    尽管Transformer具有显著提高预测准确性的能力，但它也可能产生过于自信的预测，并需要校准的不确定性估计，这通常可以通过高斯过程（GPs）来解决。现有的工作将对称核应用于变分推断下的注意力核；然而，忽略了注意力核本质上是不对称的事实。此外，推导出大规模数据的GP后验的复杂度仍然很高。在这项工作中，我们提出了一种用于构建具有不确定性感知的自注意力的核-特征对稀疏变 分高斯过程（KEP-SVGP），其中通过核SVD（KSVD）解决了注意力核的不对称性，并获得了降低的复杂度。通过KEP-SVGP，i）由于与注意力核的KSVD相对应的两组奇异向量引导的SVGP对完全表征了不对称性；ii）仅使用少量与KSVD相对应的伴随特征函数，推导SVGP后验概率密度可以实现较低的复杂度。

    While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
    
[^13]: 深度条件生成学习：模型与误差分析

    Deep Conditional Generative Learning: Model and Error Analysis

    [https://rss.arxiv.org/abs/2402.01460](https://rss.arxiv.org/abs/2402.01460)

    提出了一种基于ODE的深度生成方法，通过条件Follmer流来学习条件分布，通过离散化和深度神经网络实现高效转化。同时，通过Wasserstein距离的非渐近收敛速率，提供了第一个端到端误差分析，数值实验证明其在不同场景下的优越性。

    

    我们介绍了一种基于普通微分方程（ODE）的深度生成方法，用于学习条件分布，称为条件Follmer流。从标准高斯分布开始，所提出的流能够以高效的方式将其转化为目标条件分布，在时间1处达到稳定。为了有效实现，我们使用欧拉方法对流进行离散化，使用深度神经网络非参数化估计速度场。此外，我们导出了学习样本的分布与目标分布之间的Wasserstein距离的非渐近收敛速率，在条件分布学习中提供了第一个全面的端到端误差分析。我们的数值实验展示了它在一系列情况下的有效性，从标准的非参数化条件密度估计问题到涉及图像数据的更复杂的挑战，说明它优于各种现有方法。

    We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow. Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network. Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow. Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing condition
    
[^14]: 在因果发现中集成大型语言模型: 一种统计因果方法

    Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach

    [https://rss.arxiv.org/abs/2402.01454](https://rss.arxiv.org/abs/2402.01454)

    本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。

    

    在实际的统计因果发现（SCD）中，将领域专家知识作为约束嵌入到算法中被广泛接受，因为这对于创建一致有意义的因果模型是重要的，尽管识别背景知识的挑战被认可。为了克服这些挑战，本文提出了一种新的因果推断方法，即通过将LLM的“统计因果提示（SCP）”与SCD方法和基于知识的因果推断（KBCI）相结合，对SCD进行先验知识增强。实验证明，GPT-4可以使LLM-KBCI的输出与带有LLM-KBCI的先验知识的SCD结果接近真实情况，如果GPT-4经历了SCP，那么SCD的结果还可以进一步改善。而且，即使LLM不含有数据集的信息，LLM仍然可以通过其背景知识来改进SCD。

    In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
    
[^15]: 在协变量偏移中改进重要性估计以提供准确的预测误差

    Improving importance estimation in covariate shift for providing accurate prediction error

    [https://rss.arxiv.org/abs/2402.01450](https://rss.arxiv.org/abs/2402.01450)

    该论文研究了在协变量偏移问题中改进重要性估计以提高预测误差的准确性。

    

    在传统的机器学习中，算法的预测基于训练集和测试集中的数据遵循相同的分布的假设。然而，在现实世界的数据中，这个条件并不成立，例如，协变量的分布发生了变化，而目标的条件分布保持不变。这种情况被称为协变量偏移问题，标准误差估计可能不再准确。在这种情况下，重要性是一种常用的度量，用于减轻协变量偏移对误差估计的影响。主要的缺点是它不容易计算。Kullback-Leibler重要性估计过程（KLIEP）能够以一种有希望的方式估计重要性。尽管它的性能很好，但它无法忽略目标信息，因为它只包括用于计算重要性的协变量信息。在这个方向上，本文探讨了如果在计算重要性时包括目标信息的潜在性能改进。

    In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. This situation is called covariate shift problem where standard error estimation may be no longer accurate. In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. The main drawback is that it is not easy to compute. The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way. Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. In this direction, this paper explores the potential performance improvement if 
    
[^16]: 随机非线性与无穷维扩散过程的条件约束

    Conditioning non-linear and infinite-dimensional diffusion processes

    [https://rss.arxiv.org/abs/2402.01434](https://rss.arxiv.org/abs/2402.01434)

    本文探索了在无穷维空间中对非线性过程进行条件约束的方法，并应用于进化生物学中的生物形态时间序列分析。

    

    生成性扩散模型和许多科学和工程中的随机模型在离散化之前自然地存在于无穷维空间中。为了将观测数据纳入统计和学习任务中，需要对观测值进行条件约束。近期的研究已经处理了在无穷维空间中对线性过程进行条件约束的问题，但尚未探索在无穷维空间中对非线性过程进行条件约束的方法。本文提出了一种在无先验离散化的情况下对函数值随机过程进行条件约束的方法。为此，我们使用了Girsanov定理的无穷维版本来对函数值随机过程进行条件约束，从而得到了涉及得分的条件过程的随机微分方程(SDE)。我们将这种技术应用于进化生物学中的生物形态时间序列分析中，通过Fourier基函数离散化，然后利用得分匹配方法学习得分函数的系数。

    Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.
    
[^17]: 通过Lipschitz正则化在规模上实现零样本机器遗忘

    Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization

    [https://rss.arxiv.org/abs/2402.01401](https://rss.arxiv.org/abs/2402.01401)

    通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。

    

    为了遵守人工智能和数据规定，从训练得到的机器学习模型中遗忘私人或受版权保护的信息的需求变得越来越重要。遗忘的关键挑战是及时忘记必要的数据，同时保持模型性能。在这项工作中，我们解决了零样本遗忘的场景，即只有一个经过训练的模型和要遗忘的数据，遗忘算法必须能够移除数据。根据这样定义，现有的最先进的方法是不够的。基于Lipschitz连续性的概念，我们提出了一种方法，通过对样本扰动的输出进行平滑处理来诱导遗忘。我们展示了这种平滑性成功地实现了遗忘，同时保持了总体模型性能。我们对我们的方法进行了广泛的经验评估，包括一系列当代基准测试，验证了我们的方法在严格的零样本约束下达到了最先进的性能。

    To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
    
[^18]: 低查询成本带噪声or同时聚类

    Query-Efficient Correlation Clustering with Noisy Oracle

    [https://rss.arxiv.org/abs/2402.01400](https://rss.arxiv.org/abs/2402.01400)

    本论文提出了一种低查询成本的聚类方法，利用纯在组合多臂赌博机探索范式实现在线学习，并设计了能在NP-hard情况下运行的多项式时间算法。

    

    我们研究了一个常见的聚类设置，其中我们需要对n个元素进行聚类，并且我们的目标是尽可能少地向返回两个元素相似性的有噪声的oracle查询。我们的设置涵盖了许多应用领域，在这些领域中，相似性函数计算起来成本高并且 inherently noisy。我们提出了两种基于纯在组合多臂赌博机探索范式(PE-CMAB)的在线学习问题的新颖表达方法固定置信度和固定预算设置。对于这两种设置，我们设计了将抽样策略与经典的相关聚类近似算法相结合的算法，并研究了它们的理论保证。我们的结果是这样的：这些算法是第一个在底层离线优化问题为NP-hard的情况下运行的多项式时间算法的例子。

    We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.
    
[^19]: 解释自监督表示学习的概率模型

    A Probabilistic Model to explain Self-Supervised Representation Learning

    [https://rss.arxiv.org/abs/2402.01399](https://rss.arxiv.org/abs/2402.01399)

    该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。

    

    自监督学习（SSL）通过利用辅助的无监督任务，例如对语义相关样本进行分类，如不同的数据增强或模态来学习表示。在众多SSL方法中，对比方法（例如SimCLR，CLIP和VicREG）因学习到的表示在下游性能上接近有监督学习而受到关注。然而，这些方法背后的机制的理论理解仍然存在困难。我们提出了一个生成潜变量模型来表示数据，并展示了几类具有鉴别性的自监督算法（包括对比方法）近似诱导其表示中的潜变量结构，从而提供了一个统一的理论框架。我们还证明了与互信息和投影头的相关性。通过生成式地拟合我们的模型（如SimVE），在常见的基准测试上（例如FashionMNIST，CIFAR10，CelebA），性能优于之前的VAE方法。

    Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
    
[^20]: 齐次化随机梯度下降中重尾现象的出现

    Emergence of heavy tails in homogenized stochastic gradient descent

    [https://rss.arxiv.org/abs/2402.01382](https://rss.arxiv.org/abs/2402.01382)

    这项研究分析了齐次化随机梯度下降算法，在数值实验中验证了参数尾指数的明确上下界，并量化了优化参数与尾指数之间的相互作用，从而为重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力之间的关系提供了贡献。

    

    随机梯度下降（SGD）通过最小化损失已经被发现导致神经网络参数的重尾分布。在这里，我们分析了SGD的连续扩散逼近，称为齐次化随机梯度下降，证明了它在渐进情况下表现出重尾特性，并给出了关于尾指数的明确上下界。我们在数值实验中验证了这些界并显示它们通常是SGD迭代的经验尾指数的近似。另外，它们的明确形式使我们能够量化优化参数和尾指数之间的相互作用。通过这样做，我们对于关于重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力的联系的讨论做出了贡献。

    It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates. In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.
    
[^21]: 通过置换子空间在训练过程中对神经元进行对齐，以改进线性模块连通性和模型融合

    Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion

    [https://rss.arxiv.org/abs/2402.01342](https://rss.arxiv.org/abs/2402.01342)

    本文提出了一个在训练过程中进行神经元对齐的方法，通过置换子空间减少了线性模块连通性的局限性，为模型融合算法的改进提供了可能性。

    

    在深度学习中，即使在相同初始化条件下，随机梯度下降算法经常产生具有功能相似但在权重空间中分散的解，这导致了线性模块连通性（LMC）的局限性。克服这些局限性对于理解深度学习动态和提高模型融合算法至关重要。以前的研究强调置换对称性在通过网络置换减少训练后的局限性方面的作用。然而，这些事后的方法需要额外的计算，在更大、更复杂的模型（如ViT，LLM）上效果较差，因为存在大量的置换矩阵。因此，在本文中，我们研究了训练过程中神经元的对齐。我们的假设是，在训练过程中的置换子空间可以免费减少LMC的局限性。我们发现，初始化时进行修剪可以支持这一假设。除了修剪之外，我们引入了TNA-PFN，一种简单而无损的算法，在训练过程中使用部分梯度掩码。TNA-PFN在理论上和实验上都得到了支持。

    In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and em
    
[^22]: 因果熵和信息增益的基本性质

    Fundamental Properties of Causal Entropy and Information Gain

    [https://rss.arxiv.org/abs/2402.01341](https://rss.arxiv.org/abs/2402.01341)

    本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。

    

    最近的发展使得能够量化在结构因果模型(SCM)下的因果控制。这是通过引入一些量来编码在干预另一个变量时某个变量熵的变化来实现的。这些量被命名为因果熵和因果信息增益，旨在解决现有信息论方法在因果性在机器学习任务中起关键作用时的局限性。我们的研究通过建立和分析这些概念的基本性质，包括界限和链规则，对因果熵和因果信息增益的概念进行了形式上的理解。此外，我们阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义。总体而言，这个探索为提升因果机器学习任务铺平了道路。

    Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks th
    
[^23]: 通过特征谱表征核岭回归的过拟合

    Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum

    [https://rss.arxiv.org/abs/2402.01297](https://rss.arxiv.org/abs/2402.01297)

    我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。

    

    我们推导了核矩阵的条件数的新界限，然后利用这些界限增强了在固定输入维度的过参数化区域中核岭回归的现有非渐近测试误差界限。对于具有多项式谱衰减的核，我们恢复了先前工作的界限；对于指数衰减，我们的界限是非平凡和新颖的。我们对过拟合的结论是双重的：(i) 谱衰减多项式的核回归器必须在存在噪声标记的训练数据的情况下得到很好的泛化；这些模型表现出所谓的温和过拟合；(ii) 如果任何核岭回归器的特征谱指数衰减，则其泛化差，即表现出灾难性过拟合。这增加了核岭回归器表现出良性过拟合的可用特征谱衰减次多项式的极端情况的表征。我们的分析结合了新的随机矩阵理论(RMT)。

    We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
    
[^24]: Transformers在上下文中学习非线性特征：关于注意力场景中的非凸均场动态研究

    Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape

    [https://rss.arxiv.org/abs/2402.01258](https://rss.arxiv.org/abs/2402.01258)

    本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。

    

    基于Transformer架构的大型语言模型展示了在上下文中学习的令人印象深刻的能力。然而，关于这一现象产生的现有理论研究仅限于对线性回归任务上训练的单层注意力的动态。在本文中，我们研究了一个由全连接层和线性注意力层组成的Transformer的优化。MLP充当了一个常见的非线性表示或特征映射，极大地增强了上下文学习的能力。我们在均场和两个时间尺度的极限情况下证明了参数分布的无限维损失景观，虽然高度非凸，但变得相当温和。我们还分析了均场动态的二阶稳定性，并表明Wasserstein梯度流几乎总是避开鞍点。此外，我们建立了获得远离临界点和接近临界点的具体改进速率的新方法。

    Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the 
    
[^25]: MIQCQP reformulation of the ReLU neural networks Lipschitz constant estimation problem

    MIQCQP reformulation of the ReLU neural networks Lipschitz constant estimation problem

    [https://rss.arxiv.org/abs/2402.01199](https://rss.arxiv.org/abs/2402.01199)

    本文提出了一种新的方法，通过引入新的约束条件，使用二次约束混合整数规划（MIQCQP）来估计ReLU神经网络的Lipschitz常数。这些问题的解可以给出Lipschitz常数的下界和上界，并且当特定条件满足时，它们与精确的Lipschitz常数重合。

    

    众所周知，为了确保神经网络的鲁棒性，其Lipschitz常数起着重要作用。然而，它的计算是NP困难的。在本文中，通过将每一层的激活区域作为新的约束考虑进去，我们提出了神经网络Lipschitz估计问题的新的二次约束混合整数规划（MIQCQP）形式。这些问题的解给出了Lipschitz常数的下界和上界，并详细讨论了它们与精确Lipschitz常数一致的条件。

    It is well established that to ensure or certify the robustness of a neural network, its Lipschitz constant plays a prominent role. However, its calculation is NP-hard. In this note, by taking into account activation regions at each layer as new constraints, we propose new quadratically constrained MIP formulations for the neural network Lipschitz estimation problem. The solutions of these problems give lower bounds and upper bounds of the Lipschitz constant and we detail conditions when they coincide with the exact Lipschitz constant.
    
[^26]: Sobolev空间中核分类器的最优性

    The Optimality of Kernel Classifiers in Sobolev Space

    [https://rss.arxiv.org/abs/2402.01148](https://rss.arxiv.org/abs/2402.01148)

    本文研究了核分类器在Sobolev空间中的最优性质，并通过对条件概率的假设和核回归理论的应用，导出了核分类器的分类超额风险上界和Sobolev空间的极小极大下界。此外，我们还提出了一种简单方法来估计插值平滑度，并将其应用于实际数据集。

    

    核方法在机器学习中广泛应用，特别是用于分类问题。然而，核分类的理论分析仍然有限。本文研究了核分类器的统计性能。在对条件概率$\eta(x)=\mathbb{P}(Y=1\mid X=x)$作出一些温和的假设后，我们利用核回归理论的最新进展，导出了核分类器分类超额风险的上界。我们还得到了Sobolev空间的极小极大下界，从而证明了所提出分类器的最优性。我们的理论结果可以扩展到超参数化神经网络分类器的泛化误差。为了使我们的理论结果在实际环境中更加适用，我们还提出了一种估计$2\eta(x)-1$插值平滑度的简单方法，并将该方法应用于实际数据集。

    Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional probability $\eta(x)=\mathbb{P}(Y=1\mid X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. We also obtain a minimax lower bound for Sobolev spaces, which shows the optimality of the proposed classifier. Our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. To make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method to real datasets.
    
[^27]: 用解缠离散图自编码器学习网络表示

    Learning Network Representations with Disentangled Graph Auto-Encoder

    [https://rss.arxiv.org/abs/2402.01143](https://rss.arxiv.org/abs/2402.01143)

    本文介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。

    

    (变分)图自编码器广泛用于学习图结构化数据的表示。然而，现实世界图的形成是一个由潜在因素影响的复杂和异质的过程。现有的编码器基本上是整体的，忽视了潜在因素的纠缠。这不仅使得图分析任务不太有效，而且使得理解和解释这些表示变得更加困难。用(变分)图自编码器学习解缠的图表示面临着重要挑战，在现有文献中尚未得到充分探索。在本文中，我们介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。具体地，我们首先设计了一个解缠的图卷积网络，使用多通道消息传递层作为编码器，聚合与每个节点相关的信息。

    The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
    
[^28]: 在线自适应预测方法中带有递减步长

    Online conformal prediction with decaying step sizes

    [https://rss.arxiv.org/abs/2402.01139](https://rss.arxiv.org/abs/2402.01139)

    本文介绍了一种在线自适应预测方法，通过使用递减步长来改进在任意序列上的覆盖率保证，并且能够同时估计总体分位数。

    

    本文介绍一种带有递减步长的在线自适应预测方法。和之前的方法一样，我们的方法也能在任意序列上回溯性地保证覆盖率。然而，与之前的方法不同的是，我们能够在存在的情况下同时估计出总体分位数。我们的理论和实验证明了显著改进的实际特性：特别是在分布稳定的情况下，覆盖率接近所期望的水平，不仅仅在观测序列的平均值上。

    We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.
    
[^29]: 在自适应约束下的自对弈强化学习中的近乎最优解

    Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints

    [https://rss.arxiv.org/abs/2402.01111](https://rss.arxiv.org/abs/2402.01111)

    本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\widetilde{O}(\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\log\log K)$。此外，还给出了所有具有$\widetilde{O}(\sqrt{K})$后悔界算法的批量复杂度下界。

    

    我们研究了具有自适应约束的多智能体强化学习问题（MARL） - 这是一种由实际应用驱动的新问题，其中部署新策略是昂贵的，并且必须最小化策略更新的次数。对于两个玩家的零和马尔可夫博弈，我们设计了一种（策略）基于消除的算法，它在后悔为$\widetilde{O}(\sqrt{H^3 S^2 ABK})$的情况下，批量复杂度仅为$O(H+\log\log K)$。在上述情况下，$S$表示状态数，$A，B$分别代表两个玩家的行动数，$H$是时间周期，$K$是游戏次数。此外，我们证明了对于所有具有$\widetilde{O}(\sqrt{K})$后悔界的算法，一种批量复杂度的下界为$\Omega(\frac{H}{\log_{A}K}+\log\log K)$，这与我们的上界在对数因子上匹配。作为副产品，我们的技术自然地扩展到学习赌博博弈和无奖励的近乎最优批量复杂度MARL。据我们所知，这些是迄今为止最好的成果。

    We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these 
    
[^30]: 基于斯坦优化梯度下降的贝叶斯深度学习在剩余寿命估计中的应用

    Bayesian Deep Learning for Remaining Useful Life Estimation via Stein Variational Gradient Descent

    [https://rss.arxiv.org/abs/2402.01098](https://rss.arxiv.org/abs/2402.01098)

    本研究使用斯坦优化梯度下降算法，将标准的频率主义神经网络转化为贝叶斯神经网络，以应对预测性维护中估计剩余寿命的不确定性问题。

    

    在预测性维护中，估计物理系统的剩余可用寿命是一项关键任务。在过去十年中，深度学习在预测性能方面显著改进了传统的基于模型和统计方法。然而，为了优化计划维护操作，量化预测中固有的不确定性也很重要。这个问题可以通过将标准的频率主义神经网络转化为贝叶斯神经网络来解决，后者能够自然地在估计周围提供置信区间。存在多种训练这些模型的方法。研究人员主要关注参数变分推理和基于采样的技术，这些技术因近似能力有限和计算负担大而闻名。在这项工作中，我们使用了斯坦优化梯度下降，这是一种最近提出的逼近难以计算分布的算法，克服了上述方法的缺点。

    A crucial task in predictive maintenance is estimating the remaining useful life of physical systems. In the last decade, deep learning has improved considerably upon traditional model-based and statistical approaches in terms of predictive performance. However, in order to optimally plan maintenance operations, it is also important to quantify the uncertainty inherent to the predictions. This issue can be addressed by turning standard frequentist neural networks into Bayesian neural networks, which are naturally capable of providing confidence intervals around the estimates. Several methods exist for training those models. Researchers have focused mostly on parametric variational inference and sampling-based techniques, which notoriously suffer from limited approximation power and large computational burden, respectively. In this work, we use Stein variational gradient descent, a recently proposed algorithm for approximating intractable distributions that overcomes the drawbacks of th
    
[^31]: 深度神经网络预测使用多少个视图？

    How many views does your deep neural network use for prediction?

    [https://rss.arxiv.org/abs/2402.01095](https://rss.arxiv.org/abs/2402.01095)

    本文提出了最小有效视图（MSVs）的概念，该概念类似于多视图，但适用于实际图像，并且通过实证研究表明，MSV的数量与模型的预测准确性之间存在关系。

    

    尽管进行了许多理论和实证分析，但深度神经网络（DNN）的泛化能力仍未完全理解。最近，Allen-Zhu和Li（2023）引入了多视图的概念来解释DNN的泛化能力，但他们的主要目标是集成或蒸馏模型，并未讨论用于特定输入预测的多视图估计方法。在本文中，我们提出了最小有效视图（MSVs），它类似于多视图，但可以高效地计算真实图像。MSVs是输入中的一组最小且不同的特征，每个特征保留了模型对该输入的预测。我们通过实证研究表明，不同模型（包括卷积和转换模型）的MSV数量与预测准确性之间存在明确的关系，这表明多视图的角度对于理解（非集成或非蒸馏）DNN的泛化能力也很重要。

    The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu & Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
    
[^32]: 神经缩放定律的动力学模型

    A Dynamical Model of Neural Scaling Laws

    [https://rss.arxiv.org/abs/2402.01092](https://rss.arxiv.org/abs/2402.01092)

    这篇论文提出了一个动力学模型来解释神经缩放定律。通过分析梯度下降训练的随机特征模型，研究发现训练时间和模型大小的缩放具有不同的幂律指数，而计算最优缩放规则要求增加训练步数快于增加模型参数，与实证观察相一致。

    

    在各种任务中，神经网络的性能随着训练时间、数据集大小和模型大小的增加而预测性地提高，跨多个数量级。这种现象被称为神经缩放定律。最重要的是计算最优缩放定律，它报告了在选择最佳模型大小时性能与计算数量的关系。我们分析了一个通过梯度下降进行训练和泛化的随机特征模型作为网络训练和泛化的可解模型。这个模型复现了关于神经缩放定律的许多观察结果。首先，我们的模型对于为什么训练时间和模型大小的缩放具有不同的幂律指数提出了一个预测。因此，理论预测了一种不对称的计算最优缩放规则，其中训练步数的增加速度快于模型参数的增加速度，与最近的实证观察一致。其次，观察到在训练的早期，网络会收敛到无限宽度情况下的结果。

    On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-wi
    
[^33]: 可扩展的高阶张量积样条模型

    Scalable Higher-Order Tensor Product Spline Models

    [https://rss.arxiv.org/abs/2402.01090](https://rss.arxiv.org/abs/2402.01090)

    我们提出了一种可扩展的高阶张量积样条模型，允许加入所有（高阶）非线性特征效应的相互作用，并具有与没有相互作用的模型成比例的计算成本。

    

    在当前大数据和透明机器学习的时代，技术在大规模应用中运作的同时，还需要提供对方法内部工作的清晰数学理解。虽然已经存在适用于大规模应用的可解释的半参数回归方法，考虑了数据的非线性，但模型的复杂性常常受到限制。主要挑战之一是这些模型中缺乏相互作用，出于更好的解释能力和不切实际的计算成本考虑而被忽略。为了克服这个限制，我们提出了一种新的方法，使用因子化方法推导出高度可扩展的高阶张量积样条模型。我们的方法允许将所有（高阶）非线性特征效应的相互作用纳入模型，同时具有与没有相互作用的模型成比例的计算成本。我们进一步开发了一种有意义的惩罚方案

    In the current era of vast data and transparent machine learning, it is essential for techniques to operate at a large scale while providing a clear mathematical comprehension of the internal workings of the method. Although there already exist interpretable semi-parametric regression methods for large-scale applications that take into account non-linearity in the data, the complexity of the models is still often limited. One of the main challenges is the absence of interactions in these models, which are left out for the sake of better interpretability but also due to impractical computational costs. To overcome this limitation, we propose a new approach using a factorization method to derive a highly scalable higher-order tensor product spline model. Our method allows for the incorporation of all (higher-order) interactions of non-linear feature effects while having computational costs proportional to a model without interactions. We further develop a meaningful penalization scheme a
    
[^34]: 无免费修剪：初始化时剪枝的信息论障碍

    No Free Prune: Information-Theoretic Barriers to Pruning at Initialization

    [https://rss.arxiv.org/abs/2402.01089](https://rss.arxiv.org/abs/2402.01089)

    本文解释了为什么在初始化时修剪神经网络困难，并提出了一个关于有效参数数量的理论解释。我们指出，在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。为此，我们怀疑在训练过程中和训练后修剪是必要的。

    

    “抽奖中奖者”是否在初始化时存在，引发了一个令人着迷的问题：深度学习是否需要大型模型，或者可以在不训练包含它们的密集模型的情况下迅速识别和训练稀疏网络。然而，尝试在初始化时找到这些稀疏子网络（“初始化时修剪”）的努力在广泛上都没有成功。我们提出了一个理论解释，基于模型的有效参数数量$p_\text{eff}$，由最终网络中非零权重的数量和稀疏掩码与数据之间的相互信息的总和给出。我们展示了“鲁棒性定律”（arXiv:2105.12806）延伸到稀疏网络，其中常规参数数量被$p_\text{eff}$所取代，这意味着一个能够在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。我们假设在训练过程中和训练后修剪。

    The existence of "lottery tickets" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ("pruning at initialization") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training 
    
[^35]: 从有噪声标签学习非可分解性能度量的多类学习

    Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures

    [https://rss.arxiv.org/abs/2402.01055](https://rss.arxiv.org/abs/2402.01055)

    本论文提出了用于从带有噪声标签的数据中学习非可分解性能度量的多类学习算法。这些算法分别适用于单调凸性和线性比率两类性能度量，并基于类条件噪声模型进行噪声校正。

    

    近年来，学习从带有噪声标签的数据中得到良好分类器引起了广泛关注。大多数关于从有噪声标签学习的工作都集中在标准的基于损失的性能度量上。然而，许多机器学习问题需要使用非可分解性能度量，这些度量不能表示为单个示例上的损失的期望或总和；其中包括类不平衡设置中的H-mean，Q-mean和G-mean，以及信息检索中的Micro F1。在本文中，我们设计了算法，用于学习两类广泛的多类非可分解性能度量，即单调凸性和线性比率，它们包括上述所有示例。我们的工作基于Narasimhan等人的Frank-Wolfe和Bisection算法(2015)。在这两种情况下，我们在广泛研究的类条件噪声模型家族下开发了算法的噪声校正版本。我们提供了遗憾(超额风险)上界。

    There has been much interest in recent years in learning good classifiers from data with noisy labels. Most work on learning from noisy labels has focused on standard loss-based performance measures. However, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the H-mean, Q-mean and G-mean in class imbalance settings, and the Micro $F_1$ in information retrieval. In this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. Our work builds on the Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. We provide regret (excess risk) bounds 
    
[^36]: 弱凸正则化器在逆问题中的收敛性：临界点和原始-对偶优化的收敛

    Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation

    [https://rss.arxiv.org/abs/2402.01052](https://rss.arxiv.org/abs/2402.01052)

    本文提出了一种关于逆问题的弱凸正则化器的收敛性问题的一般化公式，并证明了通过一类弱凸正则化器的实现可以达到收敛，并应用于学习的正则化中实现了对计算机层析成像中学习对抗性正则化器性能的提高。

    

    变分正则化是解决逆问题的主要方法，最近有很多研究利用深度学习的正则化方法来提高性能。然而，在解决这种正则化收敛性的问题上，很少有关于临界点收敛性的结果，而非全局极小值点的收敛性。本文提出了一种关于临界点收敛性的一般化公式，并证明了这是通过一类弱凸正则化器实现的。我们证明了与相关变分问题相关的原始-对偶混合梯度方法的收敛性，并在给定Kurdyka-Lojasiewicz条件的情况下，证明了O(log(k)/k)的遗传收敛速度。最后，将这个理论应用于学习的正则化中，我们证明了输入为弱凸神经网络（IWCNN）的通用逼近性，并通过实验证明，IWCNN可以提高计算机层析成像中学习对抗性正则化器的性能。

    Variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. However, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. In this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. We prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a Kurdyka-Lojasiewicz condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (IWCNN), and show empirically that IWCNNs can lead to improved performance of learned adversarial regularisers for computed tomography (
    
[^37]: 基于分布式MCMC推理的贝叶斯非参数潜在分块模型

    Distributed MCMC inference for Bayesian Non-Parametric Latent Block Model

    [https://rss.arxiv.org/abs/2402.01050](https://rss.arxiv.org/abs/2402.01050)

    本文提出了一种基于分布式MCMC推理的贝叶斯非参数潜在分块模型方法，通过将观测值和特征划分为分区，并采用Master/Worker架构来提高聚类标签准确性和执行时间。

    

    本文介绍了一种新颖的分布式马尔可夫链蒙特卡罗（MCMC）推理方法，用于贝叶斯非参数潜在分块模型（DisNPLBM），采用Master/Worker架构。我们的非参数共聚类算法使用潜在多元高斯块分布将观测值和特征划分为分区。行上的工作负载均匀分布在工人之间，他们只与主节点进行通信，而不与彼此通信。通过实验结果，DisNPLBM证明了其对聚类标签准确性和执行时间的影响。此外，我们还通过将我们的方法应用于共同聚类基因表达数据来展示一个真实的用例。代码源可公开访问https://github.com/redakhoufache/Distributed-NPLBM。

    In this paper, we introduce a novel Distributed Markov Chain Monte Carlo (MCMC) inference method for the Bayesian Non-Parametric Latent Block Model (DisNPLBM), employing the Master/Worker architecture. Our non-parametric co-clustering algorithm divides observations and features into partitions using latent multivariate Gaussian block distributions. The workload on rows is evenly distributed among workers, who exclusively communicate with the master and not among themselves. DisNPLBM demonstrates its impact on cluster labeling accuracy and execution times through experimental results. Moreover, we present a real-use case applying our approach to co-cluster gene expression data. The code source is publicly available at https://github.com/redakhoufache/Distributed-NPLBM.
    
[^38]: 时间不齐次随机微分方程的费舍尔信息耗散

    Fisher information dissipation for time inhomogeneous stochastic differential equations

    [https://rss.arxiv.org/abs/2402.01036](https://rss.arxiv.org/abs/2402.01036)

    本文提供了时间不齐次随机微分方程的费舍尔信息耗散方法。通过将Langevin动力学的概率转移方程构造为关于时间依赖的最优传输度量的修正梯度流，我们得到了概率密度函数的收敛性保证，并在几个时间不齐次Langevin动力学中进行了验证。

    

    我们为时间不齐次变系数随机微分方程(SDEs)提供了一个Lyapunov收敛性分析。三个典型的例子包括过阻尼、不可逆漂移和欠阻尼Langevin动力学。我们首先将Langevin动力学的概率转移方程构造为概率空间中关于时间依赖的最优传输度量的Kullback-Leibler散度的修正梯度流。这个公式包含了梯度和非梯度方向，取决于一类时间依赖的目标分布。然后我们选择一个时间依赖的相对费舍尔信息泛函作为Lyapunov函数。我们发展了一个时间依赖的Hessian矩阵条件，保证了SDE的概率密度函数的收敛性。我们验证了几个时间不齐次Langevin动力学的所提出的条件。对于过阻尼Langevin动力学，我们证明了在$L^1$距离上的$O(t^{-1/2})$收敛。

    We provide a Lyapunov convergence analysis for time-inhomogeneous variable coefficient stochastic differential equations (SDEs). Three typical examples include overdamped, irreversible drift, and underdamped Langevin dynamics. We first formula the probability transition equation of Langevin dynamics as a modified gradient flow of the Kullback-Leibler divergence in the probability space with respect to time-dependent optimal transport metrics. This formulation contains both gradient and non-gradient directions depending on a class of time-dependent target distribution. We then select a time-dependent relative Fisher information functional as a Lyapunov functional. We develop a time-dependent Hessian matrix condition, which guarantees the convergence of the probability density function of the SDE. We verify the proposed conditions for several time-inhomogeneous Langevin dynamics. For the overdamped Langevin dynamics, we prove the $O(t^{-1/2})$ convergence in $L^1$ distance for the simula
    
[^39]: 多元概率时间序列预测与相关误差

    Multivariate Probabilistic Time Series Forecasting with Correlated Errors

    [https://rss.arxiv.org/abs/2402.01000](https://rss.arxiv.org/abs/2402.01000)

    本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。

    

    建模误差之间的相关性与模型能够准确量化概率时间序列预测中的预测不确定性密切相关。最近的多元模型在考虑误差之间的同时相关性方面取得了显著进展，然而，对于统计简化的目的，对这些误差的常见假设是它们在时间上是独立的。然而，实际观测往往偏离了这个假设，因为误差通常由于各种因素（如排除时间相关的协变量）而表现出显著的自相关性。在这项工作中，我们提出了一种基于低秩加对角线参数化协方差矩阵的高效方法，可以有效地刻画误差的自相关性。所提出的方法具有几个可取的特性：复杂度不随时间序列数目增加，得到的协方差可以用于校准预测，且具有较好的性能。

    Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
    
[^40]: 信任学习理论

    Credal Learning Theory

    [https://rss.arxiv.org/abs/2402.00957](https://rss.arxiv.org/abs/2402.00957)

    本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。

    

    统计学习理论是机器学习的基础，为从未知概率分布中学习到的模型的风险提供理论边界。然而，在实际部署中，数据分布可能会变化，导致领域适应/泛化问题。在本文中，我们建立了一个“信任”学习理论的基础，使用概率的凸集（信任集）来建模数据生成分布的变异性。我们认为，这样的信任集可以从有限样本的训练集中推断出来。对于有限假设空间（无论是否可实现）和无限模型空间，推导出界限，这直接推广了经典结果。

    Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
    
[^41]: 多项式神经网络的几何性质

    Geometry of Polynomial Neural Networks

    [https://rss.arxiv.org/abs/2402.00949](https://rss.arxiv.org/abs/2402.00949)

    本研究利用代数几何工具研究了具有单项式激活函数的多项式神经网络的表达性和学习过程，通过对神经流形的维度和学习度的研究，提供了网络表达能力和训练复杂度的度量，并给出了可学函数数量的上界。

    

    我们研究了具有单项式激活函数的多项式神经网络（PNN）的表达性和学习过程。网络的权重参数化了神经流形。在本文中，我们使用代数几何工具研究了某些神经流形：我们给出了半代数集的明确描述并特征化了它们的Zariski闭包，称为神经多样性。我们研究了它们的维度并将一个代数度量，学习度，与神经多样性相关联。维度作为网络表达能力的几何度量，学习度是训练网络的复杂度度量，并提供可学函数数量的上界。这些理论结果还伴随着实验证明。

    We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.
    
[^42]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^43]: 在线变分顺序蒙特卡洛方法

    Online Variational Sequential Monte Carlo

    [https://rss.arxiv.org/abs/2312.12616](https://rss.arxiv.org/abs/2312.12616)

    本文提出了一种在线学习的算法，名为在线VSMC，它基于变分顺序蒙特卡洛方法，在处理数据流时能够实时进行模型参数估计和粒子提议适应。

    

    状态空间模型（SSM）是AI和统计机器学习中最经典的生成模型，对于任何形式的参数学习或潜在状态推断，通常需要计算复杂的潜在状态后验分布。本文在变分顺序蒙特卡洛（VSMC）方法的基础上进行了研究，该方法通过结合粒子方法和变分推断，提供了计算高效且准确的模型参数估计和贝叶斯潜在状态推断。传统的VSMC方法在离线模式下运行，通过重复处理给定的数据批次，而我们使用随机逼近方法将VSMC代理ELBO的梯度逼近分布到时间上，从而实现了在数据流存在的情况下的在线学习。这导致了一种名为在线VSMC的算法，能够高效地进行参数估计和粒子提议适应，而且完全实时处理数据。

    Being the most classical generative model for serial data, state-space models (SSM) are fundamental in AI and statistical machine learning. In SSM, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. In this work, we build upon the variational sequential Monte Carlo (VSMC) method, which provides computationally efficient and accurate model parameter estimation and Bayesian latent-state inference by combining particle methods and variational inference. While standard VSMC operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the VSMC surrogate ELBO in time using stochastic approximation, allowing for online learning in the presence of streams of data. This results in an algorithm, online VSMC, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. In addition, we prov
    
[^44]: 边际拉普拉斯分数

    Marginal Laplacian Score

    [https://rss.arxiv.org/abs/2311.17795](https://rss.arxiv.org/abs/2311.17795)

    边际拉普拉斯分数（MLS）是一种针对高维度不平衡数据的改进版拉普拉斯分数（LS），通过保留数据集边缘的局部结构，提供了一种有效的无监督特征选择方法。该方法被成功地集成到不同iable无监督特征选择（DUFS）算法中，在合成和公共数据集上展示了稳健且改进的性能。

    

    高维度不平衡数据给机器学习带来了挑战。在没有足够或高质量的标签的情况下，无监督特征选择方法对后续算法的成功至关重要。因此，我们引入了一种边际拉普拉斯分数（MLS），它是著名的拉普拉斯分数（LS）的修改版本，旨在更好地处理不平衡数据。我们假设少数类或异常类在特征的边缘中更频繁出现。因此，MLS旨在保留数据集边缘的局部结构。我们将其集成到利用拉普拉斯分数的现代特征选择方法中。我们将MLS算法集成到可微无监督特征选择（DUFS）中，得到了DUFS-MLS。所提出的方法在合成和公共数据集上展示了稳健且改进的性能。

    High-dimensional imbalanced data poses a machine learning challenge. In the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms. Therefore, we introduce a Marginal Laplacian Score (MLS), a modification of the well known Laplacian Score (LS) tailored to better address imbalanced data. We introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features. Consequently, MLS aims to preserve the local structure of the dataset's margin. We propose its integration into modern feature selection methods that utilize the Laplacian score. We integrate the MLS algorithm into the Differentiable Unsupervised Feature Selection (DUFS), resulting in DUFS-MLS. The proposed methods demonstrate robust and improved performance on synthetic and public datasets.
    
[^45]: 正则流是否是解锁指数机制的关键？经过准确性和隐私双重约束的差分隐私机器学习的一条路径

    Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML

    [https://rss.arxiv.org/abs/2311.09200](https://rss.arxiv.org/abs/2311.09200)

    通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。

    

    当前的差分隐私机器学习（ML）的最先进且事实标准是差分隐私随机梯度下降（DPSGD）。然而，这种方法本质上是浪费的。通过向每个梯度添加噪声，它会在每个梯度步骤中降低整体隐私。尽管经过15年的丰富研究，推进了组合定理、子采样方法和实现技术，但当前的隐私机器学习方法往往无法达到足够的准确性和隐私保护。与此同时，为了私下优化而设计的指数机制（ExpM）历来被排除在现代机器学习算法的私下训练之外，主要是因为ExpM需要从一种历来难以处理的密度中进行采样。尽管最近发现了正则流模型（NFs），这是一种用于逼近难以处理分布的表达深度网络，但ExpM仍然处于背景中。我们的观点是利用正则流来绕过ExpM的历史障碍是一个潜在的方法。

    The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
    
[^46]: 高维混合类别高斯过程在绿色飞机多学科设计优化中的应用

    High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft

    [https://rss.arxiv.org/abs/2311.06130](https://rss.arxiv.org/abs/2311.06130)

    本文介绍了一种创新的降维算法，利用偏最小二乘回归来减少构建混合变量高斯过程所需的超参数数量。这种方法在结构和多学科应用中表现出良好的潜力，适用于绿色飞机的优化等多个领域。

    

    最近，基于高斯过程（GP）的混合类别元模型在贝叶斯优化中引起了越来越多的关注。在这个背景下，可以使用不同的方法来构建混合类别的GP。其中许多方法涉及大量的超参数；事实上，用于构建GP的策略越通用和精确，需要估计的超参数越多。本文引入了一种创新的降维算法，该算法依赖于偏最小二乘回归，以减少用于构建混合变量GP的超参数数量。我们的目标是将常用于处理连续输入的经典降维技术推广到处理混合类别输入。所提出方法的潜力在结构和多学科应用中得到了很好的证明。目标应用包括悬臂梁的分析以及绿色飞机的优化结果。

    Recently, there has been a growing interest in mixed-categorical metamodels based on Gaussian Process (GP) for Bayesian optimization. In this context, different approaches can be used to build the mixed-categorical GP. Many of these approaches involve a high number of hyperparameters; in fact, the more general and precise the strategy used to build the GP, the greater the number of hyperparameters to estimate. This paper introduces an innovative dimension reduction algorithm that relies on partial least squares regression to reduce the number of hyperparameters used to build a mixed-variable GP. Our goal is to generalize classical dimension reduction techniques commonly used within GP (for continuous inputs) to handle mixed-categorical inputs. The good potential of the proposed method is demonstrated in both structural and multidisciplinary application contexts. The targeted applications include the analysis of a cantilever beam as well as the optimization of a green aircraft, resultin
    
[^47]: 基于前向$\chi^2$散度的变分重要抽样方法

    Forward $\chi^2$ Divergence Based Variational Importance Sampling

    [https://rss.arxiv.org/abs/2311.02516](https://rss.arxiv.org/abs/2311.02516)

    引入了一种基于前向$\chi^2$散度的变分重要抽样方法(VIS)，通过直接估计和最大化对数似然来增强对复杂后验分布的估计性能。实验证明，VIS方法在多种潜变量模型中均优于最先进的基线方法，表现出更高的对数似然和模型参数估计准确性。

    

    最大化对数似然是学习潜变量模型的关键方面，而变分推断（VI）是目前常用的方法。然而，当处理复杂的后验分布时，VI在实现高对数似然方面可能遇到挑战。针对这个限制，我们引入了一种新颖的变分重要抽样（VIS）方法，直接估计和最大化对数似然。VIS利用通过最小化前向$\chi^2$散度实现的最佳提议分布来增强对数似然估计。我们将VIS应用于多种流行的潜变量模型，包括混合模型、变分自编码器和部分观测广义线性模型。结果表明，我们的方法在对数似然和模型参数估计方面始终优于最先进的基线方法。

    Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.
    
[^48]: 变分线性化Laplace近似在贝叶斯深度学习中的应用

    Variational Linearized Laplace Approximation for Bayesian Deep Learning

    [https://rss.arxiv.org/abs/2302.12565](https://rss.arxiv.org/abs/2302.12565)

    本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。

    

    最近，线性化Laplace近似（LLA）被用来对预训练深度神经网络（DNNs）的预测进行不确定性估计。然而，在训练点或DNN参数较多的情况下，其广泛应用受到了计算成本的限制。因此，其他LLA的近似方法，如Kronecker分解或对角线GGN矩阵的近似，被使用，可能会影响模型的性能。为了解决这些挑战，我们提出了一种基于变分稀疏高斯过程（GP）的LLA近似方法。我们的方法基于GP的对偶RKHS公式，并保留了原始DNN的预测均值。此外，它允许有效的随机优化，从而在训练数据集的大小中实现子线性训练时间。具体而言，其训练成本与训练点的数量无关。我们比较了我们的方法与其他近似方法的性能，并展示了其在准确性和计算效率方面的优势。

    The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose
    
[^49]: 生成对抗学习Sinkhorn算法初始化

    Generative Adversarial Learning of Sinkhorn Algorithm Initializations

    [https://rss.arxiv.org/abs/2212.00133](https://rss.arxiv.org/abs/2212.00133)

    本文通过生成对抗学习的方法，训练神经网络来学习Sinkhorn算法的初始化，显著加快收敛速度，同时保持算法的可微分性和并行性，并证明了网络的普适性和独立求解能力。

    

    Sinkhorn算法是近似求解离散概率分布之间熵正则输运（OT）距离的最先进方法。我们展示了通过训练神经网络来学习算法初始化，可以显著加快收敛速度，同时保持Sinkhorn算法的可微分性和并行性。我们通过对抗训练的方式使用第二个生成网络和自监督引导损失来训练我们的预测网络。预测网络具有普适性，能够推广到任意固定维度和成本的概率分布对，并且我们证明生成网络可以在训练过程中产生任意概率分布对。此外，我们还展示了我们的网络可以作为独立的OT求解器来近似正则化输运问题。

    The Sinkhorn algorithm is the state-of-the-art to approximate solutions of entropic optimal transport (OT) distances between discrete probability distributions. We show that meticulously training a neural network to learn initializations to the algorithm via the entropic OT dual problem can significantly speed up convergence, while maintaining desirable properties of the Sinkhorn algorithm, such as differentiability and parallelizability. We train our predictive network in an adversarial fashion using a second, generating network and a self-supervised bootstrapping loss. The predictive network is universal in the sense that it is able to generalize to any pair of distributions of fixed dimension and cost at inference, and we prove that we can make the generating network universal in the sense that it is capable of producing any pair of distributions during training. Furthermore, we show that our network can even be used as a standalone OT solver to approximate regularized transport dis
    
[^50]: 高阶精确度的两个样本网络推断与哈希

    Higher-order accurate two-sample network inference and network hashing

    [https://rss.arxiv.org/abs/2208.07573](https://rss.arxiv.org/abs/2208.07573)

    本文提出了一种新的方法和算法工具箱，解决了网络比较中的多个挑战，并具有高阶精确度和功率优化的特点。这个方法在速度和准确度方面优于现有工具，并且具有强大的理论保证。

    

    网络比较的两个样本假设检验面临许多重大挑战，包括：利用重复的网络观测和已知的节点注册，但不需要它们一起操作；放松强结构性假设；实现有限样本高阶精确度；处理不同的网络大小和稀疏程度；快速计算和内存节省；在多重检验中控制假阳性发现率（FDR）；以及在理论上对有限样本精确度和最小极值性的理解。在本文中，我们开发了一个全面的工具箱，其中包括一种新的主要方法及其变体，所有这些方法都伴随着强大的理论保证，以解决这些挑战。我们的方法在速度和准确度上优于现有工具，并被证明是功率优化的。我们的算法对于处理各种数据结构（单一或重复的网络观测；已知或未知的节点注册）非常友好和多功能。

    Two-sample hypothesis testing for network comparison presents many significant challenges, including: leveraging repeated network observations and known node registration, but without requiring them to operate; relaxing strong structural assumptions; achieving finite-sample higher-order accuracy; handling different network sizes and sparsity levels; fast computation and memory parsimony; controlling false discovery rate (FDR) in multiple testing; and theoretical understandings, particularly regarding finite-sample accuracy and minimax optimality. In this paper, we develop a comprehensive toolbox, featuring a novel main method and its variants, all accompanied by strong theoretical guarantees, to address these challenges. Our method outperforms existing tools in speed and accuracy, and it is proved power-optimal. Our algorithms are user-friendly and versatile in handling various data structures (single or repeated network observations; known or unknown node registration). We also develo
    
[^51]: 统计学习视角下的简单克里金方法

    A Statistical Learning View of Simple Kriging

    [https://rss.arxiv.org/abs/2202.07365](https://rss.arxiv.org/abs/2202.07365)

    本文从统计学习的视角对简单克里金方法进行了分析，解决了在大数据时代中，考虑复杂空间相关结构的大规模数据集预测问题。

    

    在大数据时代，特别是由于地理定位传感器的普及，出现了越来越多展示可能具有复杂空间相关结构的大规模数据集。在这种情况下，标准的统计学习的概率理论不直接适用，并且从这些数据中学习的预测规则的泛化能力的保证有待建立。本文从统计学习的视角对简单克里金方法进行了分析，即通过进行非参数有限样本的预测分析。给定$d\geq 1$个由未知协方差结构的平方可积随机场$X=\{X_s\}_{s\in S}$在$S\subset \mathbb{R}^2$中的$s_1,\; \ldots,\; s_d$处的实现值，目标是用最小二次风险预测它在$S$中的任何其他位置$s\in S$上的未知值。预测规则是从训练空间数据集导出的：来自独立于待预测位置的实现$X'$。

    In the Big Data era, with the ubiquity of geolocation sensors in particular, massive datasets exhibiting a possibly complex spatial dependence structure are becoming increasingly available. In this context, the standard probabilistic theory of statistical learning does not apply directly and guarantees of the generalization capacity of predictive rules learned from such data are left to establish. We analyze here the simple Kriging task from a statistical learning perspective, i.e. by carrying out a nonparametric finite-sample predictive analysis. Given $d\geq 1$ values taken by a realization of a square integrable random field $X=\{X_s\}_{s\in S}$, $S\subset \mathbb{R}^2$, with unknown covariance structure, at sites $s_1,\; \ldots,\; s_d$ in $S$, the goal is to predict the unknown values it takes at any other location $s\in S$ with minimum quadratic risk. The prediction rule being derived from a training spatial dataset: a single realization $X'$ of $X$, independent from those to be p
    
[^52]: 使用Sinkhorn散度的分布式强化学习

    Distributional Reinforcement Learning by Sinkhorn Divergence

    [https://rss.arxiv.org/abs/2202.00769](https://rss.arxiv.org/abs/2202.00769)

    本文提出了SinkhornDRL方法，使用Sinkhorn散度来减小当前和目标Bellman回报分布之间的差异，并通过理论证明和实证实验展示了该方法的优越性。

    

    分布式强化学习的实证成功高度依赖于分布表示和分布散度的选择。本文提出了Sinkhorn分布强化学习 (SinkhornDRL)的方法，该方法从回报分布中学习无限制的统计量，并利用Sinkhorn散度来减小当前和目标Bellman回报分布之间的差异。从理论上来讲，我们证明了SinkhornDRL的收缩性质，与Sinkhorn散度在Wasserstein距离和最大均值差异 (MMD)之间的插值性质一致。我们还建立了Sinkhorn散度与带有正则化Moment Matching行为的正则化MMD之间的等价关系，从而解释了SinkhornDRL的优越性。在实证上，我们展示了SinkhornDRL在Atari游戏套件上始终表现比现有算法更好或可比。

    The empirical success of distributional reinforcement learning~(RL) highly depends on the distribution representation and the choice of distribution divergence. In this paper, we propose \textit{Sinkhorn distributional RL~(SinkhornDRL)} that learns unrestricted statistics from return distributions and leverages Sinkhorn divergence to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the contraction properties of SinkhornDRL, consistent with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy~(MMD). We also establish the equivalence between Sinkhorn divergence and a regularized MMD with a regularized Moment Matching behavior, contributing to explaining the superiority of SinkhornDRL. Empirically, we show that SinkhornDRL is consistently better or comparable to existing algorithms on the Atari games suite.
    
[^53]: 预测中缺失数据的简单填补规则：理论保证与实证性能的对比

    Simple Imputation Rules for Prediction with Missing Data: Contrasting Theoretical Guarantees with Empirical Performance

    [https://rss.arxiv.org/abs/2104.03158](https://rss.arxiv.org/abs/2104.03158)

    本研究对缺失数据问题进行了研究，通过对比理论和实证结果，展示了一些简单填补规则在预测任务中的性能。在广泛填补方法家族中，我们发现均值填补是最优的，而众数填补是次优的。实证结果除了支持理论发现外，还强调了理论和实践之间的差距和未来研究的机会。

    

    缺失数据是实际数据集中常见的问题。本文通过对比理论和实证证据来研究填补-回归流程的性能。我们建立了适用于广泛填补方法家族的渐进一致性。尽管常识认为“好”的填补方法生成的数据集是合理的，但是我们发现，在预测方面，粗糙数据也可能是好的。我们发现其中一些结论是，众数填补在渐进上是次优的，而均值填补在渐进上是最优的。然后我们在大量的合成、半真实和真实数据集上详尽评估了这些理论结论的有效性。尽管我们收集的实证证据大多支持我们的理论发现，但也强调了理论与实践之间的差距和未来研究的机会，以解决MAR假设的相关性，填补和回归任务之间的复杂相互依赖的问题。

    Missing data is a common issue in real-world datasets. This paper studies the performance of impute-then-regress pipelines by contrasting theoretical and empirical evidence. We establish the asymptotic consistency of such pipelines for a broad family of imputation methods. While common sense suggests that a `good' imputation method produces datasets that are plausible, we show, on the contrary, that, as far as prediction is concerned, crude can be good. Among others, we find that mode-impute is asymptotically sub-optimal, while mean-impute is asymptotically optimal. We then exhaustively assess the validity of these theoretical conclusions on a large corpus of synthetic, semi-real, and real datasets. While the empirical evidence we collect mostly supports our theoretical findings, it also highlights gaps between theory and practice and opportunities for future research, regarding the relevance of the MAR assumption, the complex interdependency between the imputation and regression tasks
    
[^54]: 使用共享神经元的RBF网络估计个体化多治疗反应曲线

    Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons. (arXiv:2401.16571v1 [stat.ME])

    [http://arxiv.org/abs/2401.16571](http://arxiv.org/abs/2401.16571)

    我们提出了一种使用共享神经元的RBF网络的非参数化治疗效应估计方法，适用于多治疗设置。该方法能够建模治疗结果的共同性，并在贝叶斯框架下实现估计和推断，通过模拟实验证明了其数值性能，应用于真实临床数据后也得到了有趣的发现。

    

    异质治疗效应估计是精确医学中的一个重要问题。我们的研究兴趣在于基于一些外部协变量，确定不同治疗方式的差异效应。我们提出了一种新颖的非参数化治疗效应估计方法，适用于多治疗设置。我们对响应曲线的非参数建模依赖于带有共享隐藏神经元的径向基函数（RBF）网络。因此，我们的模型有助于建模治疗结果的共同性。我们在贝叶斯框架下开发了估计和推断方案，并通过高效的马尔科夫链蒙特卡罗算法进行实现，适当地处理了分析各个方面的不确定性。通过模拟实验，展示了该方法的数值性能。将我们提出的方法应用于MIMIC数据后，我们得到了关于不同治疗策略对ICU住院时间和12小时SOFA评分的影响的一些有趣发现。

    Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function (RBF)-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of ICU stay and 12-hour SOFA sc
    
[^55]: 基于潜在决策模型的具有隐藏约束的贝叶斯优化方法

    Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])

    [http://arxiv.org/abs/2310.18449](http://arxiv.org/abs/2310.18449)

    本文介绍了一种基于潜在决策模型的贝叶斯优化方法，通过利用变分自编码器学习可行决策的分布，在原始空间和潜在空间之间实现了双向映射，从而解决了公共决策制定中的隐藏约束问题。

    

    贝叶斯优化（BO）已经成为解决复杂决策问题的强大工具，尤其在公共政策领域如警察划区方面。然而，由于定义可行区域的复杂性和决策的高维度，其在公共决策制定中的广泛应用受到了阻碍。本文介绍了一种新的贝叶斯优化方法——隐藏约束潜在空间贝叶斯优化（HC-LSBO），该方法集成了潜在决策模型。该方法利用变分自编码器来学习可行决策的分布，实现了原始决策空间与较低维度的潜在空间之间的双向映射。通过这种方式，HC-LSBO捕捉了公共决策制定中固有的隐藏约束的细微差别，在潜在空间中进行优化的同时，在原始空间中评估目标。我们通过对合成数据集和真实数据集进行数值实验来验证我们的方法，特别关注大规模问题。

    Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
    
[^56]: 在贝叶斯优化中的随机探索：最佳遗憾和计算效率优化

    Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency. (arXiv:2310.15351v1 [cs.LG])

    [http://arxiv.org/abs/2310.15351](http://arxiv.org/abs/2310.15351)

    本论文研究了在贝叶斯优化中使用随机探索的方法，并证明了其能够实现最佳的误差率和最优遗憾保证。同时，所提出的算法通过随机探索避免了每次迭代中非凸获取函数的昂贵优化，具有计算上的优势。

    

    我们考虑使用高斯过程模型的贝叶斯优化，也称为基于核的赌博优化。我们研究使用从分布中随机抽样来探索领域的方法。我们证明了这种随机探索方法能够实现最佳的误差率。我们的分析基于在本研究中建立的无限维希尔伯特空间中的新型集中边界，这可能具有独立的意义。我们进一步开发了一种基于随机探索和领域缩小的算法，并在无噪声和有噪声环境下建立其最佳遗憾保证。在无噪声环境中，我们的分析填补了在遗憾性能方面存在的差距，从而解决了COLT中的一个开放问题。由于随机探索消除了每次迭代中选择查询点的非凸获取函数的昂贵优化，所以所提出的算法也具有计算优势。

    We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance and thereby resolves a COLT open problem. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.
    
[^57]: 几乎等变性通过李代数卷积

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    

    最近，在机器学习中，模型相对于群作用的等变性已成为一个重要的研究课题。然而，赋予一个架构具体的群等变性对模型所期望看到的数据变换类型施加了强大的先验。严格等变模型强制执行对称性，但真实世界的数据并不总是符合这样的严格等变性，可能是因为数据中的噪声或仅编码了近似或部分对称性的潜在物理定律。在这种情况下，严格等变性的先验实际上可能过于强大，导致模型在真实数据上表现不佳。因此，在这项工作中，我们研究了一个相关的主题，即几乎等变性。我们提供了一个与当前文献中现有定义不同的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^58]: 在无限时域的马尔可夫决策过程中，利用离线数据集进行高效在线学习：一种贝叶斯方法

    Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])

    [http://arxiv.org/abs/2310.11531](http://arxiv.org/abs/2310.11531)

    本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。

    

    本文研究了当存在一个离线数据集时，如何在无限时域设置下进行高效的在线强化学习问题。我们假设离线数据集是由一个专家生成的，但其能力水平未知，即它不是完美的，也不一定使用最优策略。我们展示了如果学习代理模拟专家使用的行为策略（由能力参数参数化），在累积遗憾最小化方面能取得明显更好的结果。我们建立了一个以 $\tilde{O}(\sqrt{T})$ 为缩放的精确有用PSRL算法遗憾的上界。这需要对贝叶斯在线学习算法在无限时域设置下进行新颖的先验相关遗憾分析。然后，我们提出了一种近似的Informed RLSVI算法，可以理解为使用离线数据集进行模仿学习，然后进行在线学习。

    In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
    
[^59]: 后验采样学习算法在序列化POMDPs中的遗憾分析

    Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])

    [http://arxiv.org/abs/2310.10107](http://arxiv.org/abs/2310.10107)

    本文分析了后验采样学习算法在序列化POMDPs中的遗憾性能，并在一定条件下提供了改进的多项式贝叶斯遗憾界。

    

    相比于马尔科夫决策过程（MDPs），部分可观察马尔科夫决策过程（POMDPs）的学习由于观察数据难以解读而变得更加困难。在本文中，我们考虑了具有未知转移和观测模型的POMDPs中的序列化学习问题。我们考虑了基于后验采样的强化学习算法（PSRL）在POMDPs中的应用，并证明其贝叶斯遗憾随着序列的数量的平方根而缩小。一般来说，遗憾随着时间长度$H$呈指数级增长，并通过提供一个下界证明了这一点。然而，在POMDP是欠完备且弱可识别的条件下，我们建立了一个多项式贝叶斯遗憾界，相比于arXiv:2204.08967的最新结果，改进了遗憾界约$\Omega(H^2\sqrt{SA})$倍。

    Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
    
[^60]: 基于神经切向核的联邦平均在深度线性神经网络上的视角

    A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network. (arXiv:2310.05495v1 [cs.LG])

    [http://arxiv.org/abs/2310.05495](http://arxiv.org/abs/2310.05495)

    本论文介绍了一种基于神经切向核视角的联邦平均方法在深度线性神经网络上的应用，并探讨了该方法面临的挑战。

    

    联邦平均（FedAvg）是一种广泛使用的范式，用于在不共享数据的情况下协同训练来自分布式客户端的模型。如今，由于其卓越性能，神经网络取得了显著的成功，这使得它成为FedAvg中的首选模型。然而，神经网络的优化问题通常是非凸的甚至是非光滑的。此外，FedAvg总是涉及多个客户端和本地更新，导致不准确的更新方向。这些属性给分析FedAvg在训练神经网络中的收敛性带来了困难。最近，神经切向核（NTK）理论已被提出，用于理解解决神经网络非凸问题中的一阶方法的收敛性。深度线性神经网络是理论学科中的经典模型，由于其简单的公式。然而，在训练深度线性神经网络上，对于FedAvg的收敛性目前还没有理论结果。

    Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the d
    
[^61]: 《来自彩票票集成的神经规模定律》

    A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])

    [http://arxiv.org/abs/2310.02258](http://arxiv.org/abs/2310.02258)

    《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。

    

    神经规模定律（NSL）指的是模型性能随着规模增加而提高的现象。Sharma＆Kaplan使用近似理论分析了NSL，并预测了MSE损失的衰减方式为$N^{-\alpha}$，其中$\alpha=4/d$，$N$为模型参数数量，$d$为内在输入维度。尽管他们的理论在某些情况下效果良好（例如ReLU网络），但令人惊讶的是，我们发现在简单的1D问题$y=x^2$中，表现出了与他们预测不同的缩放定律（$\alpha=1$而不是$\alpha=4$）。我们打开了神经网络并发现新的缩放定律源于彩票票集成：平均而言，更宽的网络有更多的“彩票票”，它们被集成来减小输出的方差。我们通过对单个神经网络进行机械解释以及对它们进行统计研究来支持集成机制。我们将$N^{-1}$的缩放定律归因于“彩票票的中心极限定理”。最后，我们讨论了它的潜在影响。

    Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma & Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications f
    
[^62]: 深度图核点过程

    Deep graph kernel point processes. (arXiv:2306.11313v1 [stat.ML])

    [http://arxiv.org/abs/2306.11313](http://arxiv.org/abs/2306.11313)

    本文提出了一种基于潜在图拓扑的图点过程方法，并开发了一种新颖的深度图核来描述事件之间的触发和抑制效应，该方法在合成和实际数据集上具有优越性。

    

    点过程模型广泛用于分析图中异步事件，反映不同类型事件之间的相互影响。预测未来事件的时间和类型是一项关键任务，并且图的大小和拓扑结构增加了问题的难度。最近的神经点过程模型揭示了捕捉复杂的事件类别之间依赖关系的可能性。然而，这些方法在每个目标事件类型的强度计算中使用了包括所有事件类别在内的未经滤波的事件记录。在本文中，我们提出了一种基于潜在图拓扑的图点过程方法。对应的无向图具有代表事件类别的节点和表示潜在贡献关系的边。然后，我们开发了一种新颖的深度图核来描述事件之间的触发和抑制效应。本质影响结构通过图神经网络-based的局部邻域信息聚合进行了融合。我们在合成和实际数据集上展示了我们提出的方法比最先进的模型更具优越性。

    Point process models are widely used to analyze asynchronous events occurring within a graph that reflect how different types of events influence one another. Predicting future events' times and types is a crucial task, and the size and topology of the graph add to the challenge of the problem. Recent neural point process models unveil the possibility of capturing intricate inter-event-category dependencies. However, such methods utilize an unfiltered history of events, including all event categories in the intensity computation for each target event type. In this work, we propose a graph point process method where event interactions occur based on a latent graph topology. The corresponding undirected graph has nodes representing event categories and edges indicating potential contribution relationships. We then develop a novel deep graph kernel to characterize the triggering and inhibiting effects between events. The intrinsic influence structures are incorporated via the graph neural
    
[^63]: 采用局部线性模型的变分梯度下降

    Variational Gradient Descent using Local Linear Models. (arXiv:2305.15577v1 [stat.ML])

    [http://arxiv.org/abs/2305.15577](http://arxiv.org/abs/2305.15577)

    本文提出了使用局部线性模型实现目标和粒子分布KL散度降低的新估计器，可以使用样本进行计算而不需要目标得分函数，具有比SVGD更简单有效的计算方法，对于高维度情况下的模型也有优化，提升估计精度。

    

    Stein Variational Gradient Descent (SVGD) 能够沿着轨迹传输粒子，从而减少目标和粒子分布之间的KL散度，但需要目标得分函数来计算更新。我们提出了一种新的SVGD视角，将其视为反向KL梯度流的局部估计器。这种视角启发我们提出了使用局部线性模型来实现相同目的的新估计器。这些提议的估计器可以仅使用目标和粒子分布的样本进行计算，而不需要目标得分函数。我们提议的变分梯度估计器利用了局部线性模型，从而在保持估计偏差与SVGD相当的效果的同时具有计算简便性。此外，我们证明，在温和的假设下，高维梯度流的估计可以转化为一个低维估计问题，从而导致更好的估计精度。我们对提议的方法进行了验证，并对其进行了比较。

    Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We vali
    
[^64]: 我不想说：在可选个人数据模型中保护用户同意

    I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13954](http://arxiv.org/abs/2210.13954)

    该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。

    

    我们研究了一种机器学习模型，其中个人可以选择与决策系统共享可选个人信息，这在现代保险定价模型中很常见。一些用户同意使用他们的数据，而其他人则反对并保持其数据未公开。本文表明，不共享数据的决定本身可以被视为信息，应该受到保护，以尊重用户的隐私。这一观察结果引发了一个被忽视的问题，即如何确保保护其个人数据的用户不会因此受到任何不利影响。为了解决这个问题，我们对仅使用获得积极用户同意的信息的模型进行了保护要求的正式化。这排除了作出共享数据与否决定所包含的隐含信息。我们提出了Protected User Consent (PUC)概念，这是我们证明在保护要求下损失最小的解决方案。

    We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
    

