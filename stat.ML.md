# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Reflected Diffusion Models.](http://arxiv.org/abs/2304.04740) | 该论文提出了反射扩散模型，通过学习反射随机微分方程的扰动评分函数，将数据约束原则性地整合到生成过程中，以取代之前采用的导致不自然样本的阈值处理方案。 |
| [^2] | [When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?.](http://arxiv.org/abs/2304.04724) | 本文表明，从高维平滑目标分布采样时，Metropolized Hamiltonian Monte Carlo (HMC)比Metropolis-adjusted Langevin算法（MALA）更有效。 |
| [^3] | [Scalable Randomized Kernel Methods for Multiview Data Integration and Prediction.](http://arxiv.org/abs/2304.04692) | 该论文开发了可伸缩的随机核方法，可以联合多个来源的数据，识别多视角数据之间非线性关系中最有效的贡献因素，并预测临床结果。在COVID-19数据上的应用发现了相关的分子标志。 |
| [^4] | [On the strong stability of ergodic iterations.](http://arxiv.org/abs/2304.04657) | 本论文研究了迭代随机函数生成的过程的强稳定性，证明了适用于递归映射的温和条件下的强稳定性，并且提供了多个应用及相关领域的新结果。 |
| [^5] | [A Survey on Recent Teacher-student Learning Studies.](http://arxiv.org/abs/2304.04615) | 本文综述了最近关于师生学习的研究进展，重点讨论了知识蒸馏的变体，如教学助理蒸馏、课程蒸馏、掩码蒸馏和解耦蒸馏等，这些变体通过引入新的组件或改变学习过程来提高知识蒸馏的性能，已经取得了有希望的结果。 |
| [^6] | [Approximation of Nonlinear Functionals Using Deep ReLU Networks.](http://arxiv.org/abs/2304.04443) | 本文研究了使用深度ReLU网络逼近非线性函数的理论性质，并使用简单的三角剖分下构建了连续分段线性插值，实现了函数深度神经网络的逼近能力。 |
| [^7] | [Partial Identification of Causal Effects Using Proxy Variables.](http://arxiv.org/abs/2304.04374) | 这篇论文提出了一种无需完备性的部分识别方法，它为我们提供了一组界限，用于在未能控制混淆因素的情形下，评估治疗对结果变量因果效应。 |
| [^8] | [Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk.](http://arxiv.org/abs/2304.04341) | 该论文探讨了随机多臂赌博问题中，如何在期望和尾部风险之间做出最优权衡。提出了一种新的策略，能够实现最坏和实例相关的优异表现，并且能够最小化遗憾尾部概率。 |
| [^9] | [PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling.](http://arxiv.org/abs/2304.04307) | PriorCVAE 提出了一种处理高斯过程先验 MCMC 参数推断的贝叶斯深度生成建模新方法，可通过将 VAE 建模条件化于随机过程超参数处理超参数推断与学习先验之间的信息流断裂问题。 |
| [^10] | [A Note on "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms".](http://arxiv.org/abs/2304.04258) | 本文提出了一种更自然和可解释的效用函数，更好地反映了KNN模型的性能，提供了相应计算过程，该方法被称为软标签KNN-SV，与原始方法具有相同的时间复杂度。 |
| [^11] | [Data-driven multinomial random forest.](http://arxiv.org/abs/2304.04240) | 本文加强了一些先前弱一些的随机森林变体的证明方法并提高了这些变体的数据利用率以获得更好的理论性能和实验性能，提出了一种数据驱动的多项式随机森林算法，其复杂度低且满足强一致性，在分类和回归问题上表现更好，并超过了标准随机森林。 |
| [^12] | [A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry.](http://arxiv.org/abs/2304.04095) | 本文证明了，在光滑和等周条件下，MALA的混合时间仅与Hessian矩阵的trace有关，而与其算子范数和log-concave没有关系。 |
| [^13] | [Best Arm Identification with Fairness Constraints on Subpopulations.](http://arxiv.org/abs/2304.04091) | 本文解决了一个有关最优臂识别的问题，该问题要求所选臂必须对所有亚群都公平。我们提出了一个算法并证明其样本复杂度。 |
| [^14] | [Deep Generative Modeling with Backward Stochastic Differential Equations.](http://arxiv.org/abs/2304.04049) | 该文提出了一种新的深度生成模型，名为BSDE-Gen，它将反向随机微分方程的灵活性与深度神经网络的强大能力结合，用于生成高维复杂目标数据，特别是在图像生成领域。BSDE-Gen的生成建模过程中引入了随机性和不确定性，是一种有效而自然的生成高维数据的方法。 |
| [^15] | [Statistical and computational rates in high rank tensor estimation.](http://arxiv.org/abs/2304.04043) | 本文提出了一种用于从高阶张量数据集中获得可能具有高秩信号的方法，包括高秩和低秩模型。提供了全面的统计和计算结果。发现高维潜变量张量具有对数秩，并实现了计算上的最优速率。 |
| [^16] | [DiscoVars: A New Data Analysis Perspective -- Application in Variable Selection for Clustering.](http://arxiv.org/abs/2304.03983) | DiscoVars是一种新的变量选择方法，它通过创建变量之间的依赖网络并根据图中心性度量排名它们来确定变量的重要性，适用于聚类等学习任务。 |
| [^17] | [Efficient Multimodal Sampling via Tempered Distribution Flow.](http://arxiv.org/abs/2304.03933) | 采样高维分布是一个基本的问题，现有的基于输运的采样方法存在局限性，TemperFlow通过自适应地学习温度分布，解决了多模态问题，并证明了优于其他方法。 |
| [^18] | [OFTER: An Online Pipeline for Time Series Forecasting.](http://arxiv.org/abs/2304.03877) | 本文介绍了OFTER，一种专门针对中等规模多种时间序列的在线预测管线，能够胜过几种最先进的基准方法。OFTER是金融多元时间序列问题的理想解决方案。 |
| [^19] | [Conservative objective models are a special kind of contrastive divergence-based energy model.](http://arxiv.org/abs/2304.03866) | 本文证明了在离线基于模型的优化中，保守的客观模型（COMs）是一种特殊的基于对比散度能量模型，同时提出了用Langevin MCMC采样器替换梯度上升采样器，以提高样本质量。 |
| [^20] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^21] | [Biological Sequence Kernels with Guaranteed Flexibility.](http://arxiv.org/abs/2304.03775) | 本文研究了生物序列核的数学挑战，探讨了生物序列空间结构和生物序列相似性的特殊性对于核的影响。 |
| [^22] | [On the universal approximation property of radial basis function neural networks.](http://arxiv.org/abs/2304.02220) | 本论文研究了一种新的径向基函数神经网络类别，证明这些网络能够逼近任何连续多元函数，还讨论了有限个固定中心的RBF网络的逼近条件。 |
| [^23] | [Phase Diagram of Initial Condensation for Two-layer Neural Networks.](http://arxiv.org/abs/2303.06561) | 本文提出了一个两层神经网络初始凝聚的相图，旨在提供对神经网络动力学区域及其与初始化相关的超参数选择的综合理解。同时，我们详细解释了小初始化导致在初始训练阶段出现凝聚的基本机制。 |
| [^24] | [OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing.](http://arxiv.org/abs/2302.03003) | 这篇论文提出了一种基于最优传输理论的非配对图像转换方案，将低质量的非显微虹膜视网膜彩照转换成高质量的对应照片，并通过增强正则化方法改进了流程的灵活性、鲁棒性和适用性。在三个公开的视网膜图像数据集上进行了验证。 |
| [^25] | [Continual Causal Effect Estimation: Challenges and Opportunities.](http://arxiv.org/abs/2301.01026) | 本文介绍了因果效应估计中的挑战与机会，现有方法主要集中在源特定和静态观测数据上，实际问题在于观测数据只能逐步获得，本文提出了一种连续因果效应估计方法。 |
| [^26] | [Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders.](http://arxiv.org/abs/2212.13067) | 本文介绍了一种使用半监督自编码器以及在线主动学习方法，以尽可能少的标记样本来开发软测量传感器，从而显著降低了成本。在实验中，作者表明这种方法能够取得好的预测效果。 |
| [^27] | [A Theoretical Study of Inductive Biases in Contrastive Learning.](http://arxiv.org/abs/2211.14699) | 本文提供了对比学习中的归纳偏差的影响的理论分析，揭示了模型选择对学习过程的影响。 |
| [^28] | [Weighted Ensemble Self-Supervised Learning.](http://arxiv.org/abs/2211.09981) | 本文提出了一种带权重集成的自监督学习方法，通过开发允许数据相关的加权交叉熵损失的框架，可以提高最近自监督学习技术的性能，而不需要改变原有的架构，其在 ImageNet-1K 数据集上的表现优于最先进的 DINO 和 MSN 方法，特别是在小样本设置中表现最佳。 |
| [^29] | [Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes.](http://arxiv.org/abs/2211.02763) | 本文介绍了一种新的方法，使用GFlowNets和变分贝叶斯联合学习因果模型的结构和机制，不仅能够处理非线性和非高斯数据，在模拟数据上也能与几个基线方法相竞争。 |
| [^30] | [Moment Estimation for Nonparametric Mixture Models Through Implicit Tensor Decomposition.](http://arxiv.org/abs/2210.14386) | 本文提出了一种隐式张量分解矩估计方法，用于在高维空间中估计条件独立的混合模型，无需对分布进行参数化，通过开发高效的无张量操作，实现了计算上的可行性，证明了算法的竞争性能，并建立了混合物的可识别性。 |
| [^31] | [Global Convergence of SGD On Two Layer Neural Nets.](http://arxiv.org/abs/2210.11452) | 该论文证明了当深度为2的神经网络采用足够平滑凸的激活函数时，SGD可以收敛到全局最小值，证明过程中引入Frobenius范数正则化与恰当分布的参数初始化，同时拓展了连续时间的收敛结果。 |
| [^32] | [Label Propagation with Weak Supervision.](http://arxiv.org/abs/2210.03594) | 本文提出了一种利用弱监督信息的标签传播算法，通过利用未标记数据上的概率假设标签，结合局部几何特性和先验信息的质量，提供了一个误差界，并提出了一个框架，用于合并多个噪声信息源。在多个基准弱监督分类任务上展示了方法的能力，显示出对现有半监督和弱监督方法的改进。 |
| [^33] | [Predictive Inference with Feature Conformal Prediction.](http://arxiv.org/abs/2210.00173) | 本文提出基于特征符合预测的预测推断方法，通过利用深度表示学习的归纳偏置，扩展了符合预测到语义特征空间。从理论和实验结果来看，该方法优于常规符合预测，并在大规模任务上展现了最先进性能。 |
| [^34] | [Deep Double Descent via Smooth Interpolation.](http://arxiv.org/abs/2209.10080) | 本文研究神经网络在插值训练数据时的损失景观，发现其损失锐度遵循非光滑的二次曲线，当神经网络的复杂度逐渐增加时，测试误差会先降后升（即“双下降”现象）。 |
| [^35] | [High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent.](http://arxiv.org/abs/2207.01560) | 本文针对高维数据中的差分隐私经验风险最小化问题，提出了一种差分隐私贪心坐标下降算法，能够通过利用模型的结构性质，在广泛的问题范围内实现对维度的对数依赖性。 |
| [^36] | [Distributionally robust risk evaluation with a causality constraint and structural information.](http://arxiv.org/abs/2203.10571) | 本文提出了具有因果约束的分布鲁棒风险评估方法，并用神经网络逼近测试函数。在结构信息有限制时，提供了高效的优化方法。 |
| [^37] | [Who Increases Emergency Department Use? New Insights from the Oregon Health Insurance Experiment.](http://arxiv.org/abs/2201.07072) | 用因果机器学习方法研究发现医疗补助对急诊利用率具有异质性影响，只有少数群体驱动了整体效应，强度边际效应是其重要驱动因素，适当确定优先群体将有效控制急诊利用率。 |
| [^38] | [Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects.](http://arxiv.org/abs/2110.08693) | 本文提出了一种扩展平方根速度函数的新表示方法和度量方法，用于分析和比较树状三维物体，从而提高物体形状差异计算的精度和效率。 |
| [^39] | [Fundamental limits and algorithms for sparse linear regression with sublinear sparsity.](http://arxiv.org/abs/2101.11156) | 本文通过将贝叶斯推断中线性区域的自适应插值方法推广到次线性区域，建立了稀疏线性回归的归一化互信息和最小均方误差的精确渐近表达式，并提出了一种修改近似信息传递算法以接近最小均方误差基本极限的方法。 |

# 详细

[^1]: 反射扩散模型

    Reflected Diffusion Models. (arXiv:2304.04740v1 [stat.ML])

    [http://arxiv.org/abs/2304.04740](http://arxiv.org/abs/2304.04740)

    该论文提出了反射扩散模型，通过学习反射随机微分方程的扰动评分函数，将数据约束原则性地整合到生成过程中，以取代之前采用的导致不自然样本的阈值处理方案。

    

    基于分数的扩散模型学习将数据映射到噪声的随机微分方程的反向。然而，对于复杂任务，数值误差可以累积并导致高度不自然的样本。以前的工作通过阈值处理来缓解漂移，每次扩散步骤后投影到自然数据域（例如图像的像素空间），但这导致训练和生成过程之间存在不匹配。为了以一种原则性的方式合并数据约束，我们提出了反射扩散模型，该模型反向演化在数据支持的反射随机微分方程上。我们的方法通过一般化的分数匹配损失函数学习扰动评分函数，并扩展了标准扩散模型的关键组件，包括扩散引导、基于似然的训练和ODE采样。我们还弥合了阈值处理的理论差距:这样的方案只是反射SDE的离散化。在标准图像基准测试中，我们的

    Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalize score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and ODE sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected SDEs. On standard image benchmarks, our 
    
[^2]: Metropolized Hamiltonian Monte Carlo何时能证明优于Metropolis-adjusted Langevin算法？

    When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?. (arXiv:2304.04724v1 [stat.CO])

    [http://arxiv.org/abs/2304.04724](http://arxiv.org/abs/2304.04724)

    本文表明，从高维平滑目标分布采样时，Metropolized Hamiltonian Monte Carlo (HMC)比Metropolis-adjusted Langevin算法（MALA）更有效。

    

    本文分析了Metropolized Hamiltonian Monte Carlo (HMC)的混合时间，使用leapfrog积分器从$\mathbb{R}^d$分布中采样，该分布的对数密度平滑，具有Frobenius范数上的李普希茨黑塞，并满足等周性。我们将梯度复杂度限制为从一个暖启动达到$\epsilon$误差的总变异距离所需的$\tilde O(d^{1/4}\text{polylog}(1/\epsilon))$，并展示了选择比1更大的leapfrog步数的好处。为了超越Wu等人（2022）对Metropolis-adjusted Langevin algorithm (MALA)的分析，其在维度依赖性上是$\tilde{O}(d^{1/2}\text{polylog}(1/\epsilon))$，我们揭示了证明中的一个关键特征：连续HMC动态的位置和速度变量的离散化的联合分布近似不变。当通过leapfrog步数的归纳来展示这个关键特征时，我们能够获得各种量的矩的估计，这些量在限制Metropolized HMC的混合时间时是至关重要的，而在MALA中已知的类似结果是错误的。我们的结果表明，在采样高维平滑目标分布时，使用具有大量leapfrog步骤的Metropolized HMC可能比使用MALA更有效。

    We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) with the leapfrog integrator to sample from a distribution on $\mathbb{R}^d$ whose log-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfies isoperimetry. We bound the gradient complexity to reach $\epsilon$ error in total variation distance from a warm start by $\tilde O(d^{1/4}\text{polylog}(1/\epsilon))$ and demonstrate the benefit of choosing the number of leapfrog steps to be larger than 1. To surpass previous analysis on Metropolis-adjusted Langevin algorithm (MALA) that has $\tilde{O}(d^{1/2}\text{polylog}(1/\epsilon))$ dimension dependency in Wu et al. (2022), we reveal a key feature in our proof that the joint distribution of the location and velocity variables of the discretization of the continuous HMC dynamics stays approximately invariant. This key feature, when shown via induction over the number of leapfrog steps, enables us to obtain estimates on moments of various quantities tha
    
[^3]: 可扩展的基于随机核方法的多视角数据集成和预测

    Scalable Randomized Kernel Methods for Multiview Data Integration and Prediction. (arXiv:2304.04692v1 [stat.ME])

    [http://arxiv.org/abs/2304.04692](http://arxiv.org/abs/2304.04692)

    该论文开发了可伸缩的随机核方法，可以联合多个来源的数据，识别多视角数据之间非线性关系中最有效的贡献因素，并预测临床结果。在COVID-19数据上的应用发现了相关的分子标志。

    

    我们开发了可扩展的基于随机核方法，用于联合多个来源的数据，同时预测结果或将单元分类为两个或多个类别。所提出的方法可以模拟多视角数据中的非线性关系，并预测临床结果，能够识别最有助于视图之间关系的变量或变量组。我们使用随机傅里叶基函数近似平移不变的核函数来构造每个视图的非线性映射，并使用这些映射和结果变量来学习视图无关的低维表示。通过模拟研究，我们发现所提出的方法在多视角数据集成方面优于其他几种线性和非线性方法。当所提出的方法应用于涉及COVID-19的基因表达、代谢组学、蛋白组学和脂质组学数据时，我们确定了几种COVID-19严重程度和预后的分子标志。

    We develop scalable randomized kernel methods for jointly associating data from multiple sources and simultaneously predicting an outcome or classifying a unit into one of two or more classes. The proposed methods model nonlinear relationships in multiview data together with predicting a clinical outcome and are capable of identifying variables or groups of variables that best contribute to the relationships among the views. We use the idea that random Fourier bases can approximate shift-invariant kernel functions to construct nonlinear mappings of each view and we use these mappings and the outcome variable to learn view-independent low-dimensional representations. Through simulation studies, we show that the proposed methods outperform several other linear and nonlinear methods for multiview data integration. When the proposed methods were applied to gene expression, metabolomics, proteomics, and lipidomics data pertaining to COVID-19, we identified several molecular signatures forCO
    
[^4]: 论随机遍历的强稳定性

    On the strong stability of ergodic iterations. (arXiv:2304.04657v1 [math.PR])

    [http://arxiv.org/abs/2304.04657](http://arxiv.org/abs/2304.04657)

    本论文研究了迭代随机函数生成的过程的强稳定性，证明了适用于递归映射的温和条件下的强稳定性，并且提供了多个应用及相关领域的新结果。

    

    我们重新审视了由随机函数迭代生成的过程，这些函数由一个平稳且符合遍历条件的序列驱动。如果存在一个随机初始化使得该过程是稳定和遍历的，并且对于任何其他初始化，两个过程之间的差异几乎肯定收敛于零，那么这样的过程被称为强稳定。在对应递归映射上施加一些温和的条件，而不在驱动序列上施加任何条件下，我们展示了迭代的强稳定性。多个应用被研究，如随机逼近和排队。此外，我们推导出了具有依赖噪声的 Langevin 型迭代和多型分支过程的新结果。

    We revisit processes generated by iterated random functions driven by a stationary and ergodic sequence. Such a process is called strongly stable if a random initialization exists, for which the process is stationary and ergodic, and for any other initialization, the difference of the two processes converges to zero almost surely. Under some mild conditions on the corresponding recursive map, without any condition on the driving sequence, we show the strong stability of iterations. Several applications are surveyed such as stochastic approximation and queuing. Furthermore, new results are deduced for Langevin-type iterations with dependent noise and for multitype branching processes.
    
[^5]: 最近关于师生学习的研究综述

    A Survey on Recent Teacher-student Learning Studies. (arXiv:2304.04615v1 [cs.LG])

    [http://arxiv.org/abs/2304.04615](http://arxiv.org/abs/2304.04615)

    本文综述了最近关于师生学习的研究进展，重点讨论了知识蒸馏的变体，如教学助理蒸馏、课程蒸馏、掩码蒸馏和解耦蒸馏等，这些变体通过引入新的组件或改变学习过程来提高知识蒸馏的性能，已经取得了有希望的结果。

    

    知识蒸馏是一种将复杂深度神经网络（DNN）的知识传递给更小更快的DNN的方法，同时保持其准确性。最近的知识蒸馏变体包括教学助理蒸馏，课程蒸馏，掩码蒸馏和解耦蒸馏，旨在通过引入附加组件或更改学习过程来改善知识蒸馏的性能。教学助理蒸馏涉及中间模型教学助理，而课程蒸馏则遵循类似于人类教育的课程。掩码蒸馏专注于传递老师学到的注意力机制，而解耦蒸馏将蒸馏损失与任务损失分离。总体而言，这些知识蒸馏的变体已经显示出在改善知识蒸馏性能方面有希望的结果。

    Knowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.
    
[^6]: 使用深度ReLU网络逼近非线性函数的研究

    Approximation of Nonlinear Functionals Using Deep ReLU Networks. (arXiv:2304.04443v1 [stat.ML])

    [http://arxiv.org/abs/2304.04443](http://arxiv.org/abs/2304.04443)

    本文研究了使用深度ReLU网络逼近非线性函数的理论性质，并使用简单的三角剖分下构建了连续分段线性插值，实现了函数深度神经网络的逼近能力。

    

    最近几年，人们提出并研究了函数神经网络，以逼近在L^p([-1,1]^s)上定义的非线性连续函数，其中s≥1，1≤p<∞。然而，除逼近的普遍性外，它们的理论性质大多数还是未知的，或者现有的分析不适用于修正的线性单元（ReLU）激活函数。为填补这个空白，我们在此通过在简单的三角剖分下构建连续分段线性插值，研究了与ReLU激活函数相关的函数深度神经网络的逼近能力。此外，我们还在温和的正则条件下建立了所提出的函数深度ReLU网络的逼近速率。最后，我们的研究也可能有助于理解函数数据学习算法。

    In recent years, functional neural networks have been proposed and studied in order to approximate nonlinear continuous functionals defined on $L^p([-1, 1]^s)$ for integers $s\ge1$ and $1\le p<\infty$. However, their theoretical properties are largely unknown beyond universality of approximation or the existing analysis does not apply to the rectified linear unit (ReLU) activation function. To fill in this void, we investigate here the approximation power of functional deep neural networks associated with the ReLU activation function by constructing a continuous piecewise linear interpolation under a simple triangulation. In addition, we establish rates of approximation of the proposed functional deep ReLU networks under mild regularity conditions. Finally, our study may also shed some light on the understanding of functional data learning algorithms.
    
[^7]: 利用代理变量进行因果效应部分识别

    Partial Identification of Causal Effects Using Proxy Variables. (arXiv:2304.04374v1 [stat.ME])

    [http://arxiv.org/abs/2304.04374](http://arxiv.org/abs/2304.04374)

    这篇论文提出了一种无需完备性的部分识别方法，它为我们提供了一组界限，用于在未能控制混淆因素的情形下，评估治疗对结果变量因果效应。

    

    近年来，近端因果推断被提出为一种在未能控制混淆因素的情形下评估治疗对结果变量因果效应的框架。其中利用未被观测到的混淆因素的代理变量进行点估计，前提是这样的代理变量对混淆因素相当有关，然而这种完备性却是经验不可检验的。本文提出了一种不要求完备性的部分识别方法，并为感兴趣的因果效应提供了一组界限。该方法建立在敏感性分析的基础上，并且比现有的基于代理变量的方法要求更弱。这项工作在模拟数据和现实数据上进行了展示。

    Proximal causal inference is a recently proposed framework for evaluating the causal effect of a treatment on an outcome variable in the presence of unmeasured confounding (Miao et al., 2018a; Tchetgen Tchetgen et al., 2020). For nonparametric point identification, the framework leverages proxy variables of unobserved confounders, provided that such proxies are sufficiently relevant for the latter, a requirement that has previously been formalized as a completeness condition. Completeness is key to connecting the observed proxy data to hidden factors via a so-called confounding bridge function, identification of which is an important step towards proxy-based point identification of causal effects. However, completeness is well-known not to be empirically testable, therefore potentially restricting the application of the proximal causal framework. In this paper, we propose partial identification methods that do not require completeness and obviate the need for identification of a bridge
    
[^8]: 随机赌博机中的遗憾分布：期望和尾部风险之间的最优权衡

    Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk. (arXiv:2304.04341v1 [stat.ML])

    [http://arxiv.org/abs/2304.04341](http://arxiv.org/abs/2304.04341)

    该论文探讨了随机多臂赌博问题中，如何在期望和尾部风险之间做出最优权衡。提出了一种新的策略，能够实现最坏和实例相关的优异表现，并且能够最小化遗憾尾部概率。

    

    本文研究了随机多臂赌博问题中，遗憾分布的期望和尾部风险之间的权衡问题。我们完全刻画了策略设计中三个期望性质之间的相互作用：最坏情况下的最优性，实例相关的一致性和轻尾风险。我们展示了期望遗憾的顺序如何影响遗憾尾部概率的衰减率，同时包括了最坏情况和实例相关的情况。我们提出了一种新的策略，以表征对于任何遗憾阈值的最优遗憾尾部概率。具体地，对于任何给定的$\alpha \in [1/2, 1)$和$\beta \in [0, \alpha]$，我们的策略可以实现平均期望遗憾$\tilde O(T^\alpha)$的最坏情况下$\alpha$-最优和期望遗憾$\tilde O(T^\beta)$的实例相关的$\beta$-一致性，并且享有一定的概率可以避免$\tilde O(T^\delta)$的遗憾($\delta \geq \alpha$在最坏情况下和$\delta \geq \beta$在实例相关的情况下)。

    We study the trade-off between expectation and tail risk for regret distribution in the stochastic multi-armed bandit problem. We fully characterize the interplay among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is proposed to characterize the optimal regret tail probability for any regret threshold. Concretely, for any given $\alpha\in[1/2, 1)$ and $\beta\in[0, \alpha]$, our policy achieves a worst-case expected regret of $\tilde O(T^\alpha)$ (we call it $\alpha$-optimal) and an instance-dependent expected regret of $\tilde O(T^\beta)$ (we call it $\beta$-consistent), while enjoys a probability of incurring an $\tilde O(T^\delta)$ regret ($\delta\geq\alpha$ in the worst-case scenario and $\delta\geq\beta$ in the instance-dependent s
    
[^9]: PriorCVAE: 基于贝叶斯深度生成建模的可扩展 MCMC 参数推断

    PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v1 [stat.ML])

    [http://arxiv.org/abs/2304.04307](http://arxiv.org/abs/2304.04307)

    PriorCVAE 提出了一种处理高斯过程先验 MCMC 参数推断的贝叶斯深度生成建模新方法，可通过将 VAE 建模条件化于随机过程超参数处理超参数推断与学习先验之间的信息流断裂问题。

    

    在应用场景中，推理速度和模型灵活性至关重要，贝叶斯推断在具有随机过程先验的模型中（如高斯过程）被广泛应用。最近的研究表明，使用变分自动编码器（VAE）等深度生成模型可以编码由 GP 先验或其有限实现引起的计算瓶颈，并且所学生成器可以代替 MCMC 推断中的原始先验。虽然此方法实现了快速而高效的推理，但它丢失了关于随机过程超参数的信息，导致超参数推断不可能和学到的先验模糊不清。我们建议解决上述问题，通过将 VAE 建模条件化于随机过程超参数，以便超参数与 GP 实现一起进行编码。

    In applied fields where the speed of inference and model flexibility are crucial, the use of Bayesian inference for models with a stochastic process as their prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature has demonstrated that the computational bottleneck caused by GP priors or their finite realizations can be encoded using deep generative models such as variational autoencoders (VAEs), and the learned generators can then be used instead of the original priors during Markov chain Monte Carlo (MCMC) inference in a drop-in manner. While this approach enables fast and highly efficient inference, it loses information about the stochastic process hyperparameters, and, as a consequence, makes inference over hyperparameters impossible and the learned priors indistinct. We propose to resolve the aforementioned issue and disentangle the learned priors by conditioning the VAE on stochastic process hyperparameters. This way, the hyperparameters are encoded alongside GP real
    
[^10]: 关于“最近邻算法的任务特定数据有效性”的注记（arXiv：2304.04258v1 [stat.ML]）

    A Note on "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms". (arXiv:2304.04258v1 [stat.ML])

    [http://arxiv.org/abs/2304.04258](http://arxiv.org/abs/2304.04258)

    本文提出了一种更自然和可解释的效用函数，更好地反映了KNN模型的性能，提供了相应计算过程，该方法被称为软标签KNN-SV，与原始方法具有相同的时间复杂度。

    

    数据有效性是一个研究单个数据点对机器学习（ML）模型影响的日益增长的研究领域。基于合作博弈论和经济学，数据 Shapley 是一种有效的数据有效性计算方法。然而，人们都知道 Shapley 值（SV）的计算可能非常昂贵。幸运的是，Jia 等人（2019）表明，对于 K 最近邻（KNN）模型，计算 Data Shapley 竟然非常简单和高效。在本笔记中，我们重审了 Jia 等人（2019）的工作，并提出了一种更自然和可解释的效用函数，更好地反映了 KNN 模型的性能。我们推导了具有新效用函数的 KNN 分类器/回归器的 Data Shapley 的相应计算过程。我们的新方法被称为软标签 KNN-SV，与原始方法具有相同的时间复杂度。我们进一步提供了一种基于局部敏感哈希（LSH）的软标签 KNN-SV 的高效近似算法。

    Data valuation is a growing research field that studies the influence of individual data points for machine learning (ML) models. Data Shapley, inspired by cooperative game theory and economics, is an effective method for data valuation. However, it is well-known that the Shapley value (SV) can be computationally expensive. Fortunately, Jia et al. (2019) showed that for K-Nearest Neighbors (KNN) models, the computation of Data Shapley is surprisingly simple and efficient.  In this note, we revisit the work of Jia et al. (2019) and propose a more natural and interpretable utility function that better reflects the performance of KNN models. We derive the corresponding calculation procedure for the Data Shapley of KNN classifiers/regressors with the new utility functions. Our new approach, dubbed soft-label KNN-SV, achieves the same time complexity as the original method. We further provide an efficient approximation algorithm for soft-label KNN-SV based on locality sensitive hashing (LSH
    
[^11]: 数据驱动的多项式随机森林

    Data-driven multinomial random forest. (arXiv:2304.04240v1 [stat.ML])

    [http://arxiv.org/abs/2304.04240](http://arxiv.org/abs/2304.04240)

    本文加强了一些先前弱一些的随机森林变体的证明方法并提高了这些变体的数据利用率以获得更好的理论性能和实验性能，提出了一种数据驱动的多项式随机森林算法，其复杂度低且满足强一致性，在分类和回归问题上表现更好，并超过了标准随机森林。

    

    本文加强了一些先前弱一些的随机森林变体的证明方法，使其成为强一致性的证明方法，并提高了这些变体的数据利用率，以获得更好的理论性能和实验性能。此外，基于多项式随机森林（MRF）和伯努利随机森林（BRF），提出了一种数据驱动的多项式随机森林（DMRF）算法，其比MRF的复杂度更低，比BRF的复杂度更高，同时满足强一致性。在分类和回归问题上，它比之前只满足弱一致性的RF变体表现更好，甚至超过了标准随机森林。据我们所知，DMRF是目前最出色的低算法复杂性的强随机森林变体。

    In this article, we strengthen the proof methods of some previously weakly consistent variants of random forests into strongly consistent proof methods, and improve the data utilization of these variants, in order to obtain better theoretical properties and experimental performance. In addition, based on the multinomial random forest (MRF) and Bernoulli random forest (BRF), we propose a data-driven multinomial random forest (DMRF) algorithm, which has lower complexity than MRF and higher complexity than BRF while satisfying strong consistency. It has better performance in classification and regression problems than previous RF variants that only satisfy weak consistency, and in most cases even surpasses standard random forest. To the best of our knowledge, DMRF is currently the most excellent strongly consistent RF variant with low algorithm complexity
    
[^12]: 元卡洛调整朗之万(MALA)在光滑且等周条件下的混合简单证明

    A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry. (arXiv:2304.04095v1 [stat.ML])

    [http://arxiv.org/abs/2304.04095](http://arxiv.org/abs/2304.04095)

    本文证明了，在光滑和等周条件下，MALA的混合时间仅与Hessian矩阵的trace有关，而与其算子范数和log-concave没有关系。

    

    本文研究了在$\mathbb{R}^d$上样本目标密度的元卡洛调整朗之万（MALA）的混合时间。我们假设目标密度满足$\psi_\mu$-等周和它的黑塞矩阵的trace和算子范数分别受$L$和$\Upsilon$的限制。我们的主要结论是，从热启动开始，为了达到$\epsilon$总变差距离，MALA在$O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$次迭代中混合。值得注意的是，该结果不仅适用于对数凹采样设置，而且混合时间仅取决于$\Upsilon$，而不是其上界$Ld$。在$m$-强对数凹和$L$-光滑采样设置中，我们的界限恢复了以前的MALA的最小值混合界限。

    We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) for sampling a target density on $\mathbb{R}^d$. We assume that the target density satisfies $\psi_\mu$-isoperimetry and that the operator norm and trace of its Hessian are bounded by $L$ and $\Upsilon$ respectively. Our main result establishes that, from a warm start, to achieve $\epsilon$-total variation distance to the target density, MALA mixes in $O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$ iterations. Notably, this result holds beyond the log-concave sampling setting and the mixing time depends on only $\Upsilon$ rather than its upper bound $L d$. In the $m$-strongly logconcave and $L$-log-smooth sampling setting, our bound recovers the previous minimax mixing bound of MALA~\cite{wu2021minimax}.
    
[^13]: 具有亚群公平性约束的最优臂识别

    Best Arm Identification with Fairness Constraints on Subpopulations. (arXiv:2304.04091v1 [cs.LG])

    [http://arxiv.org/abs/2304.04091](http://arxiv.org/abs/2304.04091)

    本文解决了一个有关最优臂识别的问题，该问题要求所选臂必须对所有亚群都公平。我们提出了一个算法并证明其样本复杂度。

    

    我们提出、分析和解决了一个具有亚群公平性约束的最优臂识别问题（BAICS）。标准的最优臂识别问题旨在选择一个期望奖励最大的臂，其中期望是针对整个人群计算的。BAICS问题要求所选臂必须对所有亚群（例如，不同的族群、年龄组或客户类型）都公平，通过满足每个亚群条件下的期望奖励大于一些阈值的约束条件。BAICS问题旨在以高置信度正确鉴定出所有符合亚群约束条件的臂中期望奖励最大的臂。我们通过证明最小样本复杂度的最佳实现下界的闭式表示来分析BAICS问题的复杂度。然后，我们设计了一种算法，并证明该算法的样本复杂度与下界的阶数匹配。

    We formulate, analyze and solve the problem of best arm identification with fairness constraints on subpopulations (BAICS). Standard best arm identification problems aim at selecting an arm that has the largest expected reward where the expectation is taken over the entire population. The BAICS problem requires that an selected arm must be fair to all subpopulations (e.g., different ethnic groups, age groups, or customer types) by satisfying constraints that the expected reward conditional on every subpopulation needs to be larger than some thresholds. The BAICS problem aims at correctly identify, with high confidence, the arm with the largest expected reward from all arms that satisfy subpopulation constraints. We analyze the complexity of the BAICS problem by proving a best achievable lower bound on the sample complexity with closed-form representation. We then design an algorithm and prove that the algorithm's sample complexity matches with the lower bound in terms of order. A brief
    
[^14]: 带反向随机微分方程的深度生成建模

    Deep Generative Modeling with Backward Stochastic Differential Equations. (arXiv:2304.04049v1 [cs.LG])

    [http://arxiv.org/abs/2304.04049](http://arxiv.org/abs/2304.04049)

    该文提出了一种新的深度生成模型，名为BSDE-Gen，它将反向随机微分方程的灵活性与深度神经网络的强大能力结合，用于生成高维复杂目标数据，特别是在图像生成领域。BSDE-Gen的生成建模过程中引入了随机性和不确定性，是一种有效而自然的生成高维数据的方法。

    

    本文提出了一种新的深度生成模型，名为BSDE-Gen，它将反向随机微分方程（BSDEs）的灵活性与深度神经网络的强大能力相结合，以生成高维复杂目标数据，特别是在图像生成领域。在生成建模过程中引入随机性和不确定性，使BSDE-Gen成为一种有效而自然的生成高维数据的方法。本文提供了BSDE-Gen的理论框架，描述了其模型架构，给出了用于训练的最大平均偏差（MMD）损失函数，并报告了实验结果。

    This paper proposes a novel deep generative model, called BSDE-Gen, which combines the flexibility of backward stochastic differential equations (BSDEs) with the power of deep neural networks for generating high-dimensional complex target data, particularly in the field of image generation. The incorporation of stochasticity and uncertainty in the generative modeling process makes BSDE-Gen an effective and natural approach for generating high-dimensional data. The paper provides a theoretical framework for BSDE-Gen, describes its model architecture, presents the maximum mean discrepancy (MMD) loss function used for training, and reports experimental results.
    
[^15]: 高阶张量估计中的统计与计算速率

    Statistical and computational rates in high rank tensor estimation. (arXiv:2304.04043v1 [stat.ME])

    [http://arxiv.org/abs/2304.04043](http://arxiv.org/abs/2304.04043)

    本文提出了一种用于从高阶张量数据集中获得可能具有高秩信号的方法，包括高秩和低秩模型。提供了全面的统计和计算结果。发现高维潜变量张量具有对数秩，并实现了计算上的最优速率。

    

    高阶张量数据集在推荐系统、神经影像和社交网络等领域中常见。本文提出了一种可能的方法，用于从噪声观察中估计可能具有高秩信号的张量。我们考虑一个包括高秩和低秩模型的生成潜变量张量模型，包括但不限于简单的超图模型、单指标模型、低秩CP模型和低秩Tucker模型。在信号张量估计的统计和计算上，我们提供了全面的结果。我们发现高维潜变量张量具有对数秩；这一事实解释了低秩张量在应用中的普遍性。此外，我们提出了一种多项式时间谱算法，实现了计算上的最优速率。我们展示了统计计算差距仅出现在三阶或更高阶的潜变量张量中。我们提供了数值实验和两个真实数据应用。

    Higher-order tensor datasets arise commonly in recommendation systems, neuroimaging, and social networks. Here we develop probable methods for estimating a possibly high rank signal tensor from noisy observations. We consider a generative latent variable tensor model that incorporates both high rank and low rank models, including but not limited to, simple hypergraphon models, single index models, low-rank CP models, and low-rank Tucker models. Comprehensive results are developed on both the statistical and computational limits for the signal tensor estimation. We find that high-dimensional latent variable tensors are of log-rank; the fact explains the pervasiveness of low-rank tensors in applications. Furthermore, we propose a polynomial-time spectral algorithm that achieves the computationally optimal rate. We show that the statistical-computational gap emerges only for latent variable tensors of order 3 or higher. Numerical experiments and two real data applications are presented to
    
[^16]: DiscoVars: 一种新的数据分析视角-应用于聚类变量选择

    DiscoVars: A New Data Analysis Perspective -- Application in Variable Selection for Clustering. (arXiv:2304.03983v1 [cs.LG])

    [http://arxiv.org/abs/2304.03983](http://arxiv.org/abs/2304.03983)

    DiscoVars是一种新的变量选择方法，它通过创建变量之间的依赖网络并根据图中心性度量排名它们来确定变量的重要性，适用于聚类等学习任务。

    

    我们提出了一种新的数据分析视角来确定变量的重要性，而不考虑底层的学习任务。传统上，无论是分类问题还是回归问题，变量选择都被认为是监督学习的重要步骤。当与数据收集和存储相关的成本相当高时，如遥感数据，变量选择也变得至关重要。因此，我们提出了一种新的方法，通过首先创建所有变量之间的依赖网络，然后通过图中心性度量来排名它们（即节点），从而从数据中选择重要变量。根据首选中心度量选择前n个变量将得到一个强有力的候选变量子集，供进一步学习任务使用，例如聚类。我们将我们的工具展示为Shiny应用程序，这是一个用户友好的接口开发环境。我们还扩展了用户界面，用于两种文献中知名的无监督变量选择方法的比较。

    We present a new data analysis perspective to determine variable importance regardless of the underlying learning task. Traditionally, variable selection is considered an important step in supervised learning for both classification and regression problems. The variable selection also becomes critical when costs associated with the data collection and storage are considerably high for cases like remote sensing. Therefore, we propose a new methodology to select important variables from the data by first creating dependency networks among all variables and then ranking them (i.e. nodes) by graph centrality measures. Selecting Top-$n$ variables according to preferred centrality measure will yield a strong candidate subset of variables for further learning tasks e.g. clustering. We present our tool as a Shiny app which is a user-friendly interface development environment. We also extend the user interface for two well-known unsupervised variable selection methods from literature for compar
    
[^17]: 通过温度分布流实现高效多模态采样

    Efficient Multimodal Sampling via Tempered Distribution Flow. (arXiv:2304.03933v1 [stat.ME])

    [http://arxiv.org/abs/2304.03933](http://arxiv.org/abs/2304.03933)

    采样高维分布是一个基本的问题，现有的基于输运的采样方法存在局限性，TemperFlow通过自适应地学习温度分布，解决了多模态问题，并证明了优于其他方法。

    

    从高维分布中采样是统计研究和实践中的一个基本问题。但是，当目标密度函数不规范化且包含孤立模式时，就会出现巨大的挑战。为了解决这个问题，我们通过拟合一个被称为输运映射的可逆变换映射（在参考概率测量和目标分布之间），使得通过输运映射将参考样本推向前沿即可实现从目标分布中进行采样。我们通过使用Wasserstein梯度流理论理论分析了现有的基于输运的采样方法的局限性，并提出了一种名为TemperFlow的新方法来解决多模态问题。TemperFlow自适应地学习一系列温度分布，以逐步接近目标分布，并证明它克服了现有方法的局限性。各种实验证明了这种新型采样器相对于现有采样方法的卓越性能。

    Sampling from high-dimensional distributions is a fundamental problem in statistical research and practice. However, great challenges emerge when the target density function is unnormalized and contains isolated modes. We tackle this difficulty by fitting an invertible transformation mapping, called a transport map, between a reference probability measure and the target distribution, so that sampling from the target distribution can be achieved by pushing forward a reference sample through the transport map. We theoretically analyze the limitations of existing transport-based sampling methods using the Wasserstein gradient flow theory, and propose a new method called TemperFlow that addresses the multimodality issue. TemperFlow adaptively learns a sequence of tempered distributions to progressively approach the target distribution, and we prove that it overcomes the limitations of existing methods. Various experiments demonstrate the superior performance of this novel sampler compared 
    
[^18]: OFTER：一种专门针对中等规模多元时间序列的在线预测管线

    OFTER: An Online Pipeline for Time Series Forecasting. (arXiv:2304.03877v1 [stat.ML])

    [http://arxiv.org/abs/2304.03877](http://arxiv.org/abs/2304.03877)

    本文介绍了OFTER，一种专门针对中等规模多种时间序列的在线预测管线，能够胜过几种最先进的基准方法。OFTER是金融多元时间序列问题的理想解决方案。

    

    本文介绍了OFTER，一种专门针对中等规模多元时间序列的预测管线。OFTER利用kNN和广义回归神经网络的非参数模型，并与降维组件集成在一起。为了避免高维度的困境，我们采用基于修改后的最大相关系数的加权范数。我们介绍的管线专门针对在线任务进行设计，具有可解释性，并能够胜过几种最先进的基准方法。算法的计算效率、在线性质以及在低信噪比环境下运行的能力使OFTER成为金融多元时间序列问题的理想解决方案，例如每日股票预测。

    We introduce OFTER, a time series forecasting pipeline tailored for mid-sized multivariate time series. OFTER utilizes the non-parametric models of k-nearest neighbors and Generalized Regression Neural Networks, integrated with a dimensionality reduction component. To circumvent the curse of dimensionality, we employ a weighted norm based on a modified version of the maximal correlation coefficient. The pipeline we introduce is specifically designed for online tasks, has an interpretable output, and is able to outperform several state-of-the art baselines. The computational efficacy of the algorithm, its online nature, and its ability to operate in low signal-to-noise regimes, render OFTER an ideal approach for financial multivariate time series problems, such as daily equity forecasting. Our work demonstrates that while deep learning models hold significant promise for time series forecasting, traditional methods carefully integrating mainstream tools remain very competitive alternati
    
[^19]: 保守的客观模型是一种特殊的基于对比散度能量模型

    Conservative objective models are a special kind of contrastive divergence-based energy model. (arXiv:2304.03866v1 [stat.ML])

    [http://arxiv.org/abs/2304.03866](http://arxiv.org/abs/2304.03866)

    本文证明了在离线基于模型的优化中，保守的客观模型（COMs）是一种特殊的基于对比散度能量模型，同时提出了用Langevin MCMC采样器替换梯度上升采样器，以提高样本质量。

    

    本文理论上证明了保守的客观模型（COMs）用于离线基于模型的优化（MBO）是一种特殊的基于对比散度能量模型，其中能量函数既表示输入的无条件概率，也表示奖励变量的条件概率。虽然最初的公式只从其学习分布中抽样模型，但我们提出了一个简单的修复方法，用Langevin MCMC采样器替换梯度上升采样器。这产生了一种特殊的概率模型，其中采样输入的概率与其预测的奖励成比例。最后，我们证明，如果将模型分解，使无条件概率和条件概率分别建模，可以获得更好的样本。

    In this work we theoretically show that conservative objective models (COMs) for offline model-based optimisation (MBO) are a special kind of contrastive divergence-based energy model, one where the energy function represents both the unconditional probability of the input and the conditional probability of the reward variable. While the initial formulation only samples modes from its learned distribution, we propose a simple fix that replaces its gradient ascent sampler with a Langevin MCMC sampler. This gives rise to a special probabilistic model where the probability of sampling an input is proportional to its predicted reward. Lastly, we show that better samples can be obtained if the model is decoupled so that the unconditional and conditional probabilities are modelled separately.
    
[^20]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^21]: 具有保证灵活性的生物序列核

    Biological Sequence Kernels with Guaranteed Flexibility. (arXiv:2304.03775v1 [stat.ML])

    [http://arxiv.org/abs/2304.03775](http://arxiv.org/abs/2304.03775)

    本文研究了生物序列核的数学挑战，探讨了生物序列空间结构和生物序列相似性的特殊性对于核的影响。

    

    将机器学习应用于生物序列（DNA，RNA和蛋白质）具有推动人类健康、环境可持续性和基础生物学理解的巨大潜力。然而，许多现有的机器学习方法在这个问题域中是无效或不可靠的。本文通过核的视角从理论上研究这些挑战。尽管没有显式使用核的方法仍然在隐含地依赖它们，包括各种深度学习和基于物理的技术。虽然其他类型数据的核已被理论上广泛研究，但生物序列空间的结构（离散、可变长度序列）以及生物序列相似性的特殊数学挑战独一无二。我们正式分析生物序列核能够多好地近似

    Applying machine learning to biological sequences - DNA, RNA and protein has enormous potential to advance human health, environmental sustainability, and fundamental biological understanding. However, many existing machine learning methods are ineffective or unreliable in this problem domain. We study these challenges theoretically, through the lens of kernels. Methods based on kernels are ubiquitous: they are used to predict molecular phenotypes, design novel proteins, compare sequence distributions, and more. Many methods that do not use kernels explicitly still rely on them implicitly, including a wide variety of both deep learning and physics-based techniques. While kernels for other types of data are well-studied theoretically, the structure of biological sequence space (discrete, variable length sequences), as well as biological notions of sequence similarity, present unique mathematical challenges. We formally analyze how well kernels for biological sequences can approximate 
    
[^22]: 关于径向基函数神经网络普适逼近性质的研究

    On the universal approximation property of radial basis function neural networks. (arXiv:2304.02220v1 [cs.LG])

    [http://arxiv.org/abs/2304.02220](http://arxiv.org/abs/2304.02220)

    本论文研究了一种新的径向基函数神经网络类别，证明这些网络能够逼近任何连续多元函数，还讨论了有限个固定中心的RBF网络的逼近条件。

    

    本文研究了一种新的径向基函数神经网络类别，其中平滑因子被替换为位移。我们在激活函数的一定条件下证明了这些网络能够逼近欧几里得空间d维紧致子集上的任何连续多元函数。对于有限个固定中心的RBF网络，我们描述了保证任意精度逼近的条件。

    In this paper we consider a new class of RBF (Radial Basis Function) neural networks, in which smoothing factors are replaced with shifts. We prove under certain conditions on the activation function that these networks are capable of approximating any continuous multivariate function on any compact subset of the $d$-dimensional Euclidean space. For RBF networks with finitely many fixed centroids we describe conditions guaranteeing approximation with arbitrary precision.
    
[^23]: 两层神经网络初始凝聚的相图

    Phase Diagram of Initial Condensation for Two-layer Neural Networks. (arXiv:2303.06561v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06561](http://arxiv.org/abs/2303.06561)

    本文提出了一个两层神经网络初始凝聚的相图，旨在提供对神经网络动力学区域及其与初始化相关的超参数选择的综合理解。同时，我们详细解释了小初始化导致在初始训练阶段出现凝聚的基本机制。

    

    神经网络在不同初始化比例下呈现出不同行为的现象一直是深度学习领域中的难题。本文基于Luo等人早期的工作，提出了一个两层神经网络初始凝聚的相图。凝聚是神经网络在训练过程中权重向量集中于独立方向的现象，在非线性学习过程中是一种特性，使神经网络拥有更好的泛化能力。我们的相图旨在提供对神经网络动力学区域及其与初始化相关的超参数选择的综合理解。此外，我们详细展示了小初始化导致在初始训练阶段出现凝聚的基本机制。

    The phenomenon of distinct behaviors exhibited by neural networks under varying scales of initialization remains an enigma in deep learning research. In this paper, based on the earlier work by Luo et al.~\cite{luo2021phase}, we present a phase diagram of initial condensation for two-layer neural networks. Condensation is a phenomenon wherein the weight vectors of neural networks concentrate on isolated orientations during the training process, and it is a feature in non-linear learning process that enables neural networks to possess better generalization abilities. Our phase diagram serves to provide a comprehensive understanding of the dynamical regimes of neural networks and their dependence on the choice of hyperparameters related to initialization. Furthermore, we demonstrate in detail the underlying mechanisms by which small initialization leads to condensation at the initial training stage.
    
[^24]: OTRE: 在优化传输引导下的非配对图像转换中结合增强正则化

    OTRE: Where Optimal Transport Guided Unpaired Image-to-Image Translation Meets Regularization by Enhancing. (arXiv:2302.03003v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.03003](http://arxiv.org/abs/2302.03003)

    这篇论文提出了一种基于最优传输理论的非配对图像转换方案，将低质量的非显微虹膜视网膜彩照转换成高质量的对应照片，并通过增强正则化方法改进了流程的灵活性、鲁棒性和适用性。在三个公开的视网膜图像数据集上进行了验证。

    

    非显微虹膜视网膜彩照作图 (CFP) 由于不需要瞳孔扩张的优点而广泛可用，然而由于操作员，系统缺陷或患者相关原因而容易导致质量较差。精确的医学诊断和自动分析需要最佳视网膜图像质量。因此，我们利用最优传输（OT）理论提出了非配对图像转换方案，将低质量的视网膜 CFP 映射到高质量的对应物。此外，我们通过插入在 OT 引导下训练的先验知识、将基于模型的图像重建方法，即去噪正则化，进行了泛化，从而提高了我们的图像增强流程在临床实践中的灵活性、鲁棒性和适用性。我们将此称为增强正则化 (RE)。我们在三个公开的视网膜图像数据集上验证了集成框架 OTRE 的质量评估。

    Non-mydriatic retinal color fundus photography (CFP) is widely available due to the advantage of not requiring pupillary dilation, however, is prone to poor quality due to operators, systemic imperfections, or patient-related causes. Optimal retinal image quality is mandated for accurate medical diagnoses and automated analyses. Herein, we leveraged the Optimal Transport (OT) theory to propose an unpaired image-to-image translation scheme for mapping low-quality retinal CFPs to high-quality counterparts. Furthermore, to improve the flexibility, robustness, and applicability of our image enhancement pipeline in the clinical practice, we generalized a state-of-the-art model-based image reconstruction method, regularization by denoising, by plugging in priors learned by our OT-guided image-to-image translation network. We named it as regularization by enhancing (RE). We validated the integrated framework, OTRE, on three publicly available retinal image datasets by assessing the quality af
    
[^25]: 连续因果效应估计：挑战与机会

    Continual Causal Effect Estimation: Challenges and Opportunities. (arXiv:2301.01026v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01026](http://arxiv.org/abs/2301.01026)

    本文介绍了因果效应估计中的挑战与机会，现有方法主要集中在源特定和静态观测数据上，实际问题在于观测数据只能逐步获得，本文提出了一种连续因果效应估计方法。

    

    在许多领域，如经济学、医疗保健、公共政策、网络挖掘、在线广告和营销活动中，理解观察数据中的因果关系非常关键。尽管在因果效应估计方面已经取得了重大进展，如处理缺失的对照结果和治疗与控制组之间的选择偏差，但现有方法主要集中在源特定和静态观测数据上。这些学习策略假设所有观测数据已经在训练阶段可用且仅来自于一个来源。这种可访问性的实际问题在各种学术和工业应用中普遍存在。在大数据时代，我们面临着利用观测数据进行因果推断的新挑战，即针对逐步可用的观测数据的可扩展性和额外领域适应性问题。

    A further understanding of cause and effect within observational data is critical across many domains, such as economics, health care, public policy, web mining, online advertising, and marketing campaigns. Although significant advances have been made to overcome the challenges in causal effect estimation with observational data, such as missing counterfactual outcomes and selection bias between treatment and control groups, the existing methods mainly focus on source-specific and stationary observational data. Such learning strategies assume that all observational data are already available during the training phase and from only one source. This practical concern of accessibility is ubiquitous in various academic and industrial applications. That's what it boiled down to: in the era of big data, we face new challenges in causal inference with observational data, i.e., the extensibility for incrementally available observational data, the adaptability for extra domain adaptation proble
    
[^26]: 使用半监督自编码器的在线主动学习进行软测量开发

    Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders. (arXiv:2212.13067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13067](http://arxiv.org/abs/2212.13067)

    本文介绍了一种使用半监督自编码器以及在线主动学习方法，以尽可能少的标记样本来开发软测量传感器，从而显著降低了成本。在实验中，作者表明这种方法能够取得好的预测效果。

    

    数据驱动的软测量在工业和化学过程中被广泛使用，以预测难以测量的过程变量。这些传感器使用的回归模型通常需要大量标记的样本，然而，由于质量检查需要高昂的时间和成本，获取标签信息可能非常昂贵。在这种情况下，主动学习方法可能非常有益，因为它们可以建议查询最具信息量的标签。然而，为回归提出的大多数主动学习策略都集中在离线场景。本文将其中一些方法适应于流式场景，并展示了如何使用基于正交自编码器的半监督架构学习低维空间中的显著特征。我们也演示了如何使用田纳西东曼过程比较预测结果。

    Data-driven soft sensors are extensively used in industrial and chemical processes to predict hard-to-measure process variables whose real value is difficult to track during routine operations. The regression models used by these sensors often require a large number of labeled examples, yet obtaining the label information can be very expensive given the high time and cost required by quality inspections. In this context, active learning methods can be highly beneficial as they can suggest the most informative labels to query. However, most of the active learning strategies proposed for regression focus on the offline setting. In this work, we adapt some of these approaches to the stream-based scenario and show how they can be used to select the most informative data points. We also demonstrate how to use a semi-supervised architecture based on orthogonal autoencoders to learn salient features in a lower dimensional space. The Tennessee Eastman Process is used to compare the predictive 
    
[^27]: 对比学习中的归纳偏差的理论研究

    A Theoretical Study of Inductive Biases in Contrastive Learning. (arXiv:2211.14699v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14699](http://arxiv.org/abs/2211.14699)

    本文提供了对比学习中的归纳偏差的影响的理论分析，揭示了模型选择对学习过程的影响。

    

    理解自监督学习是重要而具有挑战性的。之前的理论研究对预训练损失的作用进行了研究，将神经网络视为普通的黑盒子。然而，Saunshi等人最近的研究表明，模型结构——之前的研究中很少关注的一个组成部分——对于自监督学习的下游性能也有显著的影响。在本研究中，我们提供了第一个将源于模型类的归纳偏差的影响纳入自监督学习的理论分析。特别地，我们关注对于视觉领域普遍使用的一种流行的自监督学习方法——对比学习。我们展示了当模型具有有限的容量时，对比表示会恢复与模型结构兼容的某些特殊聚类结构，但会忽略数据分布中的许多其他聚类结构。因此，我们的理论可以捕捉对比学习更为复杂的行为，提供了有关模型结构选择如何影响学习过程的见解。

    Understanding self-supervised learning is important but challenging. Previous theoretical works study the role of pretraining losses, and view neural networks as general black boxes. However, the recent work of Saunshi et al. argues that the model architecture -- a component largely ignored by previous works -- also has significant influences on the downstream performance of self-supervised learning. In this work, we provide the first theoretical analysis of self-supervised learning that incorporates the effect of inductive biases originating from the model class. In particular, we focus on contrastive learning -- a popular self-supervised learning method that is widely used in the vision domain. We show that when the model has limited capacity, contrastive representations would recover certain special clustering structures that are compatible with the model architecture, but ignore many other clustering structures in the data distribution. As a result, our theory can capture the more 
    
[^28]: 带权重集成的自监督学习

    Weighted Ensemble Self-Supervised Learning. (arXiv:2211.09981v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09981](http://arxiv.org/abs/2211.09981)

    本文提出了一种带权重集成的自监督学习方法，通过开发允许数据相关的加权交叉熵损失的框架，可以提高最近自监督学习技术的性能，而不需要改变原有的架构，其在 ImageNet-1K 数据集上的表现优于最先进的 DINO 和 MSN 方法，特别是在小样本设置中表现最佳。

    

    集成在监督学习中已被证明是提高模型性能、不确定性估计和健壮性的有效技术。自监督学习的进展使得利用大规模未标记语料库进行最先进的小样本和监督学习成为可能。本文研究了如何通过开发一个允许数据相关的加权交叉熵损失的框架来改进最近的自监督学习技术。我们避免对表示骨干进行集成；这个选择产生了一种高效的集成方法，它的训练成本很小，对下游评估不需要进行架构改变或计算开销。我们的方法在 ImageNet-1K 数据集上使用了两种最先进的自监督学习方法 DINO (Caron 等人，2021) 和 MSN (Assran 等人，2022)，在多个评估指标上均优于它们，尤其在小样本设置中表现最佳。我们探讨了几种加权方案，并发现…（未完成）

    Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that th
    
[^29]: GFlowNets与变分贝叶斯的因果结构和机制学习

    Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes. (arXiv:2211.02763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02763](http://arxiv.org/abs/2211.02763)

    本文介绍了一种新的方法，使用GFlowNets和变分贝叶斯联合学习因果模型的结构和机制，不仅能够处理非线性和非高斯数据，在模拟数据上也能与几个基线方法相竞争。

    

    贝叶斯因果结构学习旨在学习有向无环图（DAG）上的后验分布和定义父变量和子变量之间关系的机制。本文引入一种新的方法，使用变分贝叶斯联合学习因果模型的结构和机制，称为变分贝叶斯DAG-GFlowNet（VBG）。我们使用GFlowNets扩展了贝叶斯因果结构学习的方法，不仅学习结构的后验分布，还学习线性高斯模型的参数。我们在模拟数据上的结果表明，VBG在建模DAG和机制的后验分布时，不仅能够处理非线性和非高斯数据，而且还能与几个基线方法相竞争。

    Bayesian causal structure learning aims to learn a posterior distribution over directed acyclic graphs (DAGs), and the mechanisms that define the relationship between parent and child variables. By taking a Bayesian approach, it is possible to reason about the uncertainty of the causal model. The notion of modelling the uncertainty over models is particularly crucial for causal structure learning since the model could be unidentifiable when given only a finite amount of observational data. In this paper, we introduce a novel method to jointly learn the structure and mechanisms of the causal model using Variational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We extend the method of Bayesian causal structure learning using GFlowNets to learn not only the posterior distribution over the structure, but also the parameters of a linear-Gaussian model. Our results on simulated data suggest that VBG is competitive against several baselines in modelling the posterior over DAGs an
    
[^30]: 非参数混合模型的隐式张量分解矩估计方法

    Moment Estimation for Nonparametric Mixture Models Through Implicit Tensor Decomposition. (arXiv:2210.14386v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2210.14386](http://arxiv.org/abs/2210.14386)

    本文提出了一种隐式张量分解矩估计方法，用于在高维空间中估计条件独立的混合模型，无需对分布进行参数化，通过开发高效的无张量操作，实现了计算上的可行性，证明了算法的竞争性能，并建立了混合物的可识别性。

    

    本文提出了一种交替最小二乘型的数值优化方案用于在 $\mathbb{R}^n$ 中估计条件独立的混合模型，无需对分布进行参数化。根据矩的方法，我们解决了一个不完整的张量分解问题以学习混合权重和各分量的均值。然后，通过线性求解，计算分量分布的累积分布函数、高阶矩和其他统计量。通过开发高效的无张量操作，避免了高阶张量所带来的高成本问题，使得计算在高维情况下更加可行。数值实验证明了该算法的竞争性能，并且它适用于许多模型和应用。此外，我们提供了理论分析，从混合物的低阶矩中建立了可识别性，并保证了 ALS 算法的局部线性收敛性。

    We present an alternating least squares type numerical optimization scheme to estimate conditionally-independent mixture models in $\mathbb{R}^n$, without parameterizing the distributions. Following the method of moments, we tackle an incomplete tensor decomposition problem to learn the mixing weights and componentwise means. Then we compute the cumulative distribution functions, higher moments and other statistics of the component distributions through linear solves. Crucially for computations in high dimensions, the steep costs associated with high-order tensors are evaded, via the development of efficient tensor-free operations. Numerical experiments demonstrate the competitive performance of the algorithm, and its applicability to many models and applications. Furthermore we provide theoretical analyses, establishing identifiability from low-order moments of the mixture and guaranteeing local linear convergence of the ALS algorithm.
    
[^31]: 两层神经网络上SGD的全局收敛性证明

    Global Convergence of SGD On Two Layer Neural Nets. (arXiv:2210.11452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11452](http://arxiv.org/abs/2210.11452)

    该论文证明了当深度为2的神经网络采用足够平滑凸的激活函数时，SGD可以收敛到全局最小值，证明过程中引入Frobenius范数正则化与恰当分布的参数初始化，同时拓展了连续时间的收敛结果。

    

    在这篇论文中，我们证明了当深度为2的网络采用足够平滑且有边界的激活函数（比如sigmoid和tanh）时，SGD可以证明性地收敛到适当正则化的$\ell_2-$经验风险的全局最小值--对于任意数据和任意数量的门。我们在[1]的研究成果上进行了扩展，并在权重上添加了恒定量的Frobenius范数正则化，同时选取了恰当的分布对初始权重进行采样。我们还给出了一个连续时间的SGD收敛结果，同样适用于如SoftPlus这样的平滑无边界的激活函数。我们的关键想法是展示了存在于固定大小的神经网络上的损失函数，它们是“Villani函数”[1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\"odinger operators, 2020. arXiv:2004.06977

    In this note we demonstrate provable convergence of SGD to the global minima of appropriately regularized $\ell_2-$empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates, if they are using adequately smooth and bounded activations like sigmoid and tanh. We build on the results in [1] and leverage a constant amount of Frobenius norm regularization on the weights, along with sampling of the initial weights from an appropriate distribution. We also give a continuous time SGD convergence result that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence loss functions on constant sized neural nets which are "Villani Functions". [1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\"odinger operators, 2020. arXiv:2004.06977
    
[^32]: 弱监督下的标签传播算法

    Label Propagation with Weak Supervision. (arXiv:2210.03594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03594](http://arxiv.org/abs/2210.03594)

    本文提出了一种利用弱监督信息的标签传播算法，通过利用未标记数据上的概率假设标签，结合局部几何特性和先验信息的质量，提供了一个误差界，并提出了一个框架，用于合并多个噪声信息源。在多个基准弱监督分类任务上展示了方法的能力，显示出对现有半监督和弱监督方法的改进。

    This paper proposes a label propagation algorithm that utilizes weak supervision information, specifically probabilistic hypothesized labels on the unlabeled data, and provides an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. The approach is demonstrated on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.

    半监督学习和弱监督学习是当前机器学习应用中旨在减少标记数据需求的重要范式。本文介绍了一种新的对经典标签传播算法（LPA）（Zhu＆Ghahramani，2002）的分析，该算法利用了有用的先验信息，特别是未标记数据上的概率假设标签。我们提供了一个误差界，利用了底层图形的局部几何特性和先验信息的质量。我们还提出了一个框架，用于合并多个噪声信息源。特别是，我们考虑了弱监督的设置，其中我们的信息来源是弱标记者。我们在多个基准弱监督分类任务上展示了我们方法的能力，显示出对现有半监督和弱监督方法的改进。

    Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
    
[^33]: 基于特征符合预测的预测推断

    Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00173](http://arxiv.org/abs/2210.00173)

    本文提出基于特征符合预测的预测推断方法，通过利用深度表示学习的归纳偏置，扩展了符合预测到语义特征空间。从理论和实验结果来看，该方法优于常规符合预测，并在大规模任务上展现了最先进性能。

    

    符合预测是一种无分布技术，用于建立有效的预测间隔。虽然传统上人们在输出空间中进行符合预测，但这并不是唯一的可能性。在本文中，我们提出了基于特征的符合预测，通过利用深度表示学习的归纳偏置，扩展了符合预测对语义特征空间的范围。从理论上讲，我们证明了基于特征的符合预测在温和假设下可以证明优于常规符合预测。我们的方法不仅可以与普通符合预测结合使用，而且可以与其他自适应符合预测方法结合使用。除了现有预测推断基准测试的实验外，我们还展示了该方法在大规模任务（如ImageNet分类和Cityscapes图像分割）上的最先进性能。

    Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on large-scale tasks such as ImageNet classification and Cityscapes image segmentation.
    
[^34]: 基于平滑插值的深度双重下降

    Deep Double Descent via Smooth Interpolation. (arXiv:2209.10080v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10080](http://arxiv.org/abs/2209.10080)

    本文研究神经网络在插值训练数据时的损失景观，发现其损失锐度遵循非光滑的二次曲线，当神经网络的复杂度逐渐增加时，测试误差会先降后升（即“双下降”现象）。

    

    近期研究表明，超参数化深度网络具有插值噪声数据和表现良好的泛化性能的能力，这种现象通过测试误差的双重下降曲线得到了表征。然而，对于深度网络插值和泛化之间的精确关系还没有得到明确的定量描述。本文通过研究神经网络函数插值训练数据时与每个训练点周围的输入变量相关联的损失景观，定量衡量训练数据的拟合锐度。我们发现，输入空间中的损失锐度遵循一个非光滑的二次曲线，这与传统的多项式回归的分析结论有一定差异。此外，我们还发现当神经网络的复杂度逐渐增加时，测试误差会先降后升（即“双下降”现象），这与之前研究的结论有所不同。

    The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing good generalization performance, has been recently characterized in terms of the double descent curve for the test error. Common intuition from polynomial regression suggests that overparameterized networks are able to sharply interpolate noisy data, without considerably deviating from the ground-truth signal, thus preserving generalization ability. At present, a precise characterization of the relationship between interpolation and generalization for deep networks is missing. In this work, we quantify sharpness of fit of the training data interpolated by neural network functions, by studying the loss landscape w.r.t. to the input variable locally to each training point, over volumes around cleanly- and noisily-labelled training samples, as we systematically increase the number of model parameters and training epochs. Our findings show that loss sharpness in the input space follows 
    
[^35]: 贪心坐标下降实现高维私有经验风险最小化

    High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent. (arXiv:2207.01560v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01560](http://arxiv.org/abs/2207.01560)

    本文针对高维数据中的差分隐私经验风险最小化问题，提出了一种差分隐私贪心坐标下降算法，能够通过利用模型的结构性质，在广泛的问题范围内实现对维度的对数依赖性。

    

    在本文中，我们研究了差分隐私经验风险最小化（DP-ERM）。研究表明，随着维度的增加，DP-ERM的最坏情况效用会多项式降低。这是私有学习大型机器学习模型的主要障碍。在高维中，一些模型参数携带的信息比其他参数更多是很常见的。为了利用这一点，我们提出了一种差分隐私贪心坐标下降（DP-GCD）算法。在每个迭代步骤中，DP-GCD沿着梯度(大致地)最大的条目进行坐标梯度步骤。我们理论上证明，DP-GCD可以通过自然地利用其结构性质（例如拟稀疏解）在广泛的问题范围内实现对维度的对数依赖性。我们通过计算合成和真实数据集来说明这种行为。

    In this paper, we study differentially private empirical risk minimization (DP-ERM). It has been shown that the worst-case utility of DP-ERM reduces polynomially as the dimension increases. This is a major obstacle to privately learning large machine learning models. In high dimension, it is common for some model's parameters to carry more information than others. To exploit this, we propose a differentially private greedy coordinate descent (DP-GCD) algorithm. At each iteration, DP-GCD privately performs a coordinate-wise gradient step along the gradients' (approximately) greatest entry. We show theoretically that DP-GCD can achieve a logarithmic dependence on the dimension for a wide range of problems by naturally exploiting their structural properties (such as quasi-sparse solutions). We illustrate this behavior numerically, both on synthetic and real datasets.
    
[^36]: 具有因果约束和结构信息的分布鲁棒风险评估

    Distributionally robust risk evaluation with a causality constraint and structural information. (arXiv:2203.10571v3 [q-fin.MF] UPDATED)

    [http://arxiv.org/abs/2203.10571](http://arxiv.org/abs/2203.10571)

    本文提出了具有因果约束的分布鲁棒风险评估方法，并用神经网络逼近测试函数。在结构信息有限制时，提供了高效的优化方法。

    

    本文研究了基于时间数据的期望函数值的分布鲁棒评估。一组备选度量通过因果最优传输进行表征。我们证明了强对偶性，并将因果约束重构为无限维测试函数空间的最小化问题。我们通过神经网络逼近测试函数，并用Rademacher复杂度证明了样本复杂度。此外，当结构信息可用于进一步限制模糊集时，我们证明了对偶形式并提供高效的优化方法。对实现波动率和股指的经验分析表明，我们的框架为经典最优传输公式提供了一种有吸引力的替代方案。

    This work studies distributionally robust evaluation of expected function values over temporal data. A set of alternative measures is characterized by the causal optimal transport. We prove the strong duality and recast the causality constraint as minimization over an infinite-dimensional test function space. We approximate test functions by neural networks and prove the sample complexity with Rademacher complexity. Moreover, when structural information is available to further restrict the ambiguity set, we prove the dual formulation and provide efficient optimization methods. Empirical analysis of realized volatility and stock indices demonstrates that our framework offers an attractive alternative to the classic optimal transport formulation.
    
[^37]: 《谁增加了紧急诊所的使用率？来自俄勒冈医疗保险实验的新见解》

    Who Increases Emergency Department Use? New Insights from the Oregon Health Insurance Experiment. (arXiv:2201.07072v4 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2201.07072](http://arxiv.org/abs/2201.07072)

    用因果机器学习方法研究发现医疗补助对急诊利用率具有异质性影响，只有少数群体驱动了整体效应，强度边际效应是其重要驱动因素，适当确定优先群体将有效控制急诊利用率。

    

    我们利用因果机器学习方法提供了一项新见解，针对医疗保险实验中的一个主要结果：医疗补助增加了紧急诊所的使用率。我们发现医疗保险对紧急诊所使用率的影响存在有意义的异质性。个性化的治疗效应分布包括了一系列正负值，表明平均效应掩盖了相当大的异质性。一个极小的群体（约14%的参与者）在治疗效应分布的右尾驱动了整体效应。我们根据人口统计学和以往的利用情况确定了具有经济重要性的增加急诊使用的优先群体。强度边际效应是急诊利用率增加的重要驱动因素。

    We provide new insights regarding the headline result that Medicaid increased emergency department (ED) use from the Oregon experiment. We find meaningful heterogeneous impacts of Medicaid on ED use using causal machine learning methods. The individualized treatment effect distribution includes a wide range of negative and positive values, suggesting the average effect masks substantial heterogeneity. A small group-about 14% of participants-in the right tail of the distribution drives the overall effect. We identify priority groups with economically significant increases in ED usage based on demographics and previous utilization. Intensive margin effects are an important driver of increases in ED utilization.
    
[^38]: 学习树状三维物体的几何和拓扑的生成模型

    Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08693](http://arxiv.org/abs/2110.08693)

    本文提出了一种扩展平方根速度函数的新表示方法和度量方法，用于分析和比较树状三维物体，从而提高物体形状差异计算的精度和效率。

    

    如何分析展现出复杂几何和拓扑变化的详细三维生物物体，例如神经元和植物树？本文提出了一个新的数学框架，用于表示、比较和计算这些树状三维对象的形状差异，并定义了一种新的度量方法来量化将一个树状物体变形为另一个物体所需的弯曲、拉伸和分支滑动。

    How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
    
[^39]: 稀疏线性回归中的基本极限和算法与次线性稀疏性。

    Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v6 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2101.11156](http://arxiv.org/abs/2101.11156)

    本文通过将贝叶斯推断中线性区域的自适应插值方法推广到次线性区域，建立了稀疏线性回归的归一化互信息和最小均方误差的精确渐近表达式，并提出了一种修改近似信息传递算法以接近最小均方误差基本极限的方法。

    

    我们在次线性稀疏性区间建立了稀疏线性回归的归一化互信息和最小均方误差（MMSE）的精确渐近表达式。我们通过将贝叶斯推断中线性区域的自适应插值方法推广到次线性区域来实现我们的结果。我们还提出了一种修改着名的近似信息传递算法以接近MMSE基本极限的方法，并对其状态演化进行了严格分析。我们的结果表明，对于稀疏信号，复制和自适应插值方法中信号维数和观测个数之间的传统线性假设是不必要的。它们还展示了如何将现有的着名的线性区域的AMP算法修改为次线性区域。

    We establish exact asymptotic expressions for the normalized mutual information and minimum mean-square-error (MMSE) of sparse linear regression in the sub-linear sparsity regime. Our result is achieved by a generalization of the adaptive interpolation method in Bayesian inference for linear regimes to sub-linear ones. A modification of the well-known approximate message passing algorithm to approach the MMSE fundamental limit is also proposed, and its state evolution is rigorously analyzed. Our results show that the traditional linear assumption between the signal dimension and number of observations in the replica and adaptive interpolation methods is not necessary for sparse signals. They also show how to modify the existing well-known AMP algorithms for linear regimes to sub-linear ones.
    

