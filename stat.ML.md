# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fundamental Properties of Causal Entropy and Information Gain](https://rss.arxiv.org/abs/2402.01341) | 本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。 |
| [^2] | [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) | LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。 |
| [^3] | [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models](https://arxiv.org/abs/2402.12336) | 通过无监督对抗微调，提出了一种强大的CLIP视觉编码器，用于增强各种视觉-语言模型的鲁棒性。恶意第三方提供操纵图像的用户隐形攻击得以杜绝。 |
| [^4] | [Generating Survival Interpretable Trajectories and Data](https://arxiv.org/abs/2402.12331) | 提出了一种新的模型，能够生成生存轨迹和数据，并通过特定结构的自动编码器解决了预测、数据补充和生成原型时间相关轨迹等任务 |
| [^5] | [Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering](https://arxiv.org/abs/2402.12302) | 提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。 |
| [^6] | [Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling](https://arxiv.org/abs/2402.12292) | 该论文引入了一种贝叶斯框架，通过将数据驱动的正则化策略融入概率框架，提出了一种正则化去噪的方法，并应用于图像反演任务中，在成像中推动了贝叶斯推断。 |
| [^7] | [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264) | 使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。 |
| [^8] | [Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis](https://arxiv.org/abs/2402.12241) | 该论文分析了在动态系统中利用梯度下降进行监督学习的递归神经网络的性能，并证明在不需要海量过参数化的情况下，梯度下降可以达到最优性。 |
| [^9] | [Kernel KMeans clustering splits for end-to-end unsupervised decision trees](https://arxiv.org/abs/2402.12232) | 提出了一种新颖的端到端训练的无监督二叉树用于聚类，称为Kauri，通过贪婪最大化 kernel KMeans 目标来执行，无需定义质心，并在多个数据集上展示其性能优于其他方法。 |
| [^10] | [Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data](https://arxiv.org/abs/2402.12190) | 提出了一种基于机器学习的框架，用于个性化反事实癌症治疗建议，集成了多种多组学技术的专家，可提供优越性能和决策解释。 |
| [^11] | [Linear bandits with polylogarithmic minimax regret](https://arxiv.org/abs/2402.12042) | 该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。 |
| [^12] | [When Do Off-Policy and On-Policy Policy Gradient Methods Align?](https://arxiv.org/abs/2402.12034) | 该论文研究了离策略和在策略策略梯度方法之间的差异，并首次提出了减小该差距的条件，同时发现在条件不满足时会产生短板。 |
| [^13] | [Universal Generalization Guarantees for Wasserstein Distributionally Robust Models](https://arxiv.org/abs/2402.11981) | 本文建立了涵盖所有实际情况的Wasserstein分布鲁棒模型确切泛化保证，不需要限制性假设，适用于各种传输成本函数和损失函数，包括深度学习。 |
| [^14] | [Bayesian Active Learning for Censored Regression](https://arxiv.org/abs/2402.11973) | 该论文提出了一种在被审查回归中的贝叶斯主动学习方法($\mathcal{C}$-BALD)，通过推导被审查分布的熵和互信息，优化目标函数，在广泛数据集和模型下表现优异。 |
| [^15] | [Stochastic Hessian Fitting on Lie Group](https://arxiv.org/abs/2402.11858) | 本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。 |
| [^16] | [Statistical Test for Generated Hypotheses by Diffusion Models](https://arxiv.org/abs/2402.11789) | 本研究提出了一种统计检验方法，通过选择性推断框架，在考虑生成图像是由训练的扩散模型产生的条件下，量化医学图像诊断结果的可靠性。 |
| [^17] | [Evaluating the Effectiveness of Index-Based Treatment Allocation](https://arxiv.org/abs/2402.11771) | 本文介绍了一种评估基于指数的资源分配策略有效性的方法，通过翻译和扩展统计文献中的最新思想，提供了有效的估计器和计算渐近正确置信区间的方法。 |
| [^18] | [Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance](https://arxiv.org/abs/2402.11742) | 该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。 |
| [^19] | [Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding](https://arxiv.org/abs/2402.11736) | 该论文研究了一种联合概率分布，其支持趋于最小化最坏情况误差，证明了它在最坏情况积分误差集中不等式上优于i.i.d.蒙特卡罗。 |
| [^20] | [Learning Memory Kernels in Generalized Langevin Equations](https://arxiv.org/abs/2402.11705) | 提出一种学习广义朗之万方程中记忆核的新方法，通过正则化Prony方法估计相关函数并在Sobolev范数Loss函数和RKHS正则化下实现回归，在指数加权的$L^2$空间内获得改进性能，对比其他回归估计器展示了其优越性。 |
| [^21] | [Doubly Robust Inference in Causal Latent Factor Models](https://arxiv.org/abs/2402.11652) | 提出了一种双重稳健的估计量框架，可以在现代数据丰富的环境中估计存在未观察混杂因素下平均处理效应，具有良好的有限样本和渐近性质，并在参数速率下将其误差收敛为零均值高斯分布。 |
| [^22] | [Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling](https://arxiv.org/abs/2402.11552) | 本文提出了使用样条拟插值进行单变量密度估计，并将其应用于聚类建模，为构建适用的多元分布提供了新方法。 |
| [^23] | [OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations](https://arxiv.org/abs/2402.11427) | OptEx是第一个通过利用并行计算来减轻一阶优化的迭代瓶颈并增强效率的框架，使用核化梯度估计实现迭代的并行化，提供理论保证。 |
| [^24] | [An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration](https://arxiv.org/abs/2402.11410) | 给出了一种简单、高效、确定性的算法，该算法的校准距离误差最多为$2\sqrt{T}$ |
| [^25] | [Data-Driven Stochastic AC-OPF using Gaussian Processes](https://arxiv.org/abs/2402.11365) | 该论文提出了一种基于高斯过程的数据驱动算法，用于解决随机交流（AC）概率约束（CC）最优潮流（OPF）问题，并通过多个IEEE测试案例展示了其实证效率。 |
| [^26] | [Variational Entropy Search for Adjusting Expected Improvement](https://arxiv.org/abs/2402.11345) | 本论文通过变分推断的方法，将期望改进（EI）视为最大值熵搜索（MES）的特殊情况，提出了变分熵搜索（VES）方法和 VES-Gamma 算法，成功调整 EI 并展示其在贝叶斯优化方面的实用性。 |
| [^27] | [Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking](https://arxiv.org/abs/2402.11339) | 通过引入预处理算法识别展现对称性的正则子超图，从而提高超图在高阶链接预测中的表达能力和区分能力。 |
| [^28] | [Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach](https://arxiv.org/abs/2402.11338) | 该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。 |
| [^29] | [Learning by Reconstruction Produces Uninformative Features For Perception](https://arxiv.org/abs/2402.11337) | 重构学习所产生的特征对感知无用，需要通过其他策略如去噪学习来缓解这种不一致性。 |
| [^30] | [Deep adaptive sampling for surrogate modeling without labeled data](https://arxiv.org/abs/2402.11283) | 本工作提出了一种用于低正则性参数微分方程的代理建模的深度自适应采样方法($\text{DAS}^2$)，通过泛化深度自适应采样（DAS）方法，能有效处理高维度数据问题。 |
| [^31] | [Adaptive Split Balancing for Optimal Random Forest](https://arxiv.org/abs/2402.11228) | 介绍了自适应分割平衡森林（ASBF），可在学习树表示的同时，在复杂情况下实现极小极优性，并提出了一个本地化版本，在H\"older类下达到最小极优性。 |
| [^32] | [AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods](https://arxiv.org/abs/2402.11215) | AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。 |
| [^33] | [Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits](https://arxiv.org/abs/2402.11156) | 提出一种新型低秩矩阵估计方法LowPopArt，通过最小化量B(Q)提供更紧密的恢复保证，同时提出了一种新颖的实验设计标准，以及两种适用于一般Arm集的低秩线性赌博算法。 |
| [^34] | [Functional Partial Least-Squares: Optimal Rates and Adaptation](https://arxiv.org/abs/2402.11134) | 该论文提出了一种新的函数偏最小二乘估计器，其在一类椭球上实现了（近乎）最优的收敛速率，并引入了适应未知逆问题度的提前停止规则。 |
| [^35] | [DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation](https://arxiv.org/abs/2402.11120) | 本文探讨了无监督领域自适应中对抗鲁棒性的问题，通过建立对抗目标损失的泛化界限来解决目标域标签缺失带来的挑战。 |
| [^36] | [Building Trees for Probabilistic Prediction via Scoring Rules](https://arxiv.org/abs/2402.11052) | 提出了一种通过得分规则构建概率预测树的方法，以改进树的预测性能并生成更好的预测分布。 |
| [^37] | [Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains](https://arxiv.org/abs/2402.11039) | 提出了一种名为RAD的方法，通过领域标注的正则化来训练鲁棒的最后一层分类器，无需显式的领域标注，在具有领域标签噪声的情况下表现优越。 |
| [^38] | [Training Bayesian Neural Networks with Sparse Subspace Variational Inference](https://arxiv.org/abs/2402.11025) | 提出了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架 |
| [^39] | [Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention](https://arxiv.org/abs/2402.10198) | 本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。 |
| [^40] | [Sampling from the Mean-Field Stationary Distribution](https://arxiv.org/abs/2402.07355) | 本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。 |
| [^41] | [Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams](https://arxiv.org/abs/2402.06122) | 本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。 |
| [^42] | [Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction](https://arxiv.org/abs/2402.02171) | 我们提出了一种名为潜在IPS（LIPS）的新的Slate Bandit OPE估计器，通过在低维度的Slate抽象空间中定义重要性权重，并通过数据驱动的方式优化Slate抽象来减小偏差和方差。 |
| [^43] | [$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation](https://arxiv.org/abs/2402.02041) | 本文提出了一种应用于神经密度比估计的$\alpha$-散度损失函数($\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。 |
| [^44] | [Hidden Minima in Two-Layer ReLU Networks](https://arxiv.org/abs/2312.16819) | 本文研究了两层ReLU网络中的隐藏极小值现象，并提出方法来研究这些隐藏极小值的独特解析性质。 |
| [^45] | [Best-of-Both-Worlds Algorithms for Linear Contextual Bandits](https://arxiv.org/abs/2312.15433) | 该论文研究了线性情境赌博问题的两全其美算法，实现了在对抗性和随机情况下接近最优的遗憾界，其中包括了针对最小次优差距的多对数级别速率和在对抗性情况下的第一阶或第二阶界以及基于Shannon熵正则项的FTRL算法。 |
| [^46] | [Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks](https://arxiv.org/abs/2312.14922) | 神经网络在高维数据中发现统计模式，研究了如何高效地从高阶累积量中提取特征，并探讨了在尖峰累积量模型中的统计和计算限制。 |
| [^47] | [Efficient Computation of Sparse and Robust Maximum Association Estimators](https://arxiv.org/abs/2311.17563) | 本文研究如何在高维稀疏设置中利用新的优化程序实现鲁棒稀疏关联估计，通过增广Lagrange算法和自适应梯度下降的组合，提供了更精确的算法，并展示了相对现有算法的优势。 |
| [^48] | [A General Framework for User-Guided Bayesian Optimization](https://arxiv.org/abs/2311.14645) | ColaBO是第一个贝叶斯原理框架，允许领域专家定制优化程序，整合先验信念以加速优化。 |
| [^49] | [On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates](https://arxiv.org/abs/2311.13584) | 我们提出了对于基于扩散的生成模型在强对数凹数据分布假设下的完整收敛理论保证，获得了对于参数估计和采样算法的最优上限估计。 |
| [^50] | [Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems](https://arxiv.org/abs/2303.05754) | 提出了一种将扩散采样和Krylov子空间方法协同结合的新型高效采样策略。 |
| [^51] | [Distributional GFlowNets with Quantile Flows](https://arxiv.org/abs/2302.05793) | 本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。 |
| [^52] | [Stochastic optimal transport in Banach Spaces for regularized estimation of multivariate quantiles](https://arxiv.org/abs/2302.00982) | 提出了一种新的随机算法，用于利用傅里叶系数解决熵最优输运问题，并研究了其在无限维Banach空间中的几乎肯定收敛性和性能表现。 |
| [^53] | [Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression](https://arxiv.org/abs/2211.10968) | 研究了在目标函数不一定包含在核空间中的情况下，分治核函数的功能线性回归算法的统计优化性。 |
| [^54] | [Variance estimation in graphs with the fused lasso](https://arxiv.org/abs/2207.12638) | 研究了在一般图结构问题中的方差估计，开发了线性时间估计器并提供了上界，允许推广到更广泛的分布类。 |
| [^55] | [Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings](https://arxiv.org/abs/2104.08928) | 提出了一种基于群稀疏矩阵分解的方法，用于在新领域进行词嵌入的传递学习，以解决不同领域单词含义差异的挑战。 |
| [^56] | [Deep-Lock: Secure Authorization for Deep Neural Networks](https://arxiv.org/abs/2008.05966) | 本文提出了一种名为Deep-Lock的通用和轻量级基于密钥的模型锁定方案，通过使用S-盒对训练完毕的DNN模型的每个参数进行加密，并确保只有在应用正确的秘密密钥时模型才能正确运行，从而防止了DNN模型的未经授权使用。 |
| [^57] | [Tail-adaptive Bayesian shrinkage](https://arxiv.org/abs/2007.02192) | 提出了一种在多样的稀疏情况下具有尾部自适应收缩特性的鲁棒稀疏估计方法，通过新的全局-局部-尾部高斯混合分布实现，能够根据稀疏程度自适应调整先验的尾部重量以适应更多或更少信号。 |
| [^58] | [Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic](https://arxiv.org/abs/1910.08597) | 该方法提出了SplitSGD，通过简单而有效的稳态检测，在检测到稳态阶段时降低学习速率，使其适用于凸问题和训练神经网络，表现优于其他随机优化方法。 |
| [^59] | [Robust Estimation of Pareto's Scale Parameter from Grouped Data.](http://arxiv.org/abs/2401.14593) | 本文介绍了一种新的稳健估计方法（MTuM），用于从分组数据中估计Pareto分布的尾指数。该方法通过应用中心极限定理和模拟研究验证了其推理合理性。 |
| [^60] | [Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo.](http://arxiv.org/abs/2401.11665) | 本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。 |
| [^61] | [Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery.](http://arxiv.org/abs/2401.05394) | 该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。 |
| [^62] | [Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors.](http://arxiv.org/abs/2401.02739) | 本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。 |
| [^63] | [Sensitivity-Aware Amortized Bayesian Inference.](http://arxiv.org/abs/2310.11122) | 本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。 |
| [^64] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^65] | [Entropy-MCMC: Sampling from Flat Basins with Ease.](http://arxiv.org/abs/2310.05401) | 本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。 |
| [^66] | [On the Posterior Distribution in Denoising: Application to Uncertainty Quantification.](http://arxiv.org/abs/2309.13598) | 该论文研究了去噪中的后验分布及其与后验均值之间的关系，并应用于预训练去噪器的不确定性量化。提出了一种高效计算后验分布主成分和近似边际分布的方法。不需要显式计算高阶矩张量或进行训练或微调。 |
| [^67] | [Human Limits in Machine Learning: Prediction of Plant Phenotypes Using Soil Microbiome Data.](http://arxiv.org/abs/2306.11157) | 本论文深入研究了机器学习模型在预测土壤与植物表型之间联系方面的潜力，证明加入土壤物理化学性质和微生物种群密度等环境特征可以提高预测准确性。 |
| [^68] | [Balanced Training of Energy-Based Models with Adaptive Flow Sampling.](http://arxiv.org/abs/2306.00684) | 本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。 |
| [^69] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^70] | [Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees.](http://arxiv.org/abs/2305.03938) | 本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。 |
| [^71] | [Applications of No-Collision Transportation Maps in Manifold Learning.](http://arxiv.org/abs/2304.00199) | 本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。 |
| [^72] | [Towards black-box parameter estimation.](http://arxiv.org/abs/2303.15041) | 本文提出了一种基于弱参数结构假设的黑盒程序，用于估计统计模型参数。该程序可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。 |
| [^73] | [A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization.](http://arxiv.org/abs/2302.08766) | 该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。 |
| [^74] | [On Sampling with Approximate Transport Maps.](http://arxiv.org/abs/2302.04763) | 本研究探讨了两种基于传输映射的抽样方法，研究结果表明，基于流的提议可以处理多峰目标，在高维度和训练不良的情况下使用依赖于重新参数化的方法更加稳健。 |
| [^75] | [Inverse Solvability and Security with Applications to Federated Learning.](http://arxiv.org/abs/2211.14115) | 介绍了逆可解性和安全性的概念，以及其在联邦学习中的应用。论文提供了模型示例，展示了如何通过增加用户数量来增加可解性和安全性。 |
| [^76] | [From Denoising Diffusions to Denoising Markov Models.](http://arxiv.org/abs/2211.03595) | 本论文提出了一个统一的框架，将去噪扩散模型推广到广泛的空间中，并导致分数匹配的原始扩展，适用于各种应用程序。 |
| [^77] | [A Theoretical Analysis of the Learning Dynamics under Class Imbalance.](http://arxiv.org/abs/2207.00391) | 本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。 |
| [^78] | [Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios.](http://arxiv.org/abs/2206.01900) | 本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。 |

# 详细

[^1]: 因果熵和信息增益的基本性质

    Fundamental Properties of Causal Entropy and Information Gain

    [https://rss.arxiv.org/abs/2402.01341](https://rss.arxiv.org/abs/2402.01341)

    本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。

    

    最近的发展使得能够量化在结构因果模型(SCM)下的因果控制。这是通过引入一些量来编码在干预另一个变量时某个变量熵的变化来实现的。这些量被命名为因果熵和因果信息增益，旨在解决现有信息论方法在因果性在机器学习任务中起关键作用时的局限性。我们的研究通过建立和分析这些概念的基本性质，包括界限和链规则，对因果熵和因果信息增益的概念进行了形式上的理解。此外，我们阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义。总体而言，这个探索为提升因果机器学习任务铺平了道路。

    Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks th
    
[^2]: LoRA+: 大规模模型的高效低秩适应性

    LoRA+: Efficient Low Rank Adaptation of Large Models

    [https://arxiv.org/abs/2402.12354](https://arxiv.org/abs/2402.12354)

    LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。

    

    在这篇论文中，我们展示了低秩适应（LoRA）最初由胡等人（2021年）引入，导致对具有大宽度（嵌入维度）的模型进行微调时表现亚优。这是因为LoRA中的适配器矩阵A和B使用相同的学习率进行更新。通过对大宽度网络进行缩放参数的论证，我们展示了对适配器矩阵A和B使用相同的学习率不利于有效的特征学习。然后，我们表明LoRA的这种次优性可以简单地通过为LoRA适配器矩阵A和B设置不同的学习率以及一个精心选择的比率来进行校正。我们将这个提出的算法称为LoRA$+$。在我们广泛的实验证明中，LoRA$+$在相同计算成本下提高了性能（1-2％的改进）和微调速度（最多提速约2倍）。

    arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
    
[^3]: Robust CLIP: 对视觉嵌入进行无监督对抗微调以获得强大的大规模视觉-语言模型

    Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models

    [https://arxiv.org/abs/2402.12336](https://arxiv.org/abs/2402.12336)

    通过无监督对抗微调，提出了一种强大的CLIP视觉编码器，用于增强各种视觉-语言模型的鲁棒性。恶意第三方提供操纵图像的用户隐形攻击得以杜绝。

    

    诸如OpenFlamingo、LLaVA和GPT-4之类的多模型基础模型越来越广泛地用于各种真实世界任务。先前的工作表明，这些模型在视觉模态上极易受到对抗性攻击的影响。这些攻击可以用来传播虚假信息或欺骗用户，因此构成了一个重大风险，这使得大型多模型基础模型的鲁棒性成为一项紧迫的问题。我们提出了一种无监督对抗微调方案，以获得强大的CLIP视觉编码器，在所有依赖于CLIP的视觉下游任务（VLMs、零样本分类）上具有鲁棒性。特别地，我们展示了一旦更换原始的CLIP模型，用户在使用VLMs时会受到恶意第三方提供的操纵图像的潜在攻击。

    arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
    
[^4]: 生成生存可解释轨迹和数据

    Generating Survival Interpretable Trajectories and Data

    [https://arxiv.org/abs/2402.12331](https://arxiv.org/abs/2402.12331)

    提出了一种新的模型，能够生成生存轨迹和数据，并通过特定结构的自动编码器解决了预测、数据补充和生成原型时间相关轨迹等任务

    

    提出了一种基于应用特定结构的自动编码器来生成生存轨迹和数据的新模型。 它解决了三个任务。 首先，它基于Beran估计器为新生成的特征向量提供事件时间的预测和生存函数的形式。 第二，该模型基于给定的训练集生成额外数据，可以补充原始数据集。 第三，最重要的是，它为对象生成了一个原型时间相关轨迹，描述了如何改变对象的特征以实现不同时间事件的时间。 轨迹可以看作是一种反事实解释。 由于将特定加权方案纳入变分自动编码器中，所提出的模型在训练和推理过程中表现出鲁棒性。 该模型还通过解决分类问题确定了新生成数据的被审查指标。

    arXiv:2402.12331v1 Announce Type: cross  Abstract: A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classificatio
    
[^5]: 谱聚类中特征向量的渐近高斯波动

    Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering

    [https://arxiv.org/abs/2402.12302](https://arxiv.org/abs/2402.12302)

    提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。

    

    谱聚类的性能依赖于相似矩阵的特征向量的条目波动，该波动直到现在仍未得到描述。本文表明，一般尖峰随机矩阵模型的信号+噪声结构被转移到相应的格拉姆核矩阵的特征向量上，并且它们的条目波动在大维度区域呈高斯分布。这种类似于中心极限定理的结果是准确预测谱聚类的分类性能的最后一块缺失的拼图。提出的证明非常通用，仅依赖于噪声的旋转不变性。对合成和真实数据的数值实验表明了这个现象的普适性。

    arXiv:2402.12302v1 Announce Type: cross  Abstract: The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.
    
[^6]: 正则化去噪：贝叶斯模型和随后的Langevin-within-split Gibbs采样

    Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling

    [https://arxiv.org/abs/2402.12292](https://arxiv.org/abs/2402.12292)

    该论文引入了一种贝叶斯框架，通过将数据驱动的正则化策略融入概率框架，提出了一种正则化去噪的方法，并应用于图像反演任务中，在成像中推动了贝叶斯推断。

    

    本文介绍了一种贝叶斯框架，通过推导出一个概率化的相对应于正则化去噪（RED）范式的方法实现对图像反演。此外，它实现了一个蒙特卡洛算法，专门设计用于从所得的后验分布中采样，基于渐近精确数据增广（AXDA）。所提出的算法是一个嵌入了一个Langevin蒙特卡洛步骤的拆分Gibbs采样（SGS）的近似实例。所提出的方法应用于常见的成像任务，如去模糊、修补和超分辨率，通过大量的数值实验展示了其有效性。这些贡献通过在概率框架内利用数据驱动的正则化策略，促进了成像中的贝叶斯推断。

    arXiv:2402.12292v1 Announce Type: cross  Abstract: This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.
    
[^7]: 使用LoRA集成在精调LLMs中的不确定性量化

    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles

    [https://arxiv.org/abs/2402.12264](https://arxiv.org/abs/2402.12264)

    使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。

    

    精调大型语言模型可以提高特定任务的性能，尽管对于精调模型学到了什么、遗忘了什么以及如何信任其预测仍然缺乏一个一般的理解。我们提出了使用计算效率高的低秩适应集成对精调LLMs进行基于后验逼近的原则性不确定性量化。我们使用基于Mistral-7b的低秩适应集成分析了三个常见的多项选择数据集，并对其在精调过程中和之后对不同目标领域的感知复杂性和模型效能进行了定量和定性的结论。具体而言，基于数值实验支持，我们对那些对于给定架构难以学习的数据领域的熵不确定性度量提出了假设。

    arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
    
[^8]: 递归神经网络的梯度下降收敛性：非渐近性分析

    Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis

    [https://arxiv.org/abs/2402.12241](https://arxiv.org/abs/2402.12241)

    该论文分析了在动态系统中利用梯度下降进行监督学习的递归神经网络的性能，并证明在不需要海量过参数化的情况下，梯度下降可以达到最优性。

    

    我们分析在监督学习设置下利用梯度下降训练的递归神经网络在动态系统中的表现，并证明梯度下降可以在\emph{不}需要海量过参数化的情况下达到最优性。我们进行了深入的非渐近性分析，(i)利用序列长度$T$、样本大小$n$和环境维度$d$给出了网络大小$m$和迭代复杂度$\tau$的尖锐界限，(ii)确定了动态系统中长期依赖对收敛和网络宽度界限的显着影响，这些界限由激活函数的Lipschitz连续性决定的截止点来表征。值得注意的是，这一分析揭示了一个妥善初始化的递归神经网络在$n$个样本的情况下，可以通过网络大小$m$仅对数地随$n$扩展就达到最优性。这与以前的工作形成鲜明对比，前者需要高阶多项式分布。

    arXiv:2402.12241v1 Announce Type: new  Abstract: We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dep
    
[^9]: 将 Kernel KMeans 聚类拆分用于端到端无监督决策树

    Kernel KMeans clustering splits for end-to-end unsupervised decision trees

    [https://arxiv.org/abs/2402.12232](https://arxiv.org/abs/2402.12232)

    提出了一种新颖的端到端训练的无监督二叉树用于聚类，称为Kauri，通过贪婪最大化 kernel KMeans 目标来执行，无需定义质心，并在多个数据集上展示其性能优于其他方法。

    

    树是获取对相对较小数据集进行可解释预测的便利模型。虽然有很多关于监督学习中端到端构建这种树的提议，但在没有标签的情况下学习用于聚类的树仍然是一个未解决的挑战。大多数作品主要集中于使用树来解释另一个聚类算法的结果，我们在这里提出了一种新颖的端到端训练的无监督二叉树用于聚类：Kauri。该方法通过贪婪最大化 kernel KMeans 目标来执行，而无需定义质心。我们在多个数据集上将此模型与最近的无监督树进行比较，并展示当使用线性核时，Kauri 的性能相同。对于其他内核，Kauri 在许多情况下表现优于内核 KMeans 和 CART 决策树的串联。

    arXiv:2402.12232v1 Announce Type: cross  Abstract: Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.
    
[^10]: 基于AI的精准肿瘤学：基于多组学数据的个性化反事实治疗建议的机器学习框架

    Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data

    [https://arxiv.org/abs/2402.12190](https://arxiv.org/abs/2402.12190)

    提出了一种基于机器学习的框架，用于个性化反事实癌症治疗建议，集成了多种多组学技术的专家，可提供优越性能和决策解释。

    

    AI驱动的精准肿瘤学具有通过利用AI模型分析复杂患者特征与对应治疗结果之间互动的潜力，有望重塑癌症治疗。新技术平台促进了及时获取多模态肿瘤生物学数据，如单细胞多组学数据，使得这种数据的质量和数量可用于数据驱动的改进临床决策。本文提出了一个模块化的机器学习框架，旨在基于训练有关多种多组学技术的机器学习专家组成的集成来进行个性化反事实癌症治疗建议。这些专门的反事实专家根据技术不断聚合为性能更优越的专家，可提供决策的置信度和解释。

    arXiv:2402.12190v1 Announce Type: cross  Abstract: AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes. New technological platforms have facilitated the timely acquisition of multimodal data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making. In this work, we propose a modular machine learning framework designed for personalized counterfactual cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies. These specialized counterfactual experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision. The
    
[^11]: 具有多对数极小极小遗憾的线性赌博机

    Linear bandits with polylogarithmic minimax regret

    [https://arxiv.org/abs/2402.12042](https://arxiv.org/abs/2402.12042)

    该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。

    

    我们研究了一种线性随机赌博机的噪声模型，对于该模型，当我们选择越来越接近未知向量的单位球上的动作时，亚高斯噪声参数以线性方式消失。我们针对这个问题引入了一种算法，其在时间长度$T$的情况下呈对数$^3（T）$的最小遗憾缩放，与典型赌博机算法的平方根遗憾缩放形成鲜明对比。我们的策略基于加权最小二乘估计，通过几何论证实现了设计矩阵$V_t$在每个时间步骤$t$处的特征值关系$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$，这些几何论证与噪声模型无关，并可能具有独立的兴趣。这使我们能够严格控制每个时间步骤的期望遗憾为$O(\frac1{t})$的数量级，从而导致累积遗憾的对数缩放。

    arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
    
[^12]: 离策略和在策略策略梯度方法何时能够一致？

    When Do Off-Policy and On-Policy Policy Gradient Methods Align?

    [https://arxiv.org/abs/2402.12034](https://arxiv.org/abs/2402.12034)

    该论文研究了离策略和在策略策略梯度方法之间的差异，并首次提出了减小该差距的条件，同时发现在条件不满足时会产生短板。

    

    策略梯度方法是广泛采用的在连续动作空间中的强化学习算法。这些方法在许多应用领域取得成功，然而由于其臭名昭著的样本效率低，它们的使用仍然局限于可以快速准确模拟的问题。改进样本效率的常见方法是修改它们的目标函数，使之能够从离策略样本中计算而无需重要性采样。一个成熟的离策略目标就是游荡目标。本文研究了游荡目标与传统在策略目标之间的差异，我们称之为在离之间的差距。我们提供了第一个理论分析，展示了减少在离差距的条件，同时建立了当这些条件未被满足时出现的缺陷的经验证据。

    arXiv:2402.12034v1 Announce Type: cross  Abstract: Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.
    
[^13]: Wasserstein分布鲁棒模型的通用泛化保证

    Universal Generalization Guarantees for Wasserstein Distributionally Robust Models

    [https://arxiv.org/abs/2402.11981](https://arxiv.org/abs/2402.11981)

    本文建立了涵盖所有实际情况的Wasserstein分布鲁棒模型确切泛化保证，不需要限制性假设，适用于各种传输成本函数和损失函数，包括深度学习。

    

    分布稳健优化已经成为一种训练鲁棒机器学习模型的吸引人方式，能够捕捉数据的不确定性和分布的变化。最近的统计分析证明，基于Wasserstein模糊集构建的鲁棒模型具有很好的泛化保证，打破了维度灾难。然而，这些结果是在特定情况下获得的，以近似代价获得，或者在实践中难以验证的假设下获得的。相反，我们在本文中建立了涵盖所有实际情况的确切泛化保证，包括任何传输成本函数和任何损失函数，可能是非凸和非平滑的情况。例如，我们的结果适用于深度学习，而不需要限制性假设。我们通过一种将非平滑分析理论与经典集中结果相结合的新颖证明技术来实现这一结果。我们的方法足够通用，可以拓展至

    arXiv:2402.11981v1 Announce Type: cross  Abstract: Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that robust models built from Wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality. However, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice. In contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth. For instance, our result applies to deep learning, without requiring restrictive assumptions. We achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results. Our approach is general enough to ext
    
[^14]: 被审查回归的贝叶斯主动学习

    Bayesian Active Learning for Censored Regression

    [https://arxiv.org/abs/2402.11973](https://arxiv.org/abs/2402.11973)

    该论文提出了一种在被审查回归中的贝叶斯主动学习方法($\mathcal{C}$-BALD)，通过推导被审查分布的熵和互信息，优化目标函数，在广泛数据集和模型下表现优异。

    

    贝叶斯主动学习基于信息论方法，专注于最大化新观测提供给模型参数的信息。通常通过最大化贝叶斯主动学习通过分歧（BALD）获得函数来实现。然而，我们强调，在新数据点遭受审查时估计BALD是具有挑战性的，其中只观察到目标的剪辑值。为了解决这个问题，我们推导了被审查分布的熵和互信息，并推导了被审查回归中的主动学习的BALD目标（$\mathcal{C}$-BALD）。我们提出了一种新颖的建模方法来估计$\mathcal{C}$-BALD目标，并将其用于被审查设置中的主动学习。通过一系列广泛的数据集和模型，我们证明$\mathcal{C}$-BALD在被审查回归中优于其他贝叶斯主动学习方法。

    arXiv:2402.11973v1 Announce Type: new  Abstract: Bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. This is commonly done by maximising the Bayesian Active Learning by Disagreement (BALD) acquisitions function. However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed. To address this, we derive the entropy and the mutual information for censored distributions and derive the BALD objective for active learning in censored regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to estimate the $\mathcal{C}$-BALD objective and use it for active learning in the censored setting. Across a wide range of datasets and models, we demonstrate that $\mathcal{C}$-BALD outperforms other Bayesian active learning methods in censored regression.
    
[^15]: 在李群上的随机Hessian拟合

    Stochastic Hessian Fitting on Lie Group

    [https://arxiv.org/abs/2402.11858](https://arxiv.org/abs/2402.11858)

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。

    

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆。使用了一个Hessian拟合准则，可用于推导大部分常用方法，如BFGS、高斯牛顿、AdaGrad等。我们的研究揭示了不同Hessian拟合方法的不同收敛速率，例如，在欧几里德空间中的梯度下降的次线性速率和对称正定（SPL）矩阵和某些李群上的梯度下降的线性速率。在特定且足够一般的李群上的Hessian拟合问题在轻微条件下被证明是强凸的。为了确认我们的分析，这些方法在不同设置下进行了测试，如有噪声的Hessian-向量乘积、时变的Hessians和低精度算术。这些发现对依赖于随机二阶优化的方法是有用的。

    arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
    
[^16]: 通过扩散模型生成的假设的统计检验

    Statistical Test for Generated Hypotheses by Diffusion Models

    [https://arxiv.org/abs/2402.11789](https://arxiv.org/abs/2402.11789)

    本研究提出了一种统计检验方法，通过选择性推断框架，在考虑生成图像是由训练的扩散模型产生的条件下，量化医学图像诊断结果的可靠性。

    

    AI的增强性能加速了其融入科学研究。特别是，利用生成式AI创建科学假设是很有前途的，并且正在越来越多地应用于各个领域。然而，当使用AI生成的假设进行关键决策（如医学诊断）时，验证它们的可靠性至关重要。在本研究中，我们考虑使用扩散模型生成的图像进行医学诊断任务，并提出了一种统计检验来量化其可靠性。所提出的统计检验的基本思想是使用选择性推断框架，我们考虑在生成的图像是由经过训练的扩散模型产生的这一事实条件下的统计检验。利用所提出的方法，医学图像诊断结果的统计可靠性可以以p值的形式量化，从而实现在控制错误率的情况下进行决策。

    arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
    
[^17]: 评估基于指数的治疗分配有效性

    Evaluating the Effectiveness of Index-Based Treatment Allocation

    [https://arxiv.org/abs/2402.11771](https://arxiv.org/abs/2402.11771)

    本文介绍了一种评估基于指数的资源分配策略有效性的方法，通过翻译和扩展统计文献中的最新思想，提供了有效的估计器和计算渐近正确置信区间的方法。

    

    当资源稀缺时，需要一种分配策略来决定谁能获得资源。本文介绍了一种评估基于指数的分配策略的方法，该策略通过使用随机对照试验的数据，将有限数量的资源分配给最需要的人。我们从统计文献中翻译和扩展了最近的想法，提出了一种高效的估计器和计算渐近正确置信区间的方法，从而有效地得出有效的统计结论。

    arXiv:2402.11771v1 Announce Type: cross  Abstract: When resources are scarce, an allocation policy is needed to decide who receives a resource. This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods. This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. This enables us to effectively draw valid statistical conclusions, a critical gap in previous work. Our extensive experiments validate our methodology in practical sett
    
[^18]: 平衡数据，不平衡光谱：揭示具有光谱不平衡的类别差异

    Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance

    [https://arxiv.org/abs/2402.11742](https://arxiv.org/abs/2402.11742)

    该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。

    

    分类模型被期望在不同类别上表现同样良好，但在实践中，它们的表现往往存在较大差距。这个类别偏见问题在样本不平衡的数据集中得到了广泛研究，但在平衡数据集中相对被忽视。在这项工作中，我们提出了特征中的光谱不平衡概念作为类别差异的潜在来源，并研究光谱不平衡与类别偏见在理论和实践中的联系。为了建立光谱不平衡与类别差距之间的关系，我们开发了一个用于研究类别差异的理论框架，并推导了高维混合模型设定中每类错误的精确表达式。然后我们在11个不同的最先进的预训练编码器中研究了这一现象，并展示了我们提出的框架如何用于比较编码器的质量，以及评估和组合数据增强策略。

    arXiv:2402.11742v1 Announce Type: new  Abstract: Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation stra
    
[^19]: 基于核Gibbs测度的蒙特卡罗：概率随机放牧的保证

    Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding

    [https://arxiv.org/abs/2402.11736](https://arxiv.org/abs/2402.11736)

    该论文研究了一种联合概率分布，其支持趋于最小化最坏情况误差，证明了它在最坏情况积分误差集中不等式上优于i.i.d.蒙特卡罗。

    

    Kernel herding属于一类确定性的四位数法，旨在通过再生核希尔伯特空间（RKHS）上的最坏情况积分误差。尽管有很强的实验支持，但在通常情况下，即RKHS是无限维时，证明这种最坏情况误差以比标准积分节点数量的平方根更快的速率减少是困难的。在这篇理论论文中，我们研究了一个关于积分节点的联合概率分布，其支持趋于最小化与核放牧相同的最坏情况误差。我们证明它优于i.i.d.蒙特卡罗，意味着在最坏情况积分误差上具有更紧的集中不等式。尽管尚未提高速率，但这表明了研究Gibbs测度的数学工具可以帮助理解核放牧及其变体在计算上的改进程度

    arXiv:2402.11736v1 Announce Type: new  Abstract: Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional. In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. We prove that it does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computation
    
[^20]: 在广义朗之万方程中学习记忆核

    Learning Memory Kernels in Generalized Langevin Equations

    [https://arxiv.org/abs/2402.11705](https://arxiv.org/abs/2402.11705)

    提出一种学习广义朗之万方程中记忆核的新方法，通过正则化Prony方法估计相关函数并在Sobolev范数Loss函数和RKHS正则化下实现回归，在指数加权的$L^2$空间内获得改进性能，对比其他回归估计器展示了其优越性。

    

    我们引入了一种新颖的方法来学习广义朗之万方程中的记忆核。该方法最初利用正则化Prony方法从轨迹数据中估计相关函数，然后通过基于Sobolev范数的回归和RKHS正则化来进行回归。我们的方法保证在指数加权的$L^2$空间内获得了改进的性能，核估计误差受控于估计相关函数的误差。我们通过数值示例展示了我们的估计器相对于依赖于$L^2$损失函数的其他回归估计器以及从逆拉普拉斯变换推导出的估计器的优越性，这些示例突显了我们的估计器在各种权重参数选择上的持续优势。此外，我们提供了包括力和漂移项在方程中的应用示例。

    arXiv:2402.11705v1 Announce Type: cross  Abstract: We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.
    
[^21]: 因果潜在因子模型中的双重稳健推断

    Doubly Robust Inference in Causal Latent Factor Models

    [https://arxiv.org/abs/2402.11652](https://arxiv.org/abs/2402.11652)

    提出了一种双重稳健的估计量框架，可以在现代数据丰富的环境中估计存在未观察混杂因素下平均处理效应，具有良好的有限样本和渐近性质，并在参数速率下将其误差收敛为零均值高斯分布。

    

    本文介绍了一种在现代数据丰富环境中估计存在未观察混杂因素下的平均处理效应的新框架，该环境具有大量单位和结果。所提出的估计量是双重稳健的，结合了结果填补、倒数概率加权以及一种用于矩阵补全的新型交叉配对程序。我们推导了有限样本和渐近保证，并展示了新估计量的误差收敛到参数速率下的零均值高斯分布。模拟结果展示了本文分析的估计量的形式特性的实际相关性。

    arXiv:2402.11652v1 Announce Type: cross  Abstract: This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.
    
[^22]: 基于样条拟插值的经验密度估计及其在Copulas聚类建模中的应用

    Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling

    [https://arxiv.org/abs/2402.11552](https://arxiv.org/abs/2402.11552)

    本文提出了使用样条拟插值进行单变量密度估计，并将其应用于聚类建模，为构建适用的多元分布提供了新方法。

    

    密度估计是各个领域内用于建模和理解数据基础分布的基本技术。密度估计的主要目标是估计随机变量的概率密度函数。在处理单变量或多变量数据时，这一过程尤为重要，对于聚类、异常检测和生成建模等任务至关重要。本文提出了使用样条拟插值来近似单变量密度，并将其应用于聚类建模的方法。所使用的聚类技术基于构建适用的多元分布，这取决于对单变量经验密度（边际密度）的估计。该逼近是通过使用提出的样条拟插值实现的，同时用于建模所寻求的聚类划分的联合分布。

    arXiv:2402.11552v1 Announce Type: cross  Abstract: Density estimation is a fundamental technique employed in various fields to model and to understand the underlying distribution of data. The primary objective of density estimation is to estimate the probability density function of a random variable. This process is particularly valuable when dealing with univariate or multivariate data and is essential for tasks such as clustering, anomaly detection, and generative modeling. In this paper we propose the mono-variate approximation of the density using spline quasi interpolation and we applied it in the context of clustering modeling. The clustering technique used is based on the construction of suitable multivariate distributions which rely on the estimation of the monovariate empirical densities (marginals). Such an approximation is achieved by using the proposed spline quasi-interpolation, while the joint distributions to model the sought clustering partition is constructed with the 
    
[^23]: OptEx: 利用近似并行化迭代加速一阶优化

    OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations

    [https://arxiv.org/abs/2402.11427](https://arxiv.org/abs/2402.11427)

    OptEx是第一个通过利用并行计算来减轻一阶优化的迭代瓶颈并增强效率的框架，使用核化梯度估计实现迭代的并行化，提供理论保证。

    

    第一阶优化（FOO）算法在诸如机器学习和信号去噪等众多计算领域中至关重要。然而，将它们应用于神经网络训练等复杂任务往往导致显著的低效，因为需要许多顺序迭代以实现收敛。为此，我们引入了第一阶优化加速近似并行迭代（OptEx），这是第一个通过利用并行计算来减轻其迭代瓶颈而增强FOO效率的框架。OptEx采用核化梯度估计来利用梯度历史进行未来梯度预测，实现了迭代的并行化 -- 这是一种曾经被认为由于FOO中固有的迭代依赖而不切实际的策略。我们为我们的核化梯度估计的可靠性和基于SGD的OptEx的迭代复杂度提供理论保证，并确认了其可靠性。

    arXiv:2402.11427v1 Announce Type: cross  Abstract: First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming t
    
[^24]: 获得$2\sqrt{T}$到校准的基本预测器

    An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration

    [https://arxiv.org/abs/2402.11410](https://arxiv.org/abs/2402.11410)

    给出了一种简单、高效、确定性的算法，该算法的校准距离误差最多为$2\sqrt{T}$

    

    Blasiok等人[2023]提出了校准距离作为一种自然的校准误差度量，与预期的校准误差(ECE)不同，它是连续的。最近，Qiao和Zheng [2024]给出了一个非构造性的论证，建立了一种在线预测器的存在，该预测器可以在对抗设置中获得$O(\sqrt{T})$的校准距离，而对于ECE来说是不可能的。他们将找到一种明确的、高效的算法作为一个需要解决的问题。我们解决了这个问题，并给出了一个非常简单、高效、确定性的算法，该算法的校准距离误差最多为$2\sqrt{T}$。

    arXiv:2402.11410v1 Announce Type: new  Abstract: Blasiok et al. [2023] proposed distance to calibration as a natural measure of calibration error that unlike expected calibration error (ECE) is continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument establishing the existence of an online predictor that can obtain $O(\sqrt{T})$ distance to calibration in the adversarial setting, which is known to be impossible for ECE. They leave as an open problem finding an explicit, efficient algorithm. We resolve this problem and give an extremely simple, efficient, deterministic algorithm that obtains distance to calibration error at most $2\sqrt{T}$.
    
[^25]: 使用高斯过程的数据驱动随机交流优化潮流

    Data-Driven Stochastic AC-OPF using Gaussian Processes

    [https://arxiv.org/abs/2402.11365](https://arxiv.org/abs/2402.11365)

    该论文提出了一种基于高斯过程的数据驱动算法，用于解决随机交流（AC）概率约束（CC）最优潮流（OPF）问题，并通过多个IEEE测试案例展示了其实证效率。

    

    这篇论文聚焦于发展一种基于机器学习的数据驱动算法，用于解决随机交流（AC）概率约束（CC）最优潮流（OPF）问题。虽然AC CC-OPF问题在学术界取得了成功，但由于高度非线性和计算要求很高，限制了其实际影响。该方法旨在解决这一限制，并通过应用于多个IEEE测试案例来展示其实证效率。为了解决非凸和计算复杂的CC AC-OPF问题，该方法依赖于机器学习高斯过程回归（GPR）模型。完整高斯过程（GP）方法能够学习一个简单但非凸的数据驱动近似至AC潮流方程，能够纳入不确定输入。该方法使用各种近似来传播GP不确定性。

    arXiv:2402.11365v1 Announce Type: new  Abstract: The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases. To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning Gaussian process regression (GPR) model. The full Gaussian process (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs. The proposed approach uses various approximations for GP-uncertainty propagation. The full GP CC-OPF ap
    
[^26]: 变分熵搜索用于调整期望改进

    Variational Entropy Search for Adjusting Expected Improvement

    [https://arxiv.org/abs/2402.11345](https://arxiv.org/abs/2402.11345)

    本论文通过变分推断的方法，将期望改进（EI）视为最大值熵搜索（MES）的特殊情况，提出了变分熵搜索（VES）方法和 VES-Gamma 算法，成功调整 EI 并展示其在贝叶斯优化方面的实用性。

    

    Bayesian optimization 是一种广泛使用的优化黑盒函数的技术，期望改进（EI）是该领域中最常用的获取函数。虽然 EI 通常被视为与其他信息理论获取函数（如熵搜索（ES）和最大值熵搜索（MES））不同，但我们的工作揭示了，通过变分推断（VI）方法，EI 可以被视为 MES 的一种特殊情况。在这一背景下，我们开发了变分熵搜索（VES）方法和 VES-Gamma 算法，通过将信息理论概念的原则整合到 EI 中来调整 EI。VES-Gamma 的有效性在各种测试函数和真实数据集中得到了证明，突出了它在贝叶斯优化场景中的理论和实际用途。

    arXiv:2402.11345v1 Announce Type: cross  Abstract: Bayesian optimization is a widely used technique for optimizing black-box functions, with Expected Improvement (EI) being the most commonly utilized acquisition function in this domain. While EI is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (ES) and max-value entropy search (MES), our work reveals that EI can be considered a special case of MES when approached through variational inference (VI). In this context, we have developed the Variational Entropy Search (VES) methodology and the VES-Gamma algorithm, which adapts EI by incorporating principles from information-theoretic concepts. The efficacy of VES-Gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in Bayesian optimization scenarios.
    
[^27]: 通过超图对称性打破进行高阶链接预测

    Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking

    [https://arxiv.org/abs/2402.11339](https://arxiv.org/abs/2402.11339)

    通过引入预处理算法识别展现对称性的正则子超图，从而提高超图在高阶链接预测中的表达能力和区分能力。

    

    一种超图由一组节点以及称为超边的节点子集合组成。更高阶链接预测是预测一个超图中是否存在缺失的超边的任务。为高阶链接预测学习的超边表示在同构下不失去区分能力时具有完全表达性。许多现有的超图表示学习器受到广义Weisfeiler Lehman-1（GWL-1）算法的表达能力限制，它是Weisfeiler Lehman-1算法的推广。然而，GWL-1的表达能力有限。事实上，具有相同GWL-1值节点的诱导子超图是无法区分的。此外，在超图上进行消息传递可能已经在GPU内存上变得计算昂贵。为了解决这些限制，我们设计了一种可以识别出展现对称性的特定正则子超图的预处理算法。

    arXiv:2402.11339v1 Announce Type: new  Abstract: A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our p
    
[^28]: 具有部分反馈的公平分类：一种基于探索的数据收集方法

    Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach

    [https://arxiv.org/abs/2402.11338](https://arxiv.org/abs/2402.11338)

    该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。

    

    在许多预测场景（例如信贷放款）中，只有过去被积极分类的样本才会观察到真实结果。这些过去的观察结果形成了用于训练分类器以进行未来预测的训练数据集。然而，这样的训练数据集缺乏关于过去（错误地）被负面分类的样本结果的信息，可能导致错误的分类器。我们提出了一种方法，利用可用数据训练分类器，并提供一系列探索策略来收集关于否则会被忽略的子群体的结果数据。对于任何探索策略，该方法都具有以下保证：（1）所有子群体都得到了探索，（2）假阳性的比例受到了限制，（3）训练的分类器收敛到一个“期望”的分类器。正确的探索策略取决于上下文；它可以选择以改善学习保证

    arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
    
[^29]: 通过重构学习产生对感知无用的特征

    Learning by Reconstruction Produces Uninformative Features For Perception

    [https://arxiv.org/abs/2402.11337](https://arxiv.org/abs/2402.11337)

    重构学习所产生的特征对感知无用，需要通过其他策略如去噪学习来缓解这种不一致性。

    

    输入空间重构是一种吸引人的表示学习范式。尽管重构和生成的可解释性，我们发现了通过重构学习与为感知学习之间的不一致性。我们展示了前者将模型的容量分配给解释观察到的方差的数据子空间--这是对后者无用的特征子空间。

    arXiv:2402.11337v1 Announce Type: cross  Abstract: Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed
    
[^30]: 无标签数据的代理建模的深度自适应采样方法

    Deep adaptive sampling for surrogate modeling without labeled data

    [https://arxiv.org/abs/2402.11283](https://arxiv.org/abs/2402.11283)

    本工作提出了一种用于低正则性参数微分方程的代理建模的深度自适应采样方法($\text{DAS}^2$)，通过泛化深度自适应采样（DAS）方法，能有效处理高维度数据问题。

    

    代理建模对于参数微分方程系统具有重要的实际意义。与传统的数值方法相比，使用物理信息的深度学习方法构建这些系统的模拟器是一个有前途的方向，因为它可以处理高维度，这需要通过训练一组随机样本来最小化损失。然而，随机样本引入了统计误差，这可能成为低正则性和高维问题近似的主要误差。本文提出了一种用于低正则性参数微分方程的代理建模的深度自适应采样方法（$\text{DAS}^2$），其中我们将深度自适应采样（DAS）方法推广到建立低正则性参数微分方程的代理模型。在参数设置中，残差损失函数可以被视为一个未归一化的概率密度函数（PDF）的估计.

    arXiv:2402.11283v1 Announce Type: cross  Abstract: Surrogate modeling is of great practical significance for parametric differential equation systems. In contrast to classical numerical methods, using physics-informed deep learning methods to construct simulators for such systems is a promising direction due to its potential to handle high dimensionality, which requires minimizing a loss over a training set of random samples. However, the random samples introduce statistical errors, which may become the dominant errors for the approximation of low-regularity and high-dimensional problems. In this work, we present a deep adaptive sampling method for surrogate modeling ($\text{DAS}^2$), where we generalize the deep adaptive sampling (DAS) method [62] [Tang, Wan and Yang, 2023] to build surrogate models for low-regularity parametric differential equations. In the parametric setting, the residual loss function can be regarded as an unnormalized probability density function (PDF) of the spa
    
[^31]: 自适应分割平衡优化随机森林

    Adaptive Split Balancing for Optimal Random Forest

    [https://arxiv.org/abs/2402.11228](https://arxiv.org/abs/2402.11228)

    介绍了自适应分割平衡森林（ASBF），可在学习树表示的同时，在复杂情况下实现极小极优性，并提出了一个本地化版本，在H\"older类下达到最小极优性。

    

    尽管随机森林通常用于回归问题，但现有方法在复杂情况下缺乏适应性，或在简单、平滑情景下失去最优性。在本研究中，我们介绍了自适应分割平衡森林（ASBF），能够从数据中学习树表示，同时在Lipschitz类下实现极小极优性。为了利用更高阶的平滑性水平，我们进一步提出了一个本地化版本，该版本在任意$q \in \mathbb{N}$和$\beta \in (0,1]$的Hölder类$\mathcal{H}^{q,\beta}$下达到最小极优性。与广泛使用的随机特征选择不同，我们考虑了对现有方法的平衡修改。我们的结果表明，过度依赖辅助随机性可能会损害树模型的逼近能力，导致次优结果。相反，一个更平衡、更少随机的方法表现出最佳性能。

    arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\"older class $\mathcal{H}^{q,\beta}$ for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall
    
[^32]: AdAdaGrad：自适应梯度方法的自适应批大小方案

    AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods

    [https://arxiv.org/abs/2402.11215](https://arxiv.org/abs/2402.11215)

    AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。

    

    随机梯度优化器中批量大小的选择对模型训练至关重要。然而，在训练过程中变化批大小的实践相对其他超参数较少探讨。我们研究了从自适应采样方法中导出的自适应批大小策略，传统上仅应用于随机梯度下降。考虑到学习速率和批大小之间的显著相互作用，以及自适应梯度方法在深度学习中的普及，我们强调在这些情境中需要自适应批大小策略。我们介绍了AdAdaGrad及其标量变体AdAdaGradNorm，它们在训练过程中逐渐增加批大小，同时使用AdaGrad和AdaGradNorm进行模型更新。我们证明了AdaGradNorm以高概率以$O(1/K)$的速度收敛，用于找到光滑非凸函数的一阶稳定点在$K$次迭代内。

    arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
    
[^33]: 高效的低秩矩阵估计、实验设计和基于Arm集的低秩赌博机

    Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits

    [https://arxiv.org/abs/2402.11156](https://arxiv.org/abs/2402.11156)

    提出一种新型低秩矩阵估计方法LowPopArt，通过最小化量B(Q)提供更紧密的恢复保证，同时提出了一种新颖的实验设计标准，以及两种适用于一般Arm集的低秩线性赌博算法。

    

    我们研究了低秩矩阵迹回归和相关的低秩矩阵赌博问题。假设可以访问协变量的分布，我们提出了一种名为LowPopArt的新型低秩矩阵估计方法，并提供了其依赖于一个新颖数量B(Q)的恢复保证，该数量表征了问题的难度，其中Q是测量分布的协方差矩阵。我们展示了我们的方法在几个问题中可以提供比经典的核范数惩罚最小二乘法（Koltchinskii等人，2011）更紧密的恢复保证。为了在从任意给定的测量集合A中进行有限测量的情况下执行高效估计，我们还提出了一种新颖的实验设计标准，该标准以计算效率最小化B(Q)。我们利用我们的新颖估计器和实验设计推导了两种适用于一般Arm集的低秩线性赌博算法，其享有改进的

    arXiv:2402.11156v1 Announce Type: cross  Abstract: We study low-rank matrix trace regression and the related problem of low-rank matrix bandits. Assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by B(Q) that characterizes the hardness of the problem, where Q is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set A, we also propose a novel experimental design criterion that minimizes B(Q) with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved 
    
[^34]: 函数偏最小二乘法：最优收敛率和自适应性

    Functional Partial Least-Squares: Optimal Rates and Adaptation

    [https://arxiv.org/abs/2402.11134](https://arxiv.org/abs/2402.11134)

    该论文提出了一种新的函数偏最小二乘估计器，其在一类椭球上实现了（近乎）最优的收敛速率，并引入了适应未知逆问题度的提前停止规则。

    

    我们考虑具有标量响应和 Hilbert 空间值预测变量的函数线性回归模型，这是一个众所周知的反问题。我们提出了一个与共轭梯度方法相关的函数偏最小二乘（PLS）估计的新公式。我们将展示该估计器在一类椭球上实现了（近乎）最优的收敛速率，并引入了一个能够适应未知逆问题度的提前停止规则。我们提供了估计器与主成分回归估计器之间的一些理论和仿真比较。

    arXiv:2402.11134v1 Announce Type: cross  Abstract: We consider the functional linear regression model with a scalar response and a Hilbert space-valued predictor, a well-known ill-posed inverse problem. We propose a new formulation of the functional partial least-squares (PLS) estimator related to the conjugate gradient method. We shall show that the estimator achieves the (nearly) optimal convergence rate on a class of ellipsoids and we introduce an early stopping rule which adapts to the unknown degree of ill-posedness. Some theoretical and simulation comparison between the estimator and the principal component regression estimator is provided.
    
[^35]: DART: 一种面向对抗鲁棒的无监督领域自适应的原则性方法

    DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2402.11120](https://arxiv.org/abs/2402.11120)

    本文探讨了无监督领域自适应中对抗鲁棒性的问题，通过建立对抗目标损失的泛化界限来解决目标域标签缺失带来的挑战。

    

    分布转移和对抗样本是部署机器学习模型面临的两个主要挑战。虽然这些挑战已被分别研究，但它们的结合仍然是一个相对未被充分探索的重要主题。本文研究了在一个常见的分布转移设置下对抗鲁棒性的问题，即无监督领域自适应（UDA）。具体地，给定一个带标签的源域 $D_S$ 和一个带有相关但不同分布的未标记目标域 $D_T$，目标是为 $D_T$ 获得一个对抗鲁棒的模型。目标域标签的缺失提出了一个独特的挑战，因为传统的对抗鲁棒性防御不能直接应用于 $D_T$。为了解决这一挑战，我们首先建立了对抗目标损失的泛化界限，其中包括与数据损失相关的项和最坏情况域分歧的度量。

    arXiv:2402.11120v1 Announce Type: new  Abstract: Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivat
    
[^36]: 通过得分规则构建概率预测树

    Building Trees for Probabilistic Prediction via Scoring Rules

    [https://arxiv.org/abs/2402.11052](https://arxiv.org/abs/2402.11052)

    提出了一种通过得分规则构建概率预测树的方法，以改进树的预测性能并生成更好的预测分布。

    

    使用数据构建的决策树在非参数预测中仍然被广泛使用。在不确定性在分析和决策中发挥重要作用时，预测概率分布优于点预测。我们研究了修改树以生成非参数预测分布。我们发现，使用标准方法构建树可能不会产生良好的预测分布，并提出将树的分割标准更改为基于适当得分规则的标准。对模拟数据和几个真实数据集的分析表明，使用这些新的分割标准会导致具有改进预测性能的树，考虑整个预测分布。

    arXiv:2402.11052v1 Announce Type: cross  Abstract: Decision trees built with data remain in widespread use for nonparametric prediction. Predicting probability distributions is preferred over point predictions when uncertainty plays a prominent role in analysis and decision-making. We study modifying a tree to produce nonparametric predictive distributions. We find the standard method for building trees may not result in good predictive distributions and propose changing the splitting criteria for trees to one based on proper scoring rules. Analysis of both simulated data and several real datasets demonstrates that using these new splitting criteria results in trees with improved predictive properties considering the entire predictive distribution.
    
[^37]: 具有领域标签噪声的亚群体转移鲁棒性通过领域标注的正则化

    Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains

    [https://arxiv.org/abs/2402.11039](https://arxiv.org/abs/2402.11039)

    提出了一种名为RAD的方法，通过领域标注的正则化来训练鲁棒的最后一层分类器，无需显式的领域标注，在具有领域标签噪声的情况下表现优越。

    

    现有针对最优组准确性(WGA)进行最后一层重新训练的方法在训练数据中过于依赖于良好标注的组。我们理论上和实践中展示了，基于注释的数据增强使用下采样或上加权用于WGA是容易受到领域标注噪声干扰，在高噪声情况下接近使用原始经验风险最小化训练的模型的WGA。我们引入了领域标注正则化(RAD)来训练具有鲁棒性的最后一层分类器，而无需明确的领域标注。我们的结果表明，RAD与其他最近提出的无领域标注技术具有竞争力。最重要的是，即使在训练数据中仅有5%的噪声，RAD也在几个公开可用数据集上胜过最先进的依赖注释的方法。

    arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.
    
[^38]: 使用稀疏子空间变分推断训练贝叶斯神经网络

    Training Bayesian Neural Networks with Sparse Subspace Variational Inference

    [https://arxiv.org/abs/2402.11025](https://arxiv.org/abs/2402.11025)

    提出了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架

    

    贝叶斯神经网络（BNN）提供了不确定性量化，但代价是大幅增加训练和推断成本。稀疏BNN已被研究用于高效推断，通常通过在训练过程中逐渐引入稀疏性或通过后续对密集BNN进行压缩。然而，如何降低巨大的训练成本仍然是一个难题，特别是考虑到需要学习不确定性。为了解决这一挑战，我们引入了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架。我们的方法从一个随机初始化的低维稀疏子空间开始，交替优化稀疏子空间基向量的选择以及相关参数。尽管基向量选择被描述为一个不可微分的问题，我们近似求解该问题。

    arXiv:2402.11025v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the opti
    
[^39]: 使用锐度感知最小化和通道注意力解锁Transformer在时间序列预测中的潜力

    Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention

    [https://arxiv.org/abs/2402.10198](https://arxiv.org/abs/2402.10198)

    本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。

    

    Transformer架构在自然语言处理和计算机视觉中取得了突破性的性能，但在多元长期预测方面，它们仍然不如更简单的线性基线。为了更好地理解这一现象，我们首先研究了一个玩具线性预测问题，展示了尽管Transformer具有高表达能力，但它们无法收敛到真正的解决方案。我们进一步确定Transformer的注意力是造成其低泛化能力的原因。基于这一认识，我们提出了一个浅层轻量级的Transformer模型，在锐度感知优化的情况下成功避免了坏的局部最小值。我们通过实验证明，这个结果适用于所有常用的实际多元时间序列数据集。特别是，相比当前最先进的模型TSMixer，SAMformer的平均性能提高了14.33%，并且参数数量减少了约4倍。

    arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
    
[^40]: 从均场稳态分布中采样

    Sampling from the Mean-Field Stationary Distribution

    [https://arxiv.org/abs/2402.07355](https://arxiv.org/abs/2402.07355)

    本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    

    我们研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，或者等价地，即包含交互项的概率测度空间上的最小化函数的复杂性。我们的主要洞察是将这个问题的两个关键方面解耦：(1) 通过有限粒子系统逼近均场SDE，通过时间均匀传播混沌，和(2) 通过标准对数凹抽样器从有限粒子稳态分布中采样。我们的方法在概念上更简单，其灵活性允许结合用于算法和理论的最新技术。这导致在许多设置中提供了改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
    
[^41]: 使用PEAK进行窥探：多个数据流均值的顺序、非参数复合假设检验

    Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams

    [https://arxiv.org/abs/2402.06122](https://arxiv.org/abs/2402.06122)

    本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。

    

    我们提出了一种新颖的非参数顺序复合假设检验方法，用于多个数据流的均值。我们的方法名为PEAK（基于期望平均资产的窥探），基于测试即博弈的框架，提供了一个在任何停止时间上的非渐进α水平测试。PEAK在计算上可行，并且能够有效拒绝在满足我们的非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。我们在强化学习中的最佳臂识别和阈值识别任务中对我们的理论结果进行了数值验证，并展示了我们的方法在计算效率上优于现有的测试方法。

    We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
    
[^42]: 通过优化抽象的方式进行Slate Bandit策略的离策略评估

    Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction

    [https://arxiv.org/abs/2402.02171](https://arxiv.org/abs/2402.02171)

    我们提出了一种名为潜在IPS（LIPS）的新的Slate Bandit OPE估计器，通过在低维度的Slate抽象空间中定义重要性权重，并通过数据驱动的方式优化Slate抽象来减小偏差和方差。

    

    我们研究了Slate上下文强盗问题中的离策略评估（OPE），其中一个策略选择称为slates的多维动作。这个问题在推荐系统、搜索引擎、营销以及医疗应用中广泛存在，然而，由于动作空间大，典型的逆倾向评分（IPS）估计器存在较大的方差，使得有效的OPE成为一个重大挑战。伪逆（PI）估计器已被引入以减小方差问题，通过假设奖励函数线性，但这可能导致显著的偏差，因为这个假设在观测数据中很难验证并且经常会被实质性违反。为了解决之前估计器的局限性，我们开发了一种新的Slate Bandit OPE估计器，称为潜在IPS（LIPS），它在低维度的Slate抽象空间中定义了重要性权重，我们通过数据驱动的方式优化Slate抽象来最小化LIPS的偏差和方差。

    We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By do
    
[^43]: 用于神经密度比估计的$\alpha$-散度损失函数

    $\alpha$-Divergence Loss Function for Neural Density Ratio Estimation

    [https://arxiv.org/abs/2402.02041](https://arxiv.org/abs/2402.02041)

    本文提出了一种应用于神经密度比估计的$\alpha$-散度损失函数($\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。

    

    最近，神经网络在机器学习中的基础技术密度比估计(DRE)方面取得了最先进的结果。然而，现有方法因DRE的损失函数而出现了优化问题：KL散度需要大样本，训练损失梯度消失，损失函数梯度有偏。因此，本文提出了一种提供简洁实现和稳定优化的$\alpha$-散度损失函数($\alpha$-Div)。此外，还给出了对所提出的损失函数的技术验证。实验证明了所提出的损失函数的稳定性，并研究了DRE任务的估计准确性。此外，本研究还提出了使用所提出的损失函数进行DRE的样本要求，以$L_1$误差的上界联系起来，该上界将高维度DRE任务中的维度诅咒作为一个共同问题。

    Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
    
[^44]: 两层ReLU网络中的隐藏极小值

    Hidden Minima in Two-Layer ReLU Networks

    [https://arxiv.org/abs/2312.16819](https://arxiv.org/abs/2312.16819)

    本文研究了两层ReLU网络中的隐藏极小值现象，并提出方法来研究这些隐藏极小值的独特解析性质。

    

    本文考虑拟合具有$d$个输入、$k$个神经元以及由目标网络生成的标签的两层ReLU网络所涉及的优化问题。最近发现了两种无穷族的虚假极小值，每个$d$对应一个极小值。属于第一类的极小值的损失在$d$增加时收敛于零。在第二类中，损失保持远离于零。那么，如何避免属于后一类的极小值呢？幸运的是，这样的极小值从不会被标准优化方法检测到。受到此现象性质的问题的启发，我们开发了研究隐藏极小值独特解析性质的方法。根据现有的分析，两种类型的Hessian谱在$O(d^{-1/2})$项模意义下一致 -- 不太乐观。因此，我们的研究通过研究损失被最小化或最大化的曲线进行，通常称为切线。

    arXiv:2312.16819v2 Announce Type: replace  Abstract: The optimization problem associated to fitting two-layer ReLU networks having $d$~inputs, $k$~neurons, and labels generated by a target network, is considered. Two types of infinite families of spurious minima, giving one minimum per $d$, were recently found. The loss at minima belonging to the first type converges to zero as $d$ increases. In the second type, the loss remains bounded away from zero. That being so, how may one avoid minima belonging to the latter type? Fortunately, such minima are never detected by standard optimization methods. Motivated by questions concerning the nature of this phenomenon, we develop methods to study distinctive analytic properties of hidden minima.   By existing analyses, the Hessian spectrum of both types agree modulo $O(d^{-1/2})$-terms -- not promising. Thus, rather, our investigation proceeds by studying curves along which the loss is minimized or maximized, generally referred to as tangency 
    
[^45]: 线性情境赌博问题的两全其美算法

    Best-of-Both-Worlds Algorithms for Linear Contextual Bandits

    [https://arxiv.org/abs/2312.15433](https://arxiv.org/abs/2312.15433)

    该论文研究了线性情境赌博问题的两全其美算法，实现了在对抗性和随机情况下接近最优的遗憾界，其中包括了针对最小次优差距的多对数级别速率和在对抗性情况下的第一阶或第二阶界以及基于Shannon熵正则项的FTRL算法。

    

    我们研究了针对$K$臂线性情境赌博问题的两全其美算法。我们的算法在对抗性和随机情况下均具有接近最优的遗憾界，而无需关于环境的先验知识。在随机情况下，我们实现了多对数级别的速率$\frac{(dK)^2\mathrm{poly}\log(dKT)}{\Delta_{\min}}$，其中$\Delta_{\min}$是$d$维情境空间中的最小次优差距。在对抗性情况下，我们获得了第一阶$\widetilde{O}(dK\sqrt{L^*})$界或者第二阶$\widetilde{O}(dK\sqrt{\Lambda^*})$界，其中$L^*$是最佳操作的累积损失，$\Lambda^*$是算法产生的损失的累积二次矩的一种概念。此外，我们基于带有Shannon熵正则项的FTRL算法开发了一种不需要知道协方差矩阵逆的算法，并实现了多对数遗憾

    arXiv:2312.15433v2 Announce Type: replace  Abstract: We study best-of-both-worlds algorithms for $K$-armed linear contextual bandits. Our algorithms deliver near-optimal regret bounds in both the adversarial and stochastic regimes, without prior knowledge about the environment. In the stochastic regime, we achieve the polylogarithmic rate $\frac{(dK)^2\mathrm{poly}\log(dKT)}{\Delta_{\min}}$, where $\Delta_{\min}$ is the minimum suboptimality gap over the $d$-dimensional context space. In the adversarial regime, we obtain either the first-order $\widetilde{O}(dK\sqrt{L^*})$ bound, or the second-order $\widetilde{O}(dK\sqrt{\Lambda^*})$ bound, where $L^*$ is the cumulative loss of the best action and $\Lambda^*$ is a notion of the cumulative second moment for the losses incurred by the algorithm. Moreover, we develop an algorithm based on FTRL with Shannon entropy regularizer that does not require the knowledge of the inverse of the covariance matrix, and achieves a polylogarithmic regre
    
[^46]: 从高阶统计量中高效学习：假设检验、随机特征和神经网络

    Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks

    [https://arxiv.org/abs/2312.14922](https://arxiv.org/abs/2312.14922)

    神经网络在高维数据中发现统计模式，研究了如何高效地从高阶累积量中提取特征，并探讨了在尖峰累积量模型中的统计和计算限制。

    

    神经网络擅长发现高维数据集中的统计模式。在实践中，度量三个或更多变量间的非高斯相关性的高阶累积量对神经网络的性能特别重要。但神经网络有多有效地从高阶累积量中提取特征？我们在尖峰累积量模型中探讨了这个问题，这里统计学家需要从$d$维输入的阶-$p\ge 4$累积量中恢复出一个特权方向或“尖峰”。我们首先通过分析所需样本数$n$来表征恢复尖峰的基本统计和计算限制，以强烈区分来自尖峰累积量模型和各向同性高斯输入的输入。我们发现，统计上的可区分性需要$n\gtrsim d$个样本，而在多项式时间内区分这两个分布则需要

    arXiv:2312.14922v2 Announce Type: replace-cross  Abstract: Neural networks excel at discovering statistical patterns in high-dimensional data sets. In practice, higher-order cumulants, which quantify the non-Gaussian correlations between three or more variables, are particularly important for the performance of neural networks. But how efficient are neural networks at extracting features from higher-order cumulants? We study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or "spike" from the order-$p\ge 4$ cumulants of $d$-dimensional inputs. We first characterise the fundamental statistical and computational limits of recovering the spike by analysing the number of samples $n$ required to strongly distinguish between inputs from the spiked cumulant model and isotropic Gaussian inputs. We find that statistical distinguishability requires $n\gtrsim d$ samples, while distinguishing the two distributions in polynomial time require
    
[^47]: 高效计算稀疏和鲁棒最大关联估计量

    Efficient Computation of Sparse and Robust Maximum Association Estimators

    [https://arxiv.org/abs/2311.17563](https://arxiv.org/abs/2311.17563)

    本文研究如何在高维稀疏设置中利用新的优化程序实现鲁棒稀疏关联估计，通过增广Lagrange算法和自适应梯度下降的组合，提供了更精确的算法，并展示了相对现有算法的优势。

    

    虽然鲁棒统计估计量受到异常值的影响较小，但它们的计算通常更具挑战性，特别是在高维稀疏设置中。新的优化程序，主要在计算机科学领域开发，为鲁棒统计领域提供了新的可能性。本文研究了如何利用这些程序来实现鲁棒稀疏关联估计。该问题被拆分为一个鲁棒估计步骤，接着是一个余项解耦的（双边）凸问题的优化。采用增广Lagrange算法和自适应梯度下降的组合，还包括适当的约束条件以诱导稀疏性。我们提供了有关算法精度的结果，并展示了在这一背景下相对现有算法的优势。高维实证示例强调了该方法的实用性。

    arXiv:2311.17563v2 Announce Type: replace-cross  Abstract: Although robust statistical estimators are less affected by outlying observations, their computation is usually more challenging. This is particularly the case in high-dimensional sparse settings. The availability of new optimization procedures, mainly developed in the computer science domain, offers new possibilities for the field of robust statistics. This paper investigates how such procedures can be used for robust sparse association estimators. The problem can be split into a robust estimation step followed by an optimization for the remaining decoupled, (bi-)convex problem. A combination of the augmented Lagrangian algorithm and adaptive gradient descent is implemented to also include suitable constraints for inducing sparsity. We provide results concerning the precision of the algorithm and show the advantages over existing algorithms in this context. High-dimensional empirical examples underline the usefulness of this p
    
[^48]: 一般框架用于用户引导的贝叶斯优化

    A General Framework for User-Guided Bayesian Optimization

    [https://arxiv.org/abs/2311.14645](https://arxiv.org/abs/2311.14645)

    ColaBO是第一个贝叶斯原理框架，允许领域专家定制优化程序，整合先验信念以加速优化。

    

    昂贵的黑盒函数优化在各种科学学科中普遍存在。贝叶斯优化是一种自动、通用且样本高效的方法，可以在最小了解基础函数动态的情况下解决这些问题。然而，贝叶斯优化能够整合关于待优化函数的先验知识或信念以加速优化的能力有限，这降低了对具有预算紧迫知识渊博的实践者的吸引力。为了允许领域专家定制优化程序，我们提出了ColaBO，这是第一个贝叶斯原理框架，用于整合超出典型核结构的先验信念，如优化器的可能位置或最佳值。ColaBO的通用性使其适用于不同蒙特卡洛收获函数和用户信念的类型。我们经验性地展示了ColaBO的能力，

    arXiv:2311.14645v2 Announce Type: replace  Abstract: The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight budgets. To allow domain experts to customize the optimization routine, we propose ColaBO, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate ColaBO's ability to substa
    
[^49]: 关于基于扩散的生成模型及其误差界限：完全收敛估计下的对数凹情况

    On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates

    [https://arxiv.org/abs/2311.13584](https://arxiv.org/abs/2311.13584)

    我们提出了对于基于扩散的生成模型在强对数凹数据分布假设下的完整收敛理论保证，获得了对于参数估计和采样算法的最优上限估计。

    

    我们在强对数凹数据分布的假设下为基于扩散的生成模型的收敛行为提供了完整的理论保证，而我们用于得分估计的逼近函数类由Lipschitz连续函数组成。我们通过一个激励性例子展示了我们方法的强大之处，即从具有未知均值的高斯分布中进行采样。在这种情况下，我们对相关的优化问题，即得分估计，提供了明确的估计，同时将其与相应的采样估计结合起来。因此，我们获得了最好的已知上限估计，涉及关键感兴趣的数量，如数据分布（具有未知均值的高斯分布）与我们的采样算法之间的Wasserstein-2距离的维度和收敛速率。

    arXiv:2311.13584v2 Announce Type: replace  Abstract: We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range o
    
[^50]: 分解扩散采样器用于加速大规模逆问题

    Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems

    [https://arxiv.org/abs/2303.05754](https://arxiv.org/abs/2303.05754)

    提出了一种将扩散采样和Krylov子空间方法协同结合的新型高效采样策略。

    

    Krylov子空间是通过将给定向量与线性变换矩阵及其连续幂相乘而生成的，广泛研究的经典优化文献中利用Krylov子空间设计算法以快速收敛大规模线性逆问题。本研究提出了一种新颖高效的扩散采样策略，将扩散采样与Krylov子空间方法协同结合起来。

    arXiv:2303.05754v3 Announce Type: replace-cross  Abstract: Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method (CG), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the CG initi
    
[^51]: 带有分布式量化流的GFlowNets

    Distributional GFlowNets with Quantile Flows

    [https://arxiv.org/abs/2302.05793](https://arxiv.org/abs/2302.05793)

    本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。

    

    生成式流网络（GFlowNets）是一种新的概率采样器系列，其中代理通过一系列决策步骤学习生成复杂组合结构的随机策略。尽管受强化学习启发，当前的GFlowNet框架在适用性上相对有限，无法处理奖励函数中的随机性。在这项工作中，我们采用分布式范式来处理GFlowNets，将每个流函数转化为一个分布，从而在训练过程中提供更多信息的学习信号。通过通过量化函数对每个边流进行参数化，我们提出的“量化匹配” GFlowNet学习算法能够学习风险敏感的策略，这是处理风险不确定性场景的基本组成部分。此外，我们发现与之前的方法相比，分布式方法由于我们增强的训练算法，可以在现有基准上实现显着改进。

    Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
    
[^52]: 在Banach空间中的随机最优输运用于多元分位数的正则化估计

    Stochastic optimal transport in Banach Spaces for regularized estimation of multivariate quantiles

    [https://arxiv.org/abs/2302.00982](https://arxiv.org/abs/2302.00982)

    提出了一种新的随机算法，用于利用傅里叶系数解决熵最优输运问题，并研究了其在无限维Banach空间中的几乎肯定收敛性和性能表现。

    

    我们介绍了一种新的随机算法，用于解决两个绝对连续概率测度$\mu$和$\nu$之间的熵最优输运（EOT）。我们的工作受蒙日-坎托罗维奇分位数的特定设置启发，其中源测度$\mu$要么是单位超立方体上的均匀分布，要么是球面均匀分布。利用源测度的知识，我们建议通过其傅里叶系数来参数化坎托罗维奇对偶势能。通过这种方式，我们的随机算法的每次迭代都会减少到两个傅里叶变换，使我们能够利用快速傅里叶变换（FFT）来实现求解EOT的快速数值方法。我们研究了我们的随机算法在取值于无限维Banach空间中的几乎肯定收敛性。然后，通过数值实验，我们展示了我们的方法在计算中的性能表现。

    arXiv:2302.00982v2 Announce Type: replace-cross  Abstract: We introduce a new stochastic algorithm for solving entropic optimal transport (EOT) between two absolutely continuous probability measures $\mu$ and $\nu$. Our work is motivated by the specific setting of Monge-Kantorovich quantiles where the source measure $\mu$ is either the uniform distribution on the unit hypercube or the spherical uniform distribution. Using the knowledge of the source measure, we propose to parametrize a Kantorovich dual potential by its Fourier coefficients. In this way, each iteration of our stochastic algorithm reduces to two Fourier transforms that enables us to make use of the Fast Fourier Transform (FFT) in order to implement a fast numerical method to solve EOT. We study the almost sure convergence of our stochastic algorithm that takes its values in an infinite-dimensional Banach space. Then, using numerical experiments, we illustrate the performances of our approach on the computation of regular
    
[^53]: 分治核函数的功能线性回归的统计优化性

    Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression

    [https://arxiv.org/abs/2211.10968](https://arxiv.org/abs/2211.10968)

    研究了在目标函数不一定包含在核空间中的情况下，分治核函数的功能线性回归算法的统计优化性。

    

    先前对于正则化的核函数空间中的功能线性回归的分析通常要求目标函数包含在这个核空间中。本文研究了在目标函数不一定驻留在基本RKHS中的情况下，分治估计器的收敛性能。作为一种基于分解的可扩展方法，功能线性回归的分治估计器可以大幅减少时间和内存中的算法复杂性。我们采用积分算子方法建立了针对分治估计器在解释变量和目标函数的各种正则性条件下的预测的尖锐有限样本上界。通过建立最小-最大下界，我们还证明了导出率的渐近最优性。最后，我们考虑了无噪声估计器的收敛性，并展示了这些率能够

    arXiv:2211.10968v3 Announce Type: replace  Abstract: Previous analysis of regularized functional linear regression in a reproducing kernel Hilbert space (RKHS) typically requires the target function to be contained in this kernel space. This paper studies the convergence performance of divide-and-conquer estimators in the scenario that the target function does not necessarily reside in the underlying RKHS. As a decomposition-based scalable approach, the divide-and-conquer estimators of functional linear regression can substantially reduce the algorithmic complexities in time and memory. We develop an integral operator approach to establish sharp finite sample upper bounds for prediction with divide-and-conquer estimators under various regularity conditions of explanatory variables and target function. We also prove the asymptotic optimality of the derived rates by building the mini-max lower bounds. Finally, we consider the convergence of noiseless estimators and show that the rates ca
    
[^54]: 具有融合套索的图中方差估计

    Variance estimation in graphs with the fused lasso

    [https://arxiv.org/abs/2207.12638](https://arxiv.org/abs/2207.12638)

    研究了在一般图结构问题中的方差估计，开发了线性时间估计器并提供了上界，允许推广到更广泛的分布类。

    

    我们研究了在一般图结构问题中的方差估计问题。首先，我们为同方差情况开发了一个线性时间估计器，可以在一般图中一致估计方差。我们证明了当均值信号具有总变化与标准尺度时，我们的估计器在链式图和二维网格图上达到最小最大率。此外，我们在一般图中提供了融合套索估计器的均方误差表现的一般上界，根据矩条件和误差尾部行为的界限。这些上界使我们能够推广到更广泛的分布类，例如亚指数分布，许多现有关于融合套索的结果仅在假设错误为亚高斯随机变量的情况下成立。利用我们的上界，我们随后研究了一个简单的总变差正则化估计器，用于估算方差信号。

    arXiv:2207.12638v3 Announce Type: replace-cross  Abstract: We study the problem of variance estimation in general graph-structured problems. First, we develop a linear time estimator for the homoscedastic case that can consistently estimate the variance in general graphs. We show that our estimator attains minimax rates for the chain and 2D grid graphs when the mean signal has total variation with canonical scaling. Furthermore, we provide general upper bounds on the mean squared error performance of the fused lasso estimator in general graphs under a moment condition and a bound on the tail behavior of the errors. These upper bounds allow us to generalize for broader classes of distributions, such as sub-exponential, many existing results on the fused lasso that are only known to hold with the assumption that errors are sub-Gaussian random variables. Exploiting our upper bounds, we then study a simple total variation regularization estimator for estimating the signal of variances in t
    
[^55]: 基于群稀疏矩阵分解的词嵌入传递学习

    Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings

    [https://arxiv.org/abs/2104.08928](https://arxiv.org/abs/2104.08928)

    提出了一种基于群稀疏矩阵分解的方法，用于在新领域进行词嵌入的传递学习，以解决不同领域单词含义差异的挑战。

    

    非结构化文本为许多领域的决策者提供了丰富的数据源，涵盖范围从零售中的产品评论到医疗保健中的护理记录。为了利用这些信息，通常会通过无监督学习算法（如矩阵分解）将单词转换为词嵌入——编码单词之间语义关系的向量。然而，从具有有限训练数据的新领域学习单词嵌入可能具有挑战性，因为在新领域中，单词的含义/用法可能不同，例如，“positive”一词通常具有正面情绪，但在医疗记录中往往具有负面情绪，因为它可能意味着患者检测呈阳性。在实践中，我们预计只有少量领域特定单词可能具有新含义。我们提出了一个直观的两阶段估计器，通过群稀疏惩罚来有效地传递学习领域特定的新含义。

    arXiv:2104.08928v3 Announce Type: replace-cross  Abstract: Unstructured text provides decision-makers with a rich data source in many domains, ranging from product reviews in retail to nursing notes in healthcare. To leverage this information, words are typically translated into word embeddings -- vectors that encode the semantic relationships between words -- through unsupervised learning algorithms such as matrix factorization. However, learning word embeddings from new domains with limited training data can be challenging, because the meaning/usage may be different in the new domain, e.g., the word ``positive'' typically has positive sentiment, but often has negative sentiment in medical notes since it may imply that a patient tested positive for a disease. In practice, we expect that only a small number of domain-specific words may have new meanings. We propose an intuitive two-stage estimator that exploits this structure via a group-sparse penalty to efficiently transfer learn dom
    
[^56]: Deep-Lock: 深度神经网络的安全授权

    Deep-Lock: Secure Authorization for Deep Neural Networks

    [https://arxiv.org/abs/2008.05966](https://arxiv.org/abs/2008.05966)

    本文提出了一种名为Deep-Lock的通用和轻量级基于密钥的模型锁定方案，通过使用S-盒对训练完毕的DNN模型的每个参数进行加密，并确保只有在应用正确的秘密密钥时模型才能正确运行，从而防止了DNN模型的未经授权使用。

    

    训练完毕的深度神经网络（DNN）模型被视为多种商业模式中的有价值的知识产权（IP）。防止IP盗窃和未经授权使用这些DNN模型已被业界提出作为一个重大关注点。本文通过提出一种通用且轻量的基于密钥的模型锁定方案—Deep-Lock，解决了防止DNN模型未经授权使用的问题，确保被锁定的模型只有在应用正确的秘密密钥时才能正确运行。Deep-Lock方案利用具有良好安全性质的S-盒对经过训练的DNN模型的每个参数进行加密，使用主密钥通过密钥调度算法生成秘密密钥。由此产生的加密权重的密集网络被发现能够抵抗模型微调攻击。最后，Deep-Lock不需要对DNN模型的结构和训练进行任何干预，使其适用于所有已存在的模型。

    arXiv:2008.05966v2 Announce Type: replace  Abstract: Trained Deep Neural Network (DNN) models are considered valuable Intellectual Properties (IP) in several business models. Prevention of IP theft and unauthorized usage of such DNN models has been raised as of significant concern by industry. In this paper, we address the problem of preventing unauthorized usage of DNN models by proposing a generic and lightweight key-based model-locking scheme, which ensures that a locked model functions correctly only upon applying the correct secret key. The proposed scheme, known as Deep-Lock, utilizes S-Boxes with good security properties to encrypt each parameter of a trained DNN model with secret keys generated from a master key via a key scheduling algorithm. The resulting dense network of encrypted weights is found robust against model fine-tuning attacks. Finally, Deep-Lock does not require any intervention in the structure and training of the DNN models, making it applicable for all existin
    
[^57]: 尾部自适应贝叶斯收缩

    Tail-adaptive Bayesian shrinkage

    [https://arxiv.org/abs/2007.02192](https://arxiv.org/abs/2007.02192)

    提出了一种在多样的稀疏情况下具有尾部自适应收缩特性的鲁棒稀疏估计方法，通过新的全局-局部-尾部高斯混合分布实现，能够根据稀疏程度自适应调整先验的尾部重量以适应更多或更少信号。

    

    本文研究了高维回归问题下多样的稀疏情况下的鲁棒贝叶斯方法。传统的收缩先验主要设计用于在所谓的超稀疏领域从成千上万个预测变量中检测少数信号。然而，当稀疏程度适中时，它们可能表现不尽人意。在本文中，我们提出了一种在多样稀疏情况下具有尾部自适应收缩特性的鲁棒稀疏估计方法。在这种特性中，先验的尾部重量会自适应调整，随着稀疏水平的增加或减少变得更大或更小，以适应先验地更多或更少的信号。我们提出了一个全局局部尾部（GLT）高斯混合分布以确保这种属性。我们考察了先验的尾部指数与基础稀疏水平之间的关系，并证明GLT后验会在...

    arXiv:2007.02192v4 Announce Type: replace-cross  Abstract: Robust Bayesian methods for high-dimensional regression problems under diverse sparse regimes are studied. Traditional shrinkage priors are primarily designed to detect a handful of signals from tens of thousands of predictors in the so-called ultra-sparsity domain. However, they may not perform desirably when the degree of sparsity is moderate. In this paper, we propose a robust sparse estimation method under diverse sparsity regimes, which has a tail-adaptive shrinkage property. In this property, the tail-heaviness of the prior adjusts adaptively, becoming larger or smaller as the sparsity level increases or decreases, respectively, to accommodate more or fewer signals, a posteriori. We propose a global-local-tail (GLT) Gaussian mixture distribution that ensures this property. We examine the role of the tail-index of the prior in relation to the underlying sparsity level and demonstrate that the GLT posterior contracts at the
    
[^58]: 通过分裂诊断实现随机优化的稳健学习率选择

    Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic

    [https://arxiv.org/abs/1910.08597](https://arxiv.org/abs/1910.08597)

    该方法提出了SplitSGD，通过简单而有效的稳态检测，在检测到稳态阶段时降低学习速率，使其适用于凸问题和训练神经网络，表现优于其他随机优化方法。

    

    这篇论文提出了SplitSGD，这是一种用于随机优化的新的动态学习率调度方法。该方法通过在检测到稳态阶段时降低学习速率，以更好地适应目标函数的局部几何结构，即当迭代处于局部最小值附近时可能会出现反弹。通过将单线程分成两个部分，并使用两个线程梯度的内积作为稳态度量来执行检测。基于这个简单但经过验证有效的稳态检测，SplitSGD易于实现，并且基本不会比标准SGD产生额外的计算成本。通过一系列广泛的实验，我们展示了该方法既适用于凸问题，也适用于训练（非凸）神经网络，表现比其他随机优化方法更好。重要的是，观察到该方法

    arXiv:1910.08597v5 Announce Type: replace-cross  Abstract: This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be v
    
[^59]: 从分组数据中稳健估计Pareto的尺度参数

    Robust Estimation of Pareto's Scale Parameter from Grouped Data. (arXiv:2401.14593v1 [stat.ME])

    [http://arxiv.org/abs/2401.14593](http://arxiv.org/abs/2401.14593)

    本文介绍了一种新的稳健估计方法（MTuM），用于从分组数据中估计Pareto分布的尾指数。该方法通过应用中心极限定理和模拟研究验证了其推理合理性。

    

    当可获取的完全观测到的从头至尾的损失严重性样本数据集存在时，存在许多稳健估计器作为最大似然估计器（MLE）的替代方案。然而，当处理分组损失严重性数据时，稳健的MLE替代方案的选择变得非常有限，只有少数方法可用，例如最小二乘法、最小Hellinger距离和最优有界影响函数。本文介绍了一种称为截断矩法的新型稳健估计技术，该方法专门用于从分组数据估计Pareto分布的尾指数。通过应用中心极限定理和通过全面的模拟研究验证了MTuM的推理合理性。

    Numerous robust estimators exist as alternatives to the maximum likelihood estimator (MLE) when a completely observed ground-up loss severity sample dataset is available. However, the options for robust alternatives to MLE become significantly limited when dealing with grouped loss severity data, with only a handful of methods like least squares, minimum Hellinger distance, and optimal bounded influence function available. This paper introduces a novel robust estimation technique, the Method of Truncated Moments (MTuM), specifically designed to estimate the tail index of a Pareto distribution from grouped data. Inferential justification of MTuM is established by employing the central limit theorem and validating them through a comprehensive simulation study.
    
[^60]: 使用欠阻尼 Langevin Monte Carlo 加速近似 Thompson 采样

    Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])

    [http://arxiv.org/abs/2401.11665](http://arxiv.org/abs/2401.11665)

    本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。

    

    使用欠阻尼 Langevin Monte Carlo 的近似 Thompson 采样方法扩展了其适用范围，从高斯后验采样扩展到更一般的平滑后验。然而，在高维问题中要求高准确性时，仍然面临可扩展性问题。为了解决这个问题，我们提出了一种近似 Thompson 采样策略，利用欠阻尼 Langevin Monte Carlo，后者是模拟高维后验的通用工具。基于标准的平滑性和对数凹性条件，我们研究了使用特定势函数的加速后验集中和采样。该设计改进了实现对数遗憾的样本复杂度，从$\mathcal{\tilde O}(d)$改进到$\mathcal{\tilde O}(\sqrt{d})$。我们还通过合成实验在高维赌博机问题中经验验证了我们算法的可扩展性和鲁棒性。

    Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
    
[^61]: 迭代正则化与k支撑范数：稀疏恢复的重要补充

    Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])

    [http://arxiv.org/abs/2401.05394](http://arxiv.org/abs/2401.05394)

    该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。

    

    稀疏恢复在机器学习和信号处理中无处不在。由于稀疏恢复的NP困难性质，现有方法通常要么受限于适用条件（甚至未知），要么计算成本高。最近，迭代正则化方法作为一种快速方法出现，因为它们可以通过提前停止一次通过来实现稀疏恢复，而不是传统方法中繁琐的网格搜索。然而，大多数这些迭代方法都基于$\ell_1$范数，需要受限的适用条件，并且在许多情况下可能会失败。因此，迭代正则化方法在更广泛的条件下实现稀疏恢复仍需进一步探索。为了解决这个问题，我们提出了一种新的迭代正则化算法IRKSN，它基于$k$支撑范数正则化而不是$\ell_1$范数。我们提供了使用IRKSN进行稀疏恢复的条件，并进行了比较。

    Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
    
[^62]: 扩散变分推断：扩散模型作为表达性变分后验

    Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])

    [http://arxiv.org/abs/2401.02739](http://arxiv.org/abs/2401.02739)

    本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。

    

    我们提出了去噪扩散变分推断（DDVI），一种用扩散模型作为表达性变分后验的潜变量模型的近似推断算法。我们的方法通过辅助潜变量增加了变分后验，从而得到一个表达性的模型类，通过反转用户指定的加噪过程在潜空间中进行扩散。我们通过优化一个受到觉醒-睡眠算法启发的边际似然新下界来拟合这些模型。我们的方法易于实现（它适配了正则化的ELBO扩展），与黑盒变分推断兼容，并且表现优于基于归一化流或对抗网络的替代近似后验类别。将我们的方法应用于深度潜变量模型时，我们的方法得到了去噪扩散变分自动编码器（DD-VAE）算法。我们将该算法应用于生物学中的一个激励任务 -- 从人类基因组中推断潜在血统 -- 超过了强基线模型。

    We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
    
[^63]: 敏感性感知的摊销贝叶斯推断

    Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])

    [http://arxiv.org/abs/2310.11122](http://arxiv.org/abs/2310.11122)

    本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。

    

    贝叶斯推断是在不确定性下进行概率推理和决策的强大框架。现代贝叶斯工作流程中的基本选择涉及似然函数和先验分布的规范、后验逼近器和数据。每个选择都可以显着影响基于模型的推断和后续决策，因此需要进行敏感性分析。在这项工作中，我们提出了一种多方面的方法，将敏感性分析整合到摊销贝叶斯推断（ABI，即基于神经网络的模拟推断）中。首先，我们利用权重共享在训练过程中编码替代似然和先验规范之间的结构相似性，以最小的计算开销。其次，我们利用神经网络的快速推断来评估对各种数据扰动或预处理程序的敏感性。与大多数其他贝叶斯方法相比，这两个步骤都避免了昂贵的计算。

    Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
    
[^64]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^65]: Entropy-MCMC: 轻松从平坦盆地进行采样

    Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05401](http://arxiv.org/abs/2310.05401)

    本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。

    

    贝叶斯深度学习依赖于对后验分布的质量估计。然而，深度神经网络的后验分布在性质上是高度多模态的，局部模式表现出不同的泛化性能。在有限的计算资源下，从原始后验分布中进行采样可能会导致次优性能，因为一些样本可能会陷入“坏”模式并出现过拟合。基于观察到低泛化误差的“好”模式通常存在于能量景观的平坦盆地中，我们提出通过偏置采样朝向这些平坦区域的后验。具体而言，我们引入了一个辅助引导变量，其稳态分布类似于平滑后验分布，并且没有尖锐的模态，以引导MCMC采样器在平坦的盆地中采样。通过将此引导变量与模型参数相结合，我们创建了一个简单的联合分布，可以在最小计算开销下实现高效采样。我们证明了我们的元算法的收敛性。

    Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
    
[^66]: 关于去噪中的后验分布：在不确定性量化中的应用

    On the Posterior Distribution in Denoising: Application to Uncertainty Quantification. (arXiv:2309.13598v1 [cs.CV])

    [http://arxiv.org/abs/2309.13598](http://arxiv.org/abs/2309.13598)

    该论文研究了去噪中的后验分布及其与后验均值之间的关系，并应用于预训练去噪器的不确定性量化。提出了一种高效计算后验分布主成分和近似边际分布的方法。不需要显式计算高阶矩张量或进行训练或微调。

    

    去噪算法在许多应用中起着核心作用，从降噪低级别成像传感器到提升基于评分的生成模型。后一类方法使用Tweedie公式，将高斯去噪的后验均值（即最小均方误差去噪器）与数据分布的评分链接起来。我们在这里推导了后验分布的高阶中心矩与后验均值的高阶导数之间的基本关系。我们利用这个结果进行预训练去噪器的不确定性量化。特别地，我们展示了如何高效计算图像任何所需区域的后验分布的主成分，以及如何近似沿这些（或任何其他）一维方向的完整边际分布。我们的方法快速且内存高效，因为它不需要显式计算或存储高阶矩张量，并且无需训练或微调。

    Denoisers play a central role in many applications, from noise suppression in low-grade imaging sensors, to empowering score-based generative models. The latter category of methods makes use of Tweedie's formula, which links the posterior mean in Gaussian denoising (i.e., the minimum MSE denoiser) with the score of the data distribution. Here, we derive a fundamental relation between the higher-order central moments of the posterior distribution, and the higher-order derivatives of the posterior mean. We harness this result for uncertainty quantification of pre-trained denoisers. Particularly, we show how to efficiently compute the principal components of the posterior distribution for any desired region of an image, as well as to approximate the full marginal distribution along those (or any other) one-dimensional directions. Our method is fast and memory efficient, as it does not explicitly compute or store the high-order moment tensors and it requires no training or fine tuning of t
    
[^67]: 机器学习中的人类限制：利用土壤微生物数据预测植物表型

    Human Limits in Machine Learning: Prediction of Plant Phenotypes Using Soil Microbiome Data. (arXiv:2306.11157v1 [stat.ML])

    [http://arxiv.org/abs/2306.11157](http://arxiv.org/abs/2306.11157)

    本论文深入研究了机器学习模型在预测土壤与植物表型之间联系方面的潜力，证明加入土壤物理化学性质和微生物种群密度等环境特征可以提高预测准确性。

    

    保护土壤健康被认为是21世纪的主要挑战之一，因为它在农业、人类健康和生物多样性方面具有广泛（可能具有威胁性的）影响。本研究通过两种模型（随机森林和贝叶斯神经网络）探索了利用机器学习模型来理解土壤和生物表型之间联系的预测潜力。结果表明，在模型中加入土壤物理化学性质和微生物种群密度等环境特征可以提高预测准确性。此外，通过探索多种数据预处理策略，如归一化、零替换和数据增强，进一步提高了预测性能。

    The preservation of soil health has been identified as one of the main challenges of the XXI century given its vast (and potentially threatening) ramifications in agriculture, human health and biodiversity. Here, we provide the first deep investigation of the predictive potential of machine-learning models to understand the connections between soil and biological phenotypes. Indeed, we investigate an integrative framework performing accurate machine-learning-based prediction of plant phenotypes from biological, chemical and physical properties of the soil via two models: random forest and Bayesian neural network. We show that prediction is improved, as evidenced by higher weighted F1 scores, when incorporating into the models environmental features like soil physicochemical properties and microbial population density in addition to the microbiome information. Furthermore, by exploring multiple data preprocessing strategies such as normalization, zero replacement, and data augmentation,
    
[^68]: 使用自适应流采样平衡训练能量基模型

    Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])

    [http://arxiv.org/abs/2306.00684](http://arxiv.org/abs/2306.00684)

    本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。

    

    能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。

    Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
    
[^69]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^70]: Adam家族算法在无平滑优化中的收敛性保证研究

    Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])

    [http://arxiv.org/abs/2305.03938](http://arxiv.org/abs/2305.03938)

    本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。

    

    本文对Adam家族算法在无平滑优化中的收敛性进行了全面研究，特别是在无平滑神经网络的训练中。我们提出了一种新的双时间尺度框架，采用双时间尺度更新方案，证明了其在温和条件下的收敛性。我们的框架包括了各种流行的Adam家族算法，在训练无平滑神经网络中提供了收敛性保证。此外，我们还开发了随机次梯度方法，结合梯度裁剪技术，用于训练具有重尾噪声的无平滑神经网络。通过我们的框架，我们展示了我们提出的方法甚至在仅假定评估噪声可积的情况下也会收敛。广泛的数值实验证明了我们提出的方法的高效性和稳健性。

    In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.
    
[^71]: 无碰撞运输图在流行学习中的应用

    Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])

    [http://arxiv.org/abs/2304.00199](http://arxiv.org/abs/2304.00199)

    本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。

    

    本文研究了引入于[Nurbekyan et al.，2020]的无碰撞运输图在图像数据的流形学习中的应用。近年来，在表示类似运动或变形现象的数据中，应用基于运输的距离和特征的研究大幅增加。事实上，固定位置比较强度通常无法显示数据结构。在[Nurbekyan et al.，2020]中开发的无碰撞图和距离类似于最优传输(OT)图的几何特征但由于无需优化，计算成本要便宜得多。本文证明无碰撞距离提供单个概率测度的平移(分别是伸缩)和装备欧几里得距离的平移(分别是伸缩)向量之间的等距性。此外，我们证明，无碰撞运输图以及OT和线性OT图，一般来说不能为旋转提供等距性。

    In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
    
[^72]: 朝黑盒参数估计迈进

    Towards black-box parameter estimation. (arXiv:2303.15041v1 [stat.ML])

    [http://arxiv.org/abs/2303.15041](http://arxiv.org/abs/2303.15041)

    本文提出了一种基于弱参数结构假设的黑盒程序，用于估计统计模型参数。该程序可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。

    

    深度学习算法最近已经被证明是估计统计模型参数的成功工具，模拟容易但似然计算具有挑战性。但这些方法的成功取决于模拟出可以充分复制观察数据的参数，并且目前缺乏有效的方法来产生这些模拟数据。我们开发了基于弱参数结构假设估计统计模型参数的新的黑盒程序。对于似然函数有较频繁出现的良好结构的情况，如时间序列，这是通过在广泛的模拟数据库上预训练深度神经网络来实现的，该数据库涵盖了各种数据大小的范围。对于其他类型的复杂依赖关系，则需要一个迭代的算法来指导多轮正确参数区域的模拟。这些方法可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。

    Deep learning algorithms have recently shown to be a successful tool in estimating parameters of statistical models for which simulation is easy, but likelihood computation is challenging. But the success of these approaches depends on simulating parameters that sufficiently reproduce the observed data, and, at present, there is a lack of efficient methods to produce these simulations. We develop new black-box procedures to estimate parameters of statistical models based only on weak parameter structure assumptions. For well-structured likelihoods with frequent occurrences, such as in time series, this is achieved by pre-training a deep neural network on an extensive simulated database that covers a wide range of data sizes. For other types of complex dependencies, an iterative algorithm guides simulations to the correct parameter region in multiple rounds. These approaches can successfully estimate and quantify the uncertainty of parameters from non-Gaussian models with complex spatia
    
[^73]: 一种双层经验风险最小化算法的下界和近似最优算法

    A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08766](http://arxiv.org/abs/2302.08766)

    该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。

    

    双层最优化问题越来越多地应用于机器学习中。在许多实际情况下，上层和下层目标对应于经验风险最小化问题，并因此具有总和结构。在这个背景下，我们提出了一个著名的SARAH算法的双层扩展。我们证明了该算法需要$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$次梯度计算才能实现$\varepsilon$稳定性，其中$n+m$是样本总数，这比先前所有的双层算法都要好。此外，我们提供了一个下界，用于得到双层问题的目标函数的近似稳定点所需的oracle调用次数。这个下界正是我们的算法所达到的，因此在样本复杂度方面是最优的。

    Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
    
[^74]: 关于使用近似传输映射进行抽样的研究

    On Sampling with Approximate Transport Maps. (arXiv:2302.04763v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04763](http://arxiv.org/abs/2302.04763)

    本研究探讨了两种基于传输映射的抽样方法，研究结果表明，基于流的提议可以处理多峰目标，在高维度和训练不良的情况下使用依赖于重新参数化的方法更加稳健。

    

    通过将分布转化为易于处理的分布，传输映射可以简化具有非平凡几何结构的分布的抽样。随着深度神经网络参数化的传统流（NF）的发展，这种方法的潜力不断提高。NF增强采样器最近提出了将马尔可夫链蒙特卡罗方法与（i）来自流的提议绘制或（ii）基于流的重新参数化相结合。在这两种情况下，学习到的传输的质量会影响性能。本研究首次阐明了这两种方法的相对优势和劣势。我们的研究得出结论：直到中等维度，可以可靠地使用基于流的提议处理多峰目标。相比之下，在高维度和训练不良的情况下，依赖于重新参数化的方法在多模式方面存在困难，但其他方面更为稳健。

    Transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. The potential of this approach has risen with the development of Normalizing Flows (NF) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target. NF-enhanced samplers recently proposed blend (Markov chain) Monte Carlo methods with either (i) proposal draws from the flow or (ii) a flow-based reparametrization. In both cases, the quality of the learned transport conditions performance. The present work clarifies for the first time the relative strengths and weaknesses of these two approaches. Our study concludes that multimodal targets can be reliably handled with flow-based proposals up to moderately high dimensions. In contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training. To furthe
    
[^75]: 逆可解性和安全性及其在联邦学习中的应用

    Inverse Solvability and Security with Applications to Federated Learning. (arXiv:2211.14115v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14115](http://arxiv.org/abs/2211.14115)

    介绍了逆可解性和安全性的概念，以及其在联邦学习中的应用。论文提供了模型示例，展示了如何通过增加用户数量来增加可解性和安全性。

    

    我们介绍了逆可解性和安全性的概念，适用于一般线性前向模型，并展示了如何将其应用于联邦学习中使用的模型。我们提供了这样的模型的示例，其逆可解性和安全性在本文中得到定义。我们还展示了如何利用参与给定迭代的大量用户来增加可解性和安全性。最后，我们讨论了所提出概念的可能扩展，包括非线性情况。

    We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.
    
[^76]: 从去噪扩散到去噪马尔科夫模型

    From Denoising Diffusions to Denoising Markov Models. (arXiv:2211.03595v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.03595](http://arxiv.org/abs/2211.03595)

    本论文提出了一个统一的框架，将去噪扩散模型推广到广泛的空间中，并导致分数匹配的原始扩展，适用于各种应用程序。

    

    去噪扩散是展现出卓越实验性能的最先进的生成模型。他们通过将数据分布扩散到高斯分布，然后学习逆转这个噪声过程以获取合成数据点。去噪扩散依赖于使用分数匹配对噪声数据密度的对数导数的逼近。当只能从先验分布和似然函数中进行抽样时，这种模型也可用于执行近似后验模拟。我们提出了一个统一框架，将此方法推广到一类广泛的空间，并导致分数匹配的原始扩展。我们通过各种应用程序说明了所得模型。

    Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.
    
[^77]: 类别不平衡下的学习动态理论分析

    A Theoretical Analysis of the Learning Dynamics under Class Imbalance. (arXiv:2207.00391v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.00391](http://arxiv.org/abs/2207.00391)

    本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。

    

    数据不平衡是机器学习中常见的问题，会严重影响模型性能。虽然有各种解决方案，但它们对学习动态的收敛影响尚未被理解。本文阐明了数据不平衡对学习的显著负面影响，当使用梯度优化器进行训练时，少数类和多数类的学习曲线会遵循次优轨迹。这种放缓与不平衡比相关，可以追溯到优化不同类别之间的竞争。我们的主要贡献在于分析了全批次（GD）和随机梯度下降（SGD）的收敛和各种对每种类别梯度做出贡献的归一化变体。我们发现GD不能保证降低每个类别的损失，但可以通过执行各自归一化梯度来解决这个问题。使用SGD时, 类别不平衡会对算法产生额外的影响。

    Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on th
    
[^78]: 在复杂的多智能体场景中估计反事实治疗结果的时间变化

    Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.01900](http://arxiv.org/abs/2206.01900)

    本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。

    

    在各种工程和科学领域中，评估多智能体系统中的干预行为（例如，人类何时应该干预自动驾驶系统，何时球员应该传给队友进行好射门）是一项具有挑战性的任务。使用反事实的长期预测来估计个体治疗效果（ITE）是评估此类干预措施的实用方法。然而，大多数传统框架没有考虑到多智能体关系的时间变化和协变量反事实预测的复杂结构，这可能导致ITE的错误评估和解释困难。在这里，我们提出了一个可解释的反事实循环网络，用于估计干预的效果。我们的模型利用图形变分循环神经网络和基于领域知识的计算来进行基于多智能体协变量和结果的长期预测的ITE估计框架，能够确认循环结构。

    Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
    

