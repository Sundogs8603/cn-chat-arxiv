# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Meta-Learning Operators to Optimality from Multi-Task Non-IID Data.](http://arxiv.org/abs/2308.04428) | 本文提出了从多任务非独立同分布数据中恢复线性操作符的方法，并发现现有的各向同性无关的元学习方法会对表示更新造成偏差，限制了表示学习的样本复杂性。为此，引入了去偏差和特征白化的适应方法。 |
| [^2] | [SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling.](http://arxiv.org/abs/2308.04365) | SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。 |
| [^3] | [Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs.](http://arxiv.org/abs/2308.04314) | 本文提出了一种新的合作赌博机算法，实现了最佳的个体遗憾和恒定的通信成本。 |
| [^4] | [Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study.](http://arxiv.org/abs/2308.04212) | 本文提出了一种基于KNN-基于LASSO的方法，在健康结果研究中建立了动态模型，能够准确捕捉健康结果和危险因素之间的年龄相关关联。 |
| [^5] | [Parallel Learning by Multitasking Neural Networks.](http://arxiv.org/abs/2308.04106) | 本文介绍了一种名为多任务Hebbian网络的神经网络模型，能够自然地实现并行学习，并处理各种模式的信号幅度分布。这对于人工智能中学习多个模式的挑战具有重要意义。 |
| [^6] | [Toward Improving Predictive Risk Modelling for New Zealand's Child Welfare System Using Clustering Methods.](http://arxiv.org/abs/2308.04060) | 本文利用主成分分析和K-Means聚类方法，初步研究了新西兰儿童福利系统的预测风险建模，发现了一些特征并了解了其对当前风险建模框架的潜在影响。 |
| [^7] | [Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization.](http://arxiv.org/abs/2308.04051) | 该论文提出了一种新的形状优化方法，通过降低设计空间维度和建模数据的生成过程，实现了提高全局优化算法效率和生成无几何异常的高质量设计。 |
| [^8] | [Optimal partitioning of directed acyclic graphs with dependent costs between clusters.](http://arxiv.org/abs/2308.03970) | 本论文提出了一种名为DCMAP的算法，用于对具有依赖成本的有向无环图进行最优分区。该算法通过优化基于DAG和集群映射的成本函数来寻找所有最优集群，并在途中返回接近最优解。实验证明在复杂系统的DBN模型中，该算法具有时间效率性。 |
| [^9] | [A new approach for evaluating internal cluster validation indices.](http://arxiv.org/abs/2308.03894) | 本文回顾了现有的聚类验证方法并提出了一种新方法，用于评估无监督分类算法在不同类型的数据中的表现。 |
| [^10] | [Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations.](http://arxiv.org/abs/2308.03882) | 本文提出了一种利用未见状态增强的策略，在离线强化学习中通过基于价值的扰动和过滤，实现了对离线数据之外的状态的利用和泛化。 |
| [^11] | [Can We Trust Race Prediction?.](http://arxiv.org/abs/2307.08496) | 本文研究了在没有敏感的种族和族裔数据的情况下，使用代理模型进行种族预测的问题。研究者训练了一个双向长短时记忆（BiLSTM）模型，并创建了一个集成模型，其在外样本（OOS）F1得分上比文献中表现最好的机器学习模型高出36.8%。此外，还构建了美国最全面的姓氏和名字分布数据库，并提供了第一个高质量的基准数据集，以公正比较现有模型，并帮助未来的模型开发者。 |
| [^12] | [Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization.](http://arxiv.org/abs/2307.03571) | 本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。 |
| [^13] | [Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders.](http://arxiv.org/abs/2305.19259) | 本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。 |
| [^14] | [Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching.](http://arxiv.org/abs/2305.17884) | 本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。 |
| [^15] | [Spatial-photonic Boltzmann machines: low-rank combinatorial optimization and statistical learning by spatial light modulation.](http://arxiv.org/abs/2303.14993) | 本文提出了一种基于空间光调制的SPBM计算模型，可以高效地解决任何伊辛问题，特别适用于具有低秩相互作用矩阵的问题，并且具有学习、分类和采样的能力。 |
| [^16] | [Causal Razors.](http://arxiv.org/abs/2302.10331) | 本文比较了许多出现在文献中的因果剃刀，并特别研究了在多项式因果模型中不太受欢迎的因果剃刀——参数最小性。逻辑结果揭示了选择合理得分标准时的困境。 |
| [^17] | [MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows.](http://arxiv.org/abs/2302.01075) | 本文提出了一个新的GAN生成建模框架MonoFlow，通过Wasserstein梯度流获得理论洞见和算法启示。该框架使用密度比例的单调递增映射重新缩放粒子演化，并通过训练鉴别器获得MonoFlow的向量场，利用相应的向量场进行粒子流的生成。 |
| [^18] | [Moment Estimation for Nonparametric Mixture Models Through Implicit Tensor Decomposition.](http://arxiv.org/abs/2210.14386) | 本文提出了一种隐式张量分解矩估计方法，用于在高维空间中估计条件独立的混合模型，无需对分布进行参数化，通过开发高效的无张量操作，实现了计算上的可行性，证明了算法的竞争性能，并建立了混合物的可识别性。 |
| [^19] | [Composite Goodness-of-fit Tests with Kernels.](http://arxiv.org/abs/2111.10275) | 本文提出了一种基于核的假设检验方法，可以解决具有挑战性的复合检验问题，其核心思想是在正确的模型规范的零假设下，非参数地估计参数（或模拟器）分布。 |
| [^20] | [Parameter selection in Gaussian process interpolation: an empirical study of selection criteria.](http://arxiv.org/abs/2107.06006) | 高斯过程插值中的参数选择是一个重要问题，本文通过评分规则和扩展似然标准的框架研究了选择适当的模型族的重要性。 |
| [^21] | [Practical and Rigorous Uncertainty Bounds for Gaussian Process Regression.](http://arxiv.org/abs/2105.02796) | 高斯过程回归提供了不确定性估计，但其贝叶斯性质限制了其在某些重要应用中的使用。为解决这一问题，我们提出了实用且严格的不确定性界限，相比现有结果更准确，并且对模型偏差有优雅的退化。 |
| [^22] | [Learning Bayesian Networks with Annealing Machine.](http://arxiv.org/abs/2006.06926) | 本文提出了一种用于贝叶斯网络结构学习的模拟退火机器方法，通过先进的候选父节点集合的确定和分解，以及整数规划问题的解决，能够在比特容量有限的情况下高效地解决基于评分的学习问题。 |
| [^23] | [Discriminator optimal transport.](http://arxiv.org/abs/1910.06832) | 这篇论文研究了判别器优化过程如何增加生成对抗网络中Wasserstein距离的对偶代价函数的下限，从而使训练好的判别器能够近似最优输运。作者提出了一种判别器最优输运（DOT）方案，通过实验证明了在不同数据集上的生成图像质量有所提升。 |

# 详细

[^1]: 从多任务非独立同分布数据中元学习操作符到最优性

    Meta-Learning Operators to Optimality from Multi-Task Non-IID Data. (arXiv:2308.04428v1 [stat.ML])

    [http://arxiv.org/abs/2308.04428](http://arxiv.org/abs/2308.04428)

    本文提出了从多任务非独立同分布数据中恢复线性操作符的方法，并发现现有的各向同性无关的元学习方法会对表示更新造成偏差，限制了表示学习的样本复杂性。为此，引入了去偏差和特征白化的适应方法。

    

    机器学习中最近取得进展的一个强大概念是从异构来源或任务的数据中提取共同特征。直观地说，将所有数据用于学习共同的表示函数，既有助于计算效率，又有助于统计泛化，因为它可以减少要在给定任务上进行微调的参数数量。为了在理论上做出这些优点的根源，我们提出了从噪声向量测量$y = Mx + w$中回复线性操作符$M$的一般模型。其中，协变量$x$既可以是非独立同分布的，也可以是非各向同性的。我们证明了现有的各向同性无关的元学习方法会对表示更新造成偏差，这导致噪声项的缩放不再有利于源任务数量。这反过来会导致表示学习的样本复杂性受到单任务数据规模的限制。我们引入了一种方法，称为去偏差和特征白化。

    A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias & Feature-Whiten}
    
[^2]: SLEM：机器学习用于路径建模和因果推断的超级学习者方程模型

    SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])

    [http://arxiv.org/abs/2308.04365](http://arxiv.org/abs/2308.04365)

    SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。

    

    因果推断是科学的关键目标，使研究人员能够通过观察数据得出关于对假定干预的预测的有意义的结论。路径模型、结构方程模型(SEMs)以及更一般的有向无环图(DAGs)能够明确地指定关于现象背后的因果结构的假设。与DAGs不同，SEMs假设线性关系，这可能导致函数错误规范，从而阻碍研究人员进行可靠的效果大小估计。相反，我们提出了超级学习者方程模型（SLEM），一种集成了机器学习超级学习者集成的路径建模技术。我们通过实证研究，证明了SLEM能够提供一致且无偏的因果效应估计，在与SEMs进行线性模型比较时表现出竞争力，并且在处理非线性关系时优于SEMs。

    Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
    
[^3]: 合作式多智能体赌博机：具有最佳个体遗憾和恒定通信成本的分布式算法

    Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs. (arXiv:2308.04314v1 [cs.LG])

    [http://arxiv.org/abs/2308.04314](http://arxiv.org/abs/2308.04314)

    本文提出了一种新的合作赌博机算法，实现了最佳的个体遗憾和恒定的通信成本。

    

    最近，对合作式多智能体多臂赌博机进行了广泛研究，其中一组分布式智能体合作玩相同的多臂赌博游戏。目标是开发具有最佳群体和个体遗憾以及智能体之间通信成本低的赌博机算法。在前期工作中，使用了两种范式来解决这个问题：领导者-跟随者和完全分布式算法。在这两种范式中，以前的算法都能达到最佳群体遗憾。领导者-跟随者算法实现了恒定的通信成本，但未能达到最佳个体遗憾。目前最先进的完全分布式算法实现了最佳个体遗憾，但未能实现恒定的通信成本。本文提出了一种简单而有效的通信策略，并将其整合到合作赌博机的学习算法中。我们的算法同时实现了两种范式的最优个体遗憾和恒定通信成本。

    Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
    
[^4]: 基于KNN-基于LASSO的区域分位数变系数模型在健康结果研究中的应用

    Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study. (arXiv:2308.04212v1 [stat.ML])

    [http://arxiv.org/abs/2308.04212](http://arxiv.org/abs/2308.04212)

    本文提出了一种基于KNN-基于LASSO的方法，在健康结果研究中建立了动态模型，能够准确捕捉健康结果和危险因素之间的年龄相关关联。

    

    健康结果，如身体质量指数和胆固醇水平，已知依赖于年龄，并表现出与其相关危险因素的变化影响。本文提出了一种新颖的框架，通过K最近邻(Lasso)基于区域分位数回归来动态建模健康结果和危险因素之间的关联，以捕捉年龄的时变效应。所提出的方法具有强大的理论性质，包括紧密的估计误差界限和在某些正则条件下检测精确的聚类模式的能力。为了高效解决所得到的优化问题，我们开发了一种交替方向乘法器(ADMM)算法。我们的实证结果表明，所提出的方法能够捕捉健康结果和其风险因素之间复杂的年龄相关关联。

    Health outcomes, such as body mass index and cholesterol levels, are known to be dependent on age and exhibit varying effects with their associated risk factors. In this paper, we propose a novel framework for dynamic modeling of the associations between health outcomes and risk factors using varying-coefficients (VC) regional quantile regression via K-nearest neighbors (KNN) fused Lasso, which captures the time-varying effects of age. The proposed method has strong theoretical properties, including a tight estimation error bound and the ability to detect exact clustered patterns under certain regularity conditions. To efficiently solve the resulting optimization problem, we develop an alternating direction method of multipliers (ADMM) algorithm. Our empirical results demonstrate the efficacy of the proposed method in capturing the complex age-dependent associations between health outcomes and their risk factors.
    
[^5]: 并行学习的多任务神经网络

    Parallel Learning by Multitasking Neural Networks. (arXiv:2308.04106v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2308.04106](http://arxiv.org/abs/2308.04106)

    本文介绍了一种名为多任务Hebbian网络的神经网络模型，能够自然地实现并行学习，并处理各种模式的信号幅度分布。这对于人工智能中学习多个模式的挑战具有重要意义。

    

    人工智能的一个现代挑战是同时学习多种模式（即并行学习）。本文中我们展示了多任务Hebbian网络（在稀疏数据集上工作的Hopfield模型变体）如何自然地执行这一复杂任务。我们关注的是并行处理有限数量（与网络大小的对数增长相对应）模式的系统，类似于标准关联神经网络在模式识别中的低存储水平。对于模式的轻度稀释，网络以层次方式处理它们，根据其信息内容以幂律分布其信号的幅度（层次制度），而对于强稀释，所有与所有模式相关的信号均具有相同的强度（并行制度）。此外，仅限于低存储设置（即远离自旋玻璃）。

    A modern challenge of Artificial Intelligence is learning multiple patterns at once (i.e.parallel learning). While this can not be accomplished by standard Hebbian associative neural networks, in this paper we show how the Multitasking Hebbian Network (a variation on theme of the Hopfield model working on sparse data-sets) is naturally able to perform this complex task. We focus on systems processing in parallel a finite (up to logarithmic growth in the size of the network) amount of patterns, mirroring the low-storage level of standard associative neural networks at work with pattern recognition. For mild dilution in the patterns, the network handles them hierarchically, distributing the amplitudes of their signals as power-laws w.r.t. their information content (hierarchical regime), while, for strong dilution, all the signals pertaining to all the patterns are raised with the same strength (parallel regime). Further, confined to the low-storage setting (i.e., far from the spin glass 
    
[^6]: 改进利用聚类方法进行新西兰儿童福利系统的预测风险建模

    Toward Improving Predictive Risk Modelling for New Zealand's Child Welfare System Using Clustering Methods. (arXiv:2308.04060v1 [stat.ML])

    [http://arxiv.org/abs/2308.04060](http://arxiv.org/abs/2308.04060)

    本文利用主成分分析和K-Means聚类方法，初步研究了新西兰儿童福利系统的预测风险建模，发现了一些特征并了解了其对当前风险建模框架的潜在影响。

    

    临床判断和预测风险模型的结合对社工在划分处于虐待风险中的儿童并决定何时采取干预措施至关重要。政府福利机构利用行政数据和机器学习算法已经开始了解决这个问题的预测风险建模工作。虽然以往的研究已经调查了与儿童虐待有关的风险因素，但仍存在很多空白，尚不清楚这些风险因素如何相互作用以及预测风险模型在具有不同特征的儿童中是否表现不同。本文通过整合主成分分析和K-Means聚类，初步发现了我们在确定这些特征及其对当前风险建模框架的潜在影响的工作。这种方法可以对新西兰（NZ）被报告有护理和保护问题的儿童的现存、尚未确定的聚类进行研究。

    The combination of clinical judgement and predictive risk models crucially assist social workers to segregate children at risk of maltreatment and decide when authorities should intervene. Predictive risk modelling to address this matter has been initiated by several governmental welfare authorities worldwide involving administrative data and machine learning algorithms. While previous studies have investigated risk factors relating to child maltreatment, several gaps remain as to understanding how such risk factors interact and whether predictive risk models perform differently for children with different features. By integrating Principal Component Analysis and K-Means clustering, this paper presents initial findings of our work on the identification of such features as well as their potential effect on current risk modelling frameworks. This approach allows examining existent, unidentified yet, clusters of New Zealand (NZ) children reported with care and protection concerns, as well
    
[^7]: 形状优化中的异常检测和设计空间维度降低的生成模型

    Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization. (arXiv:2308.04051v1 [stat.ML])

    [http://arxiv.org/abs/2308.04051](http://arxiv.org/abs/2308.04051)

    该论文提出了一种新的形状优化方法，通过降低设计空间维度和建模数据的生成过程，实现了提高全局优化算法效率和生成无几何异常的高质量设计。

    

    我们的工作提出了一种新颖的形状优化方法，其两个目标是提高全局优化算法的效率，同时在优化过程中生成没有几何异常的高质量设计。通过减少定义新的减少子空间的原始设计变量的数量，并使用概率线性潜变量模型来建模数据的底层生成过程，如因子分析和概率主成分分析，来实现这一目标。我们展示了当形状修改方法是线性的且设计变量在均匀随机采样时，数据近似服从高斯分布，这是由于直接应用了中心极限定理。利用马氏距离来衡量模型不确定性，并且论文证明异常设计往往具有较高的该度量值。

    Our work presents a novel approach to shape optimization, that has the twofold objective to improve the efficiency of global optimization algorithms while promoting the generation of high-quality designs during the optimization process free of geometrical anomalies. This is accomplished by reducing the number of the original design variables defining a new reduced subspace where the geometrical variance is maximized and modeling the underlying generative process of the data via probabilistic linear latent variable models such as Factor Analysis and Probabilistic Principal Component Analysis. We show that the data follows approximately a Gaussian distribution when the shape modification method is linear and the design variables are sampled uniformly at random, due to the direct application of the central limit theorem. The model uncertainty is measured in terms of Mahalanobis distance, and the paper demonstrates that anomalous designs tend to exhibit a high value of this metric. This en
    
[^8]: 对具有依赖成本的有向无环图进行最优分区

    Optimal partitioning of directed acyclic graphs with dependent costs between clusters. (arXiv:2308.03970v1 [cs.DS])

    [http://arxiv.org/abs/2308.03970](http://arxiv.org/abs/2308.03970)

    本论文提出了一种名为DCMAP的算法，用于对具有依赖成本的有向无环图进行最优分区。该算法通过优化基于DAG和集群映射的成本函数来寻找所有最优集群，并在途中返回接近最优解。实验证明在复杂系统的DBN模型中，该算法具有时间效率性。

    

    许多统计推断场景，包括贝叶斯网络、马尔可夫过程和隐马尔可夫模型，可以通过将基础的有向无环图（DAG）划分成集群来支持。然而，在统计推断中，最优划分是具有挑战性的，因为要优化的成本取决于集群内的节点以及通过父节点和/或子节点连接的集群之间的映射，我们将其称为依赖集群。我们提出了一种名为DCMAP的新算法，用于具有依赖集群的最优集群映射。在基于DAG和集群映射的任意定义的正成本函数的基础上，我们证明DCMAP收敛于找到所有最优集群，并在途中返回接近最优解。通过实验证明，该算法对使用计算成本函数的一个海草复杂系统的DBN模型具有时间效率性。对于一个25个和50个节点的DBN，搜索空间大小分别为$9.91\times 10^9$和$1.5$

    Many statistical inference contexts, including Bayesian Networks (BNs), Markov processes and Hidden Markov Models (HMMS) could be supported by partitioning (i.e.~mapping) the underlying Directed Acyclic Graph (DAG) into clusters. However, optimal partitioning is challenging, especially in statistical inference as the cost to be optimised is dependent on both nodes within a cluster, and the mapping of clusters connected via parent and/or child nodes, which we call dependent clusters. We propose a novel algorithm called DCMAP for optimal cluster mapping with dependent clusters. Given an arbitrarily defined, positive cost function based on the DAG and cluster mappings, we show that DCMAP converges to find all optimal clusters, and returns near-optimal solutions along the way. Empirically, we find that the algorithm is time-efficient for a DBN model of a seagrass complex system using a computation cost function. For a 25 and 50-node DBN, the search space size was $9.91\times 10^9$ and $1.5
    
[^9]: 一种评估内部聚类验证指标的新方法

    A new approach for evaluating internal cluster validation indices. (arXiv:2308.03894v1 [cs.LG])

    [http://arxiv.org/abs/2308.03894](http://arxiv.org/abs/2308.03894)

    本文回顾了现有的聚类验证方法并提出了一种新方法，用于评估无监督分类算法在不同类型的数据中的表现。

    

    无监督分类有很多不同的方法可供选择。由于没有一个算法和参数设置在所有类型的数据中表现最佳，因此需要进行聚类验证来选择真正表现最好的算法。为此，提出了几种不使用任何额外（外部）信息的内部验证指标。可以通过将它们应用于具有已知聚类结构的数据集的分类来评估这些内部验证指标。评估方法在如何使用真实分类信息方面存在差异。本文回顾了这些方法，考虑了它们的优势和劣势，然后提出了一种新方法。

    A vast number of different methods are available for unsupervised classification. Since no algorithm and parameter setting performs best in all types of data, there is a need for cluster validation to select the actually best-performing algorithm. Several indices were proposed for this purpose without using any additional (external) information. These internal validation indices can be evaluated by applying them to classifications of datasets with a known cluster structure. Evaluation approaches differ in how they use the information on the ground-truth classification. This paper reviews these approaches, considering their advantages and disadvantages, and then suggests a new approach.
    
[^10]: 通过未见过的状态增强利用广义化在离线强化学习中

    Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])

    [http://arxiv.org/abs/2308.03882](http://arxiv.org/abs/2308.03882)

    本文提出了一种利用未见状态增强的策略，在离线强化学习中通过基于价值的扰动和过滤，实现了对离线数据之外的状态的利用和泛化。

    

    离线强化学习方法通过对未见过的状态和动作进行保守价值评估来平衡探索和利用。无模型方法会对所有未见过的动作进行惩罚，而有模型方法可以进一步通过模型展开对未见过的状态进行利用。然而，由于两个因素，这些方法在找到离线数据之外的未见过的状态时存在困难：(a)由于级联模型误差，模型的展开范围非常短，(b)模型展开仅以离线数据中观察到的状态为起点。我们放宽了第二个假设，并提出了一种新颖的未见过状态增强策略，以允许学得的模型和价值估计在未见状态中泛化。我们的策略通过对观察到的状态进行基于价值的扰动来找到未见过的状态，然后通过过滤具有过高的启发性不确定性估计（高误差）或过低的（过于相似）

    Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
    
[^11]: 我们能相信种族预测吗？

    Can We Trust Race Prediction?. (arXiv:2307.08496v1 [cs.LG])

    [http://arxiv.org/abs/2307.08496](http://arxiv.org/abs/2307.08496)

    本文研究了在没有敏感的种族和族裔数据的情况下，使用代理模型进行种族预测的问题。研究者训练了一个双向长短时记忆（BiLSTM）模型，并创建了一个集成模型，其在外样本（OOS）F1得分上比文献中表现最好的机器学习模型高出36.8%。此外，还构建了美国最全面的姓氏和名字分布数据库，并提供了第一个高质量的基准数据集，以公正比较现有模型，并帮助未来的模型开发者。

    

    在没有敏感的种族和族裔数据的情况下，研究人员、监管机构和公司都借助代理模型。在本文中，我使用来自美国50个州的选民注册数据训练了一个双向长短时记忆（BiLSTM）模型，并创建了一个集成模型，其在外样本（OOS）F1得分上比文献中表现最好的机器学习模型高出36.8%。此外，我构建了美国最全面的姓氏和名字分布数据库，以改进贝叶斯改进姓氏地理编码（BISG）和贝叶斯改进名字姓氏地理编码（BIFSG）的覆盖和准确性。最后，我提供了第一个高质量的基准数据集，以公正比较现有模型，并帮助未来的模型开发者。

    In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
    
[^12]: 平滑边缘：利用Hadamard超参数化在稀疏正则化的平滑优化中的一般框架

    Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])

    [http://arxiv.org/abs/2307.03571](http://arxiv.org/abs/2307.03571)

    本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。

    

    本文介绍了一种用于（结构化）稀疏正则化问题中的$\ell_q$和$\ell_{p,q}$正则化的平滑方法。这些非平滑且可能非凸的问题的优化通常依赖于专门的过程。相比之下，我们的一般框架与主流的一阶优化方法（如随机梯度下降和加速变体）兼容，无需任何修改。这是通过平滑优化转移实现的，其中选定模型参数的超参数化使用Hadamard乘积和惩罚的改变。在超参数问题中，通过用替代参数进行平滑和凸性的$\ell_2$正则化，能够在原始参数化中引入非平滑和非凸性的$\ell_q$或$\ell_{p,q}$正则化。我们证明了我们的方法不仅能够得到匹配的全局最小值，还能得到等价的局部最小值。这在非凸稀疏正则化中尤其有用，因为在这种情况下找到全局最小值非常困难。

    This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
    
[^13]: Shuffle SGD总是比SGD更好：对具有任意数据顺序的SGD进行改进分析

    Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])

    [http://arxiv.org/abs/2305.19259](http://arxiv.org/abs/2305.19259)

    本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。

    

    随机梯度下降（SGD）算法被广泛用于优化神经网络，随机重排（RR）和单次洗牌（SS）是通过循环遍历训练数据的随机或单个排列的常见选择，然而这些算法在非凸情况下的收敛性质尚未完全理解。现有结果表明，在实际的训练场景中，当时代的数量小于训练集大小时，RR可能表现不如SGD。本文分析了一种允许任意数据排序的普通SGD算法，并展示了在非凸函数情况下的改进收敛速度。具体而言，我们的分析表明，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。总的来说，我们的研究凸显了使用随机/单次洗牌的SGD的好处，并为其非凸收敛性质提供了新的见解。

    Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
    
[^14]: 通过草图技术，将粒子方法和张量网络方法结合用于偏微分方程求解

    Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching. (arXiv:2305.17884v1 [math.NA])

    [http://arxiv.org/abs/2305.17884](http://arxiv.org/abs/2305.17884)

    本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。

    

    本文提出了一种解决高维偏微分方程的张量网络框架，其中我们采用粒子模拟更新解决方案，并使用最近提出的张量列车草图技术将新解决方案重新估计为张量网络。我们的方法还可以被解释为通过假设粒子来自底层张量网络来执行粒子数控制的可替代方法。我们通过将其应用于两种特定的情景来展示我们方法的通用性和灵活性：通过Langevin动力学模拟Fokker-Planck方程和通过辅助场量子蒙特卡罗模拟量子虚时间演化。

    In this paper, we propose a general framework for solving high-dimensional partial differential equations with tensor networks. Our approach offers a comprehensive solution methodology, wherein we employ a combination of particle simulations to update the solution and re-estimations of the new solution as a tensor-network using a recently proposed tensor train sketching technique. Our method can also be interpreted as an alternative approach for performing particle number control by assuming the particles originate from an underlying tensor network. We demonstrate the versatility and flexibility of our approach by applying it to two specific scenarios: simulating the Fokker-Planck equation through Langevin dynamics and quantum imaginary time evolution via auxiliary-field quantum Monte Carlo.
    
[^15]: 空间-光子Boltzmann机：利用空间光调制进行低秩组合优化和统计学习（arXiv:2303.14993v1 [cond-mat.dis-nn] CROSS LISTED）

    Spatial-photonic Boltzmann machines: low-rank combinatorial optimization and statistical learning by spatial light modulation. (arXiv:2303.14993v1 [cond-mat.dis-nn] CROSS LISTED)

    [http://arxiv.org/abs/2303.14993](http://arxiv.org/abs/2303.14993)

    本文提出了一种基于空间光调制的SPBM计算模型，可以高效地解决任何伊辛问题，特别适用于具有低秩相互作用矩阵的问题，并且具有学习、分类和采样的能力。

    

    空间-光子伊辛机（SPIM）[D. Pierangeli et al., Phys. Rev. Lett. 122, 213902 (2019)]是一种使用空间光调制有效解决大规模组合优化问题的光学架构。然而，SPIM仅能容纳具有秩为一的相互作用矩阵的伊辛问题，这限制了其在各种实际问题中的适用性。在本文中，我们提出了一种新的SPIM计算模型，可以在不改变其光学实现的情况下容纳任何伊辛问题。该模型对于具有低秩相互作用矩阵的伊辛问题（如背包问题）特别有效。此外，该模型具有学习能力，因此可以被称为空间光子Boltzmann机（SPBM）。我们证明了使用具有低秩相互作用的SPBM有效地实现了MNIST手写数字图像的学习、分类和采样。因此，所提出的SPBM模型表现出更高的实用性。

    The spatial-photonic Ising machine (SPIM) [D. Pierangeli et al., Phys. Rev. Lett. 122, 213902 (2019)] is a promising optical architecture utilizing spatial light modulation for solving large-scale combinatorial optimization problems efficiently. However, the SPIM can accommodate Ising problems with only rank-one interaction matrices, which limits its applicability to various real-world problems. In this Letter, we propose a new computing model for the SPIM that can accommodate any Ising problem without changing its optical implementation. The proposed model is particularly efficient for Ising problems with low-rank interaction matrices, such as knapsack problems. Moreover, the model acquires learning ability and can thus be termed a spatial-photonic Boltzmann machine (SPBM). We demonstrate that learning, classification, and sampling of the MNIST handwritten digit images are achieved efficiently using SPBMs with low-rank interactions. Thus, the proposed SPBM model exhibits higher practi
    
[^16]: 因果剃刀

    Causal Razors. (arXiv:2302.10331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10331](http://arxiv.org/abs/2302.10331)

    本文比较了许多出现在文献中的因果剃刀，并特别研究了在多项式因果模型中不太受欢迎的因果剃刀——参数最小性。逻辑结果揭示了选择合理得分标准时的困境。

    

    在进行因果推断时，必须对真实因果机制如何与底层联合概率分布相对应做出假设。本文将这些假设称为因果剃刀。我们回顾了许多出现在文献中的因果剃刀，对它们进行了全面的逻辑比较。特别地，我们对在多项式因果模型中不太受欢迎的因果剃刀——参数最小性进行了深入的研究，并研究了它与其他广泛研究的因果剃刀之间的逻辑关系。我们的逻辑结果在为基于分数的因果搜索算法选择合理得分标准时提出了困境。

    When performing causal discovery, assumptions have to be made on how the true causal mechanism corresponds to the underlying joint probability distribution. These assumptions are labeled as causal razors in this work. We review numerous causal razors that appeared in the literature, and offer a comprehensive logical comparison of them. In particular, we scrutinize an unpopular causal razor, namely parameter minimality, in multinomial causal models and its logical relations with other well-studied causal razors. Our logical result poses a dilemma in selecting a reasonable scoring criterion for score-based casual search algorithms.
    
[^17]: MonoFlow: 从Wasserstein梯度流的角度重新思考Divergence GANs

    MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01075](http://arxiv.org/abs/2302.01075)

    本文提出了一个新的GAN生成建模框架MonoFlow，通过Wasserstein梯度流获得理论洞见和算法启示。该框架使用密度比例的单调递增映射重新缩放粒子演化，并通过训练鉴别器获得MonoFlow的向量场，利用相应的向量场进行粒子流的生成。

    

    传统上，生成对抗网络（GANs）的对抗训练是通过判别器来估计离散度，生成器学习最小化这个离散度。我们认为，尽管许多GANs变体都是按照这个范例开发的，但当前GANs的理论理解和实际算法是不一致的。在本文中，通过利用展示了样本空间内粒子演化的Wasserstein梯度流来获得GANs的理论洞见和算法启示，我们介绍了一个统一的生成建模框架MonoFlow：粒子演化通过密度比例的单调递增映射进行重新缩放。在我们的框架下，对抗性训练可以被视为一个过程，首先通过训练鉴别器获得MonoFlow的向量场，然后生成器学习由相应向量场所定义的粒子流。

    The conventional understanding of adversarial training in generative adversarial networks (GANs) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of GANs were developed following this paradigm, the current theoretical understanding of GANs and their practical algorithms are inconsistent. In this paper, we leverage Wasserstein gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of GANs. We introduce a unified generative modeling framework - MonoFlow: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining MonoFlow's vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We al
    
[^18]: 非参数混合模型的隐式张量分解矩估计方法

    Moment Estimation for Nonparametric Mixture Models Through Implicit Tensor Decomposition. (arXiv:2210.14386v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2210.14386](http://arxiv.org/abs/2210.14386)

    本文提出了一种隐式张量分解矩估计方法，用于在高维空间中估计条件独立的混合模型，无需对分布进行参数化，通过开发高效的无张量操作，实现了计算上的可行性，证明了算法的竞争性能，并建立了混合物的可识别性。

    

    本文提出了一种交替最小二乘型的数值优化方案用于在 $\mathbb{R}^n$ 中估计条件独立的混合模型，无需对分布进行参数化。根据矩的方法，我们解决了一个不完整的张量分解问题以学习混合权重和各分量的均值。然后，通过线性求解，计算分量分布的累积分布函数、高阶矩和其他统计量。通过开发高效的无张量操作，避免了高阶张量所带来的高成本问题，使得计算在高维情况下更加可行。数值实验证明了该算法的竞争性能，并且它适用于许多模型和应用。此外，我们提供了理论分析，从混合物的低阶矩中建立了可识别性，并保证了 ALS 算法的局部线性收敛性。

    We present an alternating least squares type numerical optimization scheme to estimate conditionally-independent mixture models in $\mathbb{R}^n$, without parameterizing the distributions. Following the method of moments, we tackle an incomplete tensor decomposition problem to learn the mixing weights and componentwise means. Then we compute the cumulative distribution functions, higher moments and other statistics of the component distributions through linear solves. Crucially for computations in high dimensions, the steep costs associated with high-order tensors are evaded, via the development of efficient tensor-free operations. Numerical experiments demonstrate the competitive performance of the algorithm, and its applicability to many models and applications. Furthermore we provide theoretical analyses, establishing identifiability from low-order moments of the mixture and guaranteeing local linear convergence of the ALS algorithm.
    
[^19]: 带有核的复合适合性检验方法

    Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.10275](http://arxiv.org/abs/2111.10275)

    本文提出了一种基于核的假设检验方法，可以解决具有挑战性的复合检验问题，其核心思想是在正确的模型规范的零假设下，非参数地估计参数（或模拟器）分布。

    

    模型错误说明可能会对概率模型的实现造成重大挑战，这促使开发出一些直接解决此问题的鲁棒方法。但是，这些更为复杂的方法是否需要取决于模型是否真的错误，目前缺乏通用的方法回答这个问题。在本文中，我们提出了一种方法。更具体地说，我们提出了基于核的假设检验方法，用于具有挑战性的复合检验问题，即我们是否感兴趣的数据来自某些参数模型族中的任何分布。我们的测试利用基于最大均值差异和核Stein差异的最小距离估计器。它们具有广泛的适用性，包括当参数模型的密度已知除标准化常数外，或者如果模型采用模拟器形式。作为我们的主要结果，我们展示了在正确的模型规范的零假设下，我们能够非参数地估计参数（或模拟器）分布。我们提供了建立我们方法有效性的理论，并通过模拟和异常检测应用案例演示了其性能。

    Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
    
[^20]: 高斯过程插值中的参数选择：选择标准的实证研究

    Parameter selection in Gaussian process interpolation: an empirical study of selection criteria. (arXiv:2107.06006v5 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2107.06006](http://arxiv.org/abs/2107.06006)

    高斯过程插值中的参数选择是一个重要问题，本文通过评分规则和扩展似然标准的框架研究了选择适当的模型族的重要性。

    

    本文重新考虑了高斯过程插值中的参数选择问题。通过在参数化族中选择高斯过程的均值和协方差函数，用户可以获得一族贝叶斯程序，用于对未知函数进行预测，并且必须选择一个希望能够提供良好预测性能的成员。我们基于评分规则的基本概念进行研究，在建立留一法选择和验证标准以及基于Fasshauer等人在2009年提出的思想的扩展似然标准的有效框架上，恢复了诸如广义交叉验证准则之类的标准选择标准。在这个设定下，我们通过对文献中的几个测试问题进行实证研究，证明了选择适当的模型族往往比选择特定的选择准则更重要。

    This article revisits the fundamental problem of parameter selection for Gaussian process interpolation. By choosing the mean and the covariance functions of a Gaussian process within parametric families, the user obtains a family of Bayesian procedures to perform predictions about the unknown function, and must choose a member of the family that will hopefully provide good predictive performances. We base our study on the general concept of scoring rules, which provides an effective framework for building leave-one-out selection and validation criteria, and a notion of extended likelihood criteria based on an idea proposed by Fasshauer and co-authors in 2009, which makes it possible to recover standard selection criteria such as, for instance, the generalized cross-validation criterion. Under this setting, we empirically show on several test problems of the literature that the choice of an appropriate family of models is often more important than the choice of a particular selection c
    
[^21]: 高斯过程回归的实用且严格的不确定性界限

    Practical and Rigorous Uncertainty Bounds for Gaussian Process Regression. (arXiv:2105.02796v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.02796](http://arxiv.org/abs/2105.02796)

    高斯过程回归提供了不确定性估计，但其贝叶斯性质限制了其在某些重要应用中的使用。为解决这一问题，我们提出了实用且严格的不确定性界限，相比现有结果更准确，并且对模型偏差有优雅的退化。

    

    高斯过程回归是一种基于贝叶斯原理的流行的非参数回归方法，可以提供其预测的不确定性估计。然而，这些估计是贝叶斯性质的，对于一些重要的应用，如具有安全保证的基于学习的控制，需要频率的不确定性界限。尽管对于高斯过程，这样严格的界限有可用，但它们过于保守，在应用中没有用处。这通常导致实践者用启发式方法替代这些界限，从而破坏了所有的理论保证。为了解决这个问题，我们引入了新的不确定性界限，既严格又实用。特别地，这些界限可以明确地评估，并且比现有结果要保守得多。此外，我们还展示了某些模型偏差只会引起优雅的退化。我们展示了这些优势以及我们结果在基于学习的控制中的实用性。

    Gaussian Process Regression is a popular nonparametric regression method based on Bayesian principles that provides uncertainty estimates for its predictions. However, these estimates are of a Bayesian nature, whereas for some important applications, like learning-based control with safety guarantees, frequentist uncertainty bounds are required. Although such rigorous bounds are available for Gaussian Processes, they are too conservative to be useful in applications. This often leads practitioners to replacing these bounds by heuristics, thus breaking all theoretical guarantees. To address this problem, we introduce new uncertainty bounds that are rigorous, yet practically useful at the same time. In particular, the bounds can be explicitly evaluated and are much less conservative than state of the art results. Furthermore, we show that certain model misspecifications lead to only graceful degradation. We demonstrate these advantages and the usefulness of our results for learning-based
    
[^22]: 用模拟退火机器学习贝叶斯网络

    Learning Bayesian Networks with Annealing Machine. (arXiv:2006.06926v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.06926](http://arxiv.org/abs/2006.06926)

    本文提出了一种用于贝叶斯网络结构学习的模拟退火机器方法，通过先进的候选父节点集合的确定和分解，以及整数规划问题的解决，能够在比特容量有限的情况下高效地解决基于评分的学习问题。

    

    最近的研究表明，模拟退火机器能够高精度地解决组合优化问题。模拟退火机器有潜力用于基于评分的贝叶斯网络结构学习。然而，模拟退火机器的比特容量目前有限。为了利用模拟退火技术，需要将基于评分的学习问题转化为在比特容量内的二次无约束二元优化问题。在本文中，我们提出了一种高效的转化方法，通过先进的候选父节点集合的确定和其分解。我们还提供了一个整数规划问题，以找到最小化所需比特数的分解。在包含变量从75到223的7个基准数据集上的实验结果表明，我们的方法所需的比特数比四代富士通数字退火器（一种采用半导体技术开发的全耦合模拟退火机器）的100K比特容量少。

    Recent studies have reported that annealing machines are capable of solving combinatorial optimization problems with high accuracy. Annealing machines can potentially be applied to score-based Bayesian network structure learning. However, the bit capacity of an annealing machine is currently limited. To utilize the annealing technology, converting score-based learning problems into quadratic unconstrained binary optimizations within the bit capacity is necessary. In this paper, we propose an efficient conversion method with the advanced identification of candidate parent sets and their decomposition. We also provide an integer programming problem to find the decomposition that minimizes the number of required bits. Experimental results on $7$ benchmark datasets with variables from $75$ to $223$ show that our approach requires less bits than the $100$K bit capacity of the fourth-generation Fujitsu Digital Annealer, a fully coupled annealing machine developed with semiconductor technolog
    
[^23]: 判别器最优输运

    Discriminator optimal transport. (arXiv:1910.06832v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1910.06832](http://arxiv.org/abs/1910.06832)

    这篇论文研究了判别器优化过程如何增加生成对抗网络中Wasserstein距离的对偶代价函数的下限，从而使训练好的判别器能够近似最优输运。作者提出了一种判别器最优输运（DOT）方案，通过实验证明了在不同数据集上的生成图像质量有所提升。

    

    在广泛的生成对抗网络中，我们展示了判别器优化过程增加了目标分布$p$和生成器分布$p_G$之间Wasserstein距离的对偶代价函数的下限。这意味着训练好的判别器可以近似从$p_G$到$p$的最优输运。基于一些实验和一点输运理论，我们提出了一种判别器最优输运（DOT）方案来改进生成的图像。我们展示了它在CIFAR-10、STL-10和一个以ImageNet为条件的预训练模型训练的无条件GAN计算的内涵分数和FID上的改进。

    Within a broad class of generative adversarial networks, we show that discriminator optimization process increases a lower bound of the dual cost function for the Wasserstein distance between the target distribution $p$ and the generator distribution $p_G$. It implies that the trained discriminator can approximate optimal transport (OT) from $p_G$ to $p$.Based on some experiments and a bit of OT theory, we propose a discriminator optimal transport (DOT) scheme to improve generated images. We show that it improves inception score and FID calculated by un-conditional GAN trained by CIFAR-10, STL-10 and a public pre-trained model of conditional GAN by ImageNet.
    

