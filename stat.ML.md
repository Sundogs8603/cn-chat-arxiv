# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generalized partitioned local depth.](http://arxiv.org/abs/2303.10167) | 本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。 |
| [^2] | [Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting.](http://arxiv.org/abs/2303.10144) | 提出了一种动态调整更新到数据比率（UTD）的方法，根据小规模的未用于训练的连续收集的经验上检测欠拟合和过拟合。该方法应用于最先进的基于模型的强化学习算法DreamerV2，可以更好地平衡欠拟合和过拟合，并且与广泛的超参数搜索具有竞争力。 |
| [^3] | [A Robustness Analysis of Blind Source Separation.](http://arxiv.org/abs/2303.10104) | 本文提出了通用框架用以量化盲源分离在结构偏差下的鲁棒性能，并在不同类型扰动的情况下给出了明确的连续性保证。 |
| [^4] | [Robust probabilistic inference via a constrained transport metric.](http://arxiv.org/abs/2303.10085) | 本文提出了一种新颖的鲁棒概率推断方法，基于约束传输度量，利用经验似然结合先验分布，用于鲁棒推断问题，实现对中心分布参数的推断，具有卓越的性能表现。 |
| [^5] | [Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices.](http://arxiv.org/abs/2303.10019) | 本文提出一种新的多元概率CRPS学习方法，应用于日前电价预测中，相比于统一组合在CRPS方面取得了显著改进。 |
| [^6] | [Neural-prior stochastic block model.](http://arxiv.org/abs/2303.09995) | 本研究提出了神经先验随机块模型，将社区建模为由节点属性决定。基于置信传递和近似消息传递的结合，可以在处理社交网络、图像分割，生物物种等方面发挥作用。 |
| [^7] | [Finding Competence Regions in Domain Generalization.](http://arxiv.org/abs/2303.09989) | 该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。 |
| [^8] | [On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering.](http://arxiv.org/abs/2303.09877) | 本论文提出了统一的框架DeepMVC，用于深度多视角聚类，并对其进行了关键观察，发现使用对比学习来对齐表示会对聚类可分性产生负面影响，特别是在视角数量较多时。同时，为了克服这种缺陷，我们开发了一些新的基于自监督方法的DeepMVC实例。 |
| [^9] | [Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness.](http://arxiv.org/abs/2303.09863) | 本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。 |
| [^10] | [Error Bounds for Kernel-Based Linear System Identification with Unknown Hyperparameters.](http://arxiv.org/abs/2303.09842) | 本研究提出了一种在超参数未知的情况下可靠地量化估计误差的方法，通过构建真实超参数的高概率集合来获得边界，并在其中找到最坏的后验协方差。该方法在数值模拟中被证明是有效的。 |
| [^11] | [Batch Updating of a Posterior Tree Distribution over a Meta-Tree.](http://arxiv.org/abs/2303.09705) | 本文提出了一个更高效的批量更新方法，用于在元树上计算后验分布。 |
| [^12] | [Unsupervised domain adaptation by learning using privileged information.](http://arxiv.org/abs/2303.09350) | 本文提出利用特权信息进行领域适应（DALUPI）算法，以在学习中放宽假设条件并提高样本效率，通过减少错误来促进医学图像分析等应用的发展。 |
| [^13] | [Bayesian Quadrature for Neural Ensemble Search.](http://arxiv.org/abs/2303.08874) | 本论文介绍了一种使用贝叶斯积分的新方法，可以在架构似然表面有分散、狭窄峰时构建加权集成神经网络，相比当前同类方法，在测试似然性、准确性和期望校准误差方面更为优秀。 |
| [^14] | [Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints.](http://arxiv.org/abs/2303.08068) | 本文提出了一种使用互信息约束下的对比条件变分自编码器进行从未标记数据中提取风格特征的方法，该方法由一个提取风格无关特征的对比学习部分和一个提取风格特征的CVAE部分组成。 |
| [^15] | [Adaptive Interventions for Global Health: A Case Study of Malaria.](http://arxiv.org/abs/2303.02075) | 介绍了如何通过移动健康应用和机器学习自适应干预来加强疟疾监测和治疗依从性，改善医护质量，提高检测率和公共卫生，减少药品短缺和为政策干预提供信息。 |
| [^16] | [Learning time-scales in two-layers neural networks.](http://arxiv.org/abs/2303.00055) | 本文研究了两层神经网络的学习动态，发现经验风险的下降速率是非单调的。在分布符合单指数模型的高维宽两层神经网络中，我们通过学习率参数化清晰的阶段转换，并提供了对网络学习动态的全面分析。我们还为早期学习时所学模型的简单性提供了理论解释。 |
| [^17] | [Reproducing kernel Hilbert spaces in the mean field limit.](http://arxiv.org/abs/2302.14446) | 本文研究了作用于具有许多测量变量的数据的内核方法在均场极限下的行为，并给出了极限再生核希尔伯特空间的详细分析。 |
| [^18] | [Performance is not enough: a story of the Rashomon's quartet.](http://arxiv.org/abs/2302.13356) | 本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能，同时其可视化揭示了极其不同的方法来理解数据中的相关性结构。 |
| [^19] | [Optimizing Orthogonalized Tensor Deflation via Random Tensor Theory.](http://arxiv.org/abs/2302.05798) | 本文提出了一种通过优化参数引入的正交张量缩减机制，以高效地从带噪声的张量中恢复相关低秩信号。 |
| [^20] | [CausalEGM: a general causal inference framework by encoding generative modeling.](http://arxiv.org/abs/2212.05925) | 论文提出了基于生成模型的CausalEGM框架，能够同时进行因果效应的解耦以及将混淆变量映射到低维潜变量空间。 |
| [^21] | [Inadmissibility of the corrected Akaike information criterion.](http://arxiv.org/abs/2211.09326) | 本文研究表明，对于未知协方差的多元线性回归模型，纠正后的赤池信息准则被证明作为库尔巴克-莱布勒差异本身的估计量是不可接受的。提供了改进估计量，并在降低秩的情况下良好工作。 |
| [^22] | [A Non-Asymptotic Framework for Approximate Message Passing in Spiked Models.](http://arxiv.org/abs/2208.03313) | 本文提出了一个非渐进的AMP理论框架，用于理解高维统计问题，解决了以往AMP理论的不足。我们的方法可以有效地预测AMP在独立初始化和谱初始化情况下的有限样本行为。 |
| [^23] | [Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model.](http://arxiv.org/abs/2005.12900) | 本文提出了两个算法——扰动模型驱动算法和保守模型驱动算法，通过生成模型在较小的样本大小下证明了它们的极小化最大算法优化性能。 |

# 详细

[^1]: 广义划分局部深度

    Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])

    [http://arxiv.org/abs/2303.10167](http://arxiv.org/abs/2303.10167)

    本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。

    

    本文提供了一个最近由Berenhaut、Moore和Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]提出的凝聚概念的概括。所提出的表述基于分区局部深度的技术并提炼了两个关键概率概念：局部相关性和支持分割。早期结果在新的背景下得到扩展，并包括在具有不确定性的数据中揭示社区的应用示例。

    In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
    
[^2]: 动态更新数据比率：减少世界模型过拟合

    Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting. (arXiv:2303.10144v1 [cs.LG])

    [http://arxiv.org/abs/2303.10144](http://arxiv.org/abs/2303.10144)

    提出了一种动态调整更新到数据比率（UTD）的方法，根据小规模的未用于训练的连续收集的经验上检测欠拟合和过拟合。该方法应用于最先进的基于模型的强化学习算法DreamerV2，可以更好地平衡欠拟合和过拟合，并且与广泛的超参数搜索具有竞争力。

    

    在监督学习的情境下，基于验证集表现的早期停止是一种流行的方法，可以找到欠拟合和过拟合之间的平衡。然而，在强化学习中，即使在诸如世界模型学习之类的监督子问题中，也不能应用早期停止，因为数据集在不断演变。为此，我们提出了一种新的通用方法，根据在未参与训练的一小部分连续收集的经验上检测欠拟合和过拟合来动态调整训练中的更新到数据比率（UTD）。我们将该方法应用于DreamerV2，这是一种最先进的基于模型的强化学习算法，并在DeepMind Control Suite和Atari $100$k基准测试上进行评估。结果表明，与DreamerV2中的默认设置相比，通过调整UTD比率来平衡欠拟合与过拟合的效果更好，而且具有与广泛超参数搜索竞争的能力。

    Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search 
    
[^3]: 盲源分离的鲁棒性分析

    A Robustness Analysis of Blind Source Separation. (arXiv:2303.10104v1 [math.ST])

    [http://arxiv.org/abs/2303.10104](http://arxiv.org/abs/2303.10104)

    本文提出了通用框架用以量化盲源分离在结构偏差下的鲁棒性能，并在不同类型扰动的情况下给出了明确的连续性保证。

    

    盲源分离（BSS）旨在从其混合物$ X = f（S）$中恢复未观察到的信号$ S $，其条件是作用于变换$ f $是可逆的但未知的。由于这是一个具有许多实际应用的基本问题，关键问题是理解当支持的统计先验假设被违反时，解决此问题的解决方案的行为如何。在线性混合的经典背景下，我们提出了一个通用框架，用于分析此类违规行为，并量化其对从$ X $中盲目恢复$ S $的影响。将$ S $建模为多维随机过程，我们引入了涵盖可能在混合物$ X $中潜在原因的空间的信息拓扑，并表明通用BSS解决方案对其定义结构假设的一般偏差的响应行为，可以获得明确的连续性保证，这样可以灵活方便地量化BSS的鲁棒性能。我们通过展示它如何涵盖已经发表的分析并对不同类型的扰动导出新的连续性保证，从而说明我们的方法的可行性和范围。

    Blind source separation (BSS) aims to recover an unobserved signal $S$ from its mixture $X=f(S)$ under the condition that the effecting transformation $f$ is invertible but unknown. As this is a basic problem with many practical applications, a fundamental issue is to understand how the solutions to this problem behave when their supporting statistical prior assumptions are violated. In the classical context of linear mixtures, we present a general framework for analysing such violations and quantifying their impact on the blind recovery of $S$ from $X$. Modelling $S$ as a multidimensional stochastic process, we introduce an informative topology on the space of possible causes underlying a mixture $X$, and show that the behaviour of a generic BSS-solution in response to general deviations from its defining structural assumptions can be profitably analysed in the form of explicit continuity guarantees with respect to this topology. This allows for a flexible and convenient quantificatio
    
[^4]: 通过约束传输度量实现鲁棒概率推断

    Robust probabilistic inference via a constrained transport metric. (arXiv:2303.10085v1 [stat.ME])

    [http://arxiv.org/abs/2303.10085](http://arxiv.org/abs/2303.10085)

    本文提出了一种新颖的鲁棒概率推断方法，基于约束传输度量，利用经验似然结合先验分布，用于鲁棒推断问题，实现对中心分布参数的推断，具有卓越的性能表现。

    

    弹性贝叶斯模型通常是使用具有大量参数且常常不可解释的参数模型的极限构建而成的。本文提出了一种新颖的方法，通过使用倾斜的经验似然的构造，结合一种新型的Wasserstein度量，集中在特定参数族附近，然后结合模型参数的先验分布，从而得到一个鲁棒的后验分布。该方法在许多鲁棒推断问题中找到应用，我们旨在在存在异常值的情况下对与中心分布相关的参数进行推断。我们提出的传输度量具有很高的计算简便性，利用了离散最优传输问题的Sinkhorn正则化，并本质上可以并行化。我们证明了我们的方法与最先进的方法相比具有卓越的性能。

    Flexible Bayesian models are typically constructed using limits of large parametric models with a multitude of parameters that are often uninterpretable. In this article, we offer a novel alternative by constructing an exponentially tilted empirical likelihood carefully designed to concentrate near a parametric family of distributions of choice with respect to a novel variant of the Wasserstein metric, which is then combined with a prior distribution on model parameters to obtain a robustified posterior. The proposed approach finds applications in a wide variety of robust inference problems, where we intend to perform inference on the parameters associated with the centering distribution in presence of outliers. Our proposed transport metric enjoys great computational simplicity, exploiting the Sinkhorn regularization for discrete optimal transport problems, and being inherently parallelizable. We demonstrate superior performance of our methodology when compared against state-of-the-ar
    
[^5]: 多元概率CRPS学习及其在日前电价预测中的应用

    Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])

    [http://arxiv.org/abs/2303.10019](http://arxiv.org/abs/2303.10019)

    本文提出一种新的多元概率CRPS学习方法，应用于日前电价预测中，相比于统一组合在CRPS方面取得了显著改进。

    

    本文提出了一种考虑分位数和协变量依赖关系的多元概率预测的结合方法，并通过平滑过程允许在线学习。通过维数降低和罚函数平滑等两种平滑方法来将标准CRPS学习框架推广到多元维度中。将该方法应用于预测日前电价，相比于统一组合，在CRPS方面取得了显著改进。

    This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
    
[^6]: 神经先验随机块模型

    Neural-prior stochastic block model. (arXiv:2303.09995v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2303.09995](http://arxiv.org/abs/2303.09995)

    本研究提出了神经先验随机块模型，将社区建模为由节点属性决定。基于置信传递和近似消息传递的结合，可以在处理社交网络、图像分割，生物物种等方面发挥作用。

    

    随机块模型（SBM）被广泛研究作为图聚类或社区检测的基准。在实践中，图数据通常带有节点属性，这些属性承载有关社区的附加信息。以前的研究通过考虑节点属性是由节点社区成员身份生成的来对这些数据进行建模。在本文中，受到使用深度神经网络作为先验的信号处理领域中一系列研究的启发，我们提出将社区建模为由节点属性决定而不是相反。我们定义了相应的模型，并将其称为神经先验SBM。我们提出了一种算法，来自于统计物理学，基于置信传递和近似消息传递的结合。我们分析了算法的性能以及贝叶斯最优性能。我们识别了可检测和精确恢复相变，以及一种算法难区域。所提出的模型和算法可以在处理诸如社交网络、图像分割，生物物种等方面发挥作用。

    The stochastic block model (SBM) is widely studied as a benchmark for graph clustering aka community detection. In practice, graph data often come with node attributes that bear additional information about the communities. Previous works modeled such data by considering that the node attributes are generated from the node community memberships. In this work, motivated by a recent surge of works in signal processing using deep neural networks as priors, we propose to model the communities as being determined by the node attributes rather than the opposite. We define the corresponding model; we call it the neural-prior SBM. We propose an algorithm, stemming from statistical physics, based on a combination of belief propagation and approximate message passing. We analyze the performance of the algorithm as well as the Bayes-optimal performance. We identify detectability and exact recovery phase transitions, as well as an algorithmically hard region. The proposed model and algorithm can b
    
[^7]: 在领域泛化中找到能力区域

    Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])

    [http://arxiv.org/abs/2303.09989](http://arxiv.org/abs/2303.09989)

    该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。

    

    我们提出了一个“学习拒绝”框架来解决领域泛化中默默失败的问题，即测试分布与训练分布不同的情况。假设有一个温和的分布偏移，我们希望在模型估计的能力预示着可信响应时接受超出分布的数据，而不是直接拒绝超出分布的数据。可信度通过与分类器性能密切相关的代理无能分数进行预测。我们对分类的无能得分进行了全面的实验评估，并强调了拒绝率与准确率之间的权衡。为了与先前的工作进行比较，我们聚焦于标准领域泛化基准，并考虑在闭合和开放世界环境下通过不同的学习表示来衡量无能。我们的结果表明，增加无能分数确实预示着降低准确性，从而导致显着的...

    We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
    
[^8]: 关于深度多视角聚类中自监督学习和对比对齐的影响

    On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering. (arXiv:2303.09877v1 [stat.ML])

    [http://arxiv.org/abs/2303.09877](http://arxiv.org/abs/2303.09877)

    本论文提出了统一的框架DeepMVC，用于深度多视角聚类，并对其进行了关键观察，发现使用对比学习来对齐表示会对聚类可分性产生负面影响，特别是在视角数量较多时。同时，为了克服这种缺陷，我们开发了一些新的基于自监督方法的DeepMVC实例。

    

    自监督学习是近年来深度多视角聚类方法中的核心组成部分。然而，我们发现基于自监督学习的深度多视角聚类方法在发展方面存在巨大差异，这可能会拖慢该领域的进展。为了解决这一问题，我们提出了一个统一的框架DeepMVC，用于深度多视角聚类，其中包括许多最近的方法。我们利用我们的框架对自监督学习的影响进行关键观察，特别是对使用对比学习来对齐表示的缺点的观察。此外，我们证明了对比对齐可能会对聚类可分性产生负面影响，并且当视角的数量增加时，这种影响会变得更加严重。受到我们的发现的启示，我们开发了几种新的DeepMVC实例，具有新形式的自监督学习。我们进行了广泛的实验，并发现（i）与我们的理论发现一致，对比对齐会降低具有许多视角的数据集的性能；（ii）所有方法都受益于一定程度的自监督学习。

    Self-supervised learning is a central component in recent approaches to deep multi-view clustering (MVC). However, we find large variations in the development of self-supervision-based methods for deep MVC, potentially slowing the progress of the field. To address this, we present DeepMVC, a unified framework for deep MVC that includes many recent methods as instances. We leverage our framework to make key observations about the effect of self-supervision, and in particular, drawbacks of aligning representations with contrastive learning. Further, we prove that contrastive alignment can negatively influence cluster separability, and that this effect becomes worse when the number of views increases. Motivated by our findings, we develop several new DeepMVC instances with new forms of self-supervision. We conduct extensive experiments and find that (i) in line with our theoretical findings, contrastive alignments decreases performance on datasets with many views; (ii) all methods benefit
    
[^9]: 通过图表自编码器进行内部数据结构的深度非参数估计：广义误差和鲁棒性。

    Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])

    [http://arxiv.org/abs/2303.09863](http://arxiv.org/abs/2303.09863)

    本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。

    

    自编码器在学习高维数据的低维潜在特征方面已经在各种应用中展现出了显着的成功。假设数据在低维流形附近采样，我们采用图表自编码器，将数据编码为一组图表上的低维潜在特征，从而保留了数据流形的拓扑和几何。我们的论文为图表自编码器的广义误差建立了统计保证，并且通过考虑$d$维流形上$n$个带噪声训练样本及其无噪声对应物来展示它们的去噪能力。通过训练自编码器，我们展示了图表自编码器能够有效地去噪输入数据和正态分布噪声。我们证明，在适当的网络架构下，图表自编码器实现了一个大致为$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$阶的平方广义误差，该误差取决于流形的内在维度，并且仅弱依赖于样本数量$n$。

    Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
    
[^10]: 未知超参数的基于核的线性系统辨识的误差界限

    Error Bounds for Kernel-Based Linear System Identification with Unknown Hyperparameters. (arXiv:2303.09842v1 [eess.SY])

    [http://arxiv.org/abs/2303.09842](http://arxiv.org/abs/2303.09842)

    本研究提出了一种在超参数未知的情况下可靠地量化估计误差的方法，通过构建真实超参数的高概率集合来获得边界，并在其中找到最坏的后验协方差。该方法在数值模拟中被证明是有效的。

    

    基于核的方法已经成功地应用于使用稳定核设计的线性系统辨识中。从高斯过程的角度来看，它可以通过后验协方差为识别的模型自动提供概率误差界限，这在鲁棒和随机控制中非常有用。然而，这些误差界限需要内核设计中真实超参数的知识，并且在估计的超参数不准确或在存在高噪声的情况下被证明是不准确的。在这项工作中，我们提供了在超参数未知的情况下可靠地量化估计误差的方法。这些边界是通过首先从边际似然函数构建真实超参数的高概率集合，然后在该集合内找到最坏情况下的后验协方差而获得的。所提出的边界被证明具有高概率包含真实模型，并且在数值模拟中验证了其有效性。

    The kernel-based method has been successfully applied in linear system identification using stable kernel designs. From a Gaussian process perspective, it automatically provides probabilistic error bounds for the identified models from the posterior covariance, which are useful in robust and stochastic control. However, the error bounds require knowledge of the true hyperparameters in the kernel design and are demonstrated to be inaccurate with estimated hyperparameters for lightly damped systems or in the presence of high noise. In this work, we provide reliable quantification of the estimation error when the hyperparameters are unknown. The bounds are obtained by first constructing a high-probability set for the true hyperparameters from the marginal likelihood function and then finding the worst-case posterior covariance within the set. The proposed bound is proven to contain the true model with a high probability and its validity is verified in numerical simulation.
    
[^11]: 在元树上批量更新后验树分布。

    Batch Updating of a Posterior Tree Distribution over a Meta-Tree. (arXiv:2303.09705v1 [cs.LG])

    [http://arxiv.org/abs/2303.09705](http://arxiv.org/abs/2303.09705)

    本文提出了一个更高效的批量更新方法，用于在元树上计算后验分布。

    

    以前，我们提出了一个由不可观察的树和一个序列更新方法表示的概率数据生成模型，用于计算一组树上的后验分布。该集合称为元树。在本文中，我们提出了一种更高效的批量更新方法。

    Previously, we proposed a probabilistic data generation model represented by an unobservable tree and a sequential updating method to calculate a posterior distribution over a set of trees. The set is called a meta-tree. In this paper, we propose a more efficient batch updating method.
    
[^12]: 利用特权信息进行无监督领域自适应

    Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])

    [http://arxiv.org/abs/2303.09350](http://arxiv.org/abs/2303.09350)

    本文提出利用特权信息进行领域适应（DALUPI）算法，以在学习中放宽假设条件并提高样本效率，通过减少错误来促进医学图像分析等应用的发展。

    

    成功的无监督领域自适应（UDA）只在强假设条件下得以实现，如协变量移位和输入领域之间的重叠。后者在高维应用中经常被违反，比如图像分类，在面对这种挑战时，图像分类仍然是算法开发的灵感和基准。本文表明，获取源域和目标域样本的有关信息能够帮助放宽这些假设，并在学习中提高样本效率，代价是收集更丰富的变量集。我们称之为利用特权信息进行领域适应（DALUPI）。为此，我们提出了一个简单的两阶段学习算法，并提出了一个针对多标签图像分类的实用端到端算法，受到我们分析的启发。通过一系列实验，包括医学图像分析的应用，我们证明了在学习过程中加入特权信息可以减少错误。

    Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
    
[^13]: 基于贝叶斯积分的神经网络集成搜索

    Bayesian Quadrature for Neural Ensemble Search. (arXiv:2303.08874v1 [stat.ML])

    [http://arxiv.org/abs/2303.08874](http://arxiv.org/abs/2303.08874)

    本论文介绍了一种使用贝叶斯积分的新方法，可以在架构似然表面有分散、狭窄峰时构建加权集成神经网络，相比当前同类方法，在测试似然性、准确性和期望校准误差方面更为优秀。

    

    集成方法可以提高神经网络的性能，但现有方法在架构似然表面有分散、狭窄峰时效果不佳。此外，现有方法构建均等加权的集成，这可能容易受到较弱架构的失效模式的影响。通过将集成视为近似边缘化架构，我们使用贝叶斯积分的工具构建集成方法——这些工具非常适合探索架构似然表面有分散、狭窄峰的情况。此外，由此产生的集成由体现其性能的架构加权权重组成。我们通过实证研究——在测试似然性、准确性和期望校准误差方面——表明我们的方法优于现有的基线，并通过削减研究验证其各成分的独立性能。

    Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.
    
[^14]: 使用互信息约束下的对比条件变分自编码器进行风格特征提取

    Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints. (arXiv:2303.08068v1 [cs.CV])

    [http://arxiv.org/abs/2303.08068](http://arxiv.org/abs/2303.08068)

    本文提出了一种使用互信息约束下的对比条件变分自编码器进行从未标记数据中提取风格特征的方法，该方法由一个提取风格无关特征的对比学习部分和一个提取风格特征的CVAE部分组成。

    

    在数据分析中，从未标记的数据中提取细粒度特征（如风格）非常重要。无监督方法（如变分自编码器（VAEs））可以提取风格，但提取的风格通常与其他特征混合。我们可以使用分类标签来指导VAEs提取风格，即条件VAEs（CVAEs）。但是，使用未标记数据仅提取风格的方法尚未建立。在本文中，我们构建了一种基于CVAE的方法，使用仅未标记的数据来提取风格特征。所提出的模型大致由两个并行部分组成; 提取风格无关特征的对比学习（CL）部分，以及提取风格特征的CVAE部分。CL模型通常以无需数据扩充的自监督方式学习与样式无关的表示，可以视为样式中的扰动。以提取的风格无关特征为条件，CVAE学习仅提取风格。在训练过程中，先训练CL模型，然后使用训练过的CL模型指导CVAE的训练。在几个数据集上评估了所提出的方法，实验结果表明所提出的方法可以有效地从未标记的数据中提取风格特征。

    It is crucial to extract fine-grained features such as styles from unlabeled data in data analysis. Unsupervised methods, such as variational autoencoders (VAEs), can extract styles, but the extracted styles are usually mixed with other features. We can isolate the styles using VAEs conditioned by class labels, known as conditional VAEs (CVAEs). However, methods to extract only styles using unlabeled data are not established. In this paper, we construct a CVAE-based method that extracts style features using only unlabeled data. The proposed model roughly consists of two parallel parts; a contrastive learning (CL) part that extracts style-independent features and a CVAE part that extracts style features. CL models generally learn representations independent of data augmentation, which can be seen as a perturbation in styles, in a self-supervised way. Taking the style-independent features as a condition, the CVAE learns to extract only styles. In the training procedure, a CL model is tra
    
[^15]: 全球健康的自适应干预：以疟疾为例的案例研究

    Adaptive Interventions for Global Health: A Case Study of Malaria. (arXiv:2303.02075v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02075](http://arxiv.org/abs/2303.02075)

    介绍了如何通过移动健康应用和机器学习自适应干预来加强疟疾监测和治疗依从性，改善医护质量，提高检测率和公共卫生，减少药品短缺和为政策干预提供信息。

    

    疟疾是可以预防、诊断和治疗的疾病；但每年仍有超过两亿个病例和两万个可预防死亡。尤其在撒哈拉以南非洲的低收入和中等收入国家，疟疾仍然是一个紧迫的公共卫生问题。我们通过移动健康应用、基于机器学习的自适应干预，可以加强疟疾监测和治疗的依从性，增加检测，衡量提供者的技能和护理质量，通过支持一线工作人员和患者（如容量建设和鼓励行为变化，如使用蚊帐）改善公共卫生，减少药店和诊所的测试库存短缺并为政策干预提供信息。

    Malaria can be prevented, diagnosed, and treated; however, every year, there are more than 200 million cases and 200.000 preventable deaths. Malaria remains a pressing public health concern in low- and middle-income countries, especially in sub-Saharan Africa. We describe how by means of mobile health applications, machine-learning-based adaptive interventions can strengthen malaria surveillance and treatment adherence, increase testing, measure provider skills and quality of care, improve public health by supporting front-line workers and patients (e.g., by capacity building and encouraging behavioral changes, like using bed nets), reduce test stockouts in pharmacies and clinics and informing public health for policy intervention.
    
[^16]: 两层神经网络中学习时间尺度的研究

    Learning time-scales in two-layers neural networks. (arXiv:2303.00055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00055](http://arxiv.org/abs/2303.00055)

    本文研究了两层神经网络的学习动态，发现经验风险的下降速率是非单调的。在分布符合单指数模型的高维宽两层神经网络中，我们通过学习率参数化清晰的阶段转换，并提供了对网络学习动态的全面分析。我们还为早期学习时所学模型的简单性提供了理论解释。

    

    多层神经网络的梯度下降学习具有多个引人注意的特点。尤其是，在大批量数据平均后，经验风险的下降速率是非单调的。几乎没有进展的长周期和快速下降的间隔交替出现。这些连续的学习阶段往往在非常不同的时间尺度上进行。最后，在早期阶段学习的模型通常是“简单的”或“易于学习的”，尽管以难以形式化的方式。本文研究了分布符合单指数模型的高维宽两层神经网络的梯度流动力学，在一系列新的严密结果、非严密数学推导和数值实验的基础上，提供了对网络学习动态的全面分析。我们特别指出，我们通过学习率参数化清晰的阶段转换，并展示了它们与长周期的出现和消失有关。我们还为早期学习时所学模型的简单性提供了理论解释，并证明它们可以用于规范训练过程。

    Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically `simpler' or `easier to learn' although in a way that is difficult to formalize.  Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numer
    
[^17]: 均场极限中的再生核希尔伯特空间

    Reproducing kernel Hilbert spaces in the mean field limit. (arXiv:2302.14446v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.14446](http://arxiv.org/abs/2302.14446)

    本文研究了作用于具有许多测量变量的数据的内核方法在均场极限下的行为，并给出了极限再生核希尔伯特空间的详细分析。

    

    内核方法是机器学习中最受欢迎和成功的技术之一，它们有一个成熟的理论和高效的算法支持。从数学角度来看，这些方法基于内核的概念和内核生成的函数空间，即再生核希尔伯特空间。受相互作用粒子系统学习方法最近的发展的推动，我们研究了作用于具有许多测量变量的数据的内核方法。我们展示了内核的严格均场极限，并提供了极限再生核希尔伯特空间的详细分析。此外，我们还提出了几个内核的示例，这些内核允许严格的均场极限。

    Kernel methods, being supported by a well-developed theory and coming with efficient algorithms, are among the most popular and successful machine learning techniques. From a mathematical point of view, these methods rest on the concept of kernels and function spaces generated by kernels, so called reproducing kernel Hilbert spaces. Motivated by recent developments of learning approaches in the context of interacting particle systems, we investigate kernel methods acting on data with many measurement variables. We show the rigorous mean field limit of kernels and provide a detailed analysis of the limiting reproducing kernel Hilbert space. Furthermore, several examples of kernels, that allow a rigorous mean field limit, are presented.
    
[^18]: 表现不足以为盈，深究Rashomon的四重奏

    Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.13356](http://arxiv.org/abs/2302.13356)

    本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能，同时其可视化揭示了极其不同的方法来理解数据中的相关性结构。

    

    预测建模通常被简化为寻找最优模型来优化选定的性能度量。但如果第二优模型能够以完全不同的方式同样描述数据呢？第三个模型呢？最有效的模型会学到完全不同的数据关系吗？受到Anscombe四重奏的启发，本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能。然而，它们的可视化揭示了极其不同的方法来理解数据中的相关性结构。引入的简单示例旨在进一步促进可视化作为比较预测模型超越性能的必要工具。我们需要开发富有洞察力的技术来解释模型集。

    Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
    
[^19]: 随机张量理论优化正交张量缩减

    Optimizing Orthogonalized Tensor Deflation via Random Tensor Theory. (arXiv:2302.05798v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05798](http://arxiv.org/abs/2302.05798)

    本文提出了一种通过优化参数引入的正交张量缩减机制，以高效地从带噪声的张量中恢复相关低秩信号。

    

    本文研究了如何从随机噪声张量中恢复可能存在相关部分的低秩信号张量问题。当底层部分是正交的时，可以通过张量缩减（由一系列秩为一的逼近组成）高效地进行恢复，而非正交的部分可能会影响张量缩减机制，从而导致恢复效率低下。基于最近发展的随机张量工具，本文通过对一个三阶、秩为二的尖峰张量的参数化缩减过程进行渐近分析，精确地处理了非正交情况。基于该分析，提出了一种通过优化缩减机制中引入的参数来进行高效张量缩减的算法，并且根据构造证明在所研究的张量模型下最优。相同的思想也可以扩展到更一般的低秩张量模型，例如更高阶和其他类型的约束条件。

    This paper tackles the problem of recovering a low-rank signal tensor with possibly correlated components from a random noisy tensor, or so-called spiked tensor model. When the underlying components are orthogonal, they can be recovered efficiently using tensor deflation which consists of successive rank-one approximations, while non-orthogonal components may alter the tensor deflation mechanism, thereby preventing efficient recovery. Relying on recently developed random tensor tools, this paper deals precisely with the non-orthogonal case by deriving an asymptotic analysis of a parameterized deflation procedure performed on an order-three and rank-two spiked tensor. Based on this analysis, an efficient tensor deflation algorithm is proposed by optimizing the parameter introduced in the deflation mechanism, which in turn is proven to be optimal by construction for the studied tensor model. The same ideas could be extended to more general low-rank tensor models, e.g., higher ranks and o
    
[^20]: CausalEGM: 基于生成模型的一般性因果推断框架

    CausalEGM: a general causal inference framework by encoding generative modeling. (arXiv:2212.05925v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.05925](http://arxiv.org/abs/2212.05925)

    论文提出了基于生成模型的CausalEGM框架，能够同时进行因果效应的解耦以及将混淆变量映射到低维潜变量空间。

    

    尽管理解和表征因果效应已经成为观察研究中至关重要的问题，但当混淆变量具有高维性时，这种问题很具挑战性。本文开发了一个用于通过编码生成建模估计因果效应的通用框架"CausalEGM"，可应用于二元和连续的治疗设置。在潜在结果框架下，我们建立了高维混淆变量空间和已知密度（例如多元正态分布）的低维潜变量空间之间的双向转换。通过这种方式，CausalEGM同时将混淆变量对治疗和结果的依赖关系进行解耦，并映射混淆变量到低维潜变量空间。通过对低维潜特征的调节，CausalEGM可以估计每个个体的因果效应或人群中的平均因果效应。我们的理论分析表明：

    Although understanding and characterizing causal effects have become essential in observational studies, it is challenging when the confounders are high-dimensional. In this article, we develop a general framework $\textit{CausalEGM}$ for estimating causal effects by encoding generative modeling, which can be applied in both binary and continuous treatment settings. Under the potential outcome framework with unconfoundedness, we establish a bidirectional transformation between the high-dimensional confounders space and a low-dimensional latent space where the density is known (e.g., multivariate normal distribution). Through this, CausalEGM simultaneously decouples the dependencies of confounders on both treatment and outcome and maps the confounders to the low-dimensional latent space. By conditioning on the low-dimensional latent features, CausalEGM can estimate the causal effect for each individual or the average causal effect within a population. Our theoretical analysis shows that
    
[^21]: 纠正后的赤池信息准则的不可允许性研究

    Inadmissibility of the corrected Akaike information criterion. (arXiv:2211.09326v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.09326](http://arxiv.org/abs/2211.09326)

    本文研究表明，对于未知协方差的多元线性回归模型，纠正后的赤池信息准则被证明作为库尔巴克-莱布勒差异本身的估计量是不可接受的。提供了改进估计量，并在降低秩的情况下良好工作。

    

    对于未知协方差的多元线性回归模型，纠正后的赤池信息准则是期望库尔巴克-莱布勒差异最小方差无偏估计。在这项研究中，基于损失估计框架，我们表明它作为库尔巴克-莱布勒差异本身的估计量是不可接受的，而不是期望的库尔巴克-莱布勒差异。我们提供了在降低秩的情况下良好工作的库尔巴克-莱布勒差异的改进估计量，并通过数值实验检验了它们的性能。

    For the multivariate linear regression model with unknown covariance, the corrected Akaike information criterion is the minimum variance unbiased estimator of the expected Kullback--Leibler discrepancy. In this study, based on the loss estimation framework, we show its inadmissibility as an estimator of the Kullback--Leibler discrepancy itself, instead of the expected Kullback--Leibler discrepancy. We provide improved estimators of the Kullback--Leibler discrepancy that work well in reduced-rank situations and examine their performance numerically.
    
[^22]: 不对称模型中近似信息传递的非渐近框架

    A Non-Asymptotic Framework for Approximate Message Passing in Spiked Models. (arXiv:2208.03313v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2208.03313](http://arxiv.org/abs/2208.03313)

    本文提出了一个非渐进的AMP理论框架，用于理解高维统计问题，解决了以往AMP理论的不足。我们的方法可以有效地预测AMP在独立初始化和谱初始化情况下的有限样本行为。

    

    近似信息传递（AMP）作为一种高效的迭代范式，已经被广泛应用于高维统计问题的求解中。然而，以往AMP理论主要集中在高维渐近性方面，未能预测当迭代次数超过$o\big(\frac{\log n}{\log\log n}\big)$（其中$n$是问题的维度）时AMP的动态。为了解决这个问题，本文建立了一个非渐近的框架，用于理解锥形矩阵估计中的AMP。我们基于新的AMP更新分解和可控残差项，提出了一种分析方法，以表征独立初始化情况下AMP的有限样本行为，并进一步推广到包括谱初始化的情况。作为这种分析方法的两个具体应用，当解决$\mathbb{Z}_2$同步问题时，我们预测了谱初始化AMP的行为，最多可以进行$O\big(\frac{n}{\mathrm{poly}\log n}\big)$个迭代。

    Approximate message passing (AMP) emerges as an effective iterative paradigm for solving high-dimensional statistical problems. However, prior AMP theory -which focused mostly on high-dimensional asymptotics -- fell short of predicting the AMP dynamics when the number of iterations surpasses $o\big(\frac{\log n}{\log\log n}\big)$ (with $n$ the problem dimension). To address this inadequacy, this paper develops a non-asymptotic framework for understanding AMP in spiked matrix estimation. Built upon new decomposition of AMP updates and controllable residual terms, we lay out an analysis recipe to characterize the finite-sample behavior of AMP in the presence of an independent initialization, which is further generalized to allow for spectral initialization. As two concrete consequences of the proposed analysis recipe: (i) when solving $\mathbb{Z}_2$ synchronization, we predict the behavior of spectrally initialized AMP for up to $O\big(\frac{n}{\mathrm{poly}\log n}\big)$ iterations, sh
    
[^23]: 用生成模型突破模型驱动强化学习中的样本大小障碍

    Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model. (arXiv:2005.12900v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.12900](http://arxiv.org/abs/2005.12900)

    本文提出了两个算法——扰动模型驱动算法和保守模型驱动算法，通过生成模型在较小的样本大小下证明了它们的极小化最大算法优化性能。

    

    本论文着眼于在有生成模型（或模拟器）的情况下，增强学习的样本效率。首先，考虑带有折扣的无限时间步长马尔科夫决策过程（MDP），其状态空间为$\mathcal{S}$，动作空间为$\mathcal{A}$。尽管有许多先前的研究在解决这个问题，但是在样本复杂度和统计精度之间权衡的完整图景尚未确定。特别是，所有的先前结果都受到严重的样本大小障碍，因为它们声称的统计保证仅在样本大小超过至少$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$时才成立。本文通过证明两个算法——扰动模型驱动算法和保守模型驱动算法——在样本大小超过$\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$的情况下就能证明它们的极小化最大算法优化性能（几乎符合一些对数因子）。除了无限时间步长M解决方案之外，我们还考虑了有限样本和近似价值迭代问题，以在实践中实现算法的应用。

    This paper is concerned with the sample efficiency of reinforcement learning, assuming access to a generative model (or simulator). We first consider $\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with state space $\mathcal{S}$ and action space $\mathcal{A}$. Despite a number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, all prior results suffer from a severe sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$. The current paper overcomes this barrier by certifying the minimax optimality of two algorithms -- a perturbed model-based algorithm and a conservative model-based algorithm -- as soon as the sample size exceeds the order of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). Moving beyond infinite-
    

