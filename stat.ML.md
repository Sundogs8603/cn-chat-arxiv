# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion](https://arxiv.org/abs/2403.12950) | 该研究建立了首个在对抗性多臂老虎机中优化且自适应的波达动态遗憾上界，揭示了在Condorcet和Borda之间严重非平稳性可学习性的基本差异 |
| [^2] | [On Safety in Safe Bayesian Optimization](https://arxiv.org/abs/2403.12948) | 本研究探讨了安全贝叶斯优化中的安全性问题，并提出了Real-\b{eta}-SafeOpt算法，有效保留了所有理论保证。 |
| [^3] | [Clustered Mallows Model](https://arxiv.org/abs/2403.12880) | 扩展了著名的Mallows模型以适应项目的无差异处理，解决了严格偏好不切实际的问题。 |
| [^4] | [Primal Methods for Variational Inequality Problems with Functional Constraints](https://arxiv.org/abs/2403.12859) | 本文提出了一种简单的原始方法，称为约束梯度方法（CGM），用于解决具有多个功能约束的变分不等式问题。 |
| [^5] | [Tighter Confidence Bounds for Sequential Kernel Regression](https://arxiv.org/abs/2403.12732) | 通过使用鞅尾巴界限和无限维凸规划的有限维重构，建立了序贯核回归的新置信区间，证明其始终比现有的置信区间更紧凑，并将其应用于核赌博问题，提高了算法的性能表现。 |
| [^6] | [Posterior Uncertainty Quantification in Neural Networks using Data Augmentation](https://arxiv.org/abs/2403.12729) | 通过提出MixupMP方法，使用数据增强构建更现实的预测分布，从而解决了在神经网络中对后验不确定性进行量化时的基本模型类错误规范化问题。 |
| [^7] | [Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine](https://arxiv.org/abs/2403.12672) | 该研究提出了一种基于累积分布改进评分可解释性的度量，建立了使用可解释度量设置阈值的准则，并通过数值实验证明其合理性。 |
| [^8] | [A Practical Guide to Statistical Distances for Evaluating Generative Models in Science](https://arxiv.org/abs/2403.12636) | 本文提供了一种实用指南，介绍了四种常用统计距离的概念，以帮助评估生成模型，无需高深的数学和统计知识。 |
| [^9] | [Equity through Access: A Case for Small-scale Deep Learning](https://arxiv.org/abs/2403.12562) | 通过引入PePR分数，研究人员展示了在资源有限的情况下，利用131种独特的DL架构在医学图像任务中的可行性。 |
| [^10] | [Community detection by spectral methods in multi-layer networks](https://arxiv.org/abs/2403.12540) | 改进的谱聚类算法在多层网络中实现了更好的社区检测性能，并证明多层网络对社区检测有优势。 |
| [^11] | [Non-negative Contrastive Learning](https://arxiv.org/abs/2403.12459) | 非负对比学习(NCL)是对非负矩阵分解(NMF)的重新演绎，通过对特征施加非负约束来获得可解释的特征，保留了NMF的可解释属性，从而得到比标准对比学习(CL)更稀疏和解耦的表示 |
| [^12] | [Do Generated Data Always Help Contrastive Learning?](https://arxiv.org/abs/2403.12448) | 生成的高质量图像已成功应用于增强对比表示学习，但我们发现有时生成的数据甚至会对对比学习造成伤害，通过研究发现更强的数据膨胀应该伴随着更弱的增强。 |
| [^13] | [Transfer in Sequential Multi-armed Bandits via Reward Samples](https://arxiv.org/abs/2403.12428) | 提出了一种基于奖励样本转移的算法，在顺序多臂老虎机问题中显著改进了累积遗憾性能 |
| [^14] | [Semisupervised score based matching algorithm to evaluate the effect of public health interventions](https://arxiv.org/abs/2403.12367) | 提出了一种基于二次评分函数的一对一匹配算法，通过设计权重最小化配对训练单元之间的得分差异，同时最大化未配对训练单元之间的得分差异 |
| [^15] | [An Alternative Graphical Lasso Algorithm for Precision Matrices](https://arxiv.org/abs/2403.12357) | 通过重新参数化精准矩阵最后一列，将正则化正态对数似然分解为两个易于最小化的凸函数，其中一个是Lasso回归问题，从而开发了一种简单的块坐标下降算法来计算GLasso更新，性能可与DP-GLas相媲美。 |
| [^16] | [Stochastic Halpern iteration in normed spaces and applications to reinforcement learning](https://arxiv.org/abs/2403.12338) | 该论文分析了随机Halpern迭代在赋范空间中的Oracle复杂度，提出了改进的算法复杂度，进而在强化学习中提出了新的同步算法应用。 |
| [^17] | [FedFisher: Leveraging Fisher Information for One-Shot Federated Learning](https://arxiv.org/abs/2403.12329) | 该论文提出了FedFisher算法，通过利用Fisher信息矩阵进行一次性联邦学习，能够在一次通信中训练出全局模型，并在理论上证明了随着神经网络宽度和客户端本地训练量的增加，FedFisher全局模型的误差会变得非常小。 |
| [^18] | [Selecting informative conformal prediction sets with false coverage rate control](https://arxiv.org/abs/2403.12295) | 提出了一种新的统一框架，用于构建信息丰富的符合预测集，同时控制所选样本的虚警覆盖率。 |
| [^19] | [Private graphon estimation via sum-of-squares](https://arxiv.org/abs/2403.12213) | 基于和平方法的私有图估计算法首次实现了学习随机块模型和图估计的纯节点差分隐私算法，具有多项式运行时间，与之前最佳的信息论节点私有机制具有相匹配的统计效用保证。 |
| [^20] | [Approximation of RKHS Functionals by Neural Networks](https://arxiv.org/abs/2403.12187) | 本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型，并建立了逼近的普适性，推导了逆多重二次、高斯和Sobolev核引起的误差界限，证明神经网络可以准确逼近广义函数线性模型中的回归映射。 |
| [^21] | [The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection](https://arxiv.org/abs/2403.12166) | 提出一种利用核心子集选择进行数据重新加权的方法，有效优化了计算时间和模型性能，突显其作为模型训练的可扩展和精确解决方案的潜力。 |
| [^22] | [Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models](https://arxiv.org/abs/2403.12158) | 引入变分方法在Dirichlet混合模型中提供了封闭形式的解，显著提高了计算效率，可用于快速模型比较和稳健估计评估。 |
| [^23] | [Graph Neural Networks for Learning Equivariant Representations of Neural Networks](https://arxiv.org/abs/2403.12143) | 本研究提出了将神经网络表示为参数的计算图的方法，利用图神经网络和变压器来实现置换对称性，使得单个模型能够处理具有多种架构的神经计算图。 |
| [^24] | [DTOR: Decision Tree Outlier Regressor to explain anomalies](https://arxiv.org/abs/2403.10903) | DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。 |
| [^25] | [How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance](https://arxiv.org/abs/2403.07310) | 本文通过高斯混合模型量化了群体不平衡对样本复杂性、收敛速率和平均以及群体级测试性能的影响，首次提供了ERM在群体级泛化的理论分析。 |
| [^26] | [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046) | 证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。 |
| [^27] | [Tuning-Free Stochastic Optimization](https://arxiv.org/abs/2402.07793) | 本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。 |
| [^28] | [Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects](https://arxiv.org/abs/2402.04906) | 本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。 |
| [^29] | [GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models](https://arxiv.org/abs/2312.03675) | GeoShapley是一种衡量机器学习模型中空间效应的博弈论方法，将位置视为模型预测博弈中的一名玩家，能够量化位置的重要性并与其他特征之间的协同作用进行量化。 |
| [^30] | [Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data](https://arxiv.org/abs/2311.04829) | 提出了一种名为功能贝叶斯 Tucker 分解（FunBaT）的方法，用于将 Tucker 分解推广到连续索引的张量数据，利用高斯过程模型潜在函数。 |
| [^31] | [One-Shot Strategic Classification Under Unknown Costs](https://arxiv.org/abs/2311.02761) | 本研究首次研究了在未知响应下一次性策略分类的情景，针对用户成本函数不确定性，提出解决方案并将任务定义为极小-极大问题。 |
| [^32] | [Oracle-Efficient Smoothed Online Learning for Piecewise Continuous Decision Making](https://arxiv.org/abs/2302.05430) | 引入广义括号数的概念，结合对手的约束与空间大小，通过Follow-the-Perturbed-Leader算法实现低遗憾，优化调用优化Oracle的次数以实现遗憾在多个问题中的有效应用。 |
| [^33] | [Smoothed Online Learning for Prediction in Piecewise Affine Systems](https://arxiv.org/abs/2301.11187) | 本文提出了基于平滑在线学习框架的算法，可以有效处理分段仿射系统中的预测和模拟问题，在弱光滑性假设下具有多项式遗憾度，并且在调用优化预测次数方面是高效的。 |
| [^34] | [Mean-field neural networks-based algorithms for McKean-Vlasov control problems *](https://arxiv.org/abs/2212.11518) | 该论文利用均场神经网络类解决McKean-Vlasov控制问题，提出了多种算法，并在不同示例上展示了数值结果，讨论比较了各种方法的优缺点。 |
| [^35] | [Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach](https://arxiv.org/abs/2210.12624) | 提出了一种随机多目标梯度校正（MoCo）方法，能够在不增加批量大小的情况下保证收敛，解决了多目标学习中梯度偏差导致性能下降的问题。 |
| [^36] | [Bayesian Nonparametrics meets Data-Driven Robust Optimization.](http://arxiv.org/abs/2401.15771) | 本文提出了一种将贝叶斯非参数方法与最新的决策理论模型相结合的鲁棒优化准则，通过这种方法，可以在线性回归问题中获得有稳定性和优越性能的结果。 |
| [^37] | [Probabilistic Modeling for Sequences of Sets in Continuous-Time.](http://arxiv.org/abs/2312.15045) | 本文提出了一个通用的连续时间序列集合的概率建模框架，适用于处理每个事件与一组项目相关联的情况。引入了适用于任何强度为基础的递归神经点过程模型的推理方法，可用于回答关于序列历史条件下的概率查询问题。 |
| [^38] | [MCRAGE: Synthetic Healthcare Data for Fairness.](http://arxiv.org/abs/2310.18430) | MCRAGE是一种使用深度生成模型来增强不平衡的医疗数据集的方法，以解决少数群体在机器学习模型中的不公平问题。 |
| [^39] | [Warped geometric information on the optimisation of Euclidean functions.](http://arxiv.org/abs/2308.08305) | 使用扭曲几何学的概念，我们提出了一种在高维欧几里德空间中优化函数的方法，并通过在重新定义的黎曼流形上进行计算，找到了函数的最优解。 |
| [^40] | [Adversarial Training Should Be Cast as a Non-Zero-Sum Game.](http://arxiv.org/abs/2306.11035) | 本论文提出了一种新的针对对抗性训练的非零和双层公式，实现了与最先进攻击相匹配并且能够达到与标准对抗性训练相同的鲁棒性水平。 |
| [^41] | [Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise.](http://arxiv.org/abs/2306.00673) | 本文提出一种新算法，可以属性高效的学习低次多项式阈值函数，并能够在噪声下进行PAC学习。 |
| [^42] | [On student-teacher deviations in distillation: does it pay to disobey?.](http://arxiv.org/abs/2301.12923) | 通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。 |

# 详细

[^1]: 优化的自适应非平稳对抗性多臂老虎机在广义波达准则下

    Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion

    [https://arxiv.org/abs/2403.12950](https://arxiv.org/abs/2403.12950)

    该研究建立了首个在对抗性多臂老虎机中优化且自适应的波达动态遗憾上界，揭示了在Condorcet和Borda之间严重非平稳性可学习性的基本差异

    

    在对抗性多臂老虎机中，学习者接收臂之间的偏好反馈，并将某个臂的遗憾定义为其相对于优胜臂的次优性。更具挑战性和实践动机的非平稳对抗性多臂老虎机变体，在这种变体中，偏好随时间变化，已经成为近期多项工作的焦点。目标是设计出算法，而无需提前了解变化量。已知结果的大部分研究了孔多塞优胜者设置，其中优先于其他任何臂的臂在任何时候都存在。然而，这样的优胜者可能并不存在，为了对比，此问题的波达版本（始终有明确定义）却受到了很少关注。在这项工作中，我们建立了第一个最优和自适应的波达动态遗憾上界，突显了在孔多塞和波达之间的严重非平稳性可学习性的基本差异。

    arXiv:2403.12950v1 Announce Type: new  Abstract: In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorce
    
[^2]: 关于安全的安全贝叶斯优化

    On Safety in Safe Bayesian Optimization

    [https://arxiv.org/abs/2403.12948](https://arxiv.org/abs/2403.12948)

    本研究探讨了安全贝叶斯优化中的安全性问题，并提出了Real-\b{eta}-SafeOpt算法，有效保留了所有理论保证。

    

    优化未知函数在安全约束下是机器人学、生物医学工程和许多其他学科中的中心任务，安全贝叶斯优化(Safe Bayesian Optimization, BO)在这方面被越来越广泛地应用。由于这些应用的安全关键性质，保证这些算法的理论安全性能能够映射到现实世界中变得至关重要。在这项工作中，我们研究了流行类别SafeOpt类型算法的三个与安全相关的问题。首先，这些算法关键依赖于高斯过程 (Gaussian Process, GP) 回归的频率不确定性界限，但具体实现通常使用使所有安全保证无效的启发式方法。我们对这个问题进行了详细分析，并引入了Real-\b{eta}-SafeOpt，这是SafeOpt算法的一种变体，利用最近的GP上界，因此保留了所有理论保证。其次，我们确定了在复制.

    arXiv:2403.12948v1 Announce Type: new  Abstract: Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing k
    
[^3]: 聚类Mallows模型

    Clustered Mallows Model

    [https://arxiv.org/abs/2403.12880](https://arxiv.org/abs/2403.12880)

    扩展了著名的Mallows模型以适应项目的无差异处理，解决了严格偏好不切实际的问题。

    

    排名是一种从实验中获得的偏好表达，评估者在实验中对项目进行排序，例如按效用降序排列。标记为{1,...,n}的n个物品的排序称为排列，反映了严格的偏好。由于多种原因，对于真实数据严格的偏好可能是不切实际的假设。例如，当物品共享共同特征时，将它们属性为相同等级可能是合理的。此外，对形成排名的决策可能有不同的重要性归因。在某些情况下，例如有大量物品的情况下，评估者可能希望将某些物品排在前几名，将其他物品排在末位，并对其他所有物品表示不确定。此外，在汇总意见时，评判机构可能对排名的某些部分明确，而对其他部分模糊。在本文中，我们扩展了著名的Mallows（Mallows, 1957）模型（MM）以适应项目的无差异处理。

    arXiv:2403.12880v1 Announce Type: cross  Abstract: Rankings are a type of preference elicitation that arise in experiments where assessors arrange items, for example, in decreasing order of utility. Orderings of n items labelled {1,...,n} denoted are permutations that reflect strict preferences. For a number of reasons, strict preferences can be unrealistic assumptions for real data. For example, when items share common traits it may be reasonable to attribute them equal ranks. Also, there can be different importance attributions to decisions that form the ranking. In a situation with, for example, a large number of items, an assessor may wish to rank at top a certain number items; to rank other items at the bottom and to express indifference to all others. In addition, when aggregating opinions, a judging body might be decisive about some parts of the rank but ambiguous for others. In this paper we extend the well-known Mallows (Mallows, 1957) model (MM) to accommodate item indifferen
    
[^4]: 具有函数约束的变分不等式问题的原始方法

    Primal Methods for Variational Inequality Problems with Functional Constraints

    [https://arxiv.org/abs/2403.12859](https://arxiv.org/abs/2403.12859)

    本文提出了一种简单的原始方法，称为约束梯度方法（CGM），用于解决具有多个功能约束的变分不等式问题。

    

    约束变分不等式问题因其在包括机器学习和运筹学在内的各个领域的广泛应用而备受认可。 首次方法已成为解决这些问题的标准方法，因其简单性和可扩展性而受到重视。 传统上，它们通常依赖于投影或线性最小化展开器来导航可行集，但在实践中，这会在具有多个功能约束的情况下变得计算昂贵。 解决这些功能约束变分不等式问题的现有努力主要集中在基于Lagrange函数的原始-对偶算法上。 这些算法及其理论分析通常需要存在并且事先了解最佳拉格朗日乘数。 本文中，我们提出了一个简单的原始方法，称为约束梯度方法（CGM），用于处理功能约束的变分不等式问题。

    arXiv:2403.12859v1 Announce Type: cross  Abstract: Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequa
    
[^5]: 对于序贯核回归的更紧凑置信区间

    Tighter Confidence Bounds for Sequential Kernel Regression

    [https://arxiv.org/abs/2403.12732](https://arxiv.org/abs/2403.12732)

    通过使用鞅尾巴界限和无限维凸规划的有限维重构，建立了序贯核回归的新置信区间，证明其始终比现有的置信区间更紧凑，并将其应用于核赌博问题，提高了算法的性能表现。

    

    置信区间是严格量化预测不确定性的重要工具。它们可以指导探索与开发的权衡，并构成许多序贯学习和决策算法的核心组成部分。更紧凑的置信区间带来了具有更好经验性能和更好性能保证的算法。在这项工作中，我们使用鞅尾巴界限和无限维凸规划的有限维重构来建立序贯核回归的新置信区间。我们证明在这一设置中，我们的新置信区间始终比现有的更紧凑。我们将我们的置信区间应用于核赌博问题，其中未来的行动取决于先前的历史。当我们的置信区间取代现有的置信区间时，KernelUCB（GP-UCB）算法具有更好的经验性能，匹配的最坏情况性能保证和可比性。

    arXiv:2403.12732v1 Announce Type: cross  Abstract: Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and compara
    
[^6]: 使用数据增强在神经网络中对后验不确定性进行量化

    Posterior Uncertainty Quantification in Neural Networks using Data Augmentation

    [https://arxiv.org/abs/2403.12729](https://arxiv.org/abs/2403.12729)

    通过提出MixupMP方法，使用数据增强构建更现实的预测分布，从而解决了在神经网络中对后验不确定性进行量化时的基本模型类错误规范化问题。

    

    在这篇论文中，我们通过一个预测框架来处理深度学习中的不确定性量化问题，该框架通过指定有关未来未见数据的预测分布的假设来捕捉模型参数的不确定性。在这个观点下，我们展示了深度集成（Lakshminarayanan等，2017）是一个基本上错误规范化的模型类，因为它假设未来数据仅支持现有观察结果 -- 这种情况在实践中很少遇到。为了解决这个局限性，我们提出了MixupMP，一种使用流行的数据增强技术构建更现实的预测分布的方法。MixupMP作为深度集成的替代方案，其中每个集成成员都是在这个预测分布的随机模拟上训练的。基于最近提出的马丁格尔后验框架（Fong等，2023），MixupMP返回隐式样本。

    arXiv:2403.12729v1 Announce Type: cross  Abstract: In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data. Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly
    
[^7]: 基于高斯伯努利受限玻尔兹曼机的异常检测评分可解释性改进

    Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine

    [https://arxiv.org/abs/2403.12672](https://arxiv.org/abs/2403.12672)

    该研究提出了一种基于累积分布改进评分可解释性的度量，建立了使用可解释度量设置阈值的准则，并通过数值实验证明其合理性。

    

    高斯伯努利受限玻尔兹曼机（GBRBM）常用于半监督异常检测，仅使用正常数据点进行训练。在基于GBRBM的异常检测中，根据边缘GBRBM的能量函数相同的评分来对正常和异常数据进行分类。然而，由于无法解释该评分，很难设置适当的分类阈值。本研究提出了一种基于累积分布改进评分可解释性的度量，并建立了使用可解释度量设置阈值的准则。数值实验结果表明，仅使用正常数据点设置阈值时，该准则是合理的。此外，由于识别度量涉及计算不可行的最小评分值的评估，我们还提出了一种最小评分的评估方法。

    arXiv:2403.12672v1 Announce Type: cross  Abstract: Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points. In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score
    
[^8]: 用于科学中评估生成模型的统计距离的实用指南

    A Practical Guide to Statistical Distances for Evaluating Generative Models in Science

    [https://arxiv.org/abs/2403.12636](https://arxiv.org/abs/2403.12636)

    本文提供了一种实用指南，介绍了四种常用统计距离的概念，以帮助评估生成模型，无需高深的数学和统计知识。

    

    生成模型在许多科学领域中是非常宝贵的，因为它们能够捕捉高维和复杂的分布，例如逼真的图像、蛋白质结构和连接组。本研究旨在为理解流行的统计距离概念提供一个易于理解的入口点，只需要数学和统计学的基础知识。我们专注于代表不同方法论的四种常用统计距离概念：使用低维投影（Sliced-Wasserstein; SW)、使用分类器获取距离（Classifier Two-Sample Tests; C2ST)、通过核进行嵌入（Maximum Mean Discrepancy; MMD) 或神经网络（Fr\'echet Inception Distance; FID)。我们强调每个距离背后的直觉，并解释它们的优点、可伸缩性、复杂性和缺陷。

    arXiv:2403.12636v1 Announce Type: new  Abstract: Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distanc
    
[^9]: 通过获取赋权：支持小规模深度学习的案例

    Equity through Access: A Case for Small-scale Deep Learning

    [https://arxiv.org/abs/2403.12562](https://arxiv.org/abs/2403.12562)

    通过引入PePR分数，研究人员展示了在资源有限的情况下，利用131种独特的DL架构在医学图像任务中的可行性。

    

    深度学习（DL）的最新进展得益于大规模数据和计算力的提升。这些大规模资源被用于训练日益庞大的模型，而这些模型在计算、数据、能源和碳排放方面消耗巨大。这些成本正在成为研究人员和从业者面临的新型准入障碍，特别是对于那些在全球南方地区资源有限的人。在这项工作中，我们全面审视了现有视觉任务的DL模型，并展示了它们在资源有限的环境中的实用性。为了考虑DL模型的资源消耗，我们引入了一个衡量性能与资源单元的新指标，我们称之为PePR分数。通过使用131种独特的DL架构（跨度从1M到130M个可训练参数）和三个医学图像数据集，我们获取了有关性能和资源之间关系的趋势。

    arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
    
[^10]: 多层网络中基于谱方法的社区检测

    Community detection by spectral methods in multi-layer networks

    [https://arxiv.org/abs/2403.12540](https://arxiv.org/abs/2403.12540)

    改进的谱聚类算法在多层网络中实现了更好的社区检测性能，并证明多层网络对社区检测有优势。

    

    多层网络中的社区检测是网络分析中一个关键问题。本文分析了两种谱聚类算法在多层度校正随机块模型（MLDCSBM）框架下进行社区检测的性能。一种算法基于邻接矩阵的和，另一种利用了去偏和的平方邻接矩阵的和。我们在网络规模和/或层数增加时建立了这些方法在MLDCSBM下进行社区检测的一致性结果。我们的定理展示了利用多层进行社区检测的优势。此外，我们的分析表明，利用去偏和的平方邻接矩阵的谱聚类通常优于利用邻接矩阵的谱聚类。数值模拟证实了我们的算法，采用了去偏和的平方邻接矩阵。

    arXiv:2403.12540v1 Announce Type: cross  Abstract: Community detection in multi-layer networks is a crucial problem in network analysis. In this paper, we analyze the performance of two spectral clustering algorithms for community detection within the multi-layer degree-corrected stochastic block model (MLDCSBM) framework. One algorithm is based on the sum of adjacency matrices, while the other utilizes the debiased sum of squared adjacency matrices. We establish consistency results for community detection using these methods under MLDCSBM as the size of the network and/or the number of layers increases. Our theorems demonstrate the advantages of utilizing multiple layers for community detection. Moreover, our analysis indicates that spectral clustering with the debiased sum of squared adjacency matrices is generally superior to spectral clustering with the sum of adjacency matrices. Numerical simulations confirm that our algorithm, employing the debiased sum of squared adjacency matri
    
[^11]: 非负对比学习

    Non-negative Contrastive Learning

    [https://arxiv.org/abs/2403.12459](https://arxiv.org/abs/2403.12459)

    非负对比学习(NCL)是对非负矩阵分解(NMF)的重新演绎，通过对特征施加非负约束来获得可解释的特征，保留了NMF的可解释属性，从而得到比标准对比学习(CL)更稀疏和解耦的表示

    

    深度表示在以黑盒方式转移到下游任务时表现出了良好的性能。然而，它们固有的不可解释性仍然是一个重大挑战，因为这些特征通常对人类理解而言是不透明的。在本文中，我们提出了非负对比学习（NCL），这是对非负矩阵分解（NMF）的复兴，旨在得出可解释的特征。NCL的力量在于强制将非负约束应用于特征，这让人想起NMF能够提取与样本集群紧密对齐的特征的能力。NCL不仅在数学上与NMF目标很好地对齐，而且保留了NMF的可解释属性，使得与标准对比学习（CL）相比，得到了更加稀疏和解耦的表示。从理论上，我们为NCL的可识别性和下游泛化性能提供了保证。从经验上看，我们展示了这些

    arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
    
[^12]: 生成的数据总是有助于对比学习吗？

    Do Generated Data Always Help Contrastive Learning?

    [https://arxiv.org/abs/2403.12448](https://arxiv.org/abs/2403.12448)

    生成的高质量图像已成功应用于增强对比表示学习，但我们发现有时生成的数据甚至会对对比学习造成伤害，通过研究发现更强的数据膨胀应该伴随着更弱的增强。

    

    对比学习（CL）已经成为无监督视觉表示学习中最成功的范式之一，然而它往往依赖大量手工数据增强。随着生成模型的兴起，特别是扩散模型，生成接近真实数据分布的逼真图像的能力得到了很好的认可。这些生成的高质量图像已成功应用于增强对比表示学习，一种称为“数据膨胀”的技术。然而，我们发现生成的数据（甚至来自像DDPM这样的好扩散模型）有时甚至会对对比学习造成伤害。我们从数据膨胀和数据增强的角度探讨了这种失败的原因。我们首次揭示了更强的数据膨胀应该伴随着更弱的增强，反之亦然的互补作用。我们还提供了严格的理论解释。

    arXiv:2403.12448v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanatio
    
[^13]: 基于奖励样本的顺序多臂老虎机中的转移

    Transfer in Sequential Multi-armed Bandits via Reward Samples

    [https://arxiv.org/abs/2403.12428](https://arxiv.org/abs/2403.12428)

    提出了一种基于奖励样本转移的算法，在顺序多臂老虎机问题中显著改进了累积遗憾性能

    

    我们考虑了一个顺序随机多臂老虎机问题，在这个问题中，代理与老虎机在多个轮次中进行交互。臂的奖励分布在一个轮次中保持不变，但在不同轮次之间可能会发生变化。我们提出了一种基于UCB的算法，用于从先前轮次中转移奖励样本，并改善所有轮次中的累积遗憾性能。我们为我们的算法提供了遗憾性能分析和实证结果，表明相比没有转移的标准UCB算法，我们的算法有显著改进。

    arXiv:2403.12428v1 Announce Type: new  Abstract: We consider a sequential stochastic multi-armed bandit problem where the agent interacts with bandit over multiple episodes. The reward distribution of the arms remain constant throughout an episode but can change over different episodes. We propose an algorithm based on UCB to transfer the reward samples from the previous episodes and improve the cumulative regret performance over all the episodes. We provide regret analysis and empirical results for our algorithm, which show significant improvement over the standard UCB algorithm without transfer.
    
[^14]: 半监督计分匹配算法评估公共卫生干预效果

    Semisupervised score based matching algorithm to evaluate the effect of public health interventions

    [https://arxiv.org/abs/2403.12367](https://arxiv.org/abs/2403.12367)

    提出了一种基于二次评分函数的一对一匹配算法，通过设计权重最小化配对训练单元之间的得分差异，同时最大化未配对训练单元之间的得分差异

    

    多元匹配算法在观察性研究中“配对”相似的研究单元，以消除由于缺乏随机性而引起的潜在偏倚和混杂效应。我们提出了一种基于二次评分函数的新型一对一匹配算法，权重$\beta$被设计为最小化配对训练单元之间的得分差异，同时最大化未配对训练单元之间的得分差异。

    arXiv:2403.12367v1 Announce Type: cross  Abstract: Multivariate matching algorithms "pair" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of "pairs" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a "training" set of paired units by domain experts, is practically intriguing.   We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights $\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units
    
[^15]: 一种用于精准矩阵的替代图形Lasso算法

    An Alternative Graphical Lasso Algorithm for Precision Matrices

    [https://arxiv.org/abs/2403.12357](https://arxiv.org/abs/2403.12357)

    通过重新参数化精准矩阵最后一列，将正则化正态对数似然分解为两个易于最小化的凸函数，其中一个是Lasso回归问题，从而开发了一种简单的块坐标下降算法来计算GLasso更新，性能可与DP-GLas相媲美。

    

    Graphical Lasso (GLasso)算法是一种快速且广泛用于估计稀疏精准矩阵的方法。本文通过使用新的并略微不同的精准矩阵最后一列的重新参数化，展示了正则化正态对数似然自然地分解成两个易于最小化的凸函数之和，其中之一是Lasso回归问题。这种分解是开发透明、简单的迭代块坐标下降算法的关键，用于计算GLasso更新，其性能与DP-GLas相当。

    arXiv:2403.12357v1 Announce Type: cross  Abstract: The Graphical Lasso (GLasso) algorithm is fast and widely used for estimating sparse precision matrices (Friedman et al., 2008). Its central role in the literature of high-dimensional covariance estimation rivals that of Lasso regression for sparse estimation of the mean vector. Some mysteries regarding its optimization target, convergence, positive-definiteness and performance have been unearthed, resolved and presented in Mazumder and Hastie (2011), leading to a new/improved (dual-primal) DP-GLasso. Using a new and slightly different reparametriztion of the last column of a precision matrix we show that the regularized normal log-likelihood naturally decouples into a sum of two easy to minimize convex functions one of which is a Lasso regression problem. This decomposition is the key in developing a transparent, simple iterative block coordinate descent algorithm for computing the GLasso updates with performance comparable to DP-GLas
    
[^16]: 随机Halpern迭代在赋范空间中的应用及其在强化学习中的应用

    Stochastic Halpern iteration in normed spaces and applications to reinforcement learning

    [https://arxiv.org/abs/2403.12338](https://arxiv.org/abs/2403.12338)

    该论文分析了随机Halpern迭代在赋范空间中的Oracle复杂度，提出了改进的算法复杂度，进而在强化学习中提出了新的同步算法应用。

    

    我们分析了具有方差减少的随机Halpern迭代的Oracle复杂度，旨在近似有界和收缩算子的不动点在一个有限维赋范空间中。我们表明，如果底层的随机Oracle具有一致有界的方差，则我们的方法展现出总的Oracle复杂度为$ \tilde{O} (\varepsilon^{-5})$，改进了最近为随机Krasnoselskii-Mann迭代建立的速率。此外，我们建立了 $\Omega (\varepsilon^{-3})$的下界，适用于广泛范围的算法，包括所有带有小批处理的平均迭代。通过适当修改我们的方法，我们推导出了在算子为 $\gamma$-收缩的情况下一个 $O(\varepsilon^{-2}(1-\gamma)^{-3})$复杂度上界。作为一个应用，我们提出了新的用于平均奖励和折扣奖励马尔可夫决策过程的同步算法。

    arXiv:2403.12338v1 Announce Type: cross  Abstract: We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, f
    
[^17]: FedFisher：利用Fisher信息进行一次性联邦学习

    FedFisher: Leveraging Fisher Information for One-Shot Federated Learning

    [https://arxiv.org/abs/2403.12329](https://arxiv.org/abs/2403.12329)

    该论文提出了FedFisher算法，通过利用Fisher信息矩阵进行一次性联邦学习，能够在一次通信中训练出全局模型，并在理论上证明了随着神经网络宽度和客户端本地训练量的增加，FedFisher全局模型的误差会变得非常小。

    

    标准的联邦学习(FL)算法通常需要服务器和客户端之间的多轮通信，这具有几个缺点，包括需要恒定的网络连通性，重复投入计算资源，以及容易受到隐私攻击的影响。一次性FL是一种新的范例，旨在通过使服务器在一轮通信中训练全局模型来解决这一挑战。在这项工作中，我们提出了FedFisher，一种新颖的用于一次性FL的算法，该算法利用在本地客户端模型上计算的Fisher信息矩阵，受到FL的贝叶斯视角的启发。首先，我们从两层过参数化的ReLU神经网络的理论角度对FedFisher进行了分析，并展示了我们的一次性FedFisher全局模型的误差会在神经网络的宽度和客户端本地训练量增加时变得无限小。接下来，我们提出了实用的F的变种

    arXiv:2403.12329v1 Announce Type: new  Abstract: Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of F
    
[^18]: 通过控制虚警覆盖率选择信息量丰富的符合预测集

    Selecting informative conformal prediction sets with false coverage rate control

    [https://arxiv.org/abs/2403.12295](https://arxiv.org/abs/2403.12295)

    提出了一种新的统一框架，用于构建信息丰富的符合预测集，同时控制所选样本的虚警覆盖率。

    

    在监督学习中，包括回归和分类，符合方法为任何机器学习预测器提供预测结果/标签的预测集合，具有有限样本覆盖率。在这里我们考虑了这样一种情况，即这种预测集合是经过选择过程得到的。该选择过程要求选择的预测集在某种明确定义的意义上是“信息量丰富的”。我们考虑了分类和回归设置，在这些设置中，分析人员可能只考虑具有预测标签集或预测区间足够小、不包括空值或遵守其他适当的“单调”约束的样本为具有信息量丰富的。虽然这涵盖了各种应用中可能感兴趣的许多设置，我们开发了一个统一的框架，用来构建这样的信息量丰富的符合预测集，同时控制所选样本上的虚警覆盖率（FCR）。

    arXiv:2403.12295v1 Announce Type: cross  Abstract: In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictors. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be `informative' in a well defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction label sets or prediction intervals small enough, excluding null values, or obeying other appropriate `monotone' constraints. While this covers many settings of possible interest in various applications, we develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the f
    
[^19]: 通过二次和方法进行私有图估计

    Private graphon estimation via sum-of-squares

    [https://arxiv.org/abs/2403.12213](https://arxiv.org/abs/2403.12213)

    基于和平方法的私有图估计算法首次实现了学习随机块模型和图估计的纯节点差分隐私算法，具有多项式运行时间，与之前最佳的信息论节点私有机制具有相匹配的统计效用保证。

    

    我们开发了用于学习随机块模型和图估计的第一个纯节点差分隐私算法，对于任意常数个块，具有多项式运行时间。统计效用保证与先前最佳的信息论（指数时间）节点私有机制相匹配。该算法基于一个基于指数机制的得分函数，该函数定义为依赖于块数量的二次和松弛。我们结果的关键要素是：(1) 在形式上定义为二次优化在双重随机矩阵的多胞体上的距离的特征化块图定义，(2) 一般的多项式优化的和平方法在任意多胞体上的收敛结果，以及(3) 执行利普希茨扩展的得分函数作为二次和算法范例的一般方法。

    arXiv:2403.12213v1 Announce Type: cross  Abstract: We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks. The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems. The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks. The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm.
    
[^20]: 神经网络逼近RKHS函数型

    Approximation of RKHS Functionals by Neural Networks

    [https://arxiv.org/abs/2403.12187](https://arxiv.org/abs/2403.12187)

    本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型，并建立了逼近的普适性，推导了逆多重二次、高斯和Sobolev核引起的误差界限，证明神经网络可以准确逼近广义函数线性模型中的回归映射。

    

    受到时间序列和图像等丰富功能性数据的启发，人们越来越感兴趣将这些数据整合到神经网络中，并从函数空间到R（即函数型）学习映射。本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型。我们建立了对RKHS上函数型逼近的普适性。具体来说，我们推导了通过逆多重二次、高斯和Sobolev核引起的明确误差界限。此外，我们将我们的研究应用于函数回归，证明了神经网络可以准确逼近广义函数线性模型中的回归映射。现有的功能性学习作品需要积分型基函数展开与一组预定义的基函数。通过在RKHS中利用插值正交投影，我们提出的网络是...

    arXiv:2403.12187v1 Announce Type: cross  Abstract: Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is 
    
[^21]: 少数个体的力量：利用核心子集选择加速和优化数据重新加权

    The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection

    [https://arxiv.org/abs/2403.12166](https://arxiv.org/abs/2403.12166)

    提出一种利用核心子集选择进行数据重新加权的方法，有效优化了计算时间和模型性能，突显其作为模型训练的可扩展和精确解决方案的潜力。

    

    随着机器学习任务不断发展，趋势是收集更大的数据集并训练规模越来越大的模型。虽然这提高了准确性，但也将计算成本提高到不可持续的水平。针对这一问题，我们的工作旨在在计算效率和模型准确性之间取得微妙的平衡，这是该领域中一直存在的挑战。我们引入了一种利用核心子集选择进行重新加权的新方法，有效优化了计算时间和模型性能。通过专注于 strategically selected coreset，我们的方法提供了一个稳健的表示，因为它有效地最小化了异常值的影响。然后，重新校准的权重被映射回并传播到整个数据集。我们的实验结果证实了这种方法的有效性，突显了它作为模型训练的可扩展和精确解决方案的潜力。

    arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
    
[^22]: 变分方法用于Dirichlet混合模型中KL散度的高效估计

    Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models

    [https://arxiv.org/abs/2403.12158](https://arxiv.org/abs/2403.12158)

    引入变分方法在Dirichlet混合模型中提供了封闭形式的解，显著提高了计算效率，可用于快速模型比较和稳健估计评估。

    

    本研究致力于在Dirichlet混合模型（DMM）中高效估计Kullback-Leibler（KL）散度，这对于对成分数据进行聚类至关重要。尽管DMM的重要性，但获得KL散度的解析解仍然是困难的。过去的方法依赖于计算密集型的蒙特卡洛方法，这促使我们引入了一种新颖的变分方法。我们的方法提供了一个封闭形式的解，显著提高了计算效率，可以快速进行模型比较和稳健的估计评估。实际数据和模拟数据的验证显示，我们的方法比传统的基于蒙特卡洛的方法更加高效准确，为快速探索不同DMM模型和推进成分数据的统计分析开辟了新途径。

    arXiv:2403.12158v1 Announce Type: cross  Abstract: This study tackles the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM), crucial for clustering compositional data. Despite the significance of DMMs, obtaining an analytically tractable solution for KL Divergence has proven elusive. Past approaches relied on computationally demanding Monte Carlo methods, motivating our introduction of a novel variational approach. Our method offers a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations. Validation using real and simulated data showcases its superior efficiency and accuracy over traditional Monte Carlo-based methods, opening new avenues for rapid exploration of diverse DMM models and advancing statistical analyses of compositional data.
    
[^23]: 用于学习神经网络等变表示的图神经网络

    Graph Neural Networks for Learning Equivariant Representations of Neural Networks

    [https://arxiv.org/abs/2403.12143](https://arxiv.org/abs/2403.12143)

    本研究提出了将神经网络表示为参数的计算图的方法，利用图神经网络和变压器来实现置换对称性，使得单个模型能够处理具有多种架构的神经计算图。

    

    处理其他神经网络参数的神经网络在诸如分类隐式神经表示、生成神经网络权重和预测泛化错误等领域中得到应用。然而，现有方法要么忽视神经网络中固有的置换对称性，要么依赖复杂的权重共享模式来实现等变性，同时忽略网络架构本身的影响。在本文中，我们提出将神经网络表示为参数的计算图，这使我们能够利用强大的保留置换对称性的图神经网络和变压器。因此，我们的方法使得单个模型能够对具有多样架构的神经计算图进行编码。我们展示了我们的方法在包括分类和编辑隐式神经表示、预测泛化错误等多种任务中的有效性。

    arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
    
[^24]: DTOR：决策树异常值回归器用于解释异常

    DTOR: Decision Tree Outlier Regressor to explain anomalies

    [https://arxiv.org/abs/2403.10903](https://arxiv.org/abs/2403.10903)

    DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。

    

    解释异常值的出现以及其产生机制在各种领域中可能非常重要。故障、欺诈、威胁等问题，除了被正确识别之外，通常需要有效的解释以有效执行可操作的对抗措施。越来越广泛地使用复杂的机器学习方法来识别异常值，使得这样的解释更具挑战性。我们提出了决策树异常值回归器（DTOR），这是一种通过估计异常检测模型生成的异常分数来为单个数据点生成基于规则的解释的技术。这是通过首先应用决策树回归器来计算估计分数，然后提取与数据点分数相关联的相对路径来实现的。我们的结果表明，即使在具有大量特征的数据集中，DTOR的鲁棒性也得到了证实。此外，与其他基于规则的方法相比

    arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
    
[^25]: 推动少数群体份额如何影响泛化？关于一层隐藏层神经网络在群体不平衡上的理论研究

    How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance

    [https://arxiv.org/abs/2403.07310](https://arxiv.org/abs/2403.07310)

    本文通过高斯混合模型量化了群体不平衡对样本复杂性、收敛速率和平均以及群体级测试性能的影响，首次提供了ERM在群体级泛化的理论分析。

    

    群体不平衡一直是经验风险最小化（ERM）中已知的问题，其中取得的高平均准确率伴随着少数群体的低准确率。尽管有算法努力改善少数群体的准确性，但关于ERM在各个群体上的理论泛化分析仍然难以实现。通过用高斯混合模型表达群体不平衡问题，本文量化了各个群体对样本复杂性、收敛速率以及平均和群体级测试性能的影响。虽然我们的理论框架集中在使用一层隐藏层神经网络进行二分类，但据我们所知，我们首次提供了ERM在群体级泛化的理论分析，除了通常研究的平均泛化性能。我们的理论结果的一些见解包括当所有群体级协方差都在...

    arXiv:2403.07310v1 Announce Type: cross  Abstract: Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in th
    
[^26]: 一个镜子的库：低维深度神经网络是具有反射特征的凸Lasso模型

    A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features

    [https://arxiv.org/abs/2403.01046](https://arxiv.org/abs/2403.01046)

    证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。

    

    我们证明在1-D数据上训练神经网络等价于解决一个带有固定、明确定义的特征字典矩阵的凸Lasso问题。具体的字典取决于激活函数和深度。我们考虑具有分段线性激活函数的两层网络，深窄的ReLU网络最多有4层，以及具有符号激活和任意深度的矩形和树网络。有趣的是，在ReLU网络中，第四层创建代表训练数据关于自身的反射的特征。Lasso表示法揭示了全局最优网络和解空间的洞察。

    arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
    
[^27]: 无调参的随机优化

    Tuning-Free Stochastic Optimization

    [https://arxiv.org/abs/2402.07793](https://arxiv.org/abs/2402.07793)

    本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。

    

    大规模机器学习问题使得调参的成本越来越高昂。这导致了需要能够即时自我调整的算法的需求。我们将“无调参”算法的概念形式化，即只给出问题参数的粗略提示即可与最优调参优化算法的性能相匹配，误差为对数多项式因子。我们特别考虑能够与最优调参的随机梯度下降(SGD)相匹配的算法。当优化的域是有界的时候，我们证明了调参自由与SGD的匹配是可能的，并且通过几个现有算法实现了这一点。我们证明了当优化的域是无界的时候，对于最小化凸平滑或者Lipschitz函数的任务，无调参优化是不可能的。我们讨论了在无界域中，何种情况下可以实现无调参优化。特别地，我们展示了最近提出的 DoG 和 DoWG 算法在噪声分布足够时是无调参的。

    Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf
    
[^28]: 预测个体治疗效果的一致性蒙特卡洛元学习模型

    Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects

    [https://arxiv.org/abs/2402.04906](https://arxiv.org/abs/2402.04906)

    本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。

    

    认识干预效果，即治疗效果，对于决策至关重要。用条件平均治疗效果 (CATE) 估计等方法通常只提供治疗效果的点估计，而常常需要额外的不确定性量化。因此，我们提出了一个新方法，即一致性蒙特卡洛 (CMC) 元学习模型，利用一致性预测系统、蒙特卡洛采样和 CATE 元学习模型，来产生可用于个性化决策的预测分布。此外，我们展示了结果噪声分布的特定假设如何严重影响这些不确定性预测。尽管如此，CMC框架展示了强大的实验覆盖范围，同时保持较小的区间宽度，以提供真实个体治疗效果的估计。

    Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
    
[^29]: GeoShapley：一种衡量机器学习模型中空间效应的博弈论方法

    GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models

    [https://arxiv.org/abs/2312.03675](https://arxiv.org/abs/2312.03675)

    GeoShapley是一种衡量机器学习模型中空间效应的博弈论方法，将位置视为模型预测博弈中的一名玩家，能够量化位置的重要性并与其他特征之间的协同作用进行量化。

    

    本文介绍了GeoShapley，这是一种衡量机器学习模型中空间效应的博弈论方法。GeoShapley通过将位置概念化为模型预测博弈中的一名玩家，扩展了博弈论中的Nobel Prize-winning Shapley值框架，从而使得可以量化位置的重要性以及位置与模型中其他特征之间的协同作用。GeoShapley是一种与模型无关的方法，可以应用于不同结构的统计学或黑盒机器学习模型中。GeoShapley的解释直接与用于解释空间效应的空间变化系数模型以及用于解释非空间效应的加法模型相连。利用模拟数据，验证了GeoShapley值与已知数据生成过程之间的关系，并用于对比七种统计学和机器学习模型。通过房价建模的实证示例来说明。

    arXiv:2312.03675v2 Announce Type: replace  Abstract: This paper introduces GeoShapley, a game theory approach to measuring spatial effects in machine learning models. GeoShapley extends the Nobel Prize-winning Shapley value framework in game theory by conceptualizing location as a player in a model prediction game, which enables the quantification of the importance of location and the synergies between location and other features in a model. GeoShapley is a model-agnostic approach and can be applied to statistical or black-box machine learning models in various structures. The interpretation of GeoShapley is directly linked with spatially varying coefficient models for explaining spatial effects and additive models for explaining non-spatial effects. Using simulated data, GeoShapley values are validated against known data-generating processes and are used for cross-comparison of seven statistical and machine learning models. An empirical example of house price modeling is used to illus
    
[^30]: 连续索引张量数据的功能贝叶斯 Tucker 分解

    Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data

    [https://arxiv.org/abs/2311.04829](https://arxiv.org/abs/2311.04829)

    提出了一种名为功能贝叶斯 Tucker 分解（FunBaT）的方法，用于将 Tucker 分解推广到连续索引的张量数据，利用高斯过程模型潜在函数。

    

    Tucker 分解是一种强大的张量模型，用于处理多方面数据。它通过将网格结构数据分解为核张量和一组对象表示（因子）之间的交互来展示低秩特性。这种分解的一个基本假设是每个方面或模式中都有有限的对象，对应于数据条目的离散索引。然而，现实世界中的数据往往并非自然地呈现在这种设置中。例如，地理数据以纬度和经度坐标的连续索引表示，无法直接适应张量模型。为了将 Tucker 分解推广到这种场景，我们提出了功能贝叶斯 Tucker 分解（FunBaT）。我们将连续索引数据视为 Tucker 核心和一组潜在函数之间的交互。我们使用高斯过程（GP）作为功能先验来建模潜在函数。然后，我们将每个高斯过程转换为

    arXiv:2311.04829v2 Announce Type: replace  Abstract: Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). A fundamental assumption of such decomposition is that there are finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However, real-world data is often not naturally posed in this setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To generalize Tucker decomposition to such scenarios, we propose Functional Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. We use Gaussian processes (GP) as functional priors to model the latent functions. Then, we convert each GP 
    
[^31]: 一次性策略分类在未知成本下的研究

    One-Shot Strategic Classification Under Unknown Costs

    [https://arxiv.org/abs/2311.02761](https://arxiv.org/abs/2311.02761)

    本研究首次研究了在未知响应下一次性策略分类的情景，针对用户成本函数不确定性，提出解决方案并将任务定义为极小-极大问题。

    

    策略分类的目标是学习对策略输入操纵具有鲁棒性的决策规则。之前的研究假设这些响应是已知的；而最近的一些研究处理未知响应，但它们专门研究重复模型部署的在线设置。然而，在许多领域，特别是在公共政策中，一个常见的激励用例中，多次部署是不可行的，甚至一个糟糕的轮次都是不可接受的。为了填补这一空白，我们首次引入了在未知响应下的一次性策略分类的正式研究，这需要在一次性选择一个分类器。着重关注用户成本函数中的不确定性，我们首先证明对于一类广泛的成本，即使对真实成本的小误差也可能在最坏情况下导致准确性降至极低水平。鉴于此，我们将任务框定为极小-极大问题，目标是识别

    arXiv:2311.02761v2 Announce Type: replace  Abstract: The goal of strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that these responses are known; while some recent works handle unknown responses, they exclusively study online settings with repeated model deployments. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use case$\unicode{x2014}$where multiple deployments are infeasible, or where even one bad round is unacceptable. To address this gap, we initiate the formal study of one-shot strategic classification under unknown responses, which requires committing to a single classifier once. Focusing on uncertainty in the users' cost function, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail trivial accuracy in the worst case. In light of this, we frame the task as a minimax problem, with the goal of identifying
    
[^32]: 针对分段连续决策制定的Oracle高效平滑在线学习

    Oracle-Efficient Smoothed Online Learning for Piecewise Continuous Decision Making

    [https://arxiv.org/abs/2302.05430](https://arxiv.org/abs/2302.05430)

    引入广义括号数的概念，结合对手的约束与空间大小，通过Follow-the-Perturbed-Leader算法实现低遗憾，优化调用优化Oracle的次数以实现遗憾在多个问题中的有效应用。

    

    平滑在线学习已经成为一种流行的框架，可以缓解在从经典学习转向对抗性学习时产生的统计和计算复杂性的显著损失。本文引入了一个新的复杂性概念，即广义括号数，将对手的约束与空间大小相结合，并展示了一个Follow-the-Perturbed-Leader实例可以实现低遗憾，并且优化调用优化Oracle的次数以实现平均遗憾的最佳缩放。

    arXiv:2302.05430v2 Announce Type: replace-cross  Abstract: Smoothed online learning has emerged as a popular framework to mitigate the substantial loss in statistical and computational complexity that arises when one moves from classical to adversarial learning. Unfortunately, for some spaces, it has been shown that efficient algorithms suffer an exponentially worse regret than that which is minimax optimal, even when the learner has access to an optimization oracle over the space. To mitigate that exponential dependence, this work introduces a new notion of complexity, the generalized bracketing numbers, which marries constraints on the adversary to the size of the space, and shows that an instantiation of Follow-the-Perturbed-Leader can attain low regret with the number of calls to the optimization oracle scaling optimally with respect to average regret. We then instantiate our bounds in several problems of interest, including online prediction and planning of piecewise continuous fu
    
[^33]: 平滑在线学习用于分段仿射系统中的预测

    Smoothed Online Learning for Prediction in Piecewise Affine Systems

    [https://arxiv.org/abs/2301.11187](https://arxiv.org/abs/2301.11187)

    本文提出了基于平滑在线学习框架的算法，可以有效处理分段仿射系统中的预测和模拟问题，在弱光滑性假设下具有多项式遗憾度，并且在调用优化预测次数方面是高效的。

    

    分段仿射（PWA）回归和规划问题对于在线学习、控制和机器人学的研究具有基础重要性，它为研究系统动态发生急剧变化提供了一个理论上和实证上可处理的设置。然而，由于穿越不同“片段”时出现的不连续性，学习在一般的顺序设置中是不可能的，实际算法被迫采用启发式方法。本文在最近开发的平滑在线学习框架基础上构建，并提供了第一个在弱光滑性假设下，具有多项式遗憾度并且在优化预测中高效的PWA系统预测和模拟算法；此外，我们将我们的结果应用到一步预测和多步模拟遗憾问题中。

    arXiv:2301.11187v2 Announce Type: replace-cross  Abstract: The problem of piecewise affine (PWA) regression and planning is of foundational importance to the study of online learning, control, and robotics, where it provides a theoretically and empirically tractable setting to study systems undergoing sharp changes in the dynamics. Unfortunately, due to the discontinuities that arise when crossing into different ``pieces,'' learning in general sequential settings is impossible and practical algorithms are forced to resort to heuristic approaches. This paper builds on the recently developed smoothed online learning framework and provides the first algorithms for prediction and simulation in PWA systems whose regret is polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our algorithms are efficient in the number of calls to an optimization oracle. We further apply our results to the problems of one-step prediction and multi-step simulation regret i
    
[^34]: 基于均场神经网络的McKean-Vlasov控制问题算法

    Mean-field neural networks-based algorithms for McKean-Vlasov control problems *

    [https://arxiv.org/abs/2212.11518](https://arxiv.org/abs/2212.11518)

    该论文利用均场神经网络类解决McKean-Vlasov控制问题，提出了多种算法，并在不同示例上展示了数值结果，讨论比较了各种方法的优缺点。

    

    本文致力于通过我们在伴随论文[25]中引入的均场神经网络类，对McKean-Vlasov控制问题进行数值解析，以便在Wasserstein空间上学习解决方案。我们提出了几种算法，基于控制学习策略或值迭代的动态规划，或者基于随机最大原理和全局或局部损失函数的反向SDE。我们展示了对不同示例的大量数值结果，以说明我们的八种算法每一种的准确性。我们讨论并比较了所有测试方法的优缺点。

    arXiv:2212.11518v2 Announce Type: replace-cross  Abstract: This paper is devoted to the numerical resolution of McKean-Vlasov control problems via the class of mean-field neural networks introduced in our companion paper [25] in order to learn the solution on the Wasserstein space. We propose several algorithms either based on dynamic programming with control learning by policy or value iteration, or backward SDE from stochastic maximum principle with global or local loss functions. Extensive numerical results on different examples are presented to illustrate the accuracy of each of our eight algorithms. We discuss and compare the pros and cons of all the tested methods.
    
[^35]: 缓解多目标学习中的梯度偏差：一种可证明收敛的随机方法

    Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach

    [https://arxiv.org/abs/2210.12624](https://arxiv.org/abs/2210.12624)

    提出了一种随机多目标梯度校正（MoCo）方法，能够在不增加批量大小的情况下保证收敛，解决了多目标学习中梯度偏差导致性能下降的问题。

    

    具有多个目标函数的机器学习问题通常出现在需要在多个性能指标（如公平性，安全性和准确性）之间进行权衡的多目标学习中；或者在多任务学习中，多个任务联合优化，共享它们之间的归纳偏差。然而，现有的随机多目标梯度方法及其变体（例如，MGDA，PCGrad，CAGrad等）都采用带偏差的噪声梯度方向，导致经验性能下降。为此，我们开发了一种用于多目标优化的随机多目标梯度校正（MoCo）方法。我们方法的独特之处在于，即使在非凸设置中也能保证收敛而不增加批量大小。对多任务监督学习和强化学习进行了模拟实验。

    arXiv:2210.12624v2 Announce Type: replace  Abstract: Machine learning problems with multiple objective functions appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic Multi-objective gradient Correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the non-convex setting. Simulations on multi-task supervised and reinforcement learning demonstra
    
[^36]: 贝叶斯非参数方法与数据驱动鲁棒优化的结合

    Bayesian Nonparametrics meets Data-Driven Robust Optimization. (arXiv:2401.15771v1 [stat.ML])

    [http://arxiv.org/abs/2401.15771](http://arxiv.org/abs/2401.15771)

    本文提出了一种将贝叶斯非参数方法与最新的决策理论模型相结合的鲁棒优化准则，通过这种方法，可以在线性回归问题中获得有稳定性和优越性能的结果。

    

    训练机器学习和统计模型通常涉及优化数据驱动的风险准则。风险通常是根据经验数据分布计算的，但由于分布不确定性，这可能导致性能不稳定和不好的样本外表现。在分布鲁棒优化的精神下，我们提出了一个新颖的鲁棒准则，将贝叶斯非参数（即狄利克雷过程）理论和最近的平滑模糊规避偏好的决策理论模型的见解相结合。首先，我们强调了与标准正则化经验风险最小化技术的新连接，其中包括岭回归和套索回归。然后，我们从理论上证明了鲁棒优化过程在有限样本和渐近统计保证方面的有利性存在。对于实际实施，我们提出并研究了基于众所周知的狄利克雷过程表示的可行近似准则。

    Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet Process representations. 
    
[^37]: 连续时间序列集合的概率建模

    Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15045](http://arxiv.org/abs/2312.15045)

    本文提出了一个通用的连续时间序列集合的概率建模框架，适用于处理每个事件与一组项目相关联的情况。引入了适用于任何强度为基础的递归神经点过程模型的推理方法，可用于回答关于序列历史条件下的概率查询问题。

    

    在连续时间事件数据的统计参数模型工具箱中，神经标记时间点过程是一个有价值的补充。这些模型适用于每个事件与单个项目（单个事件类型或“标记”）相关联的序列，但不适用于每个事件与一组项目相关联的实际情况。本文中，我们开发了一个通用的连续时间集合数值数据建模框架，与任何基于强度的递归神经点过程模型兼容。此外，我们还开发了推理方法，可使用这些模型回答诸如“在考虑序列历史的条件下，项目A在项目B之前观察到的概率”等概率查询问题。由于问题设置的连续时间性质和每个事件的潜在结果空间的组合极大，对于神经模型来说，计算这些查询的精确答案通常是不可行的。

    Neural marked temporal point processes have been a valuable addition to the existing toolbox of statistical parametric models for continuous-time event data. These models are useful for sequences where each event is associated with a single item (a single type of event or a "mark") -- but such models are not suited for the practical situation where each event is associated with a set of items. In this work, we develop a general framework for modeling set-valued data in continuous-time, compatible with any intensity-based recurrent neural point process model. In addition, we develop inference methods that can use such models to answer probabilistic queries such as "the probability of item $A$ being observed before item $B$," conditioned on sequence history. Computing exact answers for such queries is generally intractable for neural models due to both the continuous-time nature of the problem setting and the combinatorially-large space of potential outcomes for each event. To address th
    
[^38]: MCRAGE: 公平性的合成医疗数据

    MCRAGE: Synthetic Healthcare Data for Fairness. (arXiv:2310.18430v1 [stat.ML])

    [http://arxiv.org/abs/2310.18430](http://arxiv.org/abs/2310.18430)

    MCRAGE是一种使用深度生成模型来增强不平衡的医疗数据集的方法，以解决少数群体在机器学习模型中的不公平问题。

    

    在医疗领域，电子健康记录（EHR）是开发诊断、治疗和管理医疗资源的机器学习模型的关键训练数据。然而，医疗数据集在种族/民族、性别和年龄等敏感属性方面往往存在不平衡。在类不平衡的EHR数据集上训练的机器学习模型在部署时，对于少数群体的个体而言，表现显著不如多数群体的样本，这可能导致少数群体的不公平医疗结果。为了解决这个挑战，我们提出了一种名为Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE)的新方法，通过由深度生成模型生成的样本来增强不平衡的数据集。MCRAGE过程包括训练一个能够从少数群体中产生高质量合成EHR样本的条件去噪扩散概率模型（CDDPM）。

    In the field of healthcare, electronic health records (EHR) serve as crucial training data for developing machine learning models for diagnosis, treatment, and the management of healthcare resources. However, medical datasets are often imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and age. Machine learning models trained on class-imbalanced EHR datasets perform significantly worse in deployment for individuals of the minority classes compared to samples from majority classes, which may lead to inequitable healthcare outcomes for minority groups. To address this challenge, we propose Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE), a novel approach to augment imbalanced datasets using samples generated by a deep generative model. The MCRAGE process involves training a Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of generating high-quality synthetic EHR samples from underrepresented classes. We use this 
    
[^39]: 在欧几里德函数优化中的扭曲几何信息

    Warped geometric information on the optimisation of Euclidean functions. (arXiv:2308.08305v1 [stat.ML])

    [http://arxiv.org/abs/2308.08305](http://arxiv.org/abs/2308.08305)

    使用扭曲几何学的概念，我们提出了一种在高维欧几里德空间中优化函数的方法，并通过在重新定义的黎曼流形上进行计算，找到了函数的最优解。

    

    我们考虑了在潜在高维欧几里德空间中优化实值函数的基本任务，例如许多机器学习任务中的损失函数或统计推断中的概率分布的对数。我们使用扭曲黎曼几何概念，将欧几里德空间上的函数优化问题重新定义为一个带有扭曲度量的黎曼流形，并在该流形上找到函数的最优解。选择用于搜索域的扭曲度量引入了一个计算友好的度量张量，使得在流形上找到最优搜索方向与测地线变得更容易计算。沿测地线进行优化通常是不可行的，但我们表明在这个特定的流形中，我们可以解析地得到高达三阶的泰勒近似。一般情况下，这些对测地线的近似不会位于流形上，但我们构造了合适的回缩方程将这些近似重新映射到流形上。

    We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction m
    
[^40]: 对抗训练应被视为一个非零和博弈

    Adversarial Training Should Be Cast as a Non-Zero-Sum Game. (arXiv:2306.11035v1 [cs.LG])

    [http://arxiv.org/abs/2306.11035](http://arxiv.org/abs/2306.11035)

    本论文提出了一种新的针对对抗性训练的非零和双层公式，实现了与最先进攻击相匹配并且能够达到与标准对抗性训练相同的鲁棒性水平。

    

    解决深度神经网络对抗性脆弱性的一个突出方法是采用对抗性训练的两个玩家零和范式，其中预测器被训练以对抗性选择的数据扰动。虽然这种方法很有前途，但是基于这种范式的算法并没有产生足够的鲁棒性，并且遭受病态行为，如强健的过拟合。为了理解这种缺陷，我们首先展示了在对抗训练算法中使用的常见基于代理的松弛方法使所训练分类器的稳健性没有任何保证。我们发现这个问题后，提出了一个新的非零和双层对抗训练公式，其中每个玩家优化不同的目标函数，我们的公式自然地产生了一个简单的算法框架，可以与最先进的攻击相匹配，并且在一些情况下，能够达到与标准对抗性训练相当的鲁棒性水平。

    One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially-chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness, and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers. The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function. Our formulation naturally yields a simple algorithmic framework that matches and in some cases outperforms state-of-the-art attacks, attains comparable levels of robustness to standard adversarial traini
    
[^41]: 属性高效的低次多项式阈值函数带噪声PAC学习

    Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise. (arXiv:2306.00673v1 [cs.DS])

    [http://arxiv.org/abs/2306.00673](http://arxiv.org/abs/2306.00673)

    本文提出一种新算法，可以属性高效的学习低次多项式阈值函数，并能够在噪声下进行PAC学习。

    

    低次多项式阈值函数（PTFs）的概念类在机器学习中起着基础作用。本文研究了$\mathbb{R}^n$上$K$稀疏度-$d$ PTFs的属性高效PAC学习，其中任何这样的概念仅依赖于输入的$K$个属性。我们的主要贡献是：提出一种新算法，在高斯边缘分布下，即使有$O(\epsilon^d)$的$\eta$被恶意噪声Bshouty et al. (2002)破坏，也可以在错误率$\epsilon$下以$O(\frac{K^{{4d}}}{\epsilon^{2d}}\cdot \log^{5d} n)$的样本PAC学习该类，算法运行时间为$({nd}/{\epsilon})^{O(d)}$。在此之前，仅为稀疏齐次超平面的特殊情况建立了属性高效的鲁棒算法。我们的关键因素是：1）将属性稀疏性转化为Hermite多项式基下chow向量的稀疏模式的结构结果；2）一种新的规范化方法，以及利用多项式近似的阈值函数的直接判别。

    The concept class of low-degree polynomial threshold functions (PTFs) plays a fundamental role in machine learning. In this paper, we study PAC learning of $K$-sparse degree-$d$ PTFs on $\mathbb{R}^n$, where any such concept depends only on $K$ out of $n$ attributes of the input. Our main contribution is a new algorithm that runs in time $({nd}/{\epsilon})^{O(d)}$ and under the Gaussian marginal distribution, PAC learns the class up to error rate $\epsilon$ with $O(\frac{K^{4d}}{\epsilon^{2d}} \cdot \log^{5d} n)$ samples even when an $\eta \leq O(\epsilon^d)$ fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces. Our key ingredients are: 1) a structural result that translates the attribute sparsity to a sparsity pattern of the Chow vector under the basis of Hermite polynomials, and 2) a novel 
    
[^42]: 关于知识蒸馏中的学生-教师偏差：违反规则是否有益？

    On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12923](http://arxiv.org/abs/2301.12923)

    通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。

    

    知识蒸馏（KD）被广泛用于通过训练学生模仿经过训练的“教师”网络的软概率来提高“学生”网络的测试准确性。然而，最近的研究表明，尽管被训练成适应教师的概率，学生不仅明显偏离这些概率，而且表现比教师更好。我们的研究旨在通过确定学生-教师偏差的确切性质，并论证它们与更好的泛化能力如何共存来解决这一看似矛盾的观察。首先，通过对图像和语言数据进行实验，我们确定这些偏差对应于学生系统性地夸大教师的自信水平。接下来，在一些简单的设置中，我们从理论和实证上建立了KD在收敛更快的过程中夸大了梯度下降的隐含偏差的证据。最后，

    Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
    

