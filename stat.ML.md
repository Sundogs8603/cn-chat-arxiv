# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the quality of randomized approximations of Tukey's depth.](http://arxiv.org/abs/2309.05657) | 本文研究了Tukey深度的随机近似质量问题，证明了在维度较高且数据从对数凹集的均匀分布中抽样的情况下，随机算法可以正确近似最大深度和接近零的深度，而对于中间深度的点，任何好的近似都需要指数复杂度。 |
| [^2] | [Boundary Peeling: Outlier Detection Method Using One-Class Peeling.](http://arxiv.org/abs/2309.05630) | 一类边界剥离是一种无监督的异常检测算法，使用了平均有符号距离和灵活的边界生成方法。在无异常值和有异常值的情况下，一类边界剥离表现出优越的性能。 |
| [^3] | [Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning.](http://arxiv.org/abs/2309.05505) | 在联邦学习中重复参数共享会泄露私人数据，本文提出了一种表示联邦学习目标，旨在在保障差分隐私的同时允许本地个性化，通过提出的新算法DPFEDREP，可以在线性表示设置中收敛到以全局为中心的球形区域。 |
| [^4] | [Comprehensive analysis of synthetic learning applied to neonatal brain MRI segmentation.](http://arxiv.org/abs/2309.05306) | 本研究针对新生儿脑MRI图像的分割任务，通过综合合成学习的方法训练了对图像对比度变化和解剖结构的空间配置具有鲁棒性的模型。在对超过700名婴儿的高质量图像数据进行实验后，我们发现标准Unet模型在少数T2加权体积上表现出惊人的性能，但这些模型学习到的特征与训练领域的强度相关。 |
| [^5] | [The fine print on tempered posteriors.](http://arxiv.org/abs/2309.05292) | 这项研究深入探讨了混杂后验参数的问题，发现了一些重要的新观点。研究结果表明，在实际情况下，随机性通常不能提高测试准确性，并且对于某些贝叶斯模型，增加随机性反而会降低测试准确性。此外，研究还解释了优化目标中温度参数的重要性，并指出温度不能简单地被视为修正了先验或似然的错误设定。 |
| [^6] | [Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood.](http://arxiv.org/abs/2309.05153) | 本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。 |
| [^7] | [Outlier Robust Adversarial Training.](http://arxiv.org/abs/2309.05145) | 异常值鲁棒对抗训练（ORAT）是一种同时处理训练数据质量和推理时间对抗攻击的模型，采用了鲁棒的排名损失函数，具有较好的泛化性能。 |
| [^8] | [DAD++: Improved Data-free Test Time Adversarial Defense.](http://arxiv.org/abs/2309.05132) | DAD++是一种改进的无数据测试时对抗防御方法，通过包含检测和修正框架以提高深度神经网络的鲁棒性。同时，引入了软检测方案以增强修正框架在检测器置信度不足时的效果。 |
| [^9] | [Nonlinear Granger Causality using Kernel Ridge Regression.](http://arxiv.org/abs/2309.05107) | 使用核岭回归的 mlcausality 是一种新的算法和Python库，用于识别非线性 Granger 因果关系，具有竞争力的性能和更精细校准的 p 值，以及显著降低的计算时间。 |
| [^10] | [Generalization error bounds for iterative learning algorithms with bounded updates.](http://arxiv.org/abs/2309.05077) | 本文研究了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，提出了一种新颖的泛化误差界限，利用了信息论技术。研究表明，在模型维度和训练数据样本数量相等的情况下，界限得到了改善。 |
| [^11] | [Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression.](http://arxiv.org/abs/2309.05030) | 本文提出了去殖民化人工智能对齐的三个建议：改变基本道德哲学为达尔玛哲学，允许多元主义的论证传统存在于对齐技术中，以及将价值认识论扩展到超越自然语言中的指令。 |
| [^12] | [SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models.](http://arxiv.org/abs/2309.05019) | 本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。 |
| [^13] | [A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning.](http://arxiv.org/abs/2309.04877) | 这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。 |
| [^14] | [Approximation Results for Gradient Descent trained Neural Networks.](http://arxiv.org/abs/2309.04860) | 该论文研究了采用梯度下降训练的神经网络的近似保证，利用连续的误差范数对网络进行分析，并发现在欠参数化的情况下相对于已有的逼近方法存在逼近率下降的问题。 |
| [^15] | [Non-linear dimension reduction in factor-augmented vector autoregressions.](http://arxiv.org/abs/2309.04821) | 本文引入了非线性维度降低方法，在因子扩展向量自回归中分析经济冲击，证明了在经济周期动荡和高度波动数据下，该方法具有良好的预测性能，并能处理COVID-19疫情引起的离群值。 |
| [^16] | [Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization.](http://arxiv.org/abs/2309.04810) | 本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。 |
| [^17] | [Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks.](http://arxiv.org/abs/2309.04742) | 本文提出了一种仿射不变集成变换方法，可以改善在ReLU网络中的预测不确定性。通过基于集合卡尔曼滤波的贝叶斯推断，我们提出了两个相互作用的粒子系统，并证明了它们的收敛性。同时，我们还探讨了这些方法用于量化预测不确定性的有效性。 |
| [^18] | [Probabilistic Safety Regions Via Finite Families of Scalable Classifiers.](http://arxiv.org/abs/2309.04627) | 本论文提出了概率安全区域的概念，用于描述一个输入空间子集，在这个子集中，误分类实例的数量可以被概率上得到控制。同时，利用可伸缩分类器来将机器学习的调参与误差控制相结合。 |
| [^19] | [Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning.](http://arxiv.org/abs/2309.04626) | 感知调整查询（PAQ）是一种新的用于收集人类反馈的查询机制，采用反向测量方案，结合了基数查询和序数查询的优点。我们将PAQ应用于度量学习问题中，通过PAQ测量来学习未知的马氏距离，并开发了一个两阶段估计器，提供了样本复杂性保证。 |
| [^20] | [Generating drawdown-realistic financial price paths using path signatures.](http://arxiv.org/abs/2309.04507) | 本文提出了一种新颖的机器学习方法，使用路径签名来生成逼近实际数据的金融价格路径，并应用于定价回撤保险期权和投资组合回撤控制策略。 |
| [^21] | [Improved theoretical guarantee for rank aggregation via spectral method.](http://arxiv.org/abs/2309.03808) | 本论文通过谱方法改进了排名聚合问题的理论保证，通过研究基于未归一化和归一化数据矩阵的谱排名算法，提供了更准确的扰动误差界限。 |
| [^22] | [Les Houches Lectures on Deep Learning at Large & Infinite Width.](http://arxiv.org/abs/2309.01592) | 本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。 |
| [^23] | [Area-norm COBRA on Conditional Survival Prediction.](http://arxiv.org/abs/2309.00417) | 本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。 |
| [^24] | [Online GentleAdaBoost -- Technical Report.](http://arxiv.org/abs/2308.14004) | 该论文研究了在线版的GentleAdaboost算法，通过在线方式将弱分类器与强分类器结合，提出了一种通过线搜索将批处理方法扩展为在线方法的方法，并与其他在线方法在各种基准数据集上进行了对比。 |
| [^25] | [Gotta match 'em all: Solution diversification in graph matching matched filters.](http://arxiv.org/abs/2308.13451) | 本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。 |
| [^26] | [SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation.](http://arxiv.org/abs/2308.07896) | SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。 |
| [^27] | [InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis.](http://arxiv.org/abs/2307.12586) | InVAErt网络是一个数据驱动的框架，用于分析和合成物理系统，具有模型反演和可识别性分析的能力。 |
| [^28] | [Towards Trustworthy Explanation: On Causal Rationalization.](http://arxiv.org/abs/2306.14115) | 该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。 |
| [^29] | [Estimating the Value of Evidence-Based Decision Making.](http://arxiv.org/abs/2306.13681) | 本文提出了一个实证框架，用于估算证据决策的价值和统计精度投资回报。 |
| [^30] | [Bayesian Numerical Integration with Neural Networks.](http://arxiv.org/abs/2305.13248) | 本文提出了一种基于神经网络架构的贝叶斯数值积分方法，称为贝叶斯 Stein 神经网络。该方法可高效地编码积分先验信息并计算积分估计的不确定性。在实际问题中，该方法展现出数量级的加速的优势。 |
| [^31] | [Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation.](http://arxiv.org/abs/2301.11936) | 这篇论文提出了量子小波变换(QRT)作为神经网络中的重要应用，通过量子计算实现了对量子态的小波变换，并且可以高效地找到大型神经网络的稀疏可训练子网络。 |
| [^32] | [Projective Integral Updates for High-Dimensional Variational Inference.](http://arxiv.org/abs/2301.08374) | 该论文介绍了一种适用于高维变分推理的投影积分更新方法，并通过降低参数敏感性来实现更强健的预测。 |
| [^33] | [Adaptive Selection of the Optimal Strategy to Improve Precision and Power in Randomized Trials.](http://arxiv.org/abs/2210.17453) | 本研究提供了一种自适应预设方法，以选择在随机试验中调整哪些变量，以及以何种形式进行调整，从而最大化精度，同时保持Ⅰ类错误控制。 |
| [^34] | [PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits.](http://arxiv.org/abs/2210.15345) | 本文提出了一种称为PopArt的高效稀疏线性估计方法，相比于Lasso，在许多问题中具有更紧的$\ell_1$恢复保证，并基于此推导出稀疏线性摇臂算法，具有改进的遗憾上界。同时，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界。 |
| [^35] | [Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation.](http://arxiv.org/abs/2210.05918) | 本研究通过引入尾平均和正则化技术，对时序差异(TD)学习算法进行了有限时间行为的研究。我们得出结论，尾平均TD能以最优速率 $O(1/t)$ 收敛，并且初始误差衰减速率更快。此外，正则化的TD版本在具有病态特征的问题上很有用。 |
| [^36] | [Generalized Kernel Regularized Least Squares.](http://arxiv.org/abs/2209.14355) | 本论文提出了广义核正则化最小二乘法 (gKRLS)，解决了核正则化最小二乘法 (KRLS) 在当前使用中的两个限制：它的扩展能力不足，且即使在小规模数据集上，其计算代价也非常高昂。 |
| [^37] | [Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility.](http://arxiv.org/abs/2205.08187) | 本文研究了具有相关权重的深度神经网络的极限行为，发现无限宽度神经网络的每一层可以通过两个简单的量来刻画，当其中至少一层的量是非平凡的时候，得到了高斯过程的混合模型。 |
| [^38] | [HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series.](http://arxiv.org/abs/2202.03224) | HERMES是一个用于预测非平稳时序数据的混合纠错模型，并且利用外部信号提供了最先进的结果。 |
| [^39] | [Fat-Shattering Dimension of $k$-fold Aggregations.](http://arxiv.org/abs/2110.04763) | 该论文估计了实值函数类聚合规则的破裂维数，并给出了关于线性和仿射函数类的更尖锐上界和匹配的下界。同时改进了已知结果，并纠正了文献中的一些错误论断。 |
| [^40] | [Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks.](http://arxiv.org/abs/2107.08195) | 本文提出了一种复杂度优化稀疏贝叶斯学习方法DQN-SBL来解决高维特征空间或大数据规模问题中的内存溢出和计算复杂度高的问题，并在大规模问题上展现了竞争力的泛化能力。 |
| [^41] | [AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks.](http://arxiv.org/abs/2105.10190) | AngularGrad是一种新的优化器，通过考虑连续梯度的方向/角度行为来优化卷积神经网络的角度收敛。通过捕捉角度信息以获得更准确的步长，优化步骤变得更加平滑。 |
| [^42] | [Explainable AI by BAPC -- Before and After correction Parameter Comparison.](http://arxiv.org/abs/2103.07155) | 该论文介绍了一种通过纠正简单的基模型来解释AI预测的局部替代方法。研究结果表明，通过确定准确性损失、准确性和替代品忠实度之间的准确关系，可以得到理想大小的解释实例邻域，以实现最大的准确性和忠实度。 |
| [^43] | [ResNet After All? Neural ODEs and Their Numerical Solution.](http://arxiv.org/abs/2007.15386) | 神经ODE模型的性能取决于训练过程中使用的数值方法，如果使用过于粗糙的解算器进行训练，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性下降。 |
| [^44] | [A Unifying Framework of Bilinear LSTMs.](http://arxiv.org/abs/1910.10294) | 本文提出了一个统一的双线性LSTM框架，通过平衡线性和双线性项的表达能力，实现了对序列数据集中输入特征的非线性交互的利用，以实现更好的性能，同时不增加更多的学习参数。 |

# 详细

[^1]: 关于Tukey深度的随机近似质量

    On the quality of randomized approximations of Tukey's depth. (arXiv:2309.05657v1 [stat.ML])

    [http://arxiv.org/abs/2309.05657](http://arxiv.org/abs/2309.05657)

    本文研究了Tukey深度的随机近似质量问题，证明了在维度较高且数据从对数凹集的均匀分布中抽样的情况下，随机算法可以正确近似最大深度和接近零的深度，而对于中间深度的点，任何好的近似都需要指数复杂度。

    

    Tukey深度（或半空间深度）是用于多元数据中心度量的广泛应用的指标。然而，在高维度下，Tukey深度的精确计算被认为是一个困难的问题。为了解决这个问题，人们提出了Tukey深度的随机近似方法。在本文中，我们探讨了这样的随机算法何时能够返回一个良好的Tukey深度近似。我们研究了数据从对数凹陷均匀分布中抽样的情况。我们证明了，如果要求算法在维度上以多项式时间运行，随机算法可以正确地近似最大深度1/2和接近零的深度。另一方面，对于任何中间深度的点，任何好的近似都需要指数复杂度。

    Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
    
[^2]: 边界剥离：使用一类剥离的异常检测方法

    Boundary Peeling: Outlier Detection Method Using One-Class Peeling. (arXiv:2309.05630v1 [stat.ML])

    [http://arxiv.org/abs/2309.05630](http://arxiv.org/abs/2309.05630)

    一类边界剥离是一种无监督的异常检测算法，使用了平均有符号距离和灵活的边界生成方法。在无异常值和有异常值的情况下，一类边界剥离表现出优越的性能。

    

    无监督异常检测在数据分析中是至关重要的阶段，仍然是一个充满活力的研究领域。一个好的异常检测算法应该具备计算效率高、对调参选择鲁棒、在不同的数据分布下表现稳定等特点。我们介绍了一类边界剥离，一种无监督异常检测算法。一类边界剥离使用了一类支持向量机不断剥离的、灵活的边界生成的平均有符号距离。一类边界剥离具有鲁棒的超参数设置，并且为了增加灵活性，可以被看作是一个集成方法。在合成数据模拟中，一类边界剥离在没有异常值的情况下优于所有先进方法，并且在有异常值存在的情况下，与基准方法相比，在正确分类方面表现出可比或更好的性能。

    Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classificati
    
[^3]: 分享你的表示：在联邦学习中保证隐私和效用的权衡的改善

    Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning. (arXiv:2309.05505v1 [cs.LG])

    [http://arxiv.org/abs/2309.05505](http://arxiv.org/abs/2309.05505)

    在联邦学习中重复参数共享会泄露私人数据，本文提出了一种表示联邦学习目标，旨在在保障差分隐私的同时允许本地个性化，通过提出的新算法DPFEDREP，可以在线性表示设置中收敛到以全局为中心的球形区域。

    

    在联邦学习中重复参数共享会导致私人数据的显著信息泄露，从而违背了数据隐私的主要目的。尽管使用最先进的差分隐私算法可以减轻信息泄露的风险，但这并非没有代价。随机化机制可能会阻止模型对有用的表示函数的学习收敛，尤其是当本地模型之间在分类函数上存在更大的不一致性时（由于数据异构性）。本文考虑一种表示联邦学习目标，鼓励各方在保障差分隐私的同时共同改进模型的共识部分，并且单独允许足够的自由进行本地个性化（无需共享）。我们证明在线性表示设置中，虽然目标是非凸的，我们提出的新算法DPFEDREP会收敛到以全局为中心的球形区域。

    Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy. Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free. Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it). We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global o
    
[^4]: 应用于新生儿脑MRI分割的综合合成学习的分析

    Comprehensive analysis of synthetic learning applied to neonatal brain MRI segmentation. (arXiv:2309.05306v1 [stat.ML])

    [http://arxiv.org/abs/2309.05306](http://arxiv.org/abs/2309.05306)

    本研究针对新生儿脑MRI图像的分割任务，通过综合合成学习的方法训练了对图像对比度变化和解剖结构的空间配置具有鲁棒性的模型。在对超过700名婴儿的高质量图像数据进行实验后，我们发现标准Unet模型在少数T2加权体积上表现出惊人的性能，但这些模型学习到的特征与训练领域的强度相关。

    

    由于脑结构形状和信号强度变化较大反映胎儿发育过程的变化，新生儿MRI图像的脑分割是一项非常具有挑战性的任务。在这种背景下，需要具有对图像对比度变化和解剖结构的空间配置的鲁棒性的分割技术。在这项工作中，我们评估了合成学习的潜力，即使用从非常少数受试者的地面真实标签生成的合成图像训练的独立于对比度的模型。我们基于发展性人类连接组计划发布的数据集进行实验，该数据集提供了超过700名26至45周孕龄的婴儿的高质量T1和T2加权图像。首先，我们证实了仅基于少数T2加权体积训练的标准Unet的令人印象深刻的性能，但也证实了这类模型学习特定于训练域的强度相关特征。然后我们进一步评估了许多基于合成学习的改进策略。

    Brain segmentation from neonatal MRI images is a very challenging task due to large changes in the shape of cerebral structures and variations in signal intensities reflecting the gestational process. In this context, there is a clear need for segmentation techniques that are robust to variations in image contrast and to the spatial configuration of anatomical structures. In this work, we evaluate the potential of synthetic learning, a contrast-independent model trained using synthetic images generated from the ground truth labels of very few subjects.We base our experiments on the dataset released by the developmental Human Connectome Project, for which high-quality T1- and T2-weighted images are available for more than 700 babies aged between 26 and 45 weeks post-conception. First, we confirm the impressive performance of a standard Unet trained on a few T2-weighted volumes, but also confirm that such models learn intensity-related features specific to the training domain. We then ev
    
[^5]: 深入研究混杂后验参数问题

    The fine print on tempered posteriors. (arXiv:2309.05292v1 [cs.LG])

    [http://arxiv.org/abs/2309.05292](http://arxiv.org/abs/2309.05292)

    这项研究深入探讨了混杂后验参数的问题，发现了一些重要的新观点。研究结果表明，在实际情况下，随机性通常不能提高测试准确性，并且对于某些贝叶斯模型，增加随机性反而会降低测试准确性。此外，研究还解释了优化目标中温度参数的重要性，并指出温度不能简单地被视为修正了先验或似然的错误设定。

    

    我们对混杂后验参数进行了详细调查，发现了一些关键而以前未被讨论的问题。与以往的结果相反，我们首先证明，在实际模型和数据集以及对后验的紧密控制的Laplace近似情况下，随机性通常并不能提高测试准确性。最低的温度通常是最优的。人们可能会认为，具有一定随机性的贝叶斯模型至少能在校准方面取得改进。然而，我们通过实验证明，当获得增益时，这是以测试准确性的降低为代价的。然后，我们讨论了使用贝叶斯模型以目标频率主义指标提供对优化目标中温度参数λ的简单解释的问题。与之前的研究相反，我们最终通过PAC-Bayesian分析表明，温度λ不能简单地被视为修正了先验或似然的错误设定。

    We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.
    
[^6]: 通过协同扩散恢复似然学习基于能量的模型

    Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])

    [http://arxiv.org/abs/2309.05153](http://arxiv.org/abs/2309.05153)

    本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。

    

    在高维数据上使用最大似然估计训练能量基准模型（EBMs）可能具有挑战性且耗时较长。因此，EBMs和其他生成框架（如GANs和扩散模型）之间存在明显的样本质量差距。为了弥补这一差距，受最近通过最大化扩散恢复似然（DRL）来学习EBMs的努力的启发，我们提出了协同扩散恢复似然（CDRL），一种有效的方法来可行地学习和从一系列EBMs中进行采样，这些EBMs定义在越来越嘈杂的数据集版本上，并与每个EBM的初始化模型配对。在每个噪声水平上，初始化模型学习在EBM的采样过程中分摊，而两个模型在协同训练框架内共同估计。初始化模型生成的样本作为起始点，经过EBM的几个采样步骤进行改进。通过改进后的样本，通过最大化恢复似然来优化EBM。

    Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
    
[^7]: 异常值鲁棒对抗训练

    Outlier Robust Adversarial Training. (arXiv:2309.05145v1 [cs.LG])

    [http://arxiv.org/abs/2309.05145](http://arxiv.org/abs/2309.05145)

    异常值鲁棒对抗训练（ORAT）是一种同时处理训练数据质量和推理时间对抗攻击的模型，采用了鲁棒的排名损失函数，具有较好的泛化性能。

    

    监督学习模型受到训练数据的固有复杂性的挑战，如异常值和少数子群，并且还受到推理时间的有意攻击，其中使用对抗样本。虽然传统的鲁棒学习方法和最近的对抗训练方法分别设计用于处理这两个挑战，但迄今为止，还没有研究开发出同时对训练数据质量低和推理时间潜在对抗攻击具有鲁棒性的模型。出于这个原因，我们在这项工作中介绍了异常值鲁棒对抗训练（ORAT）。ORAT基于一个双层优化公式的对抗训练，采用鲁棒的基于排名的损失函数。理论上，我们证明了ORAT的学习目标满足二分类的H-一致性，从而将其确立为对抗0/1损失的合适替代。此外，我们分析了它的泛化能力，并提供了统一的解释。

    Supervised learning models are challenged by the intrinsic complexities of training data such as outliers and minority subpopulations and intentional attacks at inference time with adversarial samples. While traditional robust learning methods and the recent adversarial training approaches are designed to handle each of the two challenges, to date, no work has been done to develop models that are robust with regard to the low-quality training data and the potential adversarial attack at inference time simultaneously. It is for this reason that we introduce Outlier Robust Adversarial Training (ORAT) in this work. ORAT is based on a bi-level optimization formulation of adversarial training with a robust rank-based loss function. Theoretically, we show that the learning objective of ORAT satisfies the $\mathcal{H}$-consistency in binary classification, which establishes it as a proper surrogate to adversarial 0/1 loss. Furthermore, we analyze its generalization ability and provide uniform
    
[^8]: DAD++：改进的无数据测试时对抗防御方法

    DAD++: Improved Data-free Test Time Adversarial Defense. (arXiv:2309.05132v1 [cs.CV])

    [http://arxiv.org/abs/2309.05132](http://arxiv.org/abs/2309.05132)

    DAD++是一种改进的无数据测试时对抗防御方法，通过包含检测和修正框架以提高深度神经网络的鲁棒性。同时，引入了软检测方案以增强修正框架在检测器置信度不足时的效果。

    

    随着深度神经网络在诸如自动驾驶汽车、医学影像、异常检测等安全关键应用中的日益部署，对抗鲁棒性已成为这些网络在实际场景中可靠性的关键问题。许多基于对抗训练和正则化技术的工作已被提出，以使这些深度网络对抗攻击具有鲁棒性。然而，这些方法要求重新训练模型或从头开始训练，使得在训练数据访问受限的情况下保护预训练模型变得不可行。为了解决这个问题，我们提出了一个包含检测和修正框架的测试时无数据对抗防御方法（DAD）。此外，为了在检测器置信度不足时进一步提高修正框架的有效性，我们提出了一种软检测方案（称为"DAD++"）。我们在几个数据集上进行了广泛的实验和消融分析。

    With the increasing deployment of deep neural networks in safety-critical applications such as self-driving cars, medical imaging, anomaly detection, etc., adversarial robustness has become a crucial concern in the reliability of these networks in real-world scenarios. A plethora of works based on adversarial training and regularization-based techniques have been proposed to make these deep networks robust against adversarial attacks. However, these methods require either retraining models or training them from scratch, making them infeasible to defend pre-trained models when access to training data is restricted. To address this problem, we propose a test time Data-free Adversarial Defense (DAD) containing detection and correction frameworks. Moreover, to further improve the efficacy of the correction framework in cases when the detector is under-confident, we propose a soft-detection scheme (dubbed as "DAD++"). We conduct a wide range of experiments and ablations on several datasets 
    
[^9]: 使用核岭回归的非线性 Granger 因果关系

    Nonlinear Granger Causality using Kernel Ridge Regression. (arXiv:2309.05107v1 [stat.ML])

    [http://arxiv.org/abs/2309.05107](http://arxiv.org/abs/2309.05107)

    使用核岭回归的 mlcausality 是一种新的算法和Python库，用于识别非线性 Granger 因果关系，具有竞争力的性能和更精细校准的 p 值，以及显著降低的计算时间。

    

    我引入了一种名为 mlcausality 的新算法和伴随的Python库，用于识别非线性 Granger 因果关系。该新算法使用了灵活的插件架构，使研究人员能够将任何非线性回归器作为基本的预测模型。随后，我对 mlcausality 进行了全面的性能分析，其中预测回归器为带有径向基函数核的核岭回归。结果表明，在多样化的模拟数据集上，mlcausality 使用核岭回归获得了竞争力的AUC得分。此外，与竞争算法相比，使用核岭回归的 mlcausality 产生更精细校准的 p 值。这种优化使得 mlcausality 在使用直观的 p 值阈值准则时获得了更高的准确度得分。最后，使用核岭回归的 mlcausality 显著降低了计算时间。

    I introduce a novel algorithm and accompanying Python library, named mlcausality, designed for the identification of nonlinear Granger causal relationships. This novel algorithm uses a flexible plug-in architecture that enables researchers to employ any nonlinear regressor as the base prediction model. Subsequently, I conduct a comprehensive performance analysis of mlcausality when the prediction regressor is the kernel ridge regressor with the radial basis function kernel. The results demonstrate that mlcausality employing kernel ridge regression achieves competitive AUC scores across a diverse set of simulated data. Furthermore, mlcausality with kernel ridge regression yields more finely calibrated $p$-values in comparison to rival algorithms. This enhancement enables mlcausality to attain superior accuracy scores when using intuitive $p$-value-based thresholding criteria. Finally, mlcausality with the kernel ridge regression exhibits significantly reduced computation times compared 
    
[^10]: 具有有界更新的迭代学习算法的泛化误差界限

    Generalization error bounds for iterative learning algorithms with bounded updates. (arXiv:2309.05077v1 [cs.LG])

    [http://arxiv.org/abs/2309.05077](http://arxiv.org/abs/2309.05077)

    本文研究了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，提出了一种新颖的泛化误差界限，利用了信息论技术。研究表明，在模型维度和训练数据样本数量相等的情况下，界限得到了改善。

    

    本文探讨了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，采用了信息论技术。我们的主要贡献是针对具有有界更新的算法提出了一种新颖的泛化误差界限，超出了以前只关注随机梯度下降（SGD）的范围。我们的方法引入了两个主要的创新之处：1）我们将互信息重新定义为更新的不确定性，提供了一种新的视角；2）我们不使用互信息的链式法则，而是采用方差分解技术来将信息分解到迭代中，从而允许简化的代理过程。我们在各种设置下分析了我们的泛化界限，并在模型维度以与训练数据样本数量相同的速率增加时展示了改进的界限。为了弥合理论与实践之间的差距，我们还研究了先前观察到的情况。

    This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously obse
    
[^11]: 去殖民化的人工智能对齐：威色达尔玛、论证和艺术表达

    Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])

    [http://arxiv.org/abs/2309.05030](http://arxiv.org/abs/2309.05030)

    本文提出了去殖民化人工智能对齐的三个建议：改变基本道德哲学为达尔玛哲学，允许多元主义的论证传统存在于对齐技术中，以及将价值认识论扩展到超越自然语言中的指令。

    

    先前的研究已经阐明了人工智能（AI）开发和部署的殖民性。然而，这些研究很少涉及到对齐：即基于细致的人类反馈，调整大型语言模型（LLM）的行为与期望值一致。除了其他实践，殖民主义还有一部分是改变被殖民民族的信仰和价值观的历史；而当前的LLM对齐实践正是这一历史的复制。我们建议通过三个提议对AI对齐进行去殖民化：（a）将基本道德哲学从西方哲学转变为达尔玛哲学，（b）在对齐技术中允许论证和多元主义的传统，以及（c）将价值的认识论扩展到超越自然语言中的指令或命令。

    Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
    
[^12]: SA-Solver：用于快速采样扩散模型的随机亚当求解器

    SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])

    [http://arxiv.org/abs/2309.05019](http://arxiv.org/abs/2309.05019)

    本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。

    

    扩散概率模型在生成任务中取得了相当大的成功。由于从扩散概率模型中进行采样相当于解扩散随机微分方程或常微分方程，这是一项耗时的工作，因此提出了许多基于改进的微分方程求解器的快速采样方法。这些技术中的大部分方法都考虑解扩散常微分方程，因为它具有更好的效率。然而，随机采样可以在生成多样化和高质量数据方面提供额外的优势。在这项工作中，我们从两个方面进行了对随机采样的综合分析：方差控制的扩散随机微分方程和线性多步扩散随机微分方程求解器。基于我们的分析，我们提出了SA-Solver，它是一种改进的高效随机亚当方法，用于解扩散随机微分方程以生成高质量的数据。我们的实验结果显示，SA-Solver实现了：1）在少步采样中与现有最先进的采样方法相比，有改进或可比性能；2）SOTA FID分数。

    Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
    
[^13]: 渐变优化和变分不等式在机器学习中的温和介绍

    A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])

    [http://arxiv.org/abs/2309.04877](http://arxiv.org/abs/2309.04877)

    这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。

    

    近年来机器学习的快速发展基于与渐变优化的紧密联系。进一步的进展部分取决于从模式识别到决策和多智能体问题的转变。在这些更广泛的背景下，涉及均衡和博弈论而不是极值的新的数学挑战出现了。基于梯度的方法仍然至关重要--考虑到机器学习问题的高维度和大规模--但简单的梯度下降不再是算法设计的出发点。我们提供了一个对机器学习中基于梯度的算法的更广泛框架的温和介绍，从鞍点和单调博弈开始，然后到一般的变分不等式。虽然我们对所提出的几个算法进行了收敛性证明，但我们的主要关注点是提供动机和直观理解。

    The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
    
[^14]: 梯度下降训练的神经网络的近似结果

    Approximation Results for Gradient Descent trained Neural Networks. (arXiv:2309.04860v1 [cs.LG])

    [http://arxiv.org/abs/2309.04860](http://arxiv.org/abs/2309.04860)

    该论文研究了采用梯度下降训练的神经网络的近似保证，利用连续的误差范数对网络进行分析，并发现在欠参数化的情况下相对于已有的逼近方法存在逼近率下降的问题。

    

    该论文对采用梯度流训练的神经网络进行了近似保证，其中误差以连续的$L_2(\mathbb{S}^{d-1})$范数在$d$维单位球面上测量，目标为Sobolev平滑。网络是完全连接的，深度恒定，宽度递增。虽然所有层都进行了训练，但梯度流的收敛性是基于对于非凸的倒数第二层的神经切向核(NTK)的论证。与标准的NTK分析不同，连续误差范数暗示了一个欠参数化的区域，在逼近时需要自然的光滑性假设。典型的过参数化通过逼近率的损失以及相对于Sobolev平滑函数的已建立的逼近方法而重新进入结果中。

    The paper contains approximation guarantees for neural networks that are trained with gradient flow, with error measured in the continuous $L_2(\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets that are Sobolev smooth. The networks are fully connected of constant depth and increasing width. Although all layers are trained, the gradient flow convergence is based on a neural tangent kernel (NTK) argument for the non-convex second but last layer. Unlike standard NTK analysis, the continuous error norm implies an under-parametrized regime, possible by the natural smoothness assumption required for approximation. The typical over-parametrization re-enters the results in form of a loss in approximation rate relative to established approximation methods for Sobolev smooth functions.
    
[^15]: 非线性维度降低在因子扩展向量自回归中的应用

    Non-linear dimension reduction in factor-augmented vector autoregressions. (arXiv:2309.04821v1 [econ.EM])

    [http://arxiv.org/abs/2309.04821](http://arxiv.org/abs/2309.04821)

    本文引入了非线性维度降低方法，在因子扩展向量自回归中分析经济冲击，证明了在经济周期动荡和高度波动数据下，该方法具有良好的预测性能，并能处理COVID-19疫情引起的离群值。

    

    本文将非线性维度降低方法引入因子扩展向量自回归模型，分析不同经济冲击的影响。作者认为在经济周期动荡时，控制大维度数据集与潜在因子之间的非线性关系尤为有用。通过模拟实验证明，非线性维度降低技术在数据高度波动时具有良好的预测性能。在实证应用中，本文排除和包含COVID-19疫情观测，确定了货币政策和不确定性冲击。这两个应用表明非线性FAVAR方法能够处理COVID-19疫情引起的极大离群值，并在两种情景下获得可靠结果。

    This paper introduces non-linear dimension reduction in factor-augmented vector autoregressions to analyze the effects of different economic shocks. I argue that controlling for non-linearities between a large-dimensional dataset and the latent factors is particularly useful during turbulent times of the business cycle. In simulations, I show that non-linear dimension reduction techniques yield good forecasting performance, especially when data is highly volatile. In an empirical application, I identify a monetary policy as well as an uncertainty shock excluding and including observations of the COVID-19 pandemic. Those two applications suggest that the non-linear FAVAR approaches are capable of dealing with the large outliers caused by the COVID-19 pandemic and yield reliable results in both scenarios.
    
[^16]: 神经潜在几何搜索：通过格罗莫夫-豪斯多夫信息驱动的贝叶斯优化来进行乘积流形推断

    Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])

    [http://arxiv.org/abs/2309.04810](http://arxiv.org/abs/2309.04810)

    本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。

    

    最近的研究表明，通过将潜在空间的几何结构与底层数据结构对齐，可以提高机器学习模型的性能。研究人员提出使用具有恒定曲率的双曲和球形空间，或者它们的组合，来更好地建模潜在空间并增强模型性能，而不仅仅依赖于欧几里得空间。然而，目前对自动识别下游任务的最佳潜在几何结构问题还没有给予足够关注。我们在数学上定义了这个新颖的问题，并将其称为神经潜在几何搜索(NLGS)。具体而言，我们引入了一种基于格罗莫夫-豪斯多夫距离的候选潜在几何结构之间的新概念距离，以实现这一目标。为了计算格罗莫夫-豪斯多夫距离，我们提出了一种通过最小查询评估搜索由恒定曲率模型空间乘积组成的潜在几何结构的原则方法。

    Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
    
[^17]: 改进ReLU网络的预测不确定性的仿射不变集成变换方法

    Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks. (arXiv:2309.04742v1 [stat.ML])

    [http://arxiv.org/abs/2309.04742](http://arxiv.org/abs/2309.04742)

    本文提出了一种仿射不变集成变换方法，可以改善在ReLU网络中的预测不确定性。通过基于集合卡尔曼滤波的贝叶斯推断，我们提出了两个相互作用的粒子系统，并证明了它们的收敛性。同时，我们还探讨了这些方法用于量化预测不确定性的有效性。

    

    我们考虑使用合适的集合卡尔曼滤波的扩展进行逻辑回归的贝叶斯推断问题。我们提出了两个相互作用的粒子系统，从近似后验分布中采样，并证明当粒子数量趋于无穷时，这些相互作用粒子系统收敛到均场极限的量化收敛速率。此外，我们应用这些技术并考察它们作为贝叶斯近似方法在ReLU网络中量化预测不确定性的有效性。

    We consider the problem of performing Bayesian inference for logistic regression using appropriate extensions of the ensemble Kalman filter. Two interacting particle systems are proposed that sample from an approximate posterior and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity. Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks.
    
[^18]: 使用可伸缩分类器的概率安全区域

    Probabilistic Safety Regions Via Finite Families of Scalable Classifiers. (arXiv:2309.04627v1 [stat.ML])

    [http://arxiv.org/abs/2309.04627](http://arxiv.org/abs/2309.04627)

    本论文提出了概率安全区域的概念，用于描述一个输入空间子集，在这个子集中，误分类实例的数量可以被概率上得到控制。同时，利用可伸缩分类器来将机器学习的调参与误差控制相结合。

    

    监督分类可以识别数据中的模式以分离不同的行为类别。然而，机器学习的数值逼近性质决定了分类算法上的误差问题。数据分析师可能会通过减小某个类别的错误来增加其他类别的错误。然而，这种设计阶段的误差控制通常以启发式的方式进行。因此，有必要发展一种理论基础，能够对获得的分类器进行概率证明。在这个视角下，我们引入了概率安全区域的概念，用来描述一个输入空间子集，其中误分类实例的数量可以概率上得到控制。然后，我们利用可伸缩分类器来将机器学习的调参与误差控制相结合。通过合成数据提供了多种测试来验证该方法，以突出所有步骤。

    Supervised classification recognizes patterns in the data to separate classes of behaviours. Canonical solutions contain misclassification errors that are intrinsic to the numerical approximating nature of machine learning. The data analyst may minimize the classification error on a class at the expense of increasing the error of the other classes. The error control of such a design phase is often done in a heuristic manner. In this context, it is key to develop theoretical foundations capable of providing probabilistic certifications to the obtained classifiers. In this perspective, we introduce the concept of probabilistic safety region to describe a subset of the input space in which the number of misclassified instances is probabilistically controlled. The notion of scalable classifiers is then exploited to link the tuning of machine learning with error control. Several tests corroborate the approach. They are provided through synthetic data in order to highlight all the steps invo
    
[^19]: 低秩度度量学习中的感知调整查询和反向测量范式

    Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning. (arXiv:2309.04626v1 [stat.ML])

    [http://arxiv.org/abs/2309.04626](http://arxiv.org/abs/2309.04626)

    感知调整查询（PAQ）是一种新的用于收集人类反馈的查询机制，采用反向测量方案，结合了基数查询和序数查询的优点。我们将PAQ应用于度量学习问题中，通过PAQ测量来学习未知的马氏距离，并开发了一个两阶段估计器，提供了样本复杂性保证。

    

    我们引入了一种新的用于收集人类反馈的查询机制，称为感知调整查询（PAQ）。PAQ采用了反向测量方案，既具有信息量又轻量级，结合了基数查询和序数查询的优点。我们将PAQ展示在度量学习问题中，利用PAQ测量来学习未知的马氏距离。这导致了一个高维低秩矩阵估计问题，无法应用标准矩阵估计器。因此，我们开发了一个从PAQ中学习度量的两阶段估计器，并提供了该估计器的样本复杂性保证。我们通过数值模拟展示了该估计器的性能和显著特性。

    We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
    
[^20]: 使用路径签名生成逼近实际数据的金融价格路径

    Generating drawdown-realistic financial price paths using path signatures. (arXiv:2309.04507v1 [q-fin.CP])

    [http://arxiv.org/abs/2309.04507](http://arxiv.org/abs/2309.04507)

    本文提出了一种新颖的机器学习方法，使用路径签名来生成逼近实际数据的金融价格路径，并应用于定价回撤保险期权和投资组合回撤控制策略。

    

    本文提出了一种新颖的生成式机器学习方法，用于模拟具有接近实际数据的回撤的金融价格序列。应用场景如定价回撤保险期权或开发投资组合回撤控制策略需要使用接近真实的回撤路径。历史情景可能不足以有效训练和回测策略，而标准的参数化蒙特卡罗方法无法充分保留回撤。我们提倡一种非参数化蒙特卡罗方法，将变分自编码生成模型与回撤重建损失函数相结合。为了克服数值复杂性和非可微性问题，我们将回撤近似为路径的矩函数，即路径签名。我们证明了回撤函数的所需正则性和近似的一致性。此外，我们使用线性回归获得了接近的数值近似解。

    A novel generative machine learning approach for the simulation of sequences of financial price data with drawdowns quantifiably close to empirical data is introduced. Applications such as pricing drawdown insurance options or developing portfolio drawdown control strategies call for a host of drawdown-realistic paths. Historical scenarios may be insufficient to effectively train and backtest the strategy, while standard parametric Monte Carlo does not adequately preserve drawdowns. We advocate a non-parametric Monte Carlo approach combining a variational autoencoder generative model with a drawdown reconstruction loss function. To overcome issues of numerical complexity and non-differentiability, we approximate drawdown as a linear function of the moments of the path, known in the literature as path signatures. We prove the required regularity of drawdown function and consistency of the approximation. Furthermore, we obtain close numerical approximations using linear regression for fr
    
[^21]: 通过谱方法改进了排名聚合的理论保证

    Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])

    [http://arxiv.org/abs/2309.03808](http://arxiv.org/abs/2309.03808)

    本论文通过谱方法改进了排名聚合问题的理论保证，通过研究基于未归一化和归一化数据矩阵的谱排名算法，提供了更准确的扰动误差界限。

    

    给定多个项目之间的成对比较，如何对它们进行排名以使得排名与观察相匹配？这个问题被称为排名聚合，在体育、推荐系统和其他网络应用中已经找到了许多应用。由于找到最小化不匹配的全局排名通常是NP困难的（称为Kemeny优化），我们将重点放在Erd\"os-R\'enyi离群点（ERO）模型上。在这个排名问题中，每个成对比较是真实分数差异的被损坏副本。我们研究了基于未归一化和归一化数据矩阵的谱排名算法。关键是理解它们在从观察数据中恢复每个项目的潜在分数方面的性能。这归结为推导未归一化/归一化数据矩阵的前几个特征向量和其总体对应物之间的逐个项扰动误差界限。通过使用留出技术，我们提供了一个更准确的$\ell_{\infty}$-norm扰动误差界限。

    Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbati
    
[^22]: 大尺度和无穷宽度下的深度学习勒让演讲

    Les Houches Lectures on Deep Learning at Large & Infinite Width. (arXiv:2309.01592v1 [stat.ML])

    [http://arxiv.org/abs/2309.01592](http://arxiv.org/abs/2309.01592)

    本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。

    

    这些演讲是在2022年勒让夏季学校统计物理和机器学习课程上展示的，着重探讨了深度神经网络在无限宽度和大宽度范围内的情况。涵盖的主题包括这些网络的各种统计和动力学特性。特别是，讲师们讨论了随机深度神经网络的特性；训练过的深度神经网络，线性模型，核函数和高斯过程之间的联系，这些联系在无穷宽度的极限下出现；以及在初始化和训练后对大但有限宽度网络的摄动和非摄动处理。

    These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
    
[^23]: 条件生存预测中的面积规范COBRA

    Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])

    [http://arxiv.org/abs/2309.00417](http://arxiv.org/abs/2309.00417)

    本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。

    

    本文探讨了一种不同的组合回归策略来计算条件生存函数。我们使用基于回归的弱学习器来创建所提出的集成技术。所提出的组合回归策略使用相似度度量作为两个生存曲线之间的面积。所提出的模型表明其表现优于随机生存森林。本文讨论了一种在组合回归设置中选择最重要变量的新技术。我们进行了一项模拟研究，表明我们对变量相关性的提议效果很好。我们还使用了三个真实数据集来说明该模型。

    The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
    
[^24]: 在线GentleAdaBoost -- 技术报告

    Online GentleAdaBoost -- Technical Report. (arXiv:2308.14004v1 [stat.ML])

    [http://arxiv.org/abs/2308.14004](http://arxiv.org/abs/2308.14004)

    该论文研究了在线版的GentleAdaboost算法，通过在线方式将弱分类器与强分类器结合，提出了一种通过线搜索将批处理方法扩展为在线方法的方法，并与其他在线方法在各种基准数据集上进行了对比。

    

    我们研究了在线版的GentleAdaboost，其中我们以在线方式将一个弱分类器与一个强分类器结合起来。我们提供了一种通过线搜索应用的方法，将批处理方法扩展为在线方法，并通过理论证明了其正确性。最后，我们在各种基准数据集上比较了我们的在线boosting方法与其他在线方法。

    We study the online variant of GentleAdaboost, where we combine a weak learner to a strong learner in an online fashion. We provide an approach to extend the batch approach to an online approach with theoretical justifications through application of line search. Finally we compare our online boosting approach with other online approaches across a variety of benchmark datasets.
    
[^25]: 抓住它们：图匹配匹配滤波中的解决方案多样化

    Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])

    [http://arxiv.org/abs/2308.13451](http://arxiv.org/abs/2308.13451)

    本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。

    

    我们提出了一种在非常大的背景图中查找多个嵌入在其中的模板图的新方法。我们的方法基于Sussman等人提出的图匹配匹配滤波技术，通过在匹配滤波算法中迭代地惩罚合适的节点对相似度矩阵来实现多样化匹配的发现。此外，我们提出了算法加速，极大地提高了我们的匹配滤波方法的可扩展性。我们在相关的Erdos-Renyi图设置中对我们的方法进行了理论上的验证，显示其在温和的模型条件下能够顺序地发现多个模板。我们还通过使用模拟模型和真实世界数据集（包括人脑连接组和大型交易知识库）进行了大量实验证明了我们方法的实用性。

    We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
    
[^26]: SciRE-Solver: 用得分积分求解器和递归导数估计快速采样扩散概率模型

    SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])

    [http://arxiv.org/abs/2308.07896](http://arxiv.org/abs/2308.07896)

    SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。

    

    扩散概率模型(DPMs)是一类强大的生成模型，以其生成高保真图像样本的能力而闻名。DPMs的实现面临的主要挑战是采样过程缓慢。在这项工作中，我们提出了一种高效的DPMs采样器。具体而言，我们针对与DPMs采样过程对应的扩散ODE提出了一个基于得分的精确解决方案范式，该范式为求解扩散ODE的数值算法开发提供了新的视角。为了实现高效的采样器，我们提出了一种递归导数估计(RDE)方法来减小估计误差。通过我们提出的解决方案范式和RDE方法，我们提出了具有收敛顺序保证的得分积分求解器(SciRE-Solver)来解决扩散ODEs。SciRE-Solver在离散时间和连续时间DPMs上获得了最先进的采样性能，并且仅需有限数量的得分函数评估(NFE)。

    Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
    
[^27]: InVAErt网络：一个数据驱动的框架用于仿真、推理和可识别性分析。

    InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis. (arXiv:2307.12586v1 [cs.LG])

    [http://arxiv.org/abs/2307.12586](http://arxiv.org/abs/2307.12586)

    InVAErt网络是一个数据驱动的框架，用于分析和合成物理系统，具有模型反演和可识别性分析的能力。

    

    目前，基于物理的系统使用生成模型和深度学习主要用于仿真任务。然而，数据驱动结构提供的出色灵活性表明应将该表示扩展到系统综合的其他方面，包括模型反演和可识别性。我们引入了InVAErt网络，这是一个综合的数据驱动分析和合成参数化物理系统的框架，它使用确定性编码器和解码器表示前向和逆向解映射，用归一化流来捕捉系统输出的概率分布，并设计了一种变分编码器来学习输入和输出之间缺乏双射性的紧凑潜在表示。我们正式研究了损失函数中惩罚系数的选择和潜在空间采样策略，因为我们发现这些因素会显著影响训练和测试性能。我们有效地验证了我们的模型。

    Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We valid
    
[^28]: 朝着可信的解释：因果关系解释论文研究

    Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])

    [http://arxiv.org/abs/2306.14115](http://arxiv.org/abs/2306.14115)

    该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。

    

    随着自然语言处理的最新进展，解释成为了通过选择输入文本的子集来解释黑盒模型中主要变化的一个基本的自我解释图。然而，现有的基于关联的解释方法在两个或多个片段高度互相关联时无法识别真正的解释，因此对预测准确性提供类似的贡献，所谓的虚假性。为了解决这一限制，我们从因果推断的角度新颖地将两个因果期望值（非虚假性和效率）引入了解释中。我们根据一种新提出的解释结构因果模型定义了一系列的因果概率，通过其理论鉴定，建立了必要和充分解释的主要组成部分。我们在真实世界的评论和医疗数据集上证明了所提出的因果关系解释的优越性能。

    With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
    
[^29]: 估算基于证据决策的价值

    Estimating the Value of Evidence-Based Decision Making. (arXiv:2306.13681v1 [stat.ME])

    [http://arxiv.org/abs/2306.13681](http://arxiv.org/abs/2306.13681)

    本文提出了一个实证框架，用于估算证据决策的价值和统计精度投资回报。

    

    商业/政策决策通常基于随机实验和观察性研究的证据。本文提出了一个实证框架来估算基于证据的决策（EBDM）的价值和统计精度投资回报。

    Business/policy decisions are often based on evidence from randomized experiments and observational studies. In this article we propose an empirical framework to estimate the value of evidence-based decision making (EBDM) and the return on the investment in statistical precision.
    
[^30]: 基于神经网络的贝叶斯数值积分方法

    Bayesian Numerical Integration with Neural Networks. (arXiv:2305.13248v1 [stat.ML])

    [http://arxiv.org/abs/2305.13248](http://arxiv.org/abs/2305.13248)

    本文提出了一种基于神经网络架构的贝叶斯数值积分方法，称为贝叶斯 Stein 神经网络。该方法可高效地编码积分先验信息并计算积分估计的不确定性。在实际问题中，该方法展现出数量级的加速的优势。

    

    贝叶斯概率数值方法对于数值积分具有显著优势：可以编码积分的先验信息，可以量化积分估计的不确定性。但是，这类方法中最流行的贝叶斯积分算法（Bayesian Quadrature）基于高斯过程模型，因此计算成本很高。为了提高可扩展性，我们提出了一种基于贝叶斯神经网络的替代方法，称为贝叶斯 Stein 神经网络。关键成分是基于 Stein 算子的神经网络结构，以及基于 Laplace 近似的贝叶斯后验的近似。我们展示了这导致了在流行的 Genz 函数基准测试和在贝叶斯动力系统分析以及大规模风力发电预测中规模的数量级加速。

    Bayesian probabilistic numerical methods for numerical integration offer significant advantages over their non-Bayesian counterparts: they can encode prior information about the integrand, and can quantify uncertainty over estimates of an integral. However, the most popular algorithm in this class, Bayesian quadrature, is based on Gaussian process models and is therefore associated with a high computational cost. To improve scalability, we propose an alternative approach based on Bayesian neural networks which we call Bayesian Stein networks. The key ingredients are a neural network architecture based on Stein operators, and an approximation of the Bayesian posterior based on the Laplace approximation. We show that this leads to orders of magnitude speed-ups on the popular Genz functions benchmark, and on challenging problems arising in the Bayesian analysis of dynamical systems, and the prediction of energy production for a large-scale wind farm.
    
[^31]: 量子小波变换：具有量子计算优势的神经网络中的重要应用

    Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation. (arXiv:2301.11936v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.11936](http://arxiv.org/abs/2301.11936)

    这篇论文提出了量子小波变换(QRT)作为神经网络中的重要应用，通过量子计算实现了对量子态的小波变换，并且可以高效地找到大型神经网络的稀疏可训练子网络。

    

    量子机器学习(QML)领域中的一个重要挑战是建立量子计算在加速神经网络等常见机器学习任务中的应用。小波变换一直是神经网络理论研究中的基本数学工具，但由于传统经典计算的数值实现需要指数级运行时间$\exp(O(D))$，因此小波变换的实际应用性受到限制，尤其在数据维度$D$增加时。为了解决这个问题，我们开发了量子小波变换(QRT)，它在量子计算的线性运行时间$O(D)$内实现了对量子态的小波变换。作为一个应用，我们还展示了利用QRT作为QML的基本子程序，可以高效地找到大型浅宽神经网络的稀疏可训练子网络，而无需对原始网络进行大规模优化。

    A significant challenge in the field of quantum machine learning (QML) is to establish applications of quantum computation to accelerate common tasks in machine learning such as those for neural networks. Ridgelet transform has been a fundamental mathematical tool in the theoretical studies of neural networks, but the practical applicability of ridgelet transform to conducting learning tasks was limited since its numerical implementation by conventional classical computation requires an exponential runtime $\exp(O(D))$ as data dimension $D$ increases. To address this problem, we develop a quantum ridgelet transform (QRT), which implements the ridgelet transform of a quantum state within a linear runtime $O(D)$ of quantum computation. As an application, we also show that one can use QRT as a fundamental subroutine for QML to efficiently find a sparse trainable subnetwork of large shallow wide neural networks without conducting large-scale optimization of the original network. This appli
    
[^32]: 高维变分推理的投影积分更新

    Projective Integral Updates for High-Dimensional Variational Inference. (arXiv:2301.08374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08374](http://arxiv.org/abs/2301.08374)

    该论文介绍了一种适用于高维变分推理的投影积分更新方法，并通过降低参数敏感性来实现更强健的预测。

    

    变分推理是一种贝叶斯推理的近似框架，通过优化简化的参数分布来代替完整的后验分布，从而改善预测中的量化不确定性。捕捉与训练数据一致的模型变化可以通过降低参数敏感性来实现更强健的预测。本研究引入了一种适用于变分推理的固定点最优化方法，当每个可行的对数密度可以表示为给定基函数的线性组合时，该方法生效。在这种情况下，优化器成为投影积分更新的一个不动点。当基函数跨越每个参数的二次函数时，可行密度为高斯分布，投影积分更新产生了准牛顿变分贝叶斯 (QNVB)。其他基函数和更新方法也是可能的。由于这些更新需要高维积分，本研究首先提出了一种高效的准随机积分序列用于均匀均匀均匀均匀均匀均匀积分。

    Variational inference is an approximation framework for Bayesian inference that seeks to improve quantified uncertainty in predictions by optimizing a simplified distribution over parameters to stand in for the full posterior. Capturing model variations that remain consistent with training data enables more robust predictions by reducing parameter sensitivity. This work introduces a fixed-point optimization for variational inference that is applicable when every feasible log density can be expressed as a linear combination of functions from a given basis. In such cases, the optimizer becomes a fixed-point of projective integral updates. When the basis spans univariate quadratics in each parameter, feasible densities are Gaussian and the projective integral updates yield quasi-Newton variational Bayes (QNVB). Other bases and updates are also possible. As these updates require high-dimensional integration, this work first proposes an efficient quasirandom quadrature sequence for mean-fie
    
[^33]: 自适应选择最优策略以提高随机试验的精度和功效

    Adaptive Selection of the Optimal Strategy to Improve Precision and Power in Randomized Trials. (arXiv:2210.17453v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.17453](http://arxiv.org/abs/2210.17453)

    本研究提供了一种自适应预设方法，以选择在随机试验中调整哪些变量，以及以何种形式进行调整，从而最大化精度，同时保持Ⅰ类错误控制。

    

    Benkeser等人展示了如何在随机试验中调整基线协变量，从而有意义地提高各种结果类型的精度。他们的研究建立在很长时间的历史基础上，始于1932年的R.A. Fisher，包括美国食品和药物管理局以及欧洲药品管理局最近的认可。本文着重探讨了一个重要的实际问题：如何选择调整方法，即哪些变量以及以何种形式，以最大化精度，同时保持Ⅰ类错误控制。Balzer等人以前提出了在TMLE中的自适应预设法，以灵活自动地从预先规定的集合中选择在小型试验（N < 40）中最大化经验效率的方法。为了避免在少数随机单位中过度拟合，之前的选择仅限于工作广义线性模型，调整单个协变量。现在，我们将自适应预设法针对具有许多随机单元的试验进行了调整。使用V-fold

    Benkeser et al. demonstrate how adjustment for baseline covariates in randomized trials can meaningfully improve precision for a variety of outcome types. Their findings build on a long history, starting in 1932 with R.A. Fisher and including more recent endorsements by the U.S. Food and Drug Administration and the European Medicines Agency. Here, we address an important practical consideration: *how* to select the adjustment approach -- which variables and in which form -- to maximize precision, while maintaining Type-I error control. Balzer et al. previously proposed *Adaptive Prespecification* within TMLE to flexibly and automatically select, from a prespecified set, the approach that maximizes empirical efficiency in small trials (N$<$40). To avoid overfitting with few randomized units, selection was previously limited to working generalized linear models, adjusting for a single covariate. Now, we tailor Adaptive Prespecification to trials with many randomized units. Using $V$-fold
    
[^34]: PopArt: 高效稀疏回归和优化稀疏线性摇臂的实验设计

    PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.15345](http://arxiv.org/abs/2210.15345)

    本文提出了一种称为PopArt的高效稀疏线性估计方法，相比于Lasso，在许多问题中具有更紧的$\ell_1$恢复保证，并基于此推导出稀疏线性摇臂算法，具有改进的遗憾上界。同时，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界。

    

    在稀疏线性摇臂中，学习代理按顺序选择一个动作并接收奖励反馈，而奖励函数线性依赖于动作的一些坐标的协变量。这在许多实际的顺序决策问题中都有应用。在本文中，我们提出了一种简单而计算高效的稀疏线性估计方法，称为PopArt，与Lasso（Tibshirani, 1996）相比，它在许多问题中具有更紧的$\ell_1$恢复保证。我们的界限自然地激发了一种凸实验设计准则，因此在计算上是高效的解决方法。基于我们的新估计器和设计准则，我们推导出稀疏线性摇臂算法，其在给定动作集的几何性方面相对于现有技术（Hao et al., 2020）具有改进的遗憾上界。最后，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界，这填补了上界和下界之间的差距。

    In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in pr
    
[^35]: 有限时间内使用线性函数逼近进行时序差异学习的分析：尾平均和正则化

    Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05918](http://arxiv.org/abs/2210.05918)

    本研究通过引入尾平均和正则化技术，对时序差异(TD)学习算法进行了有限时间行为的研究。我们得出结论，尾平均TD能以最优速率 $O(1/t)$ 收敛，并且初始误差衰减速率更快。此外，正则化的TD版本在具有病态特征的问题上很有用。

    

    本文研究了将流行的时序差异(TD)学习算法与尾平均相结合时的有限时间行为。我们在不需要关于底层投影TD不动点矩阵的特征值信息的步长选择下，推导了尾平均TD迭代的参数误差的有限时间界。我们的分析表明，尾平均TD以期望速率和高概率收敛于最优的 $O(1/t)$ 速率。此外，我们的界限展示了初始误差(偏差)的更快衰减速率，这是对所有迭代的平均值的改进。我们还提出并分析了一种结合正则化的TD变体。通过分析，我们得出结论认为正则化的TD版本在具有病态特征的问题上是有用的。

    We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
    
[^36]: 广义核正则化最小二乘法

    Generalized Kernel Regularized Least Squares. (arXiv:2209.14355v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14355](http://arxiv.org/abs/2209.14355)

    本论文提出了广义核正则化最小二乘法 (gKRLS)，解决了核正则化最小二乘法 (KRLS) 在当前使用中的两个限制：它的扩展能力不足，且即使在小规模数据集上，其计算代价也非常高昂。

    

    核正则化最小二乘法 (KRLS) 是一种流行的方法，用于灵活地估计具有复杂变量关系的模型。然而，其可用性因两个原因而受到许多研究人员的限制。首先，现有方法缺乏灵活性，不允许将KRLS与理论动机下的扩展如随机效应、未经正则化的固定效应或非高斯结果组合使用。其次，即使是规模较小的数据集，估计也非常计算密集。本文通过引入广义KRLS (gKRLS) 来解决这两个问题。我们指出，KRLS可以重新设定为分层模型，从而允许轻松推理和模块化模型构建，在其中KRLS可以与随机效应、样条和未经正则化的固定效应并用。在计算方面，我们还实现了随机草图方法，以极大地加速估计，并在估计质量上承担有限的惩罚。我们证明gKRLS可适用于具有大量样本的数据集的拟合。

    Kernel Regularized Least Squares (KRLS) is a popular method for flexibly estimating models that may have complex relationships between variables. However, its usefulness to many researchers is limited for two reasons. First, existing approaches are inflexible and do not allow KRLS to be combined with theoretically-motivated extensions such as random effects, unregularized fixed effects, or non-Gaussian outcomes. Second, estimation is extremely computationally intensive for even modestly sized datasets. Our paper addresses both concerns by introducing generalized KRLS (gKRLS). We note that KRLS can be re-formulated as a hierarchical model thereby allowing easy inference and modular model construction where KRLS can be used alongside random effects, splines, and unregularized fixed effects. Computationally, we also implement random sketching to dramatically accelerate estimation while incurring a limited penalty in estimation quality. We demonstrate that gKRLS can be fit on datasets with
    
[^37]: 具有相关权重的深度神经网络：高斯过程混合极限、重尾、稀疏性和可压缩性

    Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility. (arXiv:2205.08187v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.08187](http://arxiv.org/abs/2205.08187)

    本文研究了具有相关权重的深度神经网络的极限行为，发现无限宽度神经网络的每一层可以通过两个简单的量来刻画，当其中至少一层的量是非平凡的时候，得到了高斯过程的混合模型。

    

    本文研究了具有相关权重并通过高斯分布混合建模的无限宽度前馈深度神经网络的极限。网络的每个隐藏节点被分配一个非负随机变量，该随机变量控制该节点的输出权重的方差。我们对这些节点随机变量做了最小的假设：它们是独立同分布的，并且在无限宽度极限下，每一层的随机变量和收敛到一些有限的随机变量。在这个模型下，我们证明了无限宽度神经网络的每一层可以通过两个简单的量来刻画：一个非负标量参数和一个正实数上的Lévy测度。如果标量参数严格为正且所有隐藏层的Lévy测度都是平凡的，那么就得到了经典的高斯过程(GP)极限，即通过独立同分布的高斯权重获得。更有趣的是，如果至少一层的Lévy测度是非平凡的，我们得到了高斯过程的混合模型。

    This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian pro
    
[^38]: HERMES: 非平稳时序时尚数据的混合纠错模型，包括外部信号

    HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series. (arXiv:2202.03224v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2202.03224](http://arxiv.org/abs/2202.03224)

    HERMES是一个用于预测非平稳时序数据的混合纠错模型，并且利用外部信号提供了最先进的结果。

    

    开发用于预测非平稳时序数据的模型和算法是一个长期存在的统计问题。对于时尚或零售行业等许多应用来说，这是至关重要的，可以帮助做出最佳库存决策并避免巨大的浪费。本文使用最先进的计算机视觉方法，跟踪社交媒体上的数千种时尚趋势，提出了一种新的时尚时序预测模型。我们的贡献有两个。首先，我们公开提供了一个收集了10000个每周时尚时序数据的数据集。由于影响力动态是新兴趋势检测的关键，我们将每个时序数据与外部弱信号关联起来，代表影响者的行为。其次，为了利用这样的数据集，我们提出了一种新的混合预测模型。我们的方法将每个时序的参数化模型与季节性组件和一个全局递归神经网络相结合，以包括间断的外部信号。这种混合模型在提议的数据集上提供了最先进的结果。

    Developing models and algorithms to predict nonstationary time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is twofold. We first provide publicly a dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviours of influencers. Secondly, to leverage such a dataset, we propose a new hybrid forecasting model. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed 
    
[^39]: $k$次聚合的破裂维数（arXiv:2110.04763v2 [math.FA]已更新）

    Fat-Shattering Dimension of $k$-fold Aggregations. (arXiv:2110.04763v2 [math.FA] UPDATED)

    [http://arxiv.org/abs/2110.04763](http://arxiv.org/abs/2110.04763)

    该论文估计了实值函数类聚合规则的破裂维数，并给出了关于线性和仿射函数类的更尖锐上界和匹配的下界。同时改进了已知结果，并纠正了文献中的一些错误论断。

    

    我们对实值函数类聚合规则的破裂维数进行了估计。后者包括选择$k$个函数（每个类选择一个）并计算它们的点函数，如中值、平均值和最大值的所有方式。该界限是基于组成类的破裂维数表述的。对于线性和仿射函数类，我们提供了一个明显更尖锐的上界和一个相匹配的下界，实现了对$k$的最优依赖。在此过程中，我们改进了几个已知结果，同时指出和纠正了文献中的一些错误论断。

    We provide estimates on the fat-shattering dimension of aggregation rules of real-valued function classes. The latter consists of all ways of choosing $k$ functions, one from each of the $k$ classes, and computing a pointwise function of them, such as the median, mean, and maximum. The bound is stated in terms of the fat-shattering dimensions of the component classes. For linear and affine function classes, we provide a considerably sharper upper bound and a matching lower bound, achieving, in particular, an optimal dependence on $k$. Along the way, we improve several known results in addition to pointing out and correcting a number of erroneous claims in the literature.
    
[^40]: 用于可扩展分类任务的复杂度优化稀疏贝叶斯学习

    Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks. (arXiv:2107.08195v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08195](http://arxiv.org/abs/2107.08195)

    本文提出了一种复杂度优化稀疏贝叶斯学习方法DQN-SBL来解决高维特征空间或大数据规模问题中的内存溢出和计算复杂度高的问题，并在大规模问题上展现了竞争力的泛化能力。

    

    稀疏贝叶斯学习（Sparse Bayesian Learning，SBL）构建了一个极其稀疏的概率模型，具有竞争力的泛化能力。然而，SBL需要求解一个复杂度为$O(M^3)$（M：特征维度）的大型协方差矩阵以更新正则化先验，这使得在特征空间维度高或数据规模大的问题中变得困难，容易遭遇内存溢出问题。本文提出了一种称为DQN-SBL的用于SBL的新型对角拟牛顿（Diagonal Quasi-Newton，DQN）方法来解决这个问题，它忽略了大型协方差矩阵的求逆，从而将复杂度降低到$O(M)$。利用各种不同规模的基准进行了对非线性和线性分类问题的全面评估。实验证明，DQN-SBL在具有非常稀疏模型的情况下具有竞争力的泛化能力，并且能够很好地扩展到大规模问题。

    Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity $O(M^3)$ (M: feature size) for updating the regularization priors, making it difficult for problems with high dimensional feature space or large data size. As it may easily suffer from the memory overflow issue in such problems. This paper addresses this issue with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity is reduced to $O(M)$. The DQN-SBL is thoroughly evaluated for non linear and linear classifications with various benchmarks of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.
    
[^41]: AngularGrad：一种用于卷积神经网络角度收敛的新优化技术

    AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.10190](http://arxiv.org/abs/2105.10190)

    AngularGrad是一种新的优化器，通过考虑连续梯度的方向/角度行为来优化卷积神经网络的角度收敛。通过捕捉角度信息以获得更准确的步长，优化步骤变得更加平滑。

    

    卷积神经网络(CNNs)使用基于随机梯度下降(SGD)的优化器进行训练。最近，自适应时刻估计(Adam)优化器因其自适应动量而变得非常流行，从而解决了SGD的梯度消失问题。然而，现有的优化器仍然无法有效利用优化曲率信息。本文提出了一种新的AngularGrad优化器，它考虑了连续梯度的方向/角度的行为。这是文献中第一次尝试利用梯度角度信息而不仅仅是梯度大小。所提出的AngularGrad根据先前迭代的梯度角度信息生成一个得分来控制步长。因此，通过角度信息捕获到更准确的近期梯度步长，优化步骤变得更加平滑。基于使用正切或余弦函数的两个AngularGrad变体得到了开发。

    Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for comp
    
[^42]: 通过BAPC——先后参数比较解释AI

    Explainable AI by BAPC -- Before and After correction Parameter Comparison. (arXiv:2103.07155v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.07155](http://arxiv.org/abs/2103.07155)

    该论文介绍了一种通过纠正简单的基模型来解释AI预测的局部替代方法。研究结果表明，通过确定准确性损失、准确性和替代品忠实度之间的准确关系，可以得到理想大小的解释实例邻域，以实现最大的准确性和忠实度。

    

    介绍了一种用于纠正简单“基”模型的AI模型的局部替代品，表示解释AI预测的分析方法。在这里，将该方法应用于基模型为线性回归的情况下进行研究。AI模型逼近了线性模型的残差误差，并以可解释的基模型参数的变化形式提出了解释。为AI模型的准确性损失、AI模型的准确性和替代品忠实度之间的准确关系制定了准则。研究结果表明，在假设观测数据存在一定噪声的情况下，这些准则导致了一个理想大小的需要解释的实例邻域，以实现最大的准确性和忠实度。

    A local surrogate for an AI-model correcting a simpler 'base' model is introduced representing an analytical method to yield explanations of AI-predictions. The approach is studied here in the context of the base model being linear regression. The AI-model approximates the residual error of the linear model and the explanations are formulated in terms of the change of the interpretable base model's parameters. Criteria are formulated for the precise relation between lost accuracy of the surrogate, the accuracy of the AI-model, and the surrogate fidelity. It is shown that, assuming a certain maximal amount of noise in the observed data, these criteria induce neighborhoods of the instances to be explained which have an ideal size in terms of maximal accuracy and fidelity.
    
[^43]: 究竟是ResNet？神经ODE及其数值解

    ResNet After All? Neural ODEs and Their Numerical Solution. (arXiv:2007.15386v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.15386](http://arxiv.org/abs/2007.15386)

    神经ODE模型的性能取决于训练过程中使用的数值方法，如果使用过于粗糙的解算器进行训练，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性下降。

    

    最近提出的神经常微分方程(ODE)框架具有连续时间扩展离散残差神经网络的特点。但是，我们在这里展示，训练的神经ODE模型实际上取决于训练过程中使用的特定数值方法。如果训练出的模型被认为是从ODE生成的流动，那么可以选择另一个数值解算器，其数值误差大小相同或更小，而不会损失性能。我们观察到，如果训练依赖于过于粗糙的离散化解算器，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性急剧下降。在这种情况下，向量场和数值方法的组合不能被解释为从ODE生成的流动，这可以被认为是神经ODE概念的致命断裂。然而，我们观察到存在一个临界步长，超过该步长，训练会产生一个有效的ODE向量场。

    A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training. If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance. We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept. We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. W
    
[^44]: 一个统一的双线性LSTM框架

    A Unifying Framework of Bilinear LSTMs. (arXiv:1910.10294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.10294](http://arxiv.org/abs/1910.10294)

    本文提出了一个统一的双线性LSTM框架，通过平衡线性和双线性项的表达能力，实现了对序列数据集中输入特征的非线性交互的利用，以实现更好的性能，同时不增加更多的学习参数。

    

    本文提出了一个新颖的统一双线性LSTM框架，可以表示和利用序列数据集中输入特征的非线性交互，以实现比线性LSTM更好的性能，同时不会增加更多需要学习的参数。为了实现这一点，我们的统一框架允许通过调整隐藏状态向量的大小与双线性项中权重矩阵的逼近质量之间的权衡来平衡线性和双线性项的表达能力，从而优化我们的双线性LSTM的性能，同时不会增加更多需要学习的参数。我们在几个基于语言的序列学习任务中对我们的双线性LSTM的性能进行了实证评估，以展示其普适性。

    This paper presents a novel unifying framework of bilinear LSTMs that can represent and utilize the nonlinear interaction of the input features present in sequence datasets for achieving superior performance over a linear LSTM and yet not incur more parameters to be learned. To realize this, our unifying framework allows the expressivity of the linear vs. bilinear terms to be balanced by correspondingly trading off between the hidden state vector size vs. approximation quality of the weight matrix in the bilinear term so as to optimize the performance of our bilinear LSTM, while not incurring more parameters to be learned. We empirically evaluate the performance of our bilinear LSTM in several language-based sequence learning tasks to demonstrate its general applicability.
    

