# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Practical and Asymptotically Exact Conditional Sampling in Diffusion Models.](http://arxiv.org/abs/2306.17775) | 本论文提出了一种名为TDS的扭转式扩散采样器，它是一种针对扩散模型的顺序蒙特卡洛算法。该方法通过使用扭转技术结合启发式近似，能够在不需要特定训练的情况下在广泛的条件分布上提供精确的样本。 |
| [^2] | [The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit.](http://arxiv.org/abs/2306.17759) | 在无限深度和宽度的比例极限下，我们通过修改Softmax-based注意力模型，研究了Transformer的协方差矩阵。我们发现在初始化时，极限分布可以用随机微分方程来描述。通过修改注意力机制并使用残差连接，我们可以控制网络的稳定性和协方差结构的行为。 |
| [^3] | [Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering.](http://arxiv.org/abs/2306.17690) | 本文提出了一种泛化时间扭曲不变字典学习算法，用于处理时间序列数据的模式识别和分类。该算法通过使用连续基函数的线性组合来构建泛化时间扭曲算子，以实现连续的时间扭曲。通过联合优化扭曲路径、字典和稀疏系数，我们的算法在时间序列分类和聚类任务中取得了优秀的性能。 |
| [^4] | [Scalable method for Bayesian experimental design without integrating over posterior distribution.](http://arxiv.org/abs/2306.17615) | 本论文提出了一种新颖的无似然方法，用于在基于偏微分方程的观测模型下解决A-最优贝叶斯实验设计问题，无需对贝叶斯后验分布进行采样或积分。 |
| [^5] | [Variational principle to regularize machine-learned density functionals: the non-interacting kinetic-energy functional.](http://arxiv.org/abs/2306.17587) | 本文为机器学习的密度泛函引入了一种新的规范方法，其中包括非相互作用动能泛函。该方法通过使用基于深度神经网络的密度泛函训练，取得了在一维系统上的优秀结果。 |
| [^6] | [The most likely common cause.](http://arxiv.org/abs/2306.17557) | 对于因果不充分的情况下的共同原因问题，我们使用广义最大似然方法来识别共同原因C，与最大熵原则密切相关。对于两个二元对称变量的研究揭示了类似于二阶相变的条件概率非解析行为。 |
| [^7] | [Efficient uniform approximation using Random Vector Functional Link networks.](http://arxiv.org/abs/2306.17501) | 本文研究了使用随机向量功能连接网络进行高效统一逼近的方法，证明了具有ReLU激活函数的RVFL网络可以逼近利普希茨连续函数，前提是隐藏层相对于输入维度是指数级宽度的。这是第一个证明了$L_\infty$逼近误差和高斯内部权重条件下的结果，给出了非渐进性的隐藏层节点数量下界。 |
| [^8] | [An Oblivious Stochastic Composite Optimization Algorithm for Eigenvalue Optimization Problems.](http://arxiv.org/abs/2306.17470) | 本论文提出了两种针对非光滑和光滑目标的无视觉随机镜像下降算法，不需要先验知识，并给出了相应收敛速度。 |
| [^9] | [Global Optimality in Bivariate Gradient-based DAG Learning.](http://arxiv.org/abs/2306.17378) | 本文研究了双变量基于梯度的有向无环图(DAG)学习问题，通过证明一种路径跟踪优化算法的全局收敛性，提供了该问题的全局最优解。 |
| [^10] | [Capturing functional connectomics using Riemannian partial least squares.](http://arxiv.org/abs/2306.17371) | 本论文介绍了一种称为R-PLS的技术，它是PLS的一种推广，用于分析功能连接组，并考虑了功能连接矩阵的正定性质。 |
| [^11] | [iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models.](http://arxiv.org/abs/2306.17361) | 本文提出了一种识别非线性加性噪声模型中因果机制转变的方法，该方法专注于在相关的结构因果模型中识别功能机制的变化，而不需要估计整个有向无环图(DAG)的结构。 |
| [^12] | [Kernel $\epsilon$-Greedy for Contextual Bandits.](http://arxiv.org/abs/2306.17329) | 本文提出了基于核的$\epsilon$-贪心策略应用于情境脉冲中的方法，通过在线加权核岭回归估计器实现对奖励函数的估计，并证明了其一致性和依赖于RKHS维度的次线性后悔率，在有限维RKHS的边际条件下实现了最优后悔率。 |
| [^13] | [Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study.](http://arxiv.org/abs/2306.17301) | 本文通过数值研究探讨了浅层神经网络在逼近和学习高频率方面的困难，重点是通过分析激活函数的谱分析来理解问题的原因。 |
| [^14] | [A Quantitative Functional Central Limit Theorem for Shallow Neural Networks.](http://arxiv.org/abs/2306.16932) | 本文证明了具有通用激活函数的单隐藏层神经网络的定量函数中心极限定理，收敛速度取决于激活函数的平滑性。 |
| [^15] | [Online Learning with Set-Valued Feedback.](http://arxiv.org/abs/2306.06247) | 本文研究了一种在线多类分类的变体，其中使用集合型反馈。通过引入新的组合维度，该论文表明确定性和随机性的在线可学习性在实现设置下不等价，并将在线多标签排名和在线多标签分类等实际学习设置作为其特定实例。 |
| [^16] | [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning.](http://arxiv.org/abs/2305.15912) | 本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。 |
| [^17] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^18] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^19] | [On the Existence of a Complexity in Fixed Budget Bandit Identification.](http://arxiv.org/abs/2303.09468) | 该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。 |
| [^20] | [Uncertainty Estimation by Fisher Information-based Evidential Deep Learning.](http://arxiv.org/abs/2303.02045) | 本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。 |
| [^21] | [First-order ANIL learns linear representations despite misspecified latent dimension.](http://arxiv.org/abs/2303.01335) | 本研究表明，在存在架构误指定的情况下，初阶ANIL可以成功学习到线性的共享表示。这个结果是基于对无限数量任务的极限情况下的推导。 |
| [^22] | [Off-Policy Evaluation with Out-of-Sample Guarantees.](http://arxiv.org/abs/2301.08649) | 本文提出了一种具有样本外保证的离线策略评估方法，可以在考虑模型配置错误的情况下，使用观察数据对决策策略的性能进行有效推断。 |
| [^23] | [Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions.](http://arxiv.org/abs/2301.06535) | 案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。 |
| [^24] | [GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond.](http://arxiv.org/abs/2211.01962) | 本研究在交互式决策的框架下，提出了一种新的复杂度度量GEC，用于样本高效强化学习。该方法能够捕捉到探索和开发之间的权衡，将RL问题划分为低GEC和高GEC两个类别，并展示了低GEC类别的丰富性质。 |
| [^25] | [Private Online Prediction from Experts: Separations and Faster Rates.](http://arxiv.org/abs/2210.13537) | 这篇论文提出了新的算法，用于在在线预测从专家中解决隐私约束的问题，并改进了现有算法的遗憾界限。研究结果表明，在纯差分隐私和近似差分隐私设置下，对于愚蠢敌手，在高维范围内的遗憾可以达到亚线性水平，与自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。 |
| [^26] | [Improving Expert Predictions with Conformal Prediction.](http://arxiv.org/abs/2201.12006) | 本研究开发了一种自动决策支持系统，通过使用符合预测构建的标签预测集合，精确地权衡了真实标签不在预测集合中的概率。 |
| [^27] | [Sufficient-Statistic Memory AMP.](http://arxiv.org/abs/2112.15327) | 该论文提出了一种在特定条件下解决AMP类型算法收敛性问题的充分统计记忆型AMP算法框架，通过充分统计约束和特定条件下的协方差矩阵性质，实现了有效的信号重构。 |
| [^28] | [Heterogeneous Distributed Lag Models to Estimate Personalized Effects of Maternal Exposures to Air Pollution.](http://arxiv.org/abs/2109.13763) | 本研究提出了一种利用异质分布滞后模型和贝叶斯加性回归树的统计学习方法，用于个性化估计孕妇暴露于细颗粒物的关键时期和新生儿体重的关系。研究发现，关键时期和关联幅度在不同的个体、家庭和社区特征水平上表现出异质性。 |
| [^29] | [The Bayesian Learning Rule.](http://arxiv.org/abs/2107.04562) | 许多机器学习算法都可以归结为贝叶斯学习规则，该规则通过利用自然梯度来逼近后验分布，从而得到广泛的算法应用。这一工作不仅统一了现有算法，还帮助我们设计新的算法。 |
| [^30] | [Approximating Probability Distributions by using Wasserstein Generative Adversarial Networks.](http://arxiv.org/abs/2103.10060) | 本文研究了Wasserstein生成对抗网络（WGANs），并使用GroupSort神经网络作为鉴别器。研究结果显示，生成器和鉴别器的容量对目标分布的逼近误差有影响，并且WGANs对鉴别器的容量要求高于生成器。此外，在鉴别器不足够强大时，低容量的生成器可能比过度深层和宽度的生成器效果更好。数值结果证实了理论结果。 |
| [^31] | [A method to integrate and classify normal distributions.](http://arxiv.org/abs/2012.14331) | 本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。 |
| [^32] | [Causal Rule Ensemble: Interpretable Discovery and Inference of Heterogeneous Treatment Effects.](http://arxiv.org/abs/2009.09036) | 本论文介绍了一种新的因果规则集合(CRE)方法，通过集成树的方式，可解释的发现和估计异质性治疗效应(HTE)，具有稳定性和对复杂异质性模式的探索能力。 |
| [^33] | [High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient.](http://arxiv.org/abs/1806.04047) | 本文研究了在高维设置中的多任务学习问题，并引入了一个估计器来处理多连接线性回归问题，称为数据丰富/共享。我们通过凸函数来描述公共参数和个体参数的结构，并提出了一种具有几何收敛速度的迭代估计算法。 |
| [^34] | [A Bayesian Filtering Algorithm for Gaussian Mixture Models.](http://arxiv.org/abs/1705.05495) | 本文提出了一种用于高斯混合模型的贝叶斯滤波算法，通过高斯混合简化步骤处理混合项数量的指数增长，并提出了统一算法的平方根实现。该算法在几个模拟系统上进行了测试。 |

# 详细

[^1]: 扩散模型中的实用和渐进精确条件采样

    Practical and Asymptotically Exact Conditional Sampling in Diffusion Models. (arXiv:2306.17775v1 [stat.ML])

    [http://arxiv.org/abs/2306.17775](http://arxiv.org/abs/2306.17775)

    本论文提出了一种名为TDS的扭转式扩散采样器，它是一种针对扩散模型的顺序蒙特卡洛算法。该方法通过使用扭转技术结合启发式近似，能够在不需要特定训练的情况下在广泛的条件分布上提供精确的样本。

    

    扩散模型在分子设计和文本到图像生成等条件生成任务中取得了成功。然而，这些成就主要依赖于任务特定的条件训练或容易出错的启发式近似。理想情况下，条件生成方法应该能够在不需要特定训练的情况下为广泛的条件分布提供精确的样本。为此，我们引入了扭转式扩散采样器(TDS)。TDS是一种针对扩散模型的顺序蒙特卡洛(SMC)算法。其主要思想是使用扭转，一种具有良好计算效率的SMC技术，来结合启发式近似而不影响渐进精确性。我们首先在模拟实验和MNIST图像修复以及类条件生成任务中发现，TDS提供了计算统计权衡，使用更多粒子得到更准确的近似结果，但同时需要更多计算资源。

    Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and on MNIST image inpainting and class-conditional generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but wi
    
[^2]: 受形状改变的Transformer：在无限深度和宽度极限中的注意力模型

    The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit. (arXiv:2306.17759v1 [stat.ML])

    [http://arxiv.org/abs/2306.17759](http://arxiv.org/abs/2306.17759)

    在无限深度和宽度的比例极限下，我们通过修改Softmax-based注意力模型，研究了Transformer的协方差矩阵。我们发现在初始化时，极限分布可以用随机微分方程来描述。通过修改注意力机制并使用残差连接，我们可以控制网络的稳定性和协方差结构的行为。

    

    在深度学习理论中，表示的协方差矩阵用作检查网络可训练性的代理。受Transformer的成功启发，我们研究了在无限深度和宽度的比例极限下，带有跳跃连接的修改Softmax-based注意力模型的协方差矩阵。我们展示了在初始化时，极限分布可以用深度与宽度比率为索引的随机微分方程（SDE）来描述。为了实现良定义的随机极限，Transformer的注意力机制通过将Softmax输出居中在单位矩阵上，并通过宽度相关的温度参数对Softmax logits进行缩放来进行修改。我们通过相应的SDE研究了网络的稳定性，展示了如何通过残差连接优雅地控制漂移和扩散的尺度。稳定SDE的存在意味着协方差结构是良 behaved 的，即使对于非常大的深度和宽度也是如此。

    In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and widt
    
[^3]: 泛化时间扭曲不变字典学习用于时间序列分类和聚类

    Generalized Time Warping Invariant Dictionary Learning for Time Series Classification and Clustering. (arXiv:2306.17690v1 [stat.ML])

    [http://arxiv.org/abs/2306.17690](http://arxiv.org/abs/2306.17690)

    本文提出了一种泛化时间扭曲不变字典学习算法，用于处理时间序列数据的模式识别和分类。该算法通过使用连续基函数的线性组合来构建泛化时间扭曲算子，以实现连续的时间扭曲。通过联合优化扭曲路径、字典和稀疏系数，我们的算法在时间序列分类和聚类任务中取得了优秀的性能。

    

    字典学习是时间序列数据模式识别和分类的有效工具。在各种字典学习技术中，动态时间扭曲（DTW）通常用于处理时间延迟、缩放、转换和其他各种时间不准确性问题。然而，由于DTW在对齐时间序列数据时是离散的性质，因此容易出现过拟合或信息损失的问题。为了解决这个问题，本文提出了一种泛化时间扭曲不变字典学习算法。我们的方法采用了泛化时间扭曲算子，该算子由连续基函数的线性组合构成，以便实现连续的时间扭曲。将所提出的算子与字典学习相结合，将其建模为一个优化问题，并采用块坐标下降法来联合优化扭曲路径、字典和稀疏系数。优化结果被用作特征提取和分类的提取。实验结果表明，我们的方法在时间序列分类和聚类任务中取得了优秀的性能。

    Dictionary learning is an effective tool for pattern recognition and classification of time series data. Among various dictionary learning techniques, the dynamic time warping (DTW) is commonly used for dealing with temporal delays, scaling, transformation, and many other kinds of temporal misalignments issues. However, the DTW suffers overfitting or information loss due to its discrete nature in aligning time series data. To address this issue, we propose a generalized time warping invariant dictionary learning algorithm in this paper. Our approach features a generalized time warping operator, which consists of linear combinations of continuous basis functions for facilitating continuous temporal warping. The integration of the proposed operator and the dictionary learning is formulated as an optimization problem, where the block coordinate descent method is employed to jointly optimize warping paths, dictionaries, and sparseness coefficients. The optimized results are then used as hy
    
[^4]: 无需对后验分布进行积分的可扩展贝叶斯实验设计方法

    Scalable method for Bayesian experimental design without integrating over posterior distribution. (arXiv:2306.17615v1 [math.NA])

    [http://arxiv.org/abs/2306.17615](http://arxiv.org/abs/2306.17615)

    本论文提出了一种新颖的无似然方法，用于在基于偏微分方程的观测模型下解决A-最优贝叶斯实验设计问题，无需对贝叶斯后验分布进行采样或积分。

    

    本文解决了在基于偏微分方程的观测模型中求解A-最优贝叶斯实验设计问题时的计算效率问题，由于需要计算复杂，A-最优性是贝叶斯实验设计中广泛使用的、易于解释的标准。该标准通过最小化预期条件方差，也称为预期后验方差，来寻求最优实验设计。本工作提出了一种新颖的无似然方法，用于寻找A-最优实验设计，而无需对贝叶斯后验分布进行采样或积分。在我们的方法中，通过使用总方差定理，通过条件期望的方差来获得预期条件方差，同时利用正交投影性质来近似条件期望。通过渐近误差估计，我们证明了后验不可计算性问题。

    We address the computational efficiency in solving the A-optimal Bayesian design of experiments problems for which the observational model is based on partial differential equations and, consequently, is computationally expensive to evaluate. A-optimality is a widely used and easy-to-interpret criterion for the Bayesian design of experiments. The criterion seeks the optimal experiment design by minimizing the expected conditional variance, also known as the expected posterior variance. This work presents a novel likelihood-free method for seeking the A-optimal design of experiments without sampling or integrating the Bayesian posterior distribution. In our approach, the expected conditional variance is obtained via the variance of the conditional expectation using the law of total variance, while we take advantage of the orthogonal projection property to approximate the conditional expectation. Through an asymptotic error estimation, we show that the intractability of the posterior doe
    
[^5]: 用变分原理来规范机器学习的密度泛函：非相互作用动能泛函

    Variational principle to regularize machine-learned density functionals: the non-interacting kinetic-energy functional. (arXiv:2306.17587v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.17587](http://arxiv.org/abs/2306.17587)

    本文为机器学习的密度泛函引入了一种新的规范方法，其中包括非相互作用动能泛函。该方法通过使用基于深度神经网络的密度泛函训练，取得了在一维系统上的优秀结果。

    

    实际密度泛函理论 (DFT) 的成功归功于 Kohn 和 Sham 的开创性工作，他们引入了使用辅助均场系统计算非相互作用动能的准确方法。然而，DFT 的全部潜力将无法释放，直到找到电子密度与非相互作用动能之间的准确关系。已经尝试了各种方法来近似这个泛函，类似于交换关联泛函，但由于动能的贡献更大且更非局域，成功程度较低。在本研究中，我们提出了一种新的高效规范方法，以深度神经网络为基础来训练密度泛函，尤其是动能泛函。该方法在（有效的）一维系统上进行了测试，包括氢链、非相互作用电子和前两个周期元素的原子，取得了出色的结果。

    Practical density functional theory (DFT) owes its success to the groundbreaking work of Kohn and Sham that introduced the exact calculation of the non-interacting kinetic energy of the electrons using an auxiliary mean-field system. However, the full power of DFT will not be unleashed until the exact relationship between the electron density and the non-interacting kinetic energy is found. Various attempts have been made to approximate this functional, similar to the exchange--correlation functional, with much less success due to the larger contribution of kinetic energy and its more non-local nature. In this work we propose a new and efficient regularization method to train density functionals based on deep neural networks, with particular interest in the kinetic-energy functional. The method is tested on (effectively) one-dimensional systems, including the hydrogen chain, non-interacting electrons, and atoms of the first two periods, with excellent results. For the atomic systems, t
    
[^6]: 最可能的共同原因

    The most likely common cause. (arXiv:2306.17557v1 [physics.data-an])

    [http://arxiv.org/abs/2306.17557](http://arxiv.org/abs/2306.17557)

    对于因果不充分的情况下的共同原因问题，我们使用广义最大似然方法来识别共同原因C，与最大熵原则密切相关。对于两个二元对称变量的研究揭示了类似于二阶相变的条件概率非解析行为。

    

    对于两个随机变量A和B的共同原因原则在因果不充分的情况下进行了研究，当它们的共同原因C被认为已经存在，但只观测到了A和B的联合概率。因此，C不能被唯一确定（潜在混杂因子问题）。我们展示了广义最大似然方法可以应用于这种情况，并且允许识别与共同原因原则一致的C。它与最大熵原则密切相关。对两个二元对称变量的研究揭示了条件概率的非解析行为，类似于二阶相变。这发生在观察到的概率分布从相关到反相关的过渡期间。讨论了广义似然方法与其他方法（如预测似然和最小共同原因熵）之间的关系。

    The common cause principle for two random variables $A$ and $B$ is examined in the case of causal insufficiency, when their common cause $C$ is known to exist, but only the joint probability of $A$ and $B$ is observed. As a result, $C$ cannot be uniquely identified (the latent confounder problem). We show that the generalized maximum likelihood method can be applied to this situation and allows identification of $C$ that is consistent with the common cause principle. It closely relates to the maximum entropy principle. Investigation of the two binary symmetric variables reveals a non-analytic behavior of conditional probabilities reminiscent of a second-order phase transition. This occurs during the transition from correlation to anti-correlation in the observed probability distribution. The relation between the generalized likelihood approach and alternative methods, such as predictive likelihood and the minimum common cause entropy, is discussed. The consideration of the common cause
    
[^7]: 使用随机向量功能连接网络进行高效统一逼近

    Efficient uniform approximation using Random Vector Functional Link networks. (arXiv:2306.17501v1 [stat.ML])

    [http://arxiv.org/abs/2306.17501](http://arxiv.org/abs/2306.17501)

    本文研究了使用随机向量功能连接网络进行高效统一逼近的方法，证明了具有ReLU激活函数的RVFL网络可以逼近利普希茨连续函数，前提是隐藏层相对于输入维度是指数级宽度的。这是第一个证明了$L_\infty$逼近误差和高斯内部权重条件下的结果，给出了非渐进性的隐藏层节点数量下界。

    

    随机向量功能连接(RVFL)网络是一个具有随机内部权重和偏置的二层神经网络。由于这种架构只需要学习外部权重，学习过程可以简化为线性优化任务，从而避免了非凸优化问题的困扰。在本文中，我们证明了具有ReLU激活函数的RVFL网络可以逼近利普希茨连续函数，前提是其隐藏层相对于输入维度是指数级宽度的。尽管之前已经证明了以$L_2$方式可以实现这样的逼近，但我们证明了在$L_\infty$逼近误差和高斯内部权重情况下的可行性。据我们所知，这是第一个这样的结果。我们给出了非渐进性的隐藏层节点数量的下界，取决于目标函数的利普希茨常数、期望的准确度和输入维度等因素。我们的证明方法根植于概率论。

    A Random Vector Functional Link (RVFL) network is a depth-2 neural network with random inner weights and biases. As only the outer weights of such architectures need to be learned, the learning process boils down to a linear optimization task, allowing one to sidestep the pitfalls of nonconvex optimization problems. In this paper, we prove that an RVFL with ReLU activation functions can approximate Lipschitz continuous functions provided its hidden layer is exponentially wide in the input dimension. Although it has been established before that such approximation can be achieved in $L_2$ sense, we prove it for $L_\infty$ approximation error and Gaussian inner weights. To the best of our knowledge, our result is the first of this kind. We give a nonasymptotic lower bound for the number of hidden layer nodes, depending on, among other things, the Lipschitz constant of the target function, the desired accuracy, and the input dimension. Our method of proof is rooted in probability theory an
    
[^8]: 一种用于特征值优化问题的无视觉随机复合优化算法

    An Oblivious Stochastic Composite Optimization Algorithm for Eigenvalue Optimization Problems. (arXiv:2306.17470v1 [math.OC])

    [http://arxiv.org/abs/2306.17470](http://arxiv.org/abs/2306.17470)

    本论文提出了两种针对非光滑和光滑目标的无视觉随机镜像下降算法，不需要先验知识，并给出了相应收敛速度。

    

    在这项工作中，我们重新审视了使用随机化一阶方法和随机平滑解决大规模半定规划问题的问题。我们引入了两种基于互补复合设置的无视觉随机镜像下降算法。一种算法设计用于非光滑目标，而加速版本则适用于光滑目标。值得注意的是，这两种算法都不需要对目标函数的Lipschitz常数或光滑度有先验知识。对于具有$\mathcal{M}-$有界预言的非光滑情况，我们证明了一个收敛速度为$ O( {\mathcal{M}}/{\sqrt{T}} ) $的收敛速度。对于具有由$D$限制的可行集的$L$-光滑情况，我们得到了一个收敛速度为$ O( {L^2 D^2}/{(T^{2}\sqrt{T})} + {(D_0^2+\sigma^2)}/{\sqrt{T}} )$的收敛速度，其中$D_0$是到最优解的起始距离，$ \sigma^2$是随机预言方差。目前只有在假设先验知识的Lipschitz常数或t情况下才能得到这些速度。

    In this work, we revisit the problem of solving large-scale semidefinite programs using randomized first-order methods and stochastic smoothing. We introduce two oblivious stochastic mirror descent algorithms based on a complementary composite setting. One algorithm is designed for non-smooth objectives, while an accelerated version is tailored for smooth objectives. Remarkably, both algorithms work without prior knowledge of the Lipschitz constant or smoothness of the objective function. For the non-smooth case with $\mathcal{M}-$bounded oracles, we prove a convergence rate of $ O( {\mathcal{M}}/{\sqrt{T}} ) $. For the $L$-smooth case with a feasible set bounded by $D$, we derive a convergence rate of $ O( {L^2 D^2}/{(T^{2}\sqrt{T})} + {(D_0^2+\sigma^2)}/{\sqrt{T}} )$, where $D_0$ is the starting distance to an optimal solution, and $ \sigma^2$ is the stochastic oracle variance. These rates had only been obtained so far by either assuming prior knowledge of the Lipschitz constant or t
    
[^9]: 双变量基于梯度的有向无环图(DAG)学习中的全局最优性

    Global Optimality in Bivariate Gradient-based DAG Learning. (arXiv:2306.17378v1 [cs.LG])

    [http://arxiv.org/abs/2306.17378](http://arxiv.org/abs/2306.17378)

    本文研究了双变量基于梯度的有向无环图(DAG)学习问题，通过证明一种路径跟踪优化算法的全局收敛性，提供了该问题的全局最优解。

    

    最近，一类新的非凸优化问题受到了学术界的关注，它源于从数据中学习无环有向图模型的统计问题。虽然现有的方法使用标准的一阶优化算法来解决这个问题，但证明这些方法的全局最优性一直是困难的。问题的难点在于，与文献中的其他非凸问题不同，这个问题并不是"良性"的，并且存在着多个虚假解，标准方法很容易陷入其中。在本文中，我们证明了一种简单的路径跟踪优化算法在双变量情况下会全局收敛到总体损失的全局最小值。

    Recently, a new class of non-convex optimization problems motivated by the statistical problem of learning an acyclic directed graphical model from data has attracted significant interest. While existing work uses standard first-order optimization schemes to solve this problem, proving the global optimality of such approaches has proven elusive. The difficulty lies in the fact that unlike other non-convex problems in the literature, this problem is not "benign", and possesses multiple spurious solutions that standard approaches can easily get trapped in. In this paper, we prove that a simple path-following optimization scheme globally converges to the global minimum of the population loss in the bivariate setting.
    
[^10]: 使用Riemannian偏最小二乘法捕获功能连接组学

    Capturing functional connectomics using Riemannian partial least squares. (arXiv:2306.17371v1 [stat.ME])

    [http://arxiv.org/abs/2306.17371](http://arxiv.org/abs/2306.17371)

    本论文介绍了一种称为R-PLS的技术，它是PLS的一种推广，用于分析功能连接组，并考虑了功能连接矩阵的正定性质。

    

    对于神经学疾病和疾病，人脑的功能和解剖连接组可以用于更好地指导有针对性的干预和治疗策略。功能磁共振成像（fMRI）是一种非侵入性的神经成像技术，可以通过时间内的血流来捕捉脑功能的时空特征。fMRI可以通过功能连接矩阵来研究功能连接组，即fMRI图像中感兴趣区域的时间序列之间的Pearson相关系数矩阵。分析功能连接的一种方法是使用偏最小二乘法（PLS），一种针对高维预测变量的多元回归技术。然而，使用PLS分析功能连接忽略了功能连接矩阵的一个重要属性，即这些矩阵是正定的。为了解决这个问题，我们引入了Riemannian流形上的PLS的推广，称为R-PLS，并将其应用于对称正定的连接矩阵。

    For neurological disorders and diseases, functional and anatomical connectomes of the human brain can be used to better inform targeted interventions and treatment strategies. Functional magnetic resonance imaging (fMRI) is a non-invasive neuroimaging technique that captures spatio-temporal brain function through blood flow over time. FMRI can be used to study the functional connectome through the functional connectivity matrix; that is, Pearson's correlation matrix between time series from the regions of interest of an fMRI image. One approach to analysing functional connectivity is using partial least squares (PLS), a multivariate regression technique designed for high-dimensional predictor data. However, analysing functional connectivity with PLS ignores a key property of the functional connectivity matrix; namely, these matrices are positive definite. To account for this, we introduce a generalisation of PLS to Riemannian manifolds, called R-PLS, and apply it to symmetric positive 
    
[^11]: iSCAN：识别非线性加性噪声模型中的因果机制转变

    iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])

    [http://arxiv.org/abs/2306.17361](http://arxiv.org/abs/2306.17361)

    本文提出了一种识别非线性加性噪声模型中因果机制转变的方法，该方法专注于在相关的结构因果模型中识别功能机制的变化，而不需要估计整个有向无环图(DAG)的结构。

    

    结构因果模型(SCM)被广泛应用于各个领域，以表示复杂系统中变量之间的因果关系。然而，真正的底层有向无环图(DAG)结构通常是未知的，并且从观测数据或干预数据中确定它仍然是一项具有挑战性的任务。然而，在许多情况下，目标是识别相关SCM之间的因果机制的变化(转变)而不是恢复整个底层DAG结构。例子包括分析健康和癌症患者之间的基因调控网络结构变化，或者在不同细胞环境下理解生物途径的变化。本文重点研究了在相同的变量集上识别两个或多个相关SCM中的$\textit{功能}$机制转变，而不需要估计每个SCM的整个DAG结构。在这种设置下，先前的工作假设使用了具有高斯噪声的线性模型；而本文中我们则考虑了非线性加性噪声模型。

    Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
    
[^12]: 基于核的$\epsilon$-贪心策略在情境脉冲中的应用

    Kernel $\epsilon$-Greedy for Contextual Bandits. (arXiv:2306.17329v1 [stat.ML])

    [http://arxiv.org/abs/2306.17329](http://arxiv.org/abs/2306.17329)

    本文提出了基于核的$\epsilon$-贪心策略应用于情境脉冲中的方法，通过在线加权核岭回归估计器实现对奖励函数的估计，并证明了其一致性和依赖于RKHS维度的次线性后悔率，在有限维RKHS的边际条件下实现了最优后悔率。

    

    我们考虑了情境脉冲中的基于核的$\epsilon$-贪心策略。更具体地说，在有限数量的臂的情况下，我们认为平均奖励函数位于再生核希尔伯特空间（RKHS）中。我们提出了一种用于奖励函数的在线加权核岭回归估计器。在对探索概率序列$\{\epsilon_t\}_t$和正则化参数$\{\lambda_t\}_t$的一些条件下，我们证明了所提出的估计器的一致性。我们还证明，对于任何核和相应的RKHS的选择，我们可以实现依赖于RKHS内在维度的次线性后悔率。此外，在有限维RKHS的边际条件下，我们实现了$\sqrt{T}$的最优后悔率。

    We consider a kernelized version of the $\epsilon$-greedy strategy for contextual bandits. More precisely, in a setting with finitely many arms, we consider that the mean reward functions lie in a reproducing kernel Hilbert space (RKHS). We propose an online weighted kernel ridge regression estimator for the reward functions. Under some conditions on the exploration probability sequence, $\{\epsilon_t\}_t$, and choice of the regularization parameter, $\{\lambda_t\}_t$, we show that the proposed estimator is consistent. We also show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\sqrt{T}$ under a margin condition for finite-dimensional RKHS.
    
[^13]: 浅层网络在逼近和学习高频率方面的困难：一个数值研究

    Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v1 [cs.LG])

    [http://arxiv.org/abs/2306.17301](http://arxiv.org/abs/2306.17301)

    本文通过数值研究探讨了浅层神经网络在逼近和学习高频率方面的困难，重点是通过分析激活函数的谱分析来理解问题的原因。

    

    本研究通过对分析和实验的综合数值研究，解释了为什么两层神经网络在机器精度和计算成本等实际因素中，处理高频率的逼近和学习存在困难。具体而言，研究了以下基本计算问题：（1）在有限的机器精度下可以达到的最佳精度，（2）实现给定精度所需的计算成本，以及（3）对扰动的稳定性。研究的关键是相应激活函数的格拉姆矩阵的谱分析，该分析还显示了激活函数属性在这个问题中的作用。

    In this work, a comprehensive numerical study involving analysis and experiments shows why a two-layer neural network has difficulties handling high frequencies in approximation and learning when machine precision and computation cost are important factors in real practice. In particular, the following fundamental computational issues are investigated: (1) the best accuracy one can achieve given a finite machine precision, (2) the computation cost to achieve a given accuracy, and (3) stability with respect to perturbations. The key to the study is the spectral analysis of the corresponding Gram matrix of the activation functions which also shows how the properties of the activation function play a role in the picture.
    
[^14]: 浅层神经网络的定量函数中心极限定理

    A Quantitative Functional Central Limit Theorem for Shallow Neural Networks. (arXiv:2306.16932v1 [math.PR] CROSS LISTED)

    [http://arxiv.org/abs/2306.16932](http://arxiv.org/abs/2306.16932)

    本文证明了具有通用激活函数的单隐藏层神经网络的定量函数中心极限定理，收敛速度取决于激活函数的平滑性。

    

    我们证明了对于具有通用激活函数的单隐藏层神经网络的定量函数中心极限定理。我们建立的收敛速度严重依赖于激活函数的平滑性，从非可微的情况（如Relu）的对数级别到非常规则激活函数的$\sqrt{n}$级别。我们的主要工具是Stein-Malliavin方法的函数版本；特别是，我们大量利用了Bourguin和Campese（2020）最近建立的定量函数中心极限定理。

    We prove a Quantitative Functional Central Limit Theorem for one-hidden-layer neural networks with generic activation function. The rates of convergence that we establish depend heavily on the smoothness of the activation function, and they range from logarithmic in non-differentiable cases such as the Relu to $\sqrt{n}$ for very regular activations. Our main tools are functional versions of the Stein-Malliavin approach; in particular, we exploit heavily a quantitative functional central limit theorem which has been recently established by Bourguin and Campese (2020).
    
[^15]: 使用集合型反馈的在线学习

    Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])

    [http://arxiv.org/abs/2306.06247](http://arxiv.org/abs/2306.06247)

    本文研究了一种在线多类分类的变体，其中使用集合型反馈。通过引入新的组合维度，该论文表明确定性和随机性的在线可学习性在实现设置下不等价，并将在线多标签排名和在线多标签分类等实际学习设置作为其特定实例。

    

    本文研究了在线多类分类的一种变体，其中学习器预测单个标签，但接收到一个标签的集合作为反馈。在该模型中，如果学习器没有输出包含在反馈集合中的标签，则会受到惩罚。我们表明，与具有单标签反馈的在线多类学习不同，在实现设置中使用集合型反馈时，确定性和随机化的在线可学习性\textit{不等价}。因此，我们提供了两个新的组合维度，分别命名为集合小石和度量破裂维度，严格描述了确定性和随机化的在线可学习性。此外，我们表明度量破裂维度在悟性设置下严格描述在线可学习性。最后，我们证明了在线多标签排名和在线多标签分类等实际学习设置是我们通用在线学习框架的具体实例。

    We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
    
[^16]: 改进ReLU网络特征学习的神经特征激活值分析

    Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])

    [http://arxiv.org/abs/2305.15912](http://arxiv.org/abs/2305.15912)

    本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。

    

    本文研究了神经网络中单个ReLU单元的特征激活值。我们将ReLU单元在输入空间中对应的特征激活值集合称为ReLU单元的特征激活集。我们建立了特征激活集与ReLU网络中学习特征之间的明确联系，并揭示了现代深度学习架构中使用的各种神经网络规范化技术如何规范化和稳定SGD优化。利用这些洞见，我们提出了一种几何方法来参数化ReLU网络以改进特征学习。我们经验性地验证了其有用性，使用了不那么精心选择的初始化方案和更大的学习率。我们报告了更好的优化稳定性，更快的收敛速度和更好的泛化性能。

    We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
    
[^17]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^18]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^19]: 固定预算赌博机标识中的复杂度存在问题

    On the Existence of a Complexity in Fixed Budget Bandit Identification. (arXiv:2303.09468v1 [stat.ML])

    [http://arxiv.org/abs/2303.09468](http://arxiv.org/abs/2303.09468)

    该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。

    

    在固定预算赌博机标识中，算法按顺序观察来自多个分布的样本，直到给定最终时间。然后，它回答关于分布集的查询。一个好的算法将有小的错误概率。虽然这个概率随着最终时间的增加呈指数级下降，但对于大多数标识任务，最佳可达率并非精确已知。我们展示了如果固定预算任务接受复杂度（定义为单个算法在所有赌博问题中实现的错误概率的下限），则该复杂度由该问题的最佳非自适应抽样过程确定。我们证明了对于几个固定预算识别任务，包括具有两个臂的伯努利最佳臂识别，不存在这样的复杂度：没有单个算法能够随处实现最佳可能速率。

    In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by a single algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate.
    
[^20]: 基于Fisher信息的证据深度学习方法用于不确定性估计

    Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02045](http://arxiv.org/abs/2303.02045)

    本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。

    This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.

    不确定性估计是使深度学习在实际应用中可靠的关键因素。最近提出的证据神经网络通过将网络输出视为证据来参数化狄利克雷分布，明确考虑不同的不确定性，并在不确定性估计方面取得了令人印象深刻的性能。然而，对于高数据不确定性样本但注释为one-hot标签的情况，这些错误标记的类别的证据学习过程会被过度惩罚并受到阻碍。为了解决这个问题，我们提出了一种新的方法，基于Fisher信息的证据深度学习（$\mathcal{I}$-EDL）。特别地，我们引入Fisher信息矩阵（FIM）来衡量每个样本所携带的证据的信息量，根据这个信息量，我们可以动态地重新加权目标损失项，使网络更加专注于不确定类别的表示学习。我们的网络的泛化能力通过优化进一步提高。

    Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
    
[^21]: 初阶ANIL在存在误指定的潜在维度情况下学习线性表示

    First-order ANIL learns linear representations despite misspecified latent dimension. (arXiv:2303.01335v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01335](http://arxiv.org/abs/2303.01335)

    本研究表明，在存在架构误指定的情况下，初阶ANIL可以成功学习到线性的共享表示。这个结果是基于对无限数量任务的极限情况下的推导。

    

    最近，由于在少样本分类和强化学习中的经验成功，元学习引起了极大的关注。元学习方法利用来自以前任务的数据以一种样本高效的方式学习新任务。特别是，模型无关的方法寻找起始点，从该起始点开始梯度下降可以迅速适应任何新任务。尽管经验上建议这样的方法通过在预训练期间学习共享表示表现良好，但对于这种行为的理论证据有限。更重要的是，并没有严格证明这些方法在存在架构误指定的情况下仍会学习到共享结构。在这个方向上，本文在无限数量的任务的极限情况下展示了，使用线性双层网络结构的初阶ANIL成功地学习到了线性的共享表示。即使是在参数化误指定的情况下，这个结果仍然成立，即网络的宽度大于

    Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialisation points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been rigorously shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with a misspecified network parameterisation; having a width larger than 
    
[^22]: 具有样本外保证的离线策略评估

    Off-Policy Evaluation with Out-of-Sample Guarantees. (arXiv:2301.08649v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.08649](http://arxiv.org/abs/2301.08649)

    本文提出了一种具有样本外保证的离线策略评估方法，可以在考虑模型配置错误的情况下，使用观察数据对决策策略的性能进行有效推断。

    

    我们考虑使用过去的观察数据评估决策策略的性能问题。策略的结果以损失（即失效或负奖励）来衡量，主要问题是在以不同且可能未知的策略下观察到过去数据时对其样本外损失进行有效推断。使用样本分割方法，我们展示了可以绘制这样的推断，并对整个损失分布而不仅仅是其均值进行有限样本覆盖保证。重要的是，该方法考虑了对过去策略的模型配置错误，包括未测量的混淆因素。该评估方法可用于在一定可信模型假设范围内使用观察数据对策略的性能进行认证。

    We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finite-sample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy - including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.
    
[^23]: 案例基础神经网络：具有时间变化的高阶交互的生存分析

    Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06535](http://arxiv.org/abs/2301.06535)

    案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。

    

    神经网络基于生存分析方法可以模拟数据驱动的协变量交互。虽然这些方法可以比回归方法提供更好的预测性能，但并不是所有的方法都可以模拟时间变化的交互和复杂的基线风险。为了解决这个问题，我们提出了一种称为案例基础神经网络（CBNNs）的新方法，它将案例基础抽样框架与灵活的神经网络结构相结合。通过使用一种新颖的抽样方案和数据增强来自然地考虑到截尾，我们构建了一个可以接受时间输入的前馈神经网络。CBNNs通过预测在给定时刻事件发生的概率来估计危险函数。我们通过模拟和三个案例研究使用两个时间依赖指标比较CBNNs与回归和神经网络基于生存分析方法的性能。首先，我们通过涉及复杂基线风险和时间变化交互的模拟来评估所有方法，其中包括CBNNs。

    Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
    
[^24]: GEC: 一种在MDP、POMDP和更多情况下交互式决策的统一框架

    GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond. (arXiv:2211.01962v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01962](http://arxiv.org/abs/2211.01962)

    本研究在交互式决策的框架下，提出了一种新的复杂度度量GEC，用于样本高效强化学习。该方法能够捕捉到探索和开发之间的权衡，将RL问题划分为低GEC和高GEC两个类别，并展示了低GEC类别的丰富性质。

    

    我们研究了在交互式决策的普遍框架下的样本高效强化学习（RL），该框架包括马尔可夫决策过程（MDP）、部分可观测马尔可夫决策过程（POMDP）和预测状态表示（PSR）作为特殊情况。为了找到赋予样本高效学习的最小假设，我们提出了一种新的复杂度度量，广义eluder系数（GEC），它表征了在线交互式决策中探索和开发之间的基本权衡。具体而言，GEC通过比较预测更新策略性能的误差与基于历史数据评估的样本内训练误差，来衡量探索的难度。我们展示了低GEC的RL问题形成了一个非常丰富的类别，其中包括低Bellman eluder维度问题、双线性类、低证人秩问题、PO-双线性类和广义正则PSR等。

    We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tr
    
[^25]: 从专家进行私密在线预测: 分离和更快的速率

    Private Online Prediction from Experts: Separations and Faster Rates. (arXiv:2210.13537v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13537](http://arxiv.org/abs/2210.13537)

    这篇论文提出了新的算法，用于在在线预测从专家中解决隐私约束的问题，并改进了现有算法的遗憾界限。研究结果表明，在纯差分隐私和近似差分隐私设置下，对于愚蠢敌手，在高维范围内的遗憾可以达到亚线性水平，与自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。

    

    在线预测从专家中是机器学习中一个基本的问题，并且已经有几篇论文研究了在隐私约束下的这个问题。我们提出并分析了针对这个问题的新算法，改进了非自适应对手的遗憾界限。对于近似差分隐私，我们的算法在随机设置下实现了遗憾界限为$\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$，对于愚蠢敌手实现了遗憾界限为$\tilde{O}(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$（其中$d$是专家数量）。对于纯DP，我们的算法是第一个在高维范围$d \ge T$ 的愚蠢敌手中获得亚线性遗憾的算法。此外，我们证明了自适应对手的新下界。我们的结果表明，与非私密设置不同，对于这个问题，自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。我们的下界也展示了一种在自适应对手和非自适应对手之间的分离。

    Online prediction from experts is a fundamental problem in machine learning and several works have studied this problem under privacy constraints. We propose and analyze new algorithms for this problem that improve over the regret bounds of the best existing algorithms for non-adaptive adversaries. For approximate differential privacy, our algorithms achieve regret bounds of $\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$ for the stochastic setting and $\tilde{O}(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$ for oblivious adversaries (where $d$ is the number of experts). For pure DP, our algorithms are the first to obtain sub-linear regret for oblivious adversaries in the high-dimensional regime $d \ge T$. Moreover, we prove new lower bounds for adaptive adversaries. Our results imply that unlike the non-private setting, there is a strong separation between the optimal regret for adaptive and non-adaptive adversaries for this problem. Our lower bounds also show a separation between 
    
[^26]: 使用符合预测改进专家预测

    Improving Expert Predictions with Conformal Prediction. (arXiv:2201.12006v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12006](http://arxiv.org/abs/2201.12006)

    本研究开发了一种自动决策支持系统，通过使用符合预测构建的标签预测集合，精确地权衡了真实标签不在预测集合中的概率。

    

    自动决策支持系统承诺帮助专家更高效准确地解决多类分类任务。然而，现有系统通常要求专家理解何时放弃自己的决策权以及何时行使自己的决策权。否则，专家可能更适合自行解决分类任务。在此工作中，我们开发了一种自动决策支持系统，它不需要专家理解何时信任系统以提高性能。我们的系统不提供单一的标签预测并让专家决定何时信任这些预测，而是提供使用符合预测构建的标签预测集合，并强制要求专家从这些集合中预测标签。通过使用符合预测，我们的系统可以精确地权衡真实标签不在预测集合中的概率，从而确定输出预测的频率。

    Automated decision support systems promise to help human experts solve multiclass classification tasks more efficiently and accurately. However, existing systems typically require experts to understand when to cede agency to the system or when to exercise their own agency. Otherwise, the experts may be better off solving the classification tasks on their own. In this work, we develop an automated decision support system that, by design, does not require experts to understand when to trust the system to improve performance. Rather than providing (single) label predictions and letting experts decide when to trust these predictions, our system provides sets of label predictions constructed using conformal prediction$\unicode{x2014}$prediction sets$\unicode{x2014}$and forcefully asks experts to predict labels from these sets. By using conformal prediction, our system can precisely trade-off the probability that the true label is not in the prediction set, which determines how frequently ou
    
[^27]: 充分统计记忆型AMP算法

    Sufficient-Statistic Memory AMP. (arXiv:2112.15327v4 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2112.15327](http://arxiv.org/abs/2112.15327)

    该论文提出了一种在特定条件下解决AMP类型算法收敛性问题的充分统计记忆型AMP算法框架，通过充分统计约束和特定条件下的协方差矩阵性质，实现了有效的信号重构。

    

    近似消息传递（AMP）类型的算法在某些大型随机线性系统的信号重构中被广泛使用。AMP类型算法的一个关键特点是它们的动态可以通过状态演化正确地描述。虽然状态演化是一个有用的分析工具，但其收敛性并不保证。为了在原则上解决AMP类型算法的状态演化的收敛问题，本文提出了一种在正确单位不变的感知矩阵、Lipschitz连续的本地处理器和充分统计约束下的充分统计记忆型AMP（SS-MAMP）算法框架。我们证明了SS-MAMP的协方差矩阵是L带状的且收敛，这是一个最优的框架（从本地MMSE/LMMSE角度）给定Lipschitz连续的AMP类型算法。

    Approximate message passing (AMP) type algorithms have been widely used in the signal reconstruction of certain large random linear systems. A key feature of the AMP-type algorithms is that their dynamics can be correctly described by state evolution. While state evolution is a useful analytic tool, its convergence is not guaranteed. To solve the convergence problem of the state evolution of AMP-type algorithms in principle, this paper proposes a sufficient-statistic memory AMP (SS-MAMP) algorithm framework under the conditions of right-unitarily invariant sensing matrices, Lipschitz-continuous local processors and the sufficient-statistic constraint (i.e., the current message of each local processor is a sufficient statistic of the signal vector given the current and all preceding messages). We show that the covariance matrices of SS-MAMP are L-banded and convergent, which is an optimal framework (from the local MMSE/LMMSE perspective) for AMP-type algorithms given the Lipschitz-conti
    
[^28]: 异质分布滞后模型用于估计孕妇暴露于空气污染物的个性化影响

    Heterogeneous Distributed Lag Models to Estimate Personalized Effects of Maternal Exposures to Air Pollution. (arXiv:2109.13763v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2109.13763](http://arxiv.org/abs/2109.13763)

    本研究提出了一种利用异质分布滞后模型和贝叶斯加性回归树的统计学习方法，用于个性化估计孕妇暴露于细颗粒物的关键时期和新生儿体重的关系。研究发现，关键时期和关联幅度在不同的个体、家庭和社区特征水平上表现出异质性。

    

    儿童健康研究支持孕妇环境暴露与儿童出生结果之间的关联。一个共同目标是确定易感性的关键时期-孕期内母体暴露与未来结果之间关联增强的时期。关键时期的时机和关联幅度在个体、家庭和社区特征的不同水平上可能是异质的。使用科罗拉多州的出生队列，我们估计了孕期每周对细颗粒物（PM$_{2.5}$）的个性化关系与新生儿体重之间的关系。为了实现这个目标，我们提出了一种统计学学习方法，结合分布滞后模型和贝叶斯加性回归树，以个体水平估计关键时期，并从一组潜在的修改因素中识别引起异质性的特征。我们发现PM$_{2.5}$的异质性证据。

    Children's health studies support an association between maternal environmental exposures and children's birth outcomes. A common goal is to identify critical windows of susceptibility--periods during gestation with increased association between maternal exposures and a future outcome. The timing of the critical windows and magnitude of the associations are likely heterogeneous across different levels of individual, family, and neighborhood characteristics. Using an administrative Colorado birth cohort we estimate the individualized relationship between weekly exposures to fine particulate matter (PM$_{2.5}$) during gestation and birth weight. To achieve this goal, we propose a statistical learning method combining distributed lag models and Bayesian additive regression trees to estimate critical windows at the individual level and identify characteristics that induce heterogeneity from a high-dimensional set of potential modifying factors. We find evidence of heterogeneity in the PM$_
    
[^29]: 贝叶斯学习规则

    The Bayesian Learning Rule. (arXiv:2107.04562v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.04562](http://arxiv.org/abs/2107.04562)

    许多机器学习算法都可以归结为贝叶斯学习规则，该规则通过利用自然梯度来逼近后验分布，从而得到广泛的算法应用。这一工作不仅统一了现有算法，还帮助我们设计新的算法。

    

    我们展示了许多机器学习算法是一个称为贝叶斯学习规则的单一算法的特例。这个规则是从贝叶斯原理推导出来的，可以从优化、深度学习和图形模型等领域得到广泛的算法。这包括经典算法如岭回归、牛顿法和卡尔曼滤波器，以及现代深度学习算法如随机梯度下降、RMSprop和Dropout。推导这些算法的关键思想是使用自然梯度估计的候选分布来逼近后验分布。不同的候选分布会导致不同的算法，对自然梯度的进一步逼近则会产生这些算法的变种。我们的工作不仅统一、泛化和改进了现有算法，还帮助我们设计新的算法。

    We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.
    
[^30]: 使用Wasserstein生成对抗网络近似概率分布

    Approximating Probability Distributions by using Wasserstein Generative Adversarial Networks. (arXiv:2103.10060v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.10060](http://arxiv.org/abs/2103.10060)

    本文研究了Wasserstein生成对抗网络（WGANs），并使用GroupSort神经网络作为鉴别器。研究结果显示，生成器和鉴别器的容量对目标分布的逼近误差有影响，并且WGANs对鉴别器的容量要求高于生成器。此外，在鉴别器不足够强大时，低容量的生成器可能比过度深层和宽度的生成器效果更好。数值结果证实了理论结果。

    

    本文研究了将GroupSort神经网络作为鉴别器的Wasserstein生成对抗网络（WGANs）。研究结果表明，目标分布的逼近误差界限取决于生成器和鉴别器的宽度和深度（容量）以及训练中的样本数量。针对生成的分布和目标分布之间的Wasserstein距离建立了量化的泛化界限。根据理论结果，WGANs对鉴别器的容量要求比生成器更高，这与一些已有结果一致。更重要的是，如果鉴别器不足够强大，与过度深层和宽度（高容量）的生成器相比，低容量的生成器的结果可能更差。使用Swiss roll和MNIST数据集得到的数值结果证实了理论结果。

    Studied here are Wasserstein generative adversarial networks (WGANs) with GroupSort neural networks as their discriminators. It is shown that the error bound of the approximation for the target distribution depends on the width and depth (capacity) of the generators and discriminators and the number of samples in training. A quantified generalization bound is established for the Wasserstein distance between the generated and target distributions. According to the theoretical results, WGANs have a higher requirement for the capacity of discriminators than that of generators, which is consistent with some existing results. More importantly, the results with overly deep and wide (high-capacity) generators may be worse than those with low-capacity generators if discriminators are insufficiently strong. Numerical results obtained using Swiss roll and MNIST datasets confirm the theoretical results.
    
[^31]: 一种整合和分类正态分布的方法

    A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14331](http://arxiv.org/abs/2012.14331)

    本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。

    

    单变量和多变量正态概率分布在模拟不确定性决策中被广泛使用。计算这些模型的性能需要在特定区域内对这些分布进行积分，这在不同的模型中可以有很大的差异。除了一些特殊情况，目前不存在通用的分析表达式、标准数值方法或软件来计算这些积分。本文提供了数学结果和开源软件，可以提供以下内容：（i）任意参数维度下任意域内法向的概率，（ii）法向向量函数的概率密度、累积分布和逆累积分布，（iii）任意数量正态分布之间的分类误差、贝叶斯最优辨别指数以及其与工作特征曲线的关系，（iv）此类问题的维度降低和可视化，以及（v）对于给定数据这些方法的可靠性测试。我们通过几个具体的例子，包括金融、生物和心理学来演示这些功能。

    Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
    
[^32]: 因果规则集合: 可解释的异质性治疗效应的发现和推理

    Causal Rule Ensemble: Interpretable Discovery and Inference of Heterogeneous Treatment Effects. (arXiv:2009.09036v5 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2009.09036](http://arxiv.org/abs/2009.09036)

    本论文介绍了一种新的因果规则集合(CRE)方法，通过集成树的方式，可解释的发现和估计异质性治疗效应(HTE)，具有稳定性和对复杂异质性模式的探索能力。

    

    在健康和社会科学中，确定研究人群中存在明显的治疗效应异质性(HTE)的亚群是至关重要的。由于其高度可解释性，决策树已被提出并广泛应用于数据驱动的HTE发现。然而，单一决策树发现HTE可能不稳定且过于简化。本文介绍了一种新的HTE发现和估计方法——因果规则集合(CRE)，采用集成树方法。CRE具有以下几个关键特点: 1) 可解释的HTE表示; 2) 探索复杂的异质性模式的能力; 3) 在亚群发现中具有高稳定性。所发现的亚群是通过可解释的决策规则定义的。亚群特定因果效应的估计通过两阶段方法进行，我们提供了理论保证。通过模拟实验证明，CRE方法是可行的。

    In health and social sciences, it is critically important to identify subgroups of the study population where there is notable heterogeneity of treatment effects (HTE) with respect to the population average. Decision trees have been proposed and commonly adopted for data-driven discovery of HTE due to their high level of interpretability. However, single-tree discovery of HTE can be unstable and oversimplified. This paper introduces Causal Rule Ensemble (CRE), a new method for HTE discovery and estimation through an ensemble-of-trees approach. CRE offers several key features, including 1) an interpretable representation of the HTE; 2) the ability to explore complex heterogeneity patterns; and 3) high stability in subgroups discovery. The discovered subgroups are defined in terms of interpretable decision rules. Estimation of subgroup-specific causal effects is performed via a two-stage approach for which we provide theoretical guarantees. Via simulations, we show that the CRE method is
    
[^33]: 高维数据丰富化：可解释、快速和数据有效

    High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient. (arXiv:1806.04047v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1806.04047](http://arxiv.org/abs/1806.04047)

    本文研究了在高维设置中的多任务学习问题，并引入了一个估计器来处理多连接线性回归问题，称为数据丰富/共享。我们通过凸函数来描述公共参数和个体参数的结构，并提出了一种具有几何收敛速度的迭代估计算法。

    

    我们考虑在高维设置中的多任务学习问题。特别地，我们引入了一个估计器，并研究了其在多个连接线性回归问题中的统计和计算特性，该问题被称为数据丰富/共享。任务间的连接由跨任务的“公共参数”捕捉，该参数通过任务级的“个体参数”进行细化。任何凸函数，如范数，都可以表征公共参数和个体参数的结构。我们勾勒了我们估计器的样本复杂度，并在几何条件下为所有参数的估计误差提供了高概率的非渐进边界。我们展示了从汇集样本中受益于公共参数的恢复。我们提出了一种具有几何收敛速度的迭代估计算法，并通过合成数据的实验补充了我们的理论分析。总的来说，我们提供了第一个全面的统计和计算分析，用于解决高维数据丰富化问题。

    We consider the problem of multi-task learning in the high dimensional setting. In particular, we introduce an estimator and investigate its statistical and computational properties for the problem of multiple connected linear regressions known as Data Enrichment/Sharing. The between-tasks connections are captured by a cross-tasks \emph{common parameter}, which gets refined by per-task \emph{individual parameters}. Any convex function, e.g., norm, can characterize the structure of both common and individual parameters. We delineate the sample complexity of our estimator and provide a high probability non-asymptotic bound for estimation error of all parameters under a geometric condition. We show that the recovery of the common parameter benefits from \emph{all} of the pooled samples. We propose an iterative estimation algorithm with a geometric convergence rate and supplement our theoretical analysis with experiments on synthetic data. Overall, we present a first thorough statistical a
    
[^34]: 一种用于高斯混合模型的贝叶斯滤波算法

    A Bayesian Filtering Algorithm for Gaussian Mixture Models. (arXiv:1705.05495v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1705.05495](http://arxiv.org/abs/1705.05495)

    本文提出了一种用于高斯混合模型的贝叶斯滤波算法，通过高斯混合简化步骤处理混合项数量的指数增长，并提出了统一算法的平方根实现。该算法在几个模拟系统上进行了测试。

    

    为一类可通过高斯混合模型建模的状态空间系统开发了一种贝叶斯滤波算法。一般来说，这个滤波问题的精确解涉及到混合项数量的指数增长，在此通过在时间更新和测量更新之后利用高斯混合简化步骤处理。此外，还提出了统一算法的平方根实现，并在几个模拟系统上进行了概要分析。其中包括两个非线性系统的状态估计，这些系统严格外于本文考虑的类别。

    A Bayesian filtering algorithm is developed for a class of state-space systems that can be modelled via Gaussian mixtures. In general, the exact solution to this filtering problem involves an exponential growth in the number of mixture terms and this is handled here by utilising a Gaussian mixture reduction step after both the time and measurement updates. In addition, a square-root implementation of the unified algorithm is presented and this algorithm is profiled on several simulated systems. This includes the state estimation for two non-linear systems that are strictly outside the class considered in this paper.
    

