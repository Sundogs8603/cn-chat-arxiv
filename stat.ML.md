# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Consistent model selection in the spiked Wigner model via AIC-type criteria.](http://arxiv.org/abs/2307.12982) | 该论文介绍了在带尖峰的Wigner模型中使用AIC类型准则进行一致性模型选择。研究发现，对于$\gamma > 2$，该准则是强一致估计的，而对于$\gamma < 2$，它几乎肯定会高估尖峰数量$k$。此外，作者还提出了一个使AIC弱一致估计的方法，并证明了某个软最小化器是强一致估计的。 |
| [^2] | [Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems.](http://arxiv.org/abs/2307.12975) | 该论文研究了基于偏好的政策学习方法在离线情境多臂赌博问题中的优势，并通过改进建模和分析，证明了这一方法相比其他政策学习方法具有更低的次优性。 |
| [^3] | [Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques.](http://arxiv.org/abs/2307.12971) | 本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。 |
| [^4] | [Anytime Model Selection in Linear Bandits.](http://arxiv.org/abs/2307.12897) | 该论文提出了一种在线性赌博机中进行任意模型选择的方法，通过模拟全信息反馈实现在遗憾方面具有指数改进的性能，并且不依赖时间界限和纯探索阶段。 |
| [^5] | [Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs).](http://arxiv.org/abs/2307.12862) | 该论文提出了一种基于随机步进特征选择的指数随机图模型 (ERGMs) 方法，用于解决社交网络建模中的多个挑战并增强模型的准确性和可解释性。 |
| [^6] | [Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials.](http://arxiv.org/abs/2307.12840) | 通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。 |
| [^7] | [Causal Fair Machine Learning via Rank-Preserving Interventional Distributions.](http://arxiv.org/abs/2307.12797) | 通过保持排序的干预分布，我们提出了一种因果公平的机器学习方法，通过在一个理想的世界中消除受保护属性对目标的因果影响来减少不公平。 |
| [^8] | [Concept-based explainability for an EEG transformer model.](http://arxiv.org/abs/2307.12745) | 本研究尝试将基于概念激活向量（CAVs）的方法应用于脑电图（EEG）数据的解释性，通过定义解释性概念和选择相关数据集，以实现对大规模转换器模型中深度学习模型的理解。 |
| [^9] | [Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport.](http://arxiv.org/abs/2307.12703) | 该论文提出了一种新的算法，通过引入相关路径来降低蒙特卡洛模拟中的方差，从而估计随机微分方程解的函数。通过政策梯度和强化学习技术，使用深度神经网络近似最优相关函数并进行校准。这与最大最优传输问题有关。 |
| [^10] | [InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis.](http://arxiv.org/abs/2307.12586) | InVAErt网络是一个数据驱动的框架，用于分析和合成物理系统，具有模型反演和可识别性分析的能力。 |
| [^11] | [Adaptive debiased machine learning using data-driven model selection techniques.](http://arxiv.org/abs/2307.12544) | 提出了一种自适应去偏机器学习（ADML）框架，通过结合数据驱动的模型选择和去偏机器学习技术，构建了渐进线性、自适应和超效率的路径可微的功能估计器。 |
| [^12] | [Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations.](http://arxiv.org/abs/2307.12508) | 在这篇论文中，我们研究了Wasserstein统计在仿射变形统计模型中的信息几何特征，比较了信息几何和Wasserstein几何的估计器的优缺点，并发现Wasserstein估计量在椭圆对称仿射变形模型中是矩估计量，在波形为高斯分布时与信息几何估计量重合。 |
| [^13] | [A faster and simpler algorithm for learning shallow networks.](http://arxiv.org/abs/2307.12496) | 本文提出了一种更简单的算法来学习浅层网络，其运行时间更短且只需要一个阶段即可。 |
| [^14] | [Model-free generalized fiducial inference.](http://arxiv.org/abs/2307.12472) | 本文提出了一种无模型的统计框架，用于不准确概率预测推理的不确定性量化，并考虑了精确概率近似模型无关的不准确推理框架的特性。 |
| [^15] | [Rates of Approximation by ReLU Shallow Neural Networks.](http://arxiv.org/abs/2307.12461) | 本文提供了ReLU浅层神经网络的均匀逼近率，可以以接近于最优逼近率的速度在H\"older空间中逼近函数。 |
| [^16] | [Information-theoretic Analysis of Test Data Sensitivity in Uncertainty.](http://arxiv.org/abs/2307.12456) | 本篇论文进行了关于贝叶斯推断中预测不确定性的信息论分析，将其严格分解为本质不确定性和认知不确定性。然而，现有的分析方法无法解释测试数据和训练数据之间的敏感性。本文通过使用新颖的分解方法研究了这种不确定性敏感性。 |
| [^17] | [DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of C{\alpha} Protein Traces.](http://arxiv.org/abs/2307.12451) | DiAMoNDBack是一个弥散去噪的自回归模型，用于将粗粒化蛋白质模型恢复到全原子分辨率，通过利用C{\alpha}坐标的信息和局部邻域中的先前反映射的骨架和侧链原子。 |
| [^18] | [Concentration for high-dimensional linear processes with dependent innovations.](http://arxiv.org/abs/2307.12395) | 本论文提出了一种针对高维线性过程的具有相关创新的集中度不等式，并利用该不等式获得了线性过程滞后自协方差矩阵最大分量范数的集中度界限。这些结果在估计高维向量自回归过程、时间序列引导和长期协方差矩阵估计中具有重要应用价值。 |
| [^19] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^20] | [Geometry-Aware Adaptation for Pretrained Models.](http://arxiv.org/abs/2307.12226) | 本论文提出了一种简单的方法，利用标签之间的距离关系来调整已训练的模型，以可靠地预测新类别或改善零样本预测的性能，而无需额外的训练。 |
| [^21] | [The Sample Complexity of Multi-Distribution Learning for VC Classes.](http://arxiv.org/abs/2307.12135) | 这个论文研究了多分布学习对VC类的样本复杂度，发现现有的上下界存在显著差距，并讨论了一些涉及统计学习中博弈动态的困难。 |
| [^22] | [Out-of-Distribution Optimality of Invariant Risk Minimization.](http://arxiv.org/abs/2307.11972) | 本文旨在提供IRM的理论验证，严格证明了解决方案可以最小化区外风险。 |
| [^23] | [Collaboratively Learning Linear Models with Structured Missing Data.](http://arxiv.org/abs/2307.11947) | 本论文研究了协同学习线性模型的问题，提出了一种分布式、半监督的算法Collab，该算法在无法访问标记数据的情况下具有通信效率和实用性，同时在渐近局部极小极值方面也表现出优异的性能。 |
| [^24] | [A Topical Approach to Capturing Customer Insight In Social Media.](http://arxiv.org/abs/2307.11775) | 本研究通过嵌入狄利克雷过程，嵌入层次狄利克雷过程和面向时间的动态嵌入三种方法，解决了在嘈杂的大数据环境中完全无监督的主题提取挑战。 |
| [^25] | [Towards Vertical Privacy-Preserving Symbolic Regression via Secure Multiparty Computation.](http://arxiv.org/abs/2307.11756) | 通过安全多方计算实现垂直隐私保护的符号回归。 |
| [^26] | [Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization.](http://arxiv.org/abs/2307.11007) | 本文研究发现，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。我们的结果表明，尖锐性与泛化之间的关系取决于数据分布和模型架构。 |
| [^27] | [Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression.](http://arxiv.org/abs/2307.08044) | 本研究提出了一种深度AFT排名回归模型，用于灵活地进行时间事件建模，从而改善预测性能并减轻严格的假设。 |
| [^28] | [Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems.](http://arxiv.org/abs/2307.06538) | 本论文以张量分解方法为基础，提出了学习线性动力系统混合模型的新方法。算法成功地应用于没有组件分离条件的情况，并可以与贝叶斯最优聚类竞争。此外，算法可以在部分观测设置下工作。 |
| [^29] | [PFNs Are Flexible Models for Real-World Bayesian Optimization.](http://arxiv.org/abs/2305.17535) | 本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。 |
| [^30] | [A Transfer Principle: Universal Approximators Between Metric Spaces From Euclidean Universal Approximators.](http://arxiv.org/abs/2304.12231) | 本论文提出使用欧几里得空间通用逼近器为构建块，构建了在任意波兰度量空间 $\mathcal{X}$ 和 $\mathcal{Y}$ 之间的通用逼近器，并通过随机化输出离散概率测度来克服某些限制。在适当的结构下提供了概率和定量保证。 |
| [^31] | [Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective.](http://arxiv.org/abs/2304.06833) | 本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。 |
| [^32] | [How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding.](http://arxiv.org/abs/2303.04245) | 本文提供了对Transformer学习语义结构的机制性理解，通过数学分析和实验证明了嵌入层和自注意力层如何对词汇的共现结构进行编码。 |
| [^33] | [From Graph Generation to Graph Classification.](http://arxiv.org/abs/2302.07989) | 本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。 |
| [^34] | [DNArch: Learning Convolutional Neural Architectures by Backpropagation.](http://arxiv.org/abs/2302.05400) | DNArch是一种通过反向传播同时学习卷积神经网络的权重和架构的方法，它不仅可以学习每一层的卷积核大小和通道数，还可以学习网络的深度和下采样层的位置和值。与现有方法不同的是，DNArch不限于预定义的神经组件，能够发现各种核大小、宽度、深度和下采样组合中的整个CNN架构。在实验中，DNArch在多个分类和密集预测任务上找到了高性能的CNN架构。 |
| [^35] | [Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data.](http://arxiv.org/abs/2210.17230) | 构建了一种新的生成算法类，能够有效地学习稀缺高维数据的任意目标分布并生成新样本，具有很好的数据整合能力。 |
| [^36] | [Minimax Optimal Kernel Operator Learning via Multilevel Training.](http://arxiv.org/abs/2209.14430) | 本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限，在多层级训练方法下，通过学习偏差以下的谱分量和忽略方差以上的分量，可以达到最优的学习速率。 |
| [^37] | [Approximate blocked Gibbs sampling for Bayesian neural networks.](http://arxiv.org/abs/2208.11389) | 本文提出了一种近似阻塞Gibbs采样方法，可以更可行地进行小批量MCMC采样，提高了前馈神经网络的预测准确性和预测不确定性的量化能力。 |
| [^38] | [A Nonparametric Approach with Marginals for Modeling Consumer Choice.](http://arxiv.org/abs/2208.06115) | 本文提出了一种基于边缘分布的简单而有效的非参数消费者选择建模方法，在任何选择集合中会把选择概率的集合一致地描述出来。 |
| [^39] | [TF-GNN: Graph Neural Networks in TensorFlow.](http://arxiv.org/abs/2207.03522) | TF-GNN是一个可扩展的 TensorFlow 图神经网络库，用于支持丰富的异构图数据。它提供了低代码解决方案，并广泛应用于谷歌的生产模型中。这个库最近作为开源项目发布，为图学习提供了强大的工具。 |
| [^40] | [When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction.](http://arxiv.org/abs/2206.02058) | 本研究发现，使用群体属性个性化机器学习模型可能降低群体水平的性能。为了确保在预测任务中公平使用群体属性，我们提出了形式化条件，并提供了相应的解决方法。我们的实证研究表明，在临床预测任务中普遍存在公平使用违规的情况，但我们也找到了简单干预手段来减轻其伤害。 |
| [^41] | [Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis.](http://arxiv.org/abs/2205.09801) | 本文通过代数分析改进了图神经网络（GNN）的表达能力，证明了GNN能够比Weisfeiler-Lehman（WL）算法更好地产生区分性表示，特别是在具有不同特征值的图上。此外，我们还发现简单的卷积结构与无信息输入产生的等变特征比WL表示更具表达能力。 |
| [^42] | [Generalizing similarity in noisy setups: the DIBS phenomenon.](http://arxiv.org/abs/2201.12803) | 本研究揭示了数据密度、噪声和相似性学习之间的相互作用，证明了数据对的密度对于泛化至关重要，并发现了一种在密集数据集上比对称标签噪声更差的泛化性能的现象，称为密度诱导的相似性破坏（DIBS）。 |
| [^43] | [Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects.](http://arxiv.org/abs/2110.08693) | 本文提出了一种扩展平方根速度函数的新表示方法和度量方法，用于分析和比较树状三维物体，从而提高物体形状差异计算的精度和效率。 |
| [^44] | [Learning Optimal Prescriptive Trees from Observational Data.](http://arxiv.org/abs/2108.13628) | 该论文介绍了一种从观察数据中学习最优处方树的方法，可以在不需要数据随机化和对树有严格假设的情况下，通过混合整数优化技术进行学习，并具有建模领域特定问题的能力。 |
| [^45] | [Learning Gaussian Graphical Models with Latent Confounders.](http://arxiv.org/abs/2105.06600) | 本文比较了两种处理具有潜在混淆因素的图模型的方法，并提出了一种结合了这两种方法优势的新方法。作者通过理论和实证研究证明了这种方法的可行性和有效性。 |

# 详细

[^1]: 在带尖峰的Wigner模型中通过AIC类型准则进行一致性模型选择

    Consistent model selection in the spiked Wigner model via AIC-type criteria. (arXiv:2307.12982v1 [math.ST])

    [http://arxiv.org/abs/2307.12982](http://arxiv.org/abs/2307.12982)

    该论文介绍了在带尖峰的Wigner模型中使用AIC类型准则进行一致性模型选择。研究发现，对于$\gamma > 2$，该准则是强一致估计的，而对于$\gamma < 2$，它几乎肯定会高估尖峰数量$k$。此外，作者还提出了一个使AIC弱一致估计的方法，并证明了某个软最小化器是强一致估计的。

    

    考虑带尖峰的Wigner模型\[ X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \]其中$G$是一个$N \times N$的GOE随机矩阵，而特征值$\lambda_i$都是有尖峰的，即超过了Baik-Ben Arous-P\'ech\'e (BBP)的阈值$\sigma$。我们考虑形式为\[ -2 \, (\text{最大化的对数似然}) + \gamma \, (\text{参数数量}) \]的AIC类型模型选择准则，用于估计尖峰数量$k$。对于$\gamma > 2$，上述准则是强一致估计的，前提是$\lambda_k > \lambda_{\gamma}$，其中$\lambda_{\gamma}$是严格高于BBP阈值的阈值，而对于$\gamma < 2$，它几乎肯定会高估$k$。虽然AIC（对应于$\gamma = 2$）并非强一致估计，但我们证明，取$\gamma = 2 + \delta_N$，其中$\delta_N \to 0$且$\delta_N \gg N^{-2/3}$，会得到$k$的弱一致估计量。我们还证明了AIC的某个软最小化器是强一致估计的。

    Consider the spiked Wigner model \[ X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[ -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma > 2$, the above criterion is strongly consistent provided $\lambda_k > \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma < 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We also show that a certain soft minimiser of AIC is strongly consistent.
    
[^2]: 从人类偏好中学习的政策在情境多臂赌博问题中的可证明优势

    Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems. (arXiv:2307.12975v1 [cs.LG])

    [http://arxiv.org/abs/2307.12975](http://arxiv.org/abs/2307.12975)

    该论文研究了基于偏好的政策学习方法在离线情境多臂赌博问题中的优势，并通过改进建模和分析，证明了这一方法相比其他政策学习方法具有更低的次优性。

    

    在决策问题中，奖励工程是一个关键的任务。在实践中，往往不存在明显的奖励函数选择。因此，一种常见的方法是在训练过程中引入人类反馈，并利用这种反馈来学习奖励函数。在使用人类反馈的所有政策学习方法中，基于偏好的方法在最近的实证应用中取得了显著的成功，如InstructGPT。在这项工作中，我们开发了一个理论，可以证明在离线情境多臂赌博问题中，基于偏好的方法具有显著的优势。具体而言，我们改进了在人类评分样本上运行政策学习方法的建模和次优性分析。然后，我们将其与基于偏好的方法的次优性保证进行比较，并表明基于偏好的方法享有更低的次优性。

    A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
    
[^3]: 大数据-供应链管理框架的预测：数据预处理和机器学习技术

    Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])

    [http://arxiv.org/abs/2307.12971](http://arxiv.org/abs/2307.12971)

    本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。

    

    本文旨在系统地识别和比较分析最先进的供应链预测策略和技术。提出了一个新的框架，将大数据分析应用于供应链管理中，包括问题识别、数据来源、探索性数据分析、机器学习模型训练、超参数调优、性能评估和优化，以及预测对人力、库存和整个供应链的影响。首先讨论了根据供应链策略收集数据的需求以及如何收集数据。文章讨论了根据周期或供应链目标需要不同类型的预测。推荐使用供应链绩效指标和误差测量系统来优化表现最佳的模型。还讨论了幻影库存对预测的不利影响以及管理决策依赖供应链绩效指标来确定模型性能参数和改进运营管理、透明度的问题。

    This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
    
[^4]: 线性赌博机中的任意模型选择

    Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v1 [stat.ML])

    [http://arxiv.org/abs/2307.12897](http://arxiv.org/abs/2307.12897)

    该论文提出了一种在线性赌博机中进行任意模型选择的方法，通过模拟全信息反馈实现在遗憾方面具有指数改进的性能，并且不依赖时间界限和纯探索阶段。

    

    在赌博优化中，模型选择是一个具有挑战性的问题，因为它不仅需要在行动选择方面平衡探索和开发，还需要在模型选择方面平衡探索和开发。一种自然的方法是依赖于将不同模型视为专家的在线学习算法。然而，现有方法在遗憾方面与模型数量$M$的规模（$\text{poly}M$）呈不良的关系。我们的关键洞察是，在线性赌博机的模型选择中，我们可以通过有利的偏差-方差权衡来模拟全信息反馈给在线学习者。这使得我们能够开发出具有指数改进（$\log M$）在遗憾方面对$M$依赖性的ALEXP。ALEXP在遗憾方面具有任意保证，并且既不需要对时间界$n$具有知识，也不依赖于初始的纯探索阶段。我们的方法利用了Lasso的一种新颖的时间均匀分析，建立了在线学习和高维统计之间的新连接。

    Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
    
[^5]: Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)

    Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs). (arXiv:2307.12862v1 [cs.SI])

    [http://arxiv.org/abs/2307.12862](http://arxiv.org/abs/2307.12862)

    该论文提出了一种基于随机步进特征选择的指数随机图模型 (ERGMs) 方法，用于解决社交网络建模中的多个挑战并增强模型的准确性和可解释性。

    

    社交网络的统计分析为各个科学领域提供了宝贵的见解，然而，由于计算负担沉重且需要考虑观察到的网络依赖关系，准确建模网络仍具有挑战性。指数随机图模型（ERGM）作为一种有前景的技术，在社交网络建模中应用于捕捉通过整合内生变量来体现网络依赖关系。然而，使用ERGM存在多个挑战，包括ERGM退化问题，从而生成不现实且毫无意义的网络结构。为了解决这些挑战并增强协作网络的建模能力，我们提出并测试了一种新方法，该方法专注于ERGM内生变量的选择。我们的方法旨在应对计算负担，并改善对观察到的网络依赖关系的适应性，从而促进更准确和有意义的解释。

    Statistical analysis of social networks provides valuable insights into complex network interactions across various scientific disciplines. However, accurate modeling of networks remains challenging due to the heavy computational burden and the need to account for observed network dependencies. Exponential Random Graph Models (ERGMs) have emerged as a promising technique used in social network modeling to capture network dependencies by incorporating endogenous variables. Nevertheless, using ERGMs poses multiple challenges, including the occurrence of ERGM degeneracy, which generates unrealistic and meaningless network structures. To address these challenges and enhance the modeling of collaboration networks, we propose and test a novel approach that focuses on endogenous variable selection within ERGMs. Our method aims to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations o
    
[^6]: 通过舒尔多项式高效学习具有一个隐藏层的ReLU网络

    Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials. (arXiv:2307.12840v1 [cs.LG])

    [http://arxiv.org/abs/2307.12840](http://arxiv.org/abs/2307.12840)

    通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。

    

    我们研究了在标准高斯分布下，关于平方损失的PAC学习$k$个ReLU激活的线性组合的问题。我们的主要结果是针对这个学习任务的一种高效算法，其样本和计算复杂性为$(dk/\epsilon)^{O(k)}$，其中$\epsilon>0$是目标精度。之前的工作给出了一个复杂性为$(dk/\epsilon)^{h(k)}$的算法，其中函数$h(k)$在$k$上的规模是超多项式的。有趣的是，我们的算法在相关统计查询算法类中接近最优。总体而言，我们的算法使用张量分解来识别一个子空间，使得所有$O(k)$阶矩在正交方向上都很小。其分析基于舒尔多项式理论，以显示较低阶误差张量的情况下，更高阶的误差张量也很小。

    We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
    
[^7]: 通过保持排序的干预分布实现因果公平的机器学习

    Causal Fair Machine Learning via Rank-Preserving Interventional Distributions. (arXiv:2307.12797v1 [cs.LG])

    [http://arxiv.org/abs/2307.12797](http://arxiv.org/abs/2307.12797)

    通过保持排序的干预分布，我们提出了一种因果公平的机器学习方法，通过在一个理想的世界中消除受保护属性对目标的因果影响来减少不公平。

    

    如果相同的个体得到相同的对待，而不同的个体得到不同的对待，那么一个决策被定义为公平的。根据这个定义，在设计机器学习模型以减少自动决策系统中的不公平时，必须引入因果思考来引入受保护属性。根据最近的提议，我们将个体定义为在一个假设的、理想的（FiND）世界中是规范上相等的，这个世界中受保护属性对目标没有（直接或间接）的因果影响。我们提出保持排序的干预分布来定义这个FiND世界的估计目标，并提出了一个估计方法。通过模拟和实证数据的验证，我们提供了对方法和生成模型的评价标准。通过这些，我们展示了我们的干预方法有效地识别出最受歧视的个体并减少不公平。

    A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
    
[^8]: EEG转换器模型的基于概念的可解释性

    Concept-based explainability for an EEG transformer model. (arXiv:2307.12745v1 [cs.LG])

    [http://arxiv.org/abs/2307.12745](http://arxiv.org/abs/2307.12745)

    本研究尝试将基于概念激活向量（CAVs）的方法应用于脑电图（EEG）数据的解释性，通过定义解释性概念和选择相关数据集，以实现对大规模转换器模型中深度学习模型的理解。

    

    深度学习模型由于其规模、结构以及训练过程中的内在随机性而变得复杂。选择数据集和归纳偏见也增加了额外的复杂性。为了解释这些挑战，Kim等人（2018）引入了概念激活向量（CAVs），旨在从人类对齐的概念角度理解深度模型的内部状态。这些概念对应于潜在空间中的方向，使用线性判别法进行识别。尽管该方法首先应用于图像分类，但后来被适应到包括自然语言处理在内的其他领域。在本研究中，我们尝试将该方法应用于Kostas等人的BENDR（2021）的脑电图（EEG）数据，以实现可解释性。这项努力的关键部分包括定义解释性概念和选择相关数据集以将概念与潜在空间相对应。我们的重点是EEG概念形成的两个机制。

    Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation
    
[^9]: 政策梯度最优相关搜索用于蒙特卡洛模拟和最大最优传输中的方差降低

    Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport. (arXiv:2307.12703v1 [stat.ML])

    [http://arxiv.org/abs/2307.12703](http://arxiv.org/abs/2307.12703)

    该论文提出了一种新的算法，通过引入相关路径来降低蒙特卡洛模拟中的方差，从而估计随机微分方程解的函数。通过政策梯度和强化学习技术，使用深度神经网络近似最优相关函数并进行校准。这与最大最优传输问题有关。

    

    我们提出了一种用于估计$f(X_T)$的方差降低算法，其中$X$是某个随机微分方程的解，$f$是一个测试函数。新的估计器是$(f(X^1_T) + f(X^2_T))/2$，其中$X^1$和$X^2$具有与$X$相同的边际分布，但路径上存在相关性，以降低方差。最优相关函数$\rho$由深度神经网络近似，并通过政策梯度和强化学习技术在$(X^1, X^2)$的轨迹上进行校准。在给定边际分布的情况下找到最优耦合与最大最优传输有关联。

    We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
    
[^10]: InVAErt网络：一个数据驱动的框架用于仿真、推理和可识别性分析。

    InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis. (arXiv:2307.12586v1 [cs.LG])

    [http://arxiv.org/abs/2307.12586](http://arxiv.org/abs/2307.12586)

    InVAErt网络是一个数据驱动的框架，用于分析和合成物理系统，具有模型反演和可识别性分析的能力。

    

    目前，基于物理的系统使用生成模型和深度学习主要用于仿真任务。然而，数据驱动结构提供的出色灵活性表明应将该表示扩展到系统综合的其他方面，包括模型反演和可识别性。我们引入了InVAErt网络，这是一个综合的数据驱动分析和合成参数化物理系统的框架，它使用确定性编码器和解码器表示前向和逆向解映射，用归一化流来捕捉系统输出的概率分布，并设计了一种变分编码器来学习输入和输出之间缺乏双射性的紧凑潜在表示。我们正式研究了损失函数中惩罚系数的选择和潜在空间采样策略，因为我们发现这些因素会显著影响训练和测试性能。我们有效地验证了我们的模型。

    Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We valid
    
[^11]: 自适应去偏机器学习方法及数据驱动模型选择技术

    Adaptive debiased machine learning using data-driven model selection techniques. (arXiv:2307.12544v1 [stat.ME])

    [http://arxiv.org/abs/2307.12544](http://arxiv.org/abs/2307.12544)

    提出了一种自适应去偏机器学习（ADML）框架，通过结合数据驱动的模型选择和去偏机器学习技术，构建了渐进线性、自适应和超效率的路径可微的功能估计器。

    

    非参数推断中的去偏机器学习估计器可能存在过高的变异性和不稳定性。为了解决这个问题，我们提出了自适应去偏机器学习（ADML）框架，通过结合数据驱动的模型选择和去偏机器学习技术，构建渐进线性、自适应和超效率的路径可微的功能估计器。通过从数据中直接学习模型结构，ADML避免了模型规范错误引入的偏差，并摆脱了参数化和半参数化模型的限制。

    Debiased machine learning estimators for nonparametric inference of smooth functionals of the data-generating distribution can suffer from excessive variability and instability. For this reason, practitioners may resort to simpler models based on parametric or semiparametric assumptions. However, such simplifying assumptions may fail to hold, and estimates may then be biased due to model misspecification. To address this problem, we propose Adaptive Debiased Machine Learning (ADML), a nonparametric framework that combines data-driven model selection and debiased machine learning techniques to construct asymptotically linear, adaptive, and superefficient estimators for pathwise differentiable functionals. By learning model structure directly from data, ADML avoids the bias introduced by model misspecification and remains free from the restrictions of parametric and semiparametric models. While they may exhibit irregular behavior for the target parameter in a nonparametric statistical mo
    
[^12]: 形状和仿射变形的Wasserstein统计的信息几何

    Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations. (arXiv:2307.12508v1 [math.ST])

    [http://arxiv.org/abs/2307.12508](http://arxiv.org/abs/2307.12508)

    在这篇论文中，我们研究了Wasserstein统计在仿射变形统计模型中的信息几何特征，比较了信息几何和Wasserstein几何的估计器的优缺点，并发现Wasserstein估计量在椭圆对称仿射变形模型中是矩估计量，在波形为高斯分布时与信息几何估计量重合。

    

    信息几何和Wasserstein几何是介绍概率分布流形中的两个主要结构，它们捕捉了不同的特征。我们在仿射变形统计模型的Li和Zhao（2023）框架中研究了Wasserstein几何的特征，它是位置-尺度模型的多维泛化。我们比较了基于信息几何和Wasserstein几何的估计器的优点和缺点。在Wasserstein几何中，概率分布的形状和仿射变形是分离的，表明在对波形扰动具有鲁棒性的同时，会损失Fisher效率。我们证明了在椭圆对称仿射变形模型的情况下Wasserstein估计量是矩估计量。它与信息几何估计量（最大似然估计量）仅在波形为高斯分布时重合。Wasserstein效率的作用是...

    Information geometry and Wasserstein geometry are two main structures introduced in a manifold of probability distributions, and they capture its different characteristics. We study characteristics of Wasserstein geometry in the framework of Li and Zhao (2023) for the affine deformation statistical model, which is a multi-dimensional generalization of the location-scale model. We compare merits and demerits of estimators based on information geometry and Wasserstein geometry. The shape of a probability distribution and its affine deformation are separated in the Wasserstein geometry, showing its robustness against the waveform perturbation in exchange for the loss in Fisher efficiency. We show that the Wasserstein estimator is the moment estimator in the case of the elliptically symmetric affine deformation model. It coincides with the information-geometrical estimator (maximum-likelihood estimator) when and only when the waveform is Gaussian. The role of the Wasserstein efficiency is 
    
[^13]: 学习浅层网络的一种更快更简单的算法

    A faster and simpler algorithm for learning shallow networks. (arXiv:2307.12496v1 [cs.LG])

    [http://arxiv.org/abs/2307.12496](http://arxiv.org/abs/2307.12496)

    本文提出了一种更简单的算法来学习浅层网络，其运行时间更短且只需要一个阶段即可。

    

    我们重新研究了学习线性组合的ReLU激活函数的问题，给出了一种在多个阶段内运行的算法，其时间复杂度为$(d/\varepsilon)^{\mathrm{quasipoly}(k)}$。本文表明，一个更简单的单阶段版本的该算法就足够了，而且其运行时间只有$(d/\varepsilon)^{O(k^2)}$。

    We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for this problem to run in $\text{poly}(d,1/\varepsilon)$ time when $k = O(1)$, where $\varepsilon$ is the target error. More precisely, their algorithm runs in time $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d/\varepsilon)^{O(k^2)}$.
    
[^14]: 无模型广义基准推理

    Model-free generalized fiducial inference. (arXiv:2307.12472v1 [stat.ML])

    [http://arxiv.org/abs/2307.12472](http://arxiv.org/abs/2307.12472)

    本文提出了一种无模型的统计框架，用于不准确概率预测推理的不确定性量化，并考虑了精确概率近似模型无关的不准确推理框架的特性。

    

    鉴于机器学习中不确定性量化方法的安全可靠性的需求，本文提出并发展了一种无模型的统计框架，用于不准确概率预测推理的不确定性量化。该框架通过提供预测集的形式，实现了对第一类错误的有限样本控制，这与一致性预测集具有相同的属性，但这种新方法还提供了更灵活的不准确概率推理工具。此外，本文提出并考虑了一种精确概率近似模型无关的不准确推理框架的理论和实证特性。通过将信念/可信度度量对近似为在可信区间中的[在某种意义上最优]概率度量，是扩大在统计和机器学习社区推广不准确概率推理方法所需的关键解决方案，目前在统计和

    Motivated by the need for the development of safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework facilitates uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an [optimal in some sense] probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities. It is largely undetermined in the statistical and
    
[^15]: ReLU浅层神经网络的逼近率

    Rates of Approximation by ReLU Shallow Neural Networks. (arXiv:2307.12461v1 [cs.LG])

    [http://arxiv.org/abs/2307.12461](http://arxiv.org/abs/2307.12461)

    本文提供了ReLU浅层神经网络的均匀逼近率，可以以接近于最优逼近率的速度在H\"older空间中逼近函数。

    

    由修正线性单元（ReLU）激活的神经网络在深度学习的最新发展中发挥着核心作用。通过这些网络逼近H\"older空间中的函数的话题对于理解所引发学习算法的效率至关重要。虽然在具有多层隐藏神经元的深层神经网络的设置中该话题已经得到很好的研究，但对于仅有一个隐藏层的浅层网络仍然是未解之谜。在本文中，我们提供了这些网络的均匀逼近率。我们表明，当$r<d/2 +2$时，ReLU浅层神经网络可以以率$O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4}})$均匀逼近H\"older空间$W_\infty^r([-1, 1]^d)$中的函数。当$d$很大时，这样的逼近率非常接近最优逼近率$O(m^{-\frac{r}{d}})$，因为$\frac{d+2}{d+4}$接近于$1$。

    Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\"older space $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4}})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\frac{r}{d}})$ in the sense that $\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.
    
[^16]: 不确定性中测试数据敏感性的信息论分析

    Information-theoretic Analysis of Test Data Sensitivity in Uncertainty. (arXiv:2307.12456v1 [stat.ML])

    [http://arxiv.org/abs/2307.12456](http://arxiv.org/abs/2307.12456)

    本篇论文进行了关于贝叶斯推断中预测不确定性的信息论分析，将其严格分解为本质不确定性和认知不确定性。然而，现有的分析方法无法解释测试数据和训练数据之间的敏感性。本文通过使用新颖的分解方法研究了这种不确定性敏感性。

    

    贝叶斯推断常被用于不确定性量化任务。Xu和Raginsky在2022年的最新分析中，将贝叶斯推断中的预测不确定性严格分解为两个不确定性，称为本质不确定性和认知不确定性，分别表示数据生成过程中的固有随机性和由于数据不足而产生的变异性。他们以信息论的方式分析了这些不确定性，假设模型是良好指定的，并将模型参数视为潜变量。然而，现有的不确定性的信息论分析不能解释被广泛认为的不确定性特性，即测试数据和训练数据之间的敏感性。这意味着当测试数据在某种意义上与训练数据相似时，认知不确定性应该变小。在这项工作中，我们使用我们的新颖分解方法研究了这种不确定性的敏感性。

    Bayesian inference is often utilized for uncertainty quantification tasks. A recent analysis by Xu and Raginsky 2022 rigorously decomposed the predictive uncertainty in Bayesian inference into two uncertainties, called aleatoric and epistemic uncertainties, which represent the inherent randomness in the data-generating process and the variability due to insufficient data, respectively. They analyzed those uncertainties in an information-theoretic way, assuming that the model is well-specified and treating the model's parameters as latent variables. However, the existing information-theoretic analysis of uncertainty cannot explain the widely believed property of uncertainty, known as the sensitivity between the test and training data. It implies that when test data are similar to training data in some sense, the epistemic uncertainty should become small. In this work, we study such uncertainty sensitivity using our novel decomposition method for the predictive uncertainty. Our analysis 
    
[^17]: DiAMoNDBack: 弥散去噪的自回归模型用于非确定性蛋白贝类C{\alpha}追踪的反映射

    DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of C{\alpha} Protein Traces. (arXiv:2307.12451v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.12451](http://arxiv.org/abs/2307.12451)

    DiAMoNDBack是一个弥散去噪的自回归模型，用于将粗粒化蛋白质模型恢复到全原子分辨率，通过利用C{\alpha}坐标的信息和局部邻域中的先前反映射的骨架和侧链原子。

    

    蛋白质的粗粒化分子模型允许访问所有原子模型无法达到的长度和时间尺度，并模拟长时间尺度下发生的聚集和折叠等过程。降低的分辨率实现了计算加速，但原子级表示对于完全理解机制细节是至关重要的。反映射是将粗粒化分子模型恢复到全原子分辨率的过程。在这项工作中，我们报告了DiAMoNDBack（非确定性蛋白贝类C{\alpha}追踪的弥散去噪自回归模型），该模型可以将全原子细节恢复到仅保留C{\alpha}坐标的粗粒化蛋白质表示。自回归生成过程从蛋白质N端到C端逐个残基地进行，条件是C{\alpha}追踪以及先前反映射的骨架和侧链原子在局部邻域内。

    Coarse-grained molecular models of proteins permit access to length and time scales unattainable by all-atom models and the simulation of processes that occur on long-time scales such as aggregation and folding. The reduced resolution realizes computational accelerations but an atomistic representation can be vital for a complete understanding of mechanistic details. Backmapping is the process of restoring all-atom resolution to coarse-grained molecular models. In this work, we report DiAMoNDBack (Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping) as an autoregressive denoising diffusion probability model to restore all-atom details to coarse-grained protein representations retaining only C{\alpha} coordinates. The autoregressive generation process proceeds from the protein N-terminus to C-terminus in a residue-by-residue fashion conditioned on the C{\alpha} trace and previously backmapped backbone and side chain atoms within the local neighborhood. The local a
    
[^18]: 高维线性过程中具有相关创新的集中度

    Concentration for high-dimensional linear processes with dependent innovations. (arXiv:2307.12395v1 [math.ST])

    [http://arxiv.org/abs/2307.12395](http://arxiv.org/abs/2307.12395)

    本论文提出了一种针对高维线性过程的具有相关创新的集中度不等式，并利用该不等式获得了线性过程滞后自协方差矩阵最大分量范数的集中度界限。这些结果在估计高维向量自回归过程、时间序列引导和长期协方差矩阵估计中具有重要应用价值。

    

    我们针对具有子韦布尔尾的混合序列上的线性过程的$l_\infty$范数开发了集中不等式。这些不等式利用了Beveridge-Nelson分解，将问题简化为向量混合序列或其加权和的上确界范数的集中度。这个不等式用于得到线性过程的滞后$h$自协方差矩阵的最大分量范数的集中度界限。这些结果对于使用$l_1$正则化估计的高维向量自回归过程的估计界限、时间序列的高维高斯引导、以及长期协方差矩阵估计非常有用。

    We develop concentration inequalities for the $l_\infty$ norm of a vector linear processes on mixingale sequences with sub-Weibull tails. These inequalities make use of the Beveridge-Nelson decomposition, which reduces the problem to concentration for sup-norm of a vector-mixingale or its weighted sum. This inequality is used to obtain a concentration bound for the maximum entrywise norm of the lag-$h$ autocovariance matrices of linear processes. These results are useful for estimation bounds for high-dimensional vector-autoregressive processes estimated using $l_1$ regularisation, high-dimensional Gaussian bootstrap for time series, and long-run covariance matrix estimation.
    
[^19]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^20]: 面向预训练模型的几何感知自适应技术

    Geometry-Aware Adaptation for Pretrained Models. (arXiv:2307.12226v1 [cs.LG])

    [http://arxiv.org/abs/2307.12226](http://arxiv.org/abs/2307.12226)

    本论文提出了一种简单的方法，利用标签之间的距离关系来调整已训练的模型，以可靠地预测新类别或改善零样本预测的性能，而无需额外的训练。

    

    机器学习模型，包括著名的零样本模型，通常在仅具有较小比例标签空间的数据集上进行训练。这些标签空间通常使用度量来衡量标签之间的距离关系。我们提出了一种简单的方法来利用这些信息，将已训练的模型调整以可靠地预测新类别，或者在零样本预测的情况下改善性能，而无需额外的训练。我们的技术是标准预测规则的替代方案，在其中将argmax替换为Fréchet平均值。我们为这种方法提供了全面的理论分析，研究了（i）学习理论结果，权衡标签空间直径、样本复杂性和模型维度，（ii）表征可能预测任何未观察到的类别的所有情景的特征，（iii）一种最优的主动学习式下一类别选择过程，以获取最佳的训练类别。

    Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes f
    
[^21]: 多分布学习对VC类的样本复杂度

    The Sample Complexity of Multi-Distribution Learning for VC Classes. (arXiv:2307.12135v1 [cs.LG])

    [http://arxiv.org/abs/2307.12135](http://arxiv.org/abs/2307.12135)

    这个论文研究了多分布学习对VC类的样本复杂度，发现现有的上下界存在显著差距，并讨论了一些涉及统计学习中博弈动态的困难。

    

    多分布学习是将PAC学习推广到具有多个数据分布的情境。我们对于PAC可学习类的已知上下界仍然存在显著差距。尽管我们了解在$k$个分布上学习具有VC维度d的类的样本复杂度为$O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$，但最好的下界是$\Omega(\epsilon^{-2}(d + k \ln(k)))$。我们讨论了这个问题的最新进展以及在统计学习中使用博弈动态的一些困难。

    Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
    
[^22]: 不变风险最小化的区外优化性

    Out-of-Distribution Optimality of Invariant Risk Minimization. (arXiv:2307.11972v1 [stat.ML])

    [http://arxiv.org/abs/2307.11972](http://arxiv.org/abs/2307.11972)

    本文旨在提供IRM的理论验证，严格证明了解决方案可以最小化区外风险。

    

    深度神经网络经常继承训练数据中嵌入的虚假相关性，因此可能无法泛化到具有与提供训练数据的领域不同的未知域。M. Arjovsky等人（2019年）引入了区外（o.o.d.）风险的概念，即所有域中的最大风险，并将由虚假相关性引起的问题规定为最小化区外风险的问题。不变风险最小化（IRM）被认为是最小化区外风险的一种有前途的方法：IRM通过解决一个双层优化问题来估计最小化的区外风险。尽管IRM以实证成功吸引了相当多的关注，但它缺乏一些理论保证。特别是，还没有确立双层优化问题给出最小化区外风险的坚实理论保证。本文旨在提供IRM的理论验证，严格证明了解决方案可以通过在大仿真跟踪数据库中进行实时仿真，其包括对周围环境的直接感知，对潜在路线规划的策略认识，同时考虑到多车辆交互，以实现该问题的全局优化目标。

    Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to
    
[^23]: 通过结构化缺失数据协同学习线性模型

    Collaboratively Learning Linear Models with Structured Missing Data. (arXiv:2307.11947v1 [stat.ML])

    [http://arxiv.org/abs/2307.11947](http://arxiv.org/abs/2307.11947)

    本论文研究了协同学习线性模型的问题，提出了一种分布式、半监督的算法Collab，该算法在无法访问标记数据的情况下具有通信效率和实用性，同时在渐近局部极小极值方面也表现出优异的性能。

    

    我们研究了$m$个代理协同学习最小二乘估计的问题。每个代理观察到不同的特征子集，例如从不同分辨率的传感器收集的数据。我们的目标是确定如何协调代理以产生最佳的估计器。我们提出了一种分布式、半监督的算法Collab，包括三个步骤：本地训练、聚合和分布。我们的方法不需要通信标记数据，使其在无法访问标记数据的情况下具有通信效率和实用性。尽管存在这个障碍，我们的方法几乎是渐近局部极小极值$\unicode{x2013}$即使在允许通信标记数据的估计器中，如插补方法。我们在真实数据和合成数据上测试了我们的方法。

    We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
    
[^24]: 捕捉社交媒体中客户见解的主题方法

    A Topical Approach to Capturing Customer Insight In Social Media. (arXiv:2307.11775v1 [cs.CL])

    [http://arxiv.org/abs/2307.11775](http://arxiv.org/abs/2307.11775)

    本研究通过嵌入狄利克雷过程，嵌入层次狄利克雷过程和面向时间的动态嵌入三种方法，解决了在嘈杂的大数据环境中完全无监督的主题提取挑战。

    

    社交媒体时代为企业带来了新的机遇。这种繁荣的信息财富超出了传统营销研究的渠道和框架，包括营销组合建模(MMM)。特别是，文本数据提出了许多数据分析从业人员必须应对的挑战。社交媒体构成了大规模、异构和嘈杂的文档来源。工业数据采集过程包括一定量的ETL。然而，数据中噪声的变异性和不同来源引入的异构性给予了临时工具的需求。换句话说，在完全无监督、嘈杂的环境中提取客户见解是一项艰巨的任务。本研究解决了在嘈杂的大数据环境中完全无监督的主题提取挑战。我们在变分自动编码器框架上提出了三种方法：嵌入狄利克雷过程、嵌入层次狄利克雷过程和面向时间的动态嵌入

    The age of social media has opened new opportunities for businesses. This flourishing wealth of information is outside traditional channels and frameworks of classical marketing research, including that of Marketing Mix Modeling (MMM). Textual data, in particular, poses many challenges that data analysis practitioners must tackle. Social media constitute massive, heterogeneous, and noisy document sources. Industrial data acquisition processes include some amount of ETL. However, the variability of noise in the data and the heterogeneity induced by different sources create the need for ad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised, noisy contexts is an arduous task. This research addresses the challenge of fully unsupervised topic extraction in noisy, Big Data contexts. We present three approaches we built on the Variational Autoencoder framework: the Embedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and the time-aware Dynamic Embe
    
[^25]: 通过安全多方计算实现垂直隐私保护的符号回归

    Towards Vertical Privacy-Preserving Symbolic Regression via Secure Multiparty Computation. (arXiv:2307.11756v1 [cs.CR])

    [http://arxiv.org/abs/2307.11756](http://arxiv.org/abs/2307.11756)

    通过安全多方计算实现垂直隐私保护的符号回归。

    

    符号回归是一种强大的数据驱动技术，用于搜索能够解释输入变量与目标变量之间关系的数学表达式。由于其高效性和灵活性，遗传编程被视为符号回归的标准搜索技术。然而，传统的遗传编程算法需要将所有数据存储在一个中央位置，这在日益关注数据隐私和安全的情况下并不总是可行的。虽然隐私保护研究近年来取得了进展，可能为解决这个问题提供了方案，但对符号回归的应用仍然较少。此外，现有的工作只关注水平分区的设置，而垂直分区的设置，另一种常见场景，尚未被研究。在这里，我们提出一种采用安全多方计算技术的方法，使各方能够共同构建符号回归模型，同时保护数据隐私。

    Symbolic Regression is a powerful data-driven technique that searches for mathematical expressions that explain the relationship between input variables and a target of interest. Due to its efficiency and flexibility, Genetic Programming can be seen as the standard search technique for Symbolic Regression. However, the conventional Genetic Programming algorithm requires storing all data in a central location, which is not always feasible due to growing concerns about data privacy and security. While privacy-preserving research has advanced recently and might offer a solution to this problem, their application to Symbolic Regression remains largely unexplored. Furthermore, the existing work only focuses on the horizontally partitioned setting, whereas the vertically partitioned setting, another popular scenario, has yet to be investigated. Herein, we propose an approach that employs a privacy-preserving technique called Secure Multiparty Computation to enable parties to jointly build Sy
    
[^26]: 尖锐性最小化算法不仅仅是为了更好地泛化而最小化尖锐性

    Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization. (arXiv:2307.11007v1 [cs.LG])

    [http://arxiv.org/abs/2307.11007](http://arxiv.org/abs/2307.11007)

    本文研究发现，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。我们的结果表明，尖锐性与泛化之间的关系取决于数据分布和模型架构。

    

    尽管进行了广泛的研究，但过参数化的神经网络能够泛化的基本原因仍然不明确。现有的理论表明，常见的随机优化器更倾向于训练损失更平坦的最小化器，因此自然而然的解释是平坦性意味着泛化。本文对这一解释进行了批判性的研究。通过理论和实证调查，我们发现对于两层ReLU网络存在以下三种情况：(1) 平坦性确实暗示泛化；(2) 存在最平坦的非泛化模型，尖锐性最小化算法无法泛化；(3) 更加令人惊讶的是，存在非泛化最平坦的模型，但尖锐性最小化算法仍然能够泛化。我们的研究结果表明，尖锐性与泛化之间的关系在一定程度上取决于数据分布和模型架构，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。

    Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve bet
    
[^27]: 柔性时间事件建模：通过排名回归优化神经网络

    Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])

    [http://arxiv.org/abs/2307.08044](http://arxiv.org/abs/2307.08044)

    本研究提出了一种深度AFT排名回归模型，用于灵活地进行时间事件建模，从而改善预测性能并减轻严格的假设。

    

    时间事件分析，也被称为生存分析，旨在根据一组特征预测事件发生的时间。这个领域面临的一个主要挑战是处理被截尾的数据，这可能使学习算法更加复杂。传统方法如Cox比例风险模型和加速失效时间（AFT）模型在这个领域很受欢迎，但它们经常需要一些假设，如比例风险和线性。特别是，AFT模型通常需要预先指定的参数分布假设。为了提高预测性能和减轻严格的假设，近年来出现了许多基于深度学习的危险模型方法。然而，神经网络文献中对于AFT的表示学习尚未广泛探索，尽管相对于以危险为重点的方法而言，它更加简单和可解释。在这项工作中，我们引入了深度AFT排名回归模型来进行时间事件预测。

    Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
    
[^28]: 张量分解与控制理论的结合：学习线性动力系统的混合模型

    Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems. (arXiv:2307.06538v1 [cs.LG])

    [http://arxiv.org/abs/2307.06538](http://arxiv.org/abs/2307.06538)

    本论文以张量分解方法为基础，提出了学习线性动力系统混合模型的新方法。算法成功地应用于没有组件分离条件的情况，并可以与贝叶斯最优聚类竞争。此外，算法可以在部分观测设置下工作。

    

    最近，Chen和Poor开始研究学习线性动力系统的混合模型。虽然线性动力系统已经在建模时间序列数据方面有广泛的应用，但使用混合模型可以带来更好的拟合或者对数据中表示的基础子群体有更丰富的理解。在这项工作中，我们提出了一种基于张量分解的学习线性动力系统混合模型的新方法。因此，我们的算法在组件无强分离条件的情况下成功，并可以用于与轨迹的贝叶斯最优聚类竞争。此外，我们的算法适用于具有挑战性的部分观测设置。我们的起点是简单但强大的观察，即经典的何-卡尔曼算法是学习潜变量模型的现代张量分解方法的近亲。这为我们提供了一个扩展到更复杂的生成模型的操作指南。

    Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.
    
[^29]: PFN是适用于实际贝叶斯优化的灵活模型。

    PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])

    [http://arxiv.org/abs/2305.17535](http://arxiv.org/abs/2305.17535)

    本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。

    

    本文使用先验数据拟合网络(PFNs)作为贝叶斯优化(BO)的灵活代理。PFN是一种神经过程，被训练用于近似后验预测分布(PPD)，适用于任何可有效采样的先验分布。我们描述了如何利用这种灵活性来进行BO的代理建模。我们使用PFN来模拟一个朴素高斯过程(GP)，一个先进的GP和一个贝叶斯神经网络(BNN)。此外，我们展示了如何将进一步的信息纳入先验，例如允许有关最优位置的提示(用户先验)，忽略不相关的维度，并通过学习获取函数来执行非远视BO。这些扩展的灵活性为使用PFN进行BO开辟了广阔的可能性。我们在人工高斯过程样本和三个不同的超参数优化测试平台上展示了PFN对BO的有用性：HPO-B、Bayesmark和PD1。

    In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
    
[^30]: 一种转移原理：从欧几里得通用逼近器到度量空间之间的通用逼近器

    A Transfer Principle: Universal Approximators Between Metric Spaces From Euclidean Universal Approximators. (arXiv:2304.12231v1 [cs.LG])

    [http://arxiv.org/abs/2304.12231](http://arxiv.org/abs/2304.12231)

    本论文提出使用欧几里得空间通用逼近器为构建块，构建了在任意波兰度量空间 $\mathcal{X}$ 和 $\mathcal{Y}$ 之间的通用逼近器，并通过随机化输出离散概率测度来克服某些限制。在适当的结构下提供了概率和定量保证。

    

    我们使用欧几里得空间通用逼近器作为构建块，构建了连续映射的度量空间之间的通用逼近器。早期结果假定输出空间 $\mathcal{Y}$ 是拓扑向量空间。我们通过“随机化”来克服这种限制：我们的逼近器输出 $\mathcal{Y}$ 上的离散概率测度。当 $\mathcal{X}$ 和 $\mathcal{Y}$ 没有附加结构时，我们证明了非常通用的定性保证；当它们具有适当的组合结构时，我们证明了 H\"older 类映射的定量保证，包括有限图之间的映射，在某些 Carnot 群之间的粗微分方程的解算子以及反问题中出现的 Banach 空间之间的连续非线性算子。特别地，我们展示了所需的 Dirac 测度数量由 $\mathcal{X}$ 和 $\mathcal{Y}$ 的组合结构决定。

    We build universal approximators of continuous maps between arbitrary Polish metric spaces $\mathcal{X}$ and $\mathcal{Y}$ using universal approximators between Euclidean spaces as building blocks. Earlier results assume that the output space $\mathcal{Y}$ is a topological vector space. We overcome this limitation by "randomization": our approximators output discrete probability measures over $\mathcal{Y}$. When $\mathcal{X}$ and $\mathcal{Y}$ are Polish without additional structure, we prove very general qualitative guarantees; when they have suitable combinatorial structure, we prove quantitative guarantees for H\"older-like maps, including maps between finite graphs, solution operators to rough differential equations between certain Carnot groups, and continuous non-linear operators between Banach spaces arising in inverse problems. In particular, we show that the required number of Dirac measures is determined by the combinatorial structure of $\mathcal{X}$ and $\mathcal{Y}$. For b
    
[^31]: 评估-优化方法与集成评估优化法：基于随机优势的观点

    Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])

    [http://arxiv.org/abs/2304.06833](http://arxiv.org/abs/2304.06833)

    本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。

    

    在数据驱动的随机优化中，除了需要优化任务，还需要从数据中估计潜在分布的模型参数。最近的文献表明，通过选择导致最佳经验目标性能的模型参数，可以集成估计和优化过程。当模型被错误地指定时，这种集成方法可以很容易地显示出优于简单的“先估计再优化”的方法。本文认为，在模型类足够丰富以涵盖真实情况的情况下，对于非线性问题，两种方法之间的性能排序在强烈的意义下被颠倒。在受限条件和当上下文特征可用时，类似的结果也成立。

    In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
    
[^32]: Transformers如何学习主题结构：走向对其机制的理解

    How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding. (arXiv:2303.04245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04245](http://arxiv.org/abs/2303.04245)

    本文提供了对Transformer学习语义结构的机制性理解，通过数学分析和实验证明了嵌入层和自注意力层如何对词汇的共现结构进行编码。

    

    尽管Transformer在许多领域都取得了成功，但对其学习机制的准确理解仍然存在较大的缺乏。虽然它们在包括各种结构化和推理任务在内的基准测试中表现出了强大的能力，但对数学理解的研究仍然滞后。最近的研究开始从表示方面研究了这个问题：即基于注意力的网络的大小/深度/复杂性用于执行某些任务。然而，并不能保证学习动态会收敛到所提出的结构上。在本文中，我们提供了细致入微的机制理解，阐明了Transformer如何学习“语义结构”，即捕捉词汇的共现结构。准确地说，我们通过数学分析和对维基百科数据以及由潜在狄利克雷分配（LDA）建模的合成数据进行的实验，展示了嵌入层和自注意力层如何对主题进行编码。

    While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks -- but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn "semantic structure", understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topi
    
[^33]: 从图生成到图分类

    From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07989](http://arxiv.org/abs/2302.07989)

    本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。

    

    本文描述了一种利用图生成模型 (GGM) 进行图分类的新方法。假设一个定义了图及其类标签的联合概率分布的 GGM，我推导了计算给定图的类标签概率的分类公式。可以使用新的条件 ELBO 来训练生成图自编码器模型进行区分。虽然利用生成模型进行分类在非关系 i.i.d. 数据中已经得到了很好的研究，但据我们所知，这是一种图分类的新方法。

    This note describes a new approach to classifying graphs that leverages graph generative models (GGM). Assuming a GGM that defines a joint probability distribution over graphs and their class labels, I derive classification formulas for the probability of a class label given a graph. A new conditional ELBO can be used to train a generative graph auto-encoder model for discrimination. While leveraging generative models for classification has been well explored for non-relational i.i.d. data, to our knowledge it is a novel approach to graph classification.
    
[^34]: DNArch: 通过反向传播学习卷积神经网络的可学习架构

    DNArch: Learning Convolutional Neural Architectures by Backpropagation. (arXiv:2302.05400v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05400](http://arxiv.org/abs/2302.05400)

    DNArch是一种通过反向传播同时学习卷积神经网络的权重和架构的方法，它不仅可以学习每一层的卷积核大小和通道数，还可以学习网络的深度和下采样层的位置和值。与现有方法不同的是，DNArch不限于预定义的神经组件，能够发现各种核大小、宽度、深度和下采样组合中的整个CNN架构。在实验中，DNArch在多个分类和密集预测任务上找到了高性能的CNN架构。

    

    我们提出了Differentiable Neural Architectures (DNArch)，一种通过反向传播同时学习卷积神经网络(CNNs)的权重和架构的方法。具体而言，DNArch允许学习(i)每一层的卷积核大小，(ii)每一层的通道数，(iii)下采样层的位置和值，以及(iv)网络的深度。为此，DNArch将神经架构视为连续的多维实体，并使用可学习的可微掩码来控制其大小。与现有方法不同，DNArch不限于预定义的可能神经组件集，而是能够发现在所有可行的核大小、宽度、深度和下采样组合中的整个CNN架构。实验证明，DNArch能够为顺序和图像数据的多个分类和密集预测任务找到有效的CNN架构。当与控制架构大小的损失项相结合时，DNArch还能在运行时间和模型性能之间实现有效的权衡。

    We present Differentiable Neural Architectures (DNArch), a method that jointly learns the weights and the architecture of Convolutional Neural Networks (CNNs) by backpropagation. In particular, DNArch allows learning (i) the size of convolutional kernels at each layer, (ii) the number of channels at each layer, (iii) the position and values of downsampling layers, and (iv) the depth of the network. To this end, DNArch views neural architectures as continuous multidimensional entities, and uses learnable differentiable masks along each dimension to control their size. Unlike existing methods, DNArch is not limited to a predefined set of possible neural components, but instead it is able to discover entire CNN architectures across all feasible combinations of kernel sizes, widths, depths and downsampling. Empirically, DNArch finds performant CNN architectures for several classification and dense prediction tasks on sequential and image data. When combined with a loss term that controls t
    
[^35]: Lipschitz正则化梯度流和高维稀缺数据的生成粒子算法

    Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data. (arXiv:2210.17230v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.17230](http://arxiv.org/abs/2210.17230)

    构建了一种新的生成算法类，能够有效地学习稀缺高维数据的任意目标分布并生成新样本，具有很好的数据整合能力。

    

    我们构建了一种新的生成算法类，能够有效地从可能稀缺、高维的数据中学习任意目标分布，并生成新的样本。这些生成算法是基于粒子的，并且是通过Lipschitz正则化Kullback-Leibler或其他f-散度的梯度流来构造的，其中来自源分布的数据可以稳定地作为粒子传输到目标分布的附近。作为数据整合的一个突出结果，我们证明了所提出的算法可以正确传输维数超过54K的基因表达数据点，而样本量通常只有几百个。

    We build a new class of generative algorithms capable of efficiently learning an arbitrary target distribution from possibly scarce, high-dimensional data and subsequently generate new samples. These generative algorithms are particle-based and are constructed as gradient flows of Lipschitz-regularized Kullback-Leibler or other $f$-divergences, where data from a source distribution can be stably transported as particles, towards the vicinity of the target distribution. As a highlighted result in data integration, we demonstrate that the proposed algorithms correctly transport gene expression data points with dimension exceeding 54K, while the sample size is typically only in the hundreds.
    
[^36]: 最小化损失的多层训练方法下的最优核算子学习

    Minimax Optimal Kernel Operator Learning via Multilevel Training. (arXiv:2209.14430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14430](http://arxiv.org/abs/2209.14430)

    本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限，在多层级训练方法下，通过学习偏差以下的谱分量和忽略方差以上的分量，可以达到最优的学习速率。

    

    在无穷维函数空间中学习映射已经在机器学习的许多领域中取得了经验上的成功，包括生成模型、函数数据分析、因果推断和多智能体强化学习。本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限。我们建立了使用Sobolev Hilbert-Schmidt范数的信息理论下界，并展示了一个规则化方法，通过学习偏差轮廓以下的谱分量并忽略方差轮廓以上的分量，可以达到最优的学习速率。同时，偏差和方差轮廓之间的谱分量给我们在设计计算上可行的机器学习算法时提供了灵活性。基于这一观察，我们开发了一个多层级的核算子学习算法，当学习线性算子时它是最优的。

    Learning mappings between infinite-dimensional function spaces has achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones that are above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators betwee
    
[^37]: Bayesian神经网络中的近似阻塞Gibbs采样

    Approximate blocked Gibbs sampling for Bayesian neural networks. (arXiv:2208.11389v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.11389](http://arxiv.org/abs/2208.11389)

    本文提出了一种近似阻塞Gibbs采样方法，可以更可行地进行小批量MCMC采样，提高了前馈神经网络的预测准确性和预测不确定性的量化能力。

    

    本文提出了一种更可行的前馈神经网络的小批量MCMC采样方法。为此，文中提出了通过阻塞Gibbs采样方案对参数进行子组抽样的方法。通过对参数空间进行划分，无论层宽如何，都能进行采样。同时，通过在深层减小建议方差，可以减轻递增深度时消失的接受率问题。在分类任务中，增加非收敛链的长度可以提高预测准确性，因此避免消失的接受率和允许更长的链运行具有实际好处。此外，非收敛链的实现有助于量化预测不确定性。一个未解决的问题是在存在增广数据的情况下如何进行前馈神经网络的小批量MCMC采样。

    In this work, minibatch MCMC sampling for feedforward neural networks is made more feasible. To this end, it is proposed to sample subgroups of parameters via a blocked Gibbs sampling scheme. By partitioning the parameter space, sampling is possible irrespective of layer width. It is also possible to alleviate vanishing acceptance rates for increasing depth by reducing the proposal variance in deeper layers. Increasing the length of a non-convergent chain increases the predictive accuracy in classification tasks, so avoiding vanishing acceptance rates and consequently enabling longer chain runs have practical benefits. Moreover, non-convergent chain realizations aid in the quantification of predictive uncertainty. An open problem is how to perform minibatch MCMC sampling for feedforward neural networks in the presence of augmented data.
    
[^38]: 基于边缘分布的非参数消费者选择建模方法

    A Nonparametric Approach with Marginals for Modeling Consumer Choice. (arXiv:2208.06115v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.06115](http://arxiv.org/abs/2208.06115)

    本文提出了一种基于边缘分布的简单而有效的非参数消费者选择建模方法，在任何选择集合中会把选择概率的集合一致地描述出来。

    

    鉴于消费者在不同选择集合中作出选择的数据，开发描述和预测消费者选择行为的简洁模型是一个主要挑战。其中一种选择模型是边缘分布模型，该模型仅需要规定随机效用的边缘分布即可解释选项数据。在本文中，我们开发了一种精确的选择概率集合的特征化方法，该集合可以在任何集合中一致地通过边缘分布模型来描述。允许根据其效用的边缘分布将选择集合进行分组，我们展示了(a)验证这个模型与选择概率数据的一致性在多项式时间内是可能的，(b)最接近拟合的方法可以简化为解决混合整数凸规划问题。我们的结果表明，与多项式Logit模型和m相比，边缘分布模型提供了更好的表现能力。

    Given data on choices made by consumers for different assortments, a key challenge is to develop parsimonious models that describe and predict consumer choice behavior. One such choice model is the marginal distribution model which requires only the specification of the marginal distributions of the random utilities of the alternatives to explain choice data. In this paper, we develop an exact characterisation of the set of choice probabilities which are representable by the marginal distribution model consistently across any collection of assortments. Allowing for the possibility of alternatives to be grouped based on the marginal distribution of their utilities, we show (a) verifying consistency of choice probability data with this model is possible in polynomial time and (b) finding the closest fit reduces to solving a mixed integer convex program. Our results show that the marginal distribution model provides much better representational power as compared to multinomial logit and m
    
[^39]: TF-GNN: TensorFlow中的图神经网络

    TF-GNN: Graph Neural Networks in TensorFlow. (arXiv:2207.03522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03522](http://arxiv.org/abs/2207.03522)

    TF-GNN是一个可扩展的 TensorFlow 图神经网络库，用于支持丰富的异构图数据。它提供了低代码解决方案，并广泛应用于谷歌的生产模型中。这个库最近作为开源项目发布，为图学习提供了强大的工具。

    

    TensorFlow-GNN (TF-GNN) 是一个在 TensorFlow 中可扩展的图神经网络库。它从底层设计来支持当今信息生态系统中出现的各种丰富的异构图数据。除了为机器学习研究人员和高级开发人员提供支持，TF-GNN还提供低代码解决方案，以赋能更广泛的开发者社区进行图学习。谷歌的许多生产模型都在使用 TF-GNN，并且它最近作为一个开源项目发布。在本文中，我们描述了 TF-GNN 的数据模型、其 Keras 消息传递 API 以及相关的能力，如图采样和分布式训练。

    TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.
    
[^40]: 当个性化造成伤害时：重新考虑在预测中使用群体属性

    When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction. (arXiv:2206.02058v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.02058](http://arxiv.org/abs/2206.02058)

    本研究发现，使用群体属性个性化机器学习模型可能降低群体水平的性能。为了确保在预测任务中公平使用群体属性，我们提出了形式化条件，并提供了相应的解决方法。我们的实证研究表明，在临床预测任务中普遍存在公平使用违规的情况，但我们也找到了简单干预手段来减轻其伤害。

    

    机器学习模型通常会使用受保护、敏感、自报告或者昂贵的分类属性进行个性化。本研究指出，使用群体属性进行个性化会降低群体水平的性能。我们提出了一种形式化条件，以确保在预测任务中“公平使用”群体属性，方法是通过训练一个额外的模型，即保证每个提供个人数据的群体会获得相对应的性能提升。我们提出了足够的条件，以确保在经验风险最小化中的公平使用，并描述了导致公平使用违规的故障模式，这是由于模型开发和部署中的标准做法所导致的。我们对临床预测任务进行了全面的实证研究。我们的结果表明，在实践中普遍存在公平使用违规，并说明了减轻其伤害的简单干预手段。

    Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the "fair use" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.
    
[^41]: 图神经网络的表达能力：通过代数分析改进表达性能

    Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis. (arXiv:2205.09801v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09801](http://arxiv.org/abs/2205.09801)

    本文通过代数分析改进了图神经网络（GNN）的表达能力，证明了GNN能够比Weisfeiler-Lehman（WL）算法更好地产生区分性表示，特别是在具有不同特征值的图上。此外，我们还发现简单的卷积结构与无信息输入产生的等变特征比WL表示更具表达能力。

    

    尽管图神经网络（GNN）取得了显著的成功，但普遍认为它们的表达能力有限，并且它们最多与Weisfeiler-Lehman（WL）算法一样具有表达能力。本文与此相反，我们证明了标准的GNN（匿名输入）产生的表示比WL算法更具有区分性。我们使用线性代数工具对GNN的表示能力进行了全新的分析，并将其与图操作符的特征值分解相关联。我们证明了GNN能够从无信息输入产生独特的输出，至少对于所有具有不同特征值的图。我们还展示了简单的卷积结构与无信息输入产生的等变特征，它们计算图中的闭合路径并且明显比WL表示具有更高的表达能力。在图同构和图分类数据集上进行了彻底的实验分析，验证了我们的理论。

    Despite the remarkable success of Graph Neural Networks (GNNs), the common belief is that their representation power is limited and that they are at most as expressive as the Weisfeiler-Lehman (WL) algorithm. In this paper, we argue the opposite and show that standard GNNs, with anonymous inputs, produce more discriminative representations than the WL algorithm. Our novel analysis employs linear algebraic tools and characterizes the representation power of GNNs with respect to the eigenvalue decomposition of the graph operators. We prove that GNNs are able to generate distinctive outputs from white uninformative inputs, for, at least, all graphs that have different eigenvalues. We also show that simple convolutional architectures with white inputs, produce equivariant features that count the closed paths in the graph and are provably more expressive than the WL representations. Thorough experimental analysis on graph isomorphism and graph classification datasets corroborates our theore
    
[^42]: 噪声设置中的相似性泛化：DIBS现象

    Generalizing similarity in noisy setups: the DIBS phenomenon. (arXiv:2201.12803v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12803](http://arxiv.org/abs/2201.12803)

    本研究揭示了数据密度、噪声和相似性学习之间的相互作用，证明了数据对的密度对于泛化至关重要，并发现了一种在密集数据集上比对称标签噪声更差的泛化性能的现象，称为密度诱导的相似性破坏（DIBS）。

    

    本文揭示了数据密度、噪声和相似性学习的普适性之间的相互作用。我们考虑了暹罗神经网络（SNNs），这是对比学习的基本形式，并探索了两种可能影响SNNs的噪声类型，即对比标签噪声（PLN）和单标签噪声（SLN）。我们的研究发现，不论训练设置如何，SNNs都表现出双降行为，并且噪声进一步加剧了这种行为。我们证明数据对的密度对于泛化至关重要。当SNNs在稀疏数据集上训练时，具有相同数量的PLN或SLN，它们的泛化性能是可比较的。然而，当使用密集数据集时，在过参数化区域中，PLN案例的泛化性能较差，相对于SLN案例，这导致了一种我们称为密度诱导的相似性破坏（DIBS）的现象。在这个情况下，PLN相似性违规变得宏观化，使得数据集被损坏到无法实现完全插值的程度。

    This work uncovers an interplay among data density, noise, and the generalization ability in similarity learning. We consider Siamese Neural Networks (SNNs), which are the basic form of contrastive learning, and explore two types of noise that can impact SNNs, Pair Label Noise (PLN) and Single Label Noise (SLN). Our investigation reveals that SNNs exhibit double descent behaviour regardless of the training setup and that it is further exacerbated by noise. We demonstrate that the density of data pairs is crucial for generalization. When SNNs are trained on sparse datasets with the same amount of PLN or SLN, they exhibit comparable generalization properties. However, when using dense datasets, PLN cases generalize worse than SLN ones in the overparametrized region, leading to a phenomenon we call Density-Induced Break of Similarity (DIBS). In this regime, PLN similarity violation becomes macroscopical, corrupting the dataset to the point where complete interpolation cannot be achieved, 
    
[^43]: 学习树状三维物体的几何和拓扑的生成模型

    Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08693](http://arxiv.org/abs/2110.08693)

    本文提出了一种扩展平方根速度函数的新表示方法和度量方法，用于分析和比较树状三维物体，从而提高物体形状差异计算的精度和效率。

    

    如何分析展现出复杂几何和拓扑变化的详细三维生物物体，例如神经元和植物树？本文提出了一个新的数学框架，用于表示、比较和计算这些树状三维对象的形状差异，并定义了一种新的度量方法来量化将一个树状物体变形为另一个物体所需的弯曲、拉伸和分支滑动。

    How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
    
[^44]: 从观察数据中学习最优的处方树

    Learning Optimal Prescriptive Trees from Observational Data. (arXiv:2108.13628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.13628](http://arxiv.org/abs/2108.13628)

    该论文介绍了一种从观察数据中学习最优处方树的方法，可以在不需要数据随机化和对树有严格假设的情况下，通过混合整数优化技术进行学习，并具有建模领域特定问题的能力。

    

    我们考虑从观察数据中学习一个适度深度的最优处方树（即，以二叉树形式表示的可解释的治疗分配策略）的问题。这个问题在许多社会重要领域（如公共卫生和个性化医学）中是存在的，这些领域中通过被动收集数据来寻找基于数据的可解释和数据驱动干预，而不是通过随机试验。我们提出了一种使用混合整数优化（MIO）技术来学习最优处方树的方法。我们证明，在温和条件下，我们的方法是渐近精确的，即随着历史数据样本的数量趋向于无穷大，它收敛到一个最优的样本外治疗分配策略。与现有文献相反，我们的方法：1）不需要数据随机化，2）不对学习到的树施加严格的假设，3）具有建模领域特定问题的能力。

    We consider the problem of learning an optimal prescriptive tree (i.e., an interpretable treatment assignment policy in the form of a binary tree) of moderate depth, from observational data. This problem arises in numerous socially important domains such as public health and personalized medicine, where interpretable and data-driven interventions are sought based on data gathered in deployment -- through passive collection of data -- rather than from randomized trials. We propose a method for learning optimal prescriptive trees using mixed-integer optimization (MIO) technology. We show that under mild conditions our method is asymptotically exact in the sense that it converges to an optimal out-of-sample treatment assignment policy as the number of historical data samples tends to infinity. Contrary to existing literature, our approach: 1) does not require data to be randomized, 2) does not impose stringent assumptions on the learned trees, and 3) has the ability to model domain specif
    
[^45]: 学习具有潜在混淆因素的高斯图模型

    Learning Gaussian Graphical Models with Latent Confounders. (arXiv:2105.06600v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2105.06600](http://arxiv.org/abs/2105.06600)

    本文比较了两种处理具有潜在混淆因素的图模型的方法，并提出了一种结合了这两种方法优势的新方法。作者通过理论和实证研究证明了这种方法的可行性和有效性。

    

    高斯图模型（GGM）被广泛应用于估计从生物学到金融等多个领域中的网络结构。在实践中，数据往往被潜在的混淆因素损坏，从而影响对真实图结构的推断。本文比较并对比了两种处理具有潜在混淆因素的图模型的策略：具有潜变量的高斯图模型（LVGGM）和基于主成分分析的混淆去除方法（PCA+GGM）。尽管这两种方法有着类似的目标，但它们是基于不同的混淆假设进行推导的。本文探讨了这两种方法之间的关系，并提出了一种结合了两种方法优势的新方法。我们证明了基于主成分分析的方法的一致性和收敛速度，并利用这些结果提供了何时使用每种方法的指导。通过模拟和两个真实世界应用，我们验证了我们方法的有效性。

    Gaussian Graphical models (GGM) are widely used to estimate the network structures in many applications ranging from biology to finance. In practice, data is often corrupted by latent confounders which biases inference of the underlying true graphical structure. In this paper, we compare and contrast two strategies for inference in graphical models with latent confounders: Gaussian graphical models with latent variables (LVGGM) and PCA-based removal of confounding (PCA+GGM). While these two approaches have similar goals, they are motivated by different assumptions about confounding. In this paper, we explore the connection between these two approaches and propose a new method, which combines the strengths of these two approaches. We prove the consistency and convergence rate for the PCA-based method and use these results to provide guidance about when to use each method. We demonstrate the effectiveness of our methodology using both simulations and in two real-world applications.
    

