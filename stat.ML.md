# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Randomized Kaczmarz with geometrically smoothed momentum.](http://arxiv.org/abs/2401.09415) | 本文研究了向随机Kaczmarz算法中添加几何平滑动量的效果，并证明了关于最小二乘损失矩阵奇异向量方向上的期望误差。数值示例证明了结果的实用性，并提出了几个问题。 |
| [^2] | [Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter Paradigm for Performance Estimation in Online and Static Settings.](http://arxiv.org/abs/2401.09376) | 该论文通过适应Hui-Walter范式，将传统应用于流行病学和医学的方法引入机器学习领域，解决了训练和评估时无法获得标签数据的问题。通过将数据划分为潜在类别，并在多个测试中独立训练模型，能够在没有真实值的情况下估计关键性能指标，并在处理在线数据时提供了新的可能性。 |
| [^3] | [High Confidence Level Inference is Almost Free using Parallel Stochastic Optimization.](http://arxiv.org/abs/2401.09346) | 本文提出了一种使用并行随机优化实现高置信水平推断的方法，通过少量独立多次运行获取分布信息构建置信区间，几乎不需要额外计算和内存，具有高效计算和快速收敛的特点。 |
| [^4] | [Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications.](http://arxiv.org/abs/2401.09339) | 本文通过对两时间尺度随机逼近（TTSA）的广义分析，利用中心极限定理（CLT）揭示了TTSA受马尔可夫噪声影响的耦合动力学，从而拓展了传统SGD的高效采样策略在分布式学习中的应用范围，同时研究了具有非线性函数逼近的GTD算法的统计特性。 |
| [^5] | [Mitigating distribution shift in machine learning-augmented hybrid simulation.](http://arxiv.org/abs/2401.09259) | 本文研究了机器学习增强的混合模拟中的分布偏移问题，并提出了基于切线空间正则化估计器的方法来控制分布偏移，从而提高模拟结果的精确性。 |
| [^6] | [An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification.](http://arxiv.org/abs/2401.09191) | 本文提出了一个用于计算多类分类中对抗训练下界的最优输运方法，并利用该方法提出了计算最优对抗风险下界和确定最优分类器的算法。通过截断类之间的高阶相互作用，避免了组合运行时间的问题。 |
| [^7] | [A Two-Scale Complexity Measure for Deep Learning Models.](http://arxiv.org/abs/2401.09184) | 这篇论文介绍了一种用于统计模型的新容量测量2sED，可以可靠地限制泛化误差，并且与训练误差具有很好的相关性。此外，对于深度学习模型，我们展示了如何通过逐层迭代的方法有效地近似2sED，从而处理大量参数的情况。 |
| [^8] | [Monitoring Machine Learning Forecasts for Platform Data Streams.](http://arxiv.org/abs/2401.09144) | 这篇论文提出了一种简单的数据驱动的监控流程，用于确定机器学习算法何时需要重新训练，以确保准确稳定的预测结果。 |
| [^9] | [Understanding Heterophily for Graph Neural Networks.](http://arxiv.org/abs/2401.09125) | 本文通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对图神经网络（GNNs）影响的理论理解。研究发现，异质性对分类的影响需要与平均节点度一起评估，并且拓扑噪声对分类有负面影响。 |
| [^10] | [Fixed-Budget Differentially Private Best Arm Identification.](http://arxiv.org/abs/2401.09073) | 本论文研究了差分隐私约束下固定预算条件下的最佳臂识别问题，提出了满足差分隐私约束的策略DP-BAI，并得到了错误概率的上界和最小-最大下界的指数衰减关系。 |
| [^11] | [Fast parallel sampling under isoperimetry.](http://arxiv.org/abs/2401.09016) | 通过并行化Langevin算法和欠阻尼的Langevin算法，我们展示了一个在等周密度下并行采样的方法。我们的算法具有TV距离保证，并在总梯度评估次数上具有线性或者平方根级别的复杂度。对于主要应用，在保证TV距离的前提下，我们得到了超立方体上离散分布族的RNC采样-计数缩减方法。 |
| [^12] | [Trade-off Between Dependence and Complexity for Nonparametric Learning -- an Empirical Process Approach.](http://arxiv.org/abs/2401.08978) | 本文提出了一个通用的界限，该界限覆盖了独立同分布以及存在长期和短期依赖的数据。主要结果表明，在非参数问题中，底层函数类的复杂性和观测之间存在一种非平凡的权衡，决定了学习速率。这一权衡揭示了一种新现象，在长期依赖的情况下，也可以达到与独立同分布设置相同的速率。 |
| [^13] | [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images.](http://arxiv.org/abs/2401.08865) | 本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。 |
| [^14] | [The Impact of Differential Feature Under-reporting on Algorithmic Fairness.](http://arxiv.org/abs/2401.08788) | 本文研究了差异特征未报告对算法公平性的影响，并提出了一个可分析的模型进行刻画。 |
| [^15] | [Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making.](http://arxiv.org/abs/2401.08691) | 本论文探讨了在银行业中解决偏见以实现公平决策的问题。通过无缝整合公平、可解释性和人类监督，构建负责任人工智能文化，以遵守规定并符合人权标准。 |
| [^16] | [Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data.](http://arxiv.org/abs/2401.07231) | 本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法，并扩展了这些方法以应用于时间序列数据。这些方法利用先验知识进行高效因果发现，并具有对因果关系顺序的特殊处理。 |
| [^17] | [Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows.](http://arxiv.org/abs/2401.00828) | 本文提出了一种基于神经算子流的方法，通过近似时间相关算子，实现了在量子场论中从底层自由理论到目标理论的离散-连续归一化流。 |
| [^18] | [Mean-field underdamped Langevin dynamics and its spacetime discretization.](http://arxiv.org/abs/2312.16360) | 这篇论文提出了一种新的算法，用于优化在概率测度空间上定义的非线性泛函，特别适用于训练均场神经网络、最大均值差异最小化和核斯坦差异最小化问题。算法基于均场欠阻尼朗之万动力学的新颖时空离散化，具有快速混合保证，并且在总变化距离下全局收敛。 |
| [^19] | [A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks.](http://arxiv.org/abs/2311.18672) | 该论文比较了不变和等变的经典和量子图神经网络，探讨了它们在高能物理数据分析中的应用，以及如何利用量子计算提供快速而高效的计算范式。同时，研究还指出通过使用不变输入和等变层，可以增强深度网络的有效性和鲁棒性。 |
| [^20] | [Hilbert's projective metric for functions of bounded growth and exponential convergence of Sinkhorn's algorithm.](http://arxiv.org/abs/2311.04041) | 本文对有界增长可积函数空间的Hilbert投影度量进行了研究，提出了某些松弛锥体内的内核积分算子具有压缩映射的性质，并应用于熵最优传输问题中，证明了Sinkhorn算法在边际分布的尾部与成本函数增长适当时呈指数收敛。 |
| [^21] | [Efficient Generalized Low-Rank Tensor Contextual Bandits.](http://arxiv.org/abs/2311.01771) | 本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。 |
| [^22] | [Post-hoc Bias Scoring Is Optimal For Fair Classification.](http://arxiv.org/abs/2310.05725) | 本研究提出了一种后验偏差评分的方法，在满足公平性约束的情况下保持高准确性，并给出了基于偏差分数的修改规则。该方法适用于各种类型的公平性约束问题。 |
| [^23] | [Implicit Gaussian process representation of vector fields over arbitrary latent manifolds.](http://arxiv.org/abs/2309.16746) | 这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。 |
| [^24] | [Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition.](http://arxiv.org/abs/2309.08436) | 本研究提出了一种基于分块注意力编码器-解码器模型的流式语音识别方法，通过在预定义的固定大小窗口上操作，实现了模型的流式运行。实验结果表明，该模型相比非流式变种具有相当的性能，并且在长篇演讲中具有很好的泛化能力。 |
| [^25] | [Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks.](http://arxiv.org/abs/2306.06155) | 本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。 |
| [^26] | [Causal Component Analysis.](http://arxiv.org/abs/2305.17225) | 本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。 |
| [^27] | [Demystifying Oversmoothing in Attention-Based Graph Neural Networks.](http://arxiv.org/abs/2305.16102) | 本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。 |
| [^28] | [Model-Informed Generative Adversarial Network (MI-GAN) for Learning Optimal Power Flow.](http://arxiv.org/abs/2206.01864) | 本文提出了一种基于优化模型信息的生成对抗网络 (MI-GAN) 框架，用于解决可再生能源不确定性对优化问题的影响，提高最优功率流问题的计算效率。 |

# 详细

[^1]: 具有几何平滑动量的随机Kaczmarz方法

    Randomized Kaczmarz with geometrically smoothed momentum. (arXiv:2401.09415v1 [math.NA])

    [http://arxiv.org/abs/2401.09415](http://arxiv.org/abs/2401.09415)

    本文研究了向随机Kaczmarz算法中添加几何平滑动量的效果，并证明了关于最小二乘损失矩阵奇异向量方向上的期望误差。数值示例证明了结果的实用性，并提出了几个问题。

    

    本文研究了向随机Kaczmarz算法中添加几何平滑动量的效果，该算法是线性最小二乘损失函数上随机梯度下降的实例。我们证明了关于定义最小二乘损失的矩阵的奇异向量方向上期望误差的结果。我们给出了几个数值示例来说明我们结果的实用性，并提出了几个问题。

    This paper studies the effect of adding geometrically smoothed momentum to the randomized Kaczmarz algorithm, which is an instance of stochastic gradient descent on a linear least squares loss function. We prove a result about the expected error in the direction of singular vectors of the matrix defining the least squares loss. We present several numerical examples illustrating the utility of our result and pose several questions.
    
[^2]: 解锁无标签数据: Hui-Walter范式在在线和静态环境中的性能评估中的集成学习

    Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter Paradigm for Performance Estimation in Online and Static Settings. (arXiv:2401.09376v1 [cs.LG])

    [http://arxiv.org/abs/2401.09376](http://arxiv.org/abs/2401.09376)

    该论文通过适应Hui-Walter范式，将传统应用于流行病学和医学的方法引入机器学习领域，解决了训练和评估时无法获得标签数据的问题。通过将数据划分为潜在类别，并在多个测试中独立训练模型，能够在没有真实值的情况下估计关键性能指标，并在处理在线数据时提供了新的可能性。

    

    在机器学习和统计建模领域，从业人员常常在可评估和训练的假设下工作，即可访问的、静态的、带有标签的数据。然而，这个假设往往偏离了现实，其中的数据可能是私有的、加密的、难以测量的或者没有标签。本文通过将传统应用于流行病学和医学的Hui-Walter范式调整到机器学习领域来弥合这个差距。这种方法使我们能够在没有真实值可用的情况下估计关键性能指标，如假阳性率、假阴性率和先验概率。我们进一步扩展了这种范式来处理在线数据，开辟了动态数据环境的新可能性。我们的方法涉及将数据划分为潜在类别，以模拟多个数据群体（如果没有自然群体可用），并独立训练模型来复制多次测试。通过在不同数据子集之间交叉制表，我们能够比较二元结果。

    In the realm of machine learning and statistical modeling, practitioners often work under the assumption of accessible, static, labeled data for evaluation and training. However, this assumption often deviates from reality where data may be private, encrypted, difficult- to-measure, or unlabeled. In this paper, we bridge this gap by adapting the Hui-Walter paradigm, a method traditionally applied in epidemiology and medicine, to the field of machine learning. This approach enables us to estimate key performance metrics such as false positive rate, false negative rate, and priors in scenarios where no ground truth is available. We further extend this paradigm for handling online data, opening up new possibilities for dynamic data environments. Our methodology involves partitioning data into latent classes to simulate multiple data populations (if natural populations are unavailable) and independently training models to replicate multiple tests. By cross-tabulating binary outcomes across
    
[^3]: 使用并行随机优化几乎免费实现高置信水平推断

    High Confidence Level Inference is Almost Free using Parallel Stochastic Optimization. (arXiv:2401.09346v1 [stat.ML])

    [http://arxiv.org/abs/2401.09346](http://arxiv.org/abs/2401.09346)

    本文提出了一种使用并行随机优化实现高置信水平推断的方法，通过少量独立多次运行获取分布信息构建置信区间，几乎不需要额外计算和内存，具有高效计算和快速收敛的特点。

    

    近年来，在在线环境中通过随机优化解决方案进行估计的不确定性量化方法备受关注。本文介绍了一种新颖的推理方法，专注于构建具有高效计算和快速收敛到名义水平的置信区间。具体而言，我们建议使用少量独立的多次运行获取分布信息并构建基于t分布的置信区间。我们的方法除了标准估计的更新之外，几乎不需要额外的计算和内存，使推理过程几乎免费。我们对置信区间提供了严格的理论保证，证明了覆盖率几乎确切，具有明确的收敛速度，从而实现了高置信水平的推断。特别地，我们为在线估计器开发了一种新的高斯拟合结果，以相对误差的方式表征了我们置信区间的覆盖特性。

    Uncertainty quantification for estimation through stochastic optimization solutions in an online setting has gained popularity recently. This paper introduces a novel inference method focused on constructing confidence intervals with efficient computation and fast convergence to the nominal level. Specifically, we propose to use a small number of independent multi-runs to acquire distribution information and construct a t-based confidence interval. Our method requires minimal additional computation and memory beyond the standard updating of estimates, making the inference process almost cost-free. We provide a rigorous theoretical guarantee for the confidence interval, demonstrating that the coverage is approximately exact with an explicit convergence rate and allowing for high confidence level inference. In particular, a new Gaussian approximation result is developed for the online estimators to characterize the coverage properties of our confidence intervals in terms of relative erro
    
[^4]: 两时间尺度带马尔可夫噪声的随机逼近中心极限定理：理论和应用

    Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications. (arXiv:2401.09339v1 [stat.ML])

    [http://arxiv.org/abs/2401.09339](http://arxiv.org/abs/2401.09339)

    本文通过对两时间尺度随机逼近（TTSA）的广义分析，利用中心极限定理（CLT）揭示了TTSA受马尔可夫噪声影响的耦合动力学，从而拓展了传统SGD的高效采样策略在分布式学习中的应用范围，同时研究了具有非线性函数逼近的GTD算法的统计特性。

    

    两时间尺度随机逼近（TTSA）是最通用的迭代随机算法框架之一。这包括了众所周知的随机优化方法，如SGD变种和用于双层或极小化问题的方法，以及类似梯度-based时序差异（GTD）算法的强化学习方法。本文通过中心极限定理（CLT）对带控制马尔可夫噪声的TTSA进行了深入的渐近分析，揭示了TTSA受底层马尔可夫链影响的耦合动力学，这在以前仅考虑鞅差异噪声的TTSA的CLT结果中没有得到解决。基于我们的CLT，我们将高效采样策略的应用范围从传统SGD扩展到了更广泛的TTSA背景下的分布式学习，从而扩大了胡等人（2022）的研究范围。此外，我们利用我们的CLT结果推导了具有非线性函数逼近的GTD算法的统计特性。

    Two-timescale stochastic approximation (TTSA) is among the most general frameworks for iterative stochastic algorithms. This includes well-known stochastic optimization methods such as SGD variants and those designed for bilevel or minimax problems, as well as reinforcement learning like the family of gradient-based temporal difference (GTD) algorithms. In this paper, we conduct an in-depth asymptotic analysis of TTSA under controlled Markovian noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA influenced by the underlying Markov chain, which has not been addressed by previous CLT results of TTSA only with Martingale difference noise. Building upon our CLT, we expand its application horizon of efficient sampling strategies from vanilla SGD to a wider TTSA context in distributed learning, thus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT result to deduce the statistical properties of GTD algorithms with nonlinear function approxi
    
[^5]: 缓解机器学习增强的混合模拟中的分布偏移

    Mitigating distribution shift in machine learning-augmented hybrid simulation. (arXiv:2401.09259v1 [math.NA])

    [http://arxiv.org/abs/2401.09259](http://arxiv.org/abs/2401.09259)

    本文研究了机器学习增强的混合模拟中的分布偏移问题，并提出了基于切线空间正则化估计器的方法来控制分布偏移，从而提高模拟结果的精确性。

    

    本文研究了机器学习增强的混合模拟中普遍存在的分布偏移问题，其中模拟算法的部分被数据驱动的替代模型取代。我们首先建立了一个数学框架来理解机器学习增强的混合模拟问题的结构，以及相关的分布偏移的原因和影响。我们在数值和理论上展示了分布偏移与模拟误差的相关性。然后，我们提出了一种基于切线空间正则化估计器的简单方法来控制分布偏移，从而提高模拟结果的长期精确性。在线性动力学情况下，我们提供了一种详尽的理论分析来量化所提出方法的有效性。此外，我们进行了几个数值实验，包括模拟部分已知的反应扩散方程以及使用基于数据驱动的投影方法求解Navier-Stokes方程。

    We study the problem of distribution shift generally arising in machine-learning augmented hybrid simulation, where parts of simulation algorithms are replaced by data-driven surrogates. We first establish a mathematical framework to understand the structure of machine-learning augmented hybrid simulation problems, and the cause and effect of the associated distribution shift. We show correlations between distribution shift and simulation error both numerically and theoretically. Then, we propose a simple methodology based on tangent-space regularized estimator to control the distribution shift, thereby improving the long-term accuracy of the simulation results. In the linear dynamics case, we provide a thorough theoretical analysis to quantify the effectiveness of the proposed method. Moreover, we conduct several numerical experiments, including simulating a partially known reaction-diffusion equation and solving Navier-Stokes equations using the projection method with a data-driven p
    
[^6]: 一个用于计算多类分类中对抗训练下界的最优输运方法

    An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification. (arXiv:2401.09191v1 [cs.LG])

    [http://arxiv.org/abs/2401.09191](http://arxiv.org/abs/2401.09191)

    本文提出了一个用于计算多类分类中对抗训练下界的最优输运方法，并利用该方法提出了计算最优对抗风险下界和确定最优分类器的算法。通过截断类之间的高阶相互作用，避免了组合运行时间的问题。

    

    尽管基于深度学习的算法取得了很大的成功，但广为人知的是神经网络可能缺乏鲁棒性。强制鲁棒性的流行范式是对抗训练（AT），然而，这引入了许多计算和理论上的困难。最近的研究在多类分类设置和多边际最优输运（MOT）之间建立了联系，为研究这个问题提供了一套新的工具。在本文中，我们利用MOT的连接，提出了计算上最简便可行的数值算法来计算最优对抗风险的普遍下界，并确定最优分类器。我们基于线性规划（LP）和熵正则化（Sinkhorn）提出了两个主要算法。我们的关键洞察是可以无害地截断类之间的高阶相互作用，从而避免了在MOT问题中通常遇到的组合运行时间。我们通过在MNIST和CI 上进行实验证实了这些结果

    Despite the success of deep learning-based algorithms, it is widely known that neural networks may fail to be robust. A popular paradigm to enforce robustness is adversarial training (AT), however, this introduces many computational and theoretical difficulties. Recent works have developed a connection between AT in the multiclass classification setting and multimarginal optimal transport (MOT), unlocking a new set of tools to study this problem. In this paper, we leverage the MOT connection to propose computationally tractable numerical algorithms for computing universal lower bounds on the optimal adversarial risk and identifying optimal classifiers. We propose two main algorithms based on linear programming (LP) and entropic regularization (Sinkhorn). Our key insight is that one can harmlessly truncate the higher order interactions between classes, preventing the combinatorial run times typically encountered in MOT problems. We validate these results with experiments on MNIST and CI
    
[^7]: 深度学习模型的两尺度复杂度测量

    A Two-Scale Complexity Measure for Deep Learning Models. (arXiv:2401.09184v1 [stat.ML])

    [http://arxiv.org/abs/2401.09184](http://arxiv.org/abs/2401.09184)

    这篇论文介绍了一种用于统计模型的新容量测量2sED，可以可靠地限制泛化误差，并且与训练误差具有很好的相关性。此外，对于深度学习模型，我们展示了如何通过逐层迭代的方法有效地近似2sED，从而处理大量参数的情况。

    

    我们引入了一种基于有效维度的统计模型新容量测量2sED。这个新的数量在对模型进行温和假设的情况下，可以可靠地限制泛化误差。此外，对于标准数据集和流行的模型架构的模拟结果表明，2sED与训练误差具有很好的相关性。对于马尔可夫模型，我们展示了如何通过逐层迭代的方法有效地从下方近似2sED，从而解决具有大量参数的深度学习模型。模拟结果表明，这种近似对不同的突出模型和数据集都很好。

    We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.
    
[^8]: 监控机器学习对平台数据流的预测

    Monitoring Machine Learning Forecasts for Platform Data Streams. (arXiv:2401.09144v1 [stat.AP])

    [http://arxiv.org/abs/2401.09144](http://arxiv.org/abs/2401.09144)

    这篇论文提出了一种简单的数据驱动的监控流程，用于确定机器学习算法何时需要重新训练，以确保准确稳定的预测结果。

    

    数据流预测对数字平台的决策至关重要。机器学习算法是生成这种预测的有吸引力的候选方案。然而，数字平台需要一个大规模的预测框架，可以灵活地应对突然的性能下降。通常情况下，以与新数据批次输入的速度相同的速度重新训练机器学习算法的计算成本太高。另一方面，不频繁的重新训练需要指定重新训练的频率，并且通常会导致预测效果的严重下降。为了确保准确稳定的预测，我们提出了一种简单的数据驱动的监控流程，来回答机器学习算法何时应该重新训练的问题。我们不是研究数据流的不稳定性，而是测试传入的流式预测损失批次是否与一个明确定义的参考批次不同。 使用一个由15分钟频率的数据流组成的新数据集，该数据集来自于在伦敦运营的按需物流平台，我们应用了这个监控流程，

    Data stream forecasts are essential inputs for decision making at digital platforms. Machine learning algorithms are appealing candidates to produce such forecasts. Yet, digital platforms require a large-scale forecast framework that can flexibly respond to sudden performance drops. Re-training ML algorithms at the same speed as new data batches enter is usually computationally too costly. On the other hand, infrequent re-training requires specifying the re-training frequency and typically comes with a severe cost of forecast deterioration. To ensure accurate and stable forecasts, we propose a simple data-driven monitoring procedure to answer the question when the ML algorithm should be re-trained. Instead of investigating instability of the data streams, we test if the incoming streaming forecast loss batch differs from a well-defined reference batch. Using a novel dataset constituting 15-min frequency data streams from an on-demand logistics platform operating in London, we apply the
    
[^9]: 理解图神经网络的异质性

    Understanding Heterophily for Graph Neural Networks. (arXiv:2401.09125v1 [cs.LG])

    [http://arxiv.org/abs/2401.09125](http://arxiv.org/abs/2401.09125)

    本文通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对图神经网络（GNNs）影响的理论理解。研究发现，异质性对分类的影响需要与平均节点度一起评估，并且拓扑噪声对分类有负面影响。

    

    具有异质性的图被认为是图神经网络（GNNs）面临挑战的情景，其中节点通过各种模式与不同的邻居相连接。本文通过将图卷积（GC）操作合并到完全连接的网络中，通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对GNNs影响的理论理解，HSBM是一个可以容纳多样的异质性模式的通用随机图模型。首先，我们展示了通过应用GC操作，可分性增益取决于两个因素，即邻域分布的欧氏距离和$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$，其中$\mathbb{E}\left[\operatorname{deg}\right]$是平均节点度。它揭示了异质性对分类的影响需要与平均节点度一起评估。其次，我们展示了拓扑噪声具有负面影响

    Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where $\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimenta
    
[^10]: 固定预算差分隐私最佳臂识别

    Fixed-Budget Differentially Private Best Arm Identification. (arXiv:2401.09073v1 [cs.LG])

    [http://arxiv.org/abs/2401.09073](http://arxiv.org/abs/2401.09073)

    本论文研究了差分隐私约束下固定预算条件下的最佳臂识别问题，提出了满足差分隐私约束的策略DP-BAI，并得到了错误概率的上界和最小-最大下界的指数衰减关系。

    

    我们在差分隐私约束下研究了线性赌博机中固定预算条件下的最佳臂识别问题，其中臂的奖励在单位区间上。给定一个有限的预算$T$和隐私参数$\varepsilon>0$，目标是在$T$个采样轮后最小化寻找平均值最大的臂的错误概率，同时满足决策者策略满足特定的$\varepsilon$-差分隐私($\varepsilon$-DP)约束条件。我们通过提出“最大绝对行列式”原则构建满足$\varepsilon$-DP约束的策略(称为DP-BAI)，并给出其错误概率的上界。此外，我们得到了错误概率的最小-最大下界，并证明这两个界在$T$上按指数衰减，界中的指数与(a)臂的次优间隙，(b)$\varepsilon$和(c)关联。

    We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\varepsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\em $\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by proposing the principle of {\em maximum absolute determinants}, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and (c) t
    
[^11]: 快速并行采样在等周密度下

    Fast parallel sampling under isoperimetry. (arXiv:2401.09016v1 [cs.DS])

    [http://arxiv.org/abs/2401.09016](http://arxiv.org/abs/2401.09016)

    通过并行化Langevin算法和欠阻尼的Langevin算法，我们展示了一个在等周密度下并行采样的方法。我们的算法具有TV距离保证，并在总梯度评估次数上具有线性或者平方根级别的复杂度。对于主要应用，在保证TV距离的前提下，我们得到了超立方体上离散分布族的RNC采样-计数缩减方法。

    

    我们展示了如何通过并行化Langevin算法（或欠阻尼的Langevin算法）从满足对数Sobolev不等式且具有平滑的对数密度的分布π中并行采样。我们证明我们的算法输出的样本遵循Kullback-Leibler（KL）散度（或者总变差（TV）距离）接近π的分布π。同时我们的算法只使用O(log(d))个并行轮次和O(d)（或O(√d)个）梯度评估。这是第一个具有TV距离保证的并行采样算法。对于我们的主要应用，我们展示了如何将我们算法的TV距离保证与之前的工作相结合，从而得到对于超立方体上的离散分布族的RNC采样-计数缩减。这些分布族在指数倾斜下保持封闭且有有界协方差。因此，我们得到了一个用于有向欧拉通路和非对称确定的RNC采样器。

    We show how to sample in parallel from a distribution $\pi$ over $\mathbb R^d$ that satisfies a log-Sobolev inequality and has a smooth log-density, by parallelizing the Langevin (resp. underdamped Langevin) algorithms. We show that our algorithm outputs samples from a distribution $\hat\pi$ that is close to $\pi$ in Kullback--Leibler (KL) divergence (resp. total variation (TV) distance), while using only $\log(d)^{O(1)}$ parallel rounds and $\widetilde{O}(d)$ (resp. $\widetilde O(\sqrt d)$) gradient evaluations in total. This constitutes the first parallel sampling algorithms with TV distance guarantees.  For our main application, we show how to combine the TV distance guarantees of our algorithms with prior works and obtain RNC sampling-to-counting reductions for families of discrete distribution on the hypercube $\{\pm 1\}^n$ that are closed under exponential tilts and have bounded covariance. Consequently, we obtain an RNC sampler for directed Eulerian tours and asymmetric determin
    
[^12]: 非参数学习中依赖性和复杂性的权衡 -- 一种经验过程方法

    Trade-off Between Dependence and Complexity for Nonparametric Learning -- an Empirical Process Approach. (arXiv:2401.08978v1 [math.ST])

    [http://arxiv.org/abs/2401.08978](http://arxiv.org/abs/2401.08978)

    本文提出了一个通用的界限，该界限覆盖了独立同分布以及存在长期和短期依赖的数据。主要结果表明，在非参数问题中，底层函数类的复杂性和观测之间存在一种非平凡的权衡，决定了学习速率。这一权衡揭示了一种新现象，在长期依赖的情况下，也可以达到与独立同分布设置相同的速率。

    

    经验过程理论已经成为理解各种统计问题泛用工具的独特工具，尤其适用于独立同分布观测数据。然而，在许多数据呈现时间依赖性的应用中（如金融、医学影像、天气预报等），相应的经验过程则了解得不多。受到这一观察的启发，我们提出了在标准的β/ρ混合假设下经验过程期望上确界的一般界限。与大多数现有工作不同，我们的结果涵盖了长期和短期依赖的情况。我们的主要结果显示，在大类非参数问题中，底层函数类的复杂性和观测之间的依赖性之间存在一种非平凡的权衡，这一权衡决定了学习速率。这种权衡揭示了一种新现象，即在长期依赖情况下，也可以达到与独立同分布设置相同的速率，证。

    Empirical process theory for i.i.d. observations has emerged as a ubiquitous tool for understanding the generalization properties of various statistical problems. However, in many applications where the data exhibit temporal dependencies (e.g., in finance, medical imaging, weather forecasting etc.), the corresponding empirical processes are much less understood. Motivated by this observation, we present a general bound on the expected supremum of empirical processes under standard $\beta/\rho$-mixing assumptions. Unlike most prior work, our results cover both the long and the short-range regimes of dependence. Our main result shows that a non-trivial trade-off between the complexity of the underlying function class and the dependence among the observations characterizes the learning rate in a large class of nonparametric problems. This trade-off reveals a new phenomenon, namely that even under long-range dependence, it is possible to attain the same rates as in the i.i.d. setting, prov
    
[^13]: Intrinsic Dataset Properties对泛化能力的影响：揭示自然图像和医学图像之间的学习差异

    The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])

    [http://arxiv.org/abs/2401.08865](http://arxiv.org/abs/2401.08865)

    本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。

    

    本文研究了神经网络在不同图像领域学习时的差异，这在从自然图像到其他专门领域（如医学图像）采用计算机视觉技术时通常被忽视。最近的研究发现，训练集的固有维度($d_{data}$)与网络的泛化错误一般会增加。然而，医学（放射学）和自然图像领域之间的这种关系的陡峭程度存在显著差异，且无现有的理论解释。我们通过建立并经验证一个与$d_{data}$相关的泛化缩放定律来解决这个知识空白，并提出考虑到医学图像数据集更高的固有“标签锐度”($K_F$)这一度量指标可以部分解释这两个领域之间的显著缩放差异。接下来，我们展示了利用测量这一指标可以提供的额外好处。

    This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
    
[^14]: 差异特征未报告对算法公平性的影响

    The Impact of Differential Feature Under-reporting on Algorithmic Fairness. (arXiv:2401.08788v1 [cs.LG])

    [http://arxiv.org/abs/2401.08788](http://arxiv.org/abs/2401.08788)

    本文研究了差异特征未报告对算法公平性的影响，并提出了一个可分析的模型进行刻画。

    

    公共部门的预测风险模型通常使用更完整的行政数据来开发，这些数据对于更大程度依赖公共服务的亚群体更为完整。例如，在美国，对于由医疗补助和医疗保险支持的个人，政府机构常常可以获得有关医疗保健利用的信息，但对于私人保险的人则没有。对公共部门算法的批评指出，差异特征未报告导致算法决策中的不公平。然而，这种数据偏见在技术视角下仍然研究不足。虽然以前的研究已经考察了添加特征噪声和明确标记为缺失的特征对公平性的影响，但缺失指标的数据缺失情况（即差异特征未报告）尚未得到研究的关注。在本研究中，我们提出了一个可分析的差异特征未报告模型，并将其应用于特征未报告对算法公平性的刻画。

    Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. In this work, we present an analytically tractable model of differential feature under-reporting which we then use to charac
    
[^15]: 在银行业实现负责任的人工智能：解决偏见以实现公平决策

    Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making. (arXiv:2401.08691v1 [stat.ML])

    [http://arxiv.org/abs/2401.08691](http://arxiv.org/abs/2401.08691)

    本论文探讨了在银行业中解决偏见以实现公平决策的问题。通过无缝整合公平、可解释性和人类监督，构建负责任人工智能文化，以遵守规定并符合人权标准。

    

    在人工智能广泛应用于各行各业的决策过程的时代，对信任的需求变得更加强烈。本论文对偏见和公平进行了全面的探讨，特别关注它们在银行业内的影响，因为以人工智能驱动的决策对社会产生了重大影响。在这个背景下，公平、可解释性和人类监督的无缝整合变得至关重要，最终形成了所谓的“负责任人工智能”。这强调了在开发符合人工智能规定和普世人权标准的企业文化时，解决偏见的重要性，特别是在自动决策系统领域。如今，将伦理原则融入到人工智能模型的开发、训练和部署中对于遵守即将到来的规定非常关键。

    In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as "Responsible AI". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcom
    
[^16]: 利用先验知识发现具有未观测变量的因果可加模型及其在时间序列数据中的应用

    Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data. (arXiv:2401.07231v1 [cs.LG])

    [http://arxiv.org/abs/2401.07231](http://arxiv.org/abs/2401.07231)

    本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法，并扩展了这些方法以应用于时间序列数据。这些方法利用先验知识进行高效因果发现，并具有对因果关系顺序的特殊处理。

    

    本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法。CAM-UV假设因果函数采用广义可加模型的形式，并存在潜在的混淆变量。首先，我们提出了一种利用先验知识进行高效因果发现的方法。然后，我们扩展了这种方法，用于推断时间序列数据的因果关系。与其他现有的因果函数模型不同，原始的CAM-UV算法不寻求观测变量之间的因果顺序，而是旨在确定每个观测变量的原因。因此，本文中提出的第一种方法利用先验知识，例如理解某些变量不能成为特定变量的原因。此外，通过融入因果在时间上的先验知识，我们将第一个算法扩展为第二种用于时间序列数据中的因果发现的方法。我们验证了第一个提出的方法。

    This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method
    
[^17]: 基于神经算子流的量子场论多格采样方法

    Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00828](http://arxiv.org/abs/2401.00828)

    本文提出了一种基于神经算子流的方法，通过近似时间相关算子，实现了在量子场论中从底层自由理论到目标理论的离散-连续归一化流。

    

    本文考虑从玻尔兹曼分布中采样离散场配置$\phi$的问题，其中$S$是某个量子场论连续欧几里得作用$\mathcal S$的格点离散化。我们将该密度近似视为底层函数密度$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$的学习算子实例。具体而言，我们提出了近似时间相关算子$\mathcal V_t$的方法，其时间积分提供了自由理论$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$的函数分布与目标理论$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$之间的映射。当选择特定的格点时，算子$\mathcal V_t$可以离散化为有限维的时间相关矢量场$V_t$，从而在离散格点上实现了连续的归一化流。

    We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
    
[^18]: 均场欠阻尼朗之万动力学及其时空离散化算法

    Mean-field underdamped Langevin dynamics and its spacetime discretization. (arXiv:2312.16360v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2312.16360](http://arxiv.org/abs/2312.16360)

    这篇论文提出了一种新的算法，用于优化在概率测度空间上定义的非线性泛函，特别适用于训练均场神经网络、最大均值差异最小化和核斯坦差异最小化问题。算法基于均场欠阻尼朗之万动力学的新颖时空离散化，具有快速混合保证，并且在总变化距离下全局收敛。

    

    我们提出了一种名为N粒子欠阻尼朗之万算法的新方法，用于优化在概率测度空间上定义的一类非线性泛函。这种公式的问题示例包括训练均场神经网络、最大均值差异最小化和核斯坦差异最小化。我们的算法基于均场欠阻尼朗之万动力学的新颖时空离散化，我们提供了一种新的快速混合保证。此外，我们证明了我们的算法在总变化距离下全局收敛，填补了动力学与实际实施之间的理论差距。

    We propose a new method called the N-particle underdamped Langevin algorithm for optimizing a special class of non-linear functionals defined over the space of probability measures. Examples of problems with this formulation include training mean-field neural networks, maximum mean discrepancy minimization and kernel Stein discrepancy minimization. Our algorithm is based on a novel spacetime discretization of the mean-field underdamped Langevin dynamics, for which we provide a new, fast mixing guarantee. In addition, we demonstrate that our algorithm converges globally in total variation distance, bridging the theoretical gap between the dynamics and its practical implementation.
    
[^19]: 不变和等变的经典和量子图神经网络的比较

    A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2311.18672](http://arxiv.org/abs/2311.18672)

    该论文比较了不变和等变的经典和量子图神经网络，探讨了它们在高能物理数据分析中的应用，以及如何利用量子计算提供快速而高效的计算范式。同时，研究还指出通过使用不变输入和等变层，可以增强深度网络的有效性和鲁棒性。

    

    机器学习算法在理解CERN大型强子对撞机(LHC)上产生的大量高能粒子碰撞数据时起着重要作用。这些碰撞事件的数据可以自然地用图结构表示。因此，深度几何方法，如图神经网络(GNNs)，已经在高能物理数据分析的各种任务中得到应用。一个典型的任务是喷注标记，其中喷注被视为具有不同特征和其组成粒子之间的边连接的点云。LHC粒子数据集的规模和复杂性的增加，以及用于其分析的计算模型，大大促进了开发替代快速且高效的计算范式，如量子计算。此外，为了增强深度网络的有效性和鲁棒性，可以通过使用不变输入和等变层来利用数据中存在的基本对称性。

    Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In t
    
[^20]: Hilbert的投影度量用于有界增长函数和Sinkhorn算法的指数收敛

    Hilbert's projective metric for functions of bounded growth and exponential convergence of Sinkhorn's algorithm. (arXiv:2311.04041v2 [math.PR] UPDATED)

    [http://arxiv.org/abs/2311.04041](http://arxiv.org/abs/2311.04041)

    本文对有界增长可积函数空间的Hilbert投影度量进行了研究，提出了某些松弛锥体内的内核积分算子具有压缩映射的性质，并应用于熵最优传输问题中，证明了Sinkhorn算法在边际分布的尾部与成本函数增长适当时呈指数收敛。

    

    受无界环境中的熵最优传输问题的启发，我们研究了有界增长可积函数空间的Hilbert投影度量版本。这些Hilbert度量版本源自锥体，这些锥体是所有非负函数的松弛，即它们包括所有在与某些测试函数相乘时具有非负积分值的函数。我们证明了内核积分算子在适当的度量规范下是压缩映射，即使内核没有与零间隔，前提是内核趋向于零受控制。作为熵最优传输的应用，我们证明了在边际分布的尾部相对于成本函数的增长足够轻时，Sinkhorn算法呈指数收敛的性质。

    Motivated by the entropic optimal transport problem in unbounded settings, we study versions of Hilbert's projective metric for spaces of integrable functions of bounded growth. These versions of Hilbert's metric originate from cones which are relaxations of the cone of all non-negative functions, in the sense that they include all functions having non-negative integral values when multiplied with certain test functions. We show that kernel integral operators are contractions with respect to suitable specifications of such metrics even for kernels which are not bounded away from zero, provided that the decay to zero of the kernel is controlled. As an application to entropic optimal transport, we show exponential convergence of Sinkhorn's algorithm in settings where the marginal distributions have sufficiently light tails compared to the growth of the cost function.
    
[^21]: 高效的广义低秩张量情境赌博算法

    Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])

    [http://arxiv.org/abs/2311.01771](http://arxiv.org/abs/2311.01771)

    本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。

    

    本文旨在构建一种新颖的赌博算法，能够充分利用多维数据和奖励函数的固有非线性特性，提供高可用和负责任的决策服务。为此，我们引入了一种广义低秩张量情境赌博模型，其中一个动作由三个特征向量组成，因此可以用张量表示。在这个模型中，奖励是通过将动作的特征张量与一个固定但未知的参数张量的内积应用于广义线性函数来确定的，而这个参数张量具有较低的管状秩。为了实现探索和利用之间的权衡，我们引入了一种名为“广义低秩张量探索子空间然后细化”的新算法（G-LowTESTR）。该算法首先收集原始数据，以探索嵌入在决策情境中的本质低秩张量子空间信息，然后将原始概率转换为可解释的结构化概率。

    In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
    
[^22]: 后验偏差评分对公平分类最优

    Post-hoc Bias Scoring Is Optimal For Fair Classification. (arXiv:2310.05725v1 [stat.ML])

    [http://arxiv.org/abs/2310.05725](http://arxiv.org/abs/2310.05725)

    本研究提出了一种后验偏差评分的方法，在满足公平性约束的情况下保持高准确性，并给出了基于偏差分数的修改规则。该方法适用于各种类型的公平性约束问题。

    

    我们考虑了一个在群体公平性约束下的二元分类问题，该问题可以是人口统计学公平性（DP），机会均等（EOp）或等概率（EO）之一。我们提出了在公平性约束下贝叶斯最优分类器的明确特征化，结果是不受约束分类器的简单修改规则。即，我们引入了一种新的实例级别的偏差度量，称为偏差分数，而修改规则则是在有限量的偏差分数之上的简单线性规则。基于这个特征化，我们开发了一种后验方法，使我们能够适应公平性约束同时保持较高的准确性。在DP和EOp约束的情况下，修改规则是基于单个偏差分数的阈值选择，而在EO约束的情况下，我们需要调整具有2个参数的线性修改规则。该方法还可以用于包含多个敏感属性的复合群体公平性标准的情况。

    We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive att
    
[^23]: 隐性高斯过程表示任意潜在流形上的向量场

    Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])

    [http://arxiv.org/abs/2309.16746](http://arxiv.org/abs/2309.16746)

    这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。

    

    高斯过程（GPs）是用于学习未知函数和量化数据中的时空不确定性的流行非参数统计模型。最近的研究扩展了GPs，用于建模分布在非欧几里得域上的标量和向量数据，包括出现在计算机视觉、动力系统和神经科学等众多领域中的平滑流形。然而，这些方法假设数据的潜在流形是已知的，限制了它们的实际效用。我们引入了RVGP，一种用于学习潜在黎曼流形上的向量信号的GP的推广。我们的方法使用与切向丛关联的连接Laplacian的特征函数进行位置编码，这些特征函数可以从基于图的常见数据近似中轻松推导出来。我们证明了RVGP在流形上具有全局规律性，使得其能够在保留奇异性的同时超分辨率和修复向量场。此外，我们使用RVGP来重构高密度数据。

    Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
    
[^24]: 基于分块注意力编码器-解码器模型的流式语音识别研究

    Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition. (arXiv:2309.08436v1 [eess.AS])

    [http://arxiv.org/abs/2309.08436](http://arxiv.org/abs/2309.08436)

    本研究提出了一种基于分块注意力编码器-解码器模型的流式语音识别方法，通过在预定义的固定大小窗口上操作，实现了模型的流式运行。实验结果表明，该模型相比非流式变种具有相当的性能，并且在长篇演讲中具有很好的泛化能力。

    

    我们研究了一种可流式运行的基于注意力的编码器-解码器模型，其中解码器或编码器和解码器都可以在预定义的固定大小的窗口（称为块）上操作。一种特殊的块结束符（EOC）符号从一个块进入到下一个块，有效地替代了传统的序列结束符。这个修改将我们的模型置于一个操作块而不是帧的转换模型，其中EOC对应空白符号。我们进一步探索了标准转换器模型和我们模型之间的其他差异。此外，我们还研究了长篇演讲的泛化能力、束搜索大小和长度规范化等相关方面。通过在Librispeech和TED-LIUM-v2上的实验，并通过连接连续的序列进行长篇试验，我们发现我们的流式模型相比于非流式变种具有竞争性的性能，并且对于长篇演讲非常泛化。

    We study a streamable attention-based encoder-decoder model in which either the decoder, or both the encoder and decoder, operate on pre-defined, fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances from one chunk to the next chunk, effectively replacing the conventional end-of-sequence symbol. This modification, while minor, situates our model as equivalent to a transducer model that operates on chunks instead of frames, where EOC corresponds to the blank symbol. We further explore the remaining differences between a standard transducer and our model. Additionally, we examine relevant aspects such as long-form speech generalization, beam size, and length normalization. Through experiments on Librispeech and TED-LIUM-v2, and by concatenating consecutive sequences for long-form trials, we find that our streamable model maintains competitive performance compared to the non-streamable variant and generalizes very well to long-form speech.
    
[^25]: 强度轮廓投影：用于动态网络的连续时间表示学习框架。

    Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])

    [http://arxiv.org/abs/2306.06155](http://arxiv.org/abs/2306.06155)

    本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。

    

    我们提出了一种名为“强度轮廓投影”的新算法框架，用于学习动态网络节点的连续时间表示，该动态网络由节点集和在连续时间内发生的瞬时交互事件的集合所特征化。我们的框架包括三个阶段：通过核平滑等方法估计节点对之间交互的强度函数；学习一个最小化某种强度重构误差的投影；通过学习的投影归纳构造出不断发展的节点表示。我们展示了我们的表示保留了网络的基本结构，并具有时间一致性，这意味着节点表示可以在不同的时间点上进行有意义的比较。同时，我们也构建了估计理论来阐明平滑作为偏差方差折衷的作用，并展示了如何随着信噪比的增加而减少平滑程度以获得更好的性能。

    We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
    
[^26]: 因果成分分析

    Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])

    [http://arxiv.org/abs/2305.17225](http://arxiv.org/abs/2305.17225)

    本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。

    

    独立成分分析(ICA)的目标是从混合观测到的变量中恢复独立的潜在变量。而因果表示学习(CRL)的目标是推断因果关系强相关性的潜在变量，以及编码它们的因果关系的未知图。我们引入了一个中间问题，称为因果成分分析(CauCA)。CauCA可以被看作是ICA的一种推广，对潜在成分之间的因果依赖建模，也是CRL的一个特例。与CRL不同的是，它预设了因果图的知识，仅关注于学习解混函数和因果机制。所有关于CauCA回收基础真相的不可能结果也适用于CRL，而可能性结果可以作为扩展CRL的基础。我们将从对潜在因果变量实施不同类型干预的多个数据集中表征CauCA的可识别性。

    Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
    
[^27]: 揭示基于注意力的图神经网络中的平滑过度现象

    Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])

    [http://arxiv.org/abs/2305.16102](http://arxiv.org/abs/2305.16102)

    本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。

    

    图神经网络中的平滑过度指的是增加网络深度导致节点表示变得相同的现象。尽管之前的研究已经证实了图卷积网络(GCN)会指数级失去表达能力，但是图注意力机制是否可以缓解平滑过度问题还存在争议。本文通过将基于注意力的图神经网络视为非线性时变动态系统，并结合非齐次矩阵乘积和联合谱半径理论的工具和技术，对这个问题进行了严格的数学分析，提出了一个明确的答案。我们证明了与流行观点相反，图注意力机制不能防止平滑过度现象，并且呈指数级失去表达能力。所提出的框架将对称GCN的平滑过度问题扩展到了更广泛的GNN模型类别中。特别地，我们的分析考虑了在现实应用中普遍存在的不对称、状态相关和有向图结构。

    Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
    
[^28]: 基于模型信息的生成对抗网络 (MI-GAN) 用于学习最优功率流

    Model-Informed Generative Adversarial Network (MI-GAN) for Learning Optimal Power Flow. (arXiv:2206.01864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01864](http://arxiv.org/abs/2206.01864)

    本文提出了一种基于优化模型信息的生成对抗网络 (MI-GAN) 框架，用于解决可再生能源不确定性对优化问题的影响，提高最优功率流问题的计算效率。

    

    最优功率流 (OPF) 问题作为电力系统运营的关键组成部分，由于可再生能源的变异性、间歇性和不可预测性给解决带来了越来越大的困难。虽然传统的优化技术，如随机化和鲁棒性优化方法，可以用来解决OPF问题，但面对可再生能源的不确定性，即优化模型中的动态系数，它们在处理大规模问题方面的效果仍然有限。因此，近年来已经开发出深度学习技术，如神经网络，以提高利用数据解决OPF问题的计算效率。然而，解决方案的可行性和最优性可能无法得到保证，系统动态也无法得到适当的处理。在本文中，我们提出了一种基于优化模型信息的生成对抗网络 (MI-GAN) 框架，以解决这些问题。

    The optimal power flow (OPF) problem, as a critical component of power system operations, becomes increasingly difficult to solve due to the variability, intermittency, and unpredictability of renewable energy brought to the power system. Although traditional optimization techniques, such as stochastic and robust optimization approaches, could be leveraged to address the OPF problem, in the face of renewable energy uncertainty, i.e., the dynamic coefficients in the optimization model, their effectiveness in dealing with large-scale problems remains limited. As a result, deep learning techniques, such as neural networks, have recently been developed to improve computational efficiency in solving OPF problems with the utilization of data. However, the feasibility and optimality of the solution may not be guaranteed, and the system dynamics cannot be properly addressed as well. In this paper, we propose an optimization model-informed generative adversarial network (MI-GAN) framework to so
    

