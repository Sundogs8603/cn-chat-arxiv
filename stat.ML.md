# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning to Act from Actionless Videos through Dense Correspondences.](http://arxiv.org/abs/2310.08576) | 本研究提出了一种通过密集对应关系从无动作视频中学习执行动作的方法，能够在不使用任何动作注释的情况下构建可靠执行多样任务的基于视频的机器人策略。 |
| [^2] | [Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining.](http://arxiv.org/abs/2310.08566) | 通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。 |
| [^3] | [Characterizing climate pathways using feature importance on echo state networks.](http://arxiv.org/abs/2310.08495) | 本文探索了使用回声状态网络（ESN）来刻画气候路径的方法，并使用特征重要性分析来了解各个变量对气候路径的影响。 |
| [^4] | [Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes.](http://arxiv.org/abs/2310.08479) | 该论文介绍了一种个性化动态超级学习算法 (POSL)，可以实现实时更新的预测。作者通过预测血液透析患者的对流容积，展示了POSL在中位绝对误差、大型校准、区分度和净收益方面的优越表现。 |
| [^5] | [Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application.](http://arxiv.org/abs/2310.08426) | 本文描述了Heterogeneity in Integration and Prediction (HIP)的扩展，并提供了一个R Shiny应用界面，能够适应多种结果类型，同时保留了HIP的优势。 |
| [^6] | [Differentially Private Non-convex Learning for Multi-layer Neural Networks.](http://arxiv.org/abs/2310.08425) | 本文研究了具有差分隐私的非凸学习问题在多层神经网络中的应用。通过提出新算法，实现了在数据维度不变的情况下达到过度群体风险。同时，对比了不同链接函数和不同模型设定的结果。 |
| [^7] | [Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis.](http://arxiv.org/abs/2310.08410) | 这项研究总结了关于评估ChatGPT在医学中性能的现有证据，并发现ChatGPT在处理医学查询时的准确率为56%。然而，由于缺乏评估标准，研究中存在方法上的不一致和详细信息不全的问题。 |
| [^8] | [How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?.](http://arxiv.org/abs/2310.08391) | 本文研究了在线性回归中的上下文学习，并发现有效的预训练只需要少量独立任务，预训练模型与贝叶斯最优算法接近。这些理论发现对ICL的统计基础提供了启示。 |
| [^9] | [Impact of multi-armed bandit strategies on deep recurrent reinforcement learning.](http://arxiv.org/abs/2310.08331) | 本研究在自动驾驶场景中使用部分可观测系统，通过部署和测试多种技术来平衡探索和利用的权衡，以预测方向盘操作。 |
| [^10] | [A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors.](http://arxiv.org/abs/2310.08287) | 本文对贝叶斯神经网络后验进行了大规模探索，研究了逼近后验的最佳方法和后验质量与不确定性量化的关系，同时发现了权重空间对称性对后验的影响。 |
| [^11] | [Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift.](http://arxiv.org/abs/2310.08237) | 该论文提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法，通过建立收敛速度为一般损失函数提供了统一的理论分析。 |
| [^12] | [Conformal inference for regression on Riemannian Manifolds.](http://arxiv.org/abs/2310.08209) | 本文研究了在黎曼流形上进行回归场景的预测集，并证明了这些区域的经验版本在大样本下的收敛性。 |
| [^13] | [On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks.](http://arxiv.org/abs/2310.08150) | 本文研究了高维向量时间序列样本协方差矩阵的函数的最大型统计量，推广了样本自协方差函数的最大偏差情况，证明了Gumbel型极值渐近性成立，并将其应用于金融和卷积网络领域。 |
| [^14] | [Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects.](http://arxiv.org/abs/2310.08115) | 提出了一种模型不可知的推断方法，在部分可辨识的因果估计中应用广泛。该方法基于最优输运问题的对偶理论，能够适应随机实验和观测研究，并且具有统一有效和双重鲁棒性。 |
| [^15] | [Learning Regularized Monotone Graphon Mean-Field Games.](http://arxiv.org/abs/2310.08089) | 本文研究了正则化图匹配场博弈的存在性和学习算法问题，在弱单调条件下提出了一种离散时间算法，并开发了动作值函数估计过程作为优化过程的子模块。 |
| [^16] | [Log-Gaussian Gamma Processes for Training Bayesian Neural Networks in Raman and CARS Spectroscopies.](http://arxiv.org/abs/2310.08055) | 本论文提出了一种利用gamma分布和log-Gaussian建模的方法，用于生成合成数据集以训练神经网络，解决了实际观测数据有限的挑战。通过应用于Raman和CARS光谱，同时训练两个贝叶斯神经网络来估计gamma过程的参数，可以估计基础的光谱并提供不确定性。 |
| [^17] | [Lattice real-time simulations with learned optimal kernels.](http://arxiv.org/abs/2310.08053) | 本论文提出了一种使用学习的最优核进行格子实时模拟的方法，在复杂朗之万方法的基础上添加先验信息，成功克服了符号问题，并扩展了实时模拟的范围，并避免了离散化问题。 |
| [^18] | [Local Graph Clustering with Noisy Labels.](http://arxiv.org/abs/2310.08031) | 本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。 |
| [^19] | [Robust 1-bit Compressed Sensing with Iterative Hard Thresholding.](http://arxiv.org/abs/2310.08019) | 本文研究了鲁棒的一比特压缩感知问题，并提出了二元迭代硬阈值化（BIHT）算法，在噪声情况下比目前已知方法表现更好。 |
| [^20] | [LEMON: Lossless model expansion.](http://arxiv.org/abs/2310.07999) | LEMON是一种无损模型扩展方法，在深度神经网络中能够通过利用小型预训练模型的知识来初始化和训练大型模型，从而大大减少训练时间，同时具有通用性适用于各种网络结构。 |
| [^21] | [RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization.](http://arxiv.org/abs/2310.07983) | RandCom是一种去中心化的随机通信跳跃方法，能够在分布式优化中通过概率性本地更新减少通信开销，并在不同的设置中实现线性加速。 |
| [^22] | [Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment.](http://arxiv.org/abs/2310.07973) | 本研究针对选择最有可能从治疗中获益的人的问题，在估计治疗效果和确定截断值时面临多重测试问题，提出一种统一的置信带方法来评估这些个体的平均治疗效果。 |
| [^23] | [Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach.](http://arxiv.org/abs/2310.07970) | 这个论文研究了代理优化算法中超参数的影响，并提出了一种自适应搜索的代理优化方法（HASSO），该方法可以动态调整超参数而不需要额外的评估。该方法旨在提高代理优化算法的可访问性、效果和收敛速度。 |
| [^24] | [Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning.](http://arxiv.org/abs/2310.07918) | 本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。 |
| [^25] | [Efficient Integrators for Diffusion Generative Models.](http://arxiv.org/abs/2310.07894) | 本文提出了共轭积分器和分裂积分器两种框架，用于加速预训练模型中的样本生成。经过实证和理论研究，提出了一种最佳性能的混合方法，能够应用于扩散生成模型。 |
| [^26] | [A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks.](http://arxiv.org/abs/2310.07891) | 这篇论文提出了一种关于两层神经网络中非线性特征学习的理论。通过一步梯度下降训练的过程中引入不同的多项式特征，该方法能够学习到目标函数的非线性组件，而更新的神经网络的性能则由这些特征所决定。 |
| [^27] | [On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism.](http://arxiv.org/abs/2310.07852) | 本文研究了在高维稀疏线性回归模型中的差分隐私模型选择问题。我们使用指数机制进行模型选择，并提出了Metropolis-Hastings算法来克服指数搜索空间的计算复杂性。我们的算法在一定边界条件下能够实现强模型恢复性质，并具有多项式混合时间和近似差分隐私性质。 |
| [^28] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^29] | [When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement.](http://arxiv.org/abs/2310.07831) | 该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。 |
| [^30] | [Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore.](http://arxiv.org/abs/2310.07811) | 该论文研究了在线强化学习中在线性$q^\pi$可实现的MDPs和线性MDPs的差异，并提出了一种新颖的学习算法，可以通过学习忽略某些状态将问题转化为线性MDP。 |
| [^31] | [Feature Learning and Generalization in Deep Networks with Orthogonal Weights.](http://arxiv.org/abs/2310.07765) | 我们通过使用正交矩阵集合初始化权重并使用tanh激活函数，解决了全连接深度神经网络在初始化中具有与深度无关的线性波动问题。此外，我们发现神经切向核（NTK）及其后代的所有相关函数在逆宽度的主导阶段在深度约为20的位置饱和，而不是不断增长。 |
| [^32] | [NECO: NEural Collapse Based Out-of-distribution detection.](http://arxiv.org/abs/2310.06823) | NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。 |
| [^33] | [Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts.](http://arxiv.org/abs/2310.05898) | Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。 |
| [^34] | [Clustering Three-Way Data with Outliers.](http://arxiv.org/abs/2310.05288) | 这项研究提出了一种用于聚类矩阵形式数据的方法，可以处理其中的异常值。 |
| [^35] | [Memorization with neural nets: going beyond the worst case.](http://arxiv.org/abs/2310.00327) | 本文研究了神经网络的插值问题，提出了一种简单的随机算法，在给定的数据集和两个类的情况下，能够以很高的概率构建一个插值的神经网络。这些结果与训练数据规模无关。 |
| [^36] | [$L^1$ Estimation: On the Optimality of Linear Estimators.](http://arxiv.org/abs/2309.09129) | 该论文研究了在$L^1$保真度条件下，从噪声观测中估计随机变量$X$的问题。结果表明，唯一能够引入线性条件中位数的先验分布是高斯分布。此外，还研究了其他$L^p$损失，并观察到对于$p \in [1,2]$，高斯分布是唯一引入线性最优贝叶斯估计器的先验分布。扩展还涵盖了特定指数族条件分布的噪声模型。 |
| [^37] | [On Regularized Sparse Logistic Regression.](http://arxiv.org/abs/2309.05925) | 本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。 |
| [^38] | [Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration.](http://arxiv.org/abs/2306.14041) | 分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。 |
| [^39] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^40] | [Generalization bounds for neural ordinary differential equations and deep residual networks.](http://arxiv.org/abs/2305.06648) | 本研究提出了神经常微分方程及其变体的泛化界限，涵盖了深度残差网络，其泛化界限与连续权重差异大小有关。 |
| [^41] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^42] | [Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit.](http://arxiv.org/abs/2304.09663) | 本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。 |
| [^43] | [An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response.](http://arxiv.org/abs/2303.17823) | 本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。 |
| [^44] | [A Complete Recipe for Diffusion Generative Models.](http://arxiv.org/abs/2303.01748) | 本文提出了一种完整的扩散生成模型的配方，利用可扩展的贝叶斯后验采样器的见解，确保收敛到所需的目标分布。在这个方法的基础上，引入了相空间Langevin扩散（PSLD），在扩展空间中进行基于评分的建模，展现出更优质的样本质量和改进的速度-质量权衡。 |
| [^45] | [Variable Selection for Kernel Two-Sample Tests.](http://arxiv.org/abs/2302.07415) | 本文提出了一种解决双样本检验中变量选择问题的框架，利用核最大均值差异统计量，以最大化方差正则化的MMD统计量。实验结果证明其超群表现。 |
| [^46] | [Efficient probabilistic reconciliation of forecasts for real-valued and count time series.](http://arxiv.org/abs/2210.02286) | 本论文提出了一种基于条件方法来调和任何类型的预测分布的新方法，并引入了一种名为“自下而上重要性抽样”的高效抽样算法。这种方法在多个时间层次的实验中显示出与基本概率预测相比的显著改进。 |
| [^47] | [A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting.](http://arxiv.org/abs/2207.14219) | 本文介绍了一种名为AEnbMIMOCQR的新颖算法，通过自适应集成的方式，在不需要数据拆分的情况下，以分布无关的方式生成多步鲍型预测区间。该方法考虑了异方差性，并对分布转变具有鲁棒性，在实验中表现优于其他竞争方法。 |
| [^48] | [Conditional Sig-Wasserstein GANs for Time Series Generation.](http://arxiv.org/abs/2006.05421) | 本论文提出了一种有条件的Sig-Wasserstein GANs框架，通过将WGANs与路径特征提取方法相结合，用于解决时间序列生成中的挑战。路径的签名作为一种通用描述数据流的统计特征，可以刻画时序模型的分布特征。 |
| [^49] | [L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes.](http://arxiv.org/abs/1908.04628) | L2P是一种利用实例之间成对关系进行学习的方法，用于解决具有重尾分布特征的预测任务。实验证明，L2P在准确度和能力方面优于竞争方法。 |

# 详细

[^1]: 通过密集对应关系从无动作视频中学习执行动作

    Learning to Act from Actionless Videos through Dense Correspondences. (arXiv:2310.08576v1 [cs.RO])

    [http://arxiv.org/abs/2310.08576](http://arxiv.org/abs/2310.08576)

    本研究提出了一种通过密集对应关系从无动作视频中学习执行动作的方法，能够在不使用任何动作注释的情况下构建可靠执行多样任务的基于视频的机器人策略。

    

    本研究提出了一种方法，通过少量视频演示构建一个基于视频的机器人策略，能够可靠地在不同机器人和环境下执行各种任务，而无需使用任何动作注释。我们的方法利用图像作为任务不可知的表示，编码状态和动作信息，并使用文本作为指定机器人目标的通用表示。通过合成“如幻般”执行动作的视频，并结合帧之间的密集对应关系，我们的方法可以推断出在环境中执行的闭合形式动作，而不需要任何显式的动作标签。这种独特的能力使我们能够仅基于RGB视频训练策略，并将学习到的策略部署到各种机器人任务中。我们在桌面操作和导航任务中展示了我们方法的有效性。此外，我们还贡献了一个高效的视频建模开源框架。

    In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling 
    
[^2]: 以Transformer为决策者：通过监督预训练实现可证明的上下文强化学习

    Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])

    [http://arxiv.org/abs/2310.08566](http://arxiv.org/abs/2310.08566)

    通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。

    

    在离线强化学习数据集上预训练的大型Transformer模型展示了令人惊叹的上下文强化学习能力，即当它们面对来自未知环境的交互轨迹时，它们能够做出良好的决策。然而，Transformer模型如何进行训练以执行上下文强化学习，在理论上尚未得到很好的理解。特别是，尚不清楚Transformer模型可以在上下文中执行哪些强化学习算法以及离线训练数据中的分布差异如何影响已学习的算法。本文提供了一个理论框架，分析了对上下文强化学习的监督预训练。这包括了两种最近提出的训练方法：算法蒸馏和决策预训练的Transformer模型。首先，在假设模型可实现的情况下，我们证明了经过监督预训练的Transformer模型将模仿专家算法在观察到的轨迹上的条件期望。广义误差的缩放范围与…

    Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
    
[^3]: 利用回声状态网络对气候路径进行特征重要性刻画

    Characterizing climate pathways using feature importance on echo state networks. (arXiv:2310.08495v1 [stat.ML])

    [http://arxiv.org/abs/2310.08495](http://arxiv.org/abs/2310.08495)

    本文探索了使用回声状态网络（ESN）来刻画气候路径的方法，并使用特征重要性分析来了解各个变量对气候路径的影响。

    

    美国2022年国防战略将气候变化列为对国家安全的严重威胁。已经提出了诸如平流层气溶胶喷射等气候干预方法作为缓解策略，但此类行动对复杂气候系统的下游影响尚不明确。开发量化与气候事件相关的源和影响变量（即气候路径）之间关系的算法技术将有助于决策。数据驱动的深度学习模型已成为建模高度非线性关系的强大工具，可能为刻画气候变量关系提供一种途径。本文探索了使用回声状态网络（ESN）对气候路径进行刻画的方法。ESN是一种计算高效的神经网络变体，专为处理时间数据而设计，最近的研究提出ESN可以用作预测时空气候数据的有用工具。与其他模型（如LSTM）相比，ESN具有更快的训练速度，更少的参数和更好的泛化能力。我们使用特征重要性分析，以了解各个变量对气候路径的影响。

    The 2022 National Defense Strategy of the United States listed climate change as a serious threat to national security. Climate intervention methods, such as stratospheric aerosol injection, have been proposed as mitigation strategies, but the downstream effects of such actions on a complex climate system are not well understood. The development of algorithmic techniques for quantifying relationships between source and impact variables related to a climate event (i.e., a climate pathway) would help inform policy decisions. Data-driven deep learning models have become powerful tools for modeling highly nonlinear relationships and may provide a route to characterize climate variable relationships. In this paper, we explore the use of an echo state network (ESN) for characterizing climate pathways. ESNs are a computationally efficient neural network variation designed for temporal data, and recent work proposes ESNs as a useful tool for forecasting spatio-temporal climate data. Like other
    
[^4]: 个性化动态超级学习：在预测血液透析的对流容积中的应用

    Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes. (arXiv:2310.08479v1 [stat.ME])

    [http://arxiv.org/abs/2310.08479](http://arxiv.org/abs/2310.08479)

    该论文介绍了一种个性化动态超级学习算法 (POSL)，可以实现实时更新的预测。作者通过预测血液透析患者的对流容积，展示了POSL在中位绝对误差、大型校准、区分度和净收益方面的优越表现。

    

    实时更新的预测是个性化医疗的一个重大挑战。通过结合参数回归和机器学习方法，个性化在线超级学习算法（POSL）可以实现动态和个性化的预测。我们将POSL应用于动态预测重复连续结果，并提出了一种新的验证个性化或动态预测模型的方法。我们通过预测进行血液透析的患者的对流容积来展示其性能。POSL在中位绝对误差、大型校准、区分度和净收益方面的表现优于其候选学习器。最后，我们讨论了使用POSL的选择和挑战。

    Obtaining continuously updated predictions is a major challenge for personalised medicine. Leveraging combinations of parametric regressions and machine learning approaches, the personalised online super learner (POSL) can achieve such dynamic and personalised predictions. We adapt POSL to predict a repeated continuous outcome dynamically and propose a new way to validate such personalised or dynamic prediction models. We illustrate its performance by predicting the convection volume of patients undergoing hemodiafiltration. POSL outperformed its candidate learners with respect to median absolute error, calibration-in-the-large, discrimination, and net benefit. We finally discuss the choices and challenges underlying the use of POSL.
    
[^5]: Heterogeneity in Integration and Prediction (HIP)的扩展与R Shiny应用

    Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application. (arXiv:2310.08426v1 [stat.ME])

    [http://arxiv.org/abs/2310.08426](http://arxiv.org/abs/2310.08426)

    本文描述了Heterogeneity in Integration and Prediction (HIP)的扩展，并提供了一个R Shiny应用界面，能够适应多种结果类型，同时保留了HIP的优势。

    

    多个数据视图在同一组参与者上的测量越来越常见，并且通过同时分析这些不同视图，有可能加深我们对许多复杂疾病的理解。同样重要的是，许多复杂疾病显示出亚组异质性（例如性别或种族）。HIP（Heterogeneity in Integration and Prediction）是最早提出的一种方法，用于集成多个数据视图，同时考虑亚组异质性，以确定特定疾病的共有和亚组特异性标记物。然而，HIP适用于连续结果，并且需要用户具备编程专长。在本文中，我们提出了HIP的扩展，以适应多类别、泊松分布和零膨胀泊松分布的结果，同时保留HIP的优势。此外，我们还介绍了一个R Shiny应用，可以通过https://multi-viewlearn.shinyapps.io/HIP_ShinyApp/访问，它提供了与Python实现的接口。

    Multiple data views measured on the same set of participants is becoming more common and has the potential to deepen our understanding of many complex diseases by analyzing these different views simultaneously. Equally important, many of these complex diseases show evidence of subgroup heterogeneity (e.g., by sex or race). HIP (Heterogeneity in Integration and Prediction) is among the first methods proposed to integrate multiple data views while also accounting for subgroup heterogeneity to identify common and subgroup-specific markers of a particular disease. However, HIP is applicable to continuous outcomes and requires programming expertise by the user. Here we propose extensions to HIP that accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while retaining the benefits of HIP. Additionally, we introduce an R Shiny application, accessible on shinyapps.io at https://multi-viewlearn.shinyapps.io/HIP_ShinyApp/, that provides an interface with the Python implementation
    
[^6]: 多层神经网络的差分隐私非凸学习

    Differentially Private Non-convex Learning for Multi-layer Neural Networks. (arXiv:2310.08425v1 [cs.LG])

    [http://arxiv.org/abs/2310.08425](http://arxiv.org/abs/2310.08425)

    本文研究了具有差分隐私的非凸学习问题在多层神经网络中的应用。通过提出新算法，实现了在数据维度不变的情况下达到过度群体风险。同时，对比了不同链接函数和不同模型设定的结果。

    

    本文关注具有单输出节点的（多层）全连接神经网络的差分隐私随机优化问题。首先, 我们研究了没有隐藏节点的情况，具体关注广义线性模型（GLMs）。我们研究了随机噪声具有零均值且链接函数既有界又Lipschitz连续的特定模型。我们提出了几种算法，并分析证明了在数据维度不变的情况下实现过度群体风险的可行性。我们还探讨了涉及ReLU链接函数的场景，发现与有界链接函数的结果相似。通过使用ReLU回归作为代表性示例, 对比了特定和不正确指定的模型。在论文的第二部分，我们将这些理念扩展到具有Sigmoid或ReLU激活函数的两层神经网络中的特定模型。

    This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.  In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. I
    
[^7]: ChatGPT生成的医学回复的评估：一项系统评述和荟萃分析

    Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis. (arXiv:2310.08410v1 [stat.ME])

    [http://arxiv.org/abs/2310.08410](http://arxiv.org/abs/2310.08410)

    这项研究总结了关于评估ChatGPT在医学中性能的现有证据，并发现ChatGPT在处理医学查询时的准确率为56%。然而，由于缺乏评估标准，研究中存在方法上的不一致和详细信息不全的问题。

    

    越来越多地在医学领域探索使用ChatGPT等大型语言模型。然而，缺乏性能评估的标准指南导致了方法上的不一致。本研究旨在总结关于评估ChatGPT在医学中性能的现有证据，并为未来的研究指明方向。我们在2023年6月15日使用关键词“ChatGPT”在十个医学文献数据库中进行搜索。共鉴定了3520篇文章，其中60篇在本文中进行了回顾和总结，17篇进行了荟萃分析。分析结果显示，ChatGPT在处理医学查询时的整体综合准确率为56%（95% CI：51%-60%，I2 = 87%）。然而，这些研究在问题资源、提问过程和评估指标上存在差异。此外，许多研究未能报告方法细节，包括ChatGPT的版本以及每个问题是独立使用还是重复使用。我们的研究结果表明，...

    Large language models such as ChatGPT are increasingly explored in medical domains. However, the absence of standard guidelines for performance evaluation has led to methodological inconsistencies. This study aims to summarize the available evidence on evaluating ChatGPT's performance in medicine and provide direction for future research. We searched ten medical literature databases on June 15, 2023, using the keyword "ChatGPT". A total of 3520 articles were identified, of which 60 were reviewed and summarized in this paper and 17 were included in the meta-analysis. The analysis showed that ChatGPT displayed an overall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing medical queries. However, the studies varied in question resource, question-asking process, and evaluation metrics. Moreover, many studies failed to report methodological details, including the version of ChatGPT and whether each question was used independently or repeatedly. Our findings revealed that 
    
[^8]: 多少个预训练任务需要用于线性回归的上下文学习？

    How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?. (arXiv:2310.08391v1 [stat.ML])

    [http://arxiv.org/abs/2310.08391](http://arxiv.org/abs/2310.08391)

    本文研究了在线性回归中的上下文学习，并发现有效的预训练只需要少量独立任务，预训练模型与贝叶斯最优算法接近。这些理论发现对ICL的统计基础提供了启示。

    

    在多样任务上进行预训练的Transformer展现了非凡的上下文学习（ICL）能力，使其能够仅基于输入上下文解决未见任务，而无需调整模型参数。本文研究了其中最简单设置的ICL：预训练线性参数化的单层线性注意力模型，用于具有高斯先验的线性回归。我们为注意力模型预训练建立了一个统计任务复杂度界，表明有效的预训练只需要少量独立任务。此外，我们证明了预训练模型与贝叶斯最优算法非常接近，即几乎实现了固定上下文长度下未见任务的贝叶斯最优风险。这些理论发现对之前的实验研究进行了补充，并为ICL的统计基础提供了启示。

    Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.
    
[^9]: 多臂赌博策略对深度循环强化学习的影响

    Impact of multi-armed bandit strategies on deep recurrent reinforcement learning. (arXiv:2310.08331v1 [stat.ML])

    [http://arxiv.org/abs/2310.08331](http://arxiv.org/abs/2310.08331)

    本研究在自动驾驶场景中使用部分可观测系统，通过部署和测试多种技术来平衡探索和利用的权衡，以预测方向盘操作。

    

    对环境的不完全了解导致智能体在不确定性下做出决策。强化学习中一个重要的困境是，在做出决策时，智能体需要在利用当前环境知识最大化累积奖励和探索行动以提高环境知识的之间进行权衡（探索-利用的平衡）。同时，另一个相关问题是状态的完全可观测性，不是所有应用都能假定。例如，当只将2D图像作为输入用于在3D模拟环境中找到最佳行动时，就存在这个问题。在本研究中，我们通过部署和测试多种技术来解决部分可观测系统中探索和利用的平衡问题，以预测自动驾驶场景中的方向盘操作。

    Incomplete knowledge of the environment leads an agent to make decisions under uncertainty. One of the major dilemmas in Reinforcement Learning (RL) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). Concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. Such as when only 2D images are considered as input in a RL approach used for finding the optimal action within a 3D simulation environment. In this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenario. More precisel
    
[^10]: 对贝叶斯神经网络后验的对称感知探索

    A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors. (arXiv:2310.08287v1 [stat.ML])

    [http://arxiv.org/abs/2310.08287](http://arxiv.org/abs/2310.08287)

    本文对贝叶斯神经网络后验进行了大规模探索，研究了逼近后验的最佳方法和后验质量与不确定性量化的关系，同时发现了权重空间对称性对后验的影响。

    

    现代深度神经网络(DNNs)的权重分布对不确定性量化和鲁棒性非常重要，由于其极高的维度，其本质非常复杂。本文提出了对深度贝叶斯神经网络(BNNs)后验分布的首次大规模探索，将其研究扩展到现实世界的视觉任务和架构。具体而言，我们研究了逼近后验的最佳方法，分析了后验质量和不确定性量化之间的关系，深入研究了后验中模式的影响，并探索了可视化后验的方法。此外，我们发现权重空间的对称性是理解后验的关键方面。为此，我们对置换和缩放对称性的影响进行了深入评估，这些对称性往往会使贝叶斯后验变得模糊。尽管第一种变换已知会复制模式，但我们还是对其进行了探索。

    The distribution of the weights of modern deep neural networks (DNNs) crucial for uncertainty quantification and robustness - is an eminently complex object due to its extremely high dimensionality. This paper proposes one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding its study to real-world vision tasks and architectures. Specifically, we investigate the optimal approach for approximating the posterior, analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior. Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior. To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior. While the first type of transformation is known for duplicating modes, we explore t
    
[^11]: 在协变量漂移下基于核方法的统一分析

    Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])

    [http://arxiv.org/abs/2310.08237](http://arxiv.org/abs/2310.08237)

    该论文提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法，通过建立收敛速度为一般损失函数提供了统一的理论分析。

    

    在实际应用中，协变量漂移是普遍存在的，即源数据和目标数据的输入分布存在显著差异。尽管在各种学习问题中具有实际重要性，但现有的大多数方法只关注于一些特定的学习任务，并没有在理论上和数值上得到很好的验证。为了解决这个问题，我们提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法。我们的理论结果适用于属于一个丰富的损失函数家族的一般损失，其中包括许多常用的方法，如均值回归、分位数回归、基于似然的分类和基于边缘的分类。本文重点研究了两类协变量漂移问题，并为一般损失函数建立了尖锐的收敛速度以提供一个统一的理论分析，该结果与文献中的最优结果一致。

    Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
    
[^12]: 在黎曼流形上进行回归的一致推断

    Conformal inference for regression on Riemannian Manifolds. (arXiv:2310.08209v1 [stat.ML])

    [http://arxiv.org/abs/2310.08209](http://arxiv.org/abs/2310.08209)

    本文研究了在黎曼流形上进行回归场景的预测集，并证明了这些区域的经验版本在大样本下的收敛性。

    

    在流形上进行回归，以及更广泛地说，对流形上的统计学有了重要的关注，因为这种类型的数据有大量的应用。圆形数据是一个经典示例，但协方差矩阵空间上的数据、主成分分析得到的Grassmann流形上的数据等也是如此。在本文中，我们研究了当响应变量$Y$位于流形上，而协变量$X$位于欧几里德空间时，回归场景的预测集。这扩展了[Lei and Wasserman, 2014]中在这一新领域中概述的概念。与一致推断中的传统原则一致，这些预测集是无分布的，表明对$(X, Y)$的联合分布没有施加特定的假设，而且它们保持非参数性质。我们证明了这些区域的经验版本在几乎必然收敛于无穷大时的收敛性。

    Regression on manifolds, and, more broadly, statistics on manifolds, has garnered significant importance in recent years due to the vast number of applications for this type of data. Circular data is a classic example, but so is data in the space of covariance matrices, data on the Grassmannian manifold obtained as a result of principal component analysis, among many others. In this work we investigate prediction sets for regression scenarios when the response variable, denoted by $Y$, resides in a manifold, and the covariable, denoted by X, lies in Euclidean space. This extends the concepts delineated in [Lei and Wasserman, 2014] to this novel context. Aligning with traditional principles in conformal inference, these prediction sets are distribution-free, indicating that no specific assumptions are imposed on the joint distribution of $(X, Y)$, and they maintain a non-parametric character. We prove the asymptotic almost sure convergence of the empirical version of these regions on th
    
[^13]: 在高维空间中研究投影样本协方差的极值渐近性，并在金融和卷积网络中应用

    On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks. (arXiv:2310.08150v1 [math.ST])

    [http://arxiv.org/abs/2310.08150](http://arxiv.org/abs/2310.08150)

    本文研究了高维向量时间序列样本协方差矩阵的函数的最大型统计量，推广了样本自协方差函数的最大偏差情况，证明了Gumbel型极值渐近性成立，并将其应用于金融和卷积网络领域。

    

    本文研究了高维向量时间序列样本协方差矩阵的某些函数的最大型统计量，以统计地确认或拒绝数据集在正常条件下被收集的零假设。本方法推广了样本自协方差函数的最大偏差情况。在线性时间序列框架下，证明了Gumbel型极值渐近性成立。作为应用，我们讨论了仅做多的最小风险组合优化、由稀疏跟踪组合进行ETF指数跟踪、用于图像分析的卷积深度学习和阵列传感器数据的分析。

    Maximum-type statistics of certain functions of the sample covariance matrix of high-dimensional vector time series are studied to statistically confirm or reject the null hypothesis that a data set has been collected under normal conditions. The approach generalizes the case of the maximal deviation of the sample autocovariances function from its assumed values. Within a linear time series framework it is shown that Gumbel-type extreme value asymptotics holds true. As applications we discuss long-only mimimal-variance portfolio optimization and subportfolio analysis with respect to idiosyncratic risks, ETF index tracking by sparse tracking portfolios, convolutional deep learners for image analysis and the analysis of array-of-sensors data.
    
[^14]: 模型不可知的辅助推断方法在部分可辨识因果效应上的应用

    Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects. (arXiv:2310.08115v1 [econ.EM])

    [http://arxiv.org/abs/2310.08115](http://arxiv.org/abs/2310.08115)

    提出了一种模型不可知的推断方法，在部分可辨识的因果估计中应用广泛。该方法基于最优输运问题的对偶理论，能够适应随机实验和观测研究，并且具有统一有效和双重鲁棒性。

    

    很多因果估计是部分可辨识的，因为它们依赖于潜在结果之间的不可观察联合分布。基于前处理协变量的分层可以获得更明确的部分可辨识性范围；然而，除非协变量为离散且支撑度相对较小，否则这种方法通常需要对给定协变量的潜在结果的条件分布进行一致估计。因此，现有的方法在模型错误或一致性假设被违反时可能失败。在本研究中，我们提出了一种基于最优输运问题的对偶理论的统一且模型不可知的推断方法，适用于广泛类别的部分可辨识估计。在随机实验中，我们的方法可以结合任何对条件分布的估计，并提供统一有效的推断，即使初始估计是任意不准确的。此外，我们的方法在观测研究中也是双重鲁棒的。

    Many causal estimands are only partially identifiable since they depend on the unobservable joint distribution between potential outcomes. Stratification on pretreatment covariates can yield sharper partial identification bounds; however, unless the covariates are discrete with relatively small support, this approach typically requires consistent estimation of the conditional distributions of the potential outcomes given the covariates. Thus, existing approaches may fail under model misspecification or if consistency assumptions are violated. In this study, we propose a unified and model-agnostic inferential approach for a wide class of partially identified estimands, based on duality theory for optimal transport problems. In randomized experiments, our approach can wrap around any estimates of the conditional distributions and provide uniformly valid inference, even if the initial estimates are arbitrarily inaccurate. Also, our approach is doubly robust in observational studies. Notab
    
[^15]: 学习正则化的单调图匹配场博弈

    Learning Regularized Monotone Graphon Mean-Field Games. (arXiv:2310.08089v1 [cs.GT])

    [http://arxiv.org/abs/2310.08089](http://arxiv.org/abs/2310.08089)

    本文研究了正则化图匹配场博弈的存在性和学习算法问题，在弱单调条件下提出了一种离散时间算法，并开发了动作值函数估计过程作为优化过程的子模块。

    

    本文研究了正则化图匹配场博弈（GMFG）中的两个基本问题。首先，我们建立了任意λ正则化GMFG（对于λ≥0）的纳什均衡存在性。这个结果所依赖的条件比以前研究非正则化GMFG（λ=0）和λ-正则化MFG的条件要弱。第二，我们提出了一种证明有效的算法来学习弱单调GMFG的纳什均衡，受到Lasry和Lions [2007]的启发。以前的文献要么只分析连续时间算法，要么需要额外的条件来分析离散时间算法。相反，我们设计了一种离散时间算法，并在弱单调条件下推导了其收敛速度。此外，我们在在线学习过程中开发和分析了动作值函数估计过程，这在单调GMFG的算法中是缺失的。这在我们的优化过程中起到了子模块的作用。

    This paper studies two fundamental problems in regularized Graphon Mean-Field Games (GMFGs). First, we establish the existence of a Nash Equilibrium (NE) of any $\lambda$-regularized GMFG (for $\lambda\geq 0$). This result relies on weaker conditions than those in previous works for analyzing both unregularized GMFGs ($\lambda=0$) and $\lambda$-regularized MFGs, which are special cases of GMFGs. Second, we propose provably efficient algorithms to learn the NE in weakly monotone GMFGs, motivated by Lasry and Lions [2007]. Previous literature either only analyzed continuous-time algorithms or required extra conditions to analyze discrete-time algorithms. In contrast, we design a discrete-time algorithm and derive its convergence rate solely under weakly monotone conditions. Furthermore, we develop and analyze the action-value function estimation procedure during the online learning process, which is absent from algorithms for monotone GMFGs. This serves as a sub-module in our optimizatio
    
[^16]: 用于Raman和CARS光谱学中贝叶斯神经网络的对数-高斯γ过程的训练方法

    Log-Gaussian Gamma Processes for Training Bayesian Neural Networks in Raman and CARS Spectroscopies. (arXiv:2310.08055v1 [stat.AP])

    [http://arxiv.org/abs/2310.08055](http://arxiv.org/abs/2310.08055)

    本论文提出了一种利用gamma分布和log-Gaussian建模的方法，用于生成合成数据集以训练神经网络，解决了实际观测数据有限的挑战。通过应用于Raman和CARS光谱，同时训练两个贝叶斯神经网络来估计gamma过程的参数，可以估计基础的光谱并提供不确定性。

    

    我们提出了一种利用gamma分布的随机变量和log-Gaussian建模的方法，用于生成适合训练神经网络的合成数据集。这种方法解决了各种应用中实际观测数据有限的挑战。我们将此方法应用于Raman和相干防-斯托克斯拉曼散射(CARS)光谱中，使用实验光谱估计gamma过程的参数。参数估计使用马尔可夫链蒙特卡洛方法进行，从而为模型提供完整贝叶斯后验分布，可用于合成数据生成。此外，我们使用高斯过程对Raman和CARS的加性和乘性背景函数进行建模。我们训练了两个贝叶斯神经网络来估计gamma过程的参数，然后可以用这些参数来估计基础的Raman光谱，并通过概率分布参数的估计同时提供不确定性。

    We propose an approach utilizing gamma-distributed random variables, coupled with log-Gaussian modeling, to generate synthetic datasets suitable for training neural networks. This addresses the challenge of limited real observations in various applications. We apply this methodology to both Raman and coherent anti-Stokes Raman scattering (CARS) spectra, using experimental spectra to estimate gamma process parameters. Parameter estimation is performed using Markov chain Monte Carlo methods, yielding a full Bayesian posterior distribution for the model which can be sampled for synthetic data generation. Additionally, we model the additive and multiplicative background functions for Raman and CARS with Gaussian processes. We train two Bayesian neural networks to estimate parameters of the gamma process which can then be used to estimate the underlying Raman spectrum and simultaneously provide uncertainty through the estimation of parameters of a probability distribution. We apply the trai
    
[^17]: 使用学习的最优核进行格子实时模拟

    Lattice real-time simulations with learned optimal kernels. (arXiv:2310.08053v1 [hep-lat])

    [http://arxiv.org/abs/2310.08053](http://arxiv.org/abs/2310.08053)

    本论文提出了一种使用学习的最优核进行格子实时模拟的方法，在复杂朗之万方法的基础上添加先验信息，成功克服了符号问题，并扩展了实时模拟的范围，并避免了离散化问题。

    

    我们提出了一种受强化学习启发的量子场实时动力学模拟策略。它在复杂朗之万方法基础上添加了系统特异的先验信息，这是克服严重的符号问题的必要前提。我们的机器学习方法的优化过程是通过使用本质上稳定的复杂朗之万随机过程求解器和从边界项的洞察中导出的新颖最优性准则实现的。这种概念和技术进步使我们能够显著扩展1+1维标量场理论中实时模拟的范围，超过现有水平，并避免了之前实时场论模拟中的离散化问题。我们讨论了该方法的局限性和有前景的未来方向。

    We present a simulation strategy for the real-time dynamics of quantum fields, inspired by reinforcement learning. It builds on the complex Langevin approach, which it amends with system specific prior information, a necessary prerequisite to overcome this exceptionally severe sign problem. The optimization process underlying our machine learning approach is made possible by deploying inherently stable solvers of the complex Langevin stochastic process and a novel optimality criterion derived from insight into so-called boundary terms. This conceptual and technical progress allows us to both significantly extend the range of real-time simulations in 1+1d scalar field theory beyond the state-of-the-art and to avoid discretization artifacts that plagued previous real-time field theory simulations. Limitations of and promising future directions are discussed.
    
[^18]: 带有噪声标签的局部图聚类

    Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])

    [http://arxiv.org/abs/2310.08031](http://arxiv.org/abs/2310.08031)

    本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。

    

    在机器学习问题中，对于带有额外节点信息（如文本、图像或标签）的图形的增加兴趣，促使了需要耗费大量资源处理整个图形的方法的流行。然而，对于从这样的数据中提取有用信息的快速局部方法（即不需要访问整个图形）的发展还很少。为此，我们提出了使用噪声节点标签作为额外节点信息的局部图聚类的研究。在这种设置下，节点根据所属簇的联属关系接收初始二进制标签：如果它们属于目标簇，则为1；否则为0。随后，这些标签的一部分会被翻转。我们研究了将噪声标签纳入局部图聚类的好处。通过构建带有这些标签的加权图形，我们研究了基于图扩散的局部聚类方法在原始图形和加权图形上的性能。从理论角度出发，

    The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider
    
[^19]: 鲁棒的一比特压缩感知与迭代硬阈值化

    Robust 1-bit Compressed Sensing with Iterative Hard Thresholding. (arXiv:2310.08019v1 [cs.IT])

    [http://arxiv.org/abs/2310.08019](http://arxiv.org/abs/2310.08019)

    本文研究了鲁棒的一比特压缩感知问题，并提出了二元迭代硬阈值化（BIHT）算法，在噪声情况下比目前已知方法表现更好。

    

    在一比特压缩感知中，目标是从仅有正负符号量化的线性测量中恢复一个k-稀疏单位向量x，使其相对于最小二乘误差ϵ（在ℓ2范数下）内。本文研究了一种带噪声的情况，即部分测量值可能被对手翻转的情况。具体地，我们分析了一种名为二元迭代硬阈值化（Binary Iterative Hard Thresholding，BIHT）的算法，在这个噪声情况下进行一比特压缩感知。最近的研究结果表明，使用近似O(k/ϵ)个无噪声测量，BIHT算法可以提供一个ϵ误差范围内的估计。这个结果是最优的和通用的，意味着一组测量可以适用于所有稀疏向量。本文还展示了在噪声情况下，BIHT算法比目前已知的所有方法都提供更好的结果。

    In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector $x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of linear measurements that are quantized to just their signs, i.e., from measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this paper, we study a noisy version where a fraction of the measurements can be flipped, potentially by an adversary. In particular, we analyze the Binary Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a properly defined loss function used for 1-bit compressed sensing, in this noisy setting. It is known from recent results that, with $\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an estimate within $\epsilon$ error. This result is optimal and universal, meaning one set of measurements work for all sparse vectors. In this paper, we show that BIHT also provides better results than all known methods for the noisy setting. We show that when up t
    
[^20]: LEMON：无损模型扩展

    LEMON: Lossless model expansion. (arXiv:2310.07999v1 [cs.LG])

    [http://arxiv.org/abs/2310.07999](http://arxiv.org/abs/2310.07999)

    LEMON是一种无损模型扩展方法，在深度神经网络中能够通过利用小型预训练模型的知识来初始化和训练大型模型，从而大大减少训练时间，同时具有通用性适用于各种网络结构。

    

    深度神经网络（特别是Transformer）的扩展对于它们的出色性能至关重要，并且进一步导致了基础模型中复杂的推理能力的出现。这种扩展通常需要从头开始训练大型模型，并使用随机初始化，而无法利用已有的小型模型所获得的知识，这些小型模型已经耗费了大量资源。为了解决这种低效率问题，我们提出了无损模型扩展（LEMON），一种使用小型但已经预训练的模型的权重来初始化扩展模型的方法。然后，我们使用专门为扩展模型定制的优化学习率调度器进行模型训练，与从头训练相比，大大减少了训练时间。值得注意的是，LEMON具有通用性，能够与各种网络结构兼容，包括Vision Transformer和BERT等模型。我们的实证结果证明了LEMON的效果。

    Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrat
    
[^21]: RandCom：去中心化随机通信跳跃方法用于分布式随机优化

    RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])

    [http://arxiv.org/abs/2310.07983](http://arxiv.org/abs/2310.07983)

    RandCom是一种去中心化的随机通信跳跃方法，能够在分布式优化中通过概率性本地更新减少通信开销，并在不同的设置中实现线性加速。

    

    具有随机通信跳过的分布式优化方法因其在加速通信复杂性方面具有的优势而受到越来越多的关注。然而，现有的研究主要集中在强凸确定性设置的集中式通信协议上。在本研究中，我们提出了一种名为RandCom的分布式优化方法，它采用了概率性的本地更新。我们分析了RandCom在随机非凸、凸和强凸设置中的性能，并证明了它能够通过通信概率来渐近地减少通信开销。此外，我们证明当节点数量增加时，RandCom能够实现线性加速。在随机强凸设置中，我们进一步证明了RandCom可以通过独立于网络的步长实现线性加速。此外，我们将RandCom应用于联邦学习，并提供了关于实现线性加速的潜力的积极结果。

    Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
    
[^22]: 对于选择最有可能从治疗中获益的人的统计性能保证

    Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment. (arXiv:2310.07973v1 [stat.ME])

    [http://arxiv.org/abs/2310.07973](http://arxiv.org/abs/2310.07973)

    本研究针对选择最有可能从治疗中获益的人的问题，在估计治疗效果和确定截断值时面临多重测试问题，提出一种统一的置信带方法来评估这些个体的平均治疗效果。

    

    在广泛的学科领域中，许多研究人员使用机器学习算法来识别一组被称为例外反应者的个体，他们最有可能从治疗中获益。一个常见的方法包括两个步骤。首先使用机器学习算法估计条件平均治疗效果或其代理。然后确定所得治疗优先顺序分数的截断值，以选择那些最有可能从治疗中获益的人。不幸的是，这些估计的治疗优先顺序分数往往存在偏差和噪声。此外，利用相同的数据既选择截断值又估计所选个体的平均治疗效果会遇到多重测试问题。为了解决这些挑战，我们开发了一个统一的置信带来实验性地评估那些治疗优先顺序分数至少与任何给定量化值相等的个体的排序平均治疗效果（GATES）。

    Across a wide array of disciplines, many researchers use machine learning (ML) algorithms to identify a subgroup of individuals, called exceptional responders, who are likely to be helped by a treatment the most. A common approach consists of two steps. One first estimates the conditional average treatment effect or its proxy using an ML algorithm. They then determine the cutoff of the resulting treatment prioritization score to select those predicted to benefit most from the treatment. Unfortunately, these estimated treatment prioritization scores are often biased and noisy. Furthermore, utilizing the same data to both choose a cutoff value and estimate the average treatment effect among the selected individuals suffer from a multiple testing problem. To address these challenges, we develop a uniform confidence band for experimentally evaluating the sorted average treatment effect (GATES) among the individuals whose treatment prioritization score is at least as high as any given quant
    
[^23]: 超参数自适应搜索用于代理优化：一种自调整方法

    Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach. (arXiv:2310.07970v1 [cs.LG])

    [http://arxiv.org/abs/2310.07970](http://arxiv.org/abs/2310.07970)

    这个论文研究了代理优化算法中超参数的影响，并提出了一种自适应搜索的代理优化方法（HASSO），该方法可以动态调整超参数而不需要额外的评估。该方法旨在提高代理优化算法的可访问性、效果和收敛速度。

    

    代理优化算法在优化昂贵的黑盒函数方面表现出了潜力。然而，它们的性能受与采样和代理拟合相关的超参数的影响很大，这对于它们的广泛应用构成挑战。我们研究了各种代理优化算法中超参数的影响，并提出了一种超参数自适应搜索的代理优化方法（HASSO）。HASSO不是一个超参数调整算法，而是一种通用的自调整代理优化算法，它在同时优化主要目标函数的过程中动态调整自己的超参数，而不需要额外的评估。其目标是提高代理优化算法对于从业者的可访问性、效果和收敛速度。我们的方法识别并修改了每个问题和代理优化方法特定的最有影响力的超参数，减少了手动调整的需求，同时不显著增加计算负担。实验结果演示了我们方法的有效性。

    Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demo
    
[^24]: 上下文化政策恢复：通过自适应模仿学习对医疗决策进行建模和解释

    Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])

    [http://arxiv.org/abs/2310.07918](http://arxiv.org/abs/2310.07918)

    本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。

    

    可解释的策略学习旨在从观察到的行为中估计可理解的决策策略；然而，现有模型在准确性和可解释性之间存在权衡。这种权衡限制了基于数据驱动的对人类决策过程的解释，例如，审计医疗决策的偏见和次优实践，我们需要决策过程的模型，能够提供复杂行为的简洁描述。现有方法基本上由于将潜在决策过程表示为通用策略而负担了这种权衡，而实际上人类决策是动态的，可以随上下文信息而大幅改变。因此，我们提出了上下文化政策恢复（CPR），将建模复杂决策过程的问题重新定义为多任务学习问题，其中复杂决策策略由特定上下文的策略组成。CPR将每个上下文特定策略建模为线性的观察-动作映射

    Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
    
[^25]: 高效扩散生成模型的积分器

    Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])

    [http://arxiv.org/abs/2310.07894](http://arxiv.org/abs/2310.07894)

    本文提出了共轭积分器和分裂积分器两种框架，用于加速预训练模型中的样本生成。经过实证和理论研究，提出了一种最佳性能的混合方法，能够应用于扩散生成模型。

    

    扩散模型在推理时间生成样本速度较慢。因此，为更广泛的扩散模型开发快速确定性/随机采样的原则性框架是一个有希望的方向。我们提出了两种互补的加速预训练模型样本生成的框架：共轭积分器和分裂积分器。共轭积分器将DDIM泛化，将反向扩散动力学映射到更易于采样的空间。相反，基于分裂的积分器，常用于分子动力学，通过巧妙地在涉及数据和辅助变量的数值更新之间交替，减少了数值模拟误差。经过广泛的实证和理论研究，我们提出了一种混合方法，该方法在增强空间中获得了报告的扩散模型的最佳性能。我们在CIFAR-10上应用相空间朗之万扩散[Pandey＆Mandt，2023]，我们的确定性和随机样本生成获得了最好的性能。

    Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samp
    
[^26]: 两层神经网络中一次梯度下降的非线性特征学习理论

    A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])

    [http://arxiv.org/abs/2310.07891](http://arxiv.org/abs/2310.07891)

    这篇论文提出了一种关于两层神经网络中非线性特征学习的理论。通过一步梯度下降训练的过程中引入不同的多项式特征，该方法能够学习到目标函数的非线性组件，而更新的神经网络的性能则由这些特征所决定。

    

    特征学习被认为是深度神经网络成功的基本原因之一。在特定条件下已经严格证明，在两层全连接神经网络中，第一层进行一步梯度下降，然后在第二层进行岭回归可以导致特征学习；特征矩阵的谱中会出现分离的一维组件，称为“spike”。然而，使用固定梯度下降步长时，这个“spike”仅提供了目标函数的线性组件的信息，因此学习非线性组件是不可能的。我们展示了当学习率随样本大小增长时，这样的训练实际上引入了多个一维组件，每个组件对应一个特定的多项式特征。我们进一步证明了更新的神经网络的极限大维度和大样本训练和测试误差完全由这些“spike”所决定。

    Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
    
[^27]: 关于通过指数机制进行高维私有模型选择的计算复杂性

    On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])

    [http://arxiv.org/abs/2310.07852](http://arxiv.org/abs/2310.07852)

    本文研究了在高维稀疏线性回归模型中的差分隐私模型选择问题。我们使用指数机制进行模型选择，并提出了Metropolis-Hastings算法来克服指数搜索空间的计算复杂性。我们的算法在一定边界条件下能够实现强模型恢复性质，并具有多项式混合时间和近似差分隐私性质。

    

    在差分隐私框架下，我们考虑了高维稀疏线性回归模型中的模型选择问题。具体而言，我们考虑了差分隐私最佳子集选择的问题，并研究了其效用保证。我们采用了广为人知的指数机制来选择最佳模型，并在一定边界条件下，建立了其强模型恢复性质。然而，指数机制的指数搜索空间导致了严重的计算瓶颈。为了克服这个挑战，我们提出了Metropolis-Hastings算法来进行采样步骤，并在问题参数$n$、$p$和$s$中建立了其到稳态分布的多项式混合时间。此外，我们还利用其混合性质建立了Metropolis-Hastings随机行走的最终估计的近似差分隐私性质。最后，我们还进行了一些说明性模拟，印证了我们主要结果的理论发现。

    We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
    
[^28]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^29]: 何时，为什么以及多少？通过细化进行的自适应学习率调度

    When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])

    [http://arxiv.org/abs/2310.07831](http://arxiv.org/abs/2310.07831)

    该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。

    

    实践中使用的学习率调度与理论推荐的几乎完全不同。我们缩小了大部分理论与实践之间的差距，并因此能够推导出新的问题自适应学习率调度。我们的关键技术贡献是对广泛类别的优化算法（包括SGD）的学习率调度进行细化分析。与大多数前期研究只研究平均迭代的收敛性不同，我们研究最后一次迭代，这是大多数人在实践中使用的。当仅考虑最坏情况分析时，我们的理论预测最佳选择是线性衰减调度：这是一种实践中常用的选择，其将步长与当前迭代次数t和总步数T成比例地设置为1 - t/T。为了超越这种最坏情况分析，我们使用观察到的梯度范数来推导适应于特定任务的细化调度。这些细化调度表现出学习率逐渐增加和学习率迅速退火。

    Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
    
[^30]: 在线RL在线性$q^\pi$可实现的MDPs中和线性MDPs一样容易，只要你学会忽略。 (arXiv:2310.07811v1 [cs.LG])

    Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore. (arXiv:2310.07811v1 [cs.LG])

    [http://arxiv.org/abs/2310.07811](http://arxiv.org/abs/2310.07811)

    该论文研究了在线强化学习中在线性$q^\pi$可实现的MDPs和线性MDPs的差异，并提出了一种新颖的学习算法，可以通过学习忽略某些状态将问题转化为线性MDP。

    

    我们考虑在线强化学习（RL）在离散的马尔可夫决策过程（MDPs）中，在线性$q^\pi$可实现的假设下，假设所有策略的动作值可以表示为状态-动作特征的线性函数。这个类别被认为比线性MDPs更一般化，其中转移内核和奖励函数被假设为特征向量的线性函数。我们的第一个贡献是展示了这两个类别之间的差异是在线性$q^\pi$可实现的MDPs中存在一些状态，在这些状态中，对于任何策略，所有的动作值都近似相等，通过跳过这些状态并按照任意固定策略在这些状态中进行转换，我们可以将问题转化为线性MDP。基于这个观察，我们推导了一种新颖（计算效率较低）的学习算法，用于在线性$q^\pi$可实现的MDPs中，该算法同时学习了应该跳过的状态，并运行另一个学习算法。

    We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorith
    
[^31]: 具有正交权重的深度网络中的特征学习与泛化

    Feature Learning and Generalization in Deep Networks with Orthogonal Weights. (arXiv:2310.07765v1 [cs.LG])

    [http://arxiv.org/abs/2310.07765](http://arxiv.org/abs/2310.07765)

    我们通过使用正交矩阵集合初始化权重并使用tanh激活函数，解决了全连接深度神经网络在初始化中具有与深度无关的线性波动问题。此外，我们发现神经切向核（NTK）及其后代的所有相关函数在逆宽度的主导阶段在深度约为20的位置饱和，而不是不断增长。

    

    通过使用从正交矩阵集合初始化的权重和tanh激活函数，我们展示了全连接深度神经网络在初始化时具有与宽度无关的前激活波动，这是通过计算证明的。此外，我们通过数值实验证明，在初始化时，涉及神经切向核（NTK）及其后代的所有相关函数在逆宽度的主导阶段饱和在深度约为20的位置，而不是像高斯初始化的情况那样不断增长。

    Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We spec
    
[^32]: NECO: 基于神经坍塌的超出分布检测

    NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])

    [http://arxiv.org/abs/2310.06823](http://arxiv.org/abs/2310.06823)

    NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。

    

    由于模型过于自信并且没有意识到其认识论限制，检测超出分布（OOD）数据是机器学习中的一个重要挑战。我们假设“神经坍塌”，一种影响超出分布数据的现象，也会影响超出分布数据。为了从这种相互作用中受益，我们引入了NECO，一种用于OOD检测的新颖的事后方法，它利用“神经坍塌”和主成分空间的几何属性来识别OOD数据。我们的大量实验表明，NECO在小规模和大规模OOD检测任务上取得了最先进的结果，同时在不同的网络架构上展示了强大的泛化能力。此外，我们还对我们的方法在OOD检测中的有效性提供了理论解释。我们计划在匿名期结束后发布代码。

    Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
    
[^33]: 狮子秘密地解决受限制优化问题：正如李雅普诺夫所预测的。

    Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])

    [http://arxiv.org/abs/2310.05898](http://arxiv.org/abs/2310.05898)

    Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。

    

    通过程序搜索发现的新优化器Lion（进化的符号动量）在训练大型AI模型方面显示出有希望的结果。它在训练效果上与AdamW相当或更好，并具有更高的内存效率。正如我们可以从随机搜索程序的结果中期待的，Lion集成了几个现有算法的元素，包括符号动量、独立的权重衰减、Polak和Nesterov动量，但又不属于任何现有的理论基础优化器类别。因此，尽管Lion作为广泛任务的通用优化器表现良好，但其理论基础仍然不明确。这种缺乏理论的明确性限制了进一步增强和扩展Lion的可能性。本文旨在揭开Lion的神秘面纱。基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数$f(x)$的同时强制执行边界约束。

    Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
    
[^34]: 带有异常值的三元数据聚类

    Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])

    [http://arxiv.org/abs/2310.05288](http://arxiv.org/abs/2310.05288)

    这项研究提出了一种用于聚类矩阵形式数据的方法，可以处理其中的异常值。

    

    矩阵变量分布是模型聚类领域的最新添加，从而可以分析具有复杂结构（如图像和时间序列）的矩阵形式数据。由于其最近的出现，关于矩阵变量数据的文献有限，对于处理这些模型中的异常值的文献更少。本文讨论了一种用于聚类矩阵变量正态数据的方法。该方法使用子集对数似然的分布，将OCLUST算法扩展到矩阵变量正态数据，并使用迭代方法检测和剪裁异常值。

    Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
    
[^35]: 神经网络的记忆化：超越最坏情况

    Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v1 [stat.ML])

    [http://arxiv.org/abs/2310.00327](http://arxiv.org/abs/2310.00327)

    本文研究了神经网络的插值问题，提出了一种简单的随机算法，在给定的数据集和两个类的情况下，能够以很高的概率构建一个插值的神经网络。这些结果与训练数据规模无关。

    

    在实践中，深度神经网络通常能够轻松地插值其训练数据。为了理解这一现象，许多研究都旨在量化神经网络架构的记忆能力：即在任意放置这些点并任意分配标签的情况下，架构能够插值的最大点数。然而，对于实际数据，人们直觉地期望存在一种良性结构，使得插值在比记忆能力建议的较小网络尺寸上已经发生。在本文中，我们通过采用实例特定的观点来研究插值。我们引入了一个简单的随机算法，它可以在多项式时间内给定一个固定的有限数据集和两个类的情况下，以很高的概率构建出一个插值三层神经网络。所需的参数数量与这两个类的几何特性及其相互排列有关。因此，我们获得了与训练数据规模无关的保证。

    In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of t
    
[^36]: $L^1$估计：线性估计器的最优性

    $L^1$ Estimation: On the Optimality of Linear Estimators. (arXiv:2309.09129v1 [math.ST])

    [http://arxiv.org/abs/2309.09129](http://arxiv.org/abs/2309.09129)

    该论文研究了在$L^1$保真度条件下，从噪声观测中估计随机变量$X$的问题。结果表明，唯一能够引入线性条件中位数的先验分布是高斯分布。此外，还研究了其他$L^p$损失，并观察到对于$p \in [1,2]$，高斯分布是唯一引入线性最优贝叶斯估计器的先验分布。扩展还涵盖了特定指数族条件分布的噪声模型。

    

    在$L^1$保真度条件下，考虑从噪声观测$Y=X+Z$中估计随机变量$X$的问题，其中$Z$是标准正态分布。众所周知，在这种情况下，最优的贝叶斯估计器是条件中位数。本文表明，在条件中位数中引入线性的唯一先验分布是高斯分布。同时，还提供了其他几个结果。特别地，证明了如果对于所有$y$，条件分布$P_{X|Y=y}$都是对称的，则$X$必须服从高斯分布。此外，我们考虑了其他的$L^p$损失，并观察到以下现象：对于$p \in [1,2]$，高斯分布是唯一引入线性最优贝叶斯估计器的先验分布，对于$p \in (2,\infty)$，有无穷多个先验分布可以引入线性性。最后，还提供了扩展，以涵盖导致特定指数族条件分布的噪声模型。

    Consider the problem of estimating a random variable $X$ from noisy observations $Y = X+ Z$, where $Z$ is standard normal, under the $L^1$ fidelity criterion. It is well known that the optimal Bayesian estimator in this setting is the conditional median. This work shows that the only prior distribution on $X$ that induces linearity in the conditional median is Gaussian.  Along the way, several other results are presented. In particular, it is demonstrated that if the conditional distribution $P_{X|Y=y}$ is symmetric for all $y$, then $X$ must follow a Gaussian distribution. Additionally, we consider other $L^p$ losses and observe the following phenomenon: for $p \in [1,2]$, Gaussian is the only prior distribution that induces a linear optimal Bayesian estimator, and for $p \in (2,\infty)$, infinitely many prior distributions on $X$ can induce linearity. Finally, extensions are provided to encompass noise models leading to conditional distributions from certain exponential families.
    
[^37]: 关于正则稀疏逻辑回归的研究

    On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])

    [http://arxiv.org/abs/2309.05925](http://arxiv.org/abs/2309.05925)

    本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。

    

    稀疏逻辑回归旨在同时进行高维数据的分类和特征选择。虽然有许多研究解决了$\ell_1$正则化逻辑回归问题，但对于与非凸惩罚相关的稀疏逻辑回归解决方案并没有等量的文献。本文提出了解决$\ell_1$正则化稀疏逻辑回归和一些满足一定先决条件的非凸惩罚正则化稀疏逻辑回归的方法，并采用类似的优化框架。在提出的优化框架中，我们利用不同的线搜索准则来保证不同正则化项的良好收敛性能。通过对真实世界数据集的二元分类任务进行经验实验，我们证明了我们提出的算法能够以较低的计算成本有效地进行分类和特征选择。

    Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
    
[^38]: 平滑的$f$-散度分布鲁棒优化：指数率效率和不带复杂性的校准。

    Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])

    [http://arxiv.org/abs/2306.14041](http://arxiv.org/abs/2306.14041)

    分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。

    

    在数据驱动的优化中，样本平均逼近已知存在一个所谓的优化者诅咒，会导致在评估解决方案性能时产生乐观偏差。可以通过在估计的目标值中增加“保证空间”或通过分布鲁棒优化（DRO）来解决这个问题，后者是一种快速增长的方法，基于最坏情况分析，为获得的目标价值提供了保护界限。然而，在所有这些现有方法中，对真实解决方案性能的统计保证界限要么需要对目标函数复杂性有限制性条件和知识，要么会表现出取决于分布维度的过于保守的速率。我们认为，在这些挑战方面，一种特殊类型的DRO在理论上提供了强大的优势：对于一大类目标函数，它获得了对真实解的解决方案性能的统计界限，这在指数衰减率方面是可能的，就其紧缩程度而言，要紧密得多。

    In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a "margin" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not
    
[^39]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^40]: 神经常微分方程与深度残差网络的泛化界限

    Generalization bounds for neural ordinary differential equations and deep residual networks. (arXiv:2305.06648v1 [stat.ML])

    [http://arxiv.org/abs/2305.06648](http://arxiv.org/abs/2305.06648)

    本研究提出了神经常微分方程及其变体的泛化界限，涵盖了深度残差网络，其泛化界限与连续权重差异大小有关。

    

    神经常微分方程（Neural ODEs）是一类流行的连续深度深度学习模型。本文考虑了一个由连续时间参数化的ODE及时变的神经ODE组成的大类。我们通过Lipschitz方法推导了这个类别的泛化界限。通过利用神经ODE和深度残差网络之间的类比，我们的方法得到了一个深度残差网络的泛化界限。这个界限与连续权重之间的差异的大小有关。我们通过数值结果演示了这个量是如何影响神经网络的泛化能力的。

    Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks.
    
[^41]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^42]: 通过最优输运和投影追踪的时变密度生成建模

    Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit. (arXiv:2304.09663v1 [stat.ML])

    [http://arxiv.org/abs/2304.09663](http://arxiv.org/abs/2304.09663)

    本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。

    

    受到流行的深度学习算法对于时态密度生成建模所带来的计算困难的启发，我们提出了一种便宜的替代方案，它需要最少的超参数调整，并且可以很好地扩展到高维问题。具体地，我们使用基于投影的最优输运求解器 [Meng等，2019] 来连接连续的样本，然后使用传输样条 [Chewi等，2020] 来插值演化的密度。当采样频率足够高时，最优映射接近于恒等映射，因此计算效率高。此外，训练过程可以高度并行化，因为所有最优映射是独立的，因此可以同时学习。最后，该方法仅基于数值线性代数而不是最小化非凸目标函数，这使我们能够轻松分析和控制算法。我们在合成和真实数据集上进行了几个数值实验。

    Motivated by the computational difficulties incurred by popular deep learning algorithms for the generative modeling of temporal densities, we propose a cheap alternative which requires minimal hyperparameter tuning and scales favorably to high dimensional problems. In particular, we use a projection-based optimal transport solver [Meng et al., 2019] to join successive samples and subsequently use transport splines [Chewi et al., 2020] to interpolate the evolving density. When the sampling frequency is sufficiently high, the optimal maps are close to the identity and are thus computationally efficient to compute. Moreover, the training process is highly parallelizable as all optimal maps are independent and can thus be learned simultaneously. Finally, the approach is based solely on numerical linear algebra rather than minimizing a nonconvex objective function, allowing us to easily analyze and control the algorithm. We present several numerical experiments on both synthetic and real-w
    
[^43]: 一种基于可解释神经网络的连续回应有序回归非比例赔率模型

    An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])

    [http://arxiv.org/abs/2303.17823](http://arxiv.org/abs/2303.17823)

    本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。

    

    本文提出了一种基于可解释神经网络的非比例赔率模型（N$^3$POM) 用于有序回归，其中反应变量不仅可以取离散值，也可以取连续值，而回归系数根据预测顺序反应也不同。与传统方法直接从离散反应估计线性系数不同，我们训练了一个非线性的神经网络，通过以反应为输入产生线性系数。由于神经网络的优势，N$^3$POM可以在保留传统有序回归的可解释性的同时具有灵活性。我们给出了充分的条件，使得在指定的用户区域内，预测的条件累积概率（CCP）满足局部单调性约束。我们还提供了一种保持单调性的随机（MPS）算法来充分训练神经网络。

    This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
    
[^44]: 一种完整的扩散生成模型的配方

    A Complete Recipe for Diffusion Generative Models. (arXiv:2303.01748v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01748](http://arxiv.org/abs/2303.01748)

    本文提出了一种完整的扩散生成模型的配方，利用可扩展的贝叶斯后验采样器的见解，确保收敛到所需的目标分布。在这个方法的基础上，引入了相空间Langevin扩散（PSLD），在扩展空间中进行基于评分的建模，展现出更优质的样本质量和改进的速度-质量权衡。

    

    基于评分的生成模型（SGMs）在各种任务中展示了出色的合成结果。然而，目前前向扩散过程的设计领域仍然较少开发，并且通常依赖物理启发法或简化假设。利用可扩展的贝叶斯后验采样器的见解，我们提出了一种完整的配方，用于在SGMs中制定前向过程，确保收敛到所需的目标分布。我们的方法揭示了几个现有的SGMs可以被看作是我们框架的特定表现形式。在这个方法的基础上，我们引入了相空间Langevin扩散（PSLD），它依赖于基于评分的建模，在由辅助变量增强的类似物理相空间的扩展空间中进行。实证结果表明，与已建立的图像合成基准上的各种竞争方法相比，PSLD展现了更优质的样本质量和改进的速度-质量权衡。

    Score-based Generative Models (SGMs) have demonstrated exceptional synthesis outcomes across various tasks. However, the current design landscape of the forward diffusion process remains largely untapped and often relies on physical heuristics or simplifying assumptions. Utilizing insights from the development of scalable Bayesian posterior samplers, we present a complete recipe for formulating forward processes in SGMs, ensuring convergence to the desired target distribution. Our approach reveals that several existing SGMs can be seen as specific manifestations of our framework. Building upon this method, we introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based modeling within an augmented space enriched by auxiliary variables akin to physical phase space. Empirical results exhibit the superior sample quality and improved speed-quality trade-off of PSLD compared to various competing approaches on established image synthesis benchmarks. Remarkably, PSLD achieves 
    
[^45]: 变量选择在核双样本检验中的应用

    Variable Selection for Kernel Two-Sample Tests. (arXiv:2302.07415v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.07415](http://arxiv.org/abs/2302.07415)

    本文提出了一种解决双样本检验中变量选择问题的框架，利用核最大均值差异统计量，以最大化方差正则化的MMD统计量。实验结果证明其超群表现。

    

    本文考虑了两样本检验中的变量选择问题，旨在选择区分两组样本的最有信息变量。为了解决该问题，我们提出了一种基于核最大均值差异（MMD）的框架。我们的方法寻求一组变量，其预先确定的大小最大化方差正则化的MMD统计量。这种计算形式也对应于在文献中研究的控制类型I错误的同时最小化异质类型II错误。我们介绍了混合整数编程公式，并提供了线性和二次类型内核函数的精确和近似算法，并具有性能保证。实验结果证明了我们的框架的卓越性能。

    We consider the variable selection problem for two-sample tests, aiming to select the most informative variables to distinguish samples from two groups. To solve this problem, we propose a framework based on the kernel maximum mean discrepancy (MMD). Our approach seeks a group of variables with a pre-specified size that maximizes the variance-regularized MMD statistics. This formulation also corresponds to the minimization of asymptotic type-II error while controlling type-I error, as studied in the literature. We present mixed-integer programming formulations and offer exact and approximation algorithms with performance guarantees for linear and quadratic types of kernel functions. Experimental results demonstrate the superior performance of our framework.
    
[^46]: 实数值和计数时间序列的高效概率调和预测

    Efficient probabilistic reconciliation of forecasts for real-valued and count time series. (arXiv:2210.02286v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.02286](http://arxiv.org/abs/2210.02286)

    本论文提出了一种基于条件方法来调和任何类型的预测分布的新方法，并引入了一种名为“自下而上重要性抽样”的高效抽样算法。这种方法在多个时间层次的实验中显示出与基本概率预测相比的显著改进。

    

    层次时间序列在许多应用领域中很常见。这些时间序列的预测需要是一致的，也就是满足层次结构的约束条件。强制一致性的最流行技术被称为调和，它调整了为每个时间序列计算的基本预测。然而，最近关于概率调和的研究存在几个局限性。在本文中，我们提出了一种新的基于条件方法来调和任何类型的预测分布。然后，我们引入了一种名为“自下而上重要性抽样”的新算法，来高效地从调和后的分布中进行抽样。它可以用于任何基本预测分布：离散的、连续的，或者以样本形式提供的，相对于当前方法，它提供了很大的速度提升。在几个时间层次的实验中，显示出与基本概率预测相比的显著改进。

    Hierarchical time series are common in several applied fields. The forecasts for these time series are required to be coherent, that is, to satisfy the constraints given by the hierarchy. The most popular technique to enforce coherence is called reconciliation, which adjusts the base forecasts computed for each time series. However, recent works on probabilistic reconciliation present several limitations. In this paper, we propose a new approach based on conditioning to reconcile any type of forecast distribution. We then introduce a new algorithm, called Bottom-Up Importance Sampling, to efficiently sample from the reconciled distribution. It can be used for any base forecast distribution: discrete, continuous, or in the form of samples, providing a major speedup compared to the current methods. Experiments on several temporal hierarchies show a significant improvement over base probabilistic forecasts.
    
[^47]: 一种用于多步鲍型自适应异方差时间序列预测的通用框架

    A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.14219](http://arxiv.org/abs/2207.14219)

    本文介绍了一种名为AEnbMIMOCQR的新颖算法，通过自适应集成的方式，在不需要数据拆分的情况下，以分布无关的方式生成多步鲍型预测区间。该方法考虑了异方差性，并对分布转变具有鲁棒性，在实验中表现优于其他竞争方法。

    

    本文介绍了一种新颖的模型无关算法，名为自适应集成批量多输入多输出鲍型分位数回归（AEnbMIMOCQR），使得预测者能够以分布无关的方式生成固定预设失配率的多步鲍型预测区间。我们的方法基于鲍型预测原理，但不需要数据拆分，并且即使在数据不可互换的情况下也能提供接近精确的覆盖率。此外，所得到的预测区间在预测时间范围内经验证明有效，并且考虑了异方差性。AEnbMIMOCQR被设计成对分布转变具有鲁棒性，这意味着其预测区间在无限的时间范围内保持可靠，而无需重新训练或对数据生成过程进行不切实际的严格假设。通过系统实验，我们证明了我们的方法在鲍型预测中优于其他竞争方法。

    This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
    
[^48]: 有条件Sig-Wasserstein GANs用于时间序列生成

    Conditional Sig-Wasserstein GANs for Time Series Generation. (arXiv:2006.05421v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.05421](http://arxiv.org/abs/2006.05421)

    本论文提出了一种有条件的Sig-Wasserstein GANs框架，通过将WGANs与路径特征提取方法相结合，用于解决时间序列生成中的挑战。路径的签名作为一种通用描述数据流的统计特征，可以刻画时序模型的分布特征。

    

    生成对抗网络（GANs）在生成样本方面取得了极大的成功，从看似高维的概率分布中生成样本。然而，这些方法在捕捉由时间序列数据引起的联合概率分布的时序依赖性方面存在困难。此外，长时间序列数据流会极大地增加目标空间的维度，可能使生成建模变得不可行。为了克服这些挑战，受计量经济学中自回归模型的启发，我们对给定过去信息的未来时间序列的条件分布非常感兴趣。我们提出了一种通用的条件Sig-WGAN框架，通过将Wasserstein-GANs（WGANs）与数学上有原则且高效的路径特征提取技术——路径的签名相结合。路径的签名是一系列分级统计量，为数据流提供了一个通用的描述，其期望值刻画了时间序列模型的分布特征。

    Generative adversarial networks (GANs) have been extremely successful in generating samples, from seemingly high dimensional probability measures. However, these methods struggle to capture the temporal dependence of joint probability distributions induced by time-series data. Furthermore, long time-series data streams hugely increase the dimension of the target space, which may render generative modelling infeasible. To overcome these challenges, motivated by the autoregressive models in econometric, we are interested in the conditional distribution of future time series given the past information. We propose the generic conditional Sig-WGAN framework by integrating Wasserstein-GANs (WGANs) with mathematically principled and efficient path feature extraction called the signature of a path. The signature of a path is a graded sequence of statistics that provides a universal description for a stream of data, and its expected value characterises the law of the time-series model. In parti
    
[^49]: L2P: 学习放置用于估计重尾分布结果

    L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes. (arXiv:1908.04628v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1908.04628](http://arxiv.org/abs/1908.04628)

    L2P是一种利用实例之间成对关系进行学习的方法，用于解决具有重尾分布特征的预测任务。实验证明，L2P在准确度和能力方面优于竞争方法。

    

    许多现实世界的预测任务具有具有重尾分布特征的结果变量。例如，销售的图书副本，艺术品拍卖价格，仓库中商品的需求等。通过学习重尾分布，"大而罕见"的实例（例如，畅销书）将具有准确的预测。大多数现有方法并不专注于学习重尾分布；因此，它们往往会对这些实例进行严重低估。为了解决这个问题，我们引入了学习放置（L2P）方法，它利用实例之间的成对关系进行学习。在训练阶段，L2P学习一对一偏好分类器：实例A是否大于实例B？在放置阶段，L2P通过将新实例放置在已知实例之间来获得预测结果。根据其放置位置，新实例被分配一个结果变量的值。实验结果表明，L2P在准确度和能力方面优于竞争方法。

    Many real-world prediction tasks have outcome variables that have characteristic heavy-tail distributions. Examples include copies of books sold, auction prices of art pieces, demand for commodities in warehouses, etc. By learning heavy-tailed distributions, "big and rare" instances (e.g., the best-sellers) will have accurate predictions. Most existing approaches are not dedicated to learning heavy-tailed distribution; thus, they heavily under-predict such instances. To tackle this problem, we introduce Learning to Place (L2P), which exploits the pairwise relationships between instances for learning. In its training phase, L2P learns a pairwise preference classifier: is instance A > instance B? In its placing phase, L2P obtains a prediction by placing the new instance among the known instances. Based on its placement, the new instance is then assigned a value for its outcome variable. Experiments on real data show that L2P outperforms competing approaches in terms of accuracy and abili
    

