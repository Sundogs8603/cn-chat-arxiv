# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Diffusion Schr\"odinger Bridge Matching.](http://arxiv.org/abs/2303.16852) | 本文介绍了一种新的方法 Iterative Markovian Fitting，用于解决高维度 Schr\"odinger桥（SBs）问题，该方法的数值实验表现出在准确性和性能方面的显著优势。 |
| [^2] | [Randomly Projected Convex Clustering Model: Motivation, Realization, and Cluster Recovery Guarantees.](http://arxiv.org/abs/2303.16841) | 本文提出了一种随机投影凸聚类模型，具有簇恢复保证和良好的鲁棒性，在聚类精度和可伸缩性方面优于许多最先进的聚类算法。 |
| [^3] | [An inexact linearized proximal algorithm for a class of DC composite optimization problems and applications.](http://arxiv.org/abs/2303.16822) | 本文提出了一种解决非凸非光滑问题的非精确线性化近端算法，并应用于鲁棒分解中的两个问题，得到了有效的数值结果。 |
| [^4] | [PAC-Bayesian bounds for learning LTI-ss systems with input from empirical loss.](http://arxiv.org/abs/2303.16816) | 本文推导出应用于有限数据点学习模型的PAC-Bayesian误差界限，使我们能够为广泛的学习/系统辨识算法提供有限样本误差边界。 |
| [^5] | [Optimal approximation of $C^k$-functions using shallow complex-valued neural networks.](http://arxiv.org/abs/2303.16813) | 本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。 |
| [^6] | [Correcting for Selection Bias and Missing Response in Regression using Privileged Information.](http://arxiv.org/abs/2303.16800) | 该研究提出了一种基于特权信息的重复回归方法，可用于在回归模型中校正选择偏差和缺失响应，这种方法易于实现且表现良好。 |
| [^7] | [Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization.](http://arxiv.org/abs/2303.16721) | 本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。 |
| [^8] | [One-Step Estimation of Differentiable Hilbert-Valued Parameters.](http://arxiv.org/abs/2303.16711) | 本文介绍了一种针对Hilbert-Valued参数进行一步估计的方法，并提供一种适用于缺乏重现核的Hilbert空间的解决方案。 |
| [^9] | [Probabilistic inverse optimal control with local linearization for non-linear partially observable systems.](http://arxiv.org/abs/2303.16698) | 本文介绍了一种针对非线性部分可观测系统的局部线性化概率逆优化控制方法，可用于特征化顺序决策任务中的行为，并且具有广泛的适用性。 |
| [^10] | [A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates.](http://arxiv.org/abs/2303.16668) | 本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。 |
| [^11] | [Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning.](http://arxiv.org/abs/2303.16535) | 本文概括了无监督深度学习中基于独立成分分析方法的最新发展，特别是对于解决非线性情况下唯一性问题提出了可识别的扩展方法。 |
| [^12] | [Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints.](http://arxiv.org/abs/2303.16510) | 本研究提出了一种简单迭代的Landing算法，可以在不强制执行正交约束的同时顺畅地吸引到正交约束流形上。我们扩展了这种算法以支持斯托菲尔（Stiefel）流形，并提供了随机和方差约减算法，这些方法与黎曼优化算法的收敛速度相同但需要更少的计算。 |
| [^13] | [An Over-parameterized Exponential Regression.](http://arxiv.org/abs/2303.16504) | 该论文介绍了一种使用指数激活函数定义的神经函数来实现过参数化指数回归模型，并提出了一种新的注意力机制。 |
| [^14] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^15] | [Maximum likelihood smoothing estimation in state-space models: An incomplete-information based approach.](http://arxiv.org/abs/2303.16364) | 本文提出了一种新方法可以从随机状态空间系统的不完整信息/数据中进行极大似然（ML）平滑估计，所得到的估计结果比传统方法精度更高。算法基于EM梯度粒子算法，递归地进行估计。 |
| [^16] | [Operator learning with PCA-Net: upper and lower complexity bounds.](http://arxiv.org/abs/2303.16317) | 本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。 |
| [^17] | [Provable Robustness for Streaming Models with a Sliding Window.](http://arxiv.org/abs/2303.16308) | 本文关注于流数据上机器学习模型的可证明鲁棒性，为使用固定大小滑动窗口的模型提供了强稳健性证明。 |
| [^18] | [Comparing Machine Learning Methods for Estimating Heterogeneous Treatment Effects by Combining Data from Multiple Randomized Controlled Trials.](http://arxiv.org/abs/2303.16299) | 本文研究了多个试验中利用数据估计个体化治疗效应的非参数方法，模拟表明直接允许试验间治疗效应的异质性的方法表现更好，单一研究方法的选择取决于治疗效应的功能形式。 |
| [^19] | [Convergence of Momentum-Based Heavy Ball Method with Batch Updating and/or Approximate Gradients.](http://arxiv.org/abs/2303.16241) | 本文研究了含有批量更新和/或近似梯度的动量重球法的收敛性，由于采用了简化的梯度计算方法，大大减少了计算消耗，同时仍能保证收敛性。 |
| [^20] | [Lifting uniform learners via distributional decomposition.](http://arxiv.org/abs/2303.16208) | 本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\mathcal{D}$中的样本。算法的核心是通过一个算法将$\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。 |
| [^21] | [FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs.](http://arxiv.org/abs/2303.00859) | FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。 |
| [^22] | [Environmental Sensor Placement with Convolutional Gaussian Neural Processes.](http://arxiv.org/abs/2211.10381) | 本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。 |
| [^23] | [A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints.](http://arxiv.org/abs/2210.15659) | 本文介绍了一种解决具有一般形式约束的变分不等式的原始-对偶方法，并使用一种温启动技术，在每次迭代中近似地解决子问题，我们证明了它的收敛性，并展示了它的收敛速度比其精确对应物更快。 |
| [^24] | [Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures.](http://arxiv.org/abs/2210.14347) | 本文提出一种在高维特征下进行二元分类的方法，使用广义最小二乘估计器估计最佳分离超平面的方向，惊人地发现超平面的插值取决于标签的编码方式。 |
| [^25] | [Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation.](http://arxiv.org/abs/2207.04475) | 本文提供了关于带有固定步长的线性随机逼近（LSA）算法的有限时间分析，得到了LSA及其Polyak-Ruppert平均化版本定义的迭代的$p$阶矩和高概率偏差界的推导。本文得到的有限时间实例相关的平均LSA迭代的界限是尖锐的，并与局部渐近极小极值限制相同，同时剩余项对混合时间有着紧密的依赖关系。 |
| [^26] | [Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits.](http://arxiv.org/abs/2206.05404) | 我们提出了一种具有新颖估计量和自标准化界限的线性上下文Bandit算法，具有$O(\sqrt{dT\log T})$遗憾界，维度相关的加法项分解遗憾累积，性能优于现有方法。 |
| [^27] | [Global Convergence of Over-parameterized Deep Equilibrium Models.](http://arxiv.org/abs/2205.13814) | 本研究探讨了深度均衡模型的训练动态，提出唯一平衡点始终存在且梯度下降的收敛速率为线性收敛到全局最优解，可通过轻微过参数化得到满足。 |
| [^28] | [Learning Identity-Preserving Transformations on Data Manifolds.](http://arxiv.org/abs/2106.12096) | 本论文提出了一种新方法，可以直接从数据流形中学习保持身份的变换，而无需在训练期间标记变换数据。该方法基于变分自编码器，可以学习一组本地自适应的保持身份的变换，可以提高模型在具有挑战性的图像分类和少样本学习任务上的推广性能。 |

# 详细

[^1]: 扩散Schr\"odinger桥匹配

    Diffusion Schr\"odinger Bridge Matching. (arXiv:2303.16852v1 [stat.ML])

    [http://arxiv.org/abs/2303.16852](http://arxiv.org/abs/2303.16852)

    本文介绍了一种新的方法 Iterative Markovian Fitting，用于解决高维度 Schr\"odinger桥（SBs）问题，该方法的数值实验表现出在准确性和性能方面的显著优势。

    

    解决运输问题，在机器学习中有着许多应用，例如新型的质量传输方法，如去噪扩散模型（DDMs）和流匹配模型（FMMs），通过随机微分方程（SDE）或常微分方程（ODE）实现这样的传输。然而，虽然在许多应用中，近似确定性动态最优传输（OT）映射是可取的，因为具有吸引人的性质，但 DDMs 和 FMMs 并不能保证提供接近 OT 映射的传输。相反，Schr\"odinger桥（SBs）计算随机动态映射，可以恢复正则熵版本的 OT。不幸的是，现有的数值方法近似 SBs 的维度缩放差或在迭代中积累误差。在这项工作中，我们介绍了迭代马尔科夫拟合，一种解决高维度 SB 问题的新方法。我们将这个方法设计为一个迭代过程，将置信传播扩展到 KL 散度，利用条件独立性降低计算复杂度，并确保一致性和收敛性质。我们的数值实验证明了相对于现有成果方法，在准确性和性能方面都有显著优势。

    Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solv
    
[^2]: 随机投影的凸聚类模型：动机，实现和簇恢复保证

    Randomly Projected Convex Clustering Model: Motivation, Realization, and Cluster Recovery Guarantees. (arXiv:2303.16841v1 [cs.LG])

    [http://arxiv.org/abs/2303.16841](http://arxiv.org/abs/2303.16841)

    本文提出了一种随机投影凸聚类模型，具有簇恢复保证和良好的鲁棒性，在聚类精度和可伸缩性方面优于许多最先进的聚类算法。

    

    本文提出了一种随机投影凸聚类模型，用于对$\mathbb{R}^d$中的$n$个高维数据点进行聚类，并假设这些点存在$K$个隐藏的簇。与用于聚类原始数据的凸聚类模型相比($d$维)，我们证明，在某些温和的条件下，如果存在凸聚类模型的完美拆分，那么通过嵌入维度$m=O(\epsilon^{-2}\log(n))$的随机投影凸聚类模型可以保留拆分结果，其中$0 < \epsilon < 1$是一些给定的参数。我们进一步证明，嵌入维度可以改进为$O(\epsilon^{-2}\log(K))$，不受数据点数量的影响。本文还通过广泛的数字实验结果展示了随机投影凸聚类模型的鲁棒性和卓越性能。本文中呈现的数字实验结果还表明，随机投影凸聚类模型在聚类精度和可伸缩性方面优于许多最先进的聚类算法。

    In this paper, we propose a randomly projected convex clustering model for clustering a collection of $n$ high dimensional data points in $\mathbb{R}^d$ with $K$ hidden clusters. Compared to the convex clustering model for clustering original data with dimension $d$, we prove that, under some mild conditions, the perfect recovery of the cluster membership assignments of the convex clustering model, if exists, can be preserved by the randomly projected convex clustering model with embedding dimension $m = O(\epsilon^{-2}\log(n))$, where $0 < \epsilon < 1$ is some given parameter. We further prove that the embedding dimension can be improved to be $O(\epsilon^{-2}\log(K))$, which is independent of the number of data points. Extensive numerical experiment results will be presented in this paper to demonstrate the robustness and superior performance of the randomly projected convex clustering model. The numerical results presented in this paper also demonstrate that the randomly projected 
    
[^3]: 一类DC复合优化问题的非精确线性近似近端算法及应用

    An inexact linearized proximal algorithm for a class of DC composite optimization problems and applications. (arXiv:2303.16822v1 [math.OC])

    [http://arxiv.org/abs/2303.16822](http://arxiv.org/abs/2303.16822)

    本文提出了一种解决非凸非光滑问题的非精确线性化近端算法，并应用于鲁棒分解中的两个问题，得到了有效的数值结果。

    

    本文研究了一类DC复合优化问题。这类问题通常由低秩矩阵恢复的鲁棒分解模型推导而来，是凸复合优化问题和具有非光滑分量的DC规划的扩展。针对这类非凸和非光滑问题，我们提出了一种非精确线性化近端算法（iLPA）。算法中，我们利用目标函数的部分线性化，计算强凸主导的非精确最小化值。迭代序列的生成收敛于潜在函数的Kurdyka-{\L}ojasiewicz（KL）性质，如果潜在函数在极限点处具有KL指数$1/2$的KL性质，则收敛具有局部R线性速率。对于后一种假设，我们利用复合结构提供了一个可验证的条件，并阐明了与凸复合优化所使用的正则性的关系。最后，我们将所提出的非精确线性近端算法应用于解决鲁棒分解中的两个重要问题：张量鲁棒主成分分析（TRPCA）和张量鲁棒低秩张量完成（TRLRTC）。对合成和真实数据的数值结果证明了我们的算法相对于现有最新算法的有效性。

    This paper is concerned with a class of DC composite optimization problems which, as an extension of the convex composite optimization problem and the DC program with nonsmooth components, often arises from robust factorization models of low-rank matrix recovery. For this class of nonconvex and nonsmooth problems, we propose an inexact linearized proximal algorithm (iLPA) which in each step computes an inexact minimizer of a strongly convex majorization constructed by the partial linearization of their objective functions. The generated iterate sequence is shown to be convergent under the Kurdyka-{\L}ojasiewicz (KL) property of a potential function, and the convergence admits a local R-linear rate if the potential function has the KL property of exponent $1/2$ at the limit point. For the latter assumption, we provide a verifiable condition by leveraging the composite structure, and clarify its relation with the regularity used for the convex composite optimization. Finally, the propose
    
[^4]: 从经验损失中学习LTI-ss系统的PAC-Bayesian界限

    PAC-Bayesian bounds for learning LTI-ss systems with input from empirical loss. (arXiv:2303.16816v1 [stat.ML])

    [http://arxiv.org/abs/2303.16816](http://arxiv.org/abs/2303.16816)

    本文推导出应用于有限数据点学习模型的PAC-Bayesian误差界限，使我们能够为广泛的学习/系统辨识算法提供有限样本误差边界。

    

    本文针对带输入的线性时不变系统（LTI）随机动力系统，推导出一种概率近似正确（PAC）-Bayesian误差界限。该界限广泛用于机器学习中，用于表征从有限数据点学习的模型的预测能力。特别地，本文导出的界限将未来的平均预测误差与模型在学习数据上生成的预测误差联系起来。因此，这使我们能够为广泛的学习/系统辨识算法提供有限样本误差界限。此外，由于LTI系统是递归神经网络（RNN）的一个子类，因此这些误差界限可能是PAC-Bayesian界限适用于RNN的第一步。

    In this paper we derive a Probably Approxilmately Correct(PAC)-Bayesian error bound for linear time-invariant (LTI) stochastic dynamical systems with inputs. Such bounds are widespread in machine learning, and they are useful for characterizing the predictive power of models learned from finitely many data points. In particular, with the bound derived in this paper relates future average prediction errors with the prediction error generated by the model on the data used for learning. In turn, this allows us to provide finite-sample error bounds for a wide class of learning/system identification algorithms. Furthermore, as LTI systems are a sub-class of recurrent neural networks (RNNs), these error bounds could be a first step towards PAC-Bayesian bounds for RNNs.
    
[^5]: 浅层复值神经网络对$C^k$-函数的最优逼近

    Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])

    [http://arxiv.org/abs/2303.16813](http://arxiv.org/abs/2303.16813)

    本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。

    

    本文证明了使用浅层复值神经网络对复立方体上$C^k$（在实变量意义下）的函数进行逼近的量化结果。具体而言，我们考虑具有单层隐藏层和$m$个神经元的神经网络，即形如$z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$的网络，并且证明了可以使用这种形式的函数逼近$C^k \left(\Omega_n;\mathbb{C}\right)$中的任何函数，当$m\to\infty$时误差为$m^{-k/(2n)}$.此外，我们还证明选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续并且在这种连续性假设下获得的逼近速率是最优的。

    We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\Omega_n := [-1,1]^n +i[-1,1]^n\subseteq \mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$ and show that one can approximate every function in $C^k \left( \Omega_n; \mathbb{C}\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \to \infty$, provided that the activation function $\phi: \mathbb{C} \to \mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\sigma_j, b_j \in \mathbb{C}$ and $\rho_j \in \mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss
    
[^6]: 利用特权信息校正回归中的选择偏差和缺失响应

    Correcting for Selection Bias and Missing Response in Regression using Privileged Information. (arXiv:2303.16800v1 [math.ST])

    [http://arxiv.org/abs/2303.16800](http://arxiv.org/abs/2303.16800)

    该研究提出了一种基于特权信息的重复回归方法，可用于在回归模型中校正选择偏差和缺失响应，这种方法易于实现且表现良好。

    

    当进行回归模型估计时，可能会出现一些标签缺失的数据，或者我们的数据可能会受到选择机制的偏差影响。当响应或选择机制是可忽略的（即，在给定特征的情况下，独立于响应变量），可以使用现成的回归方法；在不可忽略的情况下，通常必须进行偏差调整。我们观察到，特权数据（即只在训练期间可用的数据）可能会使不可忽略的选择机制可忽略，并将这种情况称为特权缺失随机（PMAR）。我们提出了一种新的基于插补的回归方法，称为重复回归，适用于PMAR。我们还考虑了一种重要性加权回归方法和两种方法的双重稳健组合。所提出的方法易于使用大多数流行的现成回归算法进行实现。我们通过大量的模拟实验和对Eye State数据集的实证评估来评估所提出的方法的性能。

    When estimating a regression model, we might have data where some labels are missing, or our data might be biased by a selection mechanism. When the response or selection mechanism is ignorable (i.e., independent of the response variable given the features) one can use off-the-shelf regression methods; in the nonignorable case one typically has to adjust for bias. We observe that privileged data (i.e. data that is only available during training) might render a nonignorable selection mechanism ignorable, and we refer to this scenario as Privilegedly Missing at Random (PMAR). We propose a novel imputation-based regression method, named repeated regression, that is suitable for PMAR. We also consider an importance weighted regression method, and a doubly robust combination of the two. The proposed methods are easy to implement with most popular out-of-the-box regression algorithms. We empirically assess the performance of the proposed methods with extensive simulated experiments and on a 
    
[^7]: 最大似然方法再探：Kullback - Leibler 散度中的规范对称性和性能保证的正则化

    Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization. (arXiv:2303.16721v1 [stat.ML])

    [http://arxiv.org/abs/2303.16721](http://arxiv.org/abs/2303.16721)

    本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。

    

    最大似然方法是估计数据背后概率的最知名方法。然而，传统方法获得与经验分布最接近的概率模型，导致过度拟合。然后，正则化方法可以防止模型过度接近错误的概率，但是对它们的性能知之甚少。正则化的思想类似于纠错代码，通过将次优解与错误接收到的代码混合，获得最优解码。纠错代码中的最优解码是基于规范对称性实现的。我们通过关注 Kullback - Leibler 散度中的规范对称性，提出了最大似然方法中的理论上保证的正则化。在我们的方法中，我们可以获得最优的模型，而无需频繁搜索正则化中经常出现的超参数。

    The maximum likelihood method is the best-known method for estimating the probabilities behind the data. However, the conventional method obtains the probability model closest to the empirical distribution, resulting in overfitting. Then regularization methods prevent the model from being excessively close to the wrong probability, but little is known systematically about their performance. The idea of regularization is similar to error-correcting codes, which obtain optimal decoding by mixing suboptimal solutions with an incorrectly received code. The optimal decoding in error-correcting codes is achieved based on gauge symmetry. We propose a theoretically guaranteed regularization in the maximum likelihood method by focusing on a gauge symmetry in Kullback -- Leibler divergence. In our approach, we obtain the optimal model without the need to search for hyperparameters frequently appearing in regularization.
    
[^8]: Hilbert-Valued参数的一步估计

    One-Step Estimation of Differentiable Hilbert-Valued Parameters. (arXiv:2303.16711v1 [math.ST])

    [http://arxiv.org/abs/2303.16711](http://arxiv.org/abs/2303.16711)

    本文介绍了一种针对Hilbert-Valued参数进行一步估计的方法，并提供一种适用于缺乏重现核的Hilbert空间的解决方案。

    

    本文提出了对光滑Hilbert-Valued参数的估计器，其中光滑性由逐路径可微条件表征。当参数空间是重现核Hilbert空间时，我们提供了一种获取高效和相关置信区间的方法。这些估计器对应于基于Hilbert-Valued有效影响函数的交叉一步估计器的概括。我们提供了理论保证，即使使用任意的辅助函数估计器，包括基于机器学习技术的估计器。我们表明，即使缺乏重现核的Hilbert空间，只要参数具有高效的影响函数，这些结果自然地可以扩展到该空间。然而，我们也揭示了不幸的事实，当不存在重现核时，许多有趣的参数即使它们在路径上是可微的，也缺乏有效的影响函数。为了处理这些情况，我们提出了一种正则化的方法。

    We present estimators for smooth Hilbert-valued parameters, where smoothness is characterized by a pathwise differentiability condition. When the parameter space is a reproducing kernel Hilbert space, we provide a means to obtain efficient, root-n rate estimators and corresponding confidence sets. These estimators correspond to generalizations of cross-fitted one-step estimators based on Hilbert-valued efficient influence functions. We give theoretical guarantees even when arbitrary estimators of nuisance functions are used, including those based on machine learning techniques. We show that these results naturally extend to Hilbert spaces that lack a reproducing kernel, as long as the parameter has an efficient influence function. However, we also uncover the unfortunate fact that, when there is no reproducing kernel, many interesting parameters fail to have an efficient influence function, even though they are pathwise differentiable. To handle these cases, we propose a regularized on
    
[^9]: 针对非线性部分可观测系统的局部线性化概率逆优化控制方法

    Probabilistic inverse optimal control with local linearization for non-linear partially observable systems. (arXiv:2303.16698v1 [cs.LG])

    [http://arxiv.org/abs/2303.16698](http://arxiv.org/abs/2303.16698)

    本文介绍了一种针对非线性部分可观测系统的局部线性化概率逆优化控制方法，可用于特征化顺序决策任务中的行为，并且具有广泛的适用性。

    

    逆优化控制方法可以用于特征化顺序决策任务中的行为。然而，大多数现有的工作要求已知控制信号，或者仅限于完全可观测或线性系统。本文介绍了一种概率逆优化控制方法，用于非线性随机系统的丢失控制信号和部分可观测性，该方法统一了现有方法。通过使用代理的感觉和控制系统的噪声特征的显式模型以及局部线性化技术，我们推导出了模型参数的近似似然函数，可以在单个正向传递中计算。我们在随机和部分可观测版本的经典控制任务，导航任务和手动达到任务上评估了我们提出的方法。该方法具有广泛的适用性，可用于模仿学习到感觉运动神经科学。

    Inverse optimal control methods can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, requires the control signals to be known, or is limited to fully-observable or linear systems. This paper introduces a probabilistic approach to inverse optimal control for stochastic non-linear systems with missing control signals and partial observability that unifies existing approaches. By using an explicit model of the noise characteristics of the sensory and control systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood for the model parameters, which can be computed within a single forward pass. We evaluate our proposed method on stochastic and partially observable version of classic control tasks, a navigation task, and a manual reaching task. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience.
    
[^10]: 基于矩阵自回归的联邦学习拜占庭容错聚合方案

    A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])

    [http://arxiv.org/abs/2303.16668](http://arxiv.org/abs/2303.16668)

    本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。

    

    本文提出了FLANDERS，一种新颖的联邦学习（FL）聚合方案，可以抵御拜占庭攻击。FLANDERS将每个FL轮次中由客户端发送的本地模型更新视为矩阵值时间序列。然后，通过将实际观测与由矩阵自回归预测模型估计的观测进行比较，识别恶意客户端作为这个时间序列的异常值。在不同FL设置下对多个数据集进行的实验证明，FLANDERS在抵御拜占庭攻击方面与最强大的基线相匹配。此外，与现有的防御策略相比， FLANDERS即使在极其严重的攻击场景下仍然非常有效。

    In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
    
[^11]: 无监督深度学习中基于非线性独立成分分析的原则分离问题

    Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning. (arXiv:2303.16535v1 [cs.LG])

    [http://arxiv.org/abs/2303.16535](http://arxiv.org/abs/2303.16535)

    本文概括了无监督深度学习中基于独立成分分析方法的最新发展，特别是对于解决非线性情况下唯一性问题提出了可识别的扩展方法。

    

    在无监督深度学习中，如何找到有用的高维数据表示，即所谓的“分离”问题至关重要。大多数方法都是启发式的，缺乏适当的理论基础。在线性表示学习中，独立成分分析（ICA）在许多应用领域取得了成功，并且具有基于良定义的概率模型的原则性。 然而，将ICA扩展到非线性情况已经成为一个棘手的问题，这是由于缺乏可识别性，即表示的唯一性。最近，已经提出了使用时间结构或某些辅助信息的非线性扩展。这些模型实际上是可识别的，因此已经开发出越来越多的算法。特别是一些自监督算法可以显示出估计非线性ICA，即使最初是从启发式角度提出的。本文总结了非线性ICA的最新进展。

    A central problem in unsupervised deep learning is how to find useful representations of high-dimensional data, sometimes called "disentanglement". Most approaches are heuristic and lack a proper theoretical foundation. In linear representation learning, independent component analysis (ICA) has been successful in many applications areas, and it is principled, i.e. based on a well-defined probabilistic model. However, extension of ICA to the nonlinear case has been problematic due to the lack of identifiability, i.e. uniqueness of the representation. Recently, nonlinear extensions that utilize temporal structure or some auxiliary information have been proposed. Such models are in fact identifiable, and consequently, an increasing number of algorithms have been developed. In particular, some self-supervised algorithms can be shown to estimate nonlinear ICA, even though they have initially been proposed from heuristic perspectives. This paper reviews the state-of-the-art of nonlinear ICA 
    
[^12]: 在正交约束下的优化问题中的不可行确定性、随机和方差约减算法

    Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints. (arXiv:2303.16510v1 [stat.ML])

    [http://arxiv.org/abs/2303.16510](http://arxiv.org/abs/2303.16510)

    本研究提出了一种简单迭代的Landing算法，可以在不强制执行正交约束的同时顺畅地吸引到正交约束流形上。我们扩展了这种算法以支持斯托菲尔（Stiefel）流形，并提供了随机和方差约减算法，这些方法与黎曼优化算法的收敛速度相同但需要更少的计算。

    

    正交约束在许多机器学习问题中都会自然地出现，从主成分分析到鲁棒性神经网络训练。传统上，这些问题需要使用黎曼优化算法来求解，该算法在强制执行约束时最耗费时间。最近，Ablin＆Peyr\'e（2022）提出了Landing算法，这是一种廉价迭代方法，它不强制执行正交约束，但会以平滑的方式吸引到流形上。在本文中，我们为Landing算法提供了新的实用和理论发展。首先，该方法被扩展到斯托菲尔流形，即矩形正交矩阵的集合。当成本函数是许多函数的平均值时，我们还考虑随机和方差约减算法。我们证明了所有这些方法的收敛速度与它们的黎曼优化算法相同，同时需要更少的计算。

    Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin & Peyr\'e (2022) proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner. In this article, we provide new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Rieman
    
[^13]: 一种过参数化指数回归模型

    An Over-parameterized Exponential Regression. (arXiv:2303.16504v1 [cs.LG])

    [http://arxiv.org/abs/2303.16504](http://arxiv.org/abs/2303.16504)

    该论文介绍了一种使用指数激活函数定义的神经函数来实现过参数化指数回归模型，并提出了一种新的注意力机制。

    

    近年来，对ReLU激活函数的研究引发了许多关注，旨在通过过参数化实现神经网络收敛。然而，最近在大型语言模型 (LLM) 领域的发展引起了人们对指数激活函数的兴趣，特别是在注意力机制中的应用。在数学上，我们使用指数激活函数定义神经函数 $F: \mathbb{R}^{d \times m} \times \mathbb{R}^d \rightarrow \mathbb{R}$。给定一组带标签的数据点 $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \subset \mathbb{R}^d \times \mathbb{R}$，其中 $n$ 表示数据的数量。这里 $F(W(t),x)$ 可以用 $F(W(t),x) := \sum_{r=1}^m a_r \exp(\langle w_r, x \rangle)$ 表示，其中 $m$ 表示神经元的数量，$w_r(t)$ 是时间 $t$ 上的权重。固定权重 $a_r$ 在训练期间不会改变，这是文献中的标准。

    Over the past few years, there has been a significant amount of research focused on studying the ReLU activation function, with the aim of achieving neural network convergence through over-parametrization. However, recent developments in the field of Large Language Models (LLMs) have sparked interest in the use of exponential activation functions, specifically in the attention mechanism.  Mathematically, we define the neural function $F: \mathbb{R}^{d \times m} \times \mathbb{R}^d \rightarrow \mathbb{R}$ using an exponential activation function. Given a set of data points with labels $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \subset \mathbb{R}^d \times \mathbb{R}$ where $n$ denotes the number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) := \sum_{r=1}^m a_r \exp(\langle w_r, x \rangle)$, where $m$ represents the number of neurons, and $w_r(t)$ are weights at time $t$. It's standard in literature that $a_r$ are the fixed weights and it's never changed during the trai
    
[^14]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^15]: 隐藏信息下基于极大似然的状态空间模型平滑估计方法研究

    Maximum likelihood smoothing estimation in state-space models: An incomplete-information based approach. (arXiv:2303.16364v1 [stat.ME])

    [http://arxiv.org/abs/2303.16364](http://arxiv.org/abs/2303.16364)

    本文提出了一种新方法可以从随机状态空间系统的不完整信息/数据中进行极大似然（ML）平滑估计，所得到的估计结果比传统方法精度更高。算法基于EM梯度粒子算法，递归地进行估计。

    

    本文重新审视了Rauch （1963年）和et al.（1965年）的经典作品，提出了一种从随机状态空间系统的不完整信息/数据中进行极大似然（ML）平滑估计的新方法。介绍了不完整数据的得分函数和条件观测信息矩阵，并建立了它们的分布式身份。利用这些身份，提出了ML smoother $\widehat{x}_{k\vert n}^s =\argmax_{x_k} \log f(x_k,\widehat{x}_{k+1\vert n}^s, y_{0:n}\vert\theta)$，$k\leq n-1$。结果表明，和ML状态估计$\widehat{x}_k=\argmax_{x_k} \log f(x_k,y_{0:k}\vert\theta)$相比，ML平滑估计器给出的状态$x_k$估计具有更少的标准误差，并且更符合对数似然。zhi。基于EM梯度粒子算法给出递归估计，该算法扩展了\cite{Lange}的ML平滑估计工作。该算法具有明确的迭代更新。

    This paper revisits classical works of Rauch (1963, et al. 1965) and develops a novel method for maximum likelihood (ML) smoothing estimation from incomplete information/data of stochastic state-space systems. Score function and conditional observed information matrices of incomplete data are introduced and their distributional identities are established. Using these identities, the ML smoother $\widehat{x}_{k\vert n}^s =\argmax_{x_k} \log f(x_k,\widehat{x}_{k+1\vert n}^s, y_{0:n}\vert\theta)$, $k\leq n-1$, is presented. The result shows that the ML smoother gives an estimate of state $x_k$ with more adherence of loglikehood having less standard errors than that of the ML state estimator $\widehat{x}_k=\argmax_{x_k} \log f(x_k,y_{0:k}\vert\theta)$, with $\widehat{x}_{n\vert n}^s=\widehat{x}_n$. Recursive estimation is given in terms of an EM-gradient-particle algorithm which extends the work of \cite{Lange} for ML smoothing estimation. The algorithm has an explicit iteration update whi
    
[^16]: PCA-Net：操作学习的复杂性上下界

    Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])

    [http://arxiv.org/abs/2303.16317](http://arxiv.org/abs/2303.16317)

    本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。

    

    神经算子在计算科学和工程中备受关注。PCA-Net是一种最近提出的神经算子架构，它将主成分分析(PCA)与神经网络相结合，以逼近潜在的算子。本文对这种方法进行了近似理论的发展，改进并显着扩展了此方向的以前的工作。在定性界限方面，本文得出了新颖的通用逼近结果，在对潜在算子和数据生成分布的最小假设的前提下。在定量限制方面，本文识别了使用PCA-Net进行高效操作学习的两个潜在障碍，通过导出下界进行了严格证明，第一个障碍与输出分布的复杂性有关，由PCA特征值的缓慢衰减来衡量；另一个障碍涉及无限维输入和输出空间之间的算子空间的内在复杂性。

    Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
    
[^17]: 滑动窗口流模型的可证明稳健性

    Provable Robustness for Streaming Models with a Sliding Window. (arXiv:2303.16308v1 [cs.LG])

    [http://arxiv.org/abs/2303.16308](http://arxiv.org/abs/2303.16308)

    本文关注于流数据上机器学习模型的可证明鲁棒性，为使用固定大小滑动窗口的模型提供了强稳健性证明。

    

    机器学习中，有关可证明的稳健性的文献主要关注静态预测问题，如图像分类等，其中假定输入样本是独立的，并且模型性能是在输入分布上的期望。对于单个输入实例，可以得出稳健性证明，但是假设该模型对每个实例单独进行评估的稳健性证明不能直接应用于许多深度学习应用中。本文关注于数据流上机器学习模型的可证明鲁棒性，其中输入作为一系列可能相关的项呈现。我们为使用固定大小的滑动窗口的模型推导出了强稳健性证明，保证了整个输入序列，而不是单个样本，并为流机器学习模型提供了强稳健性保证。

    The literature on provable robustness in machine learning has primarily focused on static prediction problems, such as image classification, in which input samples are assumed to be independent and model performance is measured as an expectation over the input distribution. Robustness certificates are derived for individual input instances with the assumption that the model is evaluated on each instance separately. However, in many deep learning applications such as online content recommendation and stock market analysis, models use historical data to make predictions. Robustness certificates based on the assumption of independent input samples are not directly applicable in such scenarios. In this work, we focus on the provable robustness of machine learning models in the context of data streams, where inputs are presented as a sequence of potentially correlated items. We derive robustness certificates for models that use a fixed-size sliding window over the input stream. Our guarante
    
[^18]: 结合多个随机对照试验数据的机器学习方法比较异质性治疗效应估计

    Comparing Machine Learning Methods for Estimating Heterogeneous Treatment Effects by Combining Data from Multiple Randomized Controlled Trials. (arXiv:2303.16299v1 [stat.ME])

    [http://arxiv.org/abs/2303.16299](http://arxiv.org/abs/2303.16299)

    本文研究了多个试验中利用数据估计个体化治疗效应的非参数方法，模拟表明直接允许试验间治疗效应的异质性的方法表现更好，单一研究方法的选择取决于治疗效应的功能形式。

    

    个性化的治疗决策可以改善健康结果，但是使用数据以可靠、精确和有普遍意义的方式进行这些决策在一个单一数据集中是有挑战性的。利用多个随机对照试验可以组合具有非混杂性治疗分配的数据集，提高估计异质性治疗效应的能力。本文讨论了几种非参数方法，用于利用多个试验的数据估计异质性治疗效应。我们将单一研究的方法扩展到多个试验的场景中，并通过模拟研究探讨它们的性能，数据生成情景具有不同水平的跨试验异质性。模拟表明，直接允许试验间治疗效应的异质性的方法比不允许的方法表现更好，并且单一研究方法的选择取决于治疗效应的功能形式。最后，通过对减少住院重新入院干预的网络荟萃分析数据的应用，我们比较了实践中的方法，并讨论了对未来研究的影响。

    Individualized treatment decisions can improve health outcomes, but using data to make these decisions in a reliable, precise, and generalizable way is challenging with a single dataset. Leveraging multiple randomized controlled trials allows for the combination of datasets with unconfounded treatment assignment to improve the power to estimate heterogeneous treatment effects. This paper discusses several non-parametric approaches for estimating heterogeneous treatment effects using data from multiple trials. We extend single-study methods to a scenario with multiple trials and explore their performance through a simulation study, with data generation scenarios that have differing levels of cross-trial heterogeneity. The simulations demonstrate that methods that directly allow for heterogeneity of the treatment effect across trials perform better than methods that do not, and that the choice of single-study method matters based on the functional form of the treatment effect. Finally, w
    
[^19]: 采用批量更新和/或近似梯度的动量重球法的收敛性

    Convergence of Momentum-Based Heavy Ball Method with Batch Updating and/or Approximate Gradients. (arXiv:2303.16241v1 [math.OC])

    [http://arxiv.org/abs/2303.16241](http://arxiv.org/abs/2303.16241)

    本文研究了含有批量更新和/或近似梯度的动量重球法的收敛性，由于采用了简化的梯度计算方法，大大减少了计算消耗，同时仍能保证收敛性。

    

    本文研究了1964年Polyak引入的凸优化和非凸优化中广为人知的“动量重球”法，并在多种情况下确立了其收敛性。当要求解参数的维度非常高时，更新一部分而不是所有参数可以提高优化效率，称之为“批量更新”，若与梯度法配合使用，则理论上只需计算需要更新的参数的梯度，而在实际中，通过反向传播等方法仅计算部分梯度并不能减少计算量。因此，为了在每一步中减少CPU使用量，可以使用一阶微分或近似梯度代替真实梯度。我们的分析表明，在各种假设下，采用近似梯度信息和/或批量更新的动量重球法仍然可以收敛。

    In this paper, we study the well-known "Heavy Ball" method for convex and nonconvex optimization introduced by Polyak in 1964, and establish its convergence under a variety of situations. Traditionally, most algorthms use "full-coordinate update," that is, at each step, very component of the argument is updated. However, when the dimension of the argument is very high, it is more efficient to update some but not all components of the argument at each iteration. We refer to this as "batch updating" in this paper.  When gradient-based algorithms are used together with batch updating, in principle it is sufficient to compute only those components of the gradient for which the argument is to be updated. However, if a method such as back propagation is used to compute these components, computing only some components of gradient does not offer much savings over computing the entire gradient. Therefore, to achieve a noticeable reduction in CPU usage at each step, one can use first-order diffe
    
[^20]: 利用分布分解提高均匀学习算法的性能

    Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])

    [http://arxiv.org/abs/2303.16208](http://arxiv.org/abs/2303.16208)

    本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\mathcal{D}$中的样本。算法的核心是通过一个算法将$\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。

    

    我们展示了如何将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布$\mathcal{D}$下有效的算法。我们的转换效率随$\mathcal{D}$的固有复杂性而变化，对于在$\{\pm 1\}^n$上的分布，其pmf由深度为$d$的决策树计算，则时间复杂度为$\mathrm{poly}(n, (md)^d)$，其中$m$是原始算法的样本复杂度。对于单调分布，我们的转换仅使用$\mathcal{D}$中的样本，而对于一般分布，我们使用子立方体条件样本。其中一个关键技术是一个算法，它在给出$\mathcal{D}$的访问权限的情况下，产生了一个最优决策树分解$\mathcal{D}$：一个逼近了$\mathcal{D}$的混合均匀分布的分离子立方体。通过这个分解，我们在每个子立方体上运行均匀分布学习器，并将结果合并起来。

    We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\mathcal{D}$, running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an optimal decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the 
    
[^21]: FuNVol：使用函数主成分和神经SDE的多资产隐含波动率市场模拟器

    FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2303.00859](http://arxiv.org/abs/2303.00859)

    FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。

    

    我们介绍了一种新的方法，使用函数数据分析和神经随机微分方程，结合概率积分变换惩罚来生成多个资产的隐含波动率表面序列，该方法忠实于历史价格。我们证明了学习IV表面和价格的联合动态产生的市场情景与历史特征一致，并且在没有静态套利的表面次流形内。最后，我们证明使用模拟表面进行对冲会生成与实现P＆L一致的损益分布。

    Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&L) distributions that are consistent with realised P&Ls.
    
[^22]: 带有卷积高斯神经过程的环境传感器放置

    Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10381](http://arxiv.org/abs/2211.10381)

    本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。

    

    环境传感器对于监测天气和气候变化的影响至关重要。然而，在像南极这样的偏远地区，最大化测量信息和有效放置传感器是具有挑战性的。概率机器学习模型可以通过预测新传感器提供的不确定性减少来评估放置信息。高斯过程模型广泛用于此目的，但难以捕捉复杂的非平稳行为并缩放到大型数据集。本文提出使用卷积高斯神经过程（ConvGNP）来解决这些问题。ConvGNP使用神经网络来参数化任意目标位置的联合高斯分布，实现了灵活性和可扩展性。使用模拟的南极地区地面温度异常作为真实数据，ConvGNP学习了空间和季节性非平稳性，并优于非平稳GP基线。在模拟的s中，

    Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
    
[^23]: 解决具有一般形式约束的变分不等式的原始-对偶方法

    A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints. (arXiv:2210.15659v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.15659](http://arxiv.org/abs/2210.15659)

    本文介绍了一种解决具有一般形式约束的变分不等式的原始-对偶方法，并使用一种温启动技术，在每次迭代中近似地解决子问题，我们证明了它的收敛性，并展示了它的收敛速度比其精确对应物更快。

    

    Yang等人最近通过一种一阶梯度方法解决了具有等式和不等式约束的变分不等式（VI）的问题。但是，提出的原始-对偶方法称为ACVI仅适用于可以计算其子问题的解析解的情况，因此一般情况仍然是一个开放的问题。在本文中，我们采用一种温启动技术，在每次迭代中近似地解决子问题，并使用在先前迭代中找到的近似解初始化变量。我们证明了它的收敛性，并表明当算子为$L$-Lipschitz且单调时，这种不精确的ACVI方法的最后一次迭代的间隙函数下降的速度为$\mathcal{O}(\frac{1}{\sqrt{K}})$，前提是错误以适当的速度下降。有趣的是，我们展示了在数值实验中，通常这种技术比其精确对应物收敛更快。此外，对于不等式约束的情况

    Yang et al. (2023) recently addressed the open problem of solving Variational Inequalities (VIs) with equality and inequality constraints through a first-order gradient method. However, the proposed primal-dual method called ACVI is applicable when we can compute analytic solutions of its subproblems; thus, the general case remains an open problem. In this paper, we adopt a warm-starting technique where we solve the subproblems approximately at each iteration and initialize the variables with the approximate solution found at the previous iteration. We prove its convergence and show that the gap function of the last iterate of this inexact-ACVI method decreases at a rate of $\mathcal{O}(\frac{1}{\sqrt{K}})$ when the operator is $L$-Lipschitz and monotone, provided that the errors decrease at appropriate rates. Interestingly, we show that often in numerical experiments, this technique converges faster than its exact counterpart. Furthermore, for the cases when the inequality constraints
    
[^24]: 在高维高斯潜在混合模型中插值判别函数

    Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures. (arXiv:2210.14347v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14347](http://arxiv.org/abs/2210.14347)

    本文提出一种在高维特征下进行二元分类的方法，使用广义最小二乘估计器估计最佳分离超平面的方向，惊人地发现超平面的插值取决于标签的编码方式。

    

    本文考虑在低维潜在高斯混合结构和非零噪声的假设模型下针对高维特征的二元分类问题。使用广义最小二乘估计器来估计最佳分离超平面的方向。所估计的超平面在训练数据上被证明是插值的。虽然正如线性回归的最近结果所预期的那样，方向向量可以一致地估计，但一个天真的插件估计未能一致地估计拦截。一个简单的校正需要一个独立的保留样本，在许多场景中可以使过程保持最小化最大值最优。后续过程的插值性质可以保留，但令人惊讶的是，它取决于标签的编码方式。

    This paper considers binary classification of high-dimensional features under a postulated model with a low-dimensional latent Gaussian mixture structure and non-vanishing noise. A generalized least squares estimator is used to estimate the direction of the optimal separating hyperplane. The estimated hyperplane is shown to interpolate on the training data. While the direction vector can be consistently estimated as could be expected from recent results in linear regression, a naive plug-in estimate fails to consistently estimate the intercept. A simple correction, that requires an independent hold-out sample, renders the procedure minimax optimal in many scenarios. The interpolation property of the latter procedure can be retained, but surprisingly depends on the way the labels are encoded.
    
[^25]: 线性随机逼近的Polyak-Ruppert平均迭代的有限时间高概率界限

    Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation. (arXiv:2207.04475v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.04475](http://arxiv.org/abs/2207.04475)

    本文提供了关于带有固定步长的线性随机逼近（LSA）算法的有限时间分析，得到了LSA及其Polyak-Ruppert平均化版本定义的迭代的$p$阶矩和高概率偏差界的推导。本文得到的有限时间实例相关的平均LSA迭代的界限是尖锐的，并与局部渐近极小极值限制相同，同时剩余项对混合时间有着紧密的依赖关系。

    

    本文提供了关于带有固定步长的线性随机逼近（LSA）算法的有限时间分析，这是统计学和机器学习中的一个核心方法。LSA被用来计算$d$维线性系统$\bar{\mathbf{A}}\theta = \bar{\mathbf{b}}$的近似解，其中$(\bar{\mathbf{A}},\bar{\mathbf{b}})$仅能通过（渐近）无偏观测$\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$来估计。我们在这里考虑的情况是$\{Z_n\}_{n \in \mathbb{N}}$是i.i.d.序列或均匀几何遍历马尔可夫链的情况。我们对LSA及其Polyak-Ruppert平均化版本定义的迭代进行$p$阶矩和高概率偏差界的推导。我们获得的有限时间实例相关的平均LSA迭代的界限是尖锐的，因为我们获得的主项与局部渐近极小极值限制相同。此外，我们的界限的剩余项对混合时间有着紧密的依赖关系。

    This paper provides a finite-time analysis of linear stochastic approximation (LSA) algorithms with fixed step size, a core method in statistics and machine learning. LSA is used to compute approximate solutions of a $d$-dimensional linear system $\bar{\mathbf{A}} \theta = \bar{\mathbf{b}}$ for which $(\bar{\mathbf{A}}, \bar{\mathbf{b}})$ can only be estimated by (asymptotically) unbiased observations $\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$. We consider here the case where $\{Z_n\}_{n \in \mathbb{N}}$ is an i.i.d. sequence or a uniformly geometrically ergodic Markov chain. We derive $p$-th moment and high-probability deviation bounds for the iterates defined by LSA and its Polyak-Ruppert-averaged version. Our finite-time instance-dependent bounds for the averaged LSA iterates are sharp in the sense that the leading term we obtain coincides with the local asymptotic minimax limit. Moreover, the remainder terms of our bounds admit a tight dependence on the mixing time 
    
[^26]: Squeeze All：线性上下文Bandit的新估计器和自标准化界限

    Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits. (arXiv:2206.05404v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05404](http://arxiv.org/abs/2206.05404)

    我们提出了一种具有新颖估计量和自标准化界限的线性上下文Bandit算法，具有$O(\sqrt{dT\log T})$遗憾界，维度相关的加法项分解遗憾累积，性能优于现有方法。

    

    我们提出了一种具有$O(\sqrt{dT\log T})$遗憾界的线性上下文Bandit算法，其中$d$是上下文的维数，$T$是时间跨度。我们的算法具有一种新颖的估计量，其中通过显式随机化嵌入了探索。根据随机性，我们的建议估计器可以从所有武器的上下文或从选定的上下文中获得贡献。我们为我们的估计器建立了一个自标准化界限，它允许将累积遗憾分解为基于维度的\textit{可加}项，而不是乘法项。我们还证明了在我们的问题设置下的$\Omega(\sqrt{dT})$的新的下限。因此，我们提出的算法的遗憾匹配下限，直到对数因子。数值实验支持理论保证，并表明我们的方法优于现有的线性Bandit算法。

    We propose a linear contextual bandit algorithm with $O(\sqrt{dT\log T})$ regret bound, where $d$ is the dimension of contexts and $T$ isthe time horizon. Our proposed algorithm is equipped with a novel estimator in which exploration is embedded through explicit randomization. Depending on the randomization, our proposed estimator takes contributions either from contexts of all arms or from selected contexts. We establish a self-normalized bound for our estimator, which allows a novel decomposition of the cumulative regret into \textit{additive} dimension-dependent terms instead of multiplicative terms. We also prove a novel lower bound of $\Omega(\sqrt{dT})$ under our problem setting. Hence, the regret of our proposed algorithm matches the lower bound up to logarithmic factors. The numerical experiments support the theoretical guarantees and show that our proposed method outperforms the existing linear bandit algorithms.
    
[^27]: 深度均衡模型全局收敛性研究

    Global Convergence of Over-parameterized Deep Equilibrium Models. (arXiv:2205.13814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13814](http://arxiv.org/abs/2205.13814)

    本研究探讨了深度均衡模型的训练动态，提出唯一平衡点始终存在且梯度下降的收敛速率为线性收敛到全局最优解，可通过轻微过参数化得到满足。

    

    深度均衡模型（DEQ）通过无限深度的加权-绑定模型中的平衡点与输入注入来隐式定义。它通过根查找直接求解平衡点，并通过隐式微分计算梯度，避免了无限运算。本研究探讨了过参数化DEQ的训练动态。通过对初始平衡点施加一定条件，我们表明唯一平衡点在训练过程中始终存在，并且针对二次损失函数，梯度下降的收敛速率证明为线性收敛到全局最优解。为了展示所需起始条件通过轻微过参数化得到满足，我们在随机DEQ上进行了细粒度的分析。我们提出了一种新的概率框架，以克服非渐近分析无限深度加权绑定模型的技术困难。

    A deep equilibrium model (DEQ) is implicitly defined through an equilibrium point of an infinite-depth weight-tied model with an input-injection. Instead of infinite computations, it solves an equilibrium point directly with root-finding and computes gradients with implicit differentiation. The training dynamics of over-parameterized DEQs are investigated in this study. By supposing a condition on the initial equilibrium point, we show that the unique equilibrium point always exists during the training process, and the gradient descent is proved to converge to a globally optimal solution at a linear convergence rate for the quadratic loss function. In order to show that the required initial condition is satisfied via mild over-parameterization, we perform a fine-grained analysis on random DEQs. We propose a novel probabilistic framework to overcome the technical difficulty in the non-asymptotic analysis of infinite-depth weight-tied models.
    
[^28]: 学习数据流形上的保持身份的变换

    Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12096](http://arxiv.org/abs/2106.12096)

    本论文提出了一种新方法，可以直接从数据流形中学习保持身份的变换，而无需在训练期间标记变换数据。该方法基于变分自编码器，可以学习一组本地自适应的保持身份的变换，可以提高模型在具有挑战性的图像分类和少样本学习任务上的推广性能。

    

    许多机器学习技术将保持身份的变换纳入其模型中，以将其性能推广到以前未见过的数据。然而，这些变换通常是从一组已知可以维持输入身份的函数中选择的（例如旋转、平移、翻转和缩放）。然而，有许多自然变化无法进行标记监督或通过数据检查来定义。如浸入式学习假设所示，许多这些自然变化存在于或靠近低维非线性流形上。几种技术通过一组学习的李群算子来表示流形变化，这些算子定义了流形上的运动方向。然而，这些方法的局限在于它们在训练其模型时需要变换标签，并且缺乏一种确定每个特定算子适用于流形的哪些区域的方法。我们通过提出一种新的方法来解决这些限制，该方法直接从数据流形中学习保持身份的变换，而无需在训练过程中标记变换数据。我们的方法基于一种修改的变分自编码器，该自编码器在端到端训练中学习生成模型和一组本地自适应的保持身份的变换。我们的实验表明，我们的方法能够学习有用的变换，从而改善模型在具有挑战性的图像分类和少样本学习任务中的推广性能。

    Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However, these approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitati
    

