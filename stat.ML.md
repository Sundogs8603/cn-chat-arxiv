# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improved motif-scaffolding with SE(3) flow matching.](http://arxiv.org/abs/2401.04082) | 本文提出了一种使用SE(3)流匹配的图案支架方法，通过图案摊销和图案引导两种方法，可以生成结构上多样性更高的支架，与之前的最先进方法相比，成功率相当甚至更高。 |
| [^2] | [Fun with Flags: Robust Principal Directions via Flag Manifolds.](http://arxiv.org/abs/2401.04071) | 本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。 |
| [^3] | [Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems.](http://arxiv.org/abs/2401.04013) | 本文研究了梯度下降学习算法在参数动力学中的线性结构，发现这种线性化现象是由于初始值附近假设函数的一阶和高阶导数之间的弱相关性所致。这一发现为深度学习模型的线性化提供了新的认识。 |
| [^4] | [A non-asymptotic distributional theory of approximate message passing for sparse and robust regression.](http://arxiv.org/abs/2401.03923) | 这项研究通过开发一种非渐近分布特征，为稀疏和鲁棒回归问题的近似传递消息算法（AMP）建立了有限样本非渐近分布理论，包括多项式次数的迭代。 |
| [^5] | [Design a Metric Robust to Complicated High Dimensional Noise for Efficient Manifold Denoising.](http://arxiv.org/abs/2401.03921) | 本文设计了一个对复杂高维噪声鲁棒的度量方法，用于高效的流形去噪，可以灵活处理多种设置，并在模拟和真实数据上进行了系统比较。 |
| [^6] | [Finite-Time Decoupled Convergence in Nonlinear Two-Time-Scale Stochastic Approximation.](http://arxiv.org/abs/2401.03893) | 本研究探讨了非线性双时间尺度随机逼近中有限时间解耦收敛的潜力，并通过引入嵌套局部线性条件证明了其可行性。 |
| [^7] | [Sampling in Unit Time with Kernel Fisher-Rao Flow.](http://arxiv.org/abs/2401.03892) | 本文提出了一种具有核Fisher-Rao流的新方法，在单位时间内从非归一化目标密度或贝叶斯后验中进行采样。方法使用了均场ODE和相互作用粒子系统，无需梯度，只需要能够从参考密度中采样并计算目标对参考密度的比率。该方法通过在几何混合的路径上沿速度场运输样本，径向输运样本。方法通过在再生核希尔伯特空间中求解泊松方程，使泊松方程的求解变得可行，并将其离散化为有限样本的均场ODE，作为实现简单的相互作用粒子系统。同时，这种方法也可以从离散时间的角度推导出均场ODE，作为蒙杰-安普尔方程连续线性化的极限。 |
| [^8] | [A topological description of loss surfaces based on Betti Numbers.](http://arxiv.org/abs/2401.03824) | 本文提出了一种基于Betti数的拓扑度量，用于评估多层神经网络中损失的复杂性，并发现复杂性受到隐藏单元数量、训练模型和激活函数的影响。 |
| [^9] | [Optimal Differentially Private PCA and Estimation for Spiked Covariance Matrices.](http://arxiv.org/abs/2401.03820) | 本文研究了在尖峰协方差模型中的最优差分隐私主成分分析和协方差估计问题，并提出了高效的差分隐私估计器，并证明了它们的最小最大性。 |
| [^10] | [Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning.](http://arxiv.org/abs/2401.03756) | 该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。 |
| [^11] | [Uncertainty Quantification on Clinical Trial Outcome Prediction.](http://arxiv.org/abs/2401.03482) | 本研究将不确定性量化方法应用于临床试验结果预测，提高模型对微妙差异的识别能力，从而改善其整体性能。 |
| [^12] | [Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks.](http://arxiv.org/abs/2401.03350) | 提出了G-$\Delta$UQ，一种新的训练框架，旨在改善图神经网络（GNN）的内在不确定性估计。该框架通过图锚定策略将随机数据中心化应用于图数据，并且能够支持部分随机的GNN。 |
| [^13] | [Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection.](http://arxiv.org/abs/2401.03341) | 本论文提出了一种将变分自编码器（VAEs）和自监督学习（SSL）相结合的新颖生成框架，用于解决在时间序列异常检测（TSAD）中由于数据稀缺引起的潜在空间的不连续性导致的重建不稳定性的问题。 |
| [^14] | [Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT.](http://arxiv.org/abs/2401.03302) | 本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。 |
| [^15] | [TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR.](http://arxiv.org/abs/2401.03251) | 本论文提出了一种用于估计端到端ASR信任度的时态词元相似度分数TeLeS，并用缩减损失来解决CEM训练中目标得分数据不平衡的问题。 |
| [^16] | [Neuronal Temporal Filters as Normal Mode Extractors.](http://arxiv.org/abs/2401.03248) | 本论文探讨了神经元如何通过预测时间序列的未来来生成行为。通过正常模态分解可以实现简单的预测，其中神经元通过学习顶层模态并将其输入投影到相关子空间。根据信噪比的不同，时间滤波器的形状会有所变化。 |
| [^17] | [Reflected Schr\"odinger Bridge for Constrained Generative Modeling.](http://arxiv.org/abs/2401.03228) | 本研究提出了一种用于在有界域内生成数据的熵正则化最优输运方法，通过推导出带有边界条件的反射正向-反向随机微分方程，解决了反射扩散模型在适应多样性领域时的限制。 |
| [^18] | [A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence.](http://arxiv.org/abs/2401.03206) | 提出了一种新的方法，通过引入先验信息，改进了Robbins-Monro算法的收敛速度。结果表明，先验信息的Robbins-Monro序列比标准序列收敛更快，特别是在前几步。 |
| [^19] | [SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning.](http://arxiv.org/abs/2401.03137) | SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。 |
| [^20] | [Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate.](http://arxiv.org/abs/2401.03058) | 本论文提出了一种新的子空间立方正则化牛顿方法，可以在解决凸优化问题时实现无维度相关的全局收敛速度，通过在低维子空间上进行二阶更新，克服了高维问题中内存需求和计算成本大的问题。 |
| [^21] | [A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning.](http://arxiv.org/abs/2401.02325) | 这篇论文提出了一种鲁棒的分位数Huber损失函数，在分布强化学习中通过捕捉噪声并调整参数来增强对异常值的鲁棒性。实证测试验证了该方法的有效性。 |
| [^22] | [Hierarchical Clustering in ${\Lambda}$CDM Cosmologies via Persistence Energy.](http://arxiv.org/abs/2401.01988) | 通过持续能量方法研究了${\Lambda}$CDM宇宙学中的层次聚类，发现持续能量与红移值之间存在相关性，揭示了宇宙结构的动力学特征。 |
| [^23] | [Scalable network reconstruction in subquadratic time.](http://arxiv.org/abs/2401.01404) | 这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。 |
| [^24] | [Safe Multi-Task Bayesian Optimization.](http://arxiv.org/abs/2312.07281) | 这项研究将鲁棒的高斯过程均匀误差界限扩展到多任务设置中，以解决安全在线优化中超参数未知的问题。 |
| [^25] | [Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference.](http://arxiv.org/abs/2312.05910) | 这篇论文介绍了一种将集合卡尔曼滤波引入变分推理框架的方法，用于近似高斯过程状态空间模型的后验分布，并且有效地利用了潜在状态和动力学之间的依赖关系，减少了变分参数的数量。 |
| [^26] | [Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study.](http://arxiv.org/abs/2311.15051) | 本研究通过实验证明，带有大学习率和学习率预热的动量梯度下降显示出大型弹射效应，将迭代朝着比梯度下降发现的更平缓的极小值方向推进。 |
| [^27] | [Differentially Private Permutation Tests: Applications to Kernel Methods.](http://arxiv.org/abs/2310.19043) | 本文提出了差分隐私排列检验的框架，扩展了经典的非私有排列检验，以在私有环境中保持有限样本有效性和差分隐私性质。该检验的功率取决于检验统计量的选择，并建立了一般条件来保证一致性和非渐进均匀的功率。 |
| [^28] | [Boosting Data Analytics With Synthetic Volume Expansion.](http://arxiv.org/abs/2310.17848) | 本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。 |
| [^29] | [Structured Learning in Time-dependent Cox Models.](http://arxiv.org/abs/2306.12528) | 本文提出了一个灵活的框架，用于在时间相关的Cox模型中进行变量选择，并适应各种复杂的分组结构，具有较低的误警率和快速的计算。 |
| [^30] | [Conditional expectation via compact kernels.](http://arxiv.org/abs/2306.10592) | 本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。 |
| [^31] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^32] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^33] | [Evaluating Self-Supervised Learning via Risk Decomposition.](http://arxiv.org/abs/2302.03068) | 通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。 |
| [^34] | [Compression, Generalization and Learning.](http://arxiv.org/abs/2301.12767) | 本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。 |
| [^35] | [ddml: Double/debiased machine learning in Stata.](http://arxiv.org/abs/2301.09397) | ddml是Stata中的双重/无偏机器学习包，支持五种不同计量模型的因果参数估计，可以灵活估计内生变量的因果效应，在许多现有监督机器学习程序中兼容。推荐与堆叠估计结合使用，提供了蒙特卡洛证据支持。 |
| [^36] | [The Survival Bandit Problem.](http://arxiv.org/abs/2206.03019) | 这个研究介绍了生存强盗问题，这是多臂赌博机问题的一个新变种。该问题的目标是最小化生存遗憾，同时要求算法加速收敛。 |
| [^37] | [Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold.](http://arxiv.org/abs/2205.11677) | 该论文提出了两种有效的算法来将标签信息与稀疏图结构相结合，解决了基于网络拓扑的聚类在稀疏图上的问题。 |
| [^38] | [Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities.](http://arxiv.org/abs/2107.11869) | 本文介绍了两种基于数据驱动的方法，用于非参数模型中使用工具变量的最优估计和推断；首先是选择最佳筛选维数的估计器，使得结构函数和其导数的估计器以最小化超范数的收敛速度收敛；其次是构建均匀置信带来获取结构函数和其导数的置信区间。模拟结果表明了这些方法的良好性能。 |
| [^39] | [A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines.](http://arxiv.org/abs/2004.05839) | 本文研究了松弛优化理论，探讨了风险与复杂度之间的联系，提出了可以从复杂度估计风险的方法，研究了支持向量方法在机器学习中的应用。 |

# 详细

[^1]: 使用SE(3)流匹配改进了图案支架技术

    Improved motif-scaffolding with SE(3) flow matching. (arXiv:2401.04082v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.04082](http://arxiv.org/abs/2401.04082)

    本文提出了一种使用SE(3)流匹配的图案支架方法，通过图案摊销和图案引导两种方法，可以生成结构上多样性更高的支架，与之前的最先进方法相比，成功率相当甚至更高。

    

    蛋白质设计通常从一个图案的期望功能开始，图案支架旨在构建一个功能性蛋白质。最近，生成模型在设计各种图案的支架方面取得了突破性的成功。然而，生成的支架往往缺乏结构多样性，这可能会影响湿实验验证的成功。在这项工作中，我们将FrameFlow，一种用于蛋白质主链生成的SE(3)流匹配模型扩展到使用两种互补的方法进行图案支架。第一种方法是图案摊销，即使用数据增强策略，将FrameFlow训练为以图案为输入。第二种方法是图案引导，它使用FrameFlow的条件分数估计进行支架构建，并且不需要额外的训练。这两种方法的成功率与之前的最先进方法相当或更高，并且可以产生结构上多样性更高2.5倍的支架。

    Protein design often begins with knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a diverse range of motifs. However, the generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow, and requires no additional training. Both approaches achieve an equivalent or higher success rate than previous state-of-the-art methods, with 2.5 times more structurally diverse scaffolds. Code: https://github.com/ mi
    
[^2]: 旗帜游戏：通过旗帜流形来获得鲁棒的主方向

    Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])

    [http://arxiv.org/abs/2401.04071](http://arxiv.org/abs/2401.04071)

    本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。

    

    主成分分析（PCA）及其对流形和异常数据的扩展，在计算机视觉和机器学习中是不可或缺的。本研究提出了PCA及其变种的统一形式，引入了基于线性子空间旗帜的框架，即逐渐增加维度的嵌套线性子空间的层次结构，不仅允许共同实现，还产生了新的未曾探索的变种。我们从广义化传统的PCA方法开始，这些方法要么最大化方差，要么最小化重构误差。我们扩展这些解释，通过考虑异常值和数据流形，开发出了大量新的降维算法。为了设计一种通用的计算方法，我们将鲁棒和对偶形式的PCA重新构建为在旗帜流形上的优化问题。然后，我们将主测地线近似（切线PCA）整合到这个基于旗帜的框架中，创造出一种新的方法。

    Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
    
[^3]: 以弱相关性作为梯度下降学习系统线性化的基本原则

    Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems. (arXiv:2401.04013v1 [cs.LG])

    [http://arxiv.org/abs/2401.04013](http://arxiv.org/abs/2401.04013)

    本文研究了梯度下降学习算法在参数动力学中的线性结构，发现这种线性化现象是由于初始值附近假设函数的一阶和高阶导数之间的弱相关性所致。这一发现为深度学习模型的线性化提供了新的认识。

    

    深度学习模型，如宽神经网络，可以被概念化为非线性动力学物理系统，其具有多个相互作用的自由度。在无限极限下，这些系统趋向于表现出简化的动力学。本文深入研究了基于梯度下降的学习算法，在其参数动力学中展示出与神经切向核类似的线性结构。我们发现，这种明显的线性化是因为在初始值附近，假设函数的一阶和高阶导数之间的弱相关性。这一洞见表明，这些弱相关性可能是此类系统中观察到的线性化的潜在原因。作为一个例证，我们展示了在宽度很大的神经网络中存在的这种弱相关性结构。利用线性和弱相关性之间的关系，我们推导出线性度偏离的一个界限。

    Deep learning models, such as wide neural networks, can be conceptualized as nonlinear dynamical physical systems characterized by a multitude of interacting degrees of freedom. Such systems in the infinite limit, tend to exhibit simplified dynamics. This paper delves into gradient descent-based learning algorithms, that display a linear structure in their parameter dynamics, reminiscent of the neural tangent kernel. We establish this apparent linearity arises due to weak correlations between the first and higher-order derivatives of the hypothesis function, concerning the parameters, taken around their initial values. This insight suggests that these weak correlations could be the underlying reason for the observed linearization in such systems. As a case in point, we showcase this weak correlations structure within neural networks in the large width limit. Exploiting the relationship between linearity and weak correlations, we derive a bound on deviations from linearity observed duri
    
[^4]: 一种非渐近分布理论的近似传递消息算法用于稀疏和鲁棒回归问题

    A non-asymptotic distributional theory of approximate message passing for sparse and robust regression. (arXiv:2401.03923v1 [math.ST])

    [http://arxiv.org/abs/2401.03923](http://arxiv.org/abs/2401.03923)

    这项研究通过开发一种非渐近分布特征，为稀疏和鲁棒回归问题的近似传递消息算法（AMP）建立了有限样本非渐近分布理论，包括多项式次数的迭代。

    

    由于经典渐近理论在高维度中失效，表征高维统计估计量的分布是一项具有挑战性的任务。本文通过为近似传递消息算法（AMP）开发非渐近分布特征来取得进展，这是一类既作为快速估计器又作为强大的理论工具的迭代算法，用于稀疏和鲁棒回归问题。之前的AMP理论主要关注高维渐近性，未能描述当迭代次数超过$o\big({\log n}/{\log \log n}\big)$时（其中$n$是样本大小）AMP的行为。我们建立了首个适用于稀疏和鲁棒回归的AMP有限样本非渐近分布理论，包括多项式次数的迭代。我们的结果得到了AMP迭代的高斯近似的近似精度，改进了之前的所有结果，并暗示着

    Characterizing the distribution of high-dimensional statistical estimators is a challenging task, due to the breakdown of classical asymptotic theory in high dimension. This paper makes progress towards this by developing non-asymptotic distributional characterizations for approximate message passing (AMP) -- a family of iterative algorithms that prove effective as both fast estimators and powerful theoretical machinery -- for both sparse and robust regression. Prior AMP theory, which focused on high-dimensional asymptotics for the most part, failed to describe the behavior of AMP when the number of iterations exceeds $o\big({\log n}/{\log \log n}\big)$ (with $n$ the sample size). We establish the first finite-sample non-asymptotic distributional theory of AMP for both sparse and robust regression that accommodates a polynomial number of iterations. Our results derive approximate accuracy of Gaussian approximation of the AMP iterates, which improves upon all prior results and implies e
    
[^5]: 设计一个对复杂高维噪声鲁棒的度量方法以实现高效的流形去噪

    Design a Metric Robust to Complicated High Dimensional Noise for Efficient Manifold Denoising. (arXiv:2401.03921v1 [stat.ML])

    [http://arxiv.org/abs/2401.03921](http://arxiv.org/abs/2401.03921)

    本文设计了一个对复杂高维噪声鲁棒的度量方法，用于高效的流形去噪，可以灵活处理多种设置，并在模拟和真实数据上进行了系统比较。

    

    在这篇论文中，我们提出了一种基于地标扩散和最优收缩的高效流形去噪方法，适用于复杂的高维噪声和紧凑流形设置。它可以灵活处理多种设置，包括高环境空间维度的流形嵌入（占据高维或低维子空间）以及可能是有色且相关的噪声。我们在模拟和真实数据集上对该方法与其他现有算法进行了系统比较。本论文主要是算法性质的研究，我们报告了一些现有工具和数值结果。更多的理论保证和比较将在正式论文中报告。

    In this manuscript, we propose an efficient manifold denoiser based on landmark diffusion and optimal shrinkage under the complicated high dimensional noise and compact manifold setup. It is flexible to handle several setups, including the high ambient space dimension with a manifold embedding that occupies a subspace of high or low dimensions, and the noise could be colored and dependent. A systematic comparison with other existing algorithms on both simulated and real datasets is provided. This manuscript is mainly algorithmic and we report several existing tools and numerical results. Theoretical guarantees and more comparisons will be reported in the official paper of this manuscript.
    
[^6]: 非线性双时间尺度随机逼近中的有限时间解耦收敛

    Finite-Time Decoupled Convergence in Nonlinear Two-Time-Scale Stochastic Approximation. (arXiv:2401.03893v1 [math.OC])

    [http://arxiv.org/abs/2401.03893](http://arxiv.org/abs/2401.03893)

    本研究探讨了非线性双时间尺度随机逼近中有限时间解耦收敛的潜力，并通过引入嵌套局部线性条件证明了其可行性。

    

    在双时间尺度随机逼近中，使用不同的步长以不同的速度更新两个迭代，每次更新都会影响另一个。先前的线性双时间尺度随机逼近研究发现，这些更新的均方误差的收敛速度仅仅取决于它们各自的步长，导致了所谓的解耦收敛。然而，在非线性随机逼近中实现这种解耦收敛的可能性仍不明确。我们的研究探讨了非线性双时间尺度随机逼近中有限时间解耦收敛的潜力。我们发现，在较弱的Lipschitz条件下，传统分析无法实现解耦收敛。这一发现在数值上得到了进一步的支持。但是通过引入一个嵌套局部线性条件，我们证明了在适当选择与平滑度相关的步长的情况下，解耦收敛仍然是可行的。

    In two-time-scale stochastic approximation (SA), two iterates are updated at varying speeds using different step sizes, with each update influencing the other. Previous studies in linear two-time-scale SA have found that the convergence rates of the mean-square errors for these updates are dependent solely on their respective step sizes, leading to what is referred to as decoupled convergence. However, the possibility of achieving this decoupled convergence in nonlinear SA remains less understood. Our research explores the potential for finite-time decoupled convergence in nonlinear two-time-scale SA. We find that under a weaker Lipschitz condition, traditional analyses are insufficient for achieving decoupled convergence. This finding is further numerically supported by a counterexample. But by introducing an additional condition of nested local linearity, we show that decoupled convergence is still feasible, contingent on the appropriate choice of step sizes associated with smoothnes
    
[^7]: 以核Fisher-Rao流进行单位时间采样

    Sampling in Unit Time with Kernel Fisher-Rao Flow. (arXiv:2401.03892v1 [stat.CO])

    [http://arxiv.org/abs/2401.03892](http://arxiv.org/abs/2401.03892)

    本文提出了一种具有核Fisher-Rao流的新方法，在单位时间内从非归一化目标密度或贝叶斯后验中进行采样。方法使用了均场ODE和相互作用粒子系统，无需梯度，只需要能够从参考密度中采样并计算目标对参考密度的比率。该方法通过在几何混合的路径上沿速度场运输样本，径向输运样本。方法通过在再生核希尔伯特空间中求解泊松方程，使泊松方程的求解变得可行，并将其离散化为有限样本的均场ODE，作为实现简单的相互作用粒子系统。同时，这种方法也可以从离散时间的角度推导出均场ODE，作为蒙杰-安普尔方程连续线性化的极限。

    

    我们引入了一种新的均场ODE和相应的相互作用粒子系统，用于从非归一化的目标密度或贝叶斯后验中进行采样。相互作用粒子系统无需梯度，可以闭合形式获得，并且只需要能够从参考密度中采样并计算（非归一化的）目标对参考密度的比率。通过求解运输样本沿两个密度的几何混合的速度场的泊松方程来获得均场ODE，这是一种特定的Fisher-Rao梯度流的路径。我们采用再生核希尔伯特空间方法来获得速度场的泊松方程，这使得泊松方程可处理，并使我们能够离散化有限样本的结果均场ODE，形成一个简单的相互作用粒子系统。均场ODE还可以通过离散时间视角从蒙杰-安普尔方程的连续线性化的极限中推导出来，这在一个已知的框架内进行。

    We introduce a new mean-field ODE and corresponding interacting particle systems for sampling from an unnormalized target density or Bayesian posterior. The interacting particle systems are gradient-free, available in closed form, and only require the ability to sample from the reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular Fisher-Rao gradient flow. We employ a reproducing kernel Hilbert space ansatz for the velocity field, which makes the Poisson equation tractable and enables us to discretize the resulting mean-field ODE over finite samples, as a simple interacting particle system. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Amp\`ere equations within a framework known 
    
[^8]: 基于Betti数的损失曲面的拓扑描述

    A topological description of loss surfaces based on Betti Numbers. (arXiv:2401.03824v1 [cs.LG])

    [http://arxiv.org/abs/2401.03824](http://arxiv.org/abs/2401.03824)

    本文提出了一种基于Betti数的拓扑度量，用于评估多层神经网络中损失的复杂性，并发现复杂性受到隐藏单元数量、训练模型和激活函数的影响。

    

    在深度学习模型的背景下，研究损失函数的曲面已经引起了人们的关注，以更好地理解基于梯度下降的训练方法。这种对一个合适的描述的寻求，既包括分析又包括拓扑，已经导致了大量努力来识别虚假最小值和表征梯度动态。我们的工作旨在为这一领域做出贡献，通过提供一个拓扑度量来评估多层神经网络中损失的复杂性。我们通过推导深层和浅层架构的损失函数复杂性的上下界以及揭示这种复杂性如何受隐藏单元数量、训练模型和激活函数影响，比较了使用常见的Sigmoid激活函数的深层和浅层架构。此外，我们发现了损失函数或模型架构的某些变化，比如在前馈网络中添加$\ell_2$正则化项或实施跳跃连接

    In the context of deep learning models, attention has recently been paid to studying the surface of the loss function in order to better understand training with methods based on gradient descent. This search for an appropriate description, both analytical and topological, has led to numerous efforts to identify spurious minima and characterize gradient dynamics. Our work aims to contribute to this field by providing a topological measure to evaluate loss complexity in the case of multilayer neural networks. We compare deep and shallow architectures with common sigmoidal activation functions by deriving upper and lower bounds on the complexity of their loss function and revealing how that complexity is influenced by the number of hidden units, training models, and the activation function used. Additionally, we found that certain variations in the loss function or model architecture, such as adding an $\ell_2$ regularization term or implementing skip connections in a feedforward network
    
[^9]: 在带有尖峰协方差矩阵中的最优差分隐私主成分分析和估计

    Optimal Differentially Private PCA and Estimation for Spiked Covariance Matrices. (arXiv:2401.03820v1 [math.ST])

    [http://arxiv.org/abs/2401.03820](http://arxiv.org/abs/2401.03820)

    本文研究了在尖峰协方差模型中的最优差分隐私主成分分析和协方差估计问题，并提出了高效的差分隐私估计器，并证明了它们的最小最大性。

    

    在当代统计学中，估计协方差矩阵及其相关的主成分是一个基本问题。尽管已开发出具有良好性质的最优估计程序，但对隐私保护的增加需求给这个经典问题引入了新的复杂性。本文研究了在尖峰协方差模型中的最优差分隐私主成分分析（PCA）和协方差估计。我们精确地刻画了在该模型下特征值和特征向量的敏感性，并建立了估计主成分和协方差矩阵的最小最大收敛率。这些收敛率包括一般的Schatten范数，包括谱范数，Frobenius范数和核范数。我们引入了计算高效的差分隐私估计器，并证明它们的最小最大性，直到对数因子。另外，匹配的minimax最小最大率也得到了证明。

    Estimating a covariance matrix and its associated principal components is a fundamental problem in contemporary statistics. While optimal estimation procedures have been developed with well-understood properties, the increasing demand for privacy preservation introduces new complexities to this classical problem. In this paper, we study optimal differentially private Principal Component Analysis (PCA) and covariance estimation within the spiked covariance model.  We precisely characterize the sensitivity of eigenvalues and eigenvectors under this model and establish the minimax rates of convergence for estimating both the principal components and covariance matrix. These rates hold up to logarithmic factors and encompass general Schatten norms, including spectral norm, Frobenius norm, and nuclear norm as special cases.  We introduce computationally efficient differentially private estimators and prove their minimax optimality, up to logarithmic factors. Additionally, matching minimax l
    
[^10]: 上下文固定预算的最佳臂识别：适应性实验设计与策略学习

    Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])

    [http://arxiv.org/abs/2401.03756](http://arxiv.org/abs/2401.03756)

    该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。

    

    个性化治疗推荐是基于证据的决策中的关键任务。在这项研究中，我们将这个任务作为一个带有上下文信息的固定预算最佳臂识别（Best Arm Identification, BAI）问题来进行建模。在这个设置中，我们考虑了一个给定多个治疗臂的自适应试验。在每一轮中，决策者观察一个刻画实验单位的上下文（协变量），并将该单位分配给其中一个治疗臂。在实验结束时，决策者推荐一个在给定上下文条件下预计产生最高期望结果的治疗臂（最佳治疗臂）。该决策的有效性通过最坏情况下的期望简单遗憾（策略遗憾）来衡量，该遗憾表示在给定上下文条件下，最佳治疗臂和推荐治疗臂的条件期望结果之间的最大差异。我们的初始步骤是推导最坏情况下期望简单遗憾的渐近下界，该下界还暗示着解决该问题的一些思路。

    Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
    
[^11]: 临床试验结果预测中的不确定性量化

    Uncertainty Quantification on Clinical Trial Outcome Prediction. (arXiv:2401.03482v1 [cs.LG])

    [http://arxiv.org/abs/2401.03482](http://arxiv.org/abs/2401.03482)

    本研究将不确定性量化方法应用于临床试验结果预测，提高模型对微妙差异的识别能力，从而改善其整体性能。

    

    不确定性量化在机器学习的不同领域中的重要性日益被认识到。准确评估模型预测的不确定性可以帮助研究人员和从业人员更深入地理解和增加信心。这在医学诊断和药物发现领域尤为重要，因为可靠的预测直接影响研究质量和患者健康。本文提出将不确定性量化纳入临床试验结果预测中。我们的主要目标是提高模型辨别微妙差异的能力，从而显著改善其整体性能。我们采用了一种选择性分类方法来实现我们的目标，并将其与层次交互网络(HINT)无缝集成，HINT是临床试验预测建模的最前沿。选择性分类涵盖了一系列不确定性量化方法，使模型能够保留信息以供进一步分析。

    The importance of uncertainty quantification is increasingly recognized in the diverse field of machine learning. Accurately assessing model prediction uncertainty can help provide deeper understanding and confidence for researchers and practitioners. This is especially critical in medical diagnosis and drug discovery areas, where reliable predictions directly impact research quality and patient health.  In this paper, we proposed incorporating uncertainty quantification into clinical trial outcome predictions. Our main goal is to enhance the model's ability to discern nuanced differences, thereby significantly improving its overall performance.  We have adopted a selective classification approach to fulfill our objective, integrating it seamlessly with the Hierarchical Interaction Network (HINT), which is at the forefront of clinical trial prediction modeling. Selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold de
    
[^12]: 准确可扩展的图神经网络表观不确定性估计

    Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2401.03350v1 [cs.LG])

    [http://arxiv.org/abs/2401.03350](http://arxiv.org/abs/2401.03350)

    提出了G-$\Delta$UQ，一种新的训练框架，旨在改善图神经网络（GNN）的内在不确定性估计。该框架通过图锚定策略将随机数据中心化应用于图数据，并且能够支持部分随机的GNN。

    

    尽管图神经网络（GNN）广泛用于节点和图表示学习任务，但在分布变化下GNN不确定性估计的可靠性仍相对较少探索。事实上，虽然事后校准策略可以用于改善内部分布校准，但它们不一定也能改进分布变化下的校准。然而，产生更好的内部不确定性估计的技术尤其有价值，因为它们可以随后与事后策略结合使用。因此，在本研究中，我们提出了一种名为G-$\Delta$UQ的新型训练框架，旨在改善内在的GNN不确定性估计。我们的框架通过新颖的图锚定策略将随机数据中心化原则应用于图数据，并能够支持部分随机的GNN。虽然主流观点是为了获得可靠的估计，需要完全随机网络，但我们发现通过功能多样性引入的中观锚定可以在保证准确性的同时降低计算成本。

    While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced b
    
[^13]: 弱增强变分自编码器在时间序列异常检测中的应用

    Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection. (arXiv:2401.03341v1 [cs.LG])

    [http://arxiv.org/abs/2401.03341](http://arxiv.org/abs/2401.03341)

    本论文提出了一种将变分自编码器（VAEs）和自监督学习（SSL）相结合的新颖生成框架，用于解决在时间序列异常检测（TSAD）中由于数据稀缺引起的潜在空间的不连续性导致的重建不稳定性的问题。

    

    由于其无监督训练和不确定性估计，深度变分自编码器（VAEs）已经成为基于重建的时间序列异常检测（TSAD）的强大工具。现有的基于VAE的TSAD方法，不论是统计方法还是深度方法，都调整元先验以估计有效捕获数据中的时空依赖关系的似然概率。然而，这些方法面临着内在数据稀缺的挑战，这在异常检测任务中经常出现。这种稀缺容易导致潜在空间中的潜在空洞，即潜在空间中的不连续区域，导致在这些不连续的空间上的非鲁棒重构。我们提出了一种将VAEs与自监督学习（SSL）结合的新颖生成框架来解决这个问题。

    Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks. Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue.
    
[^14]: 行动中的现实主义：使用YOLOv8和DeiT从医学图像中诊断脑肿瘤的异常感知

    Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])

    [http://arxiv.org/abs/2401.03302](http://arxiv.org/abs/2401.03302)

    本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。

    

    在医学科学领域，由于脑肿瘤在患者中的罕见程度，可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者结果至关重要。本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤。来自国家脑映射实验室（NBML）的精选数据集包括81名患者，其中包括30例肿瘤病例和51例正常病例。检测和分类流程被分为两个连续的任务。检测阶段包括全面的数据分析和预处理，以修改图像样本和每个类别的患者数量，以符合真实世界场景中的异常分布（9个正常样本对应1个肿瘤样本）。此外，在测试中除了常见的评估指标外，我们还采用了... [摘要长度已达到上限]

    In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
    
[^15]: TeLeS：用于估计端到端ASR信任度的时态词元相似度分数

    TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR. (arXiv:2401.03251v1 [eess.AS])

    [http://arxiv.org/abs/2401.03251](http://arxiv.org/abs/2401.03251)

    本论文提出了一种用于估计端到端ASR信任度的时态词元相似度分数TeLeS，并用缩减损失来解决CEM训练中目标得分数据不平衡的问题。

    

    从端到端（E2E）自动语音识别（ASR）模型的预测中得出信心估计有助于ASR的下游和上游任务。基于类别概率的置信度得分不能准确地表示过于自信的ASR预测的质量。辅助置信度估计模型（CEM）可校准这些预测。最先进的解决方案使用二进制目标得分进行CEM训练。然而，二进制标签不能揭示预测词的细粒度信息，如参考语音和假设语音之间的时态对齐以及预测词是否完全错误或包含拼写错误。为解决这个问题，我们提出了一种新的时态词元相似度（TeLeS）置信度分数来训练CEM。为了解决CEM训练中目标得分的数据不平衡问题，我们使用缩减损失来聚焦于难以学习的数据点并最小化轻易学习的数据点的影响。我们在三种语言训练的ASR模型上进行了实验。

    Confidence estimation of predictions from an End-to-End (E2E) Automatic Speech Recognition (ASR) model benefits ASR's downstream and upstream tasks. Class-probability-based confidence scores do not accurately represent the quality of overconfident ASR predictions. An ancillary Confidence Estimation Model (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use binary target scores for CEM training. However, the binary labels do not reveal the granular information of predicted words, such as temporal alignment between reference and hypothesis and whether the predicted word is entirely incorrect or contains spelling errors. Addressing this issue, we propose a novel Temporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address the data imbalance of target scores while training CEM, we use shrinkage loss to focus on hard-to-learn data points and minimise the impact of easily learned data points. We conduct experiments with ASR models trained in three languages
    
[^16]: 神经元时间滤波器作为正常模态提取器

    Neuronal Temporal Filters as Normal Mode Extractors. (arXiv:2401.03248v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.03248](http://arxiv.org/abs/2401.03248)

    本论文探讨了神经元如何通过预测时间序列的未来来生成行为。通过正常模态分解可以实现简单的预测，其中神经元通过学习顶层模态并将其输入投影到相关子空间。根据信噪比的不同，时间滤波器的形状会有所变化。

    

    为了在生理延迟情况下产生行为，大脑必须预测未来。在这里，我们通过考虑一个神经元对标量时间序列输入的未来进行预测，探讨了预测可能是大脑功能的核心。假设滞后向量（由时间序列的若干连续元素组成的向量）的动力学是局部线性的，正常模态分解将动力学分解为相互独立演化的（特征）模态，从而实现简单的预测。我们提出，神经元学习顶层模态，并将其输入投影到相关子空间。根据这个解释，神经元的时间滤波器对应于广义特征值问题的左特征向量。我们在由线性系统生成的合成数据的噪声观测上对此算法的操作进行了数学分析。有趣的是，时间滤波器的形状随信噪比（SNR）而变化：嘈杂的输入产生一个单相滤波器。

    To generate actions in the face of physiological delays, the brain must predict the future. Here we explore how prediction may lie at the core of brain function by considering a neuron predicting the future of a scalar time series input. Assuming that the dynamics of the lag vector (a vector composed of several consecutive elements of the time series) are locally linear, Normal Mode Decomposition decomposes the dynamics into independently evolving (eigen-)modes allowing for straightforward prediction. We propose that a neuron learns the top mode and projects its input onto the associated subspace. Under this interpretation, the temporal filter of a neuron corresponds to the left eigenvector of a generalized eigenvalue problem. We mathematically analyze the operation of such an algorithm on noisy observations of synthetic data generated by a linear system. Interestingly, the shape of the temporal filter varies with the signal-to-noise ratio (SNR): a noisy input yields a monophasic filte
    
[^17]: 有约束生成建模的反射Schr\"odinger桥算法

    Reflected Schr\"odinger Bridge for Constrained Generative Modeling. (arXiv:2401.03228v1 [stat.ML])

    [http://arxiv.org/abs/2401.03228](http://arxiv.org/abs/2401.03228)

    本研究提出了一种用于在有界域内生成数据的熵正则化最优输运方法，通过推导出带有边界条件的反射正向-反向随机微分方程，解决了反射扩散模型在适应多样性领域时的限制。

    

    扩散模型已经成为实际应用中大规模生成模型的首选方法。这些应用通常涉及在有界域内限制的数据分布，通常需要 ad-hoc 阈值技术来强制边界。反射扩散模型旨在通过由反射布朗运动控制的后向过程生成数据分布，以增加泛化能力。然而，反射扩散模型可能不容易适应各种领域，需要正确导出的差同胚映射，并且不能保证最优输运特性。为了克服这些限制，我们引入了反射Schr\"odinger桥算法：一种用于在各种有界域内生成数据的熵正则化最优输运方法。我们推导了带有 Neumann 和 Robin 边界条件的优雅反射正向-反向随机微分方程，并将基于散度的似然训练扩展到有界领域。

    Diffusion models have become the go-to method for large-scale generative models in real-world applications. These applications often involve data distributions confined within bounded domains, typically requiring ad-hoc thresholding techniques for boundary enforcement. Reflected diffusion models (Lou23) aim to enhance generalizability by generating the data distribution through a backward process governed by reflected Brownian motion. However, reflected diffusion models may not easily adapt to diverse domains without the derivation of proper diffeomorphic mappings and do not guarantee optimal transport properties. To overcome these limitations, we introduce the Reflected Schrodinger Bridge algorithm: an entropy-regularized optimal transport approach tailored for generating data within diverse bounded domains. We derive elegant reflected forward-backward stochastic differential equations with Neumann and Robin boundary conditions, extend divergence-based likelihood training to bounded d
    
[^18]: 一种可以利用先验信息加快收敛速度的Robbins-Monro序列

    A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence. (arXiv:2401.03206v1 [cs.LG])

    [http://arxiv.org/abs/2401.03206](http://arxiv.org/abs/2401.03206)

    提出了一种新的方法，通过引入先验信息，改进了Robbins-Monro算法的收敛速度。结果表明，先验信息的Robbins-Monro序列比标准序列收敛更快，特别是在前几步。

    

    我们提出了一种新方法，通过将目标点的先验信息引入Robbins-Monro迭代来改善收敛速度。我们实现了不需要回归模型的先验信息的融合，这也会带来额外的约束。我们证明了这种先验信息的Robbins-Monro序列对于广泛的先验分布都是收敛的，即使是错误的先验分布，如高斯分布、高斯分布的加权和（例如在核密度估计中），以及大于零的有界任意分布函数。我们还通过数值分析来了解序列的性能和参数的影响。结果表明，先验信息的Robbins-Monro序列比标准序列收敛更快，特别是在前几步，这对于测量函数数目有限和误差较大的应用特别重要。

    We propose a new method to improve the convergence speed of the Robbins-Monro algorithm by introducing prior information about the target point into the Robbins-Monro iteration. We achieve the incorporation of prior information without the need of a -- potentially wrong -- regression model, which would also entail additional constraints. We show that this prior-information Robbins-Monro sequence is convergent for a wide range of prior distributions, even wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel density estimate, as well as bounded arbitrary distribution functions greater than zero. We furthermore analyse the sequence numerically to understand its performance and the influence of parameters. The results demonstrate that the prior-information Robbins-Monro sequence converges faster than the standard one, especially during the first steps, which are particularly important for applications where the number of function measurements is limited, and when the 
    
[^19]: SPQR:使用尖峰随机模型控制Q-集合的独立性，用于强化学习

    SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])

    [http://arxiv.org/abs/2401.03137](http://arxiv.org/abs/2401.03137)

    SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。

    

    缓解过高估计偏差是深度强化学习在更复杂任务或包含超出分布数据的离线数据集上获得成功表现的关键挑战。为了克服过高估计偏差，研究了Q-learning的集成方法来利用多个Q函数的多样性。由于网络初始化一直是促进Q函数多样性的主要方法，因此在文献中研究了启发式设计的多样性注入方法。然而，先前的研究并未尝试从理论角度保证集成的独立性。通过引入基于随机矩阵理论的Q-集合独立性的新型正则化损失，我们提出了一种用于强化学习的尖峰Wishart Q-集合独立性正则化方法（SPQR）。

    Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence 
    
[^20]: Krylov立方正则化牛顿法：具有无维收敛速度的子空间二阶方法

    Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate. (arXiv:2401.03058v1 [math.OC])

    [http://arxiv.org/abs/2401.03058](http://arxiv.org/abs/2401.03058)

    本论文提出了一种新的子空间立方正则化牛顿方法，可以在解决凸优化问题时实现无维度相关的全局收敛速度，通过在低维子空间上进行二阶更新，克服了高维问题中内存需求和计算成本大的问题。

    

    二阶优化算法，如立方正则化牛顿法，以其快速收敛速度而闻名；然而，在高维问题中，它们变得不实用，因为需要大量的内存和计算成本。一个有前景的方法是在低维子空间中执行二阶更新，从而产生子空间二阶方法。然而，现有的大多数子空间二阶方法随机选择子空间，因此收敛速度较慢，这取决于问题的维度d。在本文中，我们引入了一种新颖的子空间立方正则化牛顿方法，用于解决凸优化问题，其达到了一个维度无关的全局收敛速度，为${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$。这里，m表示子空间维度，可以显著小于d。我们的主要创新不是采用随机子空间，而是进行立方正则化...

    Second-order optimization methods, such as cubic regularized Newton methods, are known for their rapid convergence rates; nevertheless, they become impractical in high-dimensional problems due to their substantial memory requirements and computational costs. One promising approach is to execute second-order updates within a lower-dimensional subspace, giving rise to subspace second-order methods. However, the majority of existing subspace second-order methods randomly select subspaces, consequently resulting in slower convergence rates depending on the problem's dimension $d$. In this paper, we introduce a novel subspace cubic regularized Newton method that achieves a dimension-independent global convergence rate of ${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$ for solving convex optimization problems. Here, $m$ represents the subspace dimension, which can be significantly smaller than $d$. Instead of adopting a random subspace, our primary innovation involves performing the cubic regul
    
[^21]: 分布强化学习中具有可解释参数调整的鲁棒分位数Huber损失

    A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])

    [http://arxiv.org/abs/2401.02325](http://arxiv.org/abs/2401.02325)

    这篇论文提出了一种鲁棒的分位数Huber损失函数，在分布强化学习中通过捕捉噪声并调整参数来增强对异常值的鲁棒性。实证测试验证了该方法的有效性。

    

    分布强化学习通过最小化分位数Huber损失函数来估计回报分布，该函数从高斯分布之间的Wasserstein距离计算中产生，捕捉到当前和目标分位数值中的噪声。与经典的分位数Huber损失相比，这种创新的损失函数增强了对异常值的鲁棒性，并且通过近似数据中噪声的数量来调整参数。实证测试在分布强化学习的常见应用Atari游戏和最近的对冲策略中验证了该方法的有效性。

    Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values. Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers. Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the eff
    
[^22]: ${\Lambda}$CDM宇宙学中的层次聚类通过持续能量方法

    Hierarchical Clustering in ${\Lambda}$CDM Cosmologies via Persistence Energy. (arXiv:2401.01988v1 [astro-ph.CO])

    [http://arxiv.org/abs/2401.01988](http://arxiv.org/abs/2401.01988)

    通过持续能量方法研究了${\Lambda}$CDM宇宙学中的层次聚类，发现持续能量与红移值之间存在相关性，揭示了宇宙结构的动力学特征。

    

    在这项研究中，我们利用拓扑数据分析的先进方法，研究宇宙网络的结构演化。我们的方法涉及到利用持续信号这一创新方法，将持续图重新概念化为$\mathbb R^2_+$空间中的信号，从而实现持续图的嵌入。利用这种方法，我们分析了三种典型的宇宙结构：团簇、纤维和虚空。其中的一个核心发现是持续能量与红移值之间的相关性，将持续同调与宇宙结构的演化联系起来，并提供了有关宇宙结构动力学的见解。

    In this research, we investigate the structural evolution of the cosmic web, employing advanced methodologies from Topological Data Analysis. Our approach involves leveraging $Persistence$ $Signals$, an innovative method from recent literature that facilitates the embedding of persistence diagrams into vector spaces by re-conceptualizing them as signals in $\mathbb R^2_+$. Utilizing this methodology, we analyze three quintessential cosmic structures: clusters, filaments, and voids. A central discovery is the correlation between $Persistence$ $Energy$ and redshift values, linking persistent homology with cosmic evolution and providing insights into the dynamics of cosmic structures.
    
[^23]: 可扩展的子二次时间网络重建

    Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])

    [http://arxiv.org/abs/2401.01404](http://arxiv.org/abs/2401.01404)

    这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。

    

    网络重建是指在只有关于条件偶联的观测数据，例如时间序列或图模型的独立样本的情况下，确定N个节点之间未观测到的成对耦合。针对这个问题提出的算法的可扩展性的主要障碍是似乎无法避免的二次复杂度O(N^2)，即要考虑每种可能的成对耦合至少一次，尽管大多数感兴趣的网络都是稀疏的，非零耦合的数量只有O(N)。在这里，我们提出了一个适用于广泛重建问题的通用算法，其在子二次时间内实现结果，其数据相关复杂度宽松上界为O(N^(3/2)logN)，但具有更典型的对数线性复杂度O(Nlog^2 N)。我们的算法依赖于一个随机的二阶邻居搜索，产生了最佳的边候选。

    Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
    
[^24]: 安全的多任务贝叶斯优化

    Safe Multi-Task Bayesian Optimization. (arXiv:2312.07281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07281](http://arxiv.org/abs/2312.07281)

    这项研究将鲁棒的高斯过程均匀误差界限扩展到多任务设置中，以解决安全在线优化中超参数未知的问题。

    

    贝叶斯优化已成为安全在线系统优化的强大工具，因其高样本效率和噪声健壮性。为了进一步加快过程，可以将减少的物理模型纳入优化过程中以加速过程，因为这些模型能够提供对实际系统的近似，并且从中进行采样要便宜得多。模型与现实之间的相似性由额外的超参数表示，并在优化过程中学习。安全性是贝叶斯优化等在线优化方法的重要标准，最近的文献已经解决了此问题，并在已知超参数的假设下提供了安全保障。然而，在实践中这是不适用的。因此，我们扩展了鲁棒高斯过程均匀误差界限，以满足多任务设置，其中涉及从超参数后验分布计算置信区域。

    Bayesian optimization has become a powerful tool for safe online optimization of systems, due to its high sample efficiency and noise robustness. For further speed-up reduced physical models of the system can be incorporated into the optimization to accelerate the process, since the models are able to offer an approximation of the actual system, and sampling from them is significantly cheaper. The similarity between model and reality is represented by additional hyperparameters and learned within the optimization process. Safety is an important criteria for online optimization methods like Bayesian optimization, which has been addressed by recent literature, which provide safety guarantees under the assumption of known hyperparameters. However, in practice this is not applicable. Therefore, we extend the robust Gaussian process uniform error bounds to meet the multi-task setting, which involves the calculation of a confidence region from the hyperparameter posterior distribution utiliz
    
[^25]: 集合卡尔曼滤波与高斯过程状态空间模型在非均场和在线推理中的应用

    Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05910](http://arxiv.org/abs/2312.05910)

    这篇论文介绍了一种将集合卡尔曼滤波引入变分推理框架的方法，用于近似高斯过程状态空间模型的后验分布，并且有效地利用了潜在状态和动力学之间的依赖关系，减少了变分参数的数量。

    

    高斯过程状态空间模型（GPSSMs）是一种多功能和原则性的非线性动态系统模型。然而，现有的GPSSMs变分学习和推理方法通常需要优化大量变分参数，导致性能和效率不足。为了解决这个问题，我们提出将集合卡尔曼滤波（EnKF），一种成熟的基于模型的滤波技术，纳入变分推理框架中，以近似潜在状态的后验分布。这种利用EnKF的方法可以有效地利用潜在状态和GP动力学之间的依赖关系，同时消除了对变分分布进行参数化的需求，从而显著减少了变分参数的数量。此外，我们还展示了我们提出的算法可以通过简单地对多个项进行求和来直接评估变分推理中的近似证据下界（ELBO）。

    Gaussian process state-space models (GPSSMs) are a versatile and principled family of nonlinear dynamical system models. However, existing variational learning and inference methods for GPSSMs often necessitate optimizing a substantial number of variational parameters, leading to inadequate performance and efficiency. To overcome this issue, we propose incorporating the ensemble Kalman filter (EnKF), a well-established model-based filtering technique, into the variational inference framework to approximate the posterior distribution of latent states. This utilization of EnKF can effectively exploit the dependencies between latent states and GP dynamics, while eliminating the need for parameterizing the variational distribution, thereby significantly reducing the number of variational parameters. Moreover, we show that our proposed algorithm allows straightforward evaluation of an approximated evidence lower bound (ELBO) in variational inference via simply summating multiple terms with 
    
[^26]: 带预热的动量梯度下降的大型弹射概念研究

    Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study. (arXiv:2311.15051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.15051](http://arxiv.org/abs/2311.15051)

    本研究通过实验证明，带有大学习率和学习率预热的动量梯度下降显示出大型弹射效应，将迭代朝着比梯度下降发现的更平缓的极小值方向推进。

    

    尽管动量梯度下降在现代深度学习中被广泛使用，但对其对训练轨迹的影响的具体理解仍然难以捉摸。本研究通过实验证明，带有大学习率和学习率预热的动量梯度下降显示出大型弹射效应，将迭代朝着比梯度下降发现的更平缓的极小值方向推进。然后我们提供了实证证据和理论直觉，表明大型弹射效应是由于动量“放大”了自稳定效应（Damian等，2023）。

    Although gradient descent with momentum is widely used in modern deep learning, a concrete understanding of its effects on the training trajectory still remains elusive. In this work, we empirically show that momentum gradient descent with a large learning rate and learning rate warmup displays large catapults, driving the iterates towards flatter minima than those found by gradient descent. We then provide empirical evidence and theoretical intuition that the large catapult is caused by momentum "amplifying" the self-stabilization effect (Damian et al., 2023).B.1
    
[^27]: 差分隐私排列检验：应用于核方法

    Differentially Private Permutation Tests: Applications to Kernel Methods. (arXiv:2310.19043v1 [math.ST])

    [http://arxiv.org/abs/2310.19043](http://arxiv.org/abs/2310.19043)

    本文提出了差分隐私排列检验的框架，扩展了经典的非私有排列检验，以在私有环境中保持有限样本有效性和差分隐私性质。该检验的功率取决于检验统计量的选择，并建立了一般条件来保证一致性和非渐进均匀的功率。

    

    近年来，人们对敏感数据的隐私问题越来越关注。为了应对这些问题，差分隐私作为一种严格的隐私保护框架应运而生，在学术界和工业界广泛认可。尽管在私有数据分析方面取得了相当大的进展，但现有的方法往往存在不实用或明显的统计效率损失。本文旨在通过引入差分隐私排列检验来缓解这些担忧。所提出的框架将经典的非私有排列检验扩展到私有环境中，以严格的方式保持有限样本有效性和差分隐私性质。所提出的检验的功率取决于一个检验统计量的选择，并建立了一般条件保证了一致性和非渐进均匀的功率。为了证明我们框架的实用性和可行性，我们重点关注重现核方法。

    Recent years have witnessed growing concerns about the privacy of sensitive data. In response to these concerns, differential privacy has emerged as a rigorous framework for privacy protection, gaining widespread recognition in both academic and industrial circles. While substantial progress has been made in private data analysis, existing methods often suffer from impracticality or a significant loss of statistical efficiency. This paper aims to alleviate these concerns in the context of hypothesis testing by introducing differentially private permutation tests. The proposed framework extends classical non-private permutation tests to private settings, maintaining both finite-sample validity and differential privacy in a rigorous manner. The power of the proposed test depends on the choice of a test statistic, and we establish general conditions for consistency and non-asymptotic uniform power. To demonstrate the utility and practicality of our framework, we focus on reproducing kerne
    
[^28]: 使用合成数据扩展提升数据分析

    Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])

    [http://arxiv.org/abs/2310.17848](http://arxiv.org/abs/2310.17848)

    本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。

    

    合成数据生成作为生成式人工智能的基石，在解决数据稀缺和隐私问题的同时，实现了前所未有的性能。随着合成数据的日益重要，人们开始关注统计方法在合成数据与原始数据上的准确性。在本文中，我们介绍了用于分析的合成数据生成框架。该框架使用高逼真度的合成数据，通过先进模型如表格扩散和生成式预训练转换器模型生成，并结合相关研究洞察进一步增强。在这个框架中的一个重要发现是生成效应：统计方法在合成数据上的错误随着合成数据的增加一开始减少，但最终可能会增加或停滞。这个现象根源于复制原始数据分布的复杂性。

    Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
    
[^29]: 时间相关的Cox模型中的结构化学习

    Structured Learning in Time-dependent Cox Models. (arXiv:2306.12528v1 [stat.ME])

    [http://arxiv.org/abs/2306.12528](http://arxiv.org/abs/2306.12528)

    本文提出了一个灵活的框架，用于在时间相关的Cox模型中进行变量选择，并适应各种复杂的分组结构，具有较低的误警率和快速的计算。

    

    具有时间相关系数和协变量的Cox模型在生存分析中得到广泛应用。在高维环境中，采用稀疏正则化技术进行变量选择，但现有的时间相关的Cox模型方法缺乏在强制特定稀疏模式（即协变量结构）方面的灵活性。我们提出了一个灵活的框架，用于在时间相关的Cox模型中进行变量选择，可适应复杂的选择规则。我们的方法可以适应任意分组结构，包括交互选择，时间性，空间性，树和有向无环图结构。它可以通过降低误警率实现准确的估计。我们开发了sox软件包，实现了一种网络流算法，用于高效地解决具有复杂协变量结构的模型。Sox提供了一个用户友好的接口，用于指定分组结构，并提供快速的计算。通过案例研究，包括一个用于确定所有致死时间预测因素的案例研究。

    Cox models with time-dependent coefficients and covariates are widely used in survival analysis. In high-dimensional settings, sparse regularization techniques are employed for variable selection, but existing methods for time-dependent Cox models lack flexibility in enforcing specific sparsity patterns (i.e., covariate structures). We propose a flexible framework for variable selection in time-dependent Cox models, accommodating complex selection rules. Our method can adapt to arbitrary grouping structures, including interaction selection, temporal, spatial, tree, and directed acyclic graph structures. It achieves accurate estimation with low false alarm rates. We develop the sox package, implementing a network flow algorithm for efficiently solving models with complex covariate structures. Sox offers a user-friendly interface for specifying grouping structures and delivers fast computation. Through examples, including a case study on identifying predictors of time to all-cause death 
    
[^30]: 基于紧核的条件期望估计

    Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])

    [http://arxiv.org/abs/2306.10592](http://arxiv.org/abs/2306.10592)

    本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。

    

    去噪、条件期望和流形学习任务通常可以在寻找两个随机变量积的条件期望的公共环境下表述。本文针对这个更一般的问题，描述了一种算子理论方法来估计条件期望。核积分算子被用作紧致化工具，将估计问题设置为在再生核希尔伯特空间中的线性逆问题。该方程的解被证明对数值逼近是稳定的，从而确保了数据驱动实现的收敛性。总体技术易于实现，还展示了其在一些实际问题中的成功应用。

    The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
    
[^31]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^32]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^33]: 通过风险分解评估自监督学习

    Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03068](http://arxiv.org/abs/2302.03068)

    通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。

    

    自监督学习（SSL）的流程设计涉及架构、增强和预训练数据等诸多选择。然而，SSL通常使用单一度量来评估，这并不能提供深入的洞察和改进方案。为解决这些问题，我们提出了一个SSL风险分解，从逼近、表示可用性、探针泛化和编码器泛化等角度对错误进行分解。我们分析了30个设计选择对169个在ImageNet上评估的SSL视觉模型的影响，并为每个组件提供了高效的估计器，为SSL模型的设计和使用提供宝贵的见解。

    Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
    
[^34]: 压缩、泛化和学习

    Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12767](http://arxiv.org/abs/2301.12767)

    本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。

    

    压缩函数是一种将观测集缩小为尺寸减小的子集的映射，同时保留其信息内容。在多个应用中，新观测使压缩集发生变化的条件被解释为新观测带来了额外的信息，在学习理论中，这对应于错误分类或错误预测。本文建立了一个新理论的基础，允许在压缩的改变概率上保持控制（与学习应用中的统计“风险”相对应）。在适当的条件下，压缩集的基数被证明是压缩的改变概率的一致估计量（不对压缩集的尺寸设置上限）；此外，在普遍适用的偏好条件下获得了前所未有的紧密的有限样本边界来评估压缩的改变概率。所有结果都可以在完全应用中使用。

    A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical "risk" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully 
    
[^35]: ddml: Stata中的双重/无偏机器学习

    ddml: Double/debiased machine learning in Stata. (arXiv:2301.09397v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2301.09397](http://arxiv.org/abs/2301.09397)

    ddml是Stata中的双重/无偏机器学习包，支持五种不同计量模型的因果参数估计，可以灵活估计内生变量的因果效应，在许多现有监督机器学习程序中兼容。推荐与堆叠估计结合使用，提供了蒙特卡洛证据支持。

    

    我们在Stata中引入了一个名为ddml的包，用于双重/无偏机器学习（DDML）。支持五种不同计量模型的因果参数估计，允许在未知函数形式和/或许多外生变量的设置中灵活估计内生变量的因果效应。ddml与Stata中的许多现有监督机器学习程序兼容。我们推荐将DDML与堆叠估计结合使用，将多个机器学习器组合成最终预测器。我们提供了蒙特卡洛证据来支持我们的建议。

    We introduce the package ddml for Double/Debiased Machine Learning (DDML) in Stata. Estimators of causal parameters for five different econometric models are supported, allowing for flexible estimation of causal effects of endogenous variables in settings with unknown functional forms and/or many exogenous variables. ddml is compatible with many existing supervised machine learning programs in Stata. We recommend using DDML in combination with stacking estimation which combines multiple machine learners into a final predictor. We provide Monte Carlo evidence to support our recommendation.
    
[^36]: 生存强盗问题

    The Survival Bandit Problem. (arXiv:2206.03019v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03019](http://arxiv.org/abs/2206.03019)

    这个研究介绍了生存强盗问题，这是多臂赌博机问题的一个新变种。该问题的目标是最小化生存遗憾，同时要求算法加速收敛。

    

    我们介绍并研究了多臂赌博机问题(MAB)的一个新变种，称为生存强盗问题(S-MAB)。虽然在这两个问题中，目标都是最大化所谓的累积奖励，但在这个新的变种中，如果累积奖励低于预设的阈值，程序将被中断。这个简单但未被探讨的MAB扩展源自许多实际应用。例如，当对自愿患者进行两种药物的测试时，人们的健康问题至关重要，如果出现严重副作用或者疾病综合症没有得到治疗，有必要能够中断实验。从理论的角度来看，S-MAB是第一种可能中断或不中断的MAB变种。我们首先对S-MAB进行形式化，将其目标定义为所谓的生存遗憾的最小化，自然推广了MAB的遗憾。然后，我们证明了同时最小化生存遗憾和加速收敛的适用于S-MAB的算法存在。

    We introduce and study a new variant of the multi-armed bandit problem (MAB), called the survival bandit problem (S-MAB). While in both problems, the objective is to maximize the so-called cumulative reward, in this new variant, the procedure is interrupted if the cumulative reward falls below a preset threshold. This simple yet unexplored extension of the MAB follows from many practical applications. For example, when testing two medicines against each other on voluntary patients, people's health are at stake, and it is necessary to be able to interrupt experiments if serious side effects occur or if the disease syndromes are not dissipated by the treatment. From a theoretical perspective, the S-MAB is the first variant of the MAB where the procedure may or may not be interrupted. We start by formalizing the S-MAB and we define its objective as the minimization of the so-called survival regret, which naturally generalizes the regret of the MAB. Then, we show that the objective of the 
    
[^37]: 稀疏图的半监督聚类：跨越了信息理论门槛

    Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.11677](http://arxiv.org/abs/2205.11677)

    该论文提出了两种有效的算法来将标签信息与稀疏图结构相结合，解决了基于网络拓扑的聚类在稀疏图上的问题。

    

    随机块模型是一种用于网络结构数据聚类和社区检测的基本随机图模型。数十年来对该问题的广泛研究已经建立了许多深刻的结果，其中Kesten-Stigum门槛处的相变现象特别有趣，从数学和应用角度都具有重要意义。它表明，如果模型参数在某个门槛以下，基于网络拓扑的任何估计器在稀疏图上都不能比随机猜测更好。然而，如果我们稍微扩展视野到普遍存在的半监督设置，这样的基本限制将完全消失。我们证明，通过揭示出任意一部分标记，可以在整个参数域内对检测问题进行处理。此外，我们引入了两种有效的算法，一种是基于组合的，一种是基于优化的，用于将标签信息与图结构相结合。我们的工作为随机块模型和半监督学习带来了全新的视角，标志着稀疏图聚类领域的重大突破。

    The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
    
[^38]: 非参数结构函数和弹性的自适应估计和均匀置信带

    Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities. (arXiv:2107.11869v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2107.11869](http://arxiv.org/abs/2107.11869)

    本文介绍了两种基于数据驱动的方法，用于非参数模型中使用工具变量的最优估计和推断；首先是选择最佳筛选维数的估计器，使得结构函数和其导数的估计器以最小化超范数的收敛速度收敛；其次是构建均匀置信带来获取结构函数和其导数的置信区间。模拟结果表明了这些方法的良好性能。

    

    我们介绍了两种基于数据驱动的方法，用于非参数模型中使用工具变量的最优估计和推断。第一种是对一类常用筛选两阶段最小二乘估计量的筛选维数的数据驱动选择。当使用该选择进行实现时，结构函数$h_0$及其导数（如弹性）的估计器以最快的（即最小化）超范数收敛速度收敛。第二种是构建用于$h_0$及其导数的均匀置信带（UCBs）。我们的UCBs在一个通用的数据生成过程类别上保证了覆盖，并以最小化速率收缩，可能有对数因子。因此，与通常的欠平滑方法的UCBs相比，我们的UCBs在渐近上具有更高的效率。作为应用，我们评估了国际贸易的垄断竞争模型中企业出口的密集边际的弹性。模拟结果显示了我们方法的良好性能。

    We introduce two data-driven procedures for optimal estimation and inference in nonparametric models using instrumental variables. The first is a data-driven choice of sieve dimension for a popular class of sieve two-stage least squares estimators. When implemented with this choice, estimators of both the structural function $h_0$ and its derivatives (such as elasticities) converge at the fastest possible (i.e., minimax) rates in sup-norm. The second is for constructing uniform confidence bands (UCBs) for $h_0$ and its derivatives. Our UCBs guarantee coverage over a generic class of data-generating processes and contract at the minimax rate, possibly up to a logarithmic factor. As such, our UCBs are asymptotically more efficient than UCBs based on the usual approach of undersmoothing. As an application, we estimate the elasticity of the intensive margin of firm exports in a monopolistic competition model of international trade. Simulations illustrate the good performance of our procedu
    
[^39]: 通过松弛优化理论及其在支持向量机中的应用，对风险进行研究

    A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. (arXiv:2004.05839v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.05839](http://arxiv.org/abs/2004.05839)

    本文研究了松弛优化理论，探讨了风险与复杂度之间的联系，提出了可以从复杂度估计风险的方法，研究了支持向量方法在机器学习中的应用。

    

    本文考虑了松弛优化，这是一种用于数据驱动设计的广泛范例。我们建立了"风险"（未能满足新的、样本外约束的概率）和"复杂度"（根据Garatti和Campi（2019）中介绍的定义）之间的深层联系，并发现这种联系对应用有深远影响，因为它意味着可以从复杂度估计风险，而复杂度可以通过数据进行测量，不需要了解数据生成机制。在本文中，我们建立了新的结果。首先，我们扩大了Garatti和Campi（2019）的范围，以涵盖机器学习中的各种算法。然后，我们研究了经典的支持向量方法，包括支持向量机（SVM）、支持向量回归（SVR）和支持向量数据描述（SVDD）。

    In this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. This approach was previously considered by the same authors of this work in Garatti and Campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper Garatti and Campi (2019)). This connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. In the present work we establish new results. First, we expand the scope of Garatti and Campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. Then, we study classical support vector methods - including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD 
    

