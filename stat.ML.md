# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The extended Ville's inequality for nonintegrable nonnegative supermartingales.](http://arxiv.org/abs/2304.01163) | 本文提出了一种新的理论来描述非负超马氏过程，并推导出一个新的极大不等式，适用于非可积情况，并说明了混合方法的扩展以及该理论在顺序统计中的应用。 |
| [^2] | [Synthesis parameter effect detection using quantitative representations and high dimensional distribution distances.](http://arxiv.org/abs/2304.01120) | 本论文基于copula理论，采用高维分布距离和排列统计方法研究一个设计实验中参数对材料微观结构的影响，发现了草酸酸度和冲压顺序对生成的氧化钚微观结构有影响，同时检测到过度的双变量效应。 |
| [^3] | [Theoretical guarantees for neural control variates in MCMC.](http://arxiv.org/abs/2304.01111) | 本文提出了一种利用神经控制变量的方差缩减方法，推导并得出了在各种遍历性假设下渐近方差的最优收敛速率。 |
| [^4] | [Artificial neural networks and time series of counts: A class of nonlinear INGARCH models.](http://arxiv.org/abs/2304.01025) | 本论文提出了一类基于人工神经网络响应函数的非线性INGARCH模型，能够更好地分析计数时间序列，并在实证分析中得到验证。 |
| [^5] | [Efficient human-in-loop deep learning model training with iterative refinement and statistical result validation.](http://arxiv.org/abs/2304.00990) | 本文提出了一种利用自动生成的训练数据和快速的人工视觉检查，提高深度学习模型精度的方法，同时保持时间、精力和成本低，其精度和速度在人类国际标准（ISIC）数据集上得到了验证。 |
| [^6] | [Diffusion Bridge Mixture Transports, Schr\"odinger Bridge Problems and Generative Modeling.](http://arxiv.org/abs/2304.00917) | 本文提出了一种新的迭代算法IDBM，用于解决动态Schr\"odinger桥问题，该算法能够在每一步有效地耦合目标度量，并在各种应用中表现出竞争力。此外，还讨论了使用扩散过程的时间反演来定义一个近似传输简单分布到目标分布的生成过程的最新进展。 |
| [^7] | [Combinatorial Optimization enriched Machine Learning to solve the Dynamic Vehicle Routing Problem with Time Windows.](http://arxiv.org/abs/2304.00789) | 本论文介绍了一种利用组合优化增强的机器学习流程，用于解决具有调度波的动态车辆路径问题。该方法在 NeurIPS 2022 车辆路径规划竞赛中获得第一名，证明了其在该问题上的有效性。 |
| [^8] | [Online stochastic Newton methods for estimating the geometric median and applications.](http://arxiv.org/abs/2304.00770) | 论文介绍了在线随机牛顿算法用于估计随机变量的几何中位数，该方法是一种鲁棒的中心趋势指标，在大量数据样本中具有广泛应用。 |
| [^9] | [High-dimensional scaling limits and fluctuations of online least-squares SGD with smooth covariance.](http://arxiv.org/abs/2304.00707) | 本文针对在线最小二乘随机梯度下降（SGD）算法，以数据生成模型的性质为基础，建立了高维缩放极限与波动模型。在低、中、高噪声设置阶段，揭示了迭代的精确三阶段相变。 |
| [^10] | [Lithium-ion Battery Online Knee Onset Detection by Matrix Profile.](http://arxiv.org/abs/2304.00691) | 该论文提出了一种基于时间信息与矩阵剖面的在线锂离子电池膝部起始点识别方法，可使用易采集的测量数据，为电池性能变化提供早期预警。 |
| [^11] | [Sequence-aware item recommendations for multiply repeated user-item interactions.](http://arxiv.org/abs/2304.00578) | 本论文设计了一种新的推荐物品方法，受到自然语言处理技术的启发，可以有效地处理用户多次与同一物品交互或用户偏好随时间变化的情况。 |
| [^12] | [Modelling customer churn for the retail industry in a deep learning based sequential framework.](http://arxiv.org/abs/2304.00575) | 本文提出了一种基于深度学习的时序框架，能够预测非合同情况下哪些客户存在流失风险，这种模型只需要基于客户行为就能获得具有个体特征的生存模型，避免了繁琐而耗时的特征工程过程。 |
| [^13] | [Copula-Based Density Estimation Models for Multivariate Zero-Inflated Continuous Data.](http://arxiv.org/abs/2304.00537) | 本文提出了两种基于copula的密度估计模型，可以处理多元相关性的零膨胀连续变量，引入矫正高斯copula以应对绑定数据问题，并提出高效的方法进行参数估计和可能性计算。 |
| [^14] | [TSCI: two stage curvature identification for causal inference with invalid instruments.](http://arxiv.org/abs/2304.00513) | TSCI 提出了无效仪器条件下使用两阶段算法进行因果推断，不需要强识别假设，可通过数据自适应方式捕捉仪器违规行为。 |
| [^15] | [Infinite-dimensional reservoir computing.](http://arxiv.org/abs/2304.00490) | 本文证明了一类新的概念类别的储备计算逼近和泛化边界，使得机器学习算法具有更好的实现性能。 |
| [^16] | [Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis.](http://arxiv.org/abs/2304.00395) | 通过一个新的公式，本文理论分析了基于核的对比学习损失的特点，证明了它能描述学习表示的结构和表现，提供一个新的限制方法，并在多个基准测试中验证其有效性。 |
| [^17] | [Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes.](http://arxiv.org/abs/2304.00232) | 该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。 |
| [^18] | [Diffusion map particle systems for generative modeling.](http://arxiv.org/abs/2304.00200) | 本文提出一种新型扩散映射粒子系统(DMPS)，可以用于高效生成建模，实验表明在包含流形结构的合成数据集上取得了比其他方法更好的效果。 |
| [^19] | [Applications of No-Collision Transportation Maps in Manifold Learning.](http://arxiv.org/abs/2304.00199) | 本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。 |
| [^20] | [Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning.](http://arxiv.org/abs/2304.00195) | 该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。 |
| [^21] | [One-Step Estimation of Differentiable Hilbert-Valued Parameters.](http://arxiv.org/abs/2303.16711) | 本文介绍了一种针对Hilbert-Valued参数进行一步估计的方法，并提供一种适用于缺乏重现核的Hilbert空间的解决方案。 |
| [^22] | [TRAK: Attributing Model Behavior at Scale.](http://arxiv.org/abs/2303.14186) | TRAK是一种适用于大规模、可微模型的数据归因方法，既有效又计算量可行。 |
| [^23] | [Indeterminate Probability Neural Network.](http://arxiv.org/abs/2303.11536) | 本文提出了一种新型通用模型——不定概率神经网络；它可以进行无监督聚类和使用很小的神经网络处理大规模分类，其理论优势体现在新的概率理论和神经网络框架中。 |
| [^24] | [Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and its Application to Bayesian Inference.](http://arxiv.org/abs/2303.05731) | 本研究利用代数几何方法给出了张量分解的实对数典范阈值的上界，并推导了其在贝叶斯推断中的应用理论误差，揭示了张量分解的数学性质。 |
| [^25] | [Vector Quantized Time Series Generation with a Bidirectional Prior Model.](http://arxiv.org/abs/2303.04743) | 本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。 |
| [^26] | [Graph Laplacians on Shared Nearest Neighbor graphs and graph Laplacians on $k$-Nearest Neighbor graphs having the same limit.](http://arxiv.org/abs/2302.12399) | 本文开创性地研究了共享最近邻图和图拉普拉斯算子，证明了共享最近邻图拉普拉斯算子和 k 近邻图拉普拉斯算子具有相同的连续极限，同时发现它们的图拉普拉斯算子逐点收敛速率与 $(k/n)^{1/m}$ 成线性关系。 |
| [^27] | [I$^2$SB: Image-to-Image Schr\"odinger Bridge.](http://arxiv.org/abs/2302.05872) | 提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。 |
| [^28] | [Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers.](http://arxiv.org/abs/2212.12474) | 本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。 |
| [^29] | [Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test.](http://arxiv.org/abs/2211.16596) | 该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。 |
| [^30] | [A Revenue Function for Comparison-Based Hierarchical Clustering.](http://arxiv.org/abs/2211.16459) | 本论文提出了一种收益函数，可以仅使用比较来衡量基于比较的分层聚类的质量。 |
| [^31] | [Quantifying the Impact of Label Noise on Federated Learning.](http://arxiv.org/abs/2211.07816) | 本文量化了标签噪声对FL的影响。实验结果表明，随着噪声水平的增加，全局模型的准确性会线性下降，同时会导致FL训练的收敛速度减缓和全局模型过拟合。 |
| [^32] | [Enhanced Bayesian Neural Networks for Macroeconomics and Finance.](http://arxiv.org/abs/2211.04752) | 该论文提出的增强贝叶斯神经网络能够模拟大量宏观经济和金融变量的通用非线性和时间变化，具有潜在的政策决策应用价值。 |
| [^33] | [Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems: A Survey.](http://arxiv.org/abs/2208.13592) | 本文综述了解决平滑（强）单调随机变分不等式的方法。 |
| [^34] | [Causal Bandits for Linear Structural Equation Models.](http://arxiv.org/abs/2208.12764) | 本文提出了针对线性结构方程模型的因果赌博算法，摒弃了已知干预分布或其边缘分布的假设。 |
| [^35] | [Bayesian Optimization with Informative Covariance.](http://arxiv.org/abs/2208.02704) | 提出了一种新颖的用于优化的信息丰富协方差函数，利用非平稳性来编码对搜索空间中某些区域的偏好，并在优化过程中自适应地促进局部探索，以提高贝叶斯优化在高维空间中的样本效率。 |
| [^36] | [When Does Re-initialization Work?.](http://arxiv.org/abs/2206.10011) | 研究表明，当没有其他正则化技术时，重新初始化神经网络有助于提高泛化性能。但是，当它与其他正则化技术一起使用时，对泛化性能的额外帮助很小，尽管最佳泛化性能变得更加稳定。 |
| [^37] | [Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models.](http://arxiv.org/abs/2206.00501) | 本文发现良性过拟合在存在标签噪声的情况下可能会失败，提醒未来需要理解欠拟合制度下的隐式偏见。 |
| [^38] | [Average Adjusted Association: Efficient Estimation with High Dimensional Confounders.](http://arxiv.org/abs/2205.14048) | 本研究提出了一个总结衡量标准——平均调整关联度（AAA），用于评估一个具有混淆影响的异质群体中的关联程度。并且我们提出了高效的估计方法，可用于各种采样场景。 |
| [^39] | [Exact Solutions of a Deep Linear Network.](http://arxiv.org/abs/2202.04777) | 本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，结果表明权重衰减与模型架构的强烈交互作用会在多于1个隐藏层的网络中创建不良极小值，并表明常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。 |
| [^40] | [Near-Optimal Learning of Extensive-Form Games with Imperfect Information.](http://arxiv.org/abs/2202.01752) | 本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。 |
| [^41] | [Likelihood Training of Schr\"odinger Bridge using Forward-Backward SDEs Theory.](http://arxiv.org/abs/2110.11291) | 本文提出了一种新的计算框架，用于基于正向-反向随机微分方程理论对薛定谔桥进行似然训练。通过这个框架，可以构建SB的似然目标，这可以成为现代深度生成模型训练的替代方法。 |
| [^42] | [A portfolio approach to massively parallel Bayesian optimization.](http://arxiv.org/abs/2110.09334) | 本文提出了一种可扩展的投资组合策略，面向大规模并行贝叶斯优化，解决并行设计选择复杂性问题，适用于噪声黑盒函数，单目标和多目标优化任务。 |
| [^43] | [Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees.](http://arxiv.org/abs/2110.03313) | 本研究提出了MASHA1和MASHA2方法，可以减少在分布式训练中的通信量，并在获得可比性质量的模型的同时，解决变分不等式和鞍点问题。 |
| [^44] | [Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning.](http://arxiv.org/abs/2109.03445) | 本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。 |
| [^45] | [Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions.](http://arxiv.org/abs/2106.02619) | 本文研究了GAN如何有效地学习那些接近真实图像分布的分层生成分布，当一个分布具有前向超分辨率结构时，通过SGDA简单地训练GAN就能够实现高效学习。 |
| [^46] | [Towards a mathematical theory of trajectory inference.](http://arxiv.org/abs/2102.09204) | 本文提出了一个数学理论框架和数值方法，可以通过时间边缘样本推断出随机过程的轨迹，特别是它可以应用于单细胞RNA测序数据的分析和轨迹推断。 |

# 详细

[^1]: 非可积非负超马氏过程的扩展维尔不等式

    The extended Ville's inequality for nonintegrable nonnegative supermartingales. (arXiv:2304.01163v1 [math.PR])

    [http://arxiv.org/abs/2304.01163](http://arxiv.org/abs/2304.01163)

    本文提出了一种新的理论来描述非负超马氏过程，并推导出一个新的极大不等式，适用于非可积情况，并说明了混合方法的扩展以及该理论在顺序统计中的应用。

    

    本文在 Robbins 的初始工作基础上，严密地提出了一种非负超马氏过程的扩展理论，不需要可积性或有限性。特别地，我们推导了 Robbins 预示的一个关键极大不等式，称为扩展维尔不等式，它加强了经典的维尔不等式（适用于可积非负超马氏过程），并适用于我们的非可积设置。我们推导了混合方法的扩展，适用于我们扩展的非负超马氏过程的 $\sigma$- 有限混合。我们介绍了我们理论在顺序统计中的一些应用，如在推导非参数置信序列和（扩展）e-过程中使用不适当混合（先验）。

    Following initial work by Robbins, we rigorously present an extended theory of nonnegative supermartingales, requiring neither integrability nor finiteness. In particular, we derive a key maximal inequality foreshadowed by Robbins, which we call the extended Ville's inequality, that strengthens the classical Ville's inequality (for integrable nonnegative supermartingales), and also applies to our nonintegrable setting. We derive an extension of the method of mixtures, which applies to $\sigma$-finite mixtures of our extended nonnegative supermartingales. We present some implications of our theory for sequential statistics, such as the use of improper mixtures (priors) in deriving nonparametric confidence sequences and (extended) e-processes.
    
[^2]: 采用数量表示和高维度分布距离检测合成参数的影响

    Synthesis parameter effect detection using quantitative representations and high dimensional distribution distances. (arXiv:2304.01120v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2304.01120](http://arxiv.org/abs/2304.01120)

    本论文基于copula理论，采用高维分布距离和排列统计方法研究一个设计实验中参数对材料微观结构的影响，发现了草酸酸度和冲压顺序对生成的氧化钚微观结构有影响，同时检测到过度的双变量效应。

    

    检测合成过程参数对材料微观结构的影响是材料科学中一个重要但难以捉摸的目标。我们开发了一种基于植被理论、高维分布距离和排列统计的方法，分析了一个合成氧化钚的设计实验中的参数效应，从而检测到冲压顺序和草酸酸度对生成的氧化钚微观结构的影响，这与文献相符。我们还检测到酸浓度、冲压顺序和沉淀温度之间的过度双变量效应。

    Detection of effects of the parameters of the synthetic process on the microstructure of materials is an important, yet elusive goal of materials science. We develop a method for detecting effects based on copula theory, high dimensional distribution distances, and permutational statistics to analyze a designed experiment synthesizing plutonium oxide from Pu(III) Oxalate. We detect effects of strike order and oxalic acid feed on the microstructure of the resulting plutonium oxide, which match the literature well. We also detect excess bivariate effects between the pairs of acid concentration, strike order and precipitation temperature.
    
[^3]: 神经控制变量在MCMC中的理论保证

    Theoretical guarantees for neural control variates in MCMC. (arXiv:2304.01111v1 [math.ST])

    [http://arxiv.org/abs/2304.01111](http://arxiv.org/abs/2304.01111)

    本文提出了一种利用神经控制变量的方差缩减方法，推导并得出了在各种遍历性假设下渐近方差的最优收敛速率。

    

    本文提出了一种基于加性控制变量和最小化渐近方差的马尔可夫链方差缩减方法。我们专注于控制变量表示为深度神经网络的特定情况。在基础马尔可夫链的各种遍历性假设下，推导了渐近方差的最优收敛速率。该方法依赖于方差缩减算法和函数逼近理论的随机误差的最新成果。

    In this paper, we propose a variance reduction approach for Markov chains based on additive control variates and the minimization of an appropriate estimate for the asymptotic variance. We focus on the particular case when control variates are represented as deep neural networks. We derive the optimal convergence rate of the asymptotic variance under various ergodicity assumptions on the underlying Markov chain. The proposed approach relies upon recent results on the stochastic errors of variance reduction algorithms and function approximation theory.
    
[^4]: 人工神经网络与计数时间序列：一类非线性INGARCH模型

    Artificial neural networks and time series of counts: A class of nonlinear INGARCH models. (arXiv:2304.01025v1 [stat.ME])

    [http://arxiv.org/abs/2304.01025](http://arxiv.org/abs/2304.01025)

    本论文提出了一类基于人工神经网络响应函数的非线性INGARCH模型，能够更好地分析计数时间序列，并在实证分析中得到验证。

    

    计数时间序列经常使用具有条件异方差性（INGARCH）的广义整数值自回归模型进行分析。这些模型使用响应函数将过去观测向量和过去条件期望映射到现在观测的条件期望。本文展示了如何将INGARCH模型与人工神经网络（ANN）响应函数相结合，以获得一类非线性INGARCH模型。ANN框架允许将许多现有的INGARCH模型解释为相应神经模型的退化版本。给出了最大似然估计、边际效应和置信区间的详细信息。计数时间序列的实证分析表明，神经INGARCH模型能够在信息损失方面胜过合理的退化竞争模型。

    Time series of counts are frequently analyzed using generalized integer-valued autoregressive models with conditional heteroskedasticity (INGARCH). These models employ response functions to map a vector of past observations and past conditional expectations to the conditional expectation of the present observation. In this paper, it is shown how INGARCH models can be combined with artificial neural network (ANN) response functions to obtain a class of nonlinear INGARCH models. The ANN framework allows for the interpretation of many existing INGARCH models as a degenerate version of a corresponding neural model. Details on maximum likelihood estimation, marginal effects and confidence intervals are given. The empirical analysis of time series of bounded and unbounded counts reveals that the neural INGARCH models are able to outperform reasonable degenerate competitor models in terms of the information loss.
    
[^5]: 迭代精炼和统计结果验证的高效人机交互深度学习模型训练

    Efficient human-in-loop deep learning model training with iterative refinement and statistical result validation. (arXiv:2304.00990v1 [cs.CV])

    [http://arxiv.org/abs/2304.00990](http://arxiv.org/abs/2304.00990)

    本文提出了一种利用自动生成的训练数据和快速的人工视觉检查，提高深度学习模型精度的方法，同时保持时间、精力和成本低，其精度和速度在人类国际标准（ISIC）数据集上得到了验证。

    

    标注和标签是深度学习在医疗数据应用中最大的挑战之一。目前的流程时间和成本都很高，因此限制了技术的广泛应用。此外，验证计量性能改进是否显著对于选择最佳模型非常重要。本文展示了一种方法，用于创建分割，这是超声成像机器学习流程数据清理的必要部分。我们提出了一个四步方法，利用自动生成的训练数据和快速的人工视觉检查，提高模型精度，同时保持时间、精力和成本低。我们还展示了多次运行实验，以允许使用统计分析。质量较差的自动地面真实数据和快速的视觉检查有效地训练了一个初始基础模型，而后用一小部分更昂贵的人工生成的地面真实数据进行精炼。本方法进行了演示，其精度和速度在人类国际标准（ISIC）数据集上得到了验证。

    Annotation and labeling of images are some of the biggest challenges in applying deep learning to medical data. Current processes are time and cost-intensive and, therefore, a limiting factor for the wide adoption of the technology. Additionally validating that measured performance improvements are significant is important to select the best model. In this paper, we demonstrate a method for creating segmentations, a necessary part of a data cleaning for ultrasound imaging machine learning pipelines. We propose a four-step method to leverage automatically generated training data and fast human visual checks to improve model accuracy while keeping the time/effort and cost low. We also showcase running experiments multiple times to allow the usage of statistical analysis. Poor quality automated ground truth data and quick visual inspections efficiently train an initial base model, which is refined using a small set of more expensive human-generated ground truth data. The method is demonst
    
[^6]: 扩散桥混合传输、薛定谔桥问题和生成建模

    Diffusion Bridge Mixture Transports, Schr\"odinger Bridge Problems and Generative Modeling. (arXiv:2304.00917v1 [stat.ML])

    [http://arxiv.org/abs/2304.00917](http://arxiv.org/abs/2304.00917)

    本文提出了一种新的迭代算法IDBM，用于解决动态Schr\"odinger桥问题，该算法能够在每一步有效地耦合目标度量，并在各种应用中表现出竞争力。此外，还讨论了使用扩散过程的时间反演来定义一个近似传输简单分布到目标分布的生成过程的最新进展。

    

    动态薛定谔桥问题寻求定义在两个目标概率分布之间的传输的随机过程，同时最优地满足最接近参考过程的Kullback-Leibler散度的准则。我们提出了一种新的基于采样的迭代算法，即迭代扩散桥混合传输（IDBM），旨在解决动态薛定谔桥问题。IDBM过程表现出在每一步实现目标度量之间的有效耦合的有吸引力的属性。我们进行了IDBM过程的初始理论研究，建立了其收敛性质。理论发现通过许多数值实验证明了IDBM过程在各种应用中出色的性能。生成建模方面的最新进展使用扩散过程的时间反演来定义一个近似传输简单分布到目标分布的生成过程。本文提出了一种新的算法，称为迭代扩散桥混合传输（IDBM），用于解决动态薛定谔桥问题。IDBM在每一步实现目标度量之间的有效耦合，并且在各种应用中表现出良好的性能。我们的理论研究证明了IDBM算法的收敛性质。通过许多数值实验进一步说明了所提出的算法的有效性。此外，还讨论了生成建模方面的最新进展，它使用扩散过程的时间反演来近似目标分布。

    The dynamic Schr\"odinger bridge problem seeks a stochastic process that defines a transport between two target probability measures, while optimally satisfying the criteria of being closest, in terms of Kullback-Leibler divergence, to a reference process.  We propose a novel sampling-based iterative algorithm, the iterated diffusion bridge mixture transport (IDBM), aimed at solving the dynamic Schr\"odinger bridge problem. The IDBM procedure exhibits the attractive property of realizing a valid coupling between the target measures at each step. We perform an initial theoretical investigation of the IDBM procedure, establishing its convergence properties. The theoretical findings are complemented by numerous numerical experiments illustrating the competitive performance of the IDBM procedure across various applications.  Recent advancements in generative modeling employ the time-reversal of a diffusion process to define a generative process that approximately transports a simple distri
    
[^7]: 利用组合优化增强的机器学习解决带时间窗的动态车辆路径问题

    Combinatorial Optimization enriched Machine Learning to solve the Dynamic Vehicle Routing Problem with Time Windows. (arXiv:2304.00789v1 [math.OC])

    [http://arxiv.org/abs/2304.00789](http://arxiv.org/abs/2304.00789)

    本论文介绍了一种利用组合优化增强的机器学习流程，用于解决具有调度波的动态车辆路径问题。该方法在 NeurIPS 2022 车辆路径规划竞赛中获得第一名，证明了其在该问题上的有效性。

    

    随着电子商务的发展和客户对物流服务的要求增加，物流服务提供商在日常规划中面临新的复杂性，主要是由于有效地处理当天的配送。现有的多阶段随机优化方法允许解决底层的动态车辆路径问题，但要么计算成本在在线设置中太高，要么（在强化学习的情况下）难以在高维组合问题上表现良好。为了减轻这些缺点，我们提出了一种新颖的机器学习流程，其中包含了一个组合优化层。我们将这个通用流程应用于具有调度波的动态车辆路径问题，这个问题最近被推广到NeurIPS 2022的EURO meets NeurIPS车辆路径规划竞赛中。我们的方法在这个比赛中排名第一，超过了所有其他方法来解决所提出的动态车辆路径问题。通过这项工作，我们验证了基于组合优化增强的机器学习在动态车辆路径问题中的有效性。

    With the rise of e-commerce and increasing customer requirements, logistics service providers face a new complexity in their daily planning, mainly due to efficiently handling same day deliveries. Existing multi-stage stochastic optimization approaches that allow to solve the underlying dynamic vehicle routing problem are either computationally too expensive for an application in online settings, or -- in the case of reinforcement learning -- struggle to perform well on high-dimensional combinatorial problems. To mitigate these drawbacks, we propose a novel machine learning pipeline that incorporates a combinatorial optimization layer. We apply this general pipeline to a dynamic vehicle routing problem with dispatching waves, which was recently promoted in the EURO Meets NeurIPS Vehicle Routing Competition at NeurIPS 2022. Our methodology ranked first in this competition, outperforming all other approaches in solving the proposed dynamic vehicle routing problem. With this work, we prov
    
[^8]: 在线随机牛顿法估计几何中位数及应用

    Online stochastic Newton methods for estimating the geometric median and applications. (arXiv:2304.00770v1 [stat.ML])

    [http://arxiv.org/abs/2304.00770](http://arxiv.org/abs/2304.00770)

    论文介绍了在线随机牛顿算法用于估计随机变量的几何中位数，该方法是一种鲁棒的中心趋势指标，在大量数据样本中具有广泛应用。

    

    在大样本的背景下，少数个体可能会破坏像平均数这样的基本统计指标。自动检测这些非典型个体是困难的，替代策略是使用鲁棒性方法。本文重点研究随机变量的几何中位数估计，它是一个鲁棒的中心趋势指标。为了处理顺序到达的大量数据样本，引入了估计几何中位数的在线随机牛顿算法，并给出了它们的收敛速度。由于中位数和海森矩阵的估计可以递归更新，因此我们还确定了中位数在任何指定方向上的置信区间，并进行在线统计检验。

    In the context of large samples, a small number of individuals might spoil basic statistical indicators like the mean. It is difficult to detect automatically these atypical individuals, and an alternative strategy is using robust approaches. This paper focuses on estimating the geometric median of a random variable, which is a robust indicator of central tendency. In order to deal with large samples of data arriving sequentially, online stochastic Newton algorithms for estimating the geometric median are introduced and we give their rates of convergence. Since estimates of the median and those of the Hessian matrix can be recursively updated, we also determine confidences intervals of the median in any designated direction and perform online statistical tests.
    
[^9]: 具有平滑协方差的在线最小二乘SGD算法的高维缩放极限与波动

    High-dimensional scaling limits and fluctuations of online least-squares SGD with smooth covariance. (arXiv:2304.00707v1 [math.PR])

    [http://arxiv.org/abs/2304.00707](http://arxiv.org/abs/2304.00707)

    本文针对在线最小二乘随机梯度下降（SGD）算法，以数据生成模型的性质为基础，建立了高维缩放极限与波动模型。在低、中、高噪声设置阶段，揭示了迭代的精确三阶段相变。

    

    本文考虑数据生成模型的性质，导出了在线最小二乘随机梯度下降（SGD）算法的高维缩放极限与波动。我们将SGD迭代视为相互作用的粒子系统，其中期望相互作用由输入的协方差结构表征。我们假设对所有达到八阶矩的矩顺滑，且无需显式假设高斯性，就能够建立无穷维的普通微分方程（ODEs）或随机微分方程（SDEs）的高维缩放极限和波动。我们的结果揭示了迭代的精确三阶段相变；当噪声方差从低、中、高噪声设置时，它从轨道运动变为扩散运动，最终变为纯随机运动。在低噪声设置下，我们进一步表征了精确的波动。

    We derive high-dimensional scaling limits and fluctuations for the online least-squares Stochastic Gradient Descent (SGD) algorithm by taking the properties of the data generating model explicitly into consideration. Our approach treats the SGD iterates as an interacting particle system, where the expected interaction is characterized by the covariance structure of the input. Assuming smoothness conditions on moments of order up to eight orders, and without explicitly assuming Gaussianity, we establish the high-dimensional scaling limits and fluctuations in the form of infinite-dimensional Ordinary Differential Equations (ODEs) or Stochastic Differential Equations (SDEs). Our results reveal a precise three-step phase transition of the iterates; it goes from being ballistic, to diffusive, and finally to purely random behavior, as the noise variance goes from low, to moderate and finally to very-high noise setting. In the low-noise setting, we further characterize the precise fluctuation
    
[^10]: 基于矩阵剖面的锂离子电池在线膝部起始检测

    Lithium-ion Battery Online Knee Onset Detection by Matrix Profile. (arXiv:2304.00691v1 [stat.ML])

    [http://arxiv.org/abs/2304.00691](http://arxiv.org/abs/2304.00691)

    该论文提出了一种基于时间信息与矩阵剖面的在线锂离子电池膝部起始点识别方法，可使用易采集的测量数据，为电池性能变化提供早期预警。

    

    锂离子电池会在到达膝部起始点后逐渐恶化并加速衰减至寿命结束前。在提供电池性能变化的早期预警方面，膝部起始点具有关键作用。然而，目前仅有有限的文献研究了在线膝部起始点的识别。此外，使用易于采集的测量数据进行识别是非常必要的。为了解决这些挑战，通过利用放电数据中的时间信息，开发了一种在线膝部起始点识别方法。首先，利用动态时间规整提取从轻微磨损阶段开始的放电电压周期中的时间动态变化。然后，在子序列相似性搜索期间，利用矩阵剖面暴露异常。当新周期的时间动态超过控制极限且剖面指数表示出制度变化时，检测到膝部起始点。最后，采用实验结果证明了所提出方法的有效性。

    Lithium-ion batteries (LiBs) degrade slightly until the knee onset, after which the deterioration accelerates to end of life (EOL). The knee onset, which marks the initiation of the accelerated degradation rate, is crucial in providing an early warning of the battery's performance changes. However, there is only limited literature on online knee onset identification. Furthermore, it is good to perform such identification using easily collected measurements. To solve these challenges, an online knee onset identification method is developed by exploiting the temporal information within the discharge data. First, the temporal dynamics embedded in the discharge voltage cycles from the slight degradation stage are extracted by the dynamic time warping. Second, the anomaly is exposed by Matrix Profile during subsequence similarity search. The knee onset is detected when the temporal dynamics of the new cycle exceed the control limit and the profile index indicates a change in regime. Finally
    
[^11]: 面向多次重复用户-物品交互的序列感知物品推荐

    Sequence-aware item recommendations for multiply repeated user-item interactions. (arXiv:2304.00578v1 [cs.IR])

    [http://arxiv.org/abs/2304.00578](http://arxiv.org/abs/2304.00578)

    本论文设计了一种新的推荐物品方法，受到自然语言处理技术的启发，可以有效地处理用户多次与同一物品交互或用户偏好随时间变化的情况。

    

    推荐系统是机器学习和数据科学最成功的应用之一，在电子商务、媒体流内容、电子邮件营销等各种应用领域取得了成功。这些系统的主要目标是分析过去的用户行为来预测哪些物品最受用户喜爱。它们通常使用矩阵完成技术（如协同过滤或矩阵分解）进行构建。然而，尽管在许多实际应用中这些方法取得了巨大成功，但在用户可能多次与同一物品进行交互或用户偏好随时间变化的情况下，它们的有效性仍然受到限制。我们受到自然语言处理技术处理和分析文本序列的启发，设计了一种新方法来推荐物品。

    Recommender systems are one of the most successful applications of machine learning and data science. They are successful in a wide variety of application domains, including e-commerce, media streaming content, email marketing, and virtually every industry where personalisation facilitates better user experience or boosts sales and customer engagement. The main goal of these systems is to analyse past user behaviour to predict which items are of most interest to users. They are typically built with the use of matrix-completion techniques such as collaborative filtering or matrix factorisation. However, although these approaches have achieved tremendous success in numerous real-world applications, their effectiveness is still limited when users might interact multiple times with the same items, or when user preferences change over time.  We were inspired by the approach that Natural Language Processing techniques take to compress, process, and analyse sequences of text. We designed a re
    
[^12]: 基于深度学习时序框架的零售客户流失建模

    Modelling customer churn for the retail industry in a deep learning based sequential framework. (arXiv:2304.00575v1 [stat.ML])

    [http://arxiv.org/abs/2304.00575](http://arxiv.org/abs/2304.00575)

    本文提出了一种基于深度学习的时序框架，能够预测非合同情况下哪些客户存在流失风险，这种模型只需要基于客户行为就能获得具有个体特征的生存模型，避免了繁琐而耗时的特征工程过程。

    

    随着世界各地的零售商增加针对不同受众的定向营销活动，准确预测哪些客户最有可能流失对于营销团队来说至关重要，以增加业务利润。本研究提出了一个深度生存框架，用于预测哪些客户在非合同情况下停止与零售公司购买。通过利用循环神经网络学习生存模型参数，我们能够仅基于客户行为获取基于个体水平消费行为的生存模型，避免训练机器学习模型时耗时的特征工程过程。

    As retailers around the world increase efforts in developing targeted marketing campaigns for different audiences, predicting accurately which customers are most likely to churn ahead of time is crucial for marketing teams in order to increase business profits. This work presents a deep survival framework to predict which customers are at risk of stopping to purchase with retail companies in non-contractual settings. By leveraging the survival model parameters to be learnt by recurrent neural networks, we are able to obtain individual level survival models for purchasing behaviour based only on individual customer behaviour and avoid time-consuming feature engineering processes usually done when training machine learning models.
    
[^13]: 基于Copula的多元零膨胀连续数据密度估计模型

    Copula-Based Density Estimation Models for Multivariate Zero-Inflated Continuous Data. (arXiv:2304.00537v1 [stat.ME])

    [http://arxiv.org/abs/2304.00537](http://arxiv.org/abs/2304.00537)

    本文提出了两种基于copula的密度估计模型，可以处理多元相关性的零膨胀连续变量，引入矫正高斯copula以应对绑定数据问题，并提出高效的方法进行参数估计和可能性计算。

    

    零膨胀连续数据在许多领域中普遍出现，其中观察到许多完全为零值数据，而其他数据则以连续方式分布。由于其分布中离散和连续的混合结构，多元情况下的统计分析非常具有挑战性。本文提出了两种基于copula的密度估计模型，可以处理零膨胀连续变量之间的多元相关性。为了克服由于零膨胀数据中的绑定数据问题使用copulas的困难，我们提出了一种新类型的copula，即矫正高斯copula，并提出了参数估计和可能性计算的高效方法。数值实验证明了我们的提议相对于传统密度估计方法的优越性。

    Zero-inflated continuous data ubiquitously appear in many fields, in which lots of exactly zero-valued data are observed while others distribute continuously. Due to the mixed structure of discreteness and continuity in its distribution, statistical analysis is challenging especially for multivariate case. In this paper, we propose two copula-based density estimation models that can cope with multivariate correlation among zero-inflated continuous variables. In order to overcome the difficulty in the use of copulas due to the tied-data problem in zero-inflated data, we propose a new type of copula, rectified Gaussian copula, and present efficient methods for parameter estimation and likelihood computation. Numerical experiments demonstrates the superiority of our proposals compared to conventional density estimation methods.
    
[^14]: TSCI：使用两阶段曲率识别在无效仪器下执行因果推断

    TSCI: two stage curvature identification for causal inference with invalid instruments. (arXiv:2304.00513v1 [stat.ME])

    [http://arxiv.org/abs/2304.00513](http://arxiv.org/abs/2304.00513)

    TSCI 提出了无效仪器条件下使用两阶段算法进行因果推断，不需要强识别假设，可通过数据自适应方式捕捉仪器违规行为。

    

    TSCI 在 R 统计计算环境中执行观测数据下的治疗效应估计，即使仪器无效也可有效运行。现有的仪器变量方法依赖于强且难以验证的识别假设，这限制了它们的实际应用。TSCI 不需要经典的仪器变量识别条件，通过两阶段算法实现。在第一阶段，机器学习用于处理治疗模型中的非线性和交互作用。在第二阶段，通过数据自适应方式选择空间来捕捉仪器违规行为，然后将这些违规行为投影出来以估计治疗效应。

    TSCI implements treatment effect estimation from observational data under invalid instruments in the R statistical computing environment. Existing instrumental variable approaches rely on arguably strong and untestable identification assumptions, which limits their practical application. TSCI does not require the classical instrumental variable identification conditions and is effective even if all instruments are invalid. TSCI implements a two-stage algorithm. In the first stage, machine learning is used to cope with nonlinearities and interactions in the treatment model. In the second stage, a space to capture the instrument violations is selected in a data-adaptive way. These violations are then projected out to estimate the treatment effect.
    
[^15]: 无限维储备计算

    Infinite-dimensional reservoir computing. (arXiv:2304.00490v1 [cs.LG])

    [http://arxiv.org/abs/2304.00490](http://arxiv.org/abs/2304.00490)

    本文证明了一类新的概念类别的储备计算逼近和泛化边界，使得机器学习算法具有更好的实现性能。

    

    本文证明了一类新的概念类别的储备计算逼近和泛化边界，将所谓的广义Barron函数扩展到动态环境。这个新类别的特征是具有某种积分表示的读出，建立在无限维状态空间系统上。结果表明，这个类别非常丰富，并具有有用的特点和全局逼近性质。

    Reservoir computing approximation and generalization bounds are proved for a new concept class of input/output systems that extends the so-called generalized Barron functionals to a dynamic context. This new class is characterized by the readouts with a certain integral representation built on infinite-dimensional state-space systems. It is shown that this class is very rich and possesses useful features and universal approximation properties. The reservoir architectures used for the approximation and estimation of elements in the new class are randomly generated echo state networks with either linear or ReLU activation functions. Their readouts are built using randomly generated neural networks in which only the output layer is trained (extreme learning machines or random feature neural networks). The results in the paper yield a fully implementable recurrent neural network-based learning algorithm with provable convergence guarantees that do not suffer from the curse of dimensionalit
    
[^16]: 通过相似性结构解析对比学习机制：理论分析

    Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis. (arXiv:2304.00395v1 [cs.LG])

    [http://arxiv.org/abs/2304.00395](http://arxiv.org/abs/2304.00395)

    通过一个新的公式，本文理论分析了基于核的对比学习损失的特点，证明了它能描述学习表示的结构和表现，提供一个新的限制方法，并在多个基准测试中验证其有效性。

    

    对比学习是一种有效的自监督表示学习方法。虽然近期的研究在理论上对对比学习有了一定的了解，但对于如何表征学习表示的聚类仍然有限。本文旨在从理论角度阐明这种聚类的特征。为此，我们考虑一种基于核的对比学习框架，称为核对比学习（KCL），核函数在将我们的理论结果应用于其他框架时起重要作用。我们利用统计依赖观点引入一个学习表示的相似性结构的公式。我们通过这个公式研究了基于核的对比损失的理论性质。我们首先证明这个公式表征了利用核对比学习框架学习的表示结构。我们证明了一个新的上界，对于负样本的边际分布有一个温和的条件，期望对比损失受到限制。此外，我们还确定了基于核的对比损失是一种新的信息论下界的特例，这促使我们开发一个新的目标，可以进一步约束学习表示在输入的亲密性和不同样本之间的可分性方面。最后，我们在几个基准测试上进行了实验，这些实验支持我们的理论发现，并表明了所提出的目标的有效性。

    Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper boun
    
[^17]: 重启贝叶斯在线变点检测用于非平稳马尔科夫决策过程

    Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])

    [http://arxiv.org/abs/2304.00232](http://arxiv.org/abs/2304.00232)

    该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。

    

    我们考虑在一个非平稳的强化学习（RL）环境中进行学习的问题，其中该设置可以被完全描述为分段平稳的离散时间马尔科夫决策过程（MDP）。我们引入了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD）变体，该算法适用于从更一般的多项式分布中生成的输入流，并在误警率和检测延迟方面提供接近最优的理论保证。基于此，我们提出了一种针对从多项式分布中采样的状态转移内核的MDPs的改进版本UCRL2算法，我们称之为R-BOCPD-UCRL2。我们进行了有限时间的性能分析，并表明R-BOCPD-UCRL2具有有利的遗憾界的$O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$。

    We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$,
    
[^18]: 基于扩散映射的粒子系统用于生成模型

    Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])

    [http://arxiv.org/abs/2304.00200](http://arxiv.org/abs/2304.00200)

    本文提出一种新型扩散映射粒子系统(DMPS)，可以用于高效生成建模，实验表明在包含流形结构的合成数据集上取得了比其他方法更好的效果。

    

    本文提出了一种新颖的扩散映射粒子系统(DMPS)，用于生成建模，该方法基于扩散映射和Laplacian调整的Wasserstein梯度下降（LAWGD）。扩散映射被用来从样本中近似Langevin扩散过程的生成器，从而学习潜在的数据生成流形。另一方面，LAWGD能够在合适的核函数选择下高效地从目标分布中抽样，我们在这里通过扩散映射计算生成器的谱逼近来构造核函数。数值实验表明，我们的方法在包括具有流形结构的合成数据集上优于其他方法。

    We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
    
[^19]: 无碰撞运输图在流行学习中的应用

    Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])

    [http://arxiv.org/abs/2304.00199](http://arxiv.org/abs/2304.00199)

    本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。

    

    本文研究了引入于[Nurbekyan et al.，2020]的无碰撞运输图在图像数据的流形学习中的应用。近年来，在表示类似运动或变形现象的数据中，应用基于运输的距离和特征的研究大幅增加。事实上，固定位置比较强度通常无法显示数据结构。在[Nurbekyan et al.，2020]中开发的无碰撞图和距离类似于最优传输(OT)图的几何特征但由于无需优化，计算成本要便宜得多。本文证明无碰撞距离提供单个概率测度的平移(分别是伸缩)和装备欧几里得距离的平移(分别是伸缩)向量之间的等距性。此外，我们证明，无碰撞运输图以及OT和线性OT图，一般来说不能为旋转提供等距性。

    In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
    
[^20]: 抽象器：基于Transformer的符号消息传递和关系推理模块

    Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])

    [http://arxiv.org/abs/2304.00195](http://arxiv.org/abs/2304.00195)

    该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    

    该论文提出了一个框架，将关系学习转化为Transformer模型，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
    
[^21]: Hilbert-Valued参数的一步估计

    One-Step Estimation of Differentiable Hilbert-Valued Parameters. (arXiv:2303.16711v1 [math.ST])

    [http://arxiv.org/abs/2303.16711](http://arxiv.org/abs/2303.16711)

    本文介绍了一种针对Hilbert-Valued参数进行一步估计的方法，并提供一种适用于缺乏重现核的Hilbert空间的解决方案。

    

    本文提出了对光滑Hilbert-Valued参数的估计器，其中光滑性由逐路径可微条件表征。当参数空间是重现核Hilbert空间时，我们提供了一种获取高效和相关置信区间的方法。这些估计器对应于基于Hilbert-Valued有效影响函数的交叉一步估计器的概括。我们提供了理论保证，即使使用任意的辅助函数估计器，包括基于机器学习技术的估计器。我们表明，即使缺乏重现核的Hilbert空间，只要参数具有高效的影响函数，这些结果自然地可以扩展到该空间。然而，我们也揭示了不幸的事实，当不存在重现核时，许多有趣的参数即使它们在路径上是可微的，也缺乏有效的影响函数。为了处理这些情况，我们提出了一种正则化的方法。

    We present estimators for smooth Hilbert-valued parameters, where smoothness is characterized by a pathwise differentiability condition. When the parameter space is a reproducing kernel Hilbert space, we provide a means to obtain efficient, root-n rate estimators and corresponding confidence sets. These estimators correspond to generalizations of cross-fitted one-step estimators based on Hilbert-valued efficient influence functions. We give theoretical guarantees even when arbitrary estimators of nuisance functions are used, including those based on machine learning techniques. We show that these results naturally extend to Hilbert spaces that lack a reproducing kernel, as long as the parameter has an efficient influence function. However, we also uncover the unfortunate fact that, when there is no reproducing kernel, many interesting parameters fail to have an efficient influence function, even though they are pathwise differentiable. To handle these cases, we propose a regularized on
    
[^22]: TRAK: 刻画大规模模型行为

    TRAK: Attributing Model Behavior at Scale. (arXiv:2303.14186v1 [stat.ML])

    [http://arxiv.org/abs/2303.14186](http://arxiv.org/abs/2303.14186)

    TRAK是一种适用于大规模、可微模型的数据归因方法，既有效又计算量可行。

    

    数据归因的目标是追踪模型预测结果的原始训练数据。虽然已经有很多工作致力于实现这一目标，但现有方法往往要求用户在计算效率和准确性之间做出选择。也就是说，在非凸场景（例如，深度神经网络领域）中，计算量可行的方法可能难以准确地归因模型预测结果，而在这类场景中有效的方法则需要训练数千个模型，这使得它们在大型模型或数据集中实际应用具有不可行性。在本文中，我们介绍了TRAK（随机投影核追踪），这是一种数据归因方法，适用于大规模、可微模型，既有效又计算量可行。具体来说，通过仅使用少量训练模型，TRAK 可以匹配需要训练数千模型才能得到的归因方法的性能。我们论证了TRAK 在各种模式和规模上的实用性。

    The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.  In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scal
    
[^23]: 不定概率神经网络

    Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])

    [http://arxiv.org/abs/2303.11536](http://arxiv.org/abs/2303.11536)

    本文提出了一种新型通用模型——不定概率神经网络；它可以进行无监督聚类和使用很小的神经网络处理大规模分类，其理论优势体现在新的概率理论和神经网络框架中。

    

    本文提出了一个称为IPNN的新型通用模型，它将神经网络和概率论结合在一起。在传统概率论中，概率的计算是基于事件的发生，而这在当前的神经网络中几乎不使用。因此，我们提出了一种新的概率理论，它是经典概率论的扩展，并使经典概率论成为我们理论的一种特殊情况。此外，对于我们提出的神经网络框架，神经网络的输出被定义为概率事件，并基于这些事件的统计分析，推导出分类任务的推理模型。IPNN展现了新的特性：它在进行分类的同时可以执行无监督聚类。此外，IPNN能够使用非常小的神经网络进行非常大的分类，例如100个输出节点的模型可以分类10亿类别。理论优势体现在新的概率理论和神经网络框架中，并且实验结果展示了IPNN在各种应用中的潜力。

    We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
    
[^24]: 张量分解的实对数典范阈值的上界及其在贝叶斯推断中的应用

    Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and its Application to Bayesian Inference. (arXiv:2303.05731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05731](http://arxiv.org/abs/2303.05731)

    本研究利用代数几何方法给出了张量分解的实对数典范阈值的上界，并推导了其在贝叶斯推断中的应用理论误差，揭示了张量分解的数学性质。

    

    张量分解现在被用于数据分析、信息压缩和知识恢复。然而，张量分解的数学性质尚未完全阐明，因为它是一种奇异学习机。在本文中，我们利用代数几何方法给出了张量分解的实对数典范阈值(RLCT)的上界，并从理论上推导了其贝叶斯泛化误差。我们还通过数值实验给出了其数学性质的考虑。

    Tensor decomposition is now being used for data analysis, information compression, and knowledge recovery. However, the mathematical property of tensor decomposition is not yet fully clarified because it is one of singular learning machines. In this paper, we give the upper bound of its real log canonical threshold (RLCT) of the tensor decomposition by using an algebraic geometrical method and derive its Bayesian generalization error theoretically. We also give considerations about its mathematical property through numerical experiments.
    
[^25]: 带有双向先验模型的向量量化时间序列生成

    Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04743](http://arxiv.org/abs/2303.04743)

    本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。

    

    时间序列生成研究主要集中在使用生成对抗网络（GAN）与递归神经网络（RNN）变体相结合。然而，训练 GAN 的基本限制和挑战仍然存在。此外，RNN族通常在远程时间步之间的时间一致性方面存在困难。受到图像生成领域成功的启发，我们提出 TimeVQVAE，这是我们所知道的第一个使用向量量化（VQ）技术解决 TSG 问题的工作。此外，离散潜在空间的先验使用双向变压器模型进行学习，可以更好地捕捉全局时间一致性。我们还提出在时间 - 频率域中进行 VQ 建模，分为低频（LF）和高频（HF）。这使我们能够保留时间序列的重要特征，并生成质量更好、模块性变化更快的新合成信号。

    Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
    
[^26]: 基于共享最近邻信息的图拉普拉斯算子和基于 k 近邻图的图拉普拉斯算子具有相同的极限

    Graph Laplacians on Shared Nearest Neighbor graphs and graph Laplacians on $k$-Nearest Neighbor graphs having the same limit. (arXiv:2302.12399v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.12399](http://arxiv.org/abs/2302.12399)

    本文开创性地研究了共享最近邻图和图拉普拉斯算子，证明了共享最近邻图拉普拉斯算子和 k 近邻图拉普拉斯算子具有相同的连续极限，同时发现它们的图拉普拉斯算子逐点收敛速率与 $(k/n)^{1/m}$ 成线性关系。

    

    共享最近邻图是一种使用共享最近邻信息构建的图形结构，其是一种基于主要的 k 近邻度量诱导的排名的二级相似度测量。SNN 测量被认为比传统距离测量法更不容易受到维数诅咒的影响，因此使用 SNN 图的方法已经被广泛应用于应用程序中，特别是在聚类高维数据集和在高维数据子空间中查找异常值中。尽管如此，仍未对 SNN 图和图拉普拉斯算子进行理论研究。在这项开创性工作中，我们首次在这个方向上做出了贡献。我们展示了 SNN 图拉普拉斯算子的大规模渐近性达到了一致的连续极限；这个极限与 k 近邻图拉普拉斯算子的极限相同。此外，我们证明了图拉普拉斯算子的逐点收敛速率与 $(k/n)^{1/m}$ 成线性关系，概率高。

    A Shared Nearest Neighbor (SNN) graph is a type of graph construction using shared nearest neighbor information, which is a secondary similarity measure based on the rankings induced by a primary $k$-nearest neighbor ($k$-NN) measure. SNN measures have been touted as being less prone to the curse of dimensionality than conventional distance measures, and thus methods using SNN graphs have been widely used in applications, particularly in clustering high-dimensional data sets and in finding outliers in subspaces of high dimensional data. Despite this, the theoretical study of SNN graphs and graph Laplacians remains unexplored. In this pioneering work, we make the first contribution in this direction. We show that large scale asymptotics of an SNN graph Laplacian reach a consistent continuum limit; this limit is the same as that of a $k$-NN graph Laplacian. Moreover, we show that the pointwise convergence rate of the graph Laplacian is linear with respect to $(k/n)^{1/m}$ with high proba
    
[^27]: I$^2$SB：图像到图像的Schr\"odinger桥

    I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05872](http://arxiv.org/abs/2302.05872)

    提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。

    

    本文提出了一种新的条件扩散模型，即图像到图像的Schr\"odinger桥（I$^2$SB），直接学习两个给定分布之间的非线性扩散过程。这些扩散桥对于图像恢复特别有用，因为退化图像是重构清晰图像的结构信息先验。 I$^2$SB属于一类可处理的Schr\"odinger桥模型，它是得分模型的非线性扩展，其边界对的边缘分布可以在解析上计算。这种通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，进而采用在标准扩散模型中使用的实用技术，使得I$^2$SB训练具有可扩展性。在ImageNet 256x256上，我们验证了I$^2$SB在各种图像恢复任务中的性能，包括修复，超分辨率，去模糊和JPEG恢复，并表明I$^2$SB超过了标准条件扩散模型，具有更可解释的生成过程。

    We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. 
    
[^28]: 物理学知识指导的高斯过程回归应用于解决线性偏微分方程

    Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12474](http://arxiv.org/abs/2212.12474)

    本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。

    

    线性偏微分方程是一类重要且广泛应用的机械模型，描述了物理过程，例如热传导、电磁学和波传播等。实践中，通常使用基于离散化的专门数值方法来解决偏微分方程。这些求解器通常使用未知模型参数的估计值以及如果可用的话，物理测量值用于初始化。这些求解器经常嵌入到具有下游应用的更大的科学模型中，因此误差量化起着关键作用。然而，经典的偏微分方程求解器忽略参数和测量不确定性，可能无法产生一致性的估计值，以用于计算其固有的逼近误差。本文通过将求解线性偏微分方程解释为物理学知识指导的高斯过程回归来解决这个问题。我们的框架基于高斯过程推理定理的一个关键推广，该定理适用于通过任意界面进行观察的情况。

    Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
    
[^29]: 面向稀有事件的动态因果发现：一种非参数条件独立性检验

    Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16596](http://arxiv.org/abs/2211.16596)

    该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。

    

    与稀有事件相关联的因果现象在许多工程问题中都存在，例如针对风险的安全分析、事故分析和预防以及极值理论等。然而，当前的因果发现方法往往无法发现在随机变量之间的原因联系，特别是在变动环境下，仅在变量第一次经历低概率实现时才会显现。为了解决这个问题，我们引入了一种新的统计独立性检验方法，用于从发生稀有但具有重要影响的时间不变动态系统收集的数据中进行因果探索。具体而言，我们利用底层数据的时间不变性来构建一个叠加的数据集，其中包括在不同时间步骤之前稀有事件发生前系统状态的数据。然后我们设计了一个在重新组织的数据上进行条件独立性检验的方法。我们提供了我们方法一致性的非渐近样本复杂度界限，并验证了它在各种模拟和真实世界数据集上的性能。

    Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
    
[^30]: 基于比较的层次聚类的收益函数

    A Revenue Function for Comparison-Based Hierarchical Clustering. (arXiv:2211.16459v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16459](http://arxiv.org/abs/2211.16459)

    本论文提出了一种收益函数，可以仅使用比较来衡量基于比较的分层聚类的质量。

    

    基于比较的学习解决的问题是当我们只有形式为“目标A与B相比较比C更相似”这样的比较，而没有显式特征或成对相似度时的学习问题。最近，已经证明在层次聚类中，可以直接使用这样的比较实现单一和完全链接，同时已经提出了几种算法来模拟平均链接的行为。因此，使用仅比较找到层次结构（或树状图）是一个被充分理解的问题。然而，当没有基准事实或显式相似性时，评估它们的意义仍然是一个开放性的问题。在本文中，我们通过提出一个新的收益函数来弥补这一差距，该函数允许我们仅使用比较来衡量树状图的好坏。文章还表明，该函数与使用成对相似性的层次聚类的Dasgupta成本密切相关。在理论方面，我们使用拥有好的性质的收益函数来分析比较的相关性，并推导了一些高斯过程的相关结论。

    Comparison-based learning addresses the problem of learning when, instead of explicit features or pairwise similarities, one only has access to comparisons of the form: \emph{Object $A$ is more similar to $B$ than to $C$.} Recently, it has been shown that, in Hierarchical Clustering, single and complete linkage can be directly implemented using only such comparisons while several algorithms have been proposed to emulate the behaviour of average linkage. Hence, finding hierarchies (or dendrograms) using only comparisons is a well understood problem. However, evaluating their meaningfulness when no ground-truth nor explicit similarities are available remains an open question.  In this paper, we bridge this gap by proposing a new revenue function that allows one to measure the goodness of dendrograms using only comparisons. We show that this function is closely related to Dasgupta's cost for hierarchical clustering that uses pairwise similarities. On the theoretical side, we use the propo
    
[^31]: 论文标题：量化标签噪声对联邦学习的影响

    Quantifying the Impact of Label Noise on Federated Learning. (arXiv:2211.07816v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07816](http://arxiv.org/abs/2211.07816)

    本文量化了标签噪声对FL的影响。实验结果表明，随着噪声水平的增加，全局模型的准确性会线性下降，同时会导致FL训练的收敛速度减缓和全局模型过拟合。

    

    联邦学习是一种分布式机器学习范例，客户端可以使用本地（人为生成的）数据集协同训练模型。然而，现有研究集中在FL算法的开发上以解决客户端之间的数据异质性，而在FL中数据质量（如标签噪声）这一重要问题被忽视。本文旨在通过对标签噪声对FL的影响进行定量研究来填补这一空白。我们推导了一种上界来衡量客户端标签噪声水平对泛化误差的影响，并使用各种FL算法在MNIST和CIFAR-10数据集上进行实验。我们的实证结果表明，随着噪声水平的增加，全局模型准确性会线性下降，这与我们的理论分析相一致。我们进一步发现，在标签噪声较高时，标签噪声会减缓FL训练的收敛速度，并导致全局模型过拟合。

    Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a model using their local (human-generated) datasets. While existing studies focus on FL algorithm development to tackle data heterogeneity across clients, the important issue of data quality (e.g., label noise) in FL is overlooked. This paper aims to fill this gap by providing a quantitative study on the impact of label noise on FL. We derive an upper bound for the generalization error that is linear in the clients' label noise level. Then we conduct experiments on MNIST and CIFAR-10 datasets using various FL algorithms. Our empirical results show that the global model accuracy linearly decreases as the noise level increases, which is consistent with our theoretical analysis. We further find that label noise slows down the convergence of FL training, and the global model tends to overfit when the noise level is high.
    
[^32]: 增强贝叶斯神经网络在宏观经济和金融领域的应用

    Enhanced Bayesian Neural Networks for Macroeconomics and Finance. (arXiv:2211.04752v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2211.04752](http://arxiv.org/abs/2211.04752)

    该论文提出的增强贝叶斯神经网络能够模拟大量宏观经济和金融变量的通用非线性和时间变化，具有潜在的政策决策应用价值。

    

    我们开发了贝叶斯神经网络 (BNNs)，它们能够模拟可能包含大量宏观经济和金融变量的通用非线性和时间变化。从方法论上讲，我们允许对网络进行一般规格的说明，可以应用于密集或稀疏数据集，并结合各种激活功能、可能非常多的神经元和误差项的随机波动。从计算的角度来看，我们为引入的通用BNNs开发了快速高效的估计算法。从实证的角度来看，我们用模拟数据和一组常见的宏观金融应用来展示我们的BNNs可以实际使用，特别是对于观测目标变量的横截面或时间序列分布的尾部，该方法特别适用于在不寻常的时间做出决策方面具有信息量。

    We develop Bayesian neural networks (BNNs) that permit to model generic nonlinearities and time variation for (possibly large sets of) macroeconomic and financial variables. From a methodological point of view, we allow for a general specification of networks that can be applied to either dense or sparse datasets, and combines various activation functions, a possibly very large number of neurons, and stochastic volatility (SV) for the error term. From a computational point of view, we develop fast and efficient estimation algorithms for the general BNNs we introduce. From an empirical point of view, we show both with simulated data and with a set of common macro and financial applications that our BNNs can be of practical use, particularly so for observations in the tails of the cross-sectional or time series distributions of the target variables, which makes the method particularly informative for policy making in uncommon times.
    
[^33]: 平滑单调随机变分不等式与鞍点问题：综述

    Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems: A Survey. (arXiv:2208.13592v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.13592](http://arxiv.org/abs/2208.13592)

    本文综述了解决平滑（强）单调随机变分不等式的方法。

    

    本文综述了解决平滑（强）单调随机变分不等式的方法。首先，我们给出了随机方法最终演变的确定性基础。然后我们回顾了一般随机公式的方法，看看有限和设置。最后部分是致力于各种最近的（不一定是随机的）算法变分不等式的进步的。

    This paper is a survey of methods for solving smooth (strongly) monotone stochastic variational inequalities. To begin with, we give the deterministic foundation from which the stochastic methods eventually evolved. Then we review methods for the general stochastic formulation, and look at the finite sum setup. The last parts of the paper are devoted to various recent (not necessarily stochastic) advances in algorithms for variational inequalities.
    
[^34]: 线性结构方程模型的因果赌博算法

    Causal Bandits for Linear Structural Equation Models. (arXiv:2208.12764v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.12764](http://arxiv.org/abs/2208.12764)

    本文提出了针对线性结构方程模型的因果赌博算法，摒弃了已知干预分布或其边缘分布的假设。

    

    本文研究了设计最优干预顺序，以最小化关于后见最佳干预的累积遗憾的问题。这是一个因果赌博问题，主要关注线性结构方程模型（SEM）和软干预的因果赌博问题。假设图的结构已知并且具有N个节点，每个节点假设两个线性机制，一个软干预和一个观测值，共产生$2^N$种可能干预。大部分现有的因果赌博算法假设至少已经完全指定了奖励节点的父节点干预分布。然而，在中等大小的图中，需要获得$2^N$个干预分布（每个干预对应一个干预分布），这变得不切实际。本文摒弃了知道这些分布或其边缘分布的假设，为频率派提出了两种算法。

    This paper studies the problem of designing an optimal sequence of interventions in a causal graphical model to minimize cumulative regret with respect to the best intervention in hindsight. This is, naturally, posed as a causal bandit problem. The focus is on causal bandits for linear structural equation models (SEMs) and soft interventions. It is assumed that the graph's structure is known and has $N$ nodes. Two linear mechanisms, one soft intervention and one observational, are assumed for each node, giving rise to $2^N$ possible interventions. Majority of the existing causal bandit algorithms assume that at least the interventional distributions of the reward node's parents are fully specified. However, there are $2^N$ such distributions (one corresponding to each intervention), acquiring which becomes prohibitive even in moderate-sized graphs. This paper dispenses with the assumption of knowing these distributions or their marginals. Two algorithms are proposed for the frequentist
    
[^35]: 带信息协方差的贝叶斯优化

    Bayesian Optimization with Informative Covariance. (arXiv:2208.02704v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.02704](http://arxiv.org/abs/2208.02704)

    提出了一种新颖的用于优化的信息丰富协方差函数，利用非平稳性来编码对搜索空间中某些区域的偏好，并在优化过程中自适应地促进局部探索，以提高贝叶斯优化在高维空间中的样本效率。

    

    贝叶斯优化是一种处理未知和昂贵目标的全局优化方法。它将一个拟合贝叶斯回归模型与一个收获函数结合起来，以决定在哪里评估目标。典型的回归模型由具有平稳协方差函数的高斯过程表示。然而，这些函数无法表达输入相关的先验信息，包括最优点可能出现的位置。平稳模型的普及导致了通过信息丰富的均值函数利用先验信息的常见做法。本文中，我们强调这些模型在高维情况下可能表现不佳。我们提出了一种新颖的用于优化的信息丰富协方差函数，利用非平稳性来编码对搜索空间中某些区域的偏好，并在优化过程中自适应地促进局部探索。我们证明了所提出的函数可以提高贝叶斯优化在高维空间中的样本效率，在基准问题中达到了最先进的性能。

    Bayesian optimization is a methodology for global optimization of unknown and expensive objectives. It combines a surrogate Bayesian regression model with an acquisition function to decide where to evaluate the objective. Typical regression models are given by Gaussian processes with stationary covariance functions. However, these functions are unable to express prior input-dependent information, including possible locations of the optimum. The ubiquity of stationary models has led to the common practice of exploiting prior information via informative mean functions. In this paper, we highlight that these models can perform poorly, especially in high dimensions. We propose novel informative covariance functions for optimization, leveraging nonstationarity to encode preferences for certain regions of the search space and adaptively promote local exploration during optimization. We demonstrate that the proposed functions can increase the sample efficiency of Bayesian optimization in high
    
[^36]: 何时重新初始化神经网络是有效的？

    When Does Re-initialization Work?. (arXiv:2206.10011v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10011](http://arxiv.org/abs/2206.10011)

    研究表明，当没有其他正则化技术时，重新初始化神经网络有助于提高泛化性能。但是，当它与其他正则化技术一起使用时，对泛化性能的额外帮助很小，尽管最佳泛化性能变得更加稳定。

    

    在最近的研究中观察到，重新初始化神经网络在训练过程中有助于提高泛化性能。然而，在深度学习实践中，这种方法并不被广泛采用，也不常用于最先进的训练协议中。因此，本文提出了一个问题，即重新初始化什么时候有效，是否应该与数据增强、权重衰减和学习率调整等正则化技术一起使用。本研究对标准训练与一些重新初始化方法进行了广泛的实证比较，训练了超过15,000个模型，并在各种图像分类基准上进行了测试，以回答这个问题。我们首先确定，当没有任何其他正则化存在时，这些方法可以在泛化方面持续改善。然而，当这些方法与其他经过精心调整的正则化技术一起使用时，重新初始化方法对泛化性能几乎没有额外的帮助，尽管在这种情况下，最佳泛化性能变得更加稳定。

    Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less
    
[^37]: 分类中的良性过拟合：更大模型的发现可证明对抗标签噪声

    Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models. (arXiv:2206.00501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00501](http://arxiv.org/abs/2206.00501)

    本文发现良性过拟合在存在标签噪声的情况下可能会失败，提醒未来需要理解欠拟合制度下的隐式偏见。

    

    良性过拟合的研究为超参数化深度学习模型的成功提供了见解。本文研究了过拟合是否在真实世界的分类任务中真的是良性的。我们开始观察到一个 ResNet 模型在 Cifar10 上表现良好，但在 ImageNet 上则不良。为了了解为什么良性过拟合在 ImageNet 实验中失败，我们在一个比数据点数量不明显大的限定条件下从理论上分析了良性过拟合。在这个轻微超参数化的设置下，我们的分析发现了一个相变：与之前的重超参数化设置不同，当存在标签噪声时，良性过拟合现在可能会失败。我们的分析解释了我们的经验观察，并通过一组 ResNet 的控制实验进行了验证。我们的工作强调了理解欠拟合制度下的隐式偏见的重要性，作为未来的一个方向。

    Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.
    
[^38]: 平均调整关联度：高维混淆因素的高效估计

    Average Adjusted Association: Efficient Estimation with High Dimensional Confounders. (arXiv:2205.14048v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.14048](http://arxiv.org/abs/2205.14048)

    本研究提出了一个总结衡量标准——平均调整关联度（AAA），用于评估一个具有混淆影响的异质群体中的关联程度。并且我们提出了高效的估计方法，可用于各种采样场景。

    

    对数比率是衡量二元结果和暴露变量关联度的一种既定指标。尽管其被广泛使用，但有关如何通过平均方式总结具有混淆因素的对数比率的讨论却很有限。为了解决这个问题，我们提出了平均调整关联度（AAA），它是一个调整了观察到的混淆因素的异质群体中的关联度的总结衡量标准。为了方便使用，我们还开发了高效的双重/无偏机器学习（DML）AAA估计器。我们的DML估计器使用了两种等效的有效影响函数形式，并适用于各种采样场景，包括随机采样，基于结果的采样和基于暴露的采样。通过真实数据和模拟，我们证明了我们提出的估算方法在测量AAA方面的实用性和有效性。

    The log odds ratio is a well-established metric for evaluating the association between binary outcome and exposure variables. Despite its widespread use, there has been limited discussion on how to summarize the log odds ratio as a function of confounders through averaging. To address this issue, we propose the Average Adjusted Association (AAA), which is a summary measure of association in a heterogeneous population, adjusted for observed confounders. To facilitate the use of it, we also develop efficient double/debiased machine learning (DML) estimators of the AAA. Our DML estimators use two equivalent forms of the efficient influence function, and are applicable in various sampling scenarios, including random sampling, outcome-based sampling, and exposure-based sampling. Through real data and simulations, we demonstrate the practicality and effectiveness of our proposed estimators in measuring the AAA.
    
[^39]: 深度线性网络的精确解析解

    Exact Solutions of a Deep Linear Network. (arXiv:2202.04777v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.04777](http://arxiv.org/abs/2202.04777)

    本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，结果表明权重衰减与模型架构的强烈交互作用会在多于1个隐藏层的网络中创建不良极小值，并表明常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。

    

    本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，这是理解神经网络理论中的基础模型。我们的结果表明，在深度神经网络架构中，零是一个特殊的点。我们展示了权重衰减与模型架构的强烈交互作用，并能够在具有超过 $1$ 个隐藏层的网络中创建不良极小值，这与仅有 $1$ 个隐藏层的网络有质的不同。实际上，我们的结果意味着常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。

    This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general.
    
[^40]: 不完美信息博弈中的近似最优学习

    Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01752](http://arxiv.org/abs/2202.01752)

    本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。

    

    本文解决了学习不完美信息广义博弈的近似最优算法设计的开放性问题。我们提出了第一种算法系列，仅需要 $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ 局游戏即可在两人零和博弈中找到一个 $\varepsilon$-近似纳什均衡，其中 $X,Y$ 是信息集的数量，$A,B$ 是两名玩家的行动数。这比已知的样本复杂度 $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ 有着 $\widetilde{\mathcal{O}}(\max\{X, Y\})$ 的巨大改进，并且在对数因子内与信息理论下限一致。我们通过两种新算法实现了这种样本复杂度：平衡在线镜面下降和平衡反事实后悔最小化。这两种算法都依赖于将“平衡探索策略”集成到它们的经典对手中的新方法。此外，我们还将我们的结果扩展到了更广泛的支持不完美信息博弈的二人博弈和多人博弈中。

    This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
    
[^41]: 利用正向-反向随机微分方程理论对薛定谔桥进行似然训练

    Likelihood Training of Schr\"odinger Bridge using Forward-Backward SDEs Theory. (arXiv:2110.11291v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.11291](http://arxiv.org/abs/2110.11291)

    本文提出了一种新的计算框架，用于基于正向-反向随机微分方程理论对薛定谔桥进行似然训练。通过这个框架，可以构建SB的似然目标，这可以成为现代深度生成模型训练的替代方法。

    

    薛定谔桥（SB）是一种熵正则化的最优输运问题，与基于分数的生成模型（SGM）相比，在深度生成建模中由于其数学灵活性而越来越受到关注。然而，尚不清楚SB的优化原则是否与现代深度生成模型的训练相关，后者通常依赖于构建对数似然目标。这引发了关于SB模型作为生成应用的原则性替代方法的适用性问题。在本工作中，我们提出了一种新的计算框架，用于基于正向-反向随机微分方程理论对SB模型进行似然训练——这是一种出现在随机最优控制中的数学方法，它将SB的最优性条件转化为一组SDE。关键是，这些SDE可以用于构建SB的似然目标，令人惊讶的是，它广义地推广了SGM的一些特殊情况。这导致了一种新的优化方法。

    Schr\"odinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory - a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimi
    
[^42]: 一种面向大规模并行贝叶斯优化的投资组合方法

    A portfolio approach to massively parallel Bayesian optimization. (arXiv:2110.09334v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2110.09334](http://arxiv.org/abs/2110.09334)

    本文提出了一种可扩展的投资组合策略，面向大规模并行贝叶斯优化，解决并行设计选择复杂性问题，适用于噪声黑盒函数，单目标和多目标优化任务。

    

    为了缩短优化研究的时间，一种方法是并行评估设计，而不是逐个评估。针对昂贵的黑盒函数，已经提出了贝叶斯优化的批处理版本。它们通过建立黑盒函数的代理模型，通过填充准则同时选择多个设计。尽管现在有更多的计算资源可以支持大规模并行性，但选择几十个并行设计的策略由于选择更多设计的复杂性而变得有限。当黑盒又具有噪声时，需要更多的评估以及重复实验，这变得更加关键。在这里，我们提出一种可扩展的策略，可以本地与大规模批处理保持同步，专注于探索/开发权衡和投资组合分配。我们将这种方法与相关方法在噪声函数、单目标和多目标优化任务上进行比较。

    One way to reduce the time of conducting optimization studies is to evaluate designs in parallel rather than just one-at-a-time. For expensive-to-evaluate black-boxes, batch versions of Bayesian optimization have been proposed. They work by building a surrogate model of the black-box to simultaneously select multiple designs via an infill criterion. Still, despite the increased availability of computing resources that enable large-scale parallelism, the strategies that work for selecting a few tens of parallel designs for evaluations become limiting due to the complexity of selecting more designs. It is even more crucial when the black-box is noisy, necessitating more evaluations as well as repeating experiments. Here we propose a scalable strategy that can keep up with massive batching natively, focused on the exploration/exploitation trade-off and a portfolio allocation. We compare the approach with related methods on noisy functions, for mono and multi-objective optimization tasks. 
    
[^43]: 压缩通讯解决变分不等式的分布式方法及理论保证

    Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees. (arXiv:2110.03313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03313](http://arxiv.org/abs/2110.03313)

    本研究提出了MASHA1和MASHA2方法，可以减少在分布式训练中的通信量，并在获得可比性质量的模型的同时，解决变分不等式和鞍点问题。

    

    变分不等式和鞍点问题在机器学习应用中越来越受关注，包括对抗性学习、GAN、运输和强化优化等方面。为了训练高性能模型，需要依赖于并行和分布式计算。然而，在分布式训练中，计算节点之间的通信成为训练的关键瓶颈，特别是对于高维度和过参数化模型。因此，重要的是使用可以减少传输信息量的策略来降低训练中的通信量，同时获得具有可比性质量的模型。本文提出了MASHA1和MASHA2等基于压缩通讯的理论方法，用于解决变分不等式和鞍点问题的分布式方法。

    Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across various applications, we need to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods al
    
[^44]: 批量异步随机逼近的收敛性及在强化学习中的应用

    Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.03445](http://arxiv.org/abs/2109.03445)

    本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。

    

    随机逼近（SA）算法是一种广泛使用的概率方法，用于在仅可用函数的有噪测量情况下找到零点或固定点。目前的文献中，区分“同步”更新和“异步”更新，在“同步”更新中，每个猜测的组件都会在每个时间更新，而在“异步”更新中，仅更新一个组件。本文研究了一种中间情况，称为“批量异步随机逼近”（BASA），在这种情况下，每个时间点仅更新“当前估计解”的一些但不是全部的组件。BASA允许用户在内存需求和时间复杂度之间进行权衡。我们开发了一种通用方法，证明此类算法收敛于所研究映射的固定点。这些收敛证明使用比现有结果更弱的假设。具体而言，现有的收敛证明要求步长参数以适当的速率下降。相反，我们仅要求每个组件具有足够的更新频率。我们在强化学习领域展示了我们方法的有用性，证明了广泛使用的SARSA算法的批量异步版本的收敛性。

    The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
    
[^45]: 前向超分辨率：GAN如何学习逼近真实世界分布的分层生成模型

    Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions. (arXiv:2106.02619v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02619](http://arxiv.org/abs/2106.02619)

    本文研究了GAN如何有效地学习那些接近真实图像分布的分层生成分布，当一个分布具有前向超分辨率结构时，通过SGDA简单地训练GAN就能够实现高效学习。

    

    生成对抗网络（GAN）是学习高复杂度真实世界分布的最成功模型之一。然而，由于最小最大训练目标的高度非凸、非凹特性，GAN在理论上仍然是深度学习模型中最难理解的。本文正式研究了GAN如何有效地学习那些接近真实图像分布的分层生成分布。我们证明了，当一个分布具有我们所称的前向超分辨率结构时，通过随机梯度下降上升（SGDA）简单地训练GAN就能够有效地学习这个分布，无论是样本还是时间复杂度。我们还提供了实证证据，表明我们所假设的“前向超分辨率”在实践中非常自然，而我们在本文中研究的底层学习机制（通过SGDA理论上允许我们高效地训练GAN）模拟了实际的学习过程。

    Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of real-life images. We prove that when a distribution has a structure that we refer to as Forward Super-Resolution, then simply training generative adversarial networks using stochastic gradient descent ascent (SGDA) can learn this distribution efficiently, both in sample and time complexities. We also provide empirical evidence that our assumption "forward super-resolution" is very natural in practice, and the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via SGDA in theory) simulates the actual lear
    
[^46]: 探索轨迹推断的数学理论

    Towards a mathematical theory of trajectory inference. (arXiv:2102.09204v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.09204](http://arxiv.org/abs/2102.09204)

    本文提出了一个数学理论框架和数值方法，可以通过时间边缘样本推断出随机过程的轨迹，特别是它可以应用于单细胞RNA测序数据的分析和轨迹推断。

    

    我们设计了一个理论框架和数值方法，可以从随机过程的时间边缘样本中推断其轨迹。这个问题出现在单细胞RNA测序数据的分析中，它提供了细胞状态的高维度测量，但不能跟踪细胞的时间轨迹。我们证明，对于一类随机过程，可以从每个时间点的时间边缘的有限样本中恢复出真实轨迹，并提供一个在实践中高效地执行此操作的算法。我们开发的方法，全局Waddington-OT(gWOT)，可以通过涉及熵正则化最优传输的所有时间点的全局平滑凸优化问题来解决。我们展示了该问题可以在实践中高效地解决，并展示了在几个合成和实际数据集上的良好重建结果。

    We devise a theoretical framework and a numerical method to infer trajectories of a stochastic process from samples of its temporal marginals. This problem arises in the analysis of single cell RNA-sequencing data, which provide high dimensional measurements of cell states but cannot track the trajectories of the cells over time. We prove that for a class of stochastic processes it is possible to recover the ground truth trajectories from limited samples of the temporal marginals at each time-point, and provide an efficient algorithm to do so in practice. The method we develop, Global Waddington-OT (gWOT), boils down to a smooth convex optimization problem posed globally over all time-points involving entropy-regularized optimal transport. We demonstrate that this problem can be solved efficiently in practice and yields good reconstructions, as we show on several synthetic and real datasets.
    

