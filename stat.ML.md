# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Gaussian Database Alignment and Gaussian Planted Matching.](http://arxiv.org/abs/2307.02459) | 高斯数据库对齐和高斯植入匹配问题具有密切联系，当数据库特征的维度较高且没有过强的特征时，数据库对齐的性能阈值收敛到植入匹配的性能阈值。 |
| [^2] | [A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty.](http://arxiv.org/abs/2307.02432) | 本文提出了一种基于概率和数据驱动的闭合模型，用于RANS模拟中考虑模型的不确定性。该模型包括参数化部分和随机变量部分，并通过贝叶斯公式和稀疏先验来识别模型不足的区域，以进行修正。训练使用间接稀疏数据，推断和学习使用随机变分推断方案。 |
| [^3] | [Multi-Task Learning with Summary Statistics.](http://arxiv.org/abs/2307.02388) | 提出了一种利用汇总统计数据的灵活多任务学习框架，可解决在真实世界设置中数据共享限制的问题。通过自适应参数选择方法和系统非渐近分析，提高了模型性能。通过大量模拟实验证明了方法的有效性。 |
| [^4] | [Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise consistency and global lower bounds.](http://arxiv.org/abs/2307.02378) | 该论文研究了从数据云中构建的随机几何图与流形之间的曲率关系，并通过概率分析证明了点态一致性以及全局结构特性传承。研究结果对图上热核的收敛性和从数据云中学习流形具有重要的应用价值。 |
| [^5] | [Algorithms, Incentives, and Democracy.](http://arxiv.org/abs/2307.02319) | 本文研究了分类算法在决策中的应用和影响，重点考察了算法设计对人群行为分布的影响，以及分类奖惩的民主化对社会的影响。 |
| [^6] | [Sumformer: Universal Approximation for Efficient Transformers.](http://arxiv.org/abs/2307.02301) | Sumformer是一种新颖且简单的架构，可以高效地逼近Transformer。通过Sumformer，我们首次给出了Linformer和Performer的通用逼近结果，并推导出一个新的证明，证明只需要一个注意力层就足以进行Transformer的通用逼近。 |
| [^7] | [Meta-Learning Adversarial Bandit Algorithms.](http://arxiv.org/abs/2307.02295) | 本论文研究了具有波段反馈的在线元学习，并设计了用于多臂赌博机和赌博线性优化的元算法。对于多臂赌博机，算法使用了Tsallis-熵的泛化Exp3，并且任务平均遗憾会随着最优解的熵的减小而改善。对于赌博线性优化，算法使用了自协调障碍正则化器初始化和调整在线镜像下降，并且任务平均遗憾与动作空间相关的度量直接变化。 |
| [^8] | [Absorbing Phase Transitions in Artificial Deep Neural Networks.](http://arxiv.org/abs/2307.02284) | 本文研究了在适当初始化的有限神经网络中的吸收相变及其普适性，证明了即使在有限网络中仍然存在着从有序状态到混沌状态的过渡，并且不同的网络架构会反映在过渡的普适类上。 |
| [^9] | [Convolutions Through the Lens of Tensor Networks.](http://arxiv.org/abs/2307.02275) | 该论文提供了一种通过张量网络理解和演化卷积的新视角，可以通过绘制和操作张量网络来进行函数转换、子张量访问和融合。研究人员还演示了卷积图表的导出以及各种自动微分操作和二阶信息逼近图表的生成，同时还提供了特定于卷积的图表转换，以优化计算性能。 |
| [^10] | [Evaluating AI systems under uncertain ground truth: a case study in dermatology.](http://arxiv.org/abs/2307.02191) | 这项研究总结了在健康领域中评估AI系统时的一个重要问题：基准事实的不确定性。现有的方法通常忽视了这一点，而该研究提出了一种使用统计模型聚合注释的框架，以更准确地评估AI系统的性能。 |
| [^11] | [DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks.](http://arxiv.org/abs/2307.02159) | 本论文提出了一种名为DiffFlow的统一SDE框架，用于处理基于分数的扩散模型和生成对抗网络。通过调整权重，可以实现快速采样和高质量样本之间的平滑过渡。 |
| [^12] | [Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso.](http://arxiv.org/abs/2307.02130) | 该论文提出了一种隐式微分的方法用于超参数调优的图形Lasso，通过求解一阶方法下的双层优化问题来实现。最终得到了图形Lasso解对其正则化超参数的雅可比矩阵。 |
| [^13] | [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model.](http://arxiv.org/abs/2307.02129) | 本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。 |
| [^14] | [Robust Graph Structure Learning with the Alignment of Features and Adjacency Matrix.](http://arxiv.org/abs/2307.02126) | 本文提出了一种新颖的正则化的鲁棒图结构学习方法，通过对齐特征信息和图信息，结合稀疏降维，提高了图神经网络的鲁棒性，特别是在受噪声影响较大的情况下。 |
| [^15] | [Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization.](http://arxiv.org/abs/2307.02108) | 这篇论文提出了一种适用于情境赌博设置的新型计算效率高的赌博算法，具有简单和累积遗憾最小化的优势，并可自适应模型错误规范和连续臂设置。该算法利用"一致臂集"（CAS）来提供在每个情境下囊括情境特定的最佳臂的一组臂，跨越情境分布。这篇论文对简单和累积遗憾保证的研究提供了正面结果，同时也揭示了无法实现实例依赖性的简单遗憾保证的消极结果。 |
| [^16] | [A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables.](http://arxiv.org/abs/2307.02071) | 本文通过对多个包含高基数分类变量的表格数据集进行实证比较，发现带随机效应的机器学习模型的预测准确性高于不带随机效应的经典模型，同时带随机效应的树提升方法优于带随机效应的深度神经网络。 |
| [^17] | [Universal Rates for Multiclass Learning.](http://arxiv.org/abs/2307.02066) | 该论文研究了多类学习的通用速率，推广了二元分类的结果，并解决了在多类设置中的一个开放问题。论文通过引入DSL树的概念，给出了类别的指数和线性速率的判别条件，并证明了当标签数无限时，速率将变慢。 |
| [^18] | [Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach.](http://arxiv.org/abs/2307.02037) | 本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。 |
| [^19] | [Ranking with Abstention.](http://arxiv.org/abs/2307.02035) | 提出了一种使用弃权进行排名的新框架，并对该框架进行了广泛的理论分析。我们的方法在文献中给出了最先进的一致性保证，可以有效地估计目标损失，并在使用等连续假设集时具有重要的应用价值。我们的实验证明了弃权方法的有效性。 |
| [^20] | [Using Random Effects Machine Learning Algorithms to Identify Vulnerability to Depression.](http://arxiv.org/abs/2307.02023) | 本研究使用随机效应机器学习算法成功识别出在抑郁风险最大的亚组中具有最大效用的变量，并提出了相关的预测模型，可以改善抑郁症的临床预测和治疗效果。 |
| [^21] | [Algorithme EM r\'egularis\'e.](http://arxiv.org/abs/2307.01955) | 本文提出了一种正则化的EM算法，用于处理小样本量下计算高斯混合模型最大似然估计的问题，并通过缩小估计值向目标协方差矩阵收缩的方式来解决协方差矩阵奇异或条件较差的问题。 |
| [^22] | [FEMDA: Une m\'ethode de classification robuste et flexible.](http://arxiv.org/abs/2307.01954) | 本文介绍了一种新的稳健且灵活的分类方法FEMDA，能够处理非高斯分布和/或受污染的数据集，并能够稳健地对数据中的尺度变化进行分类。 |
| [^23] | [A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks.](http://arxiv.org/abs/2307.01951) | 本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。 |
| [^24] | [MDI+: A Flexible Random Forest-Based Feature Importance Framework.](http://arxiv.org/abs/2307.01932) | MDI+是一种灵活的基于随机森林的特征重要性框架，通过替换线性回归模型和度量，利用正则化的广义线性模型和更适合数据结构的度量来推广MDI。此外，MDI+还引入了其他特征来减轻决策树对加法或平滑模型的已知偏差。 |
| [^25] | [Learning ECG signal features without backpropagation.](http://arxiv.org/abs/2307.01930) | 该论文提出了一种用于生成时间序列数据表示的新方法，依靠理论物理的思想以数据驱动的方式构建紧凑的表示。该方法能够捕捉数据的基本结构和任务特定信息，同时保持直观、可解释和可验证性，并可以在广义设置中应用。 |
| [^26] | [Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics.](http://arxiv.org/abs/2307.01770) | 本文提出了一种快速计算最优输运的方法，通过切片Wasserstein广义测地线进行近似，得到了一个基于一维最优投影的代理距离min-SWGG，并提供了相关的传输计划。这种方法具有较低的计算复杂度，适用于优化算法。 |
| [^27] | [Online Learning and Solving Infinite Games with an ERM Oracle.](http://arxiv.org/abs/2307.01689) | 这项工作提出了一种仅依赖ERM预言机调用的在线学习算法，该算法在可实现情况下具有有限的遗憾，并在不可知情况下具有亚线性增长的遗憾。同时，还提供了类似的结果用于非参数博弈环境中的学习算法，即仅依赖最佳响应预言机的学习算法，并收敛到近似极小-极大均衡点。 |
| [^28] | [Training Energy-Based Models with Diffusion Contrastive Divergences.](http://arxiv.org/abs/2307.01668) | 本文提出了一种使用扩散对比发散（DCD）训练能量模型（EBM）的方法，相较于传统的对比发散（CD），DCD在计算效率上更高，并且不受非可忽略梯度项的限制。 |
| [^29] | [Approximate information for efficient exploration-exploitation strategies.](http://arxiv.org/abs/2307.01563) | 本文提出了一种称为AIM的算法，用于解决决策中的探索-利用困境，特别针对多臂赌博机问题。AIM算法利用近似熵梯度来选择每个时间点要拉动的手臂，与Infomax和Thompson抽样相比，在性能上能够匹配，同时具有更好的计算速度、确定性和可计算性。经实证评估表明，AIM算法符合Lai-Robbins渐进界，对于不同的先验具有鲁棒性。 |
| [^30] | [Accelerated stochastic approximation with state-dependent noise.](http://arxiv.org/abs/2307.01497) | 该论文研究了一类具有状态相关噪声的随机平滑凸优化问题。通过引入两种非欧几里得加速随机逼近算法，实现了在精度、问题参数和小批量大小方面的最优性。 |
| [^31] | [Free energy of Bayesian Convolutional Neural Network with Skip Connection.](http://arxiv.org/abs/2307.01417) | 本文研究了具有跳连接的贝叶斯卷积神经网络的自由能，揭示了其不依赖于过度参数化，并且具有类似的泛化误差性质。 |
| [^32] | [Adaptive Principal Component Regression with Applications to Panel Data.](http://arxiv.org/abs/2307.01357) | 本文提出了自适应主成分回归方法，并在面板数据中的应用中获得了均匀有限样本保证。该方法可以用于面板数据中的实验设计，特别是当干预方案是自适应分配的情况。 |
| [^33] | [INGB: Informed Nonlinear Granular Ball Oversampling Framework for Noisy Imbalanced Classification.](http://arxiv.org/abs/2307.01224) | 本文提出了一种以颗粒球为方向的知情非线性过采样框架INGB，通过模拟数据集的空间分布特征并优化颗粒球空间，利用高维稀疏性和各向同性高斯分布进行非线性过采样，以改善不均衡分类问题。 |
| [^34] | [CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery.](http://arxiv.org/abs/2307.00859) | CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。 |
| [^35] | [A Quantitative Functional Central Limit Theorem for Shallow Neural Networks.](http://arxiv.org/abs/2306.16932) | 本文证明了具有通用激活函数的单隐藏层神经网络的定量函数中心极限定理，收敛速度取决于激活函数的平滑性。 |
| [^36] | [Convergence and concentration properties of constant step-size SGD through Markov chains.](http://arxiv.org/abs/2306.11497) | 本文通过马尔科夫链研究了常步长随机梯度下降的性质，证明了迭代收敛于一个不变分布，并获得了高置信度边界。 |
| [^37] | [Bayes optimal learning in high-dimensional linear regression with network side information.](http://arxiv.org/abs/2306.05679) | 本文首次研究了具有网络辅助信息的高维线性回归中的贝叶斯最优学习问题，引入了Reg-Graph模型并提出了基于AMP的迭代算法，在实验中优于现有的几种网络辅助回归方法。 |
| [^38] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^39] | [Active Cost-aware Labeling of Streaming Data.](http://arxiv.org/abs/2304.06808) | 本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。 |
| [^40] | [Causal Dependence Plots.](http://arxiv.org/abs/2303.04209) | 本论文提出了因果依赖图（CDPs）来解释人工智能或机器学习模型的因果依赖关系。CDPs与传统方法不同，可以模块化地结合因果学习或敏感度分析方法。这些图表可以成为可解释机器学习工具包中的强大工具，并对相关应用做出贡献。 |
| [^41] | [Exploring Local Norms in Exp-concave Statistical Learning.](http://arxiv.org/abs/2302.10726) | 这篇论文研究了在 exp-concave 统计学习中使用经验风险最小化的问题，提出了适用于广泛类别的有界 exp-concave 损失的过量风险界，维度和样本大小对结果有影响，并且基于统一几何假设和本地规范的概念。 |
| [^42] | [The Expressive Power of Tuning Only the Normalization Layers.](http://arxiv.org/abs/2302.07937) | 本研究发现，仅调整神经网络的归一化层参数就可以达到高准确性，甚至可以重建比原网络小O(根号宽度)倍的目标网络。 |
| [^43] | [Dimension Reduction and MARS.](http://arxiv.org/abs/2302.05790) | 本文改进了多元自适应回归样条（MARS）的性能，通过使用协变量的线性组合来实现足够的维度约减，提高了估计效率。 |
| [^44] | [SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces.](http://arxiv.org/abs/2301.11832) | SOBER算法是一种在离散和混合空间上进行高并行贝叶斯优化的方法，能够进行可扩展和多样化的批量全局优化和积分，且优于11个竞争基线方法。 |
| [^45] | [Fine-tuning Neural-Operator architectures for training and generalization.](http://arxiv.org/abs/2301.11509) | 本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。 |
| [^46] | [Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions.](http://arxiv.org/abs/2301.06535) | 案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。 |
| [^47] | [Statistical Comparisons of Classifiers by Generalized Stochastic Dominance.](http://arxiv.org/abs/2209.01857) | 这篇论文通过采用决策理论的最新发展，提出了一种基于广义随机优势的分类器比较框架，该框架通过解决易处理的线性规划问题进行操作，并通过适应的两样本观察随机化测试进行统计测试。 |
| [^48] | [Dynamic Ranking and Translation Synchronization.](http://arxiv.org/abs/2207.01455) | 本论文研究了动态排名和翻译同步问题，主要关注成对比较数据随时间变化的情况，并给出了相应的理论结果。 |
| [^49] | [Tight Bounds on the Hardness of Learning Simple Nonparametric Mixtures.](http://arxiv.org/abs/2203.15150) | 我们研究了学习非参数混合模型中组件分布的难度，并建立了学习每个组件分布的样本复杂性的严格界限。我们的主要结果是需要$(\frac{1}{\varepsilon})^{\Omega(\log\log \frac{1}{\varepsilon})}$个样本来估计每个组件分布。 |
| [^50] | [Meta-Learning for Simple Regret Minimization.](http://arxiv.org/abs/2202.12888) | 本论文提出了用于在赌博机中进行简单遗憾最小化的元学习框架，并提出了首个贝叶斯和频率派元学习算法。贝叶斯算法具有先验分布并且具有较小的元简单遗憾，而频率派算法更通用且可以在更多的设置中进行分析。通过将算法应用于不同的赌博机问题，我们验证了理论的有效性。 |
| [^51] | [A Non-Classical Parameterization for Density Estimation Using Sample Moments.](http://arxiv.org/abs/2201.04786) | 本文提出了一种使用样本矩方法进行非传统参数化密度估计的方法，通过平方Hellinger距离进行参数化，并通过凸优化得到唯一解。通过幂矩估计提出了估计器的统计性质和渐近误差上界，模拟结果验证了该方法的性能。 |
| [^52] | [On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods.](http://arxiv.org/abs/2109.10933) | 本研究证明了在特定选择的Θ和ν下，范数测试和内积/正交性测试在随机梯度下降方法的收敛速度方面是等价的，同时指出在最理想情况下，内积/正交性测试可以像范数测试一样廉价。 |
| [^53] | [Optimization on manifolds: A symplectic approach.](http://arxiv.org/abs/2107.11231) | 这项工作提出了一种在平滑流形上解决优化问题的通用框架，利用流形上的辛算法进行加速，可以处理具有非线性约束的问题，并具有良好的收敛性能。 |
| [^54] | [Transfer Learning in Deep Reinforcement Learning: A Survey.](http://arxiv.org/abs/2009.07888) | 这篇综述调查了深度强化学习领域中的迁移学习方法的最新进展，并提供了一个对这些方法进行分类的框架。分析了它们的目标、方法学、兼容的强化学习背景以及实际应用，并探讨了迁移学习与其他相关主题之间的联系。 |
| [^55] | [Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN.](http://arxiv.org/abs/1705.03387) | 本论文提出了一种使用生成对抗网络来防御对抗扰动的新方法，通过交替训练分类器和生成器网络，生成器网络生成对抗扰动以欺骗分类器网络，同时分类器网络被训练以正确分类原始和对抗图像，这一过程使分类器网络对对抗扰动更加鲁棒，同时还能有效地降低网络的过拟合问题。 |

# 详细

[^1]: 高斯数据库对齐和高斯植入匹配

    Gaussian Database Alignment and Gaussian Planted Matching. (arXiv:2307.02459v1 [cs.IT])

    [http://arxiv.org/abs/2307.02459](http://arxiv.org/abs/2307.02459)

    高斯数据库对齐和高斯植入匹配问题具有密切联系，当数据库特征的维度较高且没有过强的特征时，数据库对齐的性能阈值收敛到植入匹配的性能阈值。

    

    数据库对齐是图对齐问题的一种变体：给定一对包含独立但相关特征的匿名数据库，问题是仅根据相关性识别特征之间的对应关系，并对齐匿名用户集。这与植入匹配密切相关，其中给定一个带有随机权重的二分图，目标是识别生成给定权重的潜在匹配。我们研究了一个具有多变量高斯特征的数据库对齐问题实例，并推导出适用于数据库对齐和植入匹配的结果，展示了它们之间的联系。当数据库特征的维度是\(\omega(\log n)\)（其中\(n\)是对齐的大小）时，数据库对齐的性能阈值会收敛到植入匹配的性能阈值，并且没有个别特征过强。对于植入匹配和数据库对齐来说，最大似然算法都适用。

    Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone. This closely relates to planted matching, where given a bigraph with random weights, the goal is to identify the underlying matching that generated the given weights. We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them. The performance thresholds for database alignment converge to that for planted matching when the dimensionality of the database features is \(\omega(\log n)\), where \(n\) is the size of the alignment, and no individual feature is too strong. The maximum likelihood algorithms for both planted matching and database alig
    
[^2]: 一种基于概率和数据驱动的RANS模拟的闭合模型，考虑到模型的不确定性

    A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty. (arXiv:2307.02432v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.02432](http://arxiv.org/abs/2307.02432)

    本文提出了一种基于概率和数据驱动的闭合模型，用于RANS模拟中考虑模型的不确定性。该模型包括参数化部分和随机变量部分，并通过贝叶斯公式和稀疏先验来识别模型不足的区域，以进行修正。训练使用间接稀疏数据，推断和学习使用随机变分推断方案。

    

    我们提出了一种基于概率和数据驱动的闭合模型，用于Reynolds平均Navier-Stokes (RANS)模拟中考虑模型的不确定性。该闭合模型包括两部分。第一部分是参数化的，利用了基于神经网络的张量基函数，这些函数依赖于应变率和旋转张量的不变量。第二部分则是随机变量，用于考虑模型误差。我们提出了一种完全贝叶斯的公式，并结合了一种稀疏先验，以识别问题领域中参数化闭合模型不足的地方，进而需要对雷诺应力张量进行随机修正。训练使用间接稀疏数据，如平均速度和压力，而不需要直接的雷诺应力数据，与大多数其他方法不同。为了推断和学习，我们采用了一种基于蒙特卡洛估计的随机变分推断方案。

    We propose a data-driven, closure model for Reynolds-averaged Navier-Stokes (RANS) simulations that incorporates aleatoric, model uncertainty. The proposed closure consists of two parts. A parametric one, which utilizes previously proposed, neural-network-based tensor basis functions dependent on the rate of strain and rotation tensor invariants. This is complemented by latent, random variables which account for aleatoric model errors. A fully Bayesian formulation is proposed, combined with a sparsity-inducing prior in order to identify regions in the problem domain where the parametric closure is insufficient and where stochastic corrections to the Reynolds stress tensor are needed. Training is performed using sparse, indirect data, such as mean velocities and pressures, in contrast to the majority of alternatives that require direct Reynolds stress data. For inference and learning, a Stochastic Variational Inference scheme is employed, which is based on Monte Carlo estimates of the p
    
[^3]: 使用汇总统计数据的多任务学习

    Multi-Task Learning with Summary Statistics. (arXiv:2307.02388v1 [stat.ME])

    [http://arxiv.org/abs/2307.02388](http://arxiv.org/abs/2307.02388)

    提出了一种利用汇总统计数据的灵活多任务学习框架，可解决在真实世界设置中数据共享限制的问题。通过自适应参数选择方法和系统非渐近分析，提高了模型性能。通过大量模拟实验证明了方法的有效性。

    

    多任务学习已经成为一个强大的机器学习范式，可以整合来自多个来源的数据，利用任务之间的相似性提高整体模型性能。然而，在真实世界的设置中，多任务学习的应用受到数据共享限制的影响，特别是在医疗领域。为了解决这个挑战，我们提出了一个灵活的多任务学习框架，利用来自各种来源的汇总统计数据。此外，我们提出了一种自适应参数选择方法，基于Lepski方法的一种变体，在仅有汇总统计数据时允许数据驱动的调参选择。我们的系统非渐近分析描述了所提方法在样本复杂度和重叠度的各种情况下的性能。我们通过大量模拟实验证明了我们的理论发现和方法的性能。这项工作为跨分析纵向数据提供了一种更灵活的训练相关模型的工具。

    Multi-task learning has emerged as a powerful machine learning paradigm for integrating data from multiple sources, leveraging similarities between tasks to improve overall model performance. However, the application of multi-task learning to real-world settings is hindered by data-sharing constraints, especially in healthcare settings. To address this challenge, we propose a flexible multi-task learning framework utilizing summary statistics from various sources. Additionally, we present an adaptive parameter selection approach based on a variant of Lepski's method, allowing for data-driven tuning parameter selection when only summary statistics are available. Our systematic non-asymptotic analysis characterizes the performance of the proposed methods under various regimes of the sample complexity and overlap. We demonstrate our theoretical findings and the performance of the method through extensive simulations. This work offers a more flexible tool for training related models across
    
[^4]: 在数据云中Ollivier的Ricci曲率的连续极限：点态一致性和全局下界

    Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise consistency and global lower bounds. (arXiv:2307.02378v1 [math.DG])

    [http://arxiv.org/abs/2307.02378](http://arxiv.org/abs/2307.02378)

    该论文研究了从数据云中构建的随机几何图与流形之间的曲率关系，并通过概率分析证明了点态一致性以及全局结构特性传承。研究结果对图上热核的收敛性和从数据云中学习流形具有重要的应用价值。

    

    让$\mathcal{M} \subseteq \mathbb{R}^d$表示一个低维流形，$\mathcal{X}= \{ x_1, \dots, x_n \}$表示从$\mathcal{M}$均匀采样得到的一组点。我们研究了从$\mathcal{X}$构建的随机几何图与流形$\mathcal{M}$的曲率之间的关系，通过Ollivier的离散Ricci曲率的连续极限。我们证明了点态、非渐近一致性结果，并且还表明，如果$\mathcal{M}$的Ricci曲率从下面严格地被一个正常数界住，那么随机几何图将以高概率继承此全局结构特性。我们讨论全局离散曲率界限在图上热核的收敛性质以及对数据云上流形学习的影响。特别地，我们展示了一致性结果允许通过外禀曲率表征流形的内在曲率。

    Let $\mathcal{M} \subseteq \mathbb{R}^d$ denote a low-dimensional manifold and let $\mathcal{X}= \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.
    
[^5]: 算法、激励和民主

    Algorithms, Incentives, and Democracy. (arXiv:2307.02319v1 [econ.TH])

    [http://arxiv.org/abs/2307.02319](http://arxiv.org/abs/2307.02319)

    本文研究了分类算法在决策中的应用和影响，重点考察了算法设计对人群行为分布的影响，以及分类奖惩的民主化对社会的影响。

    

    分类算法在房屋、信贷和执法等领域越来越广泛地被用于决策，影响人们的生活。这些算法可以有意地改变个体行为（通过欺诈预测算法预防欺诈行为），也可以无意中改变行为（通过内容排序算法传播虚假信息），它们越来越面临公众审查和监管。本文研究了算法设计者通过最优分类如何影响人群行为的分布，有时会产生意想不到的结果。然后我们研究了对算法分类的奖惩或利害关系进行民主化的影响，以探讨社会如何潜在地防止（或促进）掠夺性分类。我们的研究结果涉及算法公平性的问题，在特定分类的情境中具有重要意义。

    Classification algorithms are increasingly used in areas such as housing, credit, and law enforcement in order to make decisions affecting peoples' lives. These algorithms can change individual behavior deliberately (a fraud prediction algorithm deterring fraud) or inadvertently (content sorting algorithms spreading misinformation), and they are increasingly facing public scrutiny and regulation. Some of these regulations, like the elimination of cash bail in some states, have focused on \textit{lowering the stakes of certain classifications}. In this paper we characterize how optimal classification by an algorithm designer can affect the distribution of behavior in a population -- sometimes in surprising ways. We then look at the effect of democratizing the rewards and punishments, or stakes, to algorithmic classification to consider how a society can potentially stem (or facilitate!) predatory classification. Our results speak to questions of algorithmic fairness in settings where be
    
[^6]: Sumformer:高效Transformer的通用逼近

    Sumformer: Universal Approximation for Efficient Transformers. (arXiv:2307.02301v1 [cs.LG])

    [http://arxiv.org/abs/2307.02301](http://arxiv.org/abs/2307.02301)

    Sumformer是一种新颖且简单的架构，可以高效地逼近Transformer。通过Sumformer，我们首次给出了Linformer和Performer的通用逼近结果，并推导出一个新的证明，证明只需要一个注意力层就足以进行Transformer的通用逼近。

    

    随着Transformer的引入，自然语言处理（NLP）取得了显著进展。ChatGPT是其中最著名的例子，即使在研究社区之外，也改变了人们对AI可能性的看法。然而，除了令人印象深刻的性能之外，Transformer相对于序列长度的二次时间和空间复杂度限制了处理长序列的能力。尽管高效Transformer架构（如Linformer和Performer）以线性复杂度出现作为有希望的解决方案，但它们的理论理解仍然有限。在本文中，我们引入了Sumformer，一种新颖且简单的架构，能够通用逼近等变序列到序列的函数。我们使用Sumformer给出了Linformer和Performer的第一个通用逼近结果。此外，我们还推导了一个新的Transformer证明，显示只需要一个注意力层就足以进行通用逼近。

    Natural language processing (NLP) made an impressive jump with the introduction of Transformers. ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation.
    
[^7]: 元学习对抗波段算法

    Meta-Learning Adversarial Bandit Algorithms. (arXiv:2307.02295v1 [cs.LG])

    [http://arxiv.org/abs/2307.02295](http://arxiv.org/abs/2307.02295)

    本论文研究了具有波段反馈的在线元学习，并设计了用于多臂赌博机和赌博线性优化的元算法。对于多臂赌博机，算法使用了Tsallis-熵的泛化Exp3，并且任务平均遗憾会随着最优解的熵的减小而改善。对于赌博线性优化，算法使用了自协调障碍正则化器初始化和调整在线镜像下降，并且任务平均遗憾与动作空间相关的度量直接变化。

    

    我们研究具有波段反馈的在线元学习，目标是在多个任务之间改善性能，如果它们根据某个自然的相似性度量是相似的。作为针对敌对的在线部分信息设置的首个目标，我们设计了元算法，将外层学习器结合在一起，同时为两种重要情况调整内部学习器的初始化和其他超参数：多臂赌博机（MAB）和赌博线性优化（BLO）。对于MAB，元学习器使用Tsallis-熵的泛化Exp3的初始化和设置超参数，如果后见之高峰的熵小，则任务平均遗憾改善。对于BLO，我们学会了使用自协调障碍正则化器初始化和调整在线镜像下降（OMD），表明任务平均遗憾与其引起的动作空间相关的度量直接变化。我们的保证基于证明无正规化跟随者与两个…

    We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two 
    
[^8]: 人工深度神经网络中的吸收相变

    Absorbing Phase Transitions in Artificial Deep Neural Networks. (arXiv:2307.02284v1 [stat.ML])

    [http://arxiv.org/abs/2307.02284](http://arxiv.org/abs/2307.02284)

    本文研究了在适当初始化的有限神经网络中的吸收相变及其普适性，证明了即使在有限网络中仍然存在着从有序状态到混沌状态的过渡，并且不同的网络架构会反映在过渡的普适类上。

    

    由于著名的平均场理论，对于各种体系的无限宽度神经网络的行为的理论理解已经迅速发展。然而，对于更实际和现实重要性更强的有限网络，缺乏清晰直观的框架来延伸我们的理解。在本文中，我们展示了适当初始化的神经网络的行为可以用吸收相变中的普遍临界现象来理解。具体而言，我们研究了全连接前馈神经网络和卷积神经网络中从有序状态到混沌状态的相变，并强调了体系架构的差异与相变的普适类之间的关系。值得注意的是，我们还成功地应用了有限尺度扩展的方法，这表明了直观的现象学。

    Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenologic
    
[^9]: 透过张量网络的视角解析卷积

    Convolutions Through the Lens of Tensor Networks. (arXiv:2307.02275v1 [cs.LG])

    [http://arxiv.org/abs/2307.02275](http://arxiv.org/abs/2307.02275)

    该论文提供了一种通过张量网络理解和演化卷积的新视角，可以通过绘制和操作张量网络来进行函数转换、子张量访问和融合。研究人员还演示了卷积图表的导出以及各种自动微分操作和二阶信息逼近图表的生成，同时还提供了特定于卷积的图表转换，以优化计算性能。

    

    尽管卷积的直观概念简单，但其分析比稠密层更加复杂，这使得理论和算法的推广变得困难。我们通过张量网络（TN）提供了对卷积的新视角，通过绘制图表、操作图表进行函数转换、子张量访问和融合来推理底层张量乘法。我们通过推导各种自动微分操作的图表以及具有完整超参数支持、批处理、通道组和任意卷积维度泛化的流行的二阶信息逼近的图表来展示这种表达能力。此外，我们基于连接模式提供了特定于卷积的转换，允许在评估之前重新连接和简化图表。最后，我们通过依赖于高效TN缩并的已建立机制来探究计算性能。我们的TN实现加速了最近提出的

    Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the generalization of theoretical and algorithmic ideas. We provide a new perspective onto convolutions through tensor networks (TNs) which allow reasoning about the underlying tensor multiplications by drawing diagrams, and manipulating them to perform function transformations, sub-tensor access, and fusion. We demonstrate this expressive power by deriving the diagrams of various autodiff operations and popular approximations of second-order information with full hyper-parameter support, batching, channel groups, and generalization to arbitrary convolution dimensions. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to re-wire and simplify diagrams before evaluation. Finally, we probe computational performance, relying on established machinery for efficient TN contraction. Our TN implementation speeds up a recently-proposed
    
[^10]: 在不确定的基准事实下评估AI系统：皮肤病例研究

    Evaluating AI systems under uncertain ground truth: a case study in dermatology. (arXiv:2307.02191v1 [cs.LG])

    [http://arxiv.org/abs/2307.02191](http://arxiv.org/abs/2307.02191)

    这项研究总结了在健康领域中评估AI系统时的一个重要问题：基准事实的不确定性。现有的方法通常忽视了这一点，而该研究提出了一种使用统计模型聚合注释的框架，以更准确地评估AI系统的性能。

    

    为了安全起见，在部署之前，卫生领域的AI系统需要经过全面的评估，将其预测结果与假定为确定的基准事实进行验证。然而，实际情况并非如此，基准事实可能是不确定的。不幸的是，在标准的AI模型评估中，这一点被大部分忽视了，但是它可能会产生严重后果，如高估未来的性能。为了避免这种情况，我们测量了基准事实的不确定性，我们假设它可以分解为两个主要部分：注释不确定性是由于缺乏可靠注释，以及由于有限的观测信息而导致的固有不确定性。在确定地聚合注释时，通常会忽视这种基准事实的不确定性，例如通过多数投票或平均值来聚合。相反，我们提出了一个框架，在该框架中使用统计模型进行注释的聚合。具体而言，我们将注释的聚合框架解释为所谓可能性的后验推断。

    For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities
    
[^11]: DiffFlow:一种用于基于分数的扩散模型和生成对抗网络的统一SDE框架

    DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks. (arXiv:2307.02159v1 [stat.ML])

    [http://arxiv.org/abs/2307.02159](http://arxiv.org/abs/2307.02159)

    本论文提出了一种名为DiffFlow的统一SDE框架，用于处理基于分数的扩散模型和生成对抗网络。通过调整权重，可以实现快速采样和高质量样本之间的平滑过渡。

    

    生成模型可以分为两种类型：显式生成模型（如基于分数的扩散模型和归一化流）和隐式生成模型（如生成对抗网络）。尽管这两种模型都取得了很大的成功，但它们各自存在限制，无法同时实现快速采样和高质量样本。在本文中，我们提出了一个统一的理论框架，用于处理基于分数的扩散模型和生成对抗网络。我们证明了以下两点：i）SDM和GAN的学习动态可以归结为一个称为鉴别器降噪扩散流（DiffFlow）的新型SDE，其中漂移可以通过真实数据和生成数据的分数的加权组合来确定；ii）通过调整不同分数项之间的权重，我们可以实现平滑过渡。

    Generative models can be categorized into two types: explicit generative models that define explicit density forms and allow exact likelihood inference, such as score-based diffusion models (SDMs) and normalizing flows; implicit generative models that directly learn a transformation from the prior to the data distribution, such as generative adversarial nets (GANs). While these two types of models have shown great success, they suffer from respective limitations that hinder them from achieving fast sampling and high sample quality simultaneously. In this paper, we propose a unified theoretic framework for SDMs and GANs. We shown that: i) the learning dynamics of both SDMs and GANs can be described as a novel SDE named Discriminator Denoising Diffusion Flow (DiffFlow) where the drift can be determined by some weighted combinations of scores of the real data and the generated data; ii) By adjusting the relative weights between different score terms, we can obtain a smooth transition betw
    
[^12]: 针对加权图形Lasso的隐式微分用于超参数调优的研究

    Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso. (arXiv:2307.02130v1 [cs.LG])

    [http://arxiv.org/abs/2307.02130](http://arxiv.org/abs/2307.02130)

    该论文提出了一种隐式微分的方法用于超参数调优的图形Lasso，通过求解一阶方法下的双层优化问题来实现。最终得到了图形Lasso解对其正则化超参数的雅可比矩阵。

    

    我们提供了一个框架和算法，用于通过一阶方法解决一个双层优化问题来调优图形Lasso的超参数。具体而言，我们导出了图形Lasso解对其正则化超参数的雅可比矩阵。

    We provide a framework and algorithm for tuning the hyperparameters of the Graphical Lasso via a bilevel optimization problem solved with a first-order method. In particular, we derive the Jacobian of the Graphical Lasso solution with respect to its regularization hyperparameters.
    
[^13]: 深度神经网络如何学习组合性数据：随机层次模型

    How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])

    [http://arxiv.org/abs/2307.02129](http://arxiv.org/abs/2307.02129)

    本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。

    

    学习一般高维任务是非常困难的，因为它需要与维度成指数增长的训练数据数量。然而，深度卷积神经网络（CNN）在克服这一挑战方面显示出了卓越的成功。一种普遍的假设是可学习任务具有高度结构化，CNN利用这种结构建立了数据的低维表示。然而，我们对它们需要多少训练数据以及这个数字如何取决于数据结构知之甚少。本文回答了针对一个简单的分类任务的这个问题，该任务旨在捕捉真实数据的相关方面：随机层次模型。在这个模型中，$n_c$个类别中的每一个对应于$m$个同义组合的高层次特征，并且这些特征又通过一个重复$L$次的迭代过程由子特征组成。我们发现，需要深度CNN学习这个任务的训练数据数量$P^*$（i）随着$n_c m^L$的增长而渐进地增长，这只有...

    Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
    
[^14]: 带有特征和邻接矩阵对齐的鲁棒图结构学习

    Robust Graph Structure Learning with the Alignment of Features and Adjacency Matrix. (arXiv:2307.02126v1 [cs.LG])

    [http://arxiv.org/abs/2307.02126](http://arxiv.org/abs/2307.02126)

    本文提出了一种新颖的正则化的鲁棒图结构学习方法，通过对齐特征信息和图信息，结合稀疏降维，提高了图神经网络的鲁棒性，特别是在受噪声影响较大的情况下。

    

    为了改进图神经网络（GNN）的鲁棒性，图结构学习（GSL）因图数据中的噪声广泛存在而受到了极大关注。许多方法已经提出用于GSL，以共同学习清洁的图结构和相应的表示。为了扩展之前的工作，本文提出了一种新颖的正则化的GSL方法，特别是通过特征信息和图信息的对齐，这主要受到我们推导的GNNs节点级Rademacher复杂性的下界的激励。此外，我们的提出的方法结合了稀疏降维，以利用与图结构相关的低维节点特征。为了评估我们方法的有效性，我们在真实世界的图上进行了实验。结果表明，我们提出的GSL方法在多个竞争基线中表现出色，特别是在图结构受噪声严重影响的情况下。总体而言，我们的研究

    To improve the robustness of graph neural networks (GNN), graph structure learning (GSL) has attracted great interest due to the pervasiveness of noise in graph data. Many approaches have been proposed for GSL to jointly learn a clean graph structure and corresponding representations. To extend the previous work, this paper proposes a novel regularized GSL approach, particularly with an alignment of feature information and graph information, which is motivated mainly by our derived lower bound of node-level Rademacher complexity for GNNs. Additionally, our proposed approach incorporates sparse dimensional reduction to leverage low-dimensional node features that are relevant to the graph structure. To evaluate the effectiveness of our approach, we conduct experiments on real-world graphs. The results demonstrate that our proposed GSL method outperforms several competitive baselines, especially in scenarios where the graph structures are heavily affected by noise. Overall, our research h
    
[^15]: 比例响应：用于简单和累积遗憾最小化的情境赌博算法

    Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v1 [cs.LG])

    [http://arxiv.org/abs/2307.02108](http://arxiv.org/abs/2307.02108)

    这篇论文提出了一种适用于情境赌博设置的新型计算效率高的赌博算法，具有简单和累积遗憾最小化的优势，并可自适应模型错误规范和连续臂设置。该算法利用"一致臂集"（CAS）来提供在每个情境下囊括情境特定的最佳臂的一组臂，跨越情境分布。这篇论文对简单和累积遗憾保证的研究提供了正面结果，同时也揭示了无法实现实例依赖性的简单遗憾保证的消极结果。

    

    在医疗保健和电子商务等领域，简单遗憾最小化是学习最佳治疗分配策略的关键问题。然而，情境赌博设置中的简单遗憾最小化问题仍未充分研究。我们提出了一种新的计算效率高的赌博算法族，针对随机情境赌博设置，在累积遗憾最小化（具有近乎最优的极小极大保证）和简单遗憾最小化（具有SOTA保证）方面具有灵活性。此外，我们的算法对模型错误规范进行自适应，并扩展到连续臂设置。这些优势来自于构建和依赖于“一致臂集”（CAS），CAS在每个情境下提供一组臂，这些臂以一定的概率囊括了情境特定的最佳臂，跨越了情境分布。我们关于简单和累积遗憾保证的积极结果与一个消极结果形成对比，后者表明一个算法无法实现实例依赖性的简单遗憾保证。

    Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-de
    
[^16]: 高基数分类变量的机器学习方法比较

    A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables. (arXiv:2307.02071v1 [cs.LG])

    [http://arxiv.org/abs/2307.02071](http://arxiv.org/abs/2307.02071)

    本文通过对多个包含高基数分类变量的表格数据集进行实证比较，发现带随机效应的机器学习模型的预测准确性高于不带随机效应的经典模型，同时带随机效应的树提升方法优于带随机效应的深度神经网络。

    

    高基数分类变量是指不同级别数量相对于数据集样本量较大的变量，也就是说，每个级别的数据点较少。机器学习方法在处理高基数变量时可能会遇到困难。本文通过对多个包含高基数分类变量的表格数据集进行实证比较，对两种最成功的机器学习方法（树提升和深度神经网络）以及线性混合效应模型的几个版本进行比较。我们发现，首先，带随机效应的机器学习模型的预测准确性高于不带随机效应的经典模型；其次，带随机效应的树提升优于带随机效应的深度神经网络。

    High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.
    
[^17]: 多类学习的通用速率

    Universal Rates for Multiclass Learning. (arXiv:2307.02066v1 [cs.LG])

    [http://arxiv.org/abs/2307.02066](http://arxiv.org/abs/2307.02066)

    该论文研究了多类学习的通用速率，推广了二元分类的结果，并解决了在多类设置中的一个开放问题。论文通过引入DSL树的概念，给出了类别的指数和线性速率的判别条件，并证明了当标签数无限时，速率将变慢。

    

    我们研究了多类分类的通用速率，确定了所有假设类的最优速率（最多对数因子）。这推广了之前关于二元分类的结果，并解决了由Kalavasis、Velegkas和Karbasi（2022年）研究的具有有限标签数的多类设置中的一个开放问题。相反，我们的结果适用于任何可数的标签空间。即使对于有限的标签空间，我们的证明也能提供更精确的学习曲线上界，因为它们不依赖于标签数。具体地，我们证明任何类别只有当它没有无限的Littlestone树时才具有指数速率，而只有当它没有无限的Daniely-Shalev-Shwartz-Littleston（DSL）树时才具有（近似）线性速率，否则就需要任意慢的速率。DSL树是我们在这项工作中定义的一种新结构，其中树的每个节点由一个伪立方体给出。

    We study universal rates for multiclass classification, establishing the optimal rates (up to log factors) for all hypothesis classes. This generalizes previous results on binary classification (Bousquet, Hanneke, Moran, van Handel, and Yehudayoff, 2021), and resolves an open question studied by Kalavasis, Velegkas, and Karbasi (2022) who handled the multiclass setting with a bounded number of class labels. In contrast, our result applies for any countable label space. Even for finite label space, our proofs provide a more precise bounds on the learning curves, as they do not depend on the number of labels. Specifically, we show that any class admits exponential rates if and only if it has no infinite Littlestone tree, and admits (near-)linear rates if and only if it has no infinite Daniely-Shalev-Shwartz-Littleston (DSL) tree, and otherwise requires arbitrarily slow rates. DSL trees are a new structure we define in this work, in which each node of the tree is given by a pseudo-cube of
    
[^18]: 无等渗性的蒙特卡洛采样：一种逆扩散方法

    Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])

    [http://arxiv.org/abs/2307.02037](http://arxiv.org/abs/2307.02037)

    本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。

    

    现代生成模型的有效性通常取决于扩散路径上得分估计的精度，重点关注扩散模型及其生成高质量数据样本的能力。本研究深入探讨了通过逆扩散进行后验采样的潜力。通过对采样文献进行研究，发现可以通过转移核的分解将得分估计转化为均值估计问题。通过估计辅助分布的均值，逆扩散过程可以产生一种新颖的后验采样算法，该算法与传统的基于梯度的马尔科夫链蒙特卡洛（MCMC）方法不同。我们提供了总变差距离下的收敛分析，并证明了所提算法的等渗性依赖性相对较低，比传统的MCMC技术表现出更高的高维采样性能。

    The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
    
[^19]: 使用弃权进行排名

    Ranking with Abstention. (arXiv:2307.02035v1 [cs.LG])

    [http://arxiv.org/abs/2307.02035](http://arxiv.org/abs/2307.02035)

    提出了一种使用弃权进行排名的新框架，并对该框架进行了广泛的理论分析。我们的方法在文献中给出了最先进的一致性保证，可以有效地估计目标损失，并在使用等连续假设集时具有重要的应用价值。我们的实验证明了弃权方法的有效性。

    

    我们介绍了一种新的弃权排名框架，学习者可以以有限成本$c$放弃对某些数据进行预测。我们对这个框架进行了广泛的理论分析，包括线性函数族和具有一层隐藏层的神经网络的一系列$H$-一致性界限。这些理论保证是文献中最先进的一致性保证，它们是预测器在假设集$H$中的目标损失估计误差的上界，以预测器的替代损失估计误差为表达形式。我们进一步指出，在实践中使用常见等连续假设集时，我们提出的弃权方法是重要的。我们报告了一系列实验结果，展示了使用弃权进行排名的有效性。

    We introduce a novel framework of ranking with abstention, where the learner can abstain from making prediction at some limited cost $c$. We present a extensive theoretical analysis of this framework including a series of $H$-consistency bounds for both the family of linear functions and that of neural networks with one hidden-layer. These theoretical guarantees are the state-of-the-art consistency guarantees in the literature, which are upper bounds on the target loss estimation error of a predictor in a hypothesis set $H$, expressed in terms of the surrogate loss estimation error of that predictor. We further argue that our proposed abstention methods are important when using common equicontinuous hypothesis sets in practice. We report the results of experiments illustrating the effectiveness of ranking with abstention.
    
[^20]: 使用随机效应机器学习算法识别抑郁的易感性

    Using Random Effects Machine Learning Algorithms to Identify Vulnerability to Depression. (arXiv:2307.02023v1 [stat.ML])

    [http://arxiv.org/abs/2307.02023](http://arxiv.org/abs/2307.02023)

    本研究使用随机效应机器学习算法成功识别出在抑郁风险最大的亚组中具有最大效用的变量，并提出了相关的预测模型，可以改善抑郁症的临床预测和治疗效果。

    

    背景：可靠地预测抑郁症的临床进展可以改善治疗效果。目前很少有研究将不同的抑郁症风险因素整合起来，以确定哪些因素的组合对于确定哪些个体最具风险具有最大效用。方法：本研究证明了数据驱动的机器学习（ML）方法，如RE-EM（随机效应/最大期望）树和MERF（混合效应随机森林）可以应用于可靠地识别在抑郁风险最大的亚组中具有最大效用的变量。185名年轻成人参与了关于抑郁风险的测量，包括反刍、担忧、消极认知风格、认知和应对灵活性以及消极生活事件，以及抑郁症状。我们训练了RE-EM树和MERF算法，并将它们与传统的线性混合模型（LMMs）进行了比较，用于预测抑郁症状的前瞻性和同时性。

    Background: Reliable prediction of clinical progression over time can improve the outcomes of depression. Little work has been done integrating various risk factors for depression, to determine the combinations of factors with the greatest utility for identifying which individuals are at the greatest risk. Method: This study demonstrates that data-driven machine learning (ML) methods such as RE-EM (Random Effects/Expectation Maximization) trees and MERF (Mixed Effects Random Forest) can be applied to reliably identify variables that have the greatest utility for classifying subgroups at greatest risk for depression. 185 young adults completed measures of depression risk, including rumination, worry, negative cognitive styles, cognitive and coping flexibilities, and negative life events, along with symptoms of depression. We trained RE-EM trees and MERF algorithms and compared them to traditional linear mixed models (LMMs) predicting depressive symptoms prospectively and concurrently wi
    
[^21]: 正则化的EM算法

    Algorithme EM r\'egularis\'e. (arXiv:2307.01955v1 [stat.ML])

    [http://arxiv.org/abs/2307.01955](http://arxiv.org/abs/2307.01955)

    本文提出了一种正则化的EM算法，用于处理小样本量下计算高斯混合模型最大似然估计的问题，并通过缩小估计值向目标协方差矩阵收缩的方式来解决协方差矩阵奇异或条件较差的问题。

    

    期望最大化(EM)算法是一种广泛用于计算高斯混合模型(GMM)最大似然估计的迭代算法。当样本量小于数据维度时，可能导致奇异或条件较差的协方差矩阵，从而降低性能。本文提出了一种正则化的EM算法，它有效地利用先验知识来处理小样本量。该方法旨在通过缩小估计值向某些结构化目标协方差矩阵收缩的方式来最大化罚函数GMM似然度，以确保协方差矩阵更新的正定性。最后，对真实数据的实验结果突出了所提算法在聚类目的下的良好性能。

    Expectation-Maximization (EM) algorithm is a widely used iterative algorithm for computing maximum likelihood estimate when dealing with Gaussian Mixture Model (GMM). When the sample size is smaller than the data dimension, this could lead to a singular or poorly conditioned covariance matrix and, thus, to performance reduction. This paper presents a regularized version of the EM algorithm that efficiently uses prior knowledge to cope with a small sample size. This method aims to maximize a penalized GMM likelihood where regularized estimation may ensure positive definiteness of covariance matrix updates by shrinking the estimators towards some structured target covariance matrices. Finally, experiments on real data highlight the good performance of the proposed algorithm for clustering purposes
    
[^22]: FEMDA: 一种稳健且灵活的分类方法

    FEMDA: Une m\'ethode de classification robuste et flexible. (arXiv:2307.01954v1 [stat.ML])

    [http://arxiv.org/abs/2307.01954](http://arxiv.org/abs/2307.01954)

    本文介绍了一种新的稳健且灵活的分类方法FEMDA，能够处理非高斯分布和/或受污染的数据集，并能够稳健地对数据中的尺度变化进行分类。

    

    线性判别分析(LDA)和二次判别分析(QDA)是众所周知的经典方法，但在非高斯分布和/或受污染的数据集中可能受到严重影响，主要是因为底层高斯假设不具备稳健性。本文研究了一种新的判别分析技术对数据尺度变化的稳健性，其中每个数据点由自己的任意椭球对称(ES)分布和自己的任意尺度参数绘制。这种模型允许可能非常异质、独立但不同分布的样本。与其他最先进的方法相比，所得到的新的决策规则简单、快速且对数据中的尺度变化具有稳健性。

    Linear and Quadratic Discriminant Analysis (LDA and QDA) are well-known classical methods but can heavily suffer from non-Gaussian distributions and/or contaminated datasets, mainly because of the underlying Gaussian assumption that is not robust. This paper studies the robustness to scale changes in the data of a new discriminant analysis technique where each data point is drawn by its own arbitrary Elliptically Symmetrical (ES) distribution and its own arbitrary scale parameter. Such a model allows for possibly very heterogeneous, independent but non-identically distributed samples. The new decision rule derived is simple, fast, and robust to scale changes in the data compared to other state-of-the-art method
    
[^23]: 图神经网络中特征演化的神经塌陷视角

    A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])

    [http://arxiv.org/abs/2307.01951](http://arxiv.org/abs/2307.01951)

    本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。

    

    图神经网络（GNNs）在图结构数据的分类任务中越来越受欢迎。然而，GNNs中图拓扑和特征演化之间的相互作用尚不清楚。本文以基于节点的分类为主题，以随机块模型图上的社区检测为例，通过“神经塌陷”现象来探索特征演化。当训练基于实例的深度分类器（例如图像分类）超过零训练误差点时，神经塌陷表现为最深层特征的类内变异性减少，并且类均值与特定的对称结构更加对齐。我们先从实证研究开始，显示类内变异性的减少在基于节点的分类环境中也普遍存在，但不及基于实例的案例那么明显。然后，我们从理论上研究了这种区别。具体而言，我们证明了即使在不考虑激活，图拓扑信息也能导致特征崩溃。

    Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimis
    
[^24]: MDI+:一种灵活的基于随机森林的特征重要性框架

    MDI+: A Flexible Random Forest-Based Feature Importance Framework. (arXiv:2307.01932v1 [stat.ME])

    [http://arxiv.org/abs/2307.01932](http://arxiv.org/abs/2307.01932)

    MDI+是一种灵活的基于随机森林的特征重要性框架，通过替换线性回归模型和度量，利用正则化的广义线性模型和更适合数据结构的度量来推广MDI。此外，MDI+还引入了其他特征来减轻决策树对加法或平滑模型的已知偏差。

    

    以不纯度减少的平均值(MDI)是随机森林(RF)中一种流行的特征重要性评估方法。我们展示了在RF中每个树的特征$X_k$的MDI等价于响应变量在决策树集合上的线性回归的未归一化$R^2$值。我们利用这种解释提出了一种灵活的特征重要性框架MDI+，MDI+通过允许分析人员将线性回归模型和$R^2$度量替换为正则化的广义线性模型(GLM)和更适合给定数据结构的度量来推广MDI。此外，MDI+还引入了其他特征来减轻决策树对加法或平滑模型的已知偏差。我们进一步提供了关于如何基于可预测性、可计算性和稳定性框架选择适当的GLM和度量的指导，以进行真实数据科学研究。大量基于数据的模拟结果显示，MDI+在性能上显著优于传统的MDI。

    Mean decrease in impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature $X_k$ in each tree in an RF is equivalent to the unnormalized $R^2$ value in a linear regression of the response on the collection of decision stumps that split on $X_k$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and $R^2$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperfor
    
[^25]: 学习ECG信号特征的非反向传播方法

    Learning ECG signal features without backpropagation. (arXiv:2307.01930v1 [cs.LG])

    [http://arxiv.org/abs/2307.01930](http://arxiv.org/abs/2307.01930)

    该论文提出了一种用于生成时间序列数据表示的新方法，依靠理论物理的思想以数据驱动的方式构建紧凑的表示。该方法能够捕捉数据的基本结构和任务特定信息，同时保持直观、可解释和可验证性，并可以在广义设置中应用。

    

    表示学习已经成为机器学习领域的一个关键研究领域，它旨在发现用于提高分类和预测等下游任务的原始数据的有效特征的有效方法。在本文中，我们提出了一种用于生成时间序列类型数据表示的新方法。这种方法依靠理论物理的思想以数据驱动的方式构建紧凑的表示，并可以捕捉到数据的基本结构和任务特定信息，同时保持直观、可解释和可验证性。这个新方法旨在识别能够有效捕捉属于特定类别的样本之间共享特征的线性规律。通过随后利用这些规律在前向方式下生成一个与分类器无关的表示，它们可以在广义设置中应用。我们展示了我们方法的有效性。

    Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach o
    
[^26]: 快速通过切片Wasserstein广义测地线实现最优输运

    Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics. (arXiv:2307.01770v1 [stat.ML])

    [http://arxiv.org/abs/2307.01770](http://arxiv.org/abs/2307.01770)

    本文提出了一种快速计算最优输运的方法，通过切片Wasserstein广义测地线进行近似，得到了一个基于一维最优投影的代理距离min-SWGG，并提供了相关的传输计划。这种方法具有较低的计算复杂度，适用于优化算法。

    

    Wassserstein距离和相关的最优输运计划在许多涉及概率度量的应用中被证明是有用的。在本文中，我们提出了一个新的平方Wasserstein距离的代理，称为min-SWGG，它基于两个输入分布的一维最优投影引导的运输映射。我们在min-SWGG和Wasserstein广义测地线之间建立了联系，其中枢纽测度在一条直线上得到支持。我们特别提供了一个新的闭合形式的精确Wasserstein距离，在其中一个分布支持在一条直线上的特殊情况下，使我们能够推导出一种适用于梯度下降优化的快速计算方案。我们表明min-SWGG是WD的上界，并且它具有与Sliced-Wasserstein类似的复杂度，同时提供了一个相关的输运计划。我们还研究了一些理论性质，如距离性、弱收敛、计算和拓扑性质等。

    Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and top
    
[^27]: 在线学习和使用ERM预言机解决无穷博弈问题

    Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])

    [http://arxiv.org/abs/2307.01689](http://arxiv.org/abs/2307.01689)

    这项工作提出了一种仅依赖ERM预言机调用的在线学习算法，该算法在可实现情况下具有有限的遗憾，并在不可知情况下具有亚线性增长的遗憾。同时，还提供了类似的结果用于非参数博弈环境中的学习算法，即仅依赖最佳响应预言机的学习算法，并收敛到近似极小-极大均衡点。

    

    在基于在线学习的情况下，ERM足以达到接近最优泛化误差的目标，但在在线学习环境下并非如此，通常的概念类算法依赖计算效率较低的预言机，如标准最优算法(SOA)。在这项工作中，我们提出了一种仅依赖ERM预言机调用的在线二分类算法，并证明在可实现的情况下具有有限的遗憾(regret)，在不可知的情况下具有亚线性增长的遗憾。我们通过底层概念类的Littlestone和阈值维度来限制遗憾。我们获得了类似的结果用于非参数博弈，其中ERM预言机可以被理解为最佳响应预言机，根据其他玩家的游戏历史找到一个玩家的最佳响应。在这种情况下，我们提供了仅依赖最佳响应预言机的学习算法，并收敛到两人零和博弈的近似极小-极大均衡点。

    While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
    
[^28]: 使用扩散对比发散训练能量模型

    Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])

    [http://arxiv.org/abs/2307.01668](http://arxiv.org/abs/2307.01668)

    本文提出了一种使用扩散对比发散（DCD）训练能量模型（EBM）的方法，相较于传统的对比发散（CD），DCD在计算效率上更高，并且不受非可忽略梯度项的限制。

    

    能量模型（EBM）广泛应用于生成建模。传统的对比发散（CD）训练目标需要使用马尔可夫链蒙特卡罗方法（MCMCs）从EBM中采样，这导致了计算负担和CD有效性之间的不可调和的折衷。MCMCs的收敛需要大量计算资源，而短期运行的MCMC会引入难以处理的额外参数梯度项。本文提出了扩散对比发散（DCD）系列的一般解释，将CD视为DCD的一种特殊情况，并通过在CD中使用不同于Langevin动力学的EBM参数自由扩散过程，提出了一种更有效的发散方法。我们展示了DCD比CD更加计算高效，并且不受非可忽略梯度项的限制。我们进行了大量实验，包括合成数据和实际应用场景的验证。

    Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data
    
[^29]: 用于高效探索-利用策略的近似信息方法

    Approximate information for efficient exploration-exploitation strategies. (arXiv:2307.01563v1 [stat.ML])

    [http://arxiv.org/abs/2307.01563](http://arxiv.org/abs/2307.01563)

    本文提出了一种称为AIM的算法，用于解决决策中的探索-利用困境，特别针对多臂赌博机问题。AIM算法利用近似熵梯度来选择每个时间点要拉动的手臂，与Infomax和Thompson抽样相比，在性能上能够匹配，同时具有更好的计算速度、确定性和可计算性。经实证评估表明，AIM算法符合Lai-Robbins渐进界，对于不同的先验具有鲁棒性。

    

    本文针对决策中潜在的探索-利用困境，重点研究多臂赌博机问题。该问题涉及一个代理决定是否利用当前的知识以获取即时收益，还是探索新的途径以获得潜在的长期回报。我们引入了一种新颖的算法，即近似信息最大化（AIM），它利用熵梯度的解析近似来选择每个时间点要拉动的手臂。AIM在性能上与Infomax和Thompson抽样相匹配，同时提供了增强的计算速度、确定性和可计算性。对AIM的实证评估表明其符合Lai-Robbins渐进界，并展示了它对一系列先验的鲁棒性。其表达式可调节，可以在不同场景下进行具体优化。

    This paper addresses the exploration-exploitation dilemma inherent in decision-making, focusing on multi-armed bandit problems. The problems involve an agent deciding whether to exploit current knowledge for immediate gains or explore new avenues for potential long-term rewards. We here introduce a novel algorithm, approximate information maximization (AIM), which employs an analytical approximation of the entropy gradient to choose which arm to pull at each point in time. AIM matches the performance of Infomax and Thompson sampling while also offering enhanced computational speed, determinism, and tractability. Empirical evaluation of AIM indicates its compliance with the Lai-Robbins asymptotic bound and demonstrates its robustness for a range of priors. Its expression is tunable, which allows for specific optimization in various settings.
    
[^30]: 具有状态相关噪声的加速随机逼近

    Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])

    [http://arxiv.org/abs/2307.01497](http://arxiv.org/abs/2307.01497)

    该论文研究了一类具有状态相关噪声的随机平滑凸优化问题。通过引入两种非欧几里得加速随机逼近算法，实现了在精度、问题参数和小批量大小方面的最优性。

    

    我们考虑具有一般噪声假设的随机平滑凸优化问题的一类问题，在这些问题中，随机梯度观测的噪声的方差与算法产生的近似解的"亚最优性" 相关。这类问题在多种应用中自然而然地出现，特别是在统计学中的广义线性回归问题中。然而，据我们所知，现有的解决这类问题的随机逼近算法在精度、问题参数和小批量大小的依赖性方面都未达到最优。我们讨论了两种非欧几里得加速随机逼近算法——随机加速梯度下降（SAGD）和随机梯度外推（SGE）——它们具有一种特殊的对偶关系

    We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
    
[^31]: 具有跳连接的贝叶斯卷积神经网络的自由能研究

    Free energy of Bayesian Convolutional Neural Network with Skip Connection. (arXiv:2307.01417v1 [cs.LG])

    [http://arxiv.org/abs/2307.01417](http://arxiv.org/abs/2307.01417)

    本文研究了具有跳连接的贝叶斯卷积神经网络的自由能，揭示了其不依赖于过度参数化，并且具有类似的泛化误差性质。

    

    自从Residual Network(ResNet)的成功之后，许多卷积神经网络(CNNs)的架构都采用了跳连接。虽然跳连接的CNN的泛化性能已在集成学习框架下得到解释，但参数数量的依赖性尚未揭示。本文中，我们展示了在贝叶斯学习中，有跳连接和无跳连接的卷积神经网络的贝叶斯自由能。具有跳连接的贝叶斯CNN的自由能上界不依赖于过度参数化，而贝叶斯CNN的泛化误差也具有相似的性质。

    Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters have not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.
    
[^32]: 自适应主成分回归在面板数据中的应用

    Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])

    [http://arxiv.org/abs/2307.01357](http://arxiv.org/abs/2307.01357)

    本文提出了自适应主成分回归方法，并在面板数据中的应用中获得了均匀有限样本保证。该方法可以用于面板数据中的实验设计，特别是当干预方案是自适应分配的情况。

    

    主成分回归(PCR)是一种流行的固定设计误差变量回归技术，它是线性回归的推广，观测的协变量受到随机噪声的污染。我们在数据收集时提供了在线（正则化）PCR的第一次均匀有限样本保证。由于分析固定设计中PCR的证明技术无法很容易地扩展到在线设置，我们的结果依赖于将现代鞅浓度的工具适应到误差变量设置中。作为我们界限的应用，我们在面板数据设置中提供了实验设计框架，当干预被自适应地分配时。我们的框架可以被认为是合成控制和合成干预框架的泛化，其中数据是通过自适应干预分配策略收集的。

    Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
    
[^33]: INGB: 用于嘈杂的不均衡分类的知情非线性颗粒球过采样框架

    INGB: Informed Nonlinear Granular Ball Oversampling Framework for Noisy Imbalanced Classification. (arXiv:2307.01224v1 [stat.ML])

    [http://arxiv.org/abs/2307.01224](http://arxiv.org/abs/2307.01224)

    本文提出了一种以颗粒球为方向的知情非线性过采样框架INGB，通过模拟数据集的空间分布特征并优化颗粒球空间，利用高维稀疏性和各向同性高斯分布进行非线性过采样，以改善不均衡分类问题。

    

    在分类问题中，数据集通常是不均衡、嘈杂或复杂的。大多数采样算法只对合成少数类过采样技术（SMOTE）的线性采样机制进行了一些改进。然而，线性过采样存在一些不可避免的缺点。线性过采样容易过拟合，并且合成样本缺乏多样性，很少考虑原始分布特征。本文提出了一种以颗粒球为方向的知情非线性过采样框架INGB。它利用颗粒球来模拟数据集的空间分布特征，并利用知情熵进一步优化颗粒球空间。然后，按照高维稀疏性和各向同性高斯分布进行非线性过采样。此外，INGB具有良好的兼容性，它不仅可以与大多数基于SMOTE的采样算法相结合，以改善分类性能。

    In classification problems, the datasets are usually imbalanced, noisy or complex. Most sampling algorithms only make some improvements to the linear sampling mechanism of the synthetic minority oversampling technique (SMOTE). Nevertheless, linear oversampling has several unavoidable drawbacks. Linear oversampling is susceptible to overfitting, and the synthetic samples lack diversity and rarely account for the original distribution characteristics. An informed nonlinear oversampling framework with the granular ball (INGB) as a new direction of oversampling is proposed in this paper. It uses granular balls to simulate the spatial distribution characteristics of datasets, and informed entropy is utilized to further optimize the granular-ball space. Then, nonlinear oversampling is performed by following high-dimensional sparsity and the isotropic Gaussian distribution. Furthermore, INGB has good compatibility. Not only can it be combined with most SMOTE-based sampling algorithms to impro
    
[^34]: CardiGraphormer: 揭示自监督学习在颠覆药物发现中的力量

    CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])

    [http://arxiv.org/abs/2307.00859](http://arxiv.org/abs/2307.00859)

    CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。

    

    在广阔的药物发现领域中，已知药物约有15,000种，但只有大约4,200种得到了批准，化学空间的组合性质提供了一项艰巨的挑战。尽管人工智能成为了有力的伙伴，传统的人工智能框架仍面临重大障碍。本文介绍了CardiGraphormer，这是一种划时代的方法，通过结合自监督学习（SSL）、图神经网络（GNN）和保持基数注意力，从而颠覆药物发现。CardiGraphormer是Graphormer和保持基数注意力的新颖组合，利用SSL学习有效的分子表示，并利用GNN提取分子指纹，提高了预测性能和可解释性，并减少了计算时间。它在处理分子结构等复杂数据方面表现出色，并能执行与节点、节点对、子图或整个图结构相关的任务。

    In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
    
[^35]: 浅层神经网络的定量函数中心极限定理

    A Quantitative Functional Central Limit Theorem for Shallow Neural Networks. (arXiv:2306.16932v1 [math.PR] CROSS LISTED)

    [http://arxiv.org/abs/2306.16932](http://arxiv.org/abs/2306.16932)

    本文证明了具有通用激活函数的单隐藏层神经网络的定量函数中心极限定理，收敛速度取决于激活函数的平滑性。

    

    我们证明了对于具有通用激活函数的单隐藏层神经网络的定量函数中心极限定理。我们建立的收敛速度严重依赖于激活函数的平滑性，从非可微的情况（如Relu）的对数级别到非常规则激活函数的$\sqrt{n}$级别。我们的主要工具是Stein-Malliavin方法的函数版本；特别是，我们大量利用了Bourguin和Campese（2020）最近建立的定量函数中心极限定理。

    We prove a Quantitative Functional Central Limit Theorem for one-hidden-layer neural networks with generic activation function. The rates of convergence that we establish depend heavily on the smoothness of the activation function, and they range from logarithmic in non-differentiable cases such as the Relu to $\sqrt{n}$ for very regular activations. Our main tools are functional versions of the Stein-Malliavin approach; in particular, we exploit heavily a quantitative functional central limit theorem which has been recently established by Bourguin and Campese (2020).
    
[^36]: 基于马尔科夫链的常步长SGD的收敛和集中性质

    Convergence and concentration properties of constant step-size SGD through Markov chains. (arXiv:2306.11497v1 [stat.ML])

    [http://arxiv.org/abs/2306.11497](http://arxiv.org/abs/2306.11497)

    本文通过马尔科夫链研究了常步长随机梯度下降的性质，证明了迭代收敛于一个不变分布，并获得了高置信度边界。

    

    本文考虑使用常步长随机梯度下降（SGD）优化平滑且强凸的目标，并通过马尔科夫链研究其性质。我们证明，对于具有轻微受控方差的无偏梯度估计，迭代以总变差距离收敛于一个不变分布。我们还在与以前工作相比梯度噪声分布的放宽假设下，在Wasserstein-2距离下建立了这种收敛性。由于极限分布的不变性质，我们的分析表明，当这些对于梯度成立时，后者继承了亚高斯或亚指数浓度特性。这允许推导出对于最终估计的高置信度边界。最后，在这种条件下，在线性情况下，对于Polyak-Ruppert序列的尾部，我们获得了一个无维度偏差限制。所有结果均为非渐近性质，并讨论了其后果。

    We consider the optimization of a smooth and strongly convex objective using constant step-size stochastic gradient descent (SGD) and study its properties through the prism of Markov chains. We show that, for unbiased gradient estimates with mildly controlled variance, the iteration converges to an invariant distribution in total variation distance. We also establish this convergence in Wasserstein-2 distance under a relaxed assumption on the gradient noise distribution compared to previous work. Thanks to the invariance property of the limit distribution, our analysis shows that the latter inherits sub-Gaussian or sub-exponential concentration properties when these hold true for the gradient. This allows the derivation of high-confidence bounds for the final estimate. Finally, under such conditions in the linear case, we obtain a dimension-free deviation bound for the Polyak-Ruppert average of a tail sequence. All our results are non-asymptotic and their consequences are discussed thr
    
[^37]: 具有网络辅助信息的高维线性回归中的贝叶斯最优学习

    Bayes optimal learning in high-dimensional linear regression with network side information. (arXiv:2306.05679v1 [math.ST])

    [http://arxiv.org/abs/2306.05679](http://arxiv.org/abs/2306.05679)

    本文首次研究了具有网络辅助信息的高维线性回归中的贝叶斯最优学习问题，引入了Reg-Graph模型并提出了基于AMP的迭代算法，在实验中优于现有的几种网络辅助回归方法。

    

    在基因组学、蛋白质组学和神经科学等应用中，具有网络辅助信息的监督学习问题经常出现。本文中，我们首次研究了具有网络辅助信息的高维线性回归中的贝叶斯最优学习问题。为此，我们首先引入了一个简单的生成模型（称为Reg-Graph模型），通过一组共同的潜在参数为监督数据和观测到的网络设定了一个联合分布。接下来，我们介绍了一种基于近似消息传递（AMP）的迭代算法，在非常一般的条件下可证明是贝叶斯最优的。此外，我们对潜在信号和观测到的数据之间的极限互信息进行了表征，从而精确量化了网络辅助信息在回归问题中的统计影响。我们对模拟数据和实际数据的实验表明，我们的方法优于现有的几种网络辅助回归方法。

    Supervised learning problems with side information in the form of a network arise frequently in applications in genomics, proteomics and neuroscience. For example, in genetic applications, the network side information can accurately capture background biological information on the intricate relations among the relevant genes. In this paper, we initiate a study of Bayes optimal learning in high-dimensional linear regression with network side information. To this end, we first introduce a simple generative model (called the Reg-Graph model) which posits a joint distribution for the supervised data and the observed network through a common set of latent parameters. Next, we introduce an iterative algorithm based on Approximate Message Passing (AMP) which is provably Bayes optimal under very general conditions. In addition, we characterize the limiting mutual information between the latent signal and the data observed, and thus precisely quantify the statistical impact of the network side 
    
[^38]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^39]: 流式数据主动计费标注

    Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v1 [cs.LG])

    [http://arxiv.org/abs/2304.06808](http://arxiv.org/abs/2304.06808)

    本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。

    

    我们研究了主动标记流数据问题，其中主动学习者面临一系列数据点，并必须通过昂贵的实验精心选择哪些点进行标记，此类问题常常出现在医疗和天文学等领域。我们首先研究的是数据输入属于$K$个离散分布之一的情况，并通过损失函数形式化描述此问题，该损失函数捕捉了标记成本和预测误差。当标记成本为$B$时，我们的算法通过选择标记点，仅在不确定性大于时间和成本相关阈值时进行，可以在$T$轮之后实现$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。我们还提供了更细致的上界，证明了在到达模式更有利时，算法可以适应到达模式，并实现更好的性能。我们还补充了两个上界的匹配下界。接下来，我们研究了在流数据具有不确定性分布的情况下标记流数据的问题，并提供与前面情况类似的结果。

    We study actively labeling streaming data, where an active learner is faced with a stream of data points and must carefully choose which of these points to label via an expensive experiment. Such problems frequently arise in applications such as healthcare and astronomy. We first study a setting when the data's inputs belong to one of $K$ discrete distributions and formalize this problem via a loss that captures the labeling cost and the prediction error. When the labeling cost is $B$, our algorithm, which chooses to label a point if the uncertainty is larger than a time and cost dependent threshold, achieves a worst-case upper bound of $O(B^{\frac{1}{3}} K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuanced upper bound which demonstrates that the algorithm can adapt to the arrival pattern, and achieves better performance when the arrival pattern is more favorable. We complement both upper bounds with matching lower bounds. We next study this pr
    
[^40]: 因果依赖图

    Causal Dependence Plots. (arXiv:2303.04209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04209](http://arxiv.org/abs/2303.04209)

    本论文提出了因果依赖图（CDPs）来解释人工智能或机器学习模型的因果依赖关系。CDPs与传统方法不同，可以模块化地结合因果学习或敏感度分析方法。这些图表可以成为可解释机器学习工具包中的强大工具，并对相关应用做出贡献。

    

    解释人工智能或机器学习模型的重要性越来越大。为了明智地使用这些数据驱动的系统，我们必须了解它们如何与世界互动，包括它们在数据输入上的因果依赖关系。在这项工作中，我们开发了因果依赖图 (CDPs)，用于可视化一个变量（结果）如何随另一个变量（预测器）的变化而变化，以及其他预测器变量的因果变化。关键是，CDPs与基于保持其他预测器恒定或假设它们独立的标准方法不同。CDPs利用辅助因果模型，因为因果结论需要因果假设。通过模拟和真实数据实验，我们展示了CDPs可以与因果学习或敏感性分析方法以模块化的方式结合使用。由于人们经常在输入-输出依赖性方面进行因果思考，CDPs可以成为xAI或可解释机器学习工具包中强有力的工具，并对应用有所贡献。

    Explaining artificial intelligence or machine learning models is increasingly important. To use such data-driven systems wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how one variable--an outcome--depends on changes in another variable--a predictor--$\textit{along with any consequent causal changes in other predictor variables}$. Crucially, CDPs differ from standard methods based on holding other predictors constant or assuming they are independent. CDPs make use of an auxiliary causal model because causal conclusions require causal assumptions. With simulations and real data experiments, we show CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to appl
    
[^41]: 在 exp-concave 统计学习中探索本地规范

    Exploring Local Norms in Exp-concave Statistical Learning. (arXiv:2302.10726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10726](http://arxiv.org/abs/2302.10726)

    这篇论文研究了在 exp-concave 统计学习中使用经验风险最小化的问题，提出了适用于广泛类别的有界 exp-concave 损失的过量风险界，维度和样本大小对结果有影响，并且基于统一几何假设和本地规范的概念。

    

    我们考虑使用经验风险最小化在一个凸类中处理带有 exp-concave 损失的随机凸优化问题。回答了一些之前研究中提出的问题，我们给出了一个对于一类广泛的有界 exp-concave 损失的 $O(d/n + \log(1/\delta)/n)$ 过量风险界，其中 $d$ 是凸参考集的维度，$n$ 是样本大小，$\delta$ 是置信水平。我们的结果基于对损失梯度的统一几何假设和本地规范的概念。

    We consider the problem of stochastic convex optimization with exp-concave losses using Empirical Risk Minimization in a convex class. Answering a question raised in several prior works, we provide a $O( d / n + \log( 1 / \delta) / n )$ excess risk bound valid for a wide class of bounded exp-concave losses, where $d$ is the dimension of the convex reference set, $n$ is the sample size, and $\delta$ is the confidence level. Our result is based on a unified geometric assumption on the gradient of losses and the notion of local norms.
    
[^42]: 仅调整归一化层的表达能力

    The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07937](http://arxiv.org/abs/2302.07937)

    本研究发现，仅调整神经网络的归一化层参数就可以达到高准确性，甚至可以重建比原网络小O(根号宽度)倍的目标网络。

    

    特征归一化转换，如批量归一化和层归一化，已成为当今先进深度神经网络不可或缺的组成部分。关于微调大型预训练模型的最近研究表明，仅调整这些仿射变换的参数就可以在下游任务中获得高准确性。这些研究结果引发了对调整冻结网络的归一化层的表达能力的问题。本文首次探讨这个问题，并显示对于随机ReLU网络，仅微调其归一化层可以重建任何大小为O(根号宽度)倍小的目标网络。我们证明，即使在随机稀疏网络中，在足够超参数化的情况下，这个结论也成立，与先前的实证工作一致。

    Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work.
    
[^43]: 维度约减与MARS

    Dimension Reduction and MARS. (arXiv:2302.05790v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.05790](http://arxiv.org/abs/2302.05790)

    本文改进了多元自适应回归样条（MARS）的性能，通过使用协变量的线性组合来实现足够的维度约减，提高了估计效率。

    

    多元自适应回归样条（MARS）是非参数多元回归估计方法中的常用方法之一。然而，由于MARS基于边际样条，为了考虑协变量之间的交互作用，必须使用边际样条的乘积，这会导致基函数数量过多，当交互作用阶数很高时，估计效率会降低。本文通过使用协变量的线性组合来改进MARS的性能，实现了足够的维度约减。MARS的特殊基函数有助于计算回归函数的梯度，并且通过对梯度的外积进行特征分析来估计线性组合。在一些技术条件下，对所提出的估计方法建立了渐近理论。包括模拟和实证应用的数值研究表明了其在维度约减和改进估计效果上的有效性。

    The multivariate adaptive regression spline (MARS) is one of the popular estimation methods for nonparametric multivariate regressions. However, as MARS is based on marginal splines, to incorporate interactions of covariates, products of the marginal splines must be used, which leads to an unmanageable number of basis functions when the order of interaction is high and results in low estimation efficiency. In this paper, we improve the performance of MARS by using linear combinations of the covariates which achieve sufficient dimension reduction. The special basis functions of MARS facilitate calculation of gradients of the regression function, and estimation of the linear combinations is obtained via eigen-analysis of the outer-product of the gradients. Under some technical conditions, the asymptotic theory is established for the proposed estimation method. Numerical studies including both simulation and empirical applications show its effectiveness in dimension reduction and improvem
    
[^44]: SOBER：离散和混合空间上高并行贝叶斯优化和贝叶斯积分

    SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces. (arXiv:2301.11832v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11832](http://arxiv.org/abs/2301.11832)

    SOBER算法是一种在离散和混合空间上进行高并行贝叶斯优化的方法，能够进行可扩展和多样化的批量全局优化和积分，且优于11个竞争基线方法。

    

    批处理贝叶斯优化和贝叶斯积分已被证明是在需并行查询昂贵的目标函数时执行优化和积分的高效方法。然而，当前的方法不适用于大批量操作。我们提出了一种新算法——SOBER，它允许在离散和混合空间上使用任意采集函数和内核进行可扩展和多样化的批量全局优化和积分。我们的方法的关键在于将全局优化的批量选择重新定义为积分问题，并将采集函数的最大化（非凸）松弛为内核重组（凸），从而有效地解决了两个任务。我们展示SOBER优于11个竞争基线方法。

    Batch Bayesian optimisation and Bayesian quadrature have been shown to be sample-efficient methods of performing optimisation and quadrature where expensive-to-evaluate objective functions can be queried in parallel. However, current methods do not scale to large batch sizes -- a frequent desideratum in practice (e.g. drug discovery or simulation-based inference). We present a novel algorithm, SOBER, which permits scalable and diversified batch global optimisation and quadrature with arbitrary acquisition functions and kernels over discrete and mixed spaces. The key to our approach is to reformulate batch selection for global optimisation as a quadrature problem, which relaxes acquisition function maximisation (non-convex) to kernel recombination (convex). Bridging global optimisation and quadrature can efficiently solve both tasks by balancing the merits of exploitative Bayesian optimisation and explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive baselines o
    
[^45]: 细调神经算符结构以提高训练和泛化能力

    Fine-tuning Neural-Operator architectures for training and generalization. (arXiv:2301.11509v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11509](http://arxiv.org/abs/2301.11509)

    本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。

    

    本篇论文全面分析了神经算符（NOs）及其衍生结构的泛化特性。通过对测试损失的经验评估、基于复杂性的泛化界限的分析以及对损失景观可视化的定性评估，我们研究了旨在提高NOs泛化能力的修改。受Transformer的成功启发，我们提出了${\textit{s}}{\text{NO}}+\varepsilon$，该方法引入了一个核积分算符来代替自关注机制。我们的结果显示，伴随着损失景观可视化的定性变化，性能显著提高了，适用于各种数据集和初始化。我们猜测，Transformer的布局使优化算法能够找到更好的极小值，并且随机深度可以提高泛化性能。由于训练动态的严格分析是深度学习最突出的未解决问题之一，因此我们还推出了一个新的训练课程，重点是逐渐增加模型容量，从而显著提高了泛化能力。

    This work provides a comprehensive analysis of the generalization properties of Neural Operators (NOs) and their derived architectures. Through empirical evaluation of the test loss, analysis of the complexity-based generalization bounds, and qualitative assessments of the visualization of the loss landscape, we investigate modifications aimed at enhancing the generalization capabilities of NOs. Inspired by the success of Transformers, we propose ${\textit{s}}{\text{NO}}+\varepsilon$, which introduces a kernel integral operator in lieu of self-Attention. Our results reveal significantly improved performance across datasets and initializations, accompanied by qualitative changes in the visualization of the loss landscape. We conjecture that the layout of Transformers enables the optimization algorithm to find better minima, and stochastic depth, improve the generalization performance. As a rigorous analysis of training dynamics is one of the most prominent unsolved problems in deep lear
    
[^46]: 案例基础神经网络：具有时间变化的高阶交互的生存分析

    Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06535](http://arxiv.org/abs/2301.06535)

    案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。

    

    神经网络基于生存分析方法可以模拟数据驱动的协变量交互。虽然这些方法可以比回归方法提供更好的预测性能，但并不是所有的方法都可以模拟时间变化的交互和复杂的基线风险。为了解决这个问题，我们提出了一种称为案例基础神经网络（CBNNs）的新方法，它将案例基础抽样框架与灵活的神经网络结构相结合。通过使用一种新颖的抽样方案和数据增强来自然地考虑到截尾，我们构建了一个可以接受时间输入的前馈神经网络。CBNNs通过预测在给定时刻事件发生的概率来估计危险函数。我们通过模拟和三个案例研究使用两个时间依赖指标比较CBNNs与回归和神经网络基于生存分析方法的性能。首先，我们通过涉及复杂基线风险和时间变化交互的模拟来评估所有方法，其中包括CBNNs。

    Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
    
[^47]: 通过广义随机优势统计比较分类器

    Statistical Comparisons of Classifiers by Generalized Stochastic Dominance. (arXiv:2209.01857v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.01857](http://arxiv.org/abs/2209.01857)

    这篇论文通过采用决策理论的最新发展，提出了一种基于广义随机优势的分类器比较框架，该框架通过解决易处理的线性规划问题进行操作，并通过适应的两样本观察随机化测试进行统计测试。

    

    尽管成为机器学习算法发展的一个关键问题，但如何在多个数据集上根据多个标准比较分类器仍然没有共识。每个比较框架都面临至少三个基本挑战：质量标准的多样性，数据集的多样性以及数据集的随机选择。本文通过采用最近在决策理论中的发展，为这场激烈的辩论增添了新的视角。基于所谓的偏好系统，我们的框架通过广义随机优势的概念对分类器进行排名，强大地绕过了繁琐且往往自相矛盾的聚合依赖。此外，我们还展示了广义随机优势可以通过解决易处理的线性规划问题进行操作，并通过适应的两样本观察随机化测试进行统计测试。这确实为我们提供了一个强大的框架。

    Although being a crucial question for the development of machine learning algorithms, there is still no consensus on how to compare classifiers over multiple data sets with respect to several criteria. Every comparison framework is confronted with (at least) three fundamental challenges: the multiplicity of quality criteria, the multiplicity of data sets and the randomness of the selection of data sets. In this paper, we add a fresh view to the vivid debate by adopting recent developments in decision theory. Based on so-called preference systems, our framework ranks classifiers by a generalized concept of stochastic dominance, which powerfully circumvents the cumbersome, and often even self-contradictory, reliance on aggregates. Moreover, we show that generalized stochastic dominance can be operationalized by solving easy-to-handle linear programs and moreover statistically tested employing an adapted two-sample observation-randomization test. This yields indeed a powerful framework fo
    
[^48]: 动态排名和翻译同步

    Dynamic Ranking and Translation Synchronization. (arXiv:2207.01455v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2207.01455](http://arxiv.org/abs/2207.01455)

    本论文研究了动态排名和翻译同步问题，主要关注成对比较数据随时间变化的情况，并给出了相应的理论结果。

    

    在许多应用中，如体育比赛或推荐系统，我们可以获得由一组$n$个项目（或选手）之间的成对比较组成的数据。目标是利用这些数据推断每个项目和/或它们的排名的潜在实力。现有结果主要关注单个比较图$G$的设置。然而，在某些情况下（如体育比赛），成对比较数据会随时间变化。对于这种动态设置，理论结果相对有限，是本文的重点。我们研究了翻译同步问题在动态设置下的扩展。在这个设置中，我们给定了一个比较图序列$(G_t)_{t\in \mathcal{T}}$，其中$\mathcal{T} \subset [0,1]$是表示时间域的格点，对于每个项目$i$和时间$t\in \mathcal{T}$，存在一个关联的未知实力参数$z^*_{t,i}\in \mathbb{R}$。

    In many applications, such as sport tournaments or recommendation systems, we have at our disposal data consisting of pairwise comparisons between a set of $n$ items (or players). The objective is to use this data to infer the latent strength of each item and/or their ranking. Existing results for this problem predominantly focus on the setting consisting of a single comparison graph $G$. However, there exist scenarios (e.g., sports tournaments) where the the pairwise comparison data evolves with time. Theoretical results for this dynamic setting are relatively limited and is the focus of this paper.  We study an extension of the \emph{translation synchronization} problem, to the dynamic setting. In this setup, we are given a sequence of comparison graphs $(G_t)_{t\in \mathcal{T}}$, where $\mathcal{T} \subset [0,1]$ is a grid representing the time domain, and for each item $i$ and time $t\in \mathcal{T}$ there is an associated unknown strength parameter $z^*_{t,i}\in \mathbb{R}$. We ai
    
[^49]: 学习简单非参数混合模型的难度的严格界限

    Tight Bounds on the Hardness of Learning Simple Nonparametric Mixtures. (arXiv:2203.15150v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15150](http://arxiv.org/abs/2203.15150)

    我们研究了学习非参数混合模型中组件分布的难度，并建立了学习每个组件分布的样本复杂性的严格界限。我们的主要结果是需要$(\frac{1}{\varepsilon})^{\Omega(\log\log \frac{1}{\varepsilon})}$个样本来估计每个组件分布。

    

    我们研究了在有限混合模型中学习非参数分布的问题，并对学习这些模型中的组件分布的样本复杂性建立了严格界限。具体而言，我们给定了来自概率密度函数$f$的i.i.d.样本，其中$$ f=w_1f_1+w_2f_2，\quad w_1+w_2=1，\quad w_1,w_2>0 $$我们感兴趣的是学习每个组件$f_i$。在对$f_i$没有任何假设的情况下，这个问题是无法解决的。为了识别组件$f_i$，我们假设每个$f_i$可以写成一个高斯函数和一个紧支撑密度$\nu_i$的卷积形式，其中$\text{supp}(\nu_1)\cap \text{supp}(\nu_2)=\emptyset$。我们的主要结果表明，为了估计每个$f_i$，需要$(\frac{1}{\varepsilon})^{\Omega(\log\log \frac{1}{\varepsilon})}$个样本。证明依赖于一个量化的塔伯利安定理，该定理提供了与高斯函数的快速逼近速度，这可能是独立感兴趣的。为了证明这是紧确的，我们还提出了一种使用$(\frac{1

    We study the problem of learning nonparametric distributions in a finite mixture, and establish tight bounds on the sample complexity for learning the component distributions in such models. Namely, we are given i.i.d. samples from a pdf $f$ where $$ f=w_1f_1+w_2f_2, \quad w_1+w_2=1, \quad w_1,w_2>0 $$ and we are interested in learning each component $f_i$. Without any assumptions on $f_i$, this problem is ill-posed. In order to identify the components $f_i$, we assume that each $f_i$ can be written as a convolution of a Gaussian and a compactly supported density $\nu_i$ with $\text{supp}(\nu_1)\cap \text{supp}(\nu_2)=\emptyset$.  Our main result shows that $(\frac{1}{\varepsilon})^{\Omega(\log\log \frac{1}{\varepsilon})}$ samples are required for estimating each $f_i$. The proof relies on a quantitative Tauberian theorem that yields a fast rate of approximation with Gaussians, which may be of independent interest. To show this is tight, we also propose an algorithm that uses $(\frac{1
    
[^50]: 用于简单遗憾最小化的元学习

    Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12888](http://arxiv.org/abs/2202.12888)

    本论文提出了用于在赌博机中进行简单遗憾最小化的元学习框架，并提出了首个贝叶斯和频率派元学习算法。贝叶斯算法具有先验分布并且具有较小的元简单遗憾，而频率派算法更通用且可以在更多的设置中进行分析。通过将算法应用于不同的赌博机问题，我们验证了理论的有效性。

    

    我们提出了一个用于在赌博机中简单遗憾最小化的元学习框架。在这个框架中，学习代理与一系列赌博机任务进行交互，这些任务是从一个未知的先验分布中独立采样的，并学习其元参数以在未来任务中表现更好。我们提出了这个设置的第一个贝叶斯和频率派元学习算法。贝叶斯算法可以访问元参数的先验分布，并且其在$m$个赌博机任务中，时间界为$n$的元简单遗憾仅为$\tilde{O}(m / \sqrt{n})$。另一方面，频率派算法的元简单遗憾为$\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$。尽管遗憾更大，但频率派算法更通用，因为它不需要元参数的先验分布，并且可以在更多的设置中进行分析。我们通过将算法应用于几类赌博机问题来验证我们的理论。

    We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\tilde{O}(m / \sqrt{n})$. On the other hand, the meta simple regret of the frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$. While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them
    
[^51]: 一种使用样本矩方法的非传统参数化密度估计

    A Non-Classical Parameterization for Density Estimation Using Sample Moments. (arXiv:2201.04786v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.04786](http://arxiv.org/abs/2201.04786)

    本文提出了一种使用样本矩方法进行非传统参数化密度估计的方法，通过平方Hellinger距离进行参数化，并通过凸优化得到唯一解。通过幂矩估计提出了估计器的统计性质和渐近误差上界，模拟结果验证了该方法的性能。

    

    概率密度估计是统计学和信号处理的核心问题。矩方法是密度估计的重要方法，但通常强烈依赖于可行函数的选择，这严重影响了性能。在本文中，我们提出了一种使用样本矩方法进行密度估计的非传统参数化方法，不需要选择这样的函数。该参数化是由平方Hellinger距离引起的，并且它的解被证明在不依赖于数据的简单先验条件下存在并且是唯一的，并且可以通过凸优化得到。通过幂矩估计提出了密度估计器的统计性质，以及估计器的渐近误差上界。给出了所提出的密度估计器在信号处理任务中的应用。通过与几种流行方法的比较，模拟结果验证了估计器的性能。

    Probability density estimation is a core problem of statistics and signal processing. Moment methods are an important means of density estimation, but they are generally strongly dependent on the choice of feasible functions, which severely affects the performance. In this paper, we propose a non-classical parametrization for density estimation using sample moments, which does not require the choice of such functions. The parametrization is induced by the squared Hellinger distance, and the solution of it, which is proved to exist and be unique subject to a simple prior that does not depend on data, and can be obtained by convex optimization. Statistical properties of the density estimator, together with an asymptotic error upper bound are proposed for the estimator by power moments. Applications of the proposed density estimator in signal processing tasks are given. Simulation results validate the performance of the estimator by a comparison to several prevailing methods. To the best 
    
[^52]: 关于随机梯度下降方法的不同自适应批量大小选择策略的等价性研究

    On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods. (arXiv:2109.10933v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2109.10933](http://arxiv.org/abs/2109.10933)

    本研究证明了在特定选择的Θ和ν下，范数测试和内积/正交性测试在随机梯度下降方法的收敛速度方面是等价的，同时指出在最理想情况下，内积/正交性测试可以像范数测试一样廉价。

    

    本研究证明了在特定选择的Θ和ν下，\cite{Bol18}中提出的范数测试和内积/正交性测试在随机梯度下降方法的收敛速度方面是等价的，其中ϵ²=θ²+ν²。这里，ϵ控制梯度范数的相对统计误差，而θ和ν分别控制梯度在梯度方向和梯度正交方向上的相对统计误差。此外，我们证明了在最理想情况下，内积/正交性测试可以像范数测试一样廉价，如果θ和ν被最优选择，但是如果ϵ²=θ²+ν²，内积/正交性测试永远不会比范数测试更具计算可承受性。最后，我们提供了两个随机优化问题来说明我们的结果。

    In this study, we demonstrate that the norm test and inner product/orthogonality test presented in \cite{Bol18} are equivalent in terms of the convergence rates associated with Stochastic Gradient Descent (SGD) methods if $\epsilon^2=\theta^2+\nu^2$ with specific choices of $\theta$ and $\nu$. Here, $\epsilon$ controls the relative statistical error of the norm of the gradient while $\theta$ and $\nu$ control the relative statistical error of the gradient in the direction of the gradient and in the direction orthogonal to the gradient, respectively. Furthermore, we demonstrate that the inner product/orthogonality test can be as inexpensive as the norm test in the best case scenario if $\theta$ and $\nu$ are optimally selected, but the inner product/orthogonality test will never be more computationally affordable than the norm test if $\epsilon^2=\theta^2+\nu^2$. Finally, we present two stochastic optimization problems to illustrate our results.
    
[^53]: 流形上的优化：一种辛算法的方法

    Optimization on manifolds: A symplectic approach. (arXiv:2107.11231v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2107.11231](http://arxiv.org/abs/2107.11231)

    这项工作提出了一种在平滑流形上解决优化问题的通用框架，利用流形上的辛算法进行加速，可以处理具有非线性约束的问题，并具有良好的收敛性能。

    

    在统计机器学习中，优化任务至关重要。近年来，人们广泛利用动力系统的工具，通过连续时间系统的适当离散化，导出加速和鲁棒性优化方法。然而，这些思想大多局限于欧几里得空间和无约束的设置，或者Riemannian梯度流。在本文中，我们提出了一种耗散型的Dirac约束哈密顿系统扩展，作为解决平滑流形上的优化问题的通用框架，包括具有非线性约束的问题。我们在流形上开发了几何/辛数值积分器，它们是"速率匹配"的，即保持连续时间的收敛速率。特别是，我们介绍了一种耗散型的RATTLE积分器，能够在局部实现最优收敛速率。我们的类别（加速的）算法不仅简单高效，而且适用于广泛的环境。

    Optimization tasks are crucial in statistical machine learning. Recently, there has been great interest in leveraging tools from dynamical systems to derive accelerated and robust optimization methods via suitable discretizations of continuous-time systems. However, these ideas have mostly been limited to Euclidean spaces and unconstrained settings, or to Riemannian gradient flows. In this work, we propose a dissipative extension of Dirac's theory of constrained Hamiltonian systems as a general framework for solving optimization problems over smooth manifolds, including problems with nonlinear constraints. We develop geometric/symplectic numerical integrators on manifolds that are "rate-matching," i.e., preserve the continuous-time rates of convergence. In particular, we introduce a dissipative RATTLE integrator able to achieve optimal convergence rate locally. Our class of (accelerated) algorithms are not only simple and efficient but also applicable to a broad range of contexts.
    
[^54]: 深度强化学习中的迁移学习综述

    Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.07888](http://arxiv.org/abs/2009.07888)

    这篇综述调查了深度强化学习领域中的迁移学习方法的最新进展，并提供了一个对这些方法进行分类的框架。分析了它们的目标、方法学、兼容的强化学习背景以及实际应用，并探讨了迁移学习与其他相关主题之间的联系。

    

    强化学习是解决序列决策问题的学习范式。近年来，随着深度神经网络的快速发展，强化学习取得了显著的进展。除了在机器人和游戏等诸多领域中具有良好前景的强化学习，迁移学习作为一种解决强化学习面临的各种挑战的方法已经出现，通过从外部专业知识中转移知识，以提高学习过程的效率和效果。在这项综述中，我们系统地调查了深度强化学习领域中的迁移学习方法的最新进展。具体而言，我们提供了一个对最先进的迁移学习方法进行分类的框架，在此框架下分析了它们的目标、方法学、兼容的强化学习背景以及实际应用。我们还探讨了迁移学习与其他相关主题之间的联系。

    Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
    
[^55]: 生成对抗性训练器：使用生成对抗网络防御对抗扰动

    Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN. (arXiv:1705.03387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1705.03387](http://arxiv.org/abs/1705.03387)

    本论文提出了一种使用生成对抗网络来防御对抗扰动的新方法，通过交替训练分类器和生成器网络，生成器网络生成对抗扰动以欺骗分类器网络，同时分类器网络被训练以正确分类原始和对抗图像，这一过程使分类器网络对对抗扰动更加鲁棒，同时还能有效地降低网络的过拟合问题。

    

    我们提出了一种新颖的技术，使用生成对抗网络使神经网络对对抗性示例具有鲁棒性。我们交替训练分类器和生成器网络。生成器网络通过使用每个图像的梯度生成对抗扰动，从而轻松欺骗分类器网络。同时，分类器网络被训练以正确分类由生成器生成的原始和对抗图像。这些过程有助于使分类器网络对对抗扰动更加鲁棒。此外，我们的对抗训练框架有效地减少了过拟合，并且优于其他正则化方法，如Dropout。我们将我们的方法应用于CIFAR数据集的有监督学习中，实验结果表明我们的方法显著降低了网络的泛化误差。据我们所知，这是第一个使用生成对抗网络来改进监督学习的方法。

    We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.
    

