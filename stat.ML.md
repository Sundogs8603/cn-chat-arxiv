# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows](https://arxiv.org/abs/2403.16995) | 语言校正流是一种基于标准概率流模型的新方法，通过学习常微分方程模型在源分布和目标分布之间传输，提供了统一和有效的生成模型和领域转移解决方案。 |
| [^2] | [The Sample Complexity of Simple Binary Hypothesis Testing](https://arxiv.org/abs/2403.16981) | 该论文导出了一个公式，用于刻画简单二元假设检验的样本复杂度（乘法常数独立于$p$、$q$和所有错误参数），适用于不同的设置条件。 |
| [^3] | [SCOD: From Heuristics to Theory](https://arxiv.org/abs/2403.16916) | 本文提出了针对SCOD问题的最优策略，包括基于贝叶斯分类器和随机线性分类器的选择器，以解决不确定或离群样本预测可靠性问题。 |
| [^4] | [Discrete Latent Graph Generative Modeling with Diffusion Bridges](https://arxiv.org/abs/2403.16883) | GLAD是一个在离散潜在空间上操作的图生成模型，通过适应扩散桥结构学习其离散潜在空间的先验，避免了依赖于原始数据空间的分解，在图生成任务中表现出优越性。 |
| [^5] | [Conformal Off-Policy Prediction for Multi-Agent Systems](https://arxiv.org/abs/2403.16871) | 这项工作介绍了MA-COPP，这是第一个解决涉及多智能体系统的离策略预测问题的一致预测方法。 |
| [^6] | [Weak Convergence Analysis of Online Neural Actor-Critic Algorithms](https://arxiv.org/abs/2403.16825) | 在线神经演员-评论算法中，我们证明当隐藏单元和训练步数的数量$\rightarrow \infty$时，单层神经网络将收敛于随机ODE，通过建立数据样本的几何遍历性和使用泊松方程证明模型更新波动消失，演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。 |
| [^7] | [Optimal convex $M$-estimation via score matching](https://arxiv.org/abs/2403.16688) | 该论文提出了一种通过得分匹配实现最佳凸$M$-估计的方法，在线性回归中能够达到最佳的渐近方差，并且在计算上高效，证明具有所有凸$M$-估计中最小的渐近协方差。 |
| [^8] | [A note on generalization bounds for losses with finite moments](https://arxiv.org/abs/2403.16681) | 本文研究了损失函数具有有限矩的泛化界，并推导了高概率的PAC-Bayes界，进一步揭示了对损失函数有界方差的情况下界的改进。此外，该研究将结果扩展到期望和单次抽样PAC-Bayes中，并获得了针对有界损失函数的快速速率界。 |
| [^9] | [Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis](https://arxiv.org/abs/2403.16523) | 从计数数据中发现因果结构的关键挑战在于非可辨识性问题，本研究发现在泊松分支结构因果模型中，如果根顶点$X$是已知的，则可以确定从$X$到其子节点$Y$的因果顺序。 |
| [^10] | [On the rates of convergence for learning with convolutional neural networks](https://arxiv.org/abs/2403.16459) | 该研究提出了对具有一定权重约束的CNNs的新逼近上界，以及对前馈神经网络的覆盖数做了新的分析，为基于CNNs的学习问题推导了收敛速率，并在学习平滑函数和二元分类方面取得了极小最优的结果。 |
| [^11] | [Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes](https://arxiv.org/abs/2403.16377) | 提出了一种神经过程方法，可以在实时条件监测信号预测中实现表示能力和敏捷性的权衡 |
| [^12] | [Learning Action-based Representations Using Invariance](https://arxiv.org/abs/2403.16369) | 提出了一种新的方法，动作双模拟编码，通过递归不变性约束扩展了单步控制性，学习了一个可以平滑折扣远期元素的多步控制度量 |
| [^13] | [Predictive Inference in Multi-environment Scenarios](https://arxiv.org/abs/2403.16336) | 本研究提出了在多环境预测问题中构建有效置信区间和置信集的方法，并展示了一种新的调整方法以适应问题难度，从而减少预测集大小，这在神经感应和物种分类数据集中的实际表现中得到验证。 |
| [^14] | [Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble](https://arxiv.org/abs/2403.16260) | 通过引入新颖的定性和定量模型集成评估方法，作者揭示了现有集成方法的关键缺陷，提出了提高传统模型集成维度的方法，以克服特征表示中的多样性限制。 |
| [^15] | [An Analytic Solution to Covariance Propagation in Neural Networks](https://arxiv.org/abs/2403.16163) | 该论文提出了一种无需样本的矩传播技术，能够准确表征神经网络的输入输出分布，其关键创新在于提供了通过非线性激活函数传递的随机变量协方差的解析解。 |
| [^16] | [Manifold Regularization Classification Model Based On Improved Diffusion Map](https://arxiv.org/abs/2403.16059) | 本文提出了基于改进扩散映射的流形正则化分类模型，通过改进标签传播模型，有效克服了原始流形正则化模型在局部区域性能上的限制。 |
| [^17] | [Learning Directed Acyclic Graphs from Partial Orderings](https://arxiv.org/abs/2403.16031) | 本文针对当可用部分因果顺序变量时学习DAGs的中间问题，提出了一个通用估计框架，并展示了有效的估计算法。 |
| [^18] | [Near-Optimal differentially private low-rank trace regression with guaranteed private initialization](https://arxiv.org/abs/2403.15999) | 该论文研究了在痕迹回归模型下以高斯测量矩阵进行差分隐私估计，提出了具有保证私有初始化的近最优算法，引入了一个高效的DP初始化算法和基于Riemannian优化的差分隐私算法，同时讨论了估计结果的非平凡差距。 |
| [^19] | [Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search](https://arxiv.org/abs/2403.15908) | 结合轨迹抽样和深度高斯协方差网络，以提高基于模型的强化学习问题的数据高效性。 |
| [^20] | [Fast and Unified Path Gradient Estimators for Normalizing Flows](https://arxiv.org/abs/2403.15881) | 提出了一种快速路径梯度估计器，显著提高了计算效率，并适用于所有实用的归一化流架构，具有正则化效果并减小了方差。 |
| [^21] | [Integrated path stability selection](https://arxiv.org/abs/2403.15877) | 该论文提出了一种基于集成稳定路径的新稳定选择方法，能够在实践中提高特征选择的灵敏度并更好地校准目标假阳性数量。 |
| [^22] | [Computational Sentence-level Metrics Predicting Human Sentence Comprehension](https://arxiv.org/abs/2403.15822) | 本研究引入了创新方法，使用多语言大型语言模型计算句子级度量，并证明这些度量能够高度准确地预测人类句子阅读速度，为未来整合LLMs和认知科学研究提供了有前景的方向。 |
| [^23] | [Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets](https://arxiv.org/abs/2403.15790) | 这里是中文总结出的一句话要点: 本文针对表格数据领域中自监督学习中的数据不平衡挑战，提出了一种用于混合表格数据集的缩放自动编码器，填补了研究中的缺口。 |
| [^24] | [Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier](https://arxiv.org/abs/2403.15778) | 这份论文着重探讨了功能数据的集成学习，并展示了如何利用不同的功能数据表示训练集成成员，以及如何通过多数投票组合基模型预测。 |
| [^25] | [Identifiable Latent Neural Causal Models](https://arxiv.org/abs/2403.15711) | 该研究确定了在潜在附加噪声模型背景下导致可识别性的分布变化类型的充分且必要条件，同时提出了当只有部分分布变化满足条件时的部分可识别性结果。 |
| [^26] | [Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs](https://arxiv.org/abs/2403.15707) | 介绍了新的Dynamic Signal Distribution (DSD)分类任务，模拟图像由$k$个维度为$d$的补丁组成，以解决CNNs相对于LCNs和FCNs的统计优势问题 |
| [^27] | [Conformal online model aggregation](https://arxiv.org/abs/2403.15527) | 该论文提出了一种基于投票的在线依从模型聚合方法，可以根据过去表现调整模型权重。 |
| [^28] | [Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective](https://arxiv.org/abs/2403.14917) | 本文通过核方法的视角研究了两层神经网络在平均场极限下的特征学习能力，展示了它们比任何核方法更有效地学习多个再现核希尔伯特空间的并集，并且神经网络会获得与目标函数对齐的数据相关核。 |
| [^29] | [A tutorial on learning from preferences and choices with Gaussian Processes](https://arxiv.org/abs/2403.11782) | 提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。 |
| [^30] | [Multivariate Gaussian Approximation for Random Forest via Region-based Stabilization](https://arxiv.org/abs/2403.09960) | 该论文通过基于区域稳定性的方法，推导出了随机森林预测的高斯逼近界限，并建立了适用于各种相关统计问题的概率结果。 |
| [^31] | [Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability](https://arxiv.org/abs/2403.03967) | 通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。 |
| [^32] | [Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage](https://arxiv.org/abs/2403.03868) | 该论文提出了一种构建具有有限样本精确覆盖的预测集的通用框架，可以解决在数据驱动情境中由于选择偏差导致的边缘有效预测区间误导问题。 |
| [^33] | [Inference for Regression with Variables Generated from Unstructured Data](https://arxiv.org/abs/2402.15585) | 提出了一种使用联合上游和下游模型进行有效推断的一步策略，显著减少了偏误，在CEO时间利用数据的应用中产生了重要效果，适合应用研究人员。 |
| [^34] | [Stochastic Hessian Fitting on Lie Group](https://arxiv.org/abs/2402.11858) | 本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。 |
| [^35] | [Featurizing Koopman Mode Decomposition](https://arxiv.org/abs/2312.09146) | 本文提出了一种命名为FKMD的先进KMD技术，通过时间嵌入和马氏距离缩放，可以增强对高维动力系统的分析和预测，特别适用于特征未知的情况，并在丙氨酸二肽数据降维和分析Lorenz吸引子和癌症研究中细胞信号问题方面取得了显著改进。 |
| [^36] | [Variational Bayes image restoration with compressive autoencoders](https://arxiv.org/abs/2311.17744) | 使用压缩自动编码器代替最先进的生成模型，提出了一种在图像恢复中的新方法。 |
| [^37] | [A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs](https://arxiv.org/abs/2311.10610) | 该论文介绍了一种针对图极限中图上信号的信号采样理论，证明了Poincar\'e不等式并展示了一致性结果。 |
| [^38] | [Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions](https://arxiv.org/abs/2311.02695) | 本文提出了一种新的方法，可以在一个环境中允许定位多个变量的干预，并在因果表示学习中首次得出了可识别性结果。 |
| [^39] | [Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input](https://arxiv.org/abs/2305.18699) | Transformer网络作为具有无限维输入的序列到序列函数，通过其特征提取能力和参数共享属性，能够避免维度诅咒，实现对目标函数的逼近和估计。 |
| [^40] | [The autoregressive neural network architecture of the Boltzmann distribution of pairwise interacting spins systems](https://arxiv.org/abs/2302.08347) | 该论文提出了将二进制成对相互作用系统的伯尔曼分布精确映射为自回归形式的方法，得到的ARNN架构具有明确定义的物理含义，并且可以通过统计物理技术推导出特定系统的新ARNN。 |
| [^41] | [Distributional Robustness Bounds Generalization Errors](https://arxiv.org/abs/2212.09962) | 分布鲁棒性界定了泛化错误，Bayesian方法在可能近似正确意义上是分布鲁棒的，同时正则化的经验风险最小化方法也被证明是等价于Bayesian方法的。 |
| [^42] | [Uncertainty Quantification of MLE for Entity Ranking with Covariates](https://arxiv.org/abs/2212.09961) | 提出了一种新颖的 Covariate-Assisted Ranking Estimation (CARE) 模型，扩展了 Bradley-Terry-Luce (BTL) 模型，通过将协变量信息结合进排名估计中，解决了实体排名问题中的不确定性。 |
| [^43] | [Online Action Learning in High Dimensions: A Conservative Perspective](https://arxiv.org/abs/2009.13961) | 该论文将$\epsilon_t$-贪心启发式方法扩展到高维度情境中，采用保守导向策略，实现在实用应用中对新奇性的重视，同时限制了采纳不寻常动作，有效控制了累积遗憾。 |
| [^44] | [Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis.](http://arxiv.org/abs/2401.11565) | 本文研究了具有噪音上下文的随机赌臂问题，并提出了一种Thompson采样算法，通过贝叶斯框架进行分析，证明了算法的贝叶斯后悔，并扩展了问题到延迟观察真实上下文的情况，并实证了算法的性能。 |
| [^45] | [PhyloGFN: Phylogenetic inference with generative flow networks.](http://arxiv.org/abs/2310.08774) | PhyloGFN是一种基于生成流网络的系统发育推断方法，通过采样复杂的组合结构，能够产生多样且高质量的进化假设，并在边缘似然估计方面具有竞争力。 |
| [^46] | [Local Search GFlowNets.](http://arxiv.org/abs/2310.02710) | 本文提出使用局部搜索训练GFlowNets，通过破坏和重构的方式探索局部邻域，分别由反向和正向策略引导，使得样本偏向高奖励解决方案。 |
| [^47] | [TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series.](http://arxiv.org/abs/2310.01327) | TACTiS-2是一种改进的多变量时间序列关注联合分布模型，采用了简化的目标函数和线性参数数量，具有更好的训练动态和最先进的性能。 |
| [^48] | [Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood.](http://arxiv.org/abs/2309.05153) | 本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。 |
| [^49] | [Conformal link prediction to control the error rate.](http://arxiv.org/abs/2306.14693) | 本研究提出了一种基于一致性推断思想的新方法，可在控制虚假发现率的前提下，识别一组真实的边。 |
| [^50] | [Samplet basis pursuit.](http://arxiv.org/abs/2306.10180) | 本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。 |
| [^51] | [Differentially Private Conditional Independence Testing.](http://arxiv.org/abs/2306.06721) | 本文介绍了两个差分隐私条件独立性检验方法，可适用于Z为连续值的一般情况。 |
| [^52] | [Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences.](http://arxiv.org/abs/2306.03111) | 本文提出了一种BootGen算法，使用代理得分函数增强生物序列生成器的训练数据集，并产生多样化的设计，将其应用于优化生物序列，取得了比竞争对手更好的结果。 |
| [^53] | [Spectral clustering in the Gaussian mixture block model.](http://arxiv.org/abs/2305.00979) | 本文首次研究了从高维高斯混合块模型中抽样的图聚类和嵌入问题。 |
| [^54] | [Sparse joint shift in multinomial classification.](http://arxiv.org/abs/2303.16971) | 该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。 |
| [^55] | [Provable Convergence of Variational Monte Carlo Methods.](http://arxiv.org/abs/2303.10599) | 本文提出了一种对变分蒙特卡罗（VMC）方法收敛性的可验证方法，在假设局部能量是次指数的条件下，使用非平稳马尔可夫链的Bernstein不等式推导出了MCMC估计量的误差界限，证明了VMC具有一阶收敛速率，在某些情况下，收敛速率是最优的。 |
| [^56] | [Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation.](http://arxiv.org/abs/2212.06370) | 本文提出了一种双重精度质量驱动的神经网络，可以自动地学习基于回归的神经网络的预测区间，而非只提供传统的目标估计。该方法通过设计一种新颖的损失函数，最小化平均预测区间宽度以及提高覆盖概率来提高PI的质量和精度，且比最先进的方法更加具有计算效率。 |
| [^57] | [Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient.](http://arxiv.org/abs/2208.07243) | 本研究分析了非零梯度的随机逼近算法的行为，并证明了指数级的集中性界限，这对于投影随机梯度下降算法的收敛速度有重要意义。 |
| [^58] | [CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity.](http://arxiv.org/abs/1902.05605) | CrossQ是一种轻量级算法，通过巧妙运用批归一化和删除目标网络的方式，提高了深度强化学习的样本效率，减少了计算成本，并且实施简单。 |

# 详细

[^1]: 语言校正流：通过概率流推动扩散语言生成

    Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows

    [https://arxiv.org/abs/2403.16995](https://arxiv.org/abs/2403.16995)

    语言校正流是一种基于标准概率流模型的新方法，通过学习常微分方程模型在源分布和目标分布之间传输，提供了统一和有效的生成模型和领域转移解决方案。

    

    最近的研究表明，在扩散语言模型基础上控制句子属性（例如情感）和结构（例如句法结构）取得了成功。一个推动高质量样本生成的关键组成部分是迭代去噪数千步。尽管有益，但从噪声开始的复杂性和学习步骤限制了其在许多NLP实际应用中的实现。本文提出了Language Rectified Flow方法。我们的方法基于标准概率流模型的重构。语言校正流学习（神经）常微分方程模型在源分布和目标分布之间传输，为生成建模和域转移提供了统一和有效的解决方案。从源分布开始，我们的语言校正流产生快速仿真和有效。

    arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe
    
[^2]: 简单二元假设检验的样本复杂度

    The Sample Complexity of Simple Binary Hypothesis Testing

    [https://arxiv.org/abs/2403.16981](https://arxiv.org/abs/2403.16981)

    该论文导出了一个公式，用于刻画简单二元假设检验的样本复杂度（乘法常数独立于$p$、$q$和所有错误参数），适用于不同的设置条件。

    

    简单的二元假设检验的样本复杂度是区分两个分布$p$和$q$所需的最小独立同分布样本数量，可以通过以下方式之一进行：(i) 无先验设置，类型-I错误最大为$\alpha$，类型-II错误最大为$\beta$; 或者 (ii) 贝叶斯设置，贝叶斯错误最大为$\delta$，先验分布为$(\alpha, 1-\alpha)$。 迄今为止，只在$\alpha = \beta$（无先验）或$\alpha = 1/2$（贝叶斯）时研究了此问题，并且已知样本复杂度可以用$p$和$q$之间的Hellinger散度来刻画，直到乘法常数。 在本文中，我们导出了一个公式，用来刻画样本复杂度（乘法常数独立于$p$、$q$和所有错误参数），适用于：(i) 先验设置中所有$0 \le \alpha, \beta \le 1/8$；以及 (ii) 贝叶斯设置中所有$\delta \le \alpha/4$。 特别地，该公式适用于

    arXiv:2403.16981v1 Announce Type: cross  Abstract: The sample complexity of simple binary hypothesis testing is the smallest number of i.i.d. samples required to distinguish between two distributions $p$ and $q$ in either: (i) the prior-free setting, with type-I error at most $\alpha$ and type-II error at most $\beta$; or (ii) the Bayesian setting, with Bayes error at most $\delta$ and prior distribution $(\alpha, 1-\alpha)$. This problem has only been studied when $\alpha = \beta$ (prior-free) or $\alpha = 1/2$ (Bayesian), and the sample complexity is known to be characterized by the Hellinger divergence between $p$ and $q$, up to multiplicative constants. In this paper, we derive a formula that characterizes the sample complexity (up to multiplicative constants that are independent of $p$, $q$, and all error parameters) for: (i) all $0 \le \alpha, \beta \le 1/8$ in the prior-free setting; and (ii) all $\delta \le \alpha/4$ in the Bayesian setting. In particular, the formula admits eq
    
[^3]: 从启发式到理论：SCOD

    SCOD: From Heuristics to Theory

    [https://arxiv.org/abs/2403.16916](https://arxiv.org/abs/2403.16916)

    本文提出了针对SCOD问题的最优策略，包括基于贝叶斯分类器和随机线性分类器的选择器，以解决不确定或离群样本预测可靠性问题。

    

    本文解决了设计可靠的预测模型，当面对不确定或离群样本时避免预测的问题 - 一种最近提出的被称为Selective Classification in the presence of Out-of-Distribution data (SCOD)的问题。我们对SCOD做出了三个关键贡献。首先，我们证明了最优的SCOD策略涉及基于贝叶斯分类器进行内部分布（ID）数据和在2D空间中表示为随机线性分类器的选择器，使用ID分类器的条件风险和ID与离群分布（OOD）数据的似然比作为输入。这与当前OOD检测方法和专为SCOD开发的Softmax Information Retaining Combination (SIRC)的次优策略形成对比。其次，我们建立了在一个无分布设置中，当仅依赖于条件分布和IID样本的逼近可能性时，SCOD问题不可能被正确学习。

    arXiv:2403.16916v1 Announce Type: new  Abstract: This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely 
    
[^4]: 带扩散桥的离散潜在图生成建模

    Discrete Latent Graph Generative Modeling with Diffusion Bridges

    [https://arxiv.org/abs/2403.16883](https://arxiv.org/abs/2403.16883)

    GLAD是一个在离散潜在空间上操作的图生成模型，通过适应扩散桥结构学习其离散潜在空间的先验，避免了依赖于原始数据空间的分解，在图生成任务中表现出优越性。

    

    学习潜在空间中的图生成模型相比于在原始数据空间上操作的模型受到较少关注，迄今表现出的性能乏善可陈。我们提出了GLAD，一个潜在空间图生成模型。与大多数先前的潜在空间图生成模型不同，GLAD在保留图结构的离散性质方面运行，无需进行诸如潜在空间连续性等不自然的假设。我们通过将扩散桥调整到其结构，来学习我们离散潜在空间的先验。通过在适当构建的潜在空间上操作，我们避免依赖于常用于在原始数据空间操作的模型中的分解。我们在一系列图基准数据集上进行实验，明显展示了离散潜在空间的优越性，并取得了最先进的图生成性能，使GLA

    arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
    
[^5]: 多智能体系统的一致离策略预测

    Conformal Off-Policy Prediction for Multi-Agent Systems

    [https://arxiv.org/abs/2403.16871](https://arxiv.org/abs/2403.16871)

    这项工作介绍了MA-COPP，这是第一个解决涉及多智能体系统的离策略预测问题的一致预测方法。

    

    离策略预测（OPP），即仅使用在一个正常（行为）策略下收集的数据来预测目标策略的结果，在数据驱动的安全关键系统分析中是一个重要问题，在这种系统中，部署新策略可能是不安全的。为了实现可信的离策略预测，最近关于一致离策略预测（COPP）的工作利用一致预测框架来在目标过程下推导带有概率保证的预测区域。现有的COPP方法可以考虑由策略切换引起的分布偏移，但仅限于单智能体系统和标量结果（例如，奖励）。在这项工作中，我们介绍了MA-COPP，这是第一个解决涉及多智能体系统的OPP问题的一致预测方法，在一个或多个“自我”智能体改变策略时为所有智能体轨迹推导联合预测区域。与单智能体场景不同，这种情况下

    arXiv:2403.16871v1 Announce Type: cross  Abstract: Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more "ego" agents change their policies. Unlike the single-agent scenario, this se
    
[^6]: 在线神经演员-评论算法的弱收敛分析

    Weak Convergence Analysis of Online Neural Actor-Critic Algorithms

    [https://arxiv.org/abs/2403.16825](https://arxiv.org/abs/2403.16825)

    在线神经演员-评论算法中，我们证明当隐藏单元和训练步数的数量$\rightarrow \infty$时，单层神经网络将收敛于随机ODE，通过建立数据样本的几何遍历性和使用泊松方程证明模型更新波动消失，演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。

    

    我们证明，使用在线演员评论算法训练的单层神经网络在隐藏单元和训练步数的数量$\rightarrow \infty$时，收敛于一个随机常微分方程（ODE）。在线演员评论算法中，随着模型的更新，数据样本的分布会动态变化，这对于任何收敛分析来说都是一个关键挑战。我们在固定演员策略下建立了数据样本的几何遍历性。然后，使用泊松方程，我们证明由于随机到达的数据样本带来的模型更新波动会随着参数更新次数的增加$\rightarrow \infty$而消失。利用泊松方程和弱收敛技术，我们证明演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。

    arXiv:2403.16825v1 Announce Type: new  Abstract: We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps $\rightarrow \infty$. In the online actor-critic algorithm, the distribution of the data samples dynamically changes as the model is updated, which is a key challenge for any convergence analysis. We establish the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the model updates around the limit distribution due to the randomly-arriving data samples vanish as the number of parameter updates $\rightarrow \infty$. Using the Poisson equation and weak convergence techniques, we prove that the actor neural network and critic neural network converge to the solutions of a system of ODEs with random initial conditions. Analysis of the 
    
[^7]: 通过得分匹配实现最佳凸$M$-估计

    Optimal convex $M$-estimation via score matching

    [https://arxiv.org/abs/2403.16688](https://arxiv.org/abs/2403.16688)

    该论文提出了一种通过得分匹配实现最佳凸$M$-估计的方法，在线性回归中能够达到最佳的渐近方差，并且在计算上高效，证明具有所有凸$M$-估计中最小的渐近协方差。

    

    在线性回归的背景下，我们构建了一个数据驱动的凸损失函数，通过该函数进行经验风险最小化可以在回归系数的下游估计中实现最佳的渐近方差。我们的半参数方法旨在最佳逼近噪声分布对数密度的导数。在总体层面上，这个拟合过程是对得分匹配的非参数拓展，对应于根据Fisher散度进行噪声分布的对数凹映射。该过程在计算上是高效的，我们证明我们的程序达到了所有凸$M$-估计中最小的渐近协方差。作为非对数凹设置的一个例子，对于柯西误差，最佳凸损失函数类似于Huber函数，并且我们的过程相对于oracle最大似然估计器实现了大于0.87的渐近效率。

    arXiv:2403.16688v1 Announce Type: cross  Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator o
    
[^8]: 一篇关于具有有限矩的损失函数泛化界的注记

    A note on generalization bounds for losses with finite moments

    [https://arxiv.org/abs/2403.16681](https://arxiv.org/abs/2403.16681)

    本文研究了损失函数具有有限矩的泛化界，并推导了高概率的PAC-Bayes界，进一步揭示了对损失函数有界方差的情况下界的改进。此外，该研究将结果扩展到期望和单次抽样PAC-Bayes中，并获得了针对有界损失函数的快速速率界。

    

    本文研究了Alquier [1]提出的截断方法，用于推导具有重尾特性的无界损失函数的高概率PAC-Bayes界。假设$p$-阶矩有界，得到的界在$p=2$时插值为缓慢的速率$1 / \sqrt{n}$，在$p \to \infty$且损失函数基本有界时插值为快速的速率$1 / n$。此外，本文导出了具有有界方差的损失函数的高概率PAC-Bayes界。该界对置信参数和依赖度量的依赖关系相比文献中先前的界具有指数级的改进。最后，本文将所有结果推广到期望保证和单次抽样PAC-Bayes中。为此，在这些设置中，它获得了[2]中针对有界损失函数的PAC-Bayes快速速率界的类似物。

    arXiv:2403.16681v1 Announce Type: cross  Abstract: This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \to \infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.
    
[^9]: 利用高阶累积量和路径分析从泊松分支结构因果模型中发现因果关系

    Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis

    [https://arxiv.org/abs/2403.16523](https://arxiv.org/abs/2403.16523)

    从计数数据中发现因果结构的关键挑战在于非可辨识性问题，本研究发现在泊松分支结构因果模型中，如果根顶点$X$是已知的，则可以确定从$X$到其子节点$Y$的因果顺序。

    

    计数数据在金融、神经科学和流行病学等领域中自然产生，在各种科学和工业场景中发现计数数据之间的因果结构是一项关键任务。计数数据的一个最常见特征是由二项式稀疏运算符和独立的泊松分布描述的固有分支结构，该结构捕捉了分支和噪声。例如，在人口计数情景中，死亡和移民对计数有贡献，其中生存遵循伯努利分布，移民遵循泊松分布。然而，由于不可辨识性问题，从这些数据中发现因果关系具有挑战性：单一因果对是马尔可夫等价的，即$X\rightarrow Y$和$Y\rightarrow X$在分布上是等价的。幸运的是，在这项工作中，我们发现如果$X$是一个根顶点，那么从$X$到其子节点$Y$的因果顺序是可识别的。

    arXiv:2403.16523v1 Announce Type: cross  Abstract: Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and
    
[^10]: 关于使用卷积神经网络进行学习收敛速率的研究

    On the rates of convergence for learning with convolutional neural networks

    [https://arxiv.org/abs/2403.16459](https://arxiv.org/abs/2403.16459)

    该研究提出了对具有一定权重约束的CNNs的新逼近上界，以及对前馈神经网络的覆盖数做了新的分析，为基于CNNs的学习问题推导了收敛速率，并在学习平滑函数和二元分类方面取得了极小最优的结果。

    

    我们研究了卷积神经网络（CNNs）的逼近和学习能力。第一个结果证明了在权重上有一定约束条件下CNNs的新逼近上界。第二个结果给出了对前馈神经网络的覆盖数的新分析，其中CNNs是其特例。该分析详细考虑了权重的大小，在某些情况下给出了比现有文献更好的上界。利用这两个结果，我们能够推导基于CNNs的估计器在许多学习问题中的收敛速率。特别地，我们在非参数回归设置中为基于CNNs的最小二乘学习平滑函数建立了极小最优的收敛速率。对于二元分类，我们推导了具有铰链损失和逻辑损失的CNN分类器的收敛速度。同时还表明所得到的速率在几种情况下是极小最优的。

    arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
    
[^11]: 使用具有标签感知的神经过程进行实时适应条件监测信号预测

    Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes

    [https://arxiv.org/abs/2403.16377](https://arxiv.org/abs/2403.16377)

    提出了一种神经过程方法，可以在实时条件监测信号预测中实现表示能力和敏捷性的权衡

    

    建立一个快速适应实时条件监测（CM）信号的预测模型对于工程系统/单元至关重要。然而，许多当前方法在表示能力和在线环境中的敏捷性之间存在权衡问题。本文提出了一种基于神经过程的方法，解决了这一权衡问题。它将CM信号中的可用观测编码到表示空间中，然后重建信号的历史和演变以进行预测。

    arXiv:2403.16377v1 Announce Type: new  Abstract: Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for predi
    
[^12]: 使用不变性学习基于动作的表示

    Learning Action-based Representations Using Invariance

    [https://arxiv.org/abs/2403.16369](https://arxiv.org/abs/2403.16369)

    提出了一种新的方法，动作双模拟编码，通过递归不变性约束扩展了单步控制性，学习了一个可以平滑折扣远期元素的多步控制度量

    

    强化学习代理使用高维度观测必须能够在许多外源性干扰中识别相关状态特征。一个能够捕捉可控性的表示通过确定影响代理控制的因素来识别这些状态元素。虽然诸如逆动力学和互信息等方法可以捕捉有限数量的时间步的可控性，但捕获长时间元素仍然是一个具有挑战性的问题。短视的可控性可以捕捉代理即将撞向墙壁的瞬间，但不能在代理还有一定距离之时捕捉墙壁的控制相关性。为解决这个问题，我们提出了动作双模拟编码，这是一种受到双模拟不变量假度量启发的方法，它通过递归不变性约束扩展了单步控制性。通过这种方式，动作双模拟学习了一个平滑折扣远期元素的多步控制度量。

    arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
    
[^13]: 多环境场景中的预测推断

    Predictive Inference in Multi-environment Scenarios

    [https://arxiv.org/abs/2403.16336](https://arxiv.org/abs/2403.16336)

    本研究提出了在多环境预测问题中构建有效置信区间和置信集的方法，并展示了一种新的调整方法以适应问题难度，从而减少预测集大小，这在神经感应和物种分类数据集中的实际表现中得到验证。

    

    我们解决了在跨多个环境的预测问题中构建有效置信区间和置信集的挑战。我们研究了适用于这些问题的两种覆盖类型，扩展了Jackknife和分裂一致方法，展示了如何在这种非传统的层次数据生成场景中获得无分布覆盖。我们的贡献还包括对非实值响应设置的扩展，以及这些一般问题中预测推断的一致性理论。我们展示了一种新的调整方法，以适应问题难度，这适用于具有层次数据的预测推断的现有方法以及我们开发的方法；这通过神经化学感应和物种分类数据集评估了这些方法的实际性能。

    arXiv:2403.16336v1 Announce Type: cross  Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.
    
[^14]: 通过深度多理解集成实现越界检测

    Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble

    [https://arxiv.org/abs/2403.16260](https://arxiv.org/abs/2403.16260)

    通过引入新颖的定性和定量模型集成评估方法，作者揭示了现有集成方法的关键缺陷，提出了提高传统模型集成维度的方法，以克服特征表示中的多样性限制。

    

    最近的研究强调了越界（OOD）特征表示领域规模对模型在OOD检测中效果的重要作用。因此，采用模型集成作为增强这一特征表示领域的突出策略已经成为一种突出的策略，利用预期的模型多样性。然而，我们引入了新颖的定性和定量模型集成评估方法，特别是损失盆/障碍可视化和自耦合指数，揭示了现有集成方法的一个关键缺陷。我们发现这些方法包含可进行仿射变换的权重，表现出有限的可变性，从而未能实现特征表示中所需的多样性。

    arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
    
[^15]: 神经网络中协方差传播的解析解

    An Analytic Solution to Covariance Propagation in Neural Networks

    [https://arxiv.org/abs/2403.16163](https://arxiv.org/abs/2403.16163)

    该论文提出了一种无需样本的矩传播技术，能够准确表征神经网络的输入输出分布，其关键创新在于提供了通过非线性激活函数传递的随机变量协方差的解析解。

    

    神经网络的不确定性量化对于衡量深度学习系统的可靠性和鲁棒性至关重要。然而，这通常涉及昂贵或不准确的采样方法和近似。本文提出了一种无需样本的矩传播技术，通过网络传播均值向量和协方差矩阵，准确表征神经网络的输入输出分布。我们的技术的一个关键优势是为通过非线性激活函数（如Heaviside、ReLU和GELU）传递的随机变量的协方差提供了解析解。通过分析经过训练的神经网络的输入输出分布以及训练贝叶斯神经网络的实验，展示了所提出技术的广泛适用性和优点。

    arXiv:2403.16163v1 Announce Type: cross  Abstract: Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.
    
[^16]: 基于改进的扩散映射的流形正则化分类模型

    Manifold Regularization Classification Model Based On Improved Diffusion Map

    [https://arxiv.org/abs/2403.16059](https://arxiv.org/abs/2403.16059)

    本文提出了基于改进扩散映射的流形正则化分类模型，通过改进标签传播模型，有效克服了原始流形正则化模型在局部区域性能上的限制。

    

    流形正则化模型是一种半监督学习模型，利用数据集的几何结构，包括少量有标签样本和大量无标签样本，生成分类器。然而，原始的流形范数限制了模型性能只局限于局部区域。为了克服这一局限，本文提出了一种改进流形正则化的方法，基于标签传播模型。我们首先增强扩散映射算法的概率转移矩阵，可用于估计Neumann热核，使其能够准确描述流形上的标签传播过程。利用该矩阵，在数据集上建立一个描述不同时间步骤下标签分布的标签传播函数。随后，我们将标签传播函数扩展到整个数据流形。我们证明了扩展的标签传播函数c

    arXiv:2403.16059v1 Announce Type: cross  Abstract: Manifold regularization model is a semi-supervised learning model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers. However, the original manifold norm limits the performance of models to local regions. To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model. We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold. Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps. Subsequently, we extend the label propagation function to the entire data manifold. We prove that the extended label propagation function c
    
[^17]: 从偏序关系中学习有向无环图

    Learning Directed Acyclic Graphs from Partial Orderings

    [https://arxiv.org/abs/2403.16031](https://arxiv.org/abs/2403.16031)

    本文针对当可用部分因果顺序变量时学习DAGs的中间问题，提出了一个通用估计框架，并展示了有效的估计算法。

    

    有向无环图（DAGs）通常用于模拟随机变量之间的因果关系。通常来说，学习DAG结构在计算和统计方面都具有挑战性。此外，在没有额外信息的情况下，边的方向可能无法从观测数据中估计出来。本文考虑了在可用部分因果顺序变量的情况下学习DAGs的中间问题。我们提出了一个利用部分顺序的通用估计框架，并提出了低维和高维问题的有效估计算法。所提出框架的优势通过数值研究进行了说明。

    arXiv:2403.16031v1 Announce Type: cross  Abstract: Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.
    
[^18]: 具有保证私有初始化的近最优差分隐私低秩痕迹回归

    Near-Optimal differentially private low-rank trace regression with guaranteed private initialization

    [https://arxiv.org/abs/2403.15999](https://arxiv.org/abs/2403.15999)

    该论文研究了在痕迹回归模型下以高斯测量矩阵进行差分隐私估计，提出了具有保证私有初始化的近最优算法，引入了一个高效的DP初始化算法和基于Riemannian优化的差分隐私算法，同时讨论了估计结果的非平凡差距。

    

    我们研究了在具有高斯测量矩阵的痕迹回归模型下，对秩为$r$的矩阵$M \in \RR^{d_1\times d_2}$进行差分隐私（DP）估计。在理论上，精确表征了非私有谱初始化的敏感性，并建立了在Schatten-$q$范数下估计$M$的差分隐私约束极小概率下界。在方法论上，本文引入了一个计算效率高的DP初始化算法，其样本大小为$n \geq \wt O (r^2 (d_1\vee d_2))$。在一定的正则条件下，DP初始化落入围绕$M$的局部球内。我们还提出了一种基于黎曼优化的用于估计$M$的差分隐私算法（DP-RGrad），通过DP初始化和样本大小$n \geq \wt O(r (d_1 + d_2))$实现了近最优收敛速度。最后，本文讨论了在差分隐私初始化和样本大小条件下的所估计的$M$之间的非平凡差距。

    arXiv:2403.15999v1 Announce Type: cross  Abstract: We study differentially private (DP) estimation of a rank-$r$ matrix $M \in \RR^{d_1\times d_2}$ under the trace regression model with Gaussian measurement matrices. Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the differential-privacy-constrained minimax lower bound for estimating $M$ under the Schatten-$q$ norm is established. Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a sample size of $n \geq \wt O (r^2 (d_1\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and sample size of $n \geq \wt O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap between the mi
    
[^19]: 使用轨迹抽样的深度高斯协方差网络进行数据高效策略搜索

    Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search

    [https://arxiv.org/abs/2403.15908](https://arxiv.org/abs/2403.15908)

    结合轨迹抽样和深度高斯协方差网络，以提高基于模型的强化学习问题的数据高效性。

    

    概率世界模型通过利用其认识不确定性指导策略，提高了基于模型的强化学习（MBRL）的数据效率，改善了探索性能并获得了新样本。此外，概率方法中的不确定性感知学习流程导致的稳健策略比不考虑不确定性的解决方案对噪声观测更不敏感。我们提出将轨迹抽样和深度高斯协方差网络（DGCN）相结合，以在最优控制环境中实现MBRL问题的数据高效解决方案。我们使用高斯过程、贝叶斯神经网络和DGCN三种不同的概率世界模型，比较了轨迹抽样和基于密度的近似法在不确定性传播方面的效果。我们通过四个不同的知名测试环境提供了经验证据，证明我们的方法提高了其他不确定性传播方法和概率世界模型组合的样本效率。

    arXiv:2403.15908v1 Announce Type: new  Abstract: Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and prob
    
[^20]: 快速统一的路径梯度估计器用于归一化流

    Fast and Unified Path Gradient Estimators for Normalizing Flows

    [https://arxiv.org/abs/2403.15881](https://arxiv.org/abs/2403.15881)

    提出了一种快速路径梯度估计器，显著提高了计算效率，并适用于所有实用的归一化流架构，具有正则化效果并减小了方差。

    

    最近的研究表明，归一化流的路径梯度估计器与变分推断的标准估计器相比具有更低的方差，从而改善了训练。然而，从计算角度看，它们往往昂贵且无法以可扩展的方式应用于最大似然训练，严重阻碍了它们的广泛采用。在这项工作中，我们克服了这些关键限制。具体来说，我们提出了一种快速路径梯度估计器，显著提高了计算效率，并适用于所有实用的归一化流架构。然后，我们证明该估计器也可以应用于最大似然训练，对此具有正则化效果，因为它可以考虑给定目标能量函数的形式。我们凭经验证明其在多个自然科学应用中表现卓越，并减小了方差。

    arXiv:2403.15881v1 Announce Type: new  Abstract: Recent work shows that path gradient estimators for normalizing flows have lower variance compared to standard estimators for variational inference, resulting in improved training. However, they are often prohibitively more expensive from a computational point of view and cannot be applied to maximum likelihood training in a scalable manner, which severely hinders their widespread adoption. In this work, we overcome these crucial limitations. Specifically, we propose a fast path gradient estimator which improves computational efficiency significantly and works for all normalizing flow architectures of practical relevance. We then show that this estimator can also be applied to maximum likelihood training for which it has a regularizing effect as it can take the form of a given target energy function into account. We empirically establish its superior performance and reduced variance for several natural sciences applications.
    
[^21]: 集成路径稳定选择

    Integrated path stability selection

    [https://arxiv.org/abs/2403.15877](https://arxiv.org/abs/2403.15877)

    该论文提出了一种基于集成稳定路径的新稳定选择方法，能够在实践中提高特征选择的灵敏度并更好地校准目标假阳性数量。

    

    稳定选择是一种广泛用于改善特征选择算法性能的方法。然而，已发现稳定选择过于保守，导致灵敏度较低。此外，对期望的假阳性数量的理论界限E(FP)相对较松，难以知道实践中会有多少假阳性。在本文中，我们提出一种基于集成稳定路径而非最大化稳定路径的新方法。这产生了对E(FP)更紧密的界限，导致实践中具有更高灵敏度的特征选择标准，并且在与目标E(FP)匹配方面更好地校准。我们提出的方法与原始稳定选择算法需要相同数量的计算，且仅需要用户指定一个输入参数，即E(FP)的目标值。我们提供了性能的理论界限。

    arXiv:2403.15877v1 Announce Type: cross  Abstract: Stability selection is a widely used method for improving the performance of feature selection algorithms. However, stability selection has been found to be highly conservative, resulting in low sensitivity. Further, the theoretical bound on the expected number of false positives, E(FP), is relatively loose, making it difficult to know how many false positives to expect in practice. In this paper, we introduce a novel method for stability selection based on integrating the stability paths rather than maximizing over them. This yields a tighter bound on E(FP), resulting in a feature selection criterion that has higher sensitivity in practice and is better calibrated in terms of matching the target E(FP). Our proposed method requires the same amount of computation as the original stability selection algorithm, and only requires the user to specify one input parameter, a target value for E(FP). We provide theoretical bounds on performance
    
[^22]: 计算句子级度量预测人类句子理解

    Computational Sentence-level Metrics Predicting Human Sentence Comprehension

    [https://arxiv.org/abs/2403.15822](https://arxiv.org/abs/2403.15822)

    本研究引入了创新方法，使用多语言大型语言模型计算句子级度量，并证明这些度量能够高度准确地预测人类句子阅读速度，为未来整合LLMs和认知科学研究提供了有前景的方向。

    

    计算心理语言学的研究大多集中在单词处理上。本研究引入了创新方法，使用多语言大型语言模型计算句子级度量。开发的度量包括句子意外性和句子相关性，然后经过测试和比较以验证它们是否可以预测人类如何跨语言整体理解句子。这些度量提供了重要的可解释性，并在预测人类句子阅读速度方面取得了很高的准确性。我们的结果表明，这些计算的句子级度量在预测和阐明读者在理解整体句子时遇到的处理困难方面异常有效，可跨越多种语言。它们出色的性能和泛化能力为未来在整合LLMs和认知科学方面的研究提供了一个有前途的途径。

    arXiv:2403.15822v1 Announce Type: new  Abstract: The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.
    
[^23]: ISS登机：不平衡的自监督：发现用于混合表格数据集的缩放自动编码器

    Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets

    [https://arxiv.org/abs/2403.15790](https://arxiv.org/abs/2403.15790)

    这里是中文总结出的一句话要点: 本文针对表格数据领域中自监督学习中的数据不平衡挑战，提出了一种用于混合表格数据集的缩放自动编码器，填补了研究中的缺口。

    

    arXiv:2403.15790v1公告类型：新摘要：自监督学习领域，特别是在表格数据领域，不太受到广泛关注。现有研究主要集中在图像数据集上。本文旨在填补这一空白，通过探讨自监督学习中数据不平衡在表格数据领域中所带来的具体挑战，重点放在自动编码器上。自动编码器广泛用于学习和构建数据集的新表示，特别是用于降维。它们也经常用于生成模型学习，如变分自动编码器中所见。在处理混合表格数据时，定性变量通常使用独热编码器与标准损失函数（均方误差或交叉熵）进行编码。在本文中，我们分析了这种方法的缺点，特别是当分类变量不平衡时。我们提出了一个新的度量以平衡学习：一个多监督的 Ba

    arXiv:2403.15790v1 Announce Type: new  Abstract: The field of imbalanced self-supervised learning, especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in self-supervised learning in the domain of tabular data, with a primary focus on autoencoders. Autoencoders are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in variational autoencoders. When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Ba
    
[^24]: 通过多样化功能表示的集成学习：功能投票分类器

    Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier

    [https://arxiv.org/abs/2403.15778](https://arxiv.org/abs/2403.15778)

    这份论文着重探讨了功能数据的集成学习，并展示了如何利用不同的功能数据表示训练集成成员，以及如何通过多数投票组合基模型预测。

    

    许多传统的统计和机器学习方法在直接应用于高维时序观测时面临挑战。在最近几十年中，功能数据分析(FDA)作为一种模拟和分析天然为时间域内的函数的数据的框架已经广泛流行起来。虽然在FDA文献中对监督分类进行了广泛探讨，但功能分类器的集成学习却最近才成为一个备受关注的话题。因此，后者从各种统计角度呈现出未经探索的方面和挑战。本文的焦点在于功能数据的集成学习，并旨在展示如何利用不同的功能数据表示来训练集成成员以及如何通过多数投票来组合基模型预测。所谓的功能投票Cla

    arXiv:2403.15778v1 Announce Type: cross  Abstract: Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although supervised classification has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Cla
    
[^25]: 可识别的潜在神经因果模型

    Identifiable Latent Neural Causal Models

    [https://arxiv.org/abs/2403.15711](https://arxiv.org/abs/2403.15711)

    该研究确定了在潜在附加噪声模型背景下导致可识别性的分布变化类型的充分且必要条件，同时提出了当只有部分分布变化满足条件时的部分可识别性结果。

    

    因果表征学习旨在从低级观测数据中揭示潜在的高级因果表征。它特别擅长预测在未见分布变化下，因为这些变化通常可以解释为干预的后果。因此，利用{已见}分布变化成为帮助识别因果表征的自然策略，进而有助于预测以前{未见}分布的情况。确定这些分布变化的类型（或条件）对于因果表征的可识别性至关重要。该工作建立了在潜在附加噪声模型背景下，表征导致可识别性的分布变化类型的充分且必要条件。此外，我们提出了当只有部分分布变化满足条件时的部分可识别性结果。

    arXiv:2403.15711v1 Announce Type: new  Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findin
    
[^26]: 地域性和权重共享在基于图像的任务中的作用：CNN、LCN和FCN之间的样本复杂性分离

    Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs

    [https://arxiv.org/abs/2403.15707](https://arxiv.org/abs/2403.15707)

    介绍了新的Dynamic Signal Distribution (DSD)分类任务，模拟图像由$k$个维度为$d$的补丁组成，以解决CNNs相对于LCNs和FCNs的统计优势问题

    

    视觉任务的特点是地域性和平移不变性。卷积神经网络（CNNs）在这些任务上表现出色，这在很大程度上归因于其架构中固有的地域性和权重共享的归纳偏差。现有的试图量化这些偏差在CNNs上相对于局部连接的卷积神经网络（LCNs）和全连接神经网络（FCNs）的统计优势的尝试可以归为以下几类：要么它们忽视优化器，仅提供具有统一收敛上界但没有分隔下界的统计收敛性，要么考虑到不真实地反映现实世界视觉任务中的地域性和平移不变性的简单任务。为了解决这些不足，我们介绍了动态信号分布（DSD）分类任务，它将图像建模为包含$k$个尺寸为$d$的补丁，标签是de

    arXiv:2403.15707v1 Announce Type: cross  Abstract: Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is de
    
[^27]: 依从在线模型聚合

    Conformal online model aggregation

    [https://arxiv.org/abs/2403.15527](https://arxiv.org/abs/2403.15527)

    该论文提出了一种基于投票的在线依从模型聚合方法，可以根据过去表现调整模型权重。

    

    依从预测为机器学习模型提供了一种合理的不确定性量化概念，而不需要做出强烈的分布假设。它适用于任何黑盒预测模型，并将点预测转换成具有预定义边际覆盖保证的集预测。然而，依从预测只在事先确定底层机器学习模型的情况下起作用。依从预测中相对较少涉及的问题是模型选择和/或聚合：对于给定的问题，应该如何依从化众多预测方法（随机森林、神经网络、正则化线性模型等）？本文提出了一种新的依从模型聚合方法，用于在线设置，该方法基于将来自多个算法的预测集进行投票，其中根据过去表现调整模型上的权重。

    arXiv:2403.15527v1 Announce Type: cross  Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.
    
[^28]: 从核方法的角度对两层神经网络进行平均场分析

    Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective

    [https://arxiv.org/abs/2403.14917](https://arxiv.org/abs/2403.14917)

    本文通过核方法的视角研究了两层神经网络在平均场极限下的特征学习能力，展示了它们比任何核方法更有效地学习多个再现核希尔伯特空间的并集，并且神经网络会获得与目标函数对齐的数据相关核。

    

    在本文中，我们通过核方法的视角研究了两层神经网络在平均场极限下的特征学习能力。为了聚焦于第一层诱导的核的动态，我们利用了两个时间尺度的极限，其中第二层比第一层移动得快得多。在这个极限下，学习问题被简化为在内在核上的最小化问题。然后，我们展示了平均场 Langevin 动力学的全局收敛性，并推导了时间和粒子离散化误差。我们还证明了两层神经网络可以比任何核方法更有效地学习多个再现核希尔伯特空间的并集，并且神经网络会获得与目标函数对齐的数据相关核。此外，我们还开发了一个收敛到全局最优的标签噪声过程，并展示自由度出现作为一种隐式正则化。

    arXiv:2403.14917v1 Announce Type: new  Abstract: In this paper, we study the feature learning ability of two-layer neural networks in the mean-field regime through the lens of kernel methods. To focus on the dynamics of the kernel induced by the first layer, we utilize a two-timescale limit, where the second layer moves much faster than the first layer. In this limit, the learning problem is reduced to the minimization problem over the intrinsic kernel. Then, we show the global convergence of the mean-field Langevin dynamics and derive time and particle discretization error. We also demonstrate that two-layer neural networks can learn a union of multiple reproducing kernel Hilbert spaces more efficiently than any kernel methods, and neural networks acquire data-dependent kernel which aligns with the target function. In addition, we develop a label noise procedure, which converges to the global optimum and show that the degrees of freedom appears as an implicit regularization.
    
[^29]: 使用高斯过程从偏好和选择中学习的教程

    A tutorial on learning from preferences and choices with Gaussian Processes

    [https://arxiv.org/abs/2403.11782](https://arxiv.org/abs/2403.11782)

    提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。

    

    偏好建模位于经济学、决策理论、机器学习和统计学的交叉点。通过理解个体的偏好及其选择方式，我们可以构建更接近他们期望的产品，为跨领域的更高效、个性化应用铺平道路。此教程的目标是提供一个连贯、全面的偏好学习框架，使用高斯过程演示如何将理性原则（来自经济学和决策理论）无缝地纳入学习过程中。通过合适地定制似然函数，这一框架使得能够构建涵盖随机效用模型、辨识限制和对象和标签偏好的多重冲突效用情景的偏好学习模型。

    arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
    
[^30]: 通过基于区域稳定性的多元高斯逼近改进随机森林

    Multivariate Gaussian Approximation for Random Forest via Region-based Stabilization

    [https://arxiv.org/abs/2403.09960](https://arxiv.org/abs/2403.09960)

    该论文通过基于区域稳定性的方法，推导出了随机森林预测的高斯逼近界限，并建立了适用于各种相关统计问题的概率结果。

    

    我们在给定由泊松过程产生的一组训练点的情况下，推导了随机森林预测的高斯逼近界限，假设数据生成过程存在相当温和的正则性假设。我们的方法基于一个关键观察：随机森林的预测满足一定的称为基于区域稳定性的几何属性。在为随机森林开发结果的过程中，我们还为基于区域稳定的泊松过程的一般泛函建立了一个概率结果，这可能是独立感兴趣的。这一普遍结果利用了Malliavin-Stein方法，并且可能适用于各种相关的统计问题。

    arXiv:2403.09960v1 Announce Type: cross  Abstract: We derive Gaussian approximation bounds for random forest predictions based on a set of training points given by a Poisson process, under fairly mild regularity assumptions on the data generating process. Our approach is based on the key observation that the random forest predictions satisfy a certain geometric property called region-based stabilization. In the process of developing our results for the random forest, we also establish a probabilistic result, which might be of independent interest, on multivariate Gaussian approximation bounds for general functionals of Poisson process that are region-based stabilizing. This general result makes use of the Malliavin-Stein method, and is potentially applicable to various related statistical problems.
    
[^31]: 环境-固有维度差异对对抗脆弱性的影响

    Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability

    [https://arxiv.org/abs/2403.03967](https://arxiv.org/abs/2403.03967)

    通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。

    

    论文介绍了对机器学习模型的对抗攻击存在且对人类来说几乎无法察觉这一事实，在理论上仍然相当神秘。文章引入了两种对抗攻击的概念：自然或在流形上的攻击，这些攻击是可以被人类/神谕感知到的；非自然或脱离流形的攻击，这些攻击则无法被感知到。文章认为脱离流形的攻击存在是数据固有维度与环境维度之间的差异的必然结果。对于2层ReLU网络，我们证明了即使维度差异不影响从观测数据空间中抽取样本的泛化性能，它仍会使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。我们的主要结果提供了在/脱离流形攻击的$\ell_2,\ell_{\infty}$攻击强度与维度差异之间明确的关系。

    arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
    
[^32]: 焦点置信: 带有选择条件覆盖的整体预测

    Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage

    [https://arxiv.org/abs/2403.03868](https://arxiv.org/abs/2403.03868)

    该论文提出了一种构建具有有限样本精确覆盖的预测集的通用框架，可以解决在数据驱动情境中由于选择偏差导致的边缘有效预测区间误导问题。

    

    整体预测建立在边缘有效的预测区间上，该区间以某种规定的概率覆盖了随机抽取的新测试点的未知结果。在实践中，常见情况是，在看到测试单元后，从业者以数据驱动的方式决定关注哪些测试单元，并希望量化焦点单元的不确定性。在这种情况下，对于这些焦点单元的边缘有效预测区间可能会因选择偏差而具有误导性。本文提出了一个构建具有有限样本精确覆盖的预测集的通用框架，该覆盖是有条件于所选单元的。其一般形式适用于任意选择规则，并将Mondrian整体预测推广到多个测试单元和非等变分类器。然后，我们为多个现实的选择规则计算了适用于我们框架的计算效率实现，包括top-K选择、优化等。

    arXiv:2403.03868v1 Announce Type: cross  Abstract: Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability. In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s). In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected. Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization
    
[^33]: 使用来自非结构化数据生成的变量进行回归的推断

    Inference for Regression with Variables Generated from Unstructured Data

    [https://arxiv.org/abs/2402.15585](https://arxiv.org/abs/2402.15585)

    提出了一种使用联合上游和下游模型进行有效推断的一步策略，显著减少了偏误，在CEO时间利用数据的应用中产生了重要效果，适合应用研究人员。

    

    分析非结构化数据的主要策略包括两个步骤。首先，使用上游信息检索模型估计感兴趣的潜在经济变量。其次，将估计值视为下游计量经济模型中的“数据”。我们建立了理论论点，解释为什么在实证合理的设置中，这种两步策略会导致偏误的推断。更具建设性的是，我们提出了一个有效推断的一步策略，该策略同时使用上游和下游模型。在模拟中，这一步策略(i) 显著减少了偏误；(ii) 在使用CEO时间利用数据的主要应用中产生了定量重要的效果；(iii) 可以很容易地被应用研究人员采用。

    arXiv:2402.15585v1 Announce Type: new  Abstract: The leading strategy for analyzing unstructured data uses two steps. First, latent variables of economic interest are estimated with an upstream information retrieval model. Second, the estimates are treated as "data" in a downstream econometric model. We establish theoretical arguments for why this two-step strategy leads to biased inference in empirically plausible settings. More constructively, we propose a one-step strategy for valid inference that uses the upstream and downstream models jointly. The one-step strategy (i) substantially reduces bias in simulations; (ii) has quantitatively important effects in a leading application using CEO time-use data; and (iii) can be readily adapted by applied researchers.
    
[^34]: 在李群上的随机Hessian拟合

    Stochastic Hessian Fitting on Lie Group

    [https://arxiv.org/abs/2402.11858](https://arxiv.org/abs/2402.11858)

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。

    

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆。使用了一个Hessian拟合准则，可用于推导大部分常用方法，如BFGS、高斯牛顿、AdaGrad等。我们的研究揭示了不同Hessian拟合方法的不同收敛速率，例如，在欧几里德空间中的梯度下降的次线性速率和对称正定（SPL）矩阵和某些李群上的梯度下降的线性速率。在特定且足够一般的李群上的Hessian拟合问题在轻微条件下被证明是强凸的。为了确认我们的分析，这些方法在不同设置下进行了测试，如有噪声的Hessian-向量乘积、时变的Hessians和低精度算术。这些发现对依赖于随机二阶优化的方法是有用的。

    arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
    
[^35]: 对Koopman模态分解进行特征化处理

    Featurizing Koopman Mode Decomposition

    [https://arxiv.org/abs/2312.09146](https://arxiv.org/abs/2312.09146)

    本文提出了一种命名为FKMD的先进KMD技术，通过时间嵌入和马氏距离缩放，可以增强对高维动力系统的分析和预测，特别适用于特征未知的情况，并在丙氨酸二肽数据降维和分析Lorenz吸引子和癌症研究中细胞信号问题方面取得了显著改进。

    

    本文介绍了一种先进的Koopman模态分解（KMD）技术：命名为特征化Koopman模态分解（FKMD），该技术利用时间嵌入和马氏距离缩放来增强对高维动力系统的分析和预测。时间嵌入扩展了观测空间，更好地捕捉基础流形结构，而应用于核函数或随机傅里叶特征的马氏距离缩放，则根据系统的动态调整观测值。这有助于在不事先知道良好特征的情况下对KMD进行特征化处理。我们发现，FKMD中的马氏距离缩放可用于对丙氨酸二肽数据进行有效的降维。我们还展示了FKMD如何改善对高维Lorenz吸引子和癌症研究中的细胞信号问题的预测。

    arXiv:2312.09146v3 Announce Type: replace-cross  Abstract: This article introduces an advanced Koopman mode decomposition (KMD) technique -- coined Featurized Koopman Mode Decomposition (FKMD) -- that uses time embedding and Mahalanobis scaling to enhance analysis and prediction of high dimensional dynamical systems. The time embedding expands the observation space to better capture underlying manifold structure, while the Mahalanobis scaling, applied to kernel or random Fourier features, adjusts observations based on the system's dynamics. This aids in featurizing KMD in cases where good features are not a priori known. We find that the Mahalanobis scaling from FKMD can be used for effective dimensionality reduction of alanine dipeptide data. We also show that FKMD improves predictions for a high-dimensional Lorenz attractor and a cell signaling problem from cancer research.
    
[^36]: 使用压缩自动编码器的变分贝叶斯图像恢复

    Variational Bayes image restoration with compressive autoencoders

    [https://arxiv.org/abs/2311.17744](https://arxiv.org/abs/2311.17744)

    使用压缩自动编码器代替最先进的生成模型，提出了一种在图像恢复中的新方法。

    

    逆问题的正则化在计算成像中至关重要。近年来，神经网络学习有效图像表示的能力已被利用来设计强大的数据驱动正则化器。本文首先提出使用压缩自动编码器。这些网络可以被看作具有灵活潜在先验的变分自动编码器，比起最先进的生成模型更小更容易训练。

    arXiv:2311.17744v2 Announce Type: replace-cross  Abstract: Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a
    
[^37]: 信号在大图上的采样的Poincar\'e不等式和一致性结果

    A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs

    [https://arxiv.org/abs/2311.10610](https://arxiv.org/abs/2311.10610)

    该论文介绍了一种针对图极限中图上信号的信号采样理论，证明了Poincar\'e不等式并展示了一致性结果。

    

    大规模图机器学习具有挑战性，因为学习模型的复杂性随着图的大小而增加。对图进行子采样是一种可行的替代方案，但在图上进行采样是非平凡的，因为图是非欧几里得的。现有的图采样技术不仅需要计算大矩阵的谱，而且在图发生变化（例如增长）时需要重复这些计算。本文介绍了一种用于一种图极限--图上的信号采样理论。我们证明了图上信号的Poincar\'e不等式，并展示了满足这一不等式的节点子集的补集是图上信号Paley-Wiener空间的唯一采样集。通过与谱聚类和高斯消元的联系，我们证明了这样的采样集是一致的，即收敛的图序列上的唯一采样集收敛到图极限上的唯一采样集。

    arXiv:2311.10610v2 Announce Type: replace  Abstract: Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit -- the graphon. We prove a Poincar\'e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related
    
[^38]: 从多节点干预中识别线性混合因果表示

    Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions

    [https://arxiv.org/abs/2311.02695](https://arxiv.org/abs/2311.02695)

    本文提出了一种新的方法，可以在一个环境中允许定位多个变量的干预，并在因果表示学习中首次得出了可识别性结果。

    

    推断从低级观察中得出高级因果变量的任务，通常称为因果表示学习，本质上是欠约束的。因此，为了解决这个问题的最近工作集中在导致潜在潜在因果变量可识别性的各种假设上。大量之前的方法考虑在因果模型上不同干预下收集的多环境数据。几乎所有这些工作共同点是对每个环境中只干预一个变量的限制性假设。在这项工作中，我们放宽了这一假设，并为因果表示学习提供了第一个允许在一个环境中通过干预定位多个变量的可识别性结果。我们的方法取决于关于各个环境中干预的覆盖范围和多样性的一般假设，其中也包含

    arXiv:2311.02695v2 Announce Type: replace-cross  Abstract: The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also include
    
[^39]: Transformer对具有无限维输入的序列到序列函数的逼近和估计能力

    Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input

    [https://arxiv.org/abs/2305.18699](https://arxiv.org/abs/2305.18699)

    Transformer网络作为具有无限维输入的序列到序列函数，通过其特征提取能力和参数共享属性，能够避免维度诅咒，实现对目标函数的逼近和估计。

    

    虽然Transformer网络在自然语言处理和计算机视觉等各种应用中取得了巨大成功，但它们的理论方面尚未得到很好理解。本文研究了Transformer作为具有无限维输入的序列到序列函数的逼近和估计能力。尽管输入和输出都是无限维的，我们表明当目标函数具有各向异性平滑性时，Transformer可以通过其特征提取能力和参数共享属性避免维度诅咒。此外，我们表明即使平滑性因输入而异，Transformer仍然可以估计每个输入的特征重要性并动态提取重要特征。然后，我们证明了Transformer实现了与固定平滑性情况下相似的收敛速率。我们的理论结果支持了Transformer在实践中取得的成功。

    arXiv:2305.18699v1 Announce Type: cross  Abstract: Despite the great success of Transformer networks in various applications such as natural language processing and computer vision, their theoretical aspects are not well understood. In this paper, we study the approximation and estimation ability of Transformers as sequence-to-sequence functions with infinite dimensional inputs. Although inputs and outputs are both infinite dimensional, we show that when the target function has anisotropic smoothness, Transformers can avoid the curse of dimensionality due to their feature extraction ability and parameter sharing property. In addition, we show that even if the smoothness changes depending on each input, Transformers can estimate the importance of features for each input and extract important features dynamically. Then, we proved that Transformers achieve similar convergence rate as in the case of the fixed smoothness. Our theoretical results support the practical success of Transformers
    
[^40]: 自伯尔曼分布的成对相互作用自旋系统的自回归神经网络架构

    The autoregressive neural network architecture of the Boltzmann distribution of pairwise interacting spins systems

    [https://arxiv.org/abs/2302.08347](https://arxiv.org/abs/2302.08347)

    该论文提出了将二进制成对相互作用系统的伯尔曼分布精确映射为自回归形式的方法，得到的ARNN架构具有明确定义的物理含义，并且可以通过统计物理技术推导出特定系统的新ARNN。

    

    arXiv:2302.08347v3 公告类型: 替换-跨 Abstract: 生成式自回归神经网络（ARNNs）最近在图像和语言生成任务中展现出杰出的结果，促成了生成模型在科学和商业应用中日益流行。本工作将二进制成对相互作用系统的伯尔曼分布精确映射为自回归形式。结果的ARNN架构具有与哈密顿耦合和外场对应的第一层的权重和偏置，具有诸如残差连接和具有明确定义物理含义的递归架构等广泛使用的结构。此外，其架构的明确表述使得可以利用统计物理技术推导特定系统的新ARNN。作为示例，从两个知名的平均场系统，居里-魏斯和Sherrington-Kirkpatrick模型，导出了新的有效ARNN架构，展示了

    arXiv:2302.08347v3 Announce Type: replace-cross  Abstract: Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated exceptional results in image and language generation tasks, contributing to the growing popularity of generative models in both scientific and commercial applications. This work presents an exact mapping of the Boltzmann distribution of binary pairwise interacting systems into autoregressive form. The resulting ARNN architecture has weights and biases of its first layer corresponding to the Hamiltonian's couplings and external fields, featuring widely used structures such as the residual connections and a recurrent architecture with clear physical meanings. Moreover, its architecture's explicit formulation enables the use of statistical physics techniques to derive new ARNNs for specific systems. As examples, new effective ARNN architectures are derived from two well-known mean-field systems, the Curie-Weiss and Sherrington-Kirkpatrick models, showing 
    
[^41]: 分布鲁棒性界定了泛化错误

    Distributional Robustness Bounds Generalization Errors

    [https://arxiv.org/abs/2212.09962](https://arxiv.org/abs/2212.09962)

    分布鲁棒性界定了泛化错误，Bayesian方法在可能近似正确意义上是分布鲁棒的，同时正则化的经验风险最小化方法也被证明是等价于Bayesian方法的。

    

    Bayesian methods, distributionally robust optimization methods, and regularization methods是值得信赖的机器学习的基石，用于抵抗分布不确定性，比如经验分布与真实基础分布之间的不确定性。本文研究了这三种框架之间的联系，特别地探讨了为何这些框架倾向于具有更小的泛化错误。具体地，首先，我们提出了“分布鲁棒性”的定量定义，提出了“鲁棒性度量”的概念，并形式化了分布鲁棒性优化中的几个哲学概念。其次，我们表明Bayesian方法在可能近似正确意义上是分布鲁棒的；此外，通过构造类似Dirichlet过程的先验于贝叶斯非参数模型中，可以证明任何正则化的经验风险最小化方法等价于

    arXiv:2212.09962v3 Announce Type: replace  Abstract: Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for "distributional robustness", propose the concept of "robustness measure", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equival
    
[^42]: 实体排名的最大似然估计不确定性量化与外生变量

    Uncertainty Quantification of MLE for Entity Ranking with Covariates

    [https://arxiv.org/abs/2212.09961](https://arxiv.org/abs/2212.09961)

    提出了一种新颖的 Covariate-Assisted Ranking Estimation (CARE) 模型，扩展了 Bradley-Terry-Luce (BTL) 模型，通过将协变量信息结合进排名估计中，解决了实体排名问题中的不确定性。

    

    本文关注基于成对比较和额外协变量信息（如所比较项目的属性）的排名问题的统计估计和推断。尽管进行了大量研究，但先前的文献中很少有人在协变量信息存在的更现实情境下研究了这个问题。为了解决这个问题，我们提出了一个新颖的模型，即 Covariate-Assisted Ranking Estimation (CARE) 模型，它通过引入协变量信息扩展了著名的 Bradley-Terry-Luce (BTL) 模型。具体而言，我们假设每个比较项目的潜在分数不是固定的 $\{\theta_i^*\}_{i=1}^n$，而是由 $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$ 给出，其中 $\alpha_i^*$ 和 ${x}_i^\top\beta^*$ 分别代表第 $i$ 个项目的潜在基准分数和协变量分数。我们加入了自然的可识别性条件，并推导了 $\ell_{\infty}$-

    arXiv:2212.09961v2 Announce Type: replace-cross  Abstract: This paper concerns with statistical estimation and inference for the ranking problems based on pairwise comparisons with additional covariate information such as the attributes of the compared items. Despite extensive studies, few prior literatures investigate this problem under the more realistic setting where covariate information exists. To tackle this issue, we propose a novel model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate information. Specifically, instead of assuming every compared item has a fixed latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and ${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th item, respectively. We impose natural identifiability conditions and derive the $\ell_{\infty}$-
    
[^43]: 高维度的在线动作学习：一个保守的观点

    Online Action Learning in High Dimensions: A Conservative Perspective

    [https://arxiv.org/abs/2009.13961](https://arxiv.org/abs/2009.13961)

    该论文将$\epsilon_t$-贪心启发式方法扩展到高维度情境中，采用保守导向策略，实现在实用应用中对新奇性的重视，同时限制了采纳不寻常动作，有效控制了累积遗憾。

    

    顺序学习问题在多个研究领域和实际应用中很常见。在本文中，我们将最流行的学习解决方案之一，$\epsilon_t$-贪心启发式，扩展到考虑保守导向的高维情境中。我们通过将原始规则用于采纳全新动作的时间的一部分，分配给在一组有前途的动作中进行更加专注的搜索来实现这一点。所得规则可能对仍然重视惊喜但限制采纳不寻常动作的实际应用有用。我们发现了对于保守高维度衰减$\epsilon_t$-贪心规则的累积遗憾提供了合理边界的概率很高。

    arXiv:2009.13961v4 Announce Type: replace-cross  Abstract: Sequential learning problems are common in several fields of research and practical applications. Examples include dynamic pricing and assortment, design of auctions and incentives and permeate a large number of sequential treatment experiments. In this paper, we extend one of the most popular learning solutions, the $\epsilon_t$-greedy heuristics, to high-dimensional contexts considering a conservative directive. We do this by allocating part of the time the original rule uses to adopt completely new actions to a more focused search in a restrictive set of promising actions. The resulting rule might be useful for practical applications that still values surprises, although at a decreasing rate, while also has restrictions on the adoption of unusual actions. With high probability, we find reasonable bounds for the cumulative regret of a conservative high-dimensional decaying $\epsilon_t$-greedy rule. Also, we provide a lower bo
    
[^44]: Thompson采样用于具有噪音上下文的随机赌臂问题的信息论性后悔分析

    Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis. (arXiv:2401.11565v1 [cs.LG])

    [http://arxiv.org/abs/2401.11565](http://arxiv.org/abs/2401.11565)

    本文研究了具有噪音上下文的随机赌臂问题，并提出了一种Thompson采样算法，通过贝叶斯框架进行分析，证明了算法的贝叶斯后悔，并扩展了问题到延迟观察真实上下文的情况，并实证了算法的性能。

    

    我们研究了一种随机上下文线性赌臂问题，其中代理通过一个未知噪声参数的噪声信道观察到真实上下文的噪声，我们的目标是设计一个动作策略，可以近似于具有奖励模型、噪声参数和从观察到的噪声上下文中真实上下文的预测分布的oracle的动作策略。在贝叶斯框架下，我们引入了一种针对具有高斯上下文噪声的高斯赌臂的Thompson采样算法。采用信息论分析，我们证明了我们的算法相对于oracle的动作策略的贝叶斯后悔。我们还将这个问题扩展到了代理在接收到奖励后延迟观察到真实上下文的情况，并展示了延迟真实上下文导致更低的贝叶斯后悔。最后，我们通过与基线算法的比较实证地展示了所提出算法的性能。

    We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.
    
[^45]: PhyloGFN: 基于生成流网络的系统发育推断

    PhyloGFN: Phylogenetic inference with generative flow networks. (arXiv:2310.08774v1 [q-bio.PE])

    [http://arxiv.org/abs/2310.08774](http://arxiv.org/abs/2310.08774)

    PhyloGFN是一种基于生成流网络的系统发育推断方法，通过采样复杂的组合结构，能够产生多样且高质量的进化假设，并在边缘似然估计方面具有竞争力。

    

    系统发育学是计算生物学的一个分支，研究生物实体之间的进化关系。尽管有着悠久的历史和众多应用，但从序列数据推断系统发育树仍然具有挑战性：树空间的高复杂性对当前的组合和概率技术构成了重要障碍。在本文中，我们采用生成流网络（GFlowNets）的框架来解决系统发育学中的两个核心问题：基于最简原则的和贝叶斯的系统发育推断。由于GFlowNets适用于采样复杂的组合结构，它们是探索和采样树拓扑和进化距离的多模态后验分布的自然选择。我们证明了我们的摊还后验采样器PhyloGFN在真实基准数据集上产生多样且高质量的进化假设。PhyloGFN在边缘似然估计方面与之前的工作相比具有竞争力。

    Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimat
    
[^46]: 本地搜索GFlowNets

    Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])

    [http://arxiv.org/abs/2310.02710](http://arxiv.org/abs/2310.02710)

    本文提出使用局部搜索训练GFlowNets，通过破坏和重构的方式探索局部邻域，分别由反向和正向策略引导，使得样本偏向高奖励解决方案。

    

    生成流网络(GFlowNets)是一种学习与奖励成比例的离散对象分布的摊还采样方法。GFlowNets具有生成多样样本的显著能力，但由于广泛样本空间上的过度探索，有时难以一致地生成高奖励的样本。本文提出使用局部搜索训练GFlowNets，通过破坏和重构的方式探索局部邻域，分别由反向和正向策略引导。这使得样本偏向高奖励解决方案，而传统的GFlowNet解决方案生成方案则使用正向策略从头生成解决方案。大量实验证明在几个生化任务中取得了显著的性能改进。

    Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train GFlowNets with local search which focuses on exploiting high rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via destruction and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.
    
[^47]: TACTiS-2：更好、更快、更简单的多变量时间序列关注联合分布模型

    TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series. (arXiv:2310.01327v1 [cs.LG])

    [http://arxiv.org/abs/2310.01327](http://arxiv.org/abs/2310.01327)

    TACTiS-2是一种改进的多变量时间序列关注联合分布模型，采用了简化的目标函数和线性参数数量，具有更好的训练动态和最先进的性能。

    

    我们引入了一种新的模型用于多变量概率时间序列预测，旨在灵活地处理包括预测、插值和它们的组合等一系列任务。基于联合分布理论，我们提出了一种简化的目标函数，用于最近引入的基于Transformer的关注联合分布模型（TACTiS）。新的目标函数的分布参数数量与变量数量呈线性而非阶乘关系。新的目标函数需要引入一种训练课程，并且需要对原始架构进行必要的改动。我们展示了得到的模型具有显著改善的训练动态，并在多样的真实世界预测任务中实现了最先进的性能，同时保持了先前工作的灵活性，如无缝处理不对齐和采样不均匀的时间序列。

    We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
    
[^48]: 通过协同扩散恢复似然学习基于能量的模型

    Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])

    [http://arxiv.org/abs/2309.05153](http://arxiv.org/abs/2309.05153)

    本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。

    

    在高维数据上使用最大似然估计训练能量基准模型（EBMs）可能具有挑战性且耗时较长。因此，EBMs和其他生成框架（如GANs和扩散模型）之间存在明显的样本质量差距。为了弥补这一差距，受最近通过最大化扩散恢复似然（DRL）来学习EBMs的努力的启发，我们提出了协同扩散恢复似然（CDRL），一种有效的方法来可行地学习和从一系列EBMs中进行采样，这些EBMs定义在越来越嘈杂的数据集版本上，并与每个EBM的初始化模型配对。在每个噪声水平上，初始化模型学习在EBM的采样过程中分摊，而两个模型在协同训练框架内共同估计。初始化模型生成的样本作为起始点，经过EBM的几个采样步骤进行改进。通过改进后的样本，通过最大化恢复似然来优化EBM。

    Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
    
[^49]: 控制误差率的一致性链路预测方法

    Conformal link prediction to control the error rate. (arXiv:2306.14693v1 [stat.ME])

    [http://arxiv.org/abs/2306.14693](http://arxiv.org/abs/2306.14693)

    本研究提出了一种基于一致性推断思想的新方法，可在控制虚假发现率的前提下，识别一组真实的边。

    

    大多数链路预测方法返回图中缺失边的连接概率的估计值。这种输出可用于按可能性大小对缺失边进行排序，但并未直接提供真实和不存在的分类。本研究考虑在控制虚假发现率的前提下，识别一组真实的边的问题。我们提出了一种基于一致性推断文献中高级思想的新方法。图形结构引入了数据中的复杂依赖关系，我们仔细考虑了这一点，因为这使得设置不同于一般的一致性推断设置，那里假定了交换性。在模拟和真实数据中，我们证明了FDR的控制。

    Most link prediction methods return estimates of the connection probability of missing edges in a graph. Such output can be used to rank the missing edges, from most to least likely to be a true edge, but it does not directly provide a classification into true and non-existent. In this work, we consider the problem of identifying a set of true edges with a control of the false discovery rate (FDR). We propose a novel method based on high-level ideas from the literature on conformal inference. The graph structure induces intricate dependence in the data, which we carefully take into account, as this makes the setup different from the usual setup in conformal inference, where exchangeability is assumed. The FDR control is empirically demonstrated for both simulated and real data.
    
[^50]: 基于Samplet基 Pursuit 的核学习方法

    Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])

    [http://arxiv.org/abs/2306.10180](http://arxiv.org/abs/2306.10180)

    本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。

    

    本文考虑了基于l1正则化的Samplet坐标下的核学习问题。在Samplet基的系数上，应用l1正则化项可以强制增加稀疏性。因此，我们称这种方法为Samplet基 Pursuit。Samplet基是波形类型的有符号测度，专门用于散乱数据。它们具有与小波相似的本地化、多分辨率分析和数据压缩性质。可以在Samplet基上稀疏地表示的信号类比单尺度基上能够表示稀疏的信号类别要大得多。特别地，仅用基函数映射的几个特征叠加即可表示的所有信号也可以在Samplet坐标下实现稀疏表示。我们提出了一种高效解决该问题的方法，将软阈值和半光滑牛顿法相结合，并将该方法与快速迭代收缩阈值算法进行了比较。实验结果表明了该方法在稀疏性和预测精度方面的优势。

    We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
    
[^51]: 差分隐私条件独立性检验

    Differentially Private Conditional Independence Testing. (arXiv:2306.06721v1 [stat.ML])

    [http://arxiv.org/abs/2306.06721](http://arxiv.org/abs/2306.06721)

    本文介绍了两个差分隐私条件独立性检验方法，可适用于Z为连续值的一般情况。

    

    条件独立性（CI）检验在统计数据分析中被广泛使用，例如，它们是许多因果图发现算法的构建块。CI测试旨在接受或拒绝$X \perp \!\!\! \perp Y \mid Z$的零假设，其中$X \in \mathbb{R}，Y \in \mathbb{R}，Z \in \mathbb{R}^d$。本文研究了在差分隐私约束下的条件独立性检验。我们设计了基于Shah和Peters（2020）的一般化协方差测量和基于Cand\`es等人的条件随机化检验的两种私人CI测试过程（在模型-X假设下）。我们提供了关于我们测试性能的理论保证，并在实证上验证它们。这些是第一个适用于Z为连续的一般情况的私人CI测试。

    Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests that work for the general case when $Z$ is continuous.
    
[^52]: 针对离线设计生物序列的得分条件生成器的自助增强训练

    Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03111](http://arxiv.org/abs/2306.03111)

    本文提出了一种BootGen算法，使用代理得分函数增强生物序列生成器的训练数据集，并产生多样化的设计，将其应用于优化生物序列，取得了比竞争对手更好的结果。

    

    本文研究了优化生物序列（如蛋白质、DNA和RNA）以最大化仅在离线数据集中评估的黑匣子得分函数的问题。我们提出了一种新颖的解决方案——得分条件生成器的自助增强训练（BootGen）算法。我们的算法重复了一个两阶段过程。在第一阶段，我们的算法使用排名加权法训练生物序列生成器，以提高基于高分数的序列生成的准确性。接下来的阶段涉及到自助增强，通过自动生成的数据并标记代理得分函数，来增强训练数据集。我们的关键思想是将基于得分的生成与代理得分函数对齐，将代理得分函数的知识传递给生成器。训练后，我们聚合来自多个自助增强生成器和代理的样本，产生多样化的设计。大量实验表明，我们的方法在生物序列优化方面胜过竞争对手。

    We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
    
[^53]: 高斯混合块模型中的谱聚类

    Spectral clustering in the Gaussian mixture block model. (arXiv:2305.00979v1 [stat.ML])

    [http://arxiv.org/abs/2305.00979](http://arxiv.org/abs/2305.00979)

    本文首次研究了从高维高斯混合块模型中抽样的图聚类和嵌入问题。

    

    高斯混合块模型是用于模拟现代网络的图分布：对于这样的模型生成一个图，我们将每个顶点 $i$ 与一个从高斯混合中抽样到的潜在特征向量 $u_i \in \mathbb{R}^d$ 相关联，当且仅当特征向量足够相似，即 $\langle u_i,u_j \rangle \ge \tau$ 时，我们才会添加边 $(i,j)$。高斯混合的不同组成部分表示可能具有不同特征分布的不同类型的节点，例如在社交网络中，每个组成部分都表示独特社区的不同属性。这些网络涉及到的自然算法任务有嵌入（恢复潜在的特征向量）和聚类（通过其混合组分将节点分组）。本文开启了对从高维高斯混合块模型抽样的图进行聚类和嵌入研究。

    Gaussian mixture block models are distributions over graphs that strive to model modern networks: to generate a graph from such a model, we associate each vertex $i$ with a latent feature vector $u_i \in \mathbb{R}^d$ sampled from a mixture of Gaussians, and we add edge $(i,j)$ if and only if the feature vectors are sufficiently similar, in that $\langle u_i,u_j \rangle \ge \tau$ for a pre-specified threshold $\tau$. The different components of the Gaussian mixture represent the fact that there may be different types of nodes with different distributions over features -- for example, in a social network each component represents the different attributes of a distinct community. Natural algorithmic tasks associated with these networks are embedding (recovering the latent feature vectors) and clustering (grouping nodes by their mixture component).  In this paper we initiate the study of clustering and embedding graphs sampled from high-dimensional Gaussian mixture block models, where the
    
[^54]: 多项式分类中的稀疏联合偏移

    Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])

    [http://arxiv.org/abs/2303.16971](http://arxiv.org/abs/2303.16971)

    该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。

    

    稀疏联合偏移（SJS）是一种针对数据集整体偏移的可处理模型，可能会导致特征和标签的边际分布以及后验概率和类条件特征分布的变化。在没有标签观测的情况下，为目标数据集拟合SJS可能会产生标签的有效预测和类先验概率的估计。我们在特征集之间传递SJS方面提供了新的结果，提出了一个基于目标分布的类后验概率的条件修正公式，确定性SJS的可辨认性以及SJS和协变量转移之间的关系。此外，我们指出了用于估计SJS特征的算法中的不一致性，因为它们可能会妨碍寻找最优解。

    Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
    
[^55]: 可验证变分蒙特卡罗方法的收敛性

    Provable Convergence of Variational Monte Carlo Methods. (arXiv:2303.10599v1 [stat.ML])

    [http://arxiv.org/abs/2303.10599](http://arxiv.org/abs/2303.10599)

    本文提出了一种对变分蒙特卡罗（VMC）方法收敛性的可验证方法，在假设局部能量是次指数的条件下，使用非平稳马尔可夫链的Bernstein不等式推导出了MCMC估计量的误差界限，证明了VMC具有一阶收敛速率，在某些情况下，收敛速率是最优的。

    

    变分蒙特卡罗（VMC）是一种用于计算量子多体问题基态能量的有前途的方法，并由于机器学习的发展而越来越受到关注。最近的VMC方法以神经网络构建试探波函数，使用马尔可夫链蒙特卡罗（MCMC）采样量子态，并用随机梯度下降（SGD）方法训练神经网络。然而，当SGD与MCMC采样与设计良好的试探波函数交互作用时，VMC的理论收敛性仍然未知。由于MCMC降低了梯度估计的难度，实际上不可避免地存在偏差。此外，局部能量可能是无界的，这使得分析MCMC采样的误差更加困难。因此，我们假设局部能量是次指数的，并使用非平稳马尔可夫链的Bernstein不等式推导出MCMC估计量的误差界限。因此，在温和假设下，VMC被证明具有一阶收敛速率。此外，我们还展示了在某些情况下，收敛速率是最优的。

    The Variational Monte Carlo (VMC) is a promising approach for computing the ground state energy of many-body quantum problems and attracts more and more interests due to the development of machine learning. The recent paradigms in VMC construct neural networks as trial wave functions, sample quantum configurations using Markov chain Monte Carlo (MCMC) and train neural networks with stochastic gradient descent (SGD) method. However, the theoretical convergence of VMC is still unknown when SGD interacts with MCMC sampling given a well-designed trial wave function. Since MCMC reduces the difficulty of estimating gradients, it has inevitable bias in practice. Moreover, the local energy may be unbounded, which makes it harder to analyze the error of MCMC sampling. Therefore, we assume that the local energy is sub-exponential and use the Bernstein inequality for non-stationary Markov chains to derive error bounds of the MCMC estimator. Consequently, VMC is proven to have a first order conver
    
[^56]: 双重精度质量驱动的神经网络用于生成预测区间

    Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06370](http://arxiv.org/abs/2212.06370)

    本文提出了一种双重精度质量驱动的神经网络，可以自动地学习基于回归的神经网络的预测区间，而非只提供传统的目标估计。该方法通过设计一种新颖的损失函数，最小化平均预测区间宽度以及提高覆盖概率来提高PI的质量和精度，且比最先进的方法更加具有计算效率。

    

    在深度学习模型在实际应用中，准确的不确定性量化对于提高其可靠性至关重要。对于回归任务，应该在深度学习模型的确定性预测之外提供预测区间(PIs)。只要Pis足够窄而且捕获了大部分的概率密度，这些Pis就是有用的或"高质量"的。本文提出了一种方法，可以自动地为回归神经网络学习预测区间，除了传统的目标预测之外。具体而言，我们训练两个伴侣神经网络：一个使用一个输出，目标估计，另一个使用两个输出，相应PI的上限和下限的值。我们的主要贡献是为生成PI的网络设计了一种新颖的损失函数，该函数考虑了目标估计网络的输出，并且具有两个优化目标：减小平均预测区间宽度和提高Pis的质量(通过其覆盖概率进行测量)。我们在几个回归数据集上评估了我们的方法，并证明了我们的方法可以产生比最先进的方法更准确且质量更高的预测区间，同时又具有计算效率。

    Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
    
[^57]: 非零梯度的随机逼近的指数集中性分析

    Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient. (arXiv:2208.07243v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.07243](http://arxiv.org/abs/2208.07243)

    本研究分析了非零梯度的随机逼近算法的行为，并证明了指数级的集中性界限，这对于投影随机梯度下降算法的收敛速度有重要意义。

    

    我们分析了随机逼近算法的行为，其中每一步迭代，期望中向目标取得进展。当进展与算法的步长成比例时，我们证明了指数级的集中性界限。这些尾部界限与更常见的随机逼近的渐近正态结果形成对比。我们开发的方法依赖于几何阻尼性证明。这扩展了Hajek（1982）对马尔可夫链的结果到随机逼近算法的领域。对于具有非零梯度的投影随机梯度下降算法，我们的结果可以用来证明$O(1/t)$和线性收敛速度。

    We analyze the behavior of stochastic approximation algorithms where iterates, in expectation, make progress towards an objective at each step. When progress is proportional to the step size of the algorithm, we prove exponential concentration bounds. These tail-bounds contrast asymptotic normality results which are more frequently associated with stochastic approximation. The methods that we develop rely on a geometric ergodicity proof. This extends a result on Markov chains due to Hajek (1982) to the area of stochastic approximation algorithms. For Projected Stochastic Gradient Descent with a non-vanishing gradient, our results can be used to prove $O(1/t)$ and linear convergence rates.
    
[^58]: CrossQ: 用于提高深度强化学习样本效率和简洁性的批归一化方法

    CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.05605](http://arxiv.org/abs/1902.05605)

    CrossQ是一种轻量级算法，通过巧妙运用批归一化和删除目标网络的方式，提高了深度强化学习的样本效率，减少了计算成本，并且实施简单。

    

    在深度强化学习中，样本效率是一个关键问题。最近的算法，如REDQ和DroQ，通过将批次标准化的更新数据（UTD）比率增加到每个环境样本上的20个梯度更新步骤，改善了样本效率。然而，这样做会带来大幅增加的计算成本。为了减少这种计算负担，我们引入了CrossQ：一种轻量级算法，它巧妙地运用批归一化，并去除了目标网络，以在保持低UTD比率为1的同时超越目前的最新样本效率。值得注意的是，CrossQ不依赖于当前方法中使用的高级偏差缩减方案。CrossQ的贡献有三个方面：（1）最先进的样本效率，（2）与REDQ和DroQ相比大幅减少计算成本，（3）实施简单，仅需要在SAC之上添加几行代码。

    Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.
    

