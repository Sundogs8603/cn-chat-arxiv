# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes.](http://arxiv.org/abs/2307.06306) | 本文将随机Polyak步长方法扩展到联邦学习，提出了新的局部自适应和几乎无需调参的FedSPS和FedDecSPS变体。我们证明了当插值条件满足时，FedSPS以线性速度收敛，一般情况下收敛到解的邻域。 |
| [^2] | [Identifiability Guarantees for Causal Disentanglement from Soft Interventions.](http://arxiv.org/abs/2307.06250) | 本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。 |
| [^3] | [SepVAE: a contrastive VAE to separate pathological patterns from healthy ones.](http://arxiv.org/abs/2307.06206) | SepVAE是一种对比VAE方法，可以从健康数据和患者数据中分离出共同的和特定的变化因素。在多个应用中表现出比现有方法更好的性能。 |
| [^4] | [Online Laplace Model Selection Revisited.](http://arxiv.org/abs/2307.06093) | 本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。 |
| [^5] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^6] | [Interpreting deep embeddings for disease progression clustering.](http://arxiv.org/abs/2307.06060) | 本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。 |
| [^7] | [Function-Space Regularization for Deep Bayesian Classification.](http://arxiv.org/abs/2307.06055) | 本研究提出了一种函数空间正则化方法来增加深度贝叶斯分类模型的不确定性量化和对抗性鲁棒性。该方法使用Dirichlet先验在预测空间中进行变分推断，并能与不同模型相结合而不影响模型的架构大小。 |
| [^8] | [Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization.](http://arxiv.org/abs/2307.06048) | 该论文研究了具有非独立同分布设置的在线库存问题，提出了一个在线算法MaxCOSD，并证明了其在考虑了非独立同分布需求和有状态动态的情况下的有效性 |
| [^9] | [balance -- a Python package for balancing biased data samples.](http://arxiv.org/abs/2307.06024) | balance是一个用于分析和调整有偏数据样本的Python软件包，通过评估初始偏差、根据倾向分数产生权重校正数据以及评估拟合权重后的偏差和方差膨胀来提供功能。 |
| [^10] | [Digital tools in occupational health, brakes or levers for building multidisciplinary dynamics?.](http://arxiv.org/abs/2307.05998) | 这项研究总结了职业健康中数字工具的方法和影响，突出刹车和杠杆以及考虑辅助措施的各种可能性。 |
| [^11] | [Outlier detection in regression: conic quadratic formulations.](http://arxiv.org/abs/2307.05975) | 本论文提出了一种在回归中解决异常值检测问题的方法，通过引入二次锥松弛形式而不是使用big-M约束，这种方法比现有的big-M公式快数个数量级。 |
| [^12] | [Newell's theory based feature transformations for spatio-temporal traffic prediction.](http://arxiv.org/abs/2307.05949) | 本文提出了一种基于Newell理论的特征转换方法用于时空交通预测，用于改善模型在不同位置的迁移性问题。 |
| [^13] | [A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models.](http://arxiv.org/abs/2307.05946) | 本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。 |
| [^14] | [Dynamic Prediction using Time-Dependent Cox Survival Neural Network.](http://arxiv.org/abs/2307.05881) | 该论文通过使用time-dependent Cox模型和神经网络，提出了一种动态预测模型来预测进行性眼部疾病年龄相关性黄斑变性（AMD）的进展。通过使用纵向眼底图像作为输入，该模型可以建立一个个体化的风险预测模型，并且在实证研究中表现出良好的效果。 |
| [^15] | [Implicit regularisation in stochastic gradient descent: from single-objective to two-player games.](http://arxiv.org/abs/2307.05789) | 本文研究了隐式正则化在随机梯度下降中的应用，通过向后误差分析构建连续时间流量来量化离散优化器的离散化误差，并提供了一种新的使用BEA的方法。 |
| [^16] | [Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning.](http://arxiv.org/abs/2307.05772) | 这篇论文提出了一种新的随机集合卷积神经网络（RS-CNN）用于分类，通过预测信念函数而不是概率矢量集合，以表示模型的置信度和认识不确定性。基于认识论深度学习方法，该模型能够估计由有限训练集引起的认识不确定性。 |
| [^17] | [Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms.](http://arxiv.org/abs/2307.05750) | 本研究分析了Fermat距离的收敛性质和其在聚类算法中的应用。我们证明了离散采样的Fermat距离在小邻域中收敛于它们的连续模拟，同时也证明了基于离散采样的Fermat距离的离散图拉普拉斯算子收敛于连续算子。 |
| [^18] | [Measure transfer via stochastic slicing and matching.](http://arxiv.org/abs/2307.05705) | 本文研究了通过切片和配准过程定义的测量转移和逼近问题的迭代方案，并对随机切片和配准方案提供了几乎必然收敛的证明。 |
| [^19] | [Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks.](http://arxiv.org/abs/2307.05639) | 本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。 |
| [^20] | [Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework.](http://arxiv.org/abs/2307.05620) | 本文提出了一个名为LS-PIE的框架，用于提高线性潜在空间的解释能力。该框架通过自动化潜在向量的聚类和排序，从而改善了主成分分析、独立成分分析等线性潜变量模型的可解释性。 |
| [^21] | [Compositional Generalization from First Principles.](http://arxiv.org/abs/2307.05596) | 本论文将组合性泛化视为数据生成过程的属性，通过导出对训练分布支持和模型架构的条件要求，实现了组合性泛化。对于机器学习中的组合性泛化问题提供了理论性的研究基础。 |
| [^22] | [Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data.](http://arxiv.org/abs/2307.05592) | 本研究提出了一种基于功能主成分分析和深度神经网络的逆UQ过程，用于时间相关响应的模型输入不确定性量化，并通过功能对齐方法解决了PCT时间序列数据中的温度下降问题。 |
| [^23] | [From Estimation to Sampling for Bayesian Linear Regression with Spike-and-Slab Prior.](http://arxiv.org/abs/2307.05558) | 该论文提出了利用后验收缩性质的高效抽样算法，其中研究了具有优势的高斯Spike-and-Slab的准似然函数，并通过两种算法实现了对稀疏信号的有效推断。 |
| [^24] | [A Bayesian Take on Gaussian Process Networks.](http://arxiv.org/abs/2306.11380) | 该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。 |
| [^25] | [Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret.](http://arxiv.org/abs/2306.03372) | 本文提出了在线黎曼梯度下降算法，用于在在线情况下估计潜在的低秩张量。其中，我们在处理连续或分类变量时提供了灵活的方法，并在在线情况下尝试了两个具体的应用，即在线张量补全和在线二元张量学习。我们还建立了逐个条目的精确错误界限，这是在在线张量补全中首次纳入噪声。我们观察到，在存在噪声的情况下，计算和统计方面存在着令人惊讶的权衡。 |
| [^26] | [Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya.](http://arxiv.org/abs/2305.19779) | 本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。 |
| [^27] | [Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization.](http://arxiv.org/abs/2304.08309) | 本论文研究了在线性化Laplace逼近(LLA)在Bayesian optimization中的应用。虽然LLA在构建贝叶斯神经网络时已被证明具有效性和高效性，但是在序列决策问题中，需要考虑其可能的局限性。 |
| [^28] | [Supervised topological data analysis for MALDI mass spectrometry imaging applications.](http://arxiv.org/abs/2302.13948) | 这项研究提出了一个新的代数拓扑框架，通过从MALDI数据中获得内在信息并转化为反映拓扑持久性的形式，实现了在肺癌亚型分类中的噪音信号区分和数据压缩的功能。 |
| [^29] | [On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process.](http://arxiv.org/abs/2302.13152) | 本文研究了安全限制马尔可夫决策过程，强调了贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立。通过将多目标优化问题表示为新的优化框架，解决了这个问题。 |
| [^30] | [Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds.](http://arxiv.org/abs/2302.02988) | 本文研究了最小化期望简单遗憾的固定预算最优臂识别问题。通过推导最坏情况期望简单遗憾的渐进下界，提出了基于HIR估计的TS-HIR策略，该策略在推荐最优臂时表现出近似最优性。 |
| [^31] | [MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows.](http://arxiv.org/abs/2302.01075) | 本文提出了一个新的GAN生成建模框架MonoFlow，通过Wasserstein梯度流获得理论洞见和算法启示。该框架使用密度比例的单调递增映射重新缩放粒子演化，并通过训练鉴别器获得MonoFlow的向量场，利用相应的向量场进行粒子流的生成。 |
| [^32] | [A Deep Learning Method for Comparing Bayesian Hierarchical Models.](http://arxiv.org/abs/2301.11873) | 这个论文提出了一种深度学习方法，用于比较贝叶斯层次模型。该方法通过支持分摊推断，能够高效地进行模型比较和性能验证。同时，作者还对四个层次证据积累模型进行了比较。 |
| [^33] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | 该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。 |
| [^34] | [A large sample theory for infinitesimal gradient boosting.](http://arxiv.org/abs/2210.00736) | 本研究研究了无穷小梯度提升在大样本极限下的渐近性质，证明了其收敛到一个确定性过程，并探讨了其使得测试误差减小的动力学以及其长时间行为。 |
| [^35] | [Dynamic mean field programming.](http://arxiv.org/abs/2206.05200) | 本文发展了一种动态均场规划方法，用于有限状态和行为的贝叶斯强化学习。通过模拟统计物理中的概念，研究了贝尔曼方程作为一种无序动力学系统，并通过均场方程计算状态行为值的统计信息。 |
| [^36] | [Divide-and-Conquer Fusion.](http://arxiv.org/abs/2110.07265) | 这篇论文提出了一种分而治之的融合方法，在解决分布式“大数据”问题或多方隐私约束时，通过精确蒙特卡罗近似来改善后验分布的质量。 |
| [^37] | [Dynamic Ranking with the BTL Model: A Nearest Neighbor based Rank Centrality Method.](http://arxiv.org/abs/2109.13743) | 本文介绍了一种基于最近邻的排名中心性方法，用于动态排名问题，在动态设置中扩展了经典的BTL模型。 |
| [^38] | [Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation.](http://arxiv.org/abs/2106.10865) | 多类分类中的良性过拟合问题进行了研究，提出了一个简单的确定性条件，当前三种算法在满足条件时会得到插值数据并具有相等准确率的分类器。 |
| [^39] | [Fast Rates for the Regret of Offline Reinforcement Learning.](http://arxiv.org/abs/2102.00479) | 本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。 |
| [^40] | [Predicting Race and Ethnicity From the Sequence of Characters in a Name.](http://arxiv.org/abs/1805.02109) | 通过分析名字中的字符序列，可以预测种族和民族，并应用于竞选资金数据和新闻报道中。 |

# 详细

[^1]: 通过随机Polyak步长的局部自适应联邦学习

    Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])

    [http://arxiv.org/abs/2307.06306](http://arxiv.org/abs/2307.06306)

    本文将随机Polyak步长方法扩展到联邦学习，提出了新的局部自适应和几乎无需调参的FedSPS和FedDecSPS变体。我们证明了当插值条件满足时，FedSPS以线性速度收敛，一般情况下收敛到解的邻域。

    

    最先进的联邦学习算法，如FedAvg，需要精心调整的步长才能达到最佳性能。现有自适应联邦方法提出的改进仅涉及额外的超参数调整，如动量参数，并且仅考虑在服务器聚合轮次中的适应性，而不是局部的。这些方法在许多实际场景下效率低下，因为它们需要过多的超参数调整，并且不能捕捉局部几何信息。本文将最近提出的随机Polyak步长方法扩展到联邦学习环境，并提出了新的局部自适应和几乎无需调参的分布式SPS变体（FedSPS和FedDecSPS）。我们证明当插值条件（过参数化）满足时，FedSPS在强凸和凸设置中以线性速度收敛，一般情况下收敛到解的邻域。

    State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance. The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally. These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information. In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case. We extend our proposed method t
    
[^2]: 从软干预中确保因果分解的可识别性

    Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])

    [http://arxiv.org/abs/2307.06250](http://arxiv.org/abs/2307.06250)

    本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。

    

    因果分解旨在通过潜在变量的相关性揭示数据的表征，其通过因果模型相互关联。如果解释数据的潜在模型是唯一的，那么这种表示是可识别的。本文关注的是当存在不配对的观测和干预数据时的情况，每个干预都会改变一个潜在变量的机制。当因果变量完全观测到时，在诚实性假设下，已经开发出了统计一致的算法来识别因果模型。我们在这里展示，即使存在未观测到的因果变量，在给定一般化的忠诚性概念的情况下仍然可以实现可识别性。我们的结果保证了我们可以恢复潜在的因果模型，预测未见组合的干预效果，在无限数据的极限情况下。我们通过开发一种自编码变分贝叶斯算法和ap来实现我们的因果分解框架。

    Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
    
[^3]: SepVAE: 一种对比VAE用于分离病理模式和健康模式

    SepVAE: a contrastive VAE to separate pathological patterns from healthy ones. (arXiv:2307.06206v1 [cs.CV])

    [http://arxiv.org/abs/2307.06206](http://arxiv.org/abs/2307.06206)

    SepVAE是一种对比VAE方法，可以从健康数据和患者数据中分离出共同的和特定的变化因素。在多个应用中表现出比现有方法更好的性能。

    

    对比分析VAE（CA-VAEs）是一类变分自编码器（VAEs），旨在从背景数据集（BG）（即健康人群）和目标数据集（TG）（即患者）之间分离共同变化因素和仅存在于目标数据集中的因素。为此，这些方法将潜在空间分为一组显著特征（即特定于目标数据集）和一组共同特征（即存在于两个数据集中）。目前，所有模型都未能有效防止潜在空间之间的信息共享，并捕捉所有显著的变化因素。为此，我们引入了两个关键的正则化损失：共同表示和显著表示之间的解缠绕项，以及显著空间中背景和目标样本之间的分类项。我们展示了对三个医学应用和一个自然图像数据集（CelebA）的先前CA-VAEs方法的更好性能。代码和数据集可在GitHub上获取。

    Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a background dataset (BG) (i.e., healthy subjects) and a target dataset (TG) (i.e., patients) from the ones that only exist in the target dataset. To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) and a set of common features (i.e., exist in both datasets). Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all salient factors of variation. To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space. We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA). Code and datasets are available on GitHu
    
[^4]: 在线 Laplace 模型选择的再探讨

    Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])

    [http://arxiv.org/abs/2307.06093](http://arxiv.org/abs/2307.06093)

    本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。

    

    Laplace 近似为神经网络提供了一个封闭形式的模型选择目标。在贝叶斯深度学习领域，将神经网络参数与超参数（如权重衰减强度）一起进行优化的在线变体方法再次引起了人们的关注。然而，这些方法违反了 Laplace 方法的一个关键假设，即近似是围绕损失的模态进行的，这就对它们的合理性提出了质疑。本研究重新推导了在线 Laplace 方法，展示了它们针对 Laplace 证据的一个修正模态的变分上界，从而避免了对平稳性的假设。在线 Laplace 方法及其修正模态的对应点满足两个条件：1. 神经网络参数是一个最大后验概率，满足 Laplace 方法的假设；2. 超参数最大化 Laplace 证据，从而促使在线方法的应用。我们通过使用全批量梯度的在线算法演示了这些最优点在实践中的近似程度。

    The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien
    
[^5]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^6]: 解读疾病进展聚类中的深度嵌入

    Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])

    [http://arxiv.org/abs/2307.06060](http://arxiv.org/abs/2307.06060)

    本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。

    

    我们提出了一种在患者聚类的背景下解读深度嵌入的新方法。我们在来自英国生物库的2型糖尿病参与者数据集上评估我们的方法，并展示出对疾病进展模式的临床意义性见解。

    We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
    
[^7]: 深度贝叶斯分类的函数空间正则化

    Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])

    [http://arxiv.org/abs/2307.06055](http://arxiv.org/abs/2307.06055)

    本研究提出了一种函数空间正则化方法来增加深度贝叶斯分类模型的不确定性量化和对抗性鲁棒性。该方法使用Dirichlet先验在预测空间中进行变分推断，并能与不同模型相结合而不影响模型的架构大小。

    

    贝叶斯深度学习方法假设模型参数为潜在随机变量，并推断后验分布以量化不确定性，增加安全性和可信度，并防止过于自信和不可预测的行为。然而，权重空间先验是特定于模型的，可能难以解释和难以指定。相反，我们在预测空间中应用Dirichlet先验，并执行近似函数空间变分推断。为此，我们将随机神经网络分类器的传统分类预测解释为来自隐式Dirichlet分布的样本。通过调整推断，可以将相同的函数空间先验与不同的模型结合在一起，而不影响模型的架构或大小。我们通过玩具实验说明了这种先验的灵活性和功效，并通过大规模图像分类实验展示了可扩展性、改进的不确定性量化和对抗性鲁棒性。

    Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior. However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify. Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference. To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution. By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size. We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments.
    
[^8]: 在线库存问题：在线凸优化中的非独立同分布设置

    Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization. (arXiv:2307.06048v1 [math.OC])

    [http://arxiv.org/abs/2307.06048](http://arxiv.org/abs/2307.06048)

    该论文研究了具有非独立同分布设置的在线库存问题，提出了一个在线算法MaxCOSD，并证明了其在考虑了非独立同分布需求和有状态动态的情况下的有效性

    

    我们研究多产品库存控制问题，其中经理根据部分历史信息作出顺序补充决策，以最小化其累积损失。我们的动机是考虑到一般需求、损失和动态，超越通常依赖新闻供应商类型损失、固定动态和不现实的独立同分布需求假设的标准模型。我们提出了MaxCOSD，一个在线算法，即使对于非独立同分布需求和有状态动态（包括易腐烂物品）的问题，也具有可证明的保证性能。我们考虑了我们所称的需求过程的非退化性假设，并认为它们是允许学习的必要条件。

    We study multi-product inventory control problems where a manager makes sequential replenishment decisions based on partial historical information in order to minimize its cumulative losses. Our motivation is to consider general demands, losses and dynamics to go beyond standard models which usually rely on newsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand assumptions. We propose MaxCOSD, an online algorithm that has provable guarantees even for problems with non-i.i.d. demands and stateful dynamics, including for instance perishability. We consider what we call non-degeneracy assumptions on the demand process, and argue that they are necessary to allow learning.
    
[^9]: balance -- 一个用于平衡有偏数据样本的Python软件包

    balance -- a Python package for balancing biased data samples. (arXiv:2307.06024v1 [stat.CO])

    [http://arxiv.org/abs/2307.06024](http://arxiv.org/abs/2307.06024)

    balance是一个用于分析和调整有偏数据样本的Python软件包，通过评估初始偏差、根据倾向分数产生权重校正数据以及评估拟合权重后的偏差和方差膨胀来提供功能。

    

    调查是一种重要的研究工具，可以提供关于情感和意见等主观体验的独特测量，这些测量无法通过其他方式进行。然而，由于调查数据是从自愿参与的人群中收集的，直接从中推断出对所关注的人群或者训练机器学习模型，可能会导致错误的估计或者性能下降的模型。本文介绍了一个开源的Python软件包balance，它由Meta开发，提供一个简单的工作流程来分析和校正有偏数据样本，使其相对于所关注的人群具有平衡性。balance工作流程包括三个步骤：了解数据的初始偏差，根据倾向分数为样本中的每个单位产生权重以校正偏差，以及在应用拟合权重后评估最终偏差和方差膨胀。该软件包提供了一个简单的API，可以用于...

    Surveys are an important research tool, providing unique measurements on subjective experiences such as sentiment and opinions that cannot be measured by other means. However, because survey data is collected from a self-selected group of participants, directly inferring insights from it to a population of interest, or training ML models on such data, can lead to erroneous estimates or under-performing models. In this paper we present balance, an open-source Python package by Meta, offering a simple workflow for analyzing and adjusting biased data samples with respect to a population of interest.  The balance workflow includes three steps: understanding the initial bias in the data relative to a target we would like to infer, adjusting the data to correct for the bias by producing weights for each unit in the sample based on propensity scores, and evaluating the final biases and the variance inflation after applying the fitted weights. The package provides a simple API that can be used
    
[^10]: 职业健康中的数字工具：构建多学科动态的刹车还是杠杆？

    Digital tools in occupational health, brakes or levers for building multidisciplinary dynamics?. (arXiv:2307.05998v1 [stat.ML])

    [http://arxiv.org/abs/2307.05998](http://arxiv.org/abs/2307.05998)

    这项研究总结了职业健康中数字工具的方法和影响，突出刹车和杠杆以及考虑辅助措施的各种可能性。

    

    数字平台的出现革命了职业健康，使职业健康服务机构能够获取数据库，为专业人员提供新的行动可能性。然而，在一个持续20年质疑多学科发展的领域中，新工具的到来有时看起来像是一个快速解决方案。这项在数字工具发展方面处于先驱地位的职业健康服务机构的研究旨在总结工具和组织变革对健康专业人员以及技术团队成员的方法和影响。这涉及到突出刹车和杠杆以及考虑各种辅助措施的各种可能性。

    The arrival of digital platforms has revolutionized occupational health by giving the possibility to Occupational Health Services (SPSTI) to acquire databases to offer professionals new possibilities for action. However, in a sector of activity that has been questioning the development of multidisciplinarity for 20 years, the arrival of new tools can sometimes seem to be a quick solution. The study, conducted in a precursor SPSTI in terms of the development of digital tools, aims to take stock of the methods and impacts of instrumental and organizational transformations for health professionals as well as for members of the technical teams of the SPSTI. It is a question of highlighting the brakes and the levers as well as the various possibilities of accompaniment to consider.
    
[^11]: 回归中的异常值检测：锥二次形式

    Outlier detection in regression: conic quadratic formulations. (arXiv:2307.05975v1 [math.OC])

    [http://arxiv.org/abs/2307.05975](http://arxiv.org/abs/2307.05975)

    本论文提出了一种在回归中解决异常值检测问题的方法，通过引入二次锥松弛形式而不是使用big-M约束，这种方法比现有的big-M公式快数个数量级。

    

    在许多应用中，当构建线性回归模型时，考虑到异常值的存在——即受损的输入数据点，是很重要的。这类问题可以通过混合整数优化问题来表达，其中每个问题由二进制变量和连续变量的二次项的乘积给出，形成三次项。现有的文献方法通常依靠使用big-M约束线性化三次项，但在实践中表现出弱放松和性能差的缺点。本工作中，我们推导了不涉及big-M约束的二次锥松弛形式。我们的计算实验表明，对于这个问题，提议的公式比现有文献中的big-M公式快数个数量级。

    In many applications, when building linear regression models, it is important to account for the presence of outliers, i.e., corrupted input data points. Such problems can be formulated as mixed-integer optimization problems involving cubic terms, each given by the product of a binary variable and a quadratic term of the continuous variables. Existing approaches in the literature, typically relying on the linearization of the cubic terms using big-M constraints, suffer from weak relaxation and poor performance in practice. In this work we derive stronger second-order conic relaxations that do not involve big-M constraints. Our computational experiments indicate that the proposed formulations are several orders-of-magnitude faster than existing big-M formulations in the literature for this problem.
    
[^12]: 基于Newell理论的特征转换用于时空交通预测

    Newell's theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v1 [cs.LG])

    [http://arxiv.org/abs/2307.05949](http://arxiv.org/abs/2307.05949)

    本文提出了一种基于Newell理论的特征转换方法用于时空交通预测，用于改善模型在不同位置的迁移性问题。

    

    深度学习模型在时空交通流预测中使用卷积或图卷积过滤器以及循环神经网络来捕捉交通数据的空间和时间依赖关系。这些模型, 如CNN-LSTM, 利用邻近检测站的交通流来预测特定位置的流量。然而, 这些模型在捕捉交通系统的更广泛动态方面具有局限性, 因为它们主要学习特定于检测配置和目标位置交通特征的特征。因此, 当在新的位置缺少用于模型训练的数据时, 这些模型的可迁移性变得具有挑战性。为了解决这个问题, 我们提出了一个基于交通流物理学的特征转换方法用于时空深度学习模型。

    Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data. These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest. However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location. Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training. To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models. This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target loc
    
[^13]: 一种贝叶斯方法用于量化交通预测模型中的不确定性和改善泛化能力

    A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])

    [http://arxiv.org/abs/2307.05946](http://arxiv.org/abs/2307.05946)

    本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。

    

    交通数据预测的深度学习模型可以通过多层架构对复杂函数进行优化建模，但这些方法的一个主要缺点是大多数方法不提供带有不确定性估计的预测结果，而这对于交通运营和控制是必需的。本研究提出了一种贝叶斯循环神经网络框架，通过引入谱归一化到其隐藏层，实现交通预测中的不确定性量化和更高的泛化能力。我们的论文表明，归一化通过控制模型的复杂性并减少对训练数据的过度拟合风险，改善了深度神经网络的泛化性能。

    Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
    
[^14]: 动态预测使用时变Cox生存神经网络

    Dynamic Prediction using Time-Dependent Cox Survival Neural Network. (arXiv:2307.05881v1 [stat.ML])

    [http://arxiv.org/abs/2307.05881](http://arxiv.org/abs/2307.05881)

    该论文通过使用time-dependent Cox模型和神经网络，提出了一种动态预测模型来预测进行性眼部疾病年龄相关性黄斑变性（AMD）的进展。通过使用纵向眼底图像作为输入，该模型可以建立一个个体化的风险预测模型，并且在实证研究中表现出良好的效果。

    

    动态预测的目标是在不断更新的新数据可用时提供个体化的风险预测。受到建立一个针对进行性眼部疾病，年龄相关性黄斑变性（AMD），我们提出了一种基于时变Cox模型的生存神经网络（tdCoxSNN）来预测其在持续时间尺度上的进展，使用纵向眼底图像。tdCoxSNN通过利用神经网络来模拟时变协变量对生存结果的非线性影响扩展了时变Cox模型。此外，通过结合卷积神经网络（CNN），tdCoxSNN可以以纵向原始图像作为输入。我们通过全面的模拟，使用两个时变精度度量标准，Brier分数和动态AUC比较和评估我们提出的方法与联合建模和里程碑方法。我们将所提出的方法应用于两个真实数据集。一个是一个大型AMD数据集。

    The target of dynamic prediction is to provide individualized risk predictions over time which can be updated as new data become available. Motivated by establishing a dynamic prediction model for the progressive eye disease, age-related macular degeneration (AMD), we proposed a time-dependent Cox model-based survival neural network (tdCoxSNN) to predict its progression on a continuous time scale using longitudinal fundus images. tdCoxSNN extends the time-dependent Cox model by utilizing a neural network to model the non-linear effect of the time-dependent covariates on the survival outcome. Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN can take the longitudinal raw images as input. We evaluate and compare our proposed method with joint modeling and landmarking approaches through comprehensive simulations using two time-dependent accuracy metrics, the Brier Score and dynamic AUC. We applied the proposed approach to two real datasets. One is a large AMD
    
[^15]: 隐式正则化在随机梯度下降中的应用：从单目标到双人游戏

    Implicit regularisation in stochastic gradient descent: from single-objective to two-player games. (arXiv:2307.05789v1 [stat.ML])

    [http://arxiv.org/abs/2307.05789](http://arxiv.org/abs/2307.05789)

    本文研究了隐式正则化在随机梯度下降中的应用，通过向后误差分析构建连续时间流量来量化离散优化器的离散化误差，并提供了一种新的使用BEA的方法。

    

    近年来，通过发现常用的基于梯度的优化器的隐式正则化效应，为深度学习优化带来了许多新的见解。理解隐式正则化不仅可以揭示优化动态，还可以用于改善性能和稳定性，涉及到从有监督学习到生成对抗网络等问题领域的两人游戏。通过向后误差分析（BEA）构建的连续时间流量来量化离散优化器的离散化误差是找到隐式正则化效应的一种方法。然而，目前BEA的使用存在限制，因为并不是通过BEA获得的所有连续时间流的向量场都可以写成梯度，这阻碍了构建揭示隐式正则化器的修正损失。在这项工作中，我们提供了一种新的使用BEA的方法，并展示了我们的方法如何用于构建连续时间流量。

    Recent years have seen many insights on deep learning optimisation being brought forward by finding implicit regularisation effects of commonly used gradient-based optimisers. Understanding implicit regularisation can not only shed light on optimisation dynamics, but it can also be used to improve performance and stability across problem domains, from supervised learning to two-player games such as Generative Adversarial Networks. An avenue for finding such implicit regularisation effects has been quantifying the discretisation errors of discrete optimisers via continuous-time flows constructed by backward error analysis (BEA). The current usage of BEA is not without limitations, since not all the vector fields of continuous-time flows obtained using BEA can be written as a gradient, hindering the construction of modified losses revealing implicit regularisers. In this work, we provide a novel approach to use BEA, and show how our approach can be used to construct continuous-time flows
    
[^16]: 随机集合卷积神经网络（RS-CNN）用于认识论深度学习

    Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])

    [http://arxiv.org/abs/2307.05772](http://arxiv.org/abs/2307.05772)

    这篇论文提出了一种新的随机集合卷积神经网络（RS-CNN）用于分类，通过预测信念函数而不是概率矢量集合，以表示模型的置信度和认识不确定性。基于认识论深度学习方法，该模型能够估计由有限训练集引起的认识不确定性。

    

    机器学习越来越多地应用于安全关键领域，对抗攻击的鲁棒性至关重要，错误的预测可能导致潜在的灾难性后果。这突出了学习系统需要能够确定模型对其预测的置信度以及与之相关联的认识不确定性的手段，“知道一个模型不知道”。在本文中，我们提出了一种新颖的用于分类的随机集合卷积神经网络（RS-CNN），其预测信念函数而不是概率矢量集合，使用随机集合的数学，即对样本空间的幂集的分布。基于认识论深度学习方法，随机集模型能够表示机器学习中由有限训练集引起的“认识性”不确定性。我们通过近似预测信念函数相关联的置信集的大小来估计认识不确定性。

    Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
    
[^17]: Fermat距离：度量逼近、谱收敛和聚类算法

    Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms. (arXiv:2307.05750v1 [stat.ML])

    [http://arxiv.org/abs/2307.05750](http://arxiv.org/abs/2307.05750)

    本研究分析了Fermat距离的收敛性质和其在聚类算法中的应用。我们证明了离散采样的Fermat距离在小邻域中收敛于它们的连续模拟，同时也证明了基于离散采样的Fermat距离的离散图拉普拉斯算子收敛于连续算子。

    

    我们分析了Fermat距离的收敛性质，这是一类在具有关联概率测度的Riemann流形上定义的密度驱动度量。Fermat距离可以在离散采样上定义，此时它们是随机的；也可以在连续设置中定义，此时它们由密度扭曲的Riemann度量下的测地线导出。我们证明了基于离散采样的Fermat距离在小邻域中收敛于它们的连续模拟，收敛速率取决于数据的内在维度和Fermat距离中密度加权的参数。这是通过利用新颖的几何和统计论证在渗流理论中允许非均匀密度和曲面域的方法完成的。然后，我们利用这些结果证明了基于离散，采样驱动的Fermat距离的离散图拉普拉斯算子收敛于相应的连续算子。

    We analyze the convergence properties of Fermat distances, a family of density-driven metrics defined on Riemannian manifolds with an associated probability measure. Fermat distances may be defined either on discrete samples from the underlying measure, in which case they are random, or in the continuum setting, in which they are induced by geodesics under a density-distorted Riemannian metric. We prove that discrete, sample-based Fermat distances converge to their continuum analogues in small neighborhoods with a precise rate that depends on the intrinsic dimensionality of the data and the parameter governing the extent of density weighting in Fermat distances. This is done by leveraging novel geometric and statistical arguments in percolation theory that allow for non-uniform densities and curved domains. Our results are then used to prove that discrete graph Laplacians based on discrete, sample-driven Fermat distances converge to corresponding continuum operators. In particular, we 
    
[^18]: 通过随机切片和配准进行测量转移

    Measure transfer via stochastic slicing and matching. (arXiv:2307.05705v1 [math.NA])

    [http://arxiv.org/abs/2307.05705](http://arxiv.org/abs/2307.05705)

    本文研究了通过切片和配准过程定义的测量转移和逼近问题的迭代方案，并对随机切片和配准方案提供了几乎必然收敛的证明。

    

    本论文研究了通过切片和配准过程定义的测量转移和逼近问题的迭代方案。类似于切片Wasserstein距离，这些方案受益于一维最优输运问题的闭式解的可用性和相关计算优势。尽管这些方案已经在数据科学应用中取得了成功，但关于它们的收敛性的结果不太多。本文的主要贡献是对随机切片和配准方案提供了几乎必然收敛的证明。该证明建立在将其解释为Wasserstein空间上的随机梯度下降方案的基础之上。同时还展示了关于逐步图像变形的数值示例。

    This paper studies iterative schemes for measure transfer and approximation problems, which are defined through a slicing-and-matching procedure. Similar to the sliced Wasserstein distance, these schemes benefit from the availability of closed-form solutions for the one-dimensional optimal transport problem and the associated computational advantages. While such schemes have already been successfully utilized in data science applications, not too many results on their convergence are available. The main contribution of this paper is an almost sure convergence proof for stochastic slicing-and-matching schemes. The proof builds on an interpretation as a stochastic gradient descent scheme on the Wasserstein space. Numerical examples on step-wise image morphing are demonstrated as well.
    
[^19]: 使用高斯径向基函数神经网络学习活跃子空间并发现重要特征

    Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])

    [http://arxiv.org/abs/2307.05639](http://arxiv.org/abs/2307.05639)

    本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。

    

    提供一个既能达到强大预测性能，又能被人类解释的模型是机器学习研究中最困难的挑战之一，由于这两个目标的冲突性。为解决这个挑战，我们提出了一种修改的径向基函数神经网络模型，通过为其高斯核添加可学习的精度矩阵。我们展示了训练完成后可以从精度矩阵的谱中提取宝贵的信息。特别是，特征向量解释了模型最敏感的方向，揭示了活跃子空间，并提出了用于监督降维的潜在应用。同时，特征向量凸显了输入和潜在变量之间的绝对变化关系，从而使我们能够基于其对预测的重要性提取输入变量的排序。

    Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
    
[^20]: 潜在空间洞察力和解释增强（LS-PIE）框架

    Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework. (arXiv:2307.05620v1 [stat.ML])

    [http://arxiv.org/abs/2307.05620](http://arxiv.org/abs/2307.05620)

    本文提出了一个名为LS-PIE的框架，用于提高线性潜在空间的解释能力。该框架通过自动化潜在向量的聚类和排序，从而改善了主成分分析、独立成分分析等线性潜变量模型的可解释性。

    

    线性潜变量模型如主成分分析（PCA）、独立成分分析（ICA）、典型相关分析（CCA）和因子分析（FA）通常会识别有序或无序的潜在方向（或载荷）。然后将数据投影到潜在方向上以获得它们的投影表示（或得分）。然而，ICA求解器通常无序地返回独立方向，并且往往将单一源分布在多个方向上作为多个子源，这对于其可用性和可解释性很不利。本文提出了一个通用框架，用于增强线性潜在空间的解释能力。尽管本文介绍的概念与语言无关，但该框架是用Python编写的。该框架自动化了潜在向量的聚类和排序，以增强解释能力。

    Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability.  This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance
    
[^21]: 从第一原理中实现组合性泛化

    Compositional Generalization from First Principles. (arXiv:2307.05596v1 [cs.LG])

    [http://arxiv.org/abs/2307.05596](http://arxiv.org/abs/2307.05596)

    本论文将组合性泛化视为数据生成过程的属性，通过导出对训练分布支持和模型架构的条件要求，实现了组合性泛化。对于机器学习中的组合性泛化问题提供了理论性的研究基础。

    

    利用我们世界的组合性质加快学习和促进泛化是人类感知的一个特点。然而，在机器学习中，即使对于具有明确组合性先验的模型，实现组合性泛化也是一个难以实现的目标。为了更好地理解组合性泛化，我们从底层开始进行探索：受可识别表示学习的启发，我们研究组合性作为数据生成过程的属性，而不是数据本身。这种改进使我们能够导出仅对训练分布的支持和模型架构有轻微条件的要求，这些条件足以实现组合性泛化。我们进一步展示了我们的理论框架如何应用于现实场景，并通过实验证实了我们的发现。我们的结果为组合性泛化的原则性理论研究奠定了基础。

    Leveraging the compositional nature of our world to expedite learning and facilitate generalization is a hallmark of human perception. In machine learning, on the other hand, achieving compositional generalization has proven to be an elusive goal, even for models with explicit compositional priors. To get a better handle on compositional generalization, we here approach it from the bottom up: Inspired by identifiable representation learning, we investigate compositionality as a property of the data-generating process rather than the data itself. This reformulation enables us to derive mild conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization. We further demonstrate how our theoretical framework applies to real-world scenarios and validate our findings empirically. Our results set the stage for a principled theoretical study of compositional generalization.
    
[^22]: 基于功能主成分分析和深度神经网络的基于贝叶斯的逆不确定性量化与瞬态实验数据

    Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data. (arXiv:2307.05592v1 [stat.ML])

    [http://arxiv.org/abs/2307.05592](http://arxiv.org/abs/2307.05592)

    本研究提出了一种基于功能主成分分析和深度神经网络的逆UQ过程，用于时间相关响应的模型输入不确定性量化，并通过功能对齐方法解决了PCT时间序列数据中的温度下降问题。

    

    逆UQ是基于实验数据对模型输入不确定性进行逆向量化的过程。本研究重点发展了一种针对时间相关响应的逆UQ过程，利用功能主成分分析（PCA）和基于深度神经网络（DNN）的代理模型进行降维。该演示基于使用FEBA瞬态实验数据来逆向量化TRACE物理模型参数，测量数据是时间相关的最高包覆温度（PCT）。由于感兴趣的数量（QoI）是时间相关的，对应于无限维响应，因此使用PCA对QoI维度进行降低，同时保留PCT的瞬态特征，以使逆UQ过程更加高效。然而，直接应用传统PCA到PCT时间序列数据中无法准确表示数据，因为在淬灭时刻会出现突然的温度下降。因此，采用了一种功能对齐方法。

    Inverse UQ is the process to inversely quantify the model input uncertainties based on experimental data. This work focuses on developing an inverse UQ process for time-dependent responses, using dimensionality reduction by functional principal component analysis (PCA) and deep neural network (DNN)-based surrogate models. The demonstration is based on the inverse UQ of TRACE physical model parameters using the FEBA transient experimental data. The measurement data is time-dependent peak cladding temperature (PCT). Since the quantity-of-interest (QoI) is time-dependent that corresponds to infinite-dimensional responses, PCA is used to reduce the QoI dimension while preserving the transient profile of the PCT, in order to make the inverse UQ process more efficient. However, conventional PCA applied directly to the PCT time series profiles can hardly represent the data precisely due to the sudden temperature drop at the time of quenching. As a result, a functional alignment method is used
    
[^23]: 从估计到抽样：具有Spike-and-Slab先验的贝叶斯线性回归

    From Estimation to Sampling for Bayesian Linear Regression with Spike-and-Slab Prior. (arXiv:2307.05558v1 [stat.CO])

    [http://arxiv.org/abs/2307.05558](http://arxiv.org/abs/2307.05558)

    该论文提出了利用后验收缩性质的高效抽样算法，其中研究了具有优势的高斯Spike-and-Slab的准似然函数，并通过两种算法实现了对稀疏信号的有效推断。

    

    我们考虑了具有稀疏诱导先验的贝叶斯线性回归，并设计了利用后验收缩性质的高效抽样算法。我们研究了具有高斯Spike-and-Slab（在统计和计算方面都具有优势）的准似然函数，并分析了基于Gibbs抽样和随机定位的两种算法，两者都基于相同（相当自然）的统计假设，还可以对稀疏信号进行有效推断。随机定位抽样器尤其适用于设计不良的数据矩阵。

    We consider Bayesian linear regression with sparsity-inducing prior and design efficient sampling algorithms leveraging posterior contraction properties. A quasi-likelihood with Gaussian spike-and-slab (that is favorable both statistically and computationally) is investigated and two algorithms based on Gibbs sampling and Stochastic Localization are analyzed, both under the same (quite natural) statistical assumptions that also enable valid inference on the sparse planted signal. The benefit of the Stochastic Localization sampler is particularly prominent for data matrix that is not well-designed.
    
[^24]: 高斯过程网络的贝叶斯方法

    A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])

    [http://arxiv.org/abs/2306.11380](http://arxiv.org/abs/2306.11380)

    该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。

    

    高斯过程网络（GPNs）是一类有向图模型，其使用高斯过程作为网络中每个变量给定其父变量的条件期望的先验分布。该模型允许以紧凑但灵活的方式描述连续联合分布，对变量之间的依赖关系仅做最少的参数假设。GPNs的贝叶斯结构学习需要计算网络结构的后验分布，即使在低维情况下，这也是计算上不可行的。本文实现了蒙特卡罗和马尔可夫链蒙特卡罗方法来从网络结构的后验分布中采样。因此，该方法遵循贝叶斯范式，通过边缘似然比较模型，并计算GPN特征的后验概率。模拟研究表明，我们的方法在恢复网络的图形结构方面优于最先进的算法，并提供其后验的准确近似。

    Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
    
[^25]: 在线张量学习：计算和统计权衡，适应性和最优遗憾

    Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])

    [http://arxiv.org/abs/2306.03372](http://arxiv.org/abs/2306.03372)

    本文提出了在线黎曼梯度下降算法，用于在在线情况下估计潜在的低秩张量。其中，我们在处理连续或分类变量时提供了灵活的方法，并在在线情况下尝试了两个具体的应用，即在线张量补全和在线二元张量学习。我们还建立了逐个条目的精确错误界限，这是在在线张量补全中首次纳入噪声。我们观察到，在存在噪声的情况下，计算和统计方面存在着令人惊讶的权衡。

    

    我们研究了一个广义框架，用于在线情况下估计潜在的低秩张量，包括线性和广义线性模型。该框架提供了一种处理连续或分类变量的灵活方法。此外，我们研究了两个具体的应用：在线张量补全和在线二元张量学习。为了应对这些挑战，我们提出了在线黎曼梯度下降算法，在所有应用程序中都可以根据适当的条件线性收敛并恢复低秩组件。此外，我们为在线张量补全建立了精确的逐个条目错误界限。值得注意的是，我们的工作代表了首次尝试在在线低秩张量恢复任务中纳入噪声的努力。有趣的是，我们观察到在存在噪声的情况下，在计算和统计方面存在着令人惊讶的权衡。增加步长可以加快收敛，但会导致更高的统计误差。

    We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
    
[^26]: 利用aggVAE进行深度学习和MCMC以处理行政边界变化：以肯尼亚的疟疾患病率为例

    Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])

    [http://arxiv.org/abs/2305.19779](http://arxiv.org/abs/2305.19779)

    本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。

    

    基于模型的疾病映射是公共卫生和疾病监测中基本的政策信息工具，分层贝叶斯模型是当前最先进的方法。当处理区域数据，如行政区划单位（例如县或省）的聚合数据时，常用的模型依赖于区域单元的相邻结构以考虑空间相关性。疾病监测系统的目标是随时间跟踪疾病结果，但在危机情况下（例如政治变化导致行政边界更改），这将带来挑战。我们提出了一种新颖、实用和易于实施的解决方案，该方案依赖于组合深层生成模型和全贝叶斯推断。我们建立在现有的变分自编码器(VAE) 工作上，并展示我们提出的聚合VAE(aggVAE)体系结构可用于在以县为层级的聚合级别处理数据，以映射肯尼亚的疟疾患病率。我们的模型可以以连续的方式考虑空间相关性，而不依赖于相邻性假设，并且能够处理行政边界的变化。结果表明，相比最先进的模型，我们的模型表现出更好的性能和更准确的疟疾患病率映射。

    Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
    
[^27]: Bayesian Optimization中线性化Laplace的优势和局限性

    Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization. (arXiv:2304.08309v1 [cs.LG])

    [http://arxiv.org/abs/2304.08309](http://arxiv.org/abs/2304.08309)

    本论文研究了在线性化Laplace逼近(LLA)在Bayesian optimization中的应用。虽然LLA在构建贝叶斯神经网络时已被证明具有效性和高效性，但是在序列决策问题中，需要考虑其可能的局限性。

    

    线性化Laplace逼近(LLA)已被证明在构建贝叶斯神经网络时有效且高效。它在理论上具有吸引力，因为它可以被看作是具有高斯过程后验的最大后验预测函数最大化的神经网络的平均函数，并且由经验神经曲面核诱导的协方差函数。然而，尽管已经研究过其在图像分类等大规模任务中的效果，但在诸如Bayesian optimization这样的序列决策问题中尚未对其进行研究，其中高斯过程是默认的代理模型，具有简单的平均函数和核函数，例如径向基函数。在本文中，我们研究了LLA在Bayesian optimization中的有用性和灵活性，并强调其强大的性能。但是，我们还提出了可能出现的一些问题和一个LLA可能存在的问题，即当搜索空间是无界的时候。

    The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-posteriori predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes -- with simple mean functions and kernels such as the radial basis function -- are the de-facto surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded.
    
[^28]: 监督拓扑数据分析在MALDI质谱成像应用中的应用

    Supervised topological data analysis for MALDI mass spectrometry imaging applications. (arXiv:2302.13948v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.13948](http://arxiv.org/abs/2302.13948)

    这项研究提出了一个新的代数拓扑框架，通过从MALDI数据中获得内在信息并转化为反映拓扑持久性的形式，实现了在肺癌亚型分类中的噪音信号区分和数据压缩的功能。

    

    背景：基质辅助激光解吸/电离质谱成像（MALDI MSI）在癌症研究中显示出重要的潜力，特别是在肿瘤分型和亚型分析中。肺癌是肿瘤相关死亡的主要原因，其中最致命的实体是腺癌（ADC）和鳞状细胞癌（SqCC）。区分这两种常见亚型对于治疗决策和患者管理至关重要。结果：我们提出了一个新的代数拓扑框架，通过从MALDI数据中获取内在信息并将其转化为反映拓扑持久性的形式。我们的框架具有两个主要优点。首先，拓扑持久性有助于区分信号和噪音。其次，它压缩了MALDI数据，节省了存储空间，并优化了后续分类任务的计算时间。我们提供了一种高效实现我们拓扑框架的算法，该算法依赖于单一调优参数。

    Background: Matrix-assisted laser desorption/ionization mass spectrometry imaging (MALDI MSI) displays significant potential for applications in cancer research, especially in tumor typing and subtyping. Lung cancer is the primary cause of tumor-related deaths, where the most lethal entities are adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between these two common subtypes is crucial for therapy decisions and successful patient management.  Results: We propose a new algebraic topological framework, which obtains intrinsic information from MALDI data and transforms it to reflect topological persistence. Our framework offers two main advantages. Firstly, topological persistence aids in distinguishing the signal from noise. Secondly, it compresses the MALDI data, saving storage space and optimizes computational time for subsequent classification tasks. We present an algorithm that efficiently implements our topological framework, relying on a single tuning param
    
[^29]: 关于贝尔曼最优性原理和安全限制马尔可夫决策过程的强化学习研究

    On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process. (arXiv:2302.13152v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2302.13152](http://arxiv.org/abs/2302.13152)

    本文研究了安全限制马尔可夫决策过程，强调了贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立。通过将多目标优化问题表示为新的优化框架，解决了这个问题。

    

    本文研究了安全限制马尔可夫决策过程，这是安全强化学习的基本框架。具体而言，我们考虑了一个有限状态和有限动作的受限马尔可夫决策过程，在这个过程中，决策者的目标是在一定的概率保证下到达目标集合，同时避免进入一个不安全的集合。因此，任何控制策略下的马尔可夫链都会表现为多链结构，因为根据定义，存在一个目标集合和一个不安全集合。决策者在导航到目标集合时还必须是最优的（基于一个成本函数）。这导致了一个多目标优化问题。我们强调贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立（正如Haviv的反例所示）。我们通过将上述多目标优化问题表示为一个新的优化框架，解决了这个反例。

    We study optimality for the safety-constrained Markov decision process which is the underlying framework for safe reinforcement learning. Specifically, we consider a constrained Markov decision process (with finite states and finite actions) where the goal of the decision maker is to reach a target set while avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the underlying Markov chain for any control policy will be multichain since by definition there exists a target set and an unsafe set. The decision maker also has to be optimal (with respect to a cost function) while navigating to the target set. This gives rise to a multi-objective optimization problem. We highlight the fact that Bellman's principle of optimality may not hold for constrained Markov decision problems with an underlying multichain structure (as shown by the counterexample due to Haviv. We resolve the counterexample by formulating the aforementioned multi-objective optimization problem as a ze
    
[^30]: 渐进最优的固定预算最优臂识别方法与方差相关界限

    Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds. (arXiv:2302.02988v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02988](http://arxiv.org/abs/2302.02988)

    本文研究了最小化期望简单遗憾的固定预算最优臂识别问题。通过推导最坏情况期望简单遗憾的渐进下界，提出了基于HIR估计的TS-HIR策略，该策略在推荐最优臂时表现出近似最优性。

    

    本文研究了最小化期望简单遗憾的固定预算最优臂识别问题。在自适应实验中，决策者根据过去的观测结果选择多个处理臂之一，并观察所选择的臂的结果。实验结束后，决策者推荐期望结果最高的处理臂。我们基于期望简单遗憾评估决策的好坏，其定义为最优臂的期望结果与推荐臂的期望结果之差。由于固有的不确定性，我们使用极小极大准则评估遗憾。首先，我们推导了最坏情况期望简单遗憾的渐进下界，该下界由潜在结果的方差（主导因素）所确定。基于这些下界，我们提出了Two-Stage (TS)-Hirano-Imbens-Ridder (HIR)策略，在推荐最优臂时利用HIR估计（Hirano et al., 2003）。我们的理论分析表明，TS-HIR策略近似是最优的，并且在一定条件下能达到极小极大准则下的渐进最优性。

    We investigate the problem of fixed-budget best arm identification (BAI) for minimizing expected simple regret. In an adaptive experiment, a decision maker draws one of multiple treatment arms based on past observations and observes the outcome of the drawn arm. After the experiment, the decision maker recommends the treatment arm with the highest expected outcome. We evaluate the decision based on the expected simple regret, which is the difference between the expected outcomes of the best arm and the recommended arm. Due to inherent uncertainty, we evaluate the regret using the minimax criterion. First, we derive asymptotic lower bounds for the worst-case expected simple regret, which are characterized by the variances of potential outcomes (leading factor). Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder (HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in recommending the best arm. Our theoretical analysis shows that the TS-HIR str
    
[^31]: MonoFlow: 从Wasserstein梯度流的角度重新思考Divergence GANs

    MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01075](http://arxiv.org/abs/2302.01075)

    本文提出了一个新的GAN生成建模框架MonoFlow，通过Wasserstein梯度流获得理论洞见和算法启示。该框架使用密度比例的单调递增映射重新缩放粒子演化，并通过训练鉴别器获得MonoFlow的向量场，利用相应的向量场进行粒子流的生成。

    

    传统上，生成对抗网络（GANs）的对抗训练是通过判别器来估计离散度，生成器学习最小化这个离散度。我们认为，尽管许多GANs变体都是按照这个范例开发的，但当前GANs的理论理解和实际算法是不一致的。在本文中，通过利用展示了样本空间内粒子演化的Wasserstein梯度流来获得GANs的理论洞见和算法启示，我们介绍了一个统一的生成建模框架MonoFlow：粒子演化通过密度比例的单调递增映射进行重新缩放。在我们的框架下，对抗性训练可以被视为一个过程，首先通过训练鉴别器获得MonoFlow的向量场，然后生成器学习由相应向量场所定义的粒子流。

    The conventional understanding of adversarial training in generative adversarial networks (GANs) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of GANs were developed following this paradigm, the current theoretical understanding of GANs and their practical algorithms are inconsistent. In this paper, we leverage Wasserstein gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of GANs. We introduce a unified generative modeling framework - MonoFlow: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining MonoFlow's vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We al
    
[^32]: 比较贝叶斯层次模型的深度学习方法

    A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11873](http://arxiv.org/abs/2301.11873)

    这个论文提出了一种深度学习方法，用于比较贝叶斯层次模型。该方法通过支持分摊推断，能够高效地进行模型比较和性能验证。同时，作者还对四个层次证据积累模型进行了比较。

    

    贝叶斯模型比较（BMC）提供了一种基于原则的方法来评估竞争计算模型的相对优势，并将不确定性传播到模型选择决策中。然而，由于高维嵌套参数结构，BMC在常见的层次模型中常常难以计算。为了解决这个难题，我们提出了一种深度学习方法，用于对任何可实例化为概率程序的层次模型集进行BMC。由于我们的方法支持分摊推断，它可以在任何实际数据应用之前，对后验模型概率进行高效的重新估计和快速性能验证。在一系列广泛的验证研究中，我们对比了我们的方法与最先进的桥式抽样方法的性能，并展示了在所有BMC设置中出色的分摊推断能力。然后，我们展示了我们的方法，通过比较先前被认为是四个层次证据积累模型。

    Bayesian model comparison (BMC) offers a principled approach for assessing the relative merits of competing computational models and propagating uncertainty into model selection decisions. However, BMC is often intractable for the popular class of hierarchical models due to their high-dimensional nested parameter structure. To address this intractability, we propose a deep learning method for performing BMC on any set of hierarchical models which can be instantiated as probabilistic programs. Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application. In a series of extensive validation studies, we benchmark the performance of our method against the state-of-the-art bridge sampling method and demonstrate excellent amortized inference across all BMC settings. We then showcase our method by comparing four hierarchical evidence accumulation models that have previously b
    
[^33]: 通过冗余性实现稀疏性：用SGD求解$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    我们提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法。我们的提议是$L_1$惩罚等价于带有权重衰减的可微重参数化的直接推广。我们证明了所提出的方法，即\textit{spred}，是$L_1$的精确求解器，并且对于通用的非凸函数，重参数化技巧是完全“良性”的。在实践中，我们展示了该方法的实用性，包括(1)训练稀疏神经网络以执行基因选择任务，其中涉及在非常高维空间中找到相关特征，以及(2)神经网络压缩任务，先前尝试应用$L_1$惩罚的方法均未成功。从概念上讲，我们的结果弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^34]: 无穷小梯度提升的大样本理论

    A large sample theory for infinitesimal gradient boosting. (arXiv:2210.00736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.00736](http://arxiv.org/abs/2210.00736)

    本研究研究了无穷小梯度提升在大样本极限下的渐近性质，证明了其收敛到一个确定性过程，并探讨了其使得测试误差减小的动力学以及其长时间行为。

    

    无穷小梯度提升是机器学习中流行的基于树的梯度提升算法的消失学习率极限。它被定义为在无穷维函数空间中的非线性常微分方程的解，其中驱动动力学的无穷小提升算子依赖于训练样本。我们研究了模型在大样本极限下的渐近性质，并证明了其收敛到一个确定性过程。这个种群极限再次被一个依赖于种群分布的微分方程所描述。我们探讨了这个种群极限的一些性质：我们证明了动力学使得测试误差减小，并考虑了它在长时间行为上的表现。

    Infinitesimal gradient boosting (Dombry and Duchamps, 2021) is defined as the vanishing-learning-rate limit of the popular tree-based gradient boosting algorithm from machine learning. It is characterized as the solution of a nonlinear ordinary differential equation in a infinite-dimensional function space where the infinitesimal boosting operator driving the dynamics depends on the training sample. We consider the asymptotic behavior of the model in the large sample limit and prove its convergence to a deterministic process. This population limit is again characterized by a differential equation that depends on the population distribution. We explore some properties of this population limit: we prove that the dynamics makes the test error decrease and we consider its long time behavior.
    
[^35]: 动态均场规划

    Dynamic mean field programming. (arXiv:2206.05200v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05200](http://arxiv.org/abs/2206.05200)

    本文发展了一种动态均场规划方法，用于有限状态和行为的贝叶斯强化学习。通过模拟统计物理中的概念，研究了贝尔曼方程作为一种无序动力学系统，并通过均场方程计算状态行为值的统计信息。

    

    在大状态空间限制下，发展了一种动态均场理论，用于有限状态和行为的贝叶斯强化学习。类比于统计物理学，对贝尔曼方程进行了研究，将马尔科夫决策过程的转移概率解释为耦合，将值函数解释为动态演化的确定性自旋。因此，平均回报和转移概率被认为是淬灭随机变量。该理论揭示了在某些假设下，在渐近状态空间极限下，状态行为值在状态行为对之间具有统计独立性，并提供了确切的分布形式。这些结果适用于有限和无限折现时间视野，在价值迭代和策略评估中均成立。状态行为值的统计信息可以从一组均场方程中计算，我们称之为动态均场规划（DMFP）。对于策略评估，可以使用期望值迭代算法。

    A dynamic mean field theory is developed for finite state and action Bayesian reinforcement learning in the large state space limit. In an analogy with statistical physics, the Bellman equation is studied as a disordered dynamical system; the Markov decision process transition probabilities are interpreted as couplings and the value functions as deterministic spins that evolve dynamically. Thus, the mean-rewards and transition probabilities are considered to be quenched random variables. The theory reveals that, under certain assumptions, the state-action values are statistically independent across state-action pairs in the asymptotic state space limit, and provides the form of the distribution exactly. The results hold in the finite and discounted infinite horizon settings, for both value iteration and policy evaluation. The state-action value statistics can be computed from a set of mean field equations, which we call dynamic mean field programming (DMFP). For policy evaluation the e
    
[^36]: 分而治之融合

    Divide-and-Conquer Fusion. (arXiv:2110.07265v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2110.07265](http://arxiv.org/abs/2110.07265)

    这篇论文提出了一种分而治之的融合方法，在解决分布式“大数据”问题或多方隐私约束时，通过精确蒙特卡罗近似来改善后验分布的质量。

    

    将几个（样本逼近的）分布，我们称为子后验分布，结合成一个比例为它们乘积的单一分布是一个常见的挑战。这在分布式“大数据”问题中经常发生，或者在多方隐私约束下工作时会遇到。许多现有方法都采用近似子后验的方式，然后找到结果（产品汇总）后验的解析近似或样本近似。对于这些方法，当子后验分布超出一定范围（比如近似为高斯分布）时，后验近似的质量较差。最近，提出了一种融合方法，该方法找到了后验的精确蒙特卡罗近似，避免了近似方法的缺点。不幸的是，现有融合方法存在一些计算限制，特别是当整合大量子后验时。

    Combining several (sample approximations of) distributions, which we term sub-posteriors, into a single distribution proportional to their product, is a common challenge. Occurring, for instance, in distributed 'big data' problems, or when working under multi-party privacy constraints. Many existing approaches resort to approximating the individual sub-posteriors for practical necessity, then find either an analytical approximation or sample approximation of the resulting (product-pooled) posterior. The quality of the posterior approximation for these approaches is poor when the sub-posteriors fall out-with a narrow range of distributional form, such as being approximately Gaussian. Recently, a Fusion approach has been proposed which finds an exact Monte Carlo approximation of the posterior, circumventing the drawbacks of approximate approaches. Unfortunately, existing Fusion approaches have a number of computational limitations, particularly when unifying a large number of sub-posteri
    
[^37]: 使用BTL模型的动态排名：一种基于最近邻的排名中心性方法

    Dynamic Ranking with the BTL Model: A Nearest Neighbor based Rank Centrality Method. (arXiv:2109.13743v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2109.13743](http://arxiv.org/abs/2109.13743)

    本文介绍了一种基于最近邻的排名中心性方法，用于动态排名问题，在动态设置中扩展了经典的BTL模型。

    

    许多应用例如推荐系统或体育比赛涉及到对n个物品进行成对比较，其目标是通过聚合比较结果来恢复物品的潜在强度和/或全局排序。近年来，这个问题从理论角度引起了很大的关注，提出了许多方法，并在适当的生成模型假设下提供了相关的统计保证。然而，这些结果通常将成对比较收集为一个比较图G，但是在许多应用中，例如锦标赛期间的足球比赛结果，成对比较的性质可能会随时间演变。与前述的静态设置相比，针对这种动态设置的理论结果相对有限。本文研究了将经典的BTL（Bradley-Terry-Luce）模型扩展到我们的动态设置的方法。

    Many applications such as recommendation systems or sports tournaments involve pairwise comparisons within a collection of $n$ items, the goal being to aggregate the binary outcomes of the comparisons in order to recover the latent strength and/or global ranking of the items. In recent years, this problem has received significant interest from a theoretical perspective with a number of methods being proposed, along with associated statistical guarantees under the assumption of a suitable generative model.  While these results typically collect the pairwise comparisons as one comparison graph $G$, however in many applications - such as the outcomes of soccer matches during a tournament - the nature of pairwise outcomes can evolve with time. Theoretical results for such a dynamic setting are relatively limited compared to the aforementioned static setting. We study in this paper an extension of the classic BTL (Bradley-Terry-Luce) model for the static setting to our dynamic setup under t
    
[^38]: 多类分类中的良性过拟合：所有路径都通往插值

    Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation. (arXiv:2106.10865v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.10865](http://arxiv.org/abs/2106.10865)

    多类分类中的良性过拟合问题进行了研究，提出了一个简单的确定性条件，当前三种算法在满足条件时会得到插值数据并具有相等准确率的分类器。

    

    在超参数化模型中，“良性过拟合”的文献大多局限于回归或二分类问题；然而，现代机器学习在多类别设置中运行。受此差异的启发，我们研究了多类线性分类中的良性过拟合。具体而言，我们考虑在可分数据上的以下训练算法：（i）交叉熵损失的经验风险最小化（ERM），收敛到多类支持向量机（SVM）解；（ii）最小二乘损失的ERM，收敛到最小范数插值（MNI）解；及（iii）一对多SVM分类器。首先，我们提供了一个简单的充分确定性条件，在该条件下，所有三种算法都会导致插值训练数据并具有相等准确率的分类器。当数据来自高斯混合模型或多项式逻辑模型时，在足够高的有效超参数化下，这个条件成立。我们还展示了...

    The literature on "benign overfitting" in overparameterized models has been mostly restricted to regression or binary classification; however, modern machine learning operates in the multiclass setting. Motivated by this discrepancy, we study benign overfitting in multiclass linear classification. Specifically, we consider the following training algorithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple sufficient deterministic condition under which all three algorithms lead to classifiers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. We also show that t
    
[^39]: 离线强化学习的遗憾快速收敛速率研究

    Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.00479](http://arxiv.org/abs/2102.00479)

    本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。

    

    本文研究了固定行为策略在无限时间折现马尔可夫决策过程（MDP）中生成的离线数据对强化学习的遗憾。现有方法（如拟合Q-迭代）的分析表明，对于遗憾的收敛速率是O(1/√n)，但实证行为表现出非常快的收敛速度。本文通过提供遗憾收敛速率的快速收敛进行更精细的遗憾分析，准确地表征了这一现象。首先，我们证明在给定最优质量函数Q*的估计的情况下，其对应的策略遗憾按照Q*估计的点对点收敛速率的指数进行收敛，从而加速了收敛速度。指数的级别取决于“决策问题”中的噪声水平，而不是估计问题。我们以线性和表格型MDP作为示例，建立了这样的噪声水平。其次，我们对拟合Q-迭代和Bellman残差进行了新的分析。

    We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret, empirical behavior exhibits \emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid
    
[^40]: 从名字中的字符序列预测种族和民族

    Predicting Race and Ethnicity From the Sequence of Characters in a Name. (arXiv:1805.02109v2 [stat.AP] UPDATED)

    [http://arxiv.org/abs/1805.02109](http://arxiv.org/abs/1805.02109)

    通过分析名字中的字符序列，可以预测种族和民族，并应用于竞选资金数据和新闻报道中。

    

    为了回答关于种族不平等和公正性的问题，我们经常需要一种从名字中推断种族和民族的方法。一种从名字中推断种族和民族的方法是依赖于人口普查局的热门姓氏列表。然而，该列表存在至少三个限制：1. 它只包含姓氏，2. 它只包含常见的姓氏，3. 它每10年更新一次。为了提供更好的泛化能力，并在名字可用时提高准确性，我们使用各种技术建立了名字中的字符与种族和民族之间的关系模型。使用长短期记忆的模型具有最佳的样本外准确性，为0.85。最佳表现的姓氏模型的样本外准确性为0.81。为了说明模型的实用性，我们将其应用于竞选资金数据，估计不同种族群体的捐款份额，并将其应用于新闻数据，估计新闻中各种种族和民族的报道情况。

    To answer questions about racial inequality and fairness, we often need a way to infer race and ethnicity from names. One way to infer race and ethnicity from names is by relying on the Census Bureau's list of popular last names. The list, however, suffers from at least three limitations: 1. it only contains last names, 2. it only includes popular last names, and 3. it is updated once every 10 years. To provide better generalization, and higher accuracy when first names are available, we model the relationship between characters in a name and race and ethnicity using various techniques. A model using Long Short-Term Memory works best with out-of-sample accuracy of .85. The best-performing last-name model achieves out-of-sample accuracy of .81. To illustrate the utility of the models, we apply them to campaign finance data to estimate the share of donations made by people of various racial groups, and to news data to estimate the coverage of various races and ethnicities in the news.
    

