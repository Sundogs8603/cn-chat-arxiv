# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Statistical Query Lower Bounds for Learning Truncated Gaussians](https://arxiv.org/abs/2403.02300) | 本研究的主要发现是对于学习截断高斯分布估计的问题，我们证明了存在一个统计查询下界，表明在这个任务中存在着超多项式信息-计算差距。 |
| [^2] | [A prediction rigidity formalism for low-cost uncertainties in trained neural networks](https://arxiv.org/abs/2403.02251) | 通过解决受限优化问题，提出了“预测刚性”作为一种获得任意预先训练回归器不确定性的方法，扩展了方法应用于神经网络，并在多种回归任务上展示了其有效性。 |
| [^3] | [Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling](https://arxiv.org/abs/2403.02233) | 本文提供了关于使用MIM自监督预训练学习transformers的首个端到端理论，揭示了transformers如何学习到在具有空间结构的数据分布上突显特征-位置相关性的本地和多样化注意模式 |
| [^4] | [Mutual Information Estimation via Normalizing Flows](https://arxiv.org/abs/2403.02187) | 通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。 |
| [^5] | [Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling](https://arxiv.org/abs/2403.02150) | 引入了Recency-Weighted Temporally-Segmented（ReWTS）集成模型，利用块状方法进行多步预测，可以专门化模型并优化预测效果。 |
| [^6] | [Max-sliced 2-Wasserstein distance](https://arxiv.org/abs/2403.02142) | 使用相同技术获得了紧支撑对称概率测度与其对称经验分布之间的期望最大切片2-瓦塞斯坦距离的上界 |
| [^7] | [Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations](https://arxiv.org/abs/2403.02051) | 在重尾扰动下，噪声SGD实现了差分隐私保证，适用于广泛的损失函数类，特别是非凸函数。 |
| [^8] | [Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks](https://arxiv.org/abs/2403.02011) | 本研究提出了一种公平潜在表示的二分图变分自动编码器方法，以解决生态网络中的抽样偏差问题，通过在损失函数中引入额外的HSIC惩罚项，确保了潜在空间结构与连续变量的独立性。 |
| [^9] | [Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities](https://arxiv.org/abs/2403.02004) | 证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。 |
| [^10] | [Capacity of the Hebbian-Hopfield network associative memory](https://arxiv.org/abs/2403.01907) | Hopfield提出了一种Hebbian学习规则的神经网络模型，研究了关联记忆的容量，指出网络的容量与模式大小线性相关，提出了容量预测值，并使用两个著名模式的吸引盆地来探讨相关问题。 |
| [^11] | [Robustness Bounds on the Successful Adversarial Examples: Theory and Practice](https://arxiv.org/abs/2403.01896) | 本文提出了一个新的成功对抗样本概率上限的理论界限，取决于扰动范数、核函数以及训练数据集中最接近的不同标签对之间的距离，并且实验证明了该理论结果的有效性。 |
| [^12] | [Improving generalisation via anchor multivariate analysis](https://arxiv.org/abs/2403.01865) | 引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。 |
| [^13] | [Deep Horseshoe Gaussian Processes](https://arxiv.org/abs/2403.01737) | 深马蹄高斯过程Deep-HGP是一种简单的先验，采用深高斯过程并允许数据驱动选择关键长度尺度参数，对于非参数回归表现出良好的性能，实现了对未知真实回归曲线的优化回复，具有自适应的收敛速率。 |
| [^14] | [Soft-constrained Schrodinger Bridge: a Stochastic Control Approach](https://arxiv.org/abs/2403.01717) | 提出了软约束薛定谔桥(SSB)控制问题，在允许终端分布与预先指定分布不同的情况下，惩罚两者之间的Kullback-Leibler散度。理论上推导出了SSB解，显示最优控制过程的终端分布是μT和其他分布的几何混合，并将结果扩展到时间序列设置。 |
| [^15] | [Dendrogram of mixing measures: Learning latent hierarchy and model selection for finite mixture models](https://arxiv.org/abs/2403.01684) | 通过混合模型的潜在混合度量的树状图，我们提出一种新的方式来总结和选择混合模型，能够在模型参数仅具有较弱可识别性时一致地选择真实混合组分的数量，并从树中获得参数估计的逐点最优收敛速率。 |
| [^16] | [CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables](https://arxiv.org/abs/2403.01673) | CATS通过构建辅助时间序列作为外生变量，有效地表示和整合多元时间序列之间的关系，提高了多元时间序列预测的效果，并且相较于之前的模型大幅减少了复杂性和参数。 |
| [^17] | [Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models](https://arxiv.org/abs/2403.01639) | 本文首次在高斯混合模型背景下进行理论研究，证明了将扩散指导纳入不仅提升了分类置信度，而且减少了分布多样性。 |
| [^18] | [Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks](https://arxiv.org/abs/2403.01636) | 通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。 |
| [^19] | [Critical windows: non-asymptotic theory for feature emergence in diffusion models](https://arxiv.org/abs/2403.01633) | 我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。 |
| [^20] | [Towards Provable Log Density Policy Gradient](https://arxiv.org/abs/2403.01605) | 提出对数密度梯度方法来估计策略梯度，修正残差误差，有望改善强化学习方法的样本复杂度。 |
| [^21] | [Calibrating doubly-robust estimators with unbalanced treatment assignment](https://arxiv.org/abs/2403.01585) | 提出了一个简单的DML估计器扩展，通过对概率得分建模进行欠采样，并校准分数以匹配原始分布，以解决处理分配不平衡问题。 |
| [^22] | [Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa](https://arxiv.org/abs/2403.01571) | 通过将Kullback-Leibler散度与Cohen's Kappa相关联，限制了分类性能的最大限度 |
| [^23] | [Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection](https://arxiv.org/abs/2403.01485) | 本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法 |
| [^24] | [Fusion of Gaussian Processes Predictions with Monte Carlo Sampling](https://arxiv.org/abs/2403.01389) | 本文提出了一种将高斯过程预测通过蒙特卡洛采样进行融合的新方法，旨在提高预测准确性。 |
| [^25] | [Large-scale variational Gaussian state-space models](https://arxiv.org/abs/2403.01371) | 该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。 |
| [^26] | [High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media](https://arxiv.org/abs/2403.01318) | 提出了高维尾指数回归方法，利用正则化估计和去偏方法进行推断，支持理论的仿真研究，并在社交媒体病毒帖子文本分析中应用。 |
| [^27] | [Near-optimal Per-Action Regret Bounds for Sleeping Bandits](https://arxiv.org/abs/2403.01315) | 该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。 |
| [^28] | [Can a Confident Prior Replace a Cold Posterior?](https://arxiv.org/abs/2403.01272) | 探讨了将后验调整替换为增加信心的先验分布的可行性，引入了实用的“DirClip”先验和“信心先验”，提供了对信心先验的一般见解。 |
| [^29] | [Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise](https://arxiv.org/abs/2403.01204) | 我们提出了一种针对具有Massart噪声的线性和ReLU回归问题的随机梯度下降方法，具有新颖的近乎线性收敛保证，首次在流式设置中为鲁棒ReLU回归提供了收敛保证，并展示了其相比于以前的方法有改进的收敛速率。 |
| [^30] | [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046) | 证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。 |
| [^31] | [On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games](https://arxiv.org/abs/2403.00993) | 明确表示信息结构是分析和解决强化学习问题的重要组成部分。 |
| [^32] | [Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling](https://arxiv.org/abs/2403.00869) | 引入了CDAM和TAM模型以改进多元时间序列预测，通过最小化冗余信息并增强互信息，利用时间相关性。 |
| [^33] | [NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions](https://arxiv.org/abs/2403.00849) | 改进了FPGA加速神经网络推断任务的方法，提出将整个子网络映射到单个LUT中，使得神经网络拓扑和精度不再影响生成的查找表的大小。 |
| [^34] | [Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting](https://arxiv.org/abs/2403.00796) | 本论文通过使用高斯过程在金融预测中探索功能和增强数据结构，提供了一种能够预测整个概率分布并进行长期预测的方法，对于准确预测和决策制定具有重要意义 |
| [^35] | [Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm](https://arxiv.org/abs/2402.19456) | 该论文分析了量子近似优化算法在尖峰张量模型中的统计估计问题，发现了QAOA在恢复阈值方面的匹配性，并展示了渐近重叠分布的正弦-高斯定律。 |
| [^36] | [Certain and Approximately Certain Models for Statistical Learning](https://arxiv.org/abs/2402.17926) | 可以直接从带有缺失值的数据中学习准确模型，构建了检查数据填充必要性的高效算法，并在不需要填充的情况下返回准确模型，显著减少数据填充所需的时间和精力 |
| [^37] | [When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning](https://arxiv.org/abs/2402.17747) | RLHF在考虑部分观察性时可能导致策略欺骗性地夸大性能或过度辩护行为，我们提出了数学条件来解决这些问题，并警告不要盲目应用RLHF在部分可观测情况下。 |
| [^38] | [Latent Attention for Linear Time Transformers](https://arxiv.org/abs/2402.17512) | 提出了一种基于潜在向量定义注意力的方法，将标准transformer中的注意力机制的时间复杂度从二次方降低到与时间线性相关，表现与标准注意力媲美，但允许上下文窗口扩展到远远超出标准的范围。 |
| [^39] | [A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data](https://arxiv.org/abs/2402.16991) | 扩散模型在研究数据的分层生成模型中展示出了在阈值时间发生相变的特性，这影响了高级特征和低级特征的重建过程。 |
| [^40] | [Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values](https://arxiv.org/abs/2402.16388) | 针对异常检测系统中不确定性量化的需求，提出了一种新颖的框架，称为交叉一致异常检测，通过校准模型的不确定性提供统计保证。 |
| [^41] | [Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation](https://arxiv.org/abs/2402.14264) | 采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性 |
| [^42] | [Sparse and Faithful Explanations Without Sparse Models](https://arxiv.org/abs/2402.09702) | 引入了稀疏解释值(SEV)，用于衡量机器学习模型的决策稀疏性。即使模型不是稀疏的，许多机器学习模型在SEV的衡量下仍具有低决策稀疏性。 |
| [^43] | [The Limits of Assumption-free Tests for Algorithm Performance](https://arxiv.org/abs/2402.07388) | 这项研究探讨了使用有限数据量回答算法性能问题的基本限制，证明了黑盒测试方法无法准确回答算法在不同训练集上的整体性能和特定模型的性能问题。 |
| [^44] | [Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks](https://arxiv.org/abs/2312.14499) | 介绍了 Hutchinson 迹估计（HTE），通过将整个 Hessian 矩阵的计算转换为 Hessian 矢量乘积（HVP），解决了 PINNs 处理高维和高阶 PDE 的挑战。 |
| [^45] | [Distributional Bellman Operators over Mean Embeddings](https://arxiv.org/abs/2312.07358) | 提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。 |
| [^46] | [$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence](https://arxiv.org/abs/2312.01133) | 通过引入学生t分布和幂分歧，提出了$t^3$VAE变分自动编码器框架，以更好地处理重尾数据，并推导出新的优化目标。 |
| [^47] | [Semiparametric Efficient Inference in Adaptive Experiments](https://arxiv.org/abs/2311.18274) | 自适应实验中提出了一种半参数高效推断方法，通过中心极限定理和置信序列实现了更紧凑的推断，具有更广泛的适用性。 |
| [^48] | [Nonparametric consistency for maximum likelihood estimation and clustering based on mixtures of elliptically-symmetric distributions](https://arxiv.org/abs/2311.06108) | 展示了椭圆对称分布混合的最大似然估计的一致性，为基于非参数分布的聚类提供了理论依据。 |
| [^49] | [Gaussian Process-Gated Hierarchical Mixtures of Experts](https://arxiv.org/abs/2302.04947) | 该论文提出了一种新颖的高斯过程门控的分层专家混合模型，通过使用GPs构建门控函数和专家，优于传统基于树的模型，同时在复杂性较低的情况下表现出良好性能，还提供了深层GPs和深度贝叶斯神经网络的可解释性。 |
| [^50] | [Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints](https://arxiv.org/abs/2212.04672) | 提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。 |
| [^51] | [Settling the Sample Complexity of Model-Based Offline Reinforcement Learning](https://arxiv.org/abs/2204.05275) | 该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。 |
| [^52] | [Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks](https://arxiv.org/abs/2105.13937) | 提出了一种基于多边形未调整的朗之万算法的新类别算法，名为TH$\varepsilon$O POULA（或简称为TheoPouLa），通过稳定性、非渐进分析和实验表明其在神经网络优化中具有卓越性能。 |
| [^53] | [Private Prediction Sets](https://arxiv.org/abs/2102.06202) | 该研究提出了一个基于符合性预测的框架，可以在保护个人隐私的同时返回可靠的不确定性量化的预测集。 |
| [^54] | [Hodge-Compositional Edge Gaussian Processes.](http://arxiv.org/abs/2310.19450) | 本论文提出了一种新的方法用于对边缘集合上的函数进行建模，该方法基于Hodge分解开发了适用于不同应用场景的无散度和无旋度的高斯过程，并通过组合它们来表示任意边缘函数。实验结果表明这种方法在流动数据推断中具有潜在的实际应用价值。 |
| [^55] | [Adaptive operator learning for infinite-dimensional Bayesian inverse problems.](http://arxiv.org/abs/2310.17844) | 该论文提出了一种自适应操作员学习框架，通过使用贪婪算法选择自适应点对预训练的近似模型进行微调，逐渐减少建模误差。这种方法可以在准确性和效率之间取得平衡，有助于有效解决贝叶斯逆问题中的计算问题。 |
| [^56] | [A connection between Tempering and Entropic Mirror Descent.](http://arxiv.org/abs/2310.11914) | 本论文研究了退火（针对顺序蒙特卡罗; SMC）和熵镜像下降之间的联系，证明了退火SMC是应用于Kullback-Leibler（KL）散度的熵镜像下降的数值近似，并提出了改进方案。 |
| [^57] | [Optimising Distributions with Natural Gradient Surrogates.](http://arxiv.org/abs/2310.11837) | 本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。 |
| [^58] | [Local Graph Clustering with Noisy Labels.](http://arxiv.org/abs/2310.08031) | 本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。 |
| [^59] | [Orlicz regrets to consistently bound statistics of random variables with an application to environmental indicators.](http://arxiv.org/abs/2310.05168) | 本文提出了一种新型的Orlicz后悔方法，用于一致地界定随机变量的统计量上下界，通过灵活评估随机变量的尾行为。与传统方法不同，此方法采用了一致性评估，并得到了将其与发散风险度量等效的充分条件。 |
| [^60] | [Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization.](http://arxiv.org/abs/2310.03234) | 本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。 |
| [^61] | [$G$-Mapper: Learning a Cover in the Mapper Construction.](http://arxiv.org/abs/2309.06634) | 本论文介绍了一种基于统计检验和聚类算法的优化Mapper图覆盖的方法，通过分割覆盖选择生成了保留数据集本质的Mapper图。 |
| [^62] | [SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models.](http://arxiv.org/abs/2309.05019) | 本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。 |
| [^63] | [Differentially Private Functional Summaries via the Independent Component Laplace Process.](http://arxiv.org/abs/2309.00125) | 本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。 |
| [^64] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^65] | [FaIRGP: A Bayesian Energy Balance Model for Surface Temperatures Emulation.](http://arxiv.org/abs/2307.10052) | FaIRGP是一种新的数据驱动代理模型，它满足能量平衡模型的物理温度响应方程，同时具备了从观测中学习和进行推断的能力。 |
| [^66] | [Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing.](http://arxiv.org/abs/2307.00494) | 使用基于图形平滑的Gibbs采样方法（GGS）优化蛋白质适应性，消除了突变距离的限制，同时提高了搜索效率。该方法在发现高适应性蛋白质方面达到了最先进水平。 |
| [^67] | [Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement.](http://arxiv.org/abs/2306.12803) | 本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。 |
| [^68] | [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning.](http://arxiv.org/abs/2306.04815) | 本文通过研究SGD训练损失中的尖峰现象，提出了“投石机”优化现象，通过增加与真实预测器的平均梯度外积对齐来促进特征学习，并证明较小的批量大小可提高泛化性能。 |
| [^69] | [Designing Decision Support Systems Using Counterfactual Prediction Sets.](http://arxiv.org/abs/2306.03928) | 本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。 |
| [^70] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^71] | [On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models.](http://arxiv.org/abs/2305.17583) | 本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。 |
| [^72] | [Learning Safety Constraints from Demonstrations with Unknown Rewards.](http://arxiv.org/abs/2305.16147) | CoCoRL是一种从不知道奖励的已知安全演示中推断约束的方法，可以用于Constrained Markov Decision Process（CMDP），并且对于几乎最优演示能够无误差收敛于真实的安全集。 |
| [^73] | [Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge.](http://arxiv.org/abs/2305.15086) | 本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。 |
| [^74] | [Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis.](http://arxiv.org/abs/2302.14186) | 本文提出了一种基于Fisher线性判别的领域自适应模型，该模型是两个假设的凸组合，可以在不访问任何单个源任务的直接信息的情况下计算最优分类器，并在基于EEG和ECG的分类设置中展示了其有效性。 |
| [^75] | [On the Expressive Power of Geometric Graph Neural Networks.](http://arxiv.org/abs/2301.09308) | 本文提出了几何版本的Weisfeiler-Leman测试(GWL)，可以区分几何图形，揭示了关键设计选择如何影响几何GNN的表现力 |

# 详细

[^1]: 对学习截断高斯分布估计的统计查询下界

    Statistical Query Lower Bounds for Learning Truncated Gaussians

    [https://arxiv.org/abs/2403.02300](https://arxiv.org/abs/2403.02300)

    本研究的主要发现是对于学习截断高斯分布估计的问题，我们证明了存在一个统计查询下界，表明在这个任务中存在着超多项式信息-计算差距。

    

    我们研究了在截断设置中估计均值的问题，其中截断集来自一个低复杂度集合$\mathcal{C}$。具体地，对于一个固定但未知的截断集$S \subseteq \mathbb{R}^d$，我们可以访问从分布$\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$截断到集合$S$的样本。目标是在$\ell_2$-范数内以精度$\epsilon>0$估计$\boldsymbol\mu$。我们的主要结果是一个统计查询（SQ）下界，表明了在这个任务中存在着超多项式信息-计算差距。具体而言，我们展示了对于这个问题的任何SQ算法的复杂度均为$d^{\mathrm{poly}(1/\epsilon)}$，即使类$\mathcal{C}$是简单的，所以信息论上$\mathrm{poly}(d/\epsilon)$个样本足够。具体地，我们的SQ下界适用于当$\mathcal{C}$是有界个数的并集时

    arXiv:2403.02300v1 Announce Type: cross  Abstract: We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union of a bounded num
    
[^2]: 一个用于训练神经网络中低成本不确定性的预测刚性形式主义

    A prediction rigidity formalism for low-cost uncertainties in trained neural networks

    [https://arxiv.org/abs/2403.02251](https://arxiv.org/abs/2403.02251)

    通过解决受限优化问题，提出了“预测刚性”作为一种获得任意预先训练回归器不确定性的方法，扩展了方法应用于神经网络，并在多种回归任务上展示了其有效性。

    

    回归方法对科学和技术应用至关重要。然而，拟合模型在其训练领域之外可能极不可靠，因此在许多应用中，量化其不确定性是至关重要的。基于受限优化问题的解，我们提出“预测刚性”作为一种获得任意预先训练回归器不确定性的方法。我们建立了我们的框架与贝叶斯推断之间的强连接，并开发了一个允许新方法应用于神经网络的最后一层逼近。这种扩展提供了不需要对神经网络本身或其训练过程进行任何修改的低成本不确定性。我们展示了我们的方法在从简单玩具模型到化学和气象学应用的广泛回归任务中的有效性。

    arXiv:2403.02251v1 Announce Type: cross  Abstract: Regression methods are fundamental for scientific and technological applications. However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications. Based on the solution of a constrained optimization problem, we propose "prediction rigidities" as a method to obtain uncertainties of arbitrary pre-trained regressors. We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.
    
[^3]: Transformers在Masked Image Modeling中能够证明学习特征-位置相关性

    Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling

    [https://arxiv.org/abs/2403.02233](https://arxiv.org/abs/2403.02233)

    本文提供了关于使用MIM自监督预训练学习transformers的首个端到端理论，揭示了transformers如何学习到在具有空间结构的数据分布上突显特征-位置相关性的本地和多样化注意模式

    

    Masked image modeling (MIM)是一种新兴的自监督视觉预训练方法，它从未屏蔽的图像中预测随机屏蔽的补丁。然而，对于基于transformers的MIM的理论理解相当有限。本文提供了有关使用MIM自监督预训练学习一层transformers的首个端到端理论。我们提出了transformers如何学习到在具有空间结构的数据分布上突显特征-位置相关性的本地和多样化注意模式的理论机制。

    arXiv:2403.02233v1 Announce Type: new  Abstract: Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the i
    
[^4]: 通过正则化流进行互信息估计

    Mutual Information Estimation via Normalizing Flows

    [https://arxiv.org/abs/2403.02187](https://arxiv.org/abs/2403.02187)

    通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。

    

    我们提出了一种新颖的方法来解决互信息（MI）估计问题，即引入基于正则化流的估计器。该估计器将原始数据映射到具有已知互信息闭合形式表达式的目标分布。我们证明了我们的方法产生了原始数据的互信息估计。通过高维数据的实验结果展示了所提出估计器的优势。

    arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
    
[^5]: 改进的时间分段集成方法用于时间序列建模

    Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling

    [https://arxiv.org/abs/2403.02150](https://arxiv.org/abs/2403.02150)

    引入了Recency-Weighted Temporally-Segmented（ReWTS）集成模型，利用块状方法进行多步预测，可以专门化模型并优化预测效果。

    

    在工艺行业中，时间序列建模面对处理复杂、多方面和不断演变的数据特征的挑战。传统的单一模型方法往往难以捕捉多样动态之间的相互作用，导致预测不够优化。因此，我们引入了Recency-Weighted Temporally-Segmented（ReWTS，发音为`roots'）集成模型，这是一种新颖的基于块的多步预测方法。ReWTS模型的关键特征有两个：1）通过将训练数据划分为数据块并对每个块训练一个模型，有助于将模型专门化为不同的动态。2）在推断阶段，一个优化过程评估最近的过去中的每个模型，并选择活动模型，以便能够召回以前学习的动态的适当混合来预测未来。这种方法不仅捕捉了每个时期的细微差异，而且能够自适应。

    arXiv:2403.02150v1 Announce Type: cross  Abstract: Time-series modeling in process industries faces the challenge of dealing with complex, multi-faceted, and evolving data characteristics. Conventional single model approaches often struggle to capture the interplay of diverse dynamics, resulting in suboptimal forecasts. Addressing this, we introduce the Recency-Weighted Temporally-Segmented (ReWTS, pronounced `roots') ensemble model, a novel chunk-based approach for multi-step forecasting. The key characteristics of the ReWTS model are twofold: 1) It facilitates specialization of models into different dynamics by segmenting the training data into `chunks' of data and training one model per chunk. 2) During inference, an optimization procedure assesses each model on the recent past and selects the active models, such that the appropriate mixture of previously learned dynamics can be recalled to forecast the future. This method not only captures the nuances of each period, but also adapt
    
[^6]: 最大切片2-瓦塞斯坦距离

    Max-sliced 2-Wasserstein distance

    [https://arxiv.org/abs/2403.02142](https://arxiv.org/abs/2403.02142)

    使用相同技术获得了紧支撑对称概率测度与其对称经验分布之间的期望最大切片2-瓦塞斯坦距离的上界

    

    这个笔记是作者在“最大切片瓦塞斯坦距离的尖锐界限”方面之前工作的延续。我们使用相同的技术获得紧支撑对称概率测度与其对称经验分布之间的期望最大切片2-瓦塞斯坦距离上界。

    arXiv:2403.02142v1 Announce Type: cross  Abstract: This note is a continuation of the author's previous work on ``Sharp bounds for the max-sliced Wasserstein distance." We use the same technique to obtain an upper bound for the expected max-sliced 2-Wasserstein distance between a compactly supported symmetric probability measure on a Euclidean space and its symmetrized empirical distribution.
    
[^7]: 在重尾扰动下噪声(S)GD的差分隐私

    Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations

    [https://arxiv.org/abs/2403.02051](https://arxiv.org/abs/2403.02051)

    在重尾扰动下，噪声SGD实现了差分隐私保证，适用于广泛的损失函数类，特别是非凸函数。

    

    将重尾噪声注入随机梯度下降(SGD)的迭代中已经引起越来越多的关注。尽管对导致的算法的各种理论性质进行了分析，主要来自学习理论和优化视角，但它们的隐私保护性质尚未建立。为了弥补这一缺口，我们为噪声SGD提供差分隐私(DP)保证，当注入的噪声遵循$\alpha$-稳定分布时，该分布包括一系列重尾分布(具有无限方差)以及高斯分布。考虑$(\epsilon,\delta)$-DP框架，我们表明带有重尾扰动的SGD实现了$(0,\tilde{\mathcal{O}}(1/n))$-DP的广泛损失函数类，这些函数可以是非凸的，这里$n$是数据点的数量。作为一项显着的副产品，与以往的工作相反，该工作要求有界se

    arXiv:2403.02051v1 Announce Type: cross  Abstract: Injecting heavy-tailed noise to the iterates of stochastic gradient descent (SGD) has received increasing attention over the past few years. While various theoretical properties of the resulting algorithm have been analyzed mainly from learning theory and optimization perspectives, their privacy preservation properties have not yet been established. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the Gaussian distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded se
    
[^8]: 公平潜在表示的二分图变分自动编码器，以解决生态网络中的抽样偏差问题

    Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks

    [https://arxiv.org/abs/2403.02011](https://arxiv.org/abs/2403.02011)

    本研究提出了一种公平潜在表示的二分图变分自动编码器方法，以解决生态网络中的抽样偏差问题，通过在损失函数中引入额外的HSIC惩罚项，确保了潜在空间结构与连续变量的独立性。

    

    我们提出一种方法，使用图嵌入来表示二分网络，以解决研究生态网络所面临的挑战，比如连接植物和传粉者等网络，需考虑许多协变量，尤其要控制抽样偏差。我们将变分图自动编码器方法调整为二分情况，从而能够在潜在空间中生成嵌入，其中两组节点的位置基于它们的连接概率。我们将在社会学中常考虑的公平性框架转化为生态学中的抽样偏差问题。通过在损失函数中添加Hilbert-Schmidt独立准则（HSIC）作为额外惩罚项，我们确保潜在空间结构与连续变量（与抽样过程相关）无关。最后，我们展示了我们的方法如何改变我们对生态网络的理解。

    arXiv:2403.02011v1 Announce Type: cross  Abstract: We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological n
    
[^9]: 粒子梯度下降的误差界限，以及log-Sobolev和Talagrand不等式的推广

    Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities

    [https://arxiv.org/abs/2403.02004](https://arxiv.org/abs/2403.02004)

    证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。

    

    我们证明了粒子梯度下降(PGD)~(Kuntz等人，2023)的非渐近误差界限，这是一种最大似然估计的算法，用于离散化自由能梯度流获得的大型潜变量模型。我们首先展示了对于满足一般化log-Sobolev和Polyak-Lojasiewicz不等式（LSI和PLI）的模型，流以指数速度收敛到自由能的极小化集合。我们通过将最优输运文献中众所周知的结果（LSI意味着Talagrand不等式）及其在优化文献中的对应物（PLI意味着所谓的二次增长条件）扩展并应用到我们的新设置，来实现这一点。我们还推广了Bakry-Emery定理，并展示了对于具有强凹对数似然的模型，LSI/PLI的概括成立。

    arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for models with strongly concave log-likelihoods. For such m
    
[^10]: Hebbian-Hopfield网络关联记忆的容量

    Capacity of the Hebbian-Hopfield network associative memory

    [https://arxiv.org/abs/2403.01907](https://arxiv.org/abs/2403.01907)

    Hopfield提出了一种Hebbian学习规则的神经网络模型，研究了关联记忆的容量，指出网络的容量与模式大小线性相关，提出了容量预测值，并使用两个著名模式的吸引盆地来探讨相关问题。

    

    在Hopfield的论文中，他提出了一种基于Hebbian学习规则的神经网络模型，并提出了它如何可以高效地作为一个关联记忆。在研究随机二进制模式时，他还发现，如果存储模式检索中容忍一小部分错误，网络的容量（最大记忆模式数，$m$）与每个模式的大小$n$呈线性关系。此外，他著名地预测了$\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$。我们研究了这个非常相同的情景，使用了两种著名模式的吸引盆地：\textbf{\emph{(i)}}来自\cite{AmiGutSom85}的AGS；以及\textbf{\emph{(ii)}}来自\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}的NLT。依赖于来自\cite{Stojnicflrdt23}的\emph{完全提升的随机对偶理论}（fl RDT），我们获得了在第一层提升上的以下明确容量特性描述：\begin{equation} \alpha ...

    arXiv:2403.01907v1 Announce Type: cross  Abstract: In \cite{Hop82}, Hopfield introduced a \emph{Hebbian} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, $m$) scales linearly with each pattern's size, $n$. Moreover, he famously predicted $\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$. We study this very same scenario with two famous pattern's basins of attraction: \textbf{\emph{(i)}} The AGS one from \cite{AmiGutSom85}; and \textbf{\emph{(ii)}} The NLT one from \cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the \emph{fully lifted random duality theory} (fl RDT) from \cite{Stojnicflrdt23}, we obtain the following explicit capacity characterizations on the first level of lifting:   \begin{equation}   \alpha
    
[^11]: 成功对抗样本的强鲁棒性界限：理论与实践

    Robustness Bounds on the Successful Adversarial Examples: Theory and Practice

    [https://arxiv.org/abs/2403.01896](https://arxiv.org/abs/2403.01896)

    本文提出了一个新的成功对抗样本概率上限的理论界限，取决于扰动范数、核函数以及训练数据集中最接近的不同标签对之间的距离，并且实验证明了该理论结果的有效性。

    

    对抗样本（AE）是一种针对机器学习的攻击方法，通过对数据添加不可感知的扰动来诱使错分。本文基于高斯过程（GP）分类，研究了成功AE的概率上限。我们证明了一个新的上界，取决于AE的扰动范数、GP中使用的核函数以及训练数据集中具有不同标签的最接近对之间的距离。令人惊讶的是，该上限不受样本数据集分布的影响。我们通过使用ImageNet的实验验证了我们的理论结果。此外，我们展示了改变核函数参数会导致成功AE概率上限的变化。

    arXiv:2403.01896v1 Announce Type: new  Abstract: Adversarial example (AE) is an attack method for machine learning, which is crafted by adding imperceptible perturbation to the data inducing misclassification. In the current paper, we investigated the upper bound of the probability of successful AEs based on the Gaussian Process (GP) classification. We proved a new upper bound that depends on AE's perturbation norm, the kernel function used in GP, and the distance of the closest pair with different labels in the training dataset. Surprisingly, the upper bound is determined regardless of the distribution of the sample dataset. We showed that our theoretical result was confirmed through the experiment using ImageNet. In addition, we showed that changing the parameters of the kernel function induces a change of the upper bound of the probability of successful AEs.
    
[^12]: 通过锚多元分析改善泛化能力

    Improving generalisation via anchor multivariate analysis

    [https://arxiv.org/abs/2403.01865](https://arxiv.org/abs/2403.01865)

    引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。

    

    我们在锚回归（AR）中引入因果正则化扩展，以改善超出分布（OOD）的泛化能力。我们提出了与锚框架相匹配的损失函数，以确保对分布转移的稳健性。各种多元分析（MVA）算法，如（正交化）PLS、RRR和MLR，均在锚框架内。我们观察到简单的正则化增强了OOD设置中的稳健性。在合成和真实的气候科学问题中，为所选算法提供了估计器，展示了其一致性和有效性。经验验证突显了锚正则化的多功能性，强调其与MVA方法的兼容性，并强调其在增强可复制性的同时抵御分布转移中的作用。扩展的AR框架推进了因果推断方法论，解决了可靠OOD泛化的需求。

    arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
    
[^13]: 深马蹄高斯过程

    Deep Horseshoe Gaussian Processes

    [https://arxiv.org/abs/2403.01737](https://arxiv.org/abs/2403.01737)

    深马蹄高斯过程Deep-HGP是一种简单的先验，采用深高斯过程并允许数据驱动选择关键长度尺度参数，对于非参数回归表现出良好的性能，实现了对未知真实回归曲线的优化回复，具有自适应的收敛速率。

    

    最近提出深高斯过程作为一种自然对象，类似于深度神经网络，可能拟合现代数据样本中存在的复杂特征，如组合结构。采用贝叶斯非参数方法，自然地利用深高斯过程作为先验分布，并将相应的后验分布用于统计推断。我们介绍了深马蹄高斯过程Deep-HGP，这是一种基于带有平方指数核的深高斯过程的新简单先验，特别是使得可以对关键长度尺度参数进行数据驱动选择。对于随机设计的非参数回归，我们展示了相应的调节后验分布以一种自适应方式，最优地在二次损失的意义下恢复未知的真回归曲线，最多只有一个对数因子。收敛速率同时对回归的平滑度和设计维度自适应。

    arXiv:2403.01737v1 Announce Type: cross  Abstract: Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated tempered posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regress
    
[^14]: 软约束薛定谔桥：一种随机控制方法

    Soft-constrained Schrodinger Bridge: a Stochastic Control Approach

    [https://arxiv.org/abs/2403.01717](https://arxiv.org/abs/2403.01717)

    提出了软约束薛定谔桥(SSB)控制问题，在允许终端分布与预先指定分布不同的情况下，惩罚两者之间的Kullback-Leibler散度。理论上推导出了SSB解，显示最优控制过程的终端分布是μT和其他分布的几何混合，并将结果扩展到时间序列设置。

    

    薛定谔桥可以被视为一个连续时间的随机控制问题，目标是找到一个最优控制扩散过程，其具有预先指定的终端分布μT。我们提出通过允许终端分布与μT不同但惩罚两个分布之间的Kullback-Leibler散度来泛化这个随机控制问题。我们将这个新的控制问题称为软约束薛定谔桥(SSB)。这项工作的主要贡献是对SSB解的理论推导，表明最优控制过程的终端分布是μT和另一些分布的几何混合。这个结果进一步扩展到时间序列设置。SSB的一个应用是鲁棒生成扩散模型的开发。我们提出了一个基于分数匹配的算法来从几何混合中进行抽样，并展示了其用途

    arXiv:2403.01717v1 Announce Type: cross  Abstract: Schr\"{o}dinger bridge can be viewed as a continuous-time stochastic control problem where the goal is to find an optimally controlled diffusion process with a pre-specified terminal distribution $\mu_T$. We propose to generalize this stochastic control problem by allowing the terminal distribution to differ from $\mu_T$ but penalizing the Kullback-Leibler divergence between the two distributions. We call this new control problem soft-constrained Schr\"{o}dinger bridge (SSB). The main contribution of this work is a theoretical derivation of the solution to SSB, which shows that the terminal distribution of the optimally controlled process is a geometric mixture of $\mu_T$ and some other distribution. This result is further extended to a time series setting. One application of SSB is the development of robust generative diffusion models. We propose a score matching-based algorithm for sampling from geometric mixtures and showcase its us
    
[^15]: 混合度量的树状图：学习潜在层次结构和有限混合模型的模型选择

    Dendrogram of mixing measures: Learning latent hierarchy and model selection for finite mixture models

    [https://arxiv.org/abs/2403.01684](https://arxiv.org/abs/2403.01684)

    通过混合模型的潜在混合度量的树状图，我们提出一种新的方式来总结和选择混合模型，能够在模型参数仅具有较弱可识别性时一致地选择真实混合组分的数量，并从树中获得参数估计的逐点最优收敛速率。

    

    我们提出了一种新的方式，通过过度拟合的潜在混合度量的层次聚类树（树状图）来汇总和选择混合模型。我们提出的方法连接了凝聚式层次聚类和混合建模。树状图的构建源自混合度量的收敛理论，因此，我们既可以一致地选择真实混合组分的数量，也可以从树中获得参数估计的逐点最优收敛速率，即使模型参数仅具有较弱可识别性。在理论上，它阐述了在层次聚类中选择最佳群集数的选择。在实践中，与传统的混合模型汇总方式相比，树状图揭示了有关亚群层次的更多信息。我们进行了几项模拟研究来支持我们的理论。我们还通过应用程序展示了该方法。

    arXiv:2403.01684v1 Announce Type: cross  Abstract: We present a new way to summarize and select mixture models via the hierarchical clustering tree (dendrogram) of an overfitted latent mixing measure. Our proposed method bridges agglomerative hierarchical clustering and mixture modeling. The dendrogram's construction is derived from the theory of convergence of the mixing measures, and as a result, we can both consistently select the true number of mixing components and obtain the pointwise optimal convergence rate for parameter estimation from the tree, even when the model parameters are only weakly identifiable. In theory, it explicates the choice of the optimal number of clusters in hierarchical clustering. In practice, the dendrogram reveals more information on the hierarchy of subpopulations compared to traditional ways of summarizing mixture models. Several simulation studies are carried out to support our theory. We also illustrate the methodology with an application to single-c
    
[^16]: CATS：通过构建辅助时间序列作为外生变量增强多元时间序列预测

    CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables

    [https://arxiv.org/abs/2403.01673](https://arxiv.org/abs/2403.01673)

    CATS通过构建辅助时间序列作为外生变量，有效地表示和整合多元时间序列之间的关系，提高了多元时间序列预测的效果，并且相较于之前的模型大幅减少了复杂性和参数。

    

    对于多元时间序列预测（MTSF），最近的深度学习应用显示，单变量模型经常优于多元模型。为了解决多元模型的不足，我们引入了一种方法，即构建辅助时间序列（CATS），它类似于2D时间上下文关注机制，从原始时间序列（OTS）生成辅助时间序列（ATS），以有效表示和整合系列间关系用于预测。ATS的关键原则-连续性，稀疏性和变异性-通过不同模块进行识别和实现。即使是基本的2层MLP作为核心预测器，CATS也取得了最先进的成果，相对于先前的多元模型，它显著减少了复杂性和参数，使其成为高效且可转移的MTSF解决方案。

    arXiv:2403.01673v1 Announce Type: cross  Abstract: For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the difficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS - continuity, sparsity, and variability - are identified and implemented through different modules. Even with a basic 2-layer MLP as core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it an efficient and transferable MTSF solution.
    
[^17]: Gaussian Mixture Models的扩散指导的理论洞见

    Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models

    [https://arxiv.org/abs/2403.01639](https://arxiv.org/abs/2403.01639)

    本文首次在高斯混合模型背景下进行理论研究，证明了将扩散指导纳入不仅提升了分类置信度，而且减少了分布多样性。

    

    扩散模型受益于将特定任务信息注入评分函数以引导样本生成朝向所需属性。这种信息被称为指导。例如，在文本到图像合成中，文本输入被编码为指导以生成语义对齐的图像。适当的指导输入与扩散模型的性能密切相关。一个常见的观察是，强有力的指导促进了与任务特定信息的紧密对齐，同时减少了生成样本的多样性。本文首次在高斯混合模型背景下提供了对理解指导对扩散模型影响的理论研究。在温和的条件下，我们证明了将扩散指导纳入不仅提升了分类置信度，而且减少了分布多样性，导致输出分布的差异熵减少。

    arXiv:2403.01639v1 Announce Type: new  Abstract: Diffusion models benefit from instillation of task-specific information into the score function to steer the sample generation towards desired properties. Such information is coined as guidance. For example, in text-to-image synthesis, text input is encoded as guidance to generate semantically aligned images. Proper guidance inputs are closely tied to the performance of diffusion models. A common observation is that strong guidance promotes a tight alignment to the task-specific information, while reducing the diversity of the generated samples. In this paper, we provide the first theoretical study towards understanding the influence of guidance on diffusion models in the context of Gaussian mixture models. Under mild conditions, we prove that incorporating diffusion guidance not only boosts classification confidence but also diminishes distribution diversity, leading to a reduction in the differential entropy of the output distribution.
    
[^18]: 通过多任务强化学习实现高效的短视探索

    Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks

    [https://arxiv.org/abs/2403.01636](https://arxiv.org/abs/2403.01636)

    通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。

    

    多任务强化学习（MTRL）方法在许多重要的强化学习（RL）任务中应用广泛，但近期MTRL理论的进展主要集中在通过假设任务间共享结构来提高统计效率，对于RL中至关重要的探索这一关键方面却大多被忽视。本文通过展示，当代理在足够多样化的任务集上训练时，具有短视探索设计（如$\epsilon$-贪心）的通用策略共享算法可以在MTRL中具有高样本效率，从我们所知，这是对“探索收益”在MTRL中的首次理论证明，也有助于解释短视探索在实践中应用广泛的成功。为了验证多样性的作用，我们在合成机器人控制任务上进行了实验证明。

    arXiv:2403.01636v1 Announce Type: cross  Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control
    
[^19]: 批判性窗口：扩散模型中特征出现的非渐进理论

    Critical windows: non-asymptotic theory for feature emergence in diffusion models

    [https://arxiv.org/abs/2403.01633](https://arxiv.org/abs/2403.01633)

    我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。

    

    我们发展理论来理解图像生成扩散模型中一个有趣的属性，我们称之为批判性窗口。实证上观察到在采样过程中存在狭窄的时间间隔，在此期间会出现最终图像的特定特征，例如图像类别或背景颜色。而这种特性对于解释性是有利的，因为意味着可以将生成的特性定位到轨迹的一个小片段，但这似乎与扩散的连续性质相矛盾。我们提出了一个形式化框架来研究这些窗口，并表明对于来自混合强对数凹密度分布的数据，这些窗口可以用一定的跨组和组内分离度量来显式地约束。我们还为诸如良条件G的具体示例实例化了这些界限。

    arXiv:2403.01633v1 Announce Type: new  Abstract: We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned G
    
[^20]: 朝向可证明的对数密度策略梯度

    Towards Provable Log Density Policy Gradient

    [https://arxiv.org/abs/2403.01605](https://arxiv.org/abs/2403.01605)

    提出对数密度梯度方法来估计策略梯度，修正残差误差，有望改善强化学习方法的样本复杂度。

    

    策略梯度方法是现代强化学习成功的关键要素。现代策略梯度方法虽然成功，但在梯度估计中引入了一个残差误差。本文认为这个残差项很重要，纠正它有可能改善强化学习方法的样本复杂度。为此，我们提出了对数密度梯度来估计策略梯度，可以纠正这个残差误差项。对数密度梯度方法通过利用状态-动作折扣分布形式来计算策略梯度。我们首先给出了准确找到标签马尔可夫决策过程（MDPs）的对数密度梯度所需的方程式。对于更复杂的环境，我们提出了一种利用后向即时（TD）方法来近似计算对数密度梯度的方法，通过利用后向的同策略样本。由于从马尔可夫链中进行后向采样是高度

    arXiv:2403.01605v1 Announce Type: cross  Abstract: Policy gradient methods are a vital ingredient behind the success of modern reinforcement learning. Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of reinforcement learning methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes (MDPs). For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly 
    
[^21]: 使用不平衡的处理分配校准双重稳健估计器

    Calibrating doubly-robust estimators with unbalanced treatment assignment

    [https://arxiv.org/abs/2403.01585](https://arxiv.org/abs/2403.01585)

    提出了一个简单的DML估计器扩展，通过对概率得分建模进行欠采样，并校准分数以匹配原始分布，以解决处理分配不平衡问题。

    

    机器学习方法，尤其是双机器学习（DML）估计器（Chernozhukov等，2018），越来越受欢迎地用于估计平均处理效应（ATE）。然而，数据集通常表现出处理分配不平衡，只有少数观测值被处理，导致稳健概率得分估计不稳定。我们提出了DML估计器的简单扩展，该扩展对概率得分建模进行了欠采样，并校准分数以匹配原始分布。本文提供了理论结果表明，该估计器保留了DML估计器的渐近特性。模拟研究说明了估计器的有限样本性能。

    arXiv:2403.01585v1 Announce Type: new  Abstract: Machine learning methods, particularly the double machine learning (DML) estimator (Chernozhukov et al., 2018), are increasingly popular for the estimation of the average treatment effect (ATE). However, datasets often exhibit unbalanced treatment assignments where only a few observations are treated, leading to unstable propensity score estimations. We propose a simple extension of the DML estimator which undersamples data for propensity score modeling and calibrates scores to match the original distribution. The paper provides theoretical results showing that the estimator retains the DML estimator's asymptotic properties. A simulation study illustrates the finite sample performance of the estimator.
    
[^22]: 将Kullback-Leibler散度与Cohen's Kappa相关联，限制分类性能

    Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa

    [https://arxiv.org/abs/2403.01571](https://arxiv.org/abs/2403.01571)

    通过将Kullback-Leibler散度与Cohen's Kappa相关联，限制了分类性能的最大限度

    

    机器学习分类算法的性能是通过估计指标来评估的，通常是从混淆矩阵中使用训练数据和交叉验证得出的。然而，这些并不证明已经实现了最佳性能。可以使用信息距离度量来估计错误率的基本限制。为此，混淆矩阵已被制定为符合Chernoff-Stein引理。这将错误率与描述两个类别的概率密度函数之间的Kullback-Leibler散度相关联。这导致了一个关键结果，将Cohen's Kappa与电阻器平均距离联系起来，这是两个Kullback-Leibler散度的并联电阻器组合。电阻器平均距离具有比特单位，可以从同一训练数据中使用分类算法估计的KullBack-Leibler散度的kNN估计中得出。

    arXiv:2403.01571v1 Announce Type: cross  Abstract: The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification
    
[^23]: 对深度生成模型的费舍尔信息度量进行近似用于检测离群分布

    Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection

    [https://arxiv.org/abs/2403.01485](https://arxiv.org/abs/2403.01485)

    本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法

    

    基于概率似然的深度生成模型，如基于评分的扩散模型和变分自动编码器，是近年来用于拟合高维数据分布（如图像、文本或音频）的先进机器学习模型之一。它们可以自然地应用于许多下游任务之一，即离群分布（OOD）检测。然而，Nalisnick等人的开创性工作表明，深度生成模型始终为OOD数据推断出比它们训练过的数据更高的对数似然，标志着一个悬而未决的问题。在这项工作中，我们分析了使用数据点对深度生成模型的参数梯度进行OOD检测，基于这样的简单直觉，即OOD数据的梯度范数应该大于训练数据。我们形式化地将梯度大小的度量量化为近似费舍尔信息度量。我们展示了费舍尔信息矩阵（FIM）具有较大的绝对值

    arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di
    
[^24]: 高斯过程预测与蒙特卡洛采样的融合

    Fusion of Gaussian Processes Predictions with Monte Carlo Sampling

    [https://arxiv.org/abs/2403.01389](https://arxiv.org/abs/2403.01389)

    本文提出了一种将高斯过程预测通过蒙特卡洛采样进行融合的新方法，旨在提高预测准确性。

    

    在科学和工程领域，我们经常使用旨在准确预测感兴趣变量的模型。认识到这些模型是对现实的近似，我们希望将多个模型应用于相同的数据并整合它们的结果。本文在贝叶斯范式内运行，依赖于高斯过程作为我们的模型。这些模型生成预测概率密度函数（pdf），目标是系统地整合它们，使用线性和对数线性汇总。我们引入了对于对数线性汇总的新方法，确定了高斯过程预测pdf的输入相关权重。通过蒙特卡洛采样从其后验中抽取权重样本实现pdf的聚合。通过使用合成数据集展示了这些方法的性能，以及基于线性汇总的方法。

    arXiv:2403.01389v1 Announce Type: new  Abstract: In science and engineering, we often work with models designed for accurate prediction of variables of interest. Recognizing that these models are approximations of reality, it becomes desirable to apply multiple models to the same data and integrate their outcomes. In this paper, we operate within the Bayesian paradigm, relying on Gaussian processes as our models. These models generate predictive probability density functions (pdfs), and the objective is to integrate them systematically, employing both linear and log-linear pooling. We introduce novel approaches for log-linear pooling, determining input-dependent weights for the predictive pdfs of the Gaussian processes. The aggregation of the pdfs is realized through Monte Carlo sampling, drawing samples of weights from their posterior. The performance of these methods, as well as those based on linear pooling, is demonstrated using a synthetic dataset.
    
[^25]: 大规模变分高斯状态空间模型

    Large-scale variational Gaussian state-space models

    [https://arxiv.org/abs/2403.01371](https://arxiv.org/abs/2403.01371)

    该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。

    

    我们介绍了一种用于状态空间模型的嵌套变分推断算法和结构化变分逼近方法，其中非线性动力学由高斯噪声驱动。值得注意的是，所提出的框架允许在没有采用对角高斯逼近的情况下有效地评估ELBO和低方差随机梯度估计，通过利用（i）通过动力学对隐状态进行边缘化的蒙特卡罗逼近的低秩结构，（ii）一个推断网络，该网络通过低秩精度矩阵更新来近似更新步骤，（iii）将当前和未来观测编码为伪观测--将近似平滑问题转换为（更简单的）近似滤波问题。整体而言，必要的统计信息和ELBO可以在$O（TL（Sr+S^2+r^2））$时间内计算，其中$T$是系列长度，$L$是状态空间维数，$S$是用于逼近的样本数量。

    arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
    
[^26]: 高维尾指数回归：以社交媒体病毒帖子文本分析为例

    High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media

    [https://arxiv.org/abs/2403.01318](https://arxiv.org/abs/2403.01318)

    提出了高维尾指数回归方法，利用正则化估计和去偏方法进行推断，支持理论的仿真研究，并在社交媒体病毒帖子文本分析中应用。

    

    受社交媒体病毒帖子的点赞分布（如点赞数量）经验性幂律的启发，我们引入了高维尾指数回归及其参数的估计和推断方法。我们提出了一种正则化估计量，证明了它的一致性，并推导了其收敛速度。为了进行推断，我们提出了去偏正则化估计，证明了去偏估计量的渐近正态性。仿真研究支持了我们的理论。这些方法被应用于对涉及 LGBTQ+ 话题的 X（原 Twitter）病毒帖子的文本分析。

    arXiv:2403.01318v1 Announce Type: cross  Abstract: Motivated by the empirical power law of the distributions of credits (e.g., the number of "likes") of viral posts in social media, we introduce the high-dimensional tail index regression and methods of estimation and inference for its parameters. We propose a regularized estimator, establish its consistency, and derive its convergence rate. To conduct inference, we propose to debias the regularized estimate, and establish the asymptotic normality of the debiased estimator. Simulation studies support our theory. These methods are applied to text analyses of viral posts in X (formerly Twitter) concerning LGBTQ+.
    
[^27]: 睡眠臂决策问题中接近最优的每次行动遗憾界

    Near-optimal Per-Action Regret Bounds for Sleeping Bandits

    [https://arxiv.org/abs/2403.01315](https://arxiv.org/abs/2403.01315)

    该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。

    

    我们推导了针对睡眠臂决策问题的接近最优每次行动遗憾界，其中敌手选择每轮可用臂的集合和它们的损失。在每轮至多有 $A$ 个可用臂的 $K$ 个总臂的情况下，已知的最好上界为 $O(K\sqrt{TA\ln{K}})$，通过间接最小化内部睡眠遗憾获得。与极小值 $\Omega(\sqrt{TA})$ 下界相比，这个上界包含额外的乘数因子 $K\ln{K}$。我们通过直接最小化每次行动遗憾，使用EXP3、EXP3-IX和带有Tsallis熵的FTRL的推广版本，从而获得了顺序为 $O(\sqrt{TA\ln{K}})$ 和 $O(\sqrt{T\sqrt{AK}})$ 的接近最优界。我们将结果扩展到了从睡眠专家获得建议的臂决策问题设置，同时推广了EXP4。这为现有的多个自适应和跟踪遗憾界的新证明铺平了道路。

    arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for 
    
[^28]: 可以用自信先验替代冷却后验吗？

    Can a Confident Prior Replace a Cold Posterior?

    [https://arxiv.org/abs/2403.01272](https://arxiv.org/abs/2403.01272)

    探讨了将后验调整替换为增加信心的先验分布的可行性，引入了实用的“DirClip”先验和“信心先验”，提供了对信心先验的一般见解。

    

    用于图像分类的基准数据集往往具有非常低的标签噪声水平。当贝叶斯神经网络在这些数据集上训练时，它们经常欠拟合，错误地表示数据的随机不确定性。一种常见的解决方案是调整后验概率，这样可以改善对训练数据的拟合，但从贝叶斯角度解释起来具有挑战性。我们探讨了后验调整是否可以被一种提高信心的先验分布替代。首先，我们引入了一个实用的采样“DirClip”先验，并且几乎与冷却后验的性能相匹配。其次，我们引入了一个“信心先验”，它在温度趋于零时直接近似于冷布局，但不能轻松抽样。最后，我们提供了一些关于提高信心的先验的一般见解，例如何时可能出现分歧以及如何通过微调来缓解数值不稳定性。

    arXiv:2403.01272v1 Announce Type: new  Abstract: Benchmark datasets used for image classification tend to have very low levels of label noise. When Bayesian neural networks are trained on these datasets, they often underfit, misrepresenting the aleatoric uncertainty of the data. A common solution is to cool the posterior, which improves fit to the training data but is challenging to interpret from a Bayesian perspective. We explore whether posterior tempering can be replaced by a confidence-inducing prior distribution. First, we introduce a "DirClip" prior that is practical to sample and nearly matches the performance of a cold posterior. Second, we introduce a "confidence prior" that directly approximates a cold likelihood in the limit of decreasing temperature but cannot be easily sampled. Lastly, we provide several general insights into confidence-inducing priors, such as when they might diverge and how fine-tuning can mitigate numerical instability.
    
[^29]: 具有Massart噪声的流式线性和修正线性系统的随机梯度下降

    Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise

    [https://arxiv.org/abs/2403.01204](https://arxiv.org/abs/2403.01204)

    我们提出了一种针对具有Massart噪声的线性和ReLU回归问题的随机梯度下降方法，具有新颖的近乎线性收敛保证，首次在流式设置中为鲁棒ReLU回归提供了收敛保证，并展示了其相比于以前的方法有改进的收敛速率。

    

    我们提出了SGD-exp，一种用于线性和ReLU回归的随机梯度下降方法，在Massart噪声（对抗性半随机破坏模型）下，完全流式设置下。我们展示了SGD-exp对真实参数的近乎线性收敛保证，最高可达50%的Massart破坏率，在对称无忧破坏情况下，任意破坏率也有保证。这是流式设置中鲁棒ReLU回归的第一个收敛保证结果，它显示了相比于以前的鲁棒方法对于L1线性回归具有改进的收敛速率，这是由于选择了指数衰减步长，这在实践中已被证明是有效的。我们的分析基于离散随机过程的漂移分析，这本身也可能是有趣的。

    arXiv:2403.01204v1 Announce Type: new  Abstract: We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to $50\%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.
    
[^30]: 一个镜子的库：低维深度神经网络是具有反射特征的凸Lasso模型

    A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features

    [https://arxiv.org/abs/2403.01046](https://arxiv.org/abs/2403.01046)

    证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。

    

    我们证明在1-D数据上训练神经网络等价于解决一个带有固定、明确定义的特征字典矩阵的凸Lasso问题。具体的字典取决于激活函数和深度。我们考虑具有分段线性激活函数的两层网络，深窄的ReLU网络最多有4层，以及具有符号激活和任意深度的矩形和树网络。有趣的是，在ReLU网络中，第四层创建代表训练数据关于自身的反射的特征。Lasso表示法揭示了全局最优网络和解空间的洞察。

    arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
    
[^31]: 论部分可观察序列团队和游戏中信息结构在强化学习中的作用

    On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games

    [https://arxiv.org/abs/2403.00993](https://arxiv.org/abs/2403.00993)

    明确表示信息结构是分析和解决强化学习问题的重要组成部分。

    

    在顺序决策问题中，信息结构描述了系统中不同时刻事件如何相互影响。本文主张明确表示信息结构是分析和解决强化学习问题的重要组成部分，并提出具有明确信息结构表示的新型强化学习模型。

    arXiv:2403.00993v1 Announce Type: cross  Abstract: In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of reinforcement learning (e.g., MDPs, POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure.   In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving reinforcement learning problems. We propose novel reinforcement learning models with an explicit representation of information structure, capturing 
    
[^32]: 利用互信息驱动的跨变量和时间建模来增强多元时间序列预测

    Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling

    [https://arxiv.org/abs/2403.00869](https://arxiv.org/abs/2403.00869)

    引入了CDAM和TAM模型以改进多元时间序列预测，通过最小化冗余信息并增强互信息，利用时间相关性。

    

    最近的进展强调了深度学习技术对多元时间序列预测（MTSF）的影响。通常，这些技术被分为两类：通道独立和通道混合方法。虽然通道独立方法通常产生更好的结果，但通道混合理论上可以通过利用变量间的相关性来提供改进。然而，我们认为在通道混合方法中整合不相关信息可能会削弱MTSF模型性能的潜在增强。为了证实这一观点，我们介绍了用于通道混合方法的跨变量去相关感知特征建模（CDAM），旨在通过最小化通道间的冗余信息同时增强相关的互信息来改进通道混合。此外，我们引入了时序相关感知建模（TAM）来利用时间相关性，这是一个步骤

    arXiv:2403.00869v1 Announce Type: new  Abstract: Recent advancements have underscored the impact of deep learning techniques on multivariate time series forecasting (MTSF). Generally, these techniques are bifurcated into two categories: Channel-independence and Channel-mixing approaches. Although Channel-independence methods typically yield better results, Channel-mixing could theoretically offer improvements by leveraging inter-variable correlations. Nonetheless, we argue that the integration of uncorrelated information in channel-mixing methods could curtail the potential enhancement in MTSF model performance. To substantiate this claim, we introduce the Cross-variable Decorrelation Aware feature Modeling (CDAM) for Channel-mixing approaches, aiming to refine Channel-mixing by minimizing redundant information between channels while enhancing relevant mutual information. Furthermore, we introduce the Temporal correlation Aware Modeling (TAM) to exploit temporal correlations, a step be
    
[^33]: NeuraLUT: 在Boolean合成函数中隐藏神经网络密度

    NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions

    [https://arxiv.org/abs/2403.00849](https://arxiv.org/abs/2403.00849)

    改进了FPGA加速神经网络推断任务的方法，提出将整个子网络映射到单个LUT中，使得神经网络拓扑和精度不再影响生成的查找表的大小。

    

    可编程门阵列（FPGA）加速器已经证明在处理延迟和资源关键的深度神经网络（DNN）推断任务方面取得了成功。神经网络中计算密集度最高的操作之一是特征和权重向量之间的点积。因此，一些先前的FPGA加速工作提出将具有量化输入和输出的神经元直接映射到查找表（LUTs）以进行硬件实现。在这些工作中，神经元的边界与LUTs的边界重合。我们建议放宽这些边界，将整个子网络映射到单个LUT。由于子网络被吸收到LUT中，分区内的神经网络拓扑和精度不会影响生成的查找表的大小。因此，我们在每个分区内使用具有浮点精度的全连接层，这些层受益于成为通用函数逼近器。

    arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
    
[^34]: 用高斯过程增强均值回归时间序列预测：金融预测中的功能和增强数据结构

    Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting

    [https://arxiv.org/abs/2403.00796](https://arxiv.org/abs/2403.00796)

    本论文通过使用高斯过程在金融预测中探索功能和增强数据结构，提供了一种能够预测整个概率分布并进行长期预测的方法，对于准确预测和决策制定具有重要意义

    

    在这篇论文中，我们探讨了利用高斯过程（GPs）来预测具有潜在结构的均值回归时间序列，使用相对未被探索的功能和增强数据结构。虽然许多传统的预测方法专注于时间序列数据的短期动态，但GPs提供了潜力，不仅可以预测平均预测值，还可以预测未来轨迹上整个概率分布。这在金融环境中特别有益，因为如果不正确的波动率评估导致资本损失，仅准确的预测可能不足够。此外，在交易选择中，GPs允许预测多个夏普比率，考虑交易成本后进行调整，有助于决策。本研究中使用的功能数据表示通过利用过去几年的信息使得可以进行更长期的预测，即使预测脱离了当前年份。

    arXiv:2403.00796v1 Announce Type: cross  Abstract: In this paper, we explore the application of Gaussian Processes (GPs) for predicting mean-reverting time series with an underlying structure, using relatively unexplored functional and augmented data structures. While many conventional forecasting methods concentrate on the short-term dynamics of time series data, GPs offer the potential to forecast not just the average prediction but the entire probability distribution over a future trajectory. This is particularly beneficial in financial contexts, where accurate predictions alone may not suffice if incorrect volatility assessments lead to capital losses. Moreover, in trade selection, GPs allow for the forecasting of multiple Sharpe ratios adjusted for transaction costs, aiding in decision-making. The functional data representation utilized in this study enables longer-term predictions by leveraging information from previous years, even as the forecast moves away from the current year
    
[^35]: 通过量子近似优化算法在尖峰张量模型中的统计估计

    Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm

    [https://arxiv.org/abs/2402.19456](https://arxiv.org/abs/2402.19456)

    该论文分析了量子近似优化算法在尖峰张量模型中的统计估计问题，发现了QAOA在恢复阈值方面的匹配性，并展示了渐近重叠分布的正弦-高斯定律。

    

    该论文分析了量子近似优化算法（QAOA）在统计估计问题，即尖峰张量模型上的表现，该模型在经典情况下展现出统计计算差距。我们证明了1步QAOA的弱恢复阈值与1步张量幂迭代的相匹配。额外的启发式计算表明，当$p$为一个固定常数时，$p$步QAOA的弱恢复阈值与$p$步张量幂迭代的相匹配。这进一步意味着通过张量展开的多步QAOA可以实现经典计算阈值$\Theta(n^{(q-2)/4})$，但不会超越尖峰$q$-张量的经典计算阈值。同时，我们表征了$p$步QAOA的渐近重叠分布，发现了通过仿真验证的有趣的正弦-高斯定律。

    arXiv:2402.19456v1 Announce Type: cross  Abstract: The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization. In this paper, we analyze the performance of the QAOA on a statistical estimation problem, namely, the spiked tensor model, which exhibits a statistical-computational gap classically. We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration. Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant. This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the classical computation threshold $\Theta(n^{(q-2)/4})$ for spiked $q$-tensors.   Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, finding an intriguing sine-Gaussian law verified through simulations. For some $p$ and $q$, the QAOA attains
    
[^36]: 统计学习的确定性和近似确定性模型

    Certain and Approximately Certain Models for Statistical Learning

    [https://arxiv.org/abs/2402.17926](https://arxiv.org/abs/2402.17926)

    可以直接从带有缺失值的数据中学习准确模型，构建了检查数据填充必要性的高效算法，并在不需要填充的情况下返回准确模型，显著减少数据填充所需的时间和精力

    

    现实世界中的数据通常是不完整的，并且包含缺失值。为了在真实世界的数据集上训练准确的模型，用户需要花费大量时间和资源填充和找到缺失数据项的正确值。本文表明，对于某些训练数据和目标模型，可以直接从具有缺失值的数据中学习准确的模型。我们提出了一种统一的方法，可以检查数据填充的必要性，以便在各种广泛使用的机器学习范例中学习准确的模型。我们构建了具有理论保证的高效算法来检查此必要性，并在不需要填充的情况下返回准确的模型。我们广泛的实验证明，我们提出的算法显著减少了数据填充所需的时间和精力，而没有带来相当大的计算开销。

    arXiv:2402.17926v1 Announce Type: cross  Abstract: Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.
    
[^37]: 当你的AI欺骗你：在奖励学习中人类评估者部分可观测性的挑战

    When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning

    [https://arxiv.org/abs/2402.17747](https://arxiv.org/abs/2402.17747)

    RLHF在考虑部分观察性时可能导致策略欺骗性地夸大性能或过度辩护行为，我们提出了数学条件来解决这些问题，并警告不要盲目应用RLHF在部分可观测情况下。

    

    强化学习从人类反馈（RLHF）的过去分析假设人类完全观察到环境。当人类反馈仅基于部分观察时会发生什么？我们对两种失败情况进行了正式定义：欺骗和过度辩护。通过将人类建模为对轨迹信念的Boltzmann-理性，我们证明了RLHF保证会导致策略欺骗性地夸大其性能、为了留下印象而过度辩护或者两者兼而有之的条件。为了帮助解决这些问题，我们数学地刻画了环境部分可观测性如何转化为（缺乏）学到的回报函数中的模糊性。在某些情况下，考虑环境部分可观测性使得在理论上可能恢复回报函数和最优策略，而在其他情况下，存在不可减少的模糊性。我们警告不要盲目应用RLHF在部分可观测情况下。

    arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
    
[^38]: Latent Attention for Linear Time Transformers

    Latent Attention for Linear Time Transformers

    [https://arxiv.org/abs/2402.17512](https://arxiv.org/abs/2402.17512)

    提出了一种基于潜在向量定义注意力的方法，将标准transformer中的注意力机制的时间复杂度从二次方降低到与时间线性相关，表现与标准注意力媲美，但允许上下文窗口扩展到远远超出标准的范围。

    

    标准transformer中的注意力机制的时间复杂度随着序列长度的增加呈二次方增长。我们引入一种通过定义潜在向量的注意力来将其降低到与时间线性相关的方法。该方法可以轻松作为标准注意力机制的替代品。我们的“Latte Transformer”模型可用于双向和单向任务，因果版本允许一种在推理语言生成任务中内存和时间高效的递归实现。标准transformer的下一个标记预测随着序列长度线性增长，而Latte Transformer计算下一个标记所需的时间是恒定的。我们的方法的实证表现可与标准注意力媲美，但允许将上下文窗口扩展到远远超出标准注意力实际可行的范围。

    arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
    
[^39]: 扩散模型中的相变揭示了数据的分层性质

    A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data

    [https://arxiv.org/abs/2402.16991](https://arxiv.org/abs/2402.16991)

    扩散模型在研究数据的分层生成模型中展示出了在阈值时间发生相变的特性，这影响了高级特征和低级特征的重建过程。

    

    理解真实数据的结构在推动现代深度学习方法方面至关重要。自然数据，如图像，被认为是由以层次和组合方式组织的特征组成的，神经网络在学习过程中捕捉到这些特征。最近的进展显示，扩散模型能够生成高质量的图像，暗示了它们捕捉到这种潜在结构的能力。我们研究了数据的分层生成模型中的这一现象。我们发现，在时间$t$后作用的反向扩散过程受到某个阈值时间处的相变控制，此时重建高级特征（如图像的类别）的概率突然下降。相反，低级特征（如图像的具体细节）的重建在整个扩散过程中平稳演变。这一结果暗示，在超出转变时间的时刻，类别已变化，但是基

    arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
    
[^40]: 具有交叉一致$p$-值的异常检测中的不确定性量化

    Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values

    [https://arxiv.org/abs/2402.16388](https://arxiv.org/abs/2402.16388)

    针对异常检测系统中不确定性量化的需求，提出了一种新颖的框架，称为交叉一致异常检测，通过校准模型的不确定性提供统计保证。

    

    随着可靠、可信和可解释机器学习的重要性日益增加，对异常检测系统进行不确定性量化的要求变得愈发重要。在这种情况下，有效控制类型I错误率($\alpha$)而又不损害系统的统计功率($1-\beta$)可以建立信任，并减少与假发现相关的成本，特别是当后续程序昂贵时。利用符合预测原则的方法有望通过校准模型的不确定性为异常检测提供相应的统计保证。该工作引入了一个新颖的异常检测框架，称为交叉一致异常检测，建立在为预测任务设计的著名交叉一致方法之上。通过这种方法，他填补了在归纳一致异常检测环境中扩展先前研究的自然研究空白

    arXiv:2402.16388v1 Announce Type: cross  Abstract: Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, rel
    
[^41]: 双稳健学习在处理效应估计中的结构不可知性最优性

    Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation

    [https://arxiv.org/abs/2402.14264](https://arxiv.org/abs/2402.14264)

    采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性

    

    平均处理效应估计是因果推断中最核心的问题，应用广泛。虽然文献中提出了许多估计策略，最近还纳入了通用的机器学习估计器，但这些方法的统计最优性仍然是一个开放的研究领域。本文采用最近引入的统计下界结构不可知框架，该框架对干扰函数没有结构性质假设，除了访问黑盒估计器以达到小误差；当只愿意考虑使用非参数回归和分类神谕作为黑盒子过程的估计策略时，这一点尤其吸引人。在这个框架内，我们证明了双稳健估计器对于平均处理效应（ATE）和平均处理效应的统计最优性。

    arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
    
[^42]: 无需稀疏模型的稀疏且准确的解释

    Sparse and Faithful Explanations Without Sparse Models

    [https://arxiv.org/abs/2402.09702](https://arxiv.org/abs/2402.09702)

    引入了稀疏解释值(SEV)，用于衡量机器学习模型的决策稀疏性。即使模型不是稀疏的，许多机器学习模型在SEV的衡量下仍具有低决策稀疏性。

    

    即使模型不满足全局的稀疏性，决策仍然可以用少量的特征准确地描述。例如，对于某人而言，尽管没有信用历史，但申请大笔贷款可能会被拒绝，这就忽视了与其信用价值相关的任何证据。在本论文中，我们引入了稀疏解释值（SEV），这是一种衡量机器学习模型稀疏性的新方法。在以上贷款拒绝的例子中，SEV为1，因为只需要一个因素来解释为什么贷款被拒绝。SEV是对决策稀疏性的衡量，而不是对整体模型稀疏性的衡量，并且我们能够证明许多机器学习模型——即使它们不是稀疏的——实际上在SEV的衡量下具有低决策稀疏性。SEV使用超立方体上的移动进行定义，使得SEV能够在各种模型类别上一致地定义，其中移动限制反映了模型的性质。

    arXiv:2402.09702v1 Announce Type: new  Abstract: Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflectin
    
[^43]: 无假设测试算法性能的限制

    The Limits of Assumption-free Tests for Algorithm Performance

    [https://arxiv.org/abs/2402.07388](https://arxiv.org/abs/2402.07388)

    这项研究探讨了使用有限数据量回答算法性能问题的基本限制，证明了黑盒测试方法无法准确回答算法在不同训练集上的整体性能和特定模型的性能问题。

    

    算法评价和比较是机器学习和统计学中基本的问题，一个算法在给定的建模任务中表现如何，哪个算法表现最佳？许多方法已经开发出来评估算法性能，通常基于交叉验证策略，将感兴趣的算法在不同的数据子集上重新训练，并评估其在留出数据点上的性能。尽管广泛使用这些程序，但对于这些方法的理论性质尚未完全理解。在这项工作中，我们探讨了在有限的数据量下回答这些问题的一些基本限制。特别地，我们区分了两个问题: 算法$A$在大小为$n$的训练集上学习问题有多好，以及在特定大小为$n$的训练数据集上运行$A$所产生的特定拟合模型有多好？我们的主要结果证明，对于任何将算法视为黑盒的测试方法，无法准确地回答这两个问题。

    Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
    
[^44]: 高维和高阶物理启发神经网络的 Hutchinson 迹估计

    Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks

    [https://arxiv.org/abs/2312.14499](https://arxiv.org/abs/2312.14499)

    介绍了 Hutchinson 迹估计（HTE），通过将整个 Hessian 矩阵的计算转换为 Hessian 矢量乘积（HVP），解决了 PINNs 处理高维和高阶 PDE 的挑战。

    

    arXiv:2312.14499v2 公告类型：替代交叉 摘要：物理启发神经网络（PINNs）已被证明在解决偏微分方程（PDEs）方面非常有效，特别是当一些数据可用时，通过无缝融合数据和物理学。然而，将PINNs扩展到高维甚至高阶PDE在自动微分在残差损失中的计算成本方面遇到了重大挑战。在这里，我们通过引入 Hutchinson 迹估计（HTE）来解决PINNs处理高维和高阶PDE的局限性。从科学计算中普遍存在的二阶高维PDE入手，HTE将整个Hessian矩阵的计算转换为Hessian矢量乘积（HVP）。这种方法通过 Taylor 模式自动微分减轻了计算瓶颈，并将内存消耗从Hessian矩阵减少到HVP。我们进一步展示了HTE收敛到或者

    arXiv:2312.14499v2 Announce Type: replace-cross  Abstract: Physics-Informed Neural Networks (PINNs) have proven effective in solving partial differential equations (PDEs), especially when some data are available by seamlessly blending data and physics. However, extending PINNs to high-dimensional and even high-order PDEs encounters significant challenges due to the computational cost associated with automatic differentiation in the residual loss. Herein, we address the limitations of PINNs in handling high-dimensional and high-order PDEs by introducing Hutchinson Trace Estimation (HTE). Starting with the second-order high-dimensional PDEs ubiquitous in scientific computing, HTE transforms the calculation of the entire Hessian matrix into a Hessian vector product (HVP). This approach alleviates the computational bottleneck via Taylor-mode automatic differentiation and significantly reduces memory consumption from the Hessian matrix to HVP. We further showcase HTE's convergence to the or
    
[^45]: 基于均值嵌入的分布式贝尔曼算子

    Distributional Bellman Operators over Mean Embeddings

    [https://arxiv.org/abs/2312.07358](https://arxiv.org/abs/2312.07358)

    提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。

    

    我们提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架。 我们基于这一框架推导出了几种新的动态规划和时序差分学习算法，提供了渐近收敛理论，并对这些算法在一系列表格任务上的实证表现进行了检验。此外，我们展示了这种方法可以与深度强化学习直接结合，得到一种新的深度强化学习代理，该代理在 Arcade Learning Environment 上优于基线分布式方法。

    arXiv:2312.07358v2 Announce Type: replace-cross  Abstract: We propose a novel algorithmic framework for distributional reinforcement learning, based on learning finite-dimensional mean embeddings of return distributions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence theory, and examine the empirical performance of the algorithms on a suite of tabular tasks. Further, we show that this approach can be straightforwardly combined with deep reinforcement learning, and obtain a new deep RL agent that improves over baseline distributional approaches on the Arcade Learning Environment.
    
[^46]: $t^3$-变分自动编码器：利用学生t分布和幂分歧学习重尾数据

    $t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence

    [https://arxiv.org/abs/2312.01133](https://arxiv.org/abs/2312.01133)

    通过引入学生t分布和幂分歧，提出了$t^3$VAE变分自动编码器框架，以更好地处理重尾数据，并推导出新的优化目标。

    

    变分自动编码器（VAE）通常采用标准正态先验作为概率潜在编码器的正则化器。然而，高斯尾部往往衰减得太快，无法有效容纳编码点，无法保留数据中隐藏的关键结构。本文探讨了使用重尾模型来抵抗过度正则化的方法。借鉴信息几何的见解，我们提出了$t^3$VAE，一种修改后的VAE框架，它将学生t分布结合到先验、编码器和解码器中。这导致了一个幂形式的联合模型分布，我们认为这可以更好地拟合真实数据集。我们通过重新表达证据下界为两个统计流形之间KL散度的联合优化，将其替换为$\gamma$-幂分歧，这是幂族的一个自然替代方法。$t^3$VAE展现出卓越的低

    arXiv:2312.01133v2 Announce Type: replace-cross  Abstract: The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-
    
[^47]: 自适应实验中半参数高效推断

    Semiparametric Efficient Inference in Adaptive Experiments

    [https://arxiv.org/abs/2311.18274](https://arxiv.org/abs/2311.18274)

    自适应实验中提出了一种半参数高效推断方法，通过中心极限定理和置信序列实现了更紧凑的推断，具有更广泛的适用性。

    

    我们考虑了一个问题，即在一个顺序实验中，治疗效应的高效推断，其中主导将受试验对象分配给治疗或对照组的策略可以随时间变化。我们首先为自适应增强反向概率加权估计器提供了一个中心极限定理，该估计器在文献中的假设比以往更弱。该中心极限定理使得在固定样本量下进行高效推断成为可能。我们随后考虑了顺序推断设定，推导出既包含渐近性质又包含非渐近性质的置信序列，这些序列明显比以前的方法更紧凑。这些任意有效的方法使得能够在数据相关的停止时间（样本量）下进行推断。此外，我们使用了最近的离线策略估计文献中的倾向分数截断技术，以降低估计器的有限样本方差而不影响其效率。

    arXiv:2311.18274v3 Announce Type: replace-cross  Abstract: We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting
    
[^48]: 基于椭圆对称分布混合的最大似然估计和聚类的非参数一致性

    Nonparametric consistency for maximum likelihood estimation and clustering based on mixtures of elliptically-symmetric distributions

    [https://arxiv.org/abs/2311.06108](https://arxiv.org/abs/2311.06108)

    展示了椭圆对称分布混合的最大似然估计的一致性，为基于非参数分布的聚类提供了理论依据。

    

    该论文展示了椭圆对称分布混合的最大似然估计器对其总体版本的一致性，其中潜在分布P是非参数的，并不一定属于估计器所基于的混合类别。当P是足够分离但非参数的分布混合时，表明了估计器的总体版本的组分对应于P的良好分离组分。这为在P具有良好分离子总体的情况下使用这样的估计器进行聚类分析提供了一些理论上的理据，即使这些子总体与混合模型所假设的不同。

    arXiv:2311.06108v2 Announce Type: replace-cross  Abstract: The consistency of the maximum likelihood estimator for mixtures of elliptically-symmetric distributions for estimating its population version is shown, where the underlying distribution $P$ is nonparametric and does not necessarily belong to the class of mixtures on which the estimator is based. In a situation where $P$ is a mixture of well enough separated but nonparametric distributions it is shown that the components of the population version of the estimator correspond to the well separated components of $P$. This provides some theoretical justification for the use of such estimators for cluster analysis in case that $P$ has well separated subpopulations even if these subpopulations differ from what the mixture model assumes.
    
[^49]: 高斯过程门控的分层专家混合模型

    Gaussian Process-Gated Hierarchical Mixtures of Experts

    [https://arxiv.org/abs/2302.04947](https://arxiv.org/abs/2302.04947)

    该论文提出了一种新颖的高斯过程门控的分层专家混合模型，通过使用GPs构建门控函数和专家，优于传统基于树的模型，同时在复杂性较低的情况下表现出良好性能，还提供了深层GPs和深度贝叶斯神经网络的可解释性。

    

    在这篇论文中，我们提出了一种新颖的高斯过程门控的分层专家混合模型（Gaussian Process-Gated Hierarchical Mixtures of Experts，GPHMEs）。与其他采用输入线性门控模型的专家混合模型不同，我们的模型采用了基于高斯过程（GPs）构建的门控函数。这些过程基于输入的非线性函数的随机特征。此外，我们模型中的专家也是用GPs构建的。GPHMEs的优化通过变分推断来实现。所提出的GPHMEs具有几个优点。它们优于在输入空间中对数据进行分区的基于树的HME基准，并且能够在减少复杂性的同时实现良好的性能。另一个优点是它们为深层GPs以及更一般的深度贝叶斯神经网络提供的可解释性。我们的GPHMEs在大规模数据集上展现了出色的性能，即使数据规模相当适中也是如此。

    arXiv:2302.04947v2 Announce Type: replace  Abstract: In this paper, we propose novel Gaussian process-gated hierarchical mixtures of experts (GPHMEs). Unlike other mixtures of experts with gating models linear in the input, our model employs gating functions built with Gaussian processes (GPs). These processes are based on random features that are non-linear functions of the inputs. Furthermore, the experts in our model are also constructed with GPs. The optimization of the GPHMEs is performed by variational inference. The proposed GPHMEs have several advantages. They outperform tree-based HME benchmarks that partition the data in the input space, and they achieve good performance with reduced complexity. Another advantage is the interpretability they provide for deep GPs, and more generally, for deep Bayesian neural networks. Our GPHMEs demonstrate excellent performance for large-scale data sets, even with quite modest sizes.
    
[^50]: Primal Dual Alternating Proximal Gradient算法用于具有耦合线性约束的非光滑非凸极小极大问题

    Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints

    [https://arxiv.org/abs/2212.04672](https://arxiv.org/abs/2212.04672)

    提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。

    

    非凸极小极大问题近年来在机器学习、信号处理和许多其他领域引起了广泛关注。本文提出了一种用于解决非光滑非凸（强）凹和非凸线性极小极大问题的原始对偶交替近端梯度（PDAPG）算法和原始对偶近端梯度（PDPG-L）算法，分别用于具有耦合线性约束的情况。这两种算法的迭代复杂度证明为 $\mathcal{O}\left( \varepsilon ^{-2} \right)$ （对应 $\mathcal{O}\left( \varepsilon ^{-4} \right)$）在非凸强凹 （对应非凸凹）情况下，以及 $\mathcal{O}\left( \varepsilon ^{-3} \right)$ 在非凸线性情况下，分别达到 $\varepsilon$-稳态点。据我们所知，它们是用于解决具有耦合线性约束的非凸极小极大问题的第一批具有迭代复杂度保证的算法。

    arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
    
[^51]: 解决模型为基础的离线强化学习的样本复杂性问题

    Settling the Sample Complexity of Model-Based Offline Reinforcement Learning

    [https://arxiv.org/abs/2204.05275](https://arxiv.org/abs/2204.05275)

    该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。

    

    本文关注离线强化学习（RL），它利用预先收集的数据进行学习，无需进一步探索。有效的离线RL应能适应分布转移和有限的数据覆盖。然而，先前的算法或分析要么受到次优样本复杂性的困扰，要么产生高昂的烧录成本以达到样本最优性，从而对样本匮乏应用中的高效离线RL构成障碍。

    arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We p
    
[^52]: 多边形未调整的朗之万算法：为神经网络创建稳定高效的自适应算法

    Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks

    [https://arxiv.org/abs/2105.13937](https://arxiv.org/abs/2105.13937)

    提出了一种基于多边形未调整的朗之万算法的新类别算法，名为TH$\varepsilon$O POULA（或简称为TheoPouLa），通过稳定性、非渐进分析和实验表明其在神经网络优化中具有卓越性能。

    

    我们提出了一种新的基于朗之万算法的算法类别，克服了当前用于微调深度学习模型的流行自适应优化器的许多已知缺陷。其理论基础依赖于近期对于具有单调系数的随机微分方程（SDEs）的欧拉多边形逼近的发展。因此，它继承了温和算法的稳定性特性，同时解决了神经网络中的其他已知问题，例如梯度消失。特别地，我们对这个新类别算法的收敛特性进行了非渐进分析和全面的理论保证，我们将这个算法命名为TH$\varepsilon$O POULA（或简称为TheoPouLa）。最后，我们展示了使用不同类型的深度学习模型进行的几个实验，结果表明TheoPouLa相对于许多流行的自适应优化算法具有卓越性能。

    arXiv:2105.13937v3 Announce Type: replace  Abstract: We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.
    
[^53]: 私人预测集

    Private Prediction Sets

    [https://arxiv.org/abs/2102.06202](https://arxiv.org/abs/2102.06202)

    该研究提出了一个基于符合性预测的框架，可以在保护个人隐私的同时返回可靠的不确定性量化的预测集。

    

    在涉及重要决策的现实环境中，部署机器学习系统通常需要可靠的不确定性量化和保护个人隐私。我们提出了一个框架，将这两个目标同时视为重要。我们的框架基于符合性预测，这种方法可以扩展预测模型，返回提供不确定性量化的预测集，这些集合可以证明以用户指定的概率（如90%）覆盖真实响应。当与经过私人训练的模型一起使用时，人们可能希望符合性预测会为生成的预测集提供隐私保证；不幸的是，情况并非如此。为了解决这一关键问题，我们开发了一种方法，该方法可以从任何预先训练的预测模型中输出差分私人预测集。我们的方法遵循分裂符合性预测的一般方法；我们使用保留数据

    arXiv:2102.06202v3 Announce Type: replace-cross  Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data 
    
[^54]: Hodge-Compositional 边缘高斯过程

    Hodge-Compositional Edge Gaussian Processes. (arXiv:2310.19450v1 [stat.ML])

    [http://arxiv.org/abs/2310.19450](http://arxiv.org/abs/2310.19450)

    本论文提出了一种新的方法用于对边缘集合上的函数进行建模，该方法基于Hodge分解开发了适用于不同应用场景的无散度和无旋度的高斯过程，并通过组合它们来表示任意边缘函数。实验结果表明这种方法在流动数据推断中具有潜在的实际应用价值。

    

    我们提出了一种基于边缘集合的2-复形结构（类似于图形，其中边缘可形成三角面）的函数建模的有原则的高斯过程（GPs）。这种方法适用于学习网络上的流动类型数据，其中边缘流可以通过离散的散度和旋度来表征。借鉴Hodge分解，我们首先开发了适用于各种应用的无散度和无旋游的边缘GPs。然后将它们组合起来创建Hodge-组合边缘GPs，这些GPs足够表达任何边缘函数。这些GPs便于对边缘函数的不同Hodge分量进行直接和独立的学习，使我们能够在超参数优化过程中捕捉它们的相关性。为了突显它们的实际潜力，我们将它们应用于货币兑换、海洋流动和供水网络中的流动数据推断，并将其与替代模型进行比较。

    We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
    
[^55]: 自适应操作员学习用于无限维贝叶斯逆问题

    Adaptive operator learning for infinite-dimensional Bayesian inverse problems. (arXiv:2310.17844v1 [math.NA])

    [http://arxiv.org/abs/2310.17844](http://arxiv.org/abs/2310.17844)

    该论文提出了一种自适应操作员学习框架，通过使用贪婪算法选择自适应点对预训练的近似模型进行微调，逐渐减少建模误差。这种方法可以在准确性和效率之间取得平衡，有助于有效解决贝叶斯逆问题中的计算问题。

    

    贝叶斯逆问题(BIPs)中的基本计算问题源于需要重复进行正向模型评估的要求。减少这种成本的一种常见策略是通过操作员学习使用计算效率高的近似方法替代昂贵的模型模拟，这受到了深度学习的最新进展的启发。然而，直接使用近似模型可能引入建模误差，加剧了逆问题已经存在的病态性。因此，在有效实施这些方法中，平衡准确性和效率至关重要。为此，我们开发了一个自适应操作员学习框架，可以通过强制在局部区域中准确拟合的代理逐渐减少建模误差。这是通过使用贪婪算法选择的自适应点在反演过程中对预训练的近似模型进行微调来实现的，该算法只需要少量的正向模型评估。

    The fundamental computational issues in Bayesian inverse problems (BIPs) governed by partial differential equations (PDEs) stem from the requirement of repeated forward model evaluations. A popular strategy to reduce such cost is to replace expensive model simulations by computationally efficient approximations using operator learning, motivated by recent progresses in deep learning. However, using the approximated model directly may introduce a modeling error, exacerbating the already ill-posedness of inverse problems. Thus, balancing between accuracy and efficiency is essential for the effective implementation of such approaches. To this end, we develop an adaptive operator learning framework that can reduce modeling error gradually by forcing the surrogate to be accurate in local areas. This is accomplished by fine-tuning the pre-trained approximate model during the inversion process with adaptive points selected by a greedy algorithm, which requires only a few forward model evaluat
    
[^56]: 退火和熵镜像下降之间的联系

    A connection between Tempering and Entropic Mirror Descent. (arXiv:2310.11914v1 [stat.CO])

    [http://arxiv.org/abs/2310.11914](http://arxiv.org/abs/2310.11914)

    本论文研究了退火（针对顺序蒙特卡罗; SMC）和熵镜像下降之间的联系，证明了退火SMC是应用于Kullback-Leibler（KL）散度的熵镜像下降的数值近似，并提出了改进方案。

    

    本论文探讨了退火（针对顺序蒙特卡罗; SMC）和熵镜像下降之间的联系，以从已知未归一化概率密度的目标概率分布中采样。我们证明了退火SMC是应用于Kullback-Leibler（KL）散度的熵镜像下降的数值近似，并获得了退火迭代的收敛速度。我们的结果从优化角度推动了退火迭代，表明退火可以用作Langevin算法的替代选择，以最小化KL散度。我们利用退火和镜像下降迭代之间的联系来证明SMC中常见的做法，并提出了文献中算法的改进方案。

    This paper explores the connections between tempering (for Sequential Monte Carlo; SMC) and entropic mirror descent to sample from a target probability distribution whose unnormalized density is known.  We establish that tempering SMC is a numerical approximation of entropic mirror descent applied to the Kullback-Leibler (KL) divergence and obtain convergence rates for the tempering iterates.  Our result motivates the tempering iterates from an optimization point of view, showing that tempering can be used as an alternative to Langevin-based algorithms to minimize the KL divergence.  We exploit the connection between tempering and mirror descent iterates to justify common practices in SMC and propose improvements to algorithms in literature.
    
[^57]: 使用自然梯度替代品优化分布

    Optimising Distributions with Natural Gradient Surrogates. (arXiv:2310.11837v1 [stat.ML])

    [http://arxiv.org/abs/2310.11837](http://arxiv.org/abs/2310.11837)

    本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。

    

    自然梯度方法已经被用于优化各种情况下的概率分布参数，通常能得到快速收敛的过程。然而，对于许多感兴趣的分布，计算自然梯度存在一些挑战。在这项工作中，我们提出了一种新的技术来解决这些问题，这涉及将优化重新定义为关于替代分布参数的优化，计算自然梯度很容易。我们给出了几个可以解释为应用这种技术的现有方法的例子，并提出了一种新的方法，可以将其应用于各种问题。我们的方法扩展了可以有效使用自然梯度的分布集合。此外，它快速、易于理解，可以使用标准的自动微分软件进行简单实现，并且不需要冗长的模型特定导数计算。我们在最大似然估计和变分推断上演示了我们的方法。

    Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and varia
    
[^58]: 带有噪声标签的局部图聚类

    Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])

    [http://arxiv.org/abs/2310.08031](http://arxiv.org/abs/2310.08031)

    本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。

    

    在机器学习问题中，对于带有额外节点信息（如文本、图像或标签）的图形的增加兴趣，促使了需要耗费大量资源处理整个图形的方法的流行。然而，对于从这样的数据中提取有用信息的快速局部方法（即不需要访问整个图形）的发展还很少。为此，我们提出了使用噪声节点标签作为额外节点信息的局部图聚类的研究。在这种设置下，节点根据所属簇的联属关系接收初始二进制标签：如果它们属于目标簇，则为1；否则为0。随后，这些标签的一部分会被翻转。我们研究了将噪声标签纳入局部图聚类的好处。通过构建带有这些标签的加权图形，我们研究了基于图扩散的局部聚类方法在原始图形和加权图形上的性能。从理论角度出发，

    The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider
    
[^59]: Orlicz后悔统一边界随机变量统计的方法及其在环境指标中的应用

    Orlicz regrets to consistently bound statistics of random variables with an application to environmental indicators. (arXiv:2310.05168v1 [math.ST])

    [http://arxiv.org/abs/2310.05168](http://arxiv.org/abs/2310.05168)

    本文提出了一种新型的Orlicz后悔方法，用于一致地界定随机变量的统计量上下界，通过灵活评估随机变量的尾行为。与传统方法不同，此方法采用了一致性评估，并得到了将其与发散风险度量等效的充分条件。

    

    针对随机变量的统计评估是设计更好的环境管理和恢复方案的主要议题。对于水质指标、洪涝和干旱水位等这些变量的上下界估计都很重要，应当在一个统一的数学框架中进行一致的评估。我们提出了一种新型的Orlicz后悔方法，用于一致地界定随机变量的统计量上下界。这里的一致性指的是上界和下界使用相同的系数和参数值进行评估，与迄今为止提出的某些风险度量不同。Orlicz后悔能够根据随机变量的尾行为灵活地评估其统计量。我们通过明确地将Orlicz后悔与发散风险度量联系起来，以更好地理解它们。我们得到了将Orlicz后悔和发散风险度量等效的充分条件。

    Evaluating environmental variables that vary stochastically is the principal topic for designing better environmental management and restoration schemes. Both the upper and lower estimates of these variables, such as water quality indices and flood and drought water levels, are important and should be consistently evaluated within a unified mathematical framework. We propose a novel pair of Orlicz regrets to consistently bound the statistics of random variables both from below and above. Here, consistency indicates that the upper and lower bounds are evaluated with common coefficients and parameter values being different from some of the risk measures proposed thus far. Orlicz regrets can flexibly evaluate the statistics of random variables based on their tail behavior. The explicit linkage between Orlicz regrets and divergence risk measures was exploited to better comprehend them. We obtain sufficient conditions to pose the Orlicz regrets as well as divergence risk measures, and furth
    
[^60]: 非光滑弱凸有限和耦合组合优化

    Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])

    [http://arxiv.org/abs/2310.03234](http://arxiv.org/abs/2310.03234)

    本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。

    

    本文研究了一类新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)。由于其在机器学习和人工智能领域的广泛应用以及其解决基于经验风险最小化的随机算法的局限性，FCCO引起了越来越多的关注。然而，目前对于FCCO的研究假设内外函数都是光滑的，限制了其能够解决更多种类的问题的潜力。我们的研究从非光滑弱凸FCCO的角度进行了扩展，其中外函数是弱凸且非递减的，内函数是弱凸的。我们分析了一种单循环算法，并确定其在找到Moreau环的ε-稳定点的复杂度。

    This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
    
[^61]: $G$-Mapper：学习Mapper构造中的覆盖

    $G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])

    [http://arxiv.org/abs/2309.06634](http://arxiv.org/abs/2309.06634)

    本论文介绍了一种基于统计检验和聚类算法的优化Mapper图覆盖的方法，通过分割覆盖选择生成了保留数据集本质的Mapper图。

    

    Mapper算法是拓扑数据分析(TDA)中一种反映给定数据集结构的可视化技术。Mapper算法需要调整多个参数以生成一个"好看的"Mapper图。该论文关注于选择覆盖参数。我们提出了一种通过根据正态性的统计检验反复分割覆盖来优化Mapper图的算法。我们的算法基于$G$-means聚类，通过迭代地进行Anderson-Darling检验来寻找$k$-means中最佳的簇数。我们的分割过程利用高斯混合模型，根据给定数据的分布精心选择覆盖。对于合成和真实数据集的实验表明，我们的算法生成的覆盖使Mapper图保留了数据集的本质。

    The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
    
[^62]: SA-Solver：用于快速采样扩散模型的随机亚当求解器

    SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])

    [http://arxiv.org/abs/2309.05019](http://arxiv.org/abs/2309.05019)

    本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。

    

    扩散概率模型在生成任务中取得了相当大的成功。由于从扩散概率模型中进行采样相当于解扩散随机微分方程或常微分方程，这是一项耗时的工作，因此提出了许多基于改进的微分方程求解器的快速采样方法。这些技术中的大部分方法都考虑解扩散常微分方程，因为它具有更好的效率。然而，随机采样可以在生成多样化和高质量数据方面提供额外的优势。在这项工作中，我们从两个方面进行了对随机采样的综合分析：方差控制的扩散随机微分方程和线性多步扩散随机微分方程求解器。基于我们的分析，我们提出了SA-Solver，它是一种改进的高效随机亚当方法，用于解扩散随机微分方程以生成高质量的数据。我们的实验结果显示，SA-Solver实现了：1）在少步采样中与现有最先进的采样方法相比，有改进或可比性能；2）SOTA FID分数。

    Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
    
[^63]: 通过独立分量拉普拉斯过程实现差分隐私的函数性摘要

    Differentially Private Functional Summaries via the Independent Component Laplace Process. (arXiv:2309.00125v1 [stat.ML])

    [http://arxiv.org/abs/2309.00125](http://arxiv.org/abs/2309.00125)

    本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。

    

    在这项工作中，我们提出了一种称为独立分量拉普拉斯过程（ICLP）机制的差分隐私函数性摘要的新机制。通过将感兴趣的函数性摘要视为真正无限维对象，并使用ICLP噪声来扰动它们，该新机制放宽了关于数据轨迹的假设，并相对于文献中的经典有限维子空间嵌入方法保留了更高的效用。我们在多个函数空间中验证了所提出机制的可行性。我们考虑了几个统计估计问题，并通过轻微过平滑摘要来证明隐私成本不会主导统计误差，并且在渐近情况下可以忽略。对合成和真实数据集的数值实验证明了所提出机制的有效性。

    In this work, we propose a new mechanism for releasing differentially private functional summaries called the Independent Component Laplace Process, or ICLP, mechanism. By treating the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over-smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism.
    
[^64]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^65]: FaIRGP:面向表面温度模拟的贝叶斯能量平衡模型

    FaIRGP: A Bayesian Energy Balance Model for Surface Temperatures Emulation. (arXiv:2307.10052v1 [stat.AP])

    [http://arxiv.org/abs/2307.10052](http://arxiv.org/abs/2307.10052)

    FaIRGP是一种新的数据驱动代理模型，它满足能量平衡模型的物理温度响应方程，同时具备了从观测中学习和进行推断的能力。

    

    代理模型或简化复杂气候模型是通过最小的计算资源生成关键气候量预测的地球系统模型。通过时间序列建模或更先进的机器学习技术，基于数据驱动的代理模型已成为一个有希望的研究方向，能够生成与最先进的地球系统模型在视觉上难以区分的空间分辨率气候响应。然而，它们缺乏物理上的可解释性，限制了它们的广泛应用。本文介绍了一种新的数据驱动代理模型FaIRGP，它满足能量平衡模型的物理温度响应方程。结果是一个既能够从观测中学习又具有坚实物理基础的代理模型，可用于对气候系统进行推断。此外，我们的贝叶斯方法提供了一种有原则且数学可行的方法来进行推断。

    Emulators, or reduced complexity climate models, are surrogate Earth system models that produce projections of key climate quantities with minimal computational resources. Using time-series modeling or more advanced machine learning techniques, data-driven emulators have emerged as a promising avenue of research, producing spatially resolved climate responses that are visually indistinguishable from state-of-the-art Earth system models. Yet, their lack of physical interpretability limits their wider adoption. In this work, we introduce FaIRGP, a data-driven emulator that satisfies the physical temperature response equations of an energy balance model. The result is an emulator that (i) enjoys the flexibility of statistical machine learning models and can learn from observations, and (ii) has a robust physical grounding with interpretable parameters that can be used to make inference about the climate system. Further, our Bayesian approach allows a principled and mathematically tractabl
    
[^66]: 使用基于图形平滑的Gibbs采样优化蛋白质适应性。

    Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing. (arXiv:2307.00494v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.00494](http://arxiv.org/abs/2307.00494)

    使用基于图形平滑的Gibbs采样方法（GGS）优化蛋白质适应性，消除了突变距离的限制，同时提高了搜索效率。该方法在发现高适应性蛋白质方面达到了最先进水平。

    

    能够设计出在给定任务上具有更高适应性的新型蛋白质对许多医学领域来说都是革命性的。然而，通过穷举搜索海量序列空间是不可行的。以前的方法将搜索限制在从参考序列的小突变半径范围内，但这样的启发式方法极大地限制了设计空间。我们的工作旨在消除突变距离的限制，同时实现高效的探索。我们提出了基于图形平滑的Gibbs采样（GGS），它通过迭代应用带有梯度的Gibbs来提出有利的突变，并使用基于图形平滑的方法去除导致假阳性的噪声梯度。我们的方法在训练集中发现了高适应性蛋白质，最多具有8个突变。我们通过研究GFP和AAV设计问题、消融试验和基准模型来阐明结果。

    The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS
    
[^67]: 具有局部可变测量尺度的随机变量的鲁棒统计比较

    Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement. (arXiv:2306.12803v1 [stat.ML])

    [http://arxiv.org/abs/2306.12803](http://arxiv.org/abs/2306.12803)

    本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。

    

    具有局部可变测量尺度的空间，在统计学和机器学习中是相当普遍的，比如说，具有不同缩放维度的多维结构。然而，如何正确地利用这些空间中编码的全部信息，仍然被认为是一个开放性问题。我们通过考虑一个基于随机变量期望的（集合）偏序关系来解决这个问题，这些随机变量映射到这些非标准空间中。当没有或完全的基数结构时，这个偏序关系包含随机优势和期望顺序作为极端情况。我们通过线性优化导出了一个适用于我们提出的广义随机优势（GSD）顺序的（正则化的）统计检验，并通过不精确概率模型使其更为鲁棒。我们的发现用多维贫困度量、金融和医学数据进行说明。

    Spaces with locally varying scale of measurement, like multidimensional structures with differently scaled dimensions, are pretty common in statistics and machine learning. Nevertheless, it is still understood as an open question how to exploit the entire information encoded in them properly. We address this problem by considering an order based on (sets of) expectations of random variables mapping into such non-standard spaces. This order contains stochastic dominance and expectation order as extreme cases when no, or respectively perfect, cardinal structure is given. We derive a (regularized) statistical test for our proposed generalized stochastic dominance (GSD) order, operationalize it by linear optimization, and robustify it by imprecise probability models. Our findings are illustrated with data from multidimensional poverty measurement, finance, and medicine.
    
[^68]: SGD中的投石机：训练损失中的尖峰及其通过特征学习对泛化的影响

    Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. (arXiv:2306.04815v1 [cs.LG])

    [http://arxiv.org/abs/2306.04815](http://arxiv.org/abs/2306.04815)

    本文通过研究SGD训练损失中的尖峰现象，提出了“投石机”优化现象，通过增加与真实预测器的平均梯度外积对齐来促进特征学习，并证明较小的批量大小可提高泛化性能。

    

    本文首先解释了神经网络在使用随机梯度下降（SGD）进行训练时为什么经常出现训练损失尖峰的现象。我们提供了证据表明，SGD训练损失中的尖峰是“投石机”，这是一种优化现象，最初在[Lewkowycz等人，2020年]的大学习率GD中观察到。我们通过实验证明这些投石机出现在由正切内核的前几个特征向量所张成的低维子空间中，适用于GD和SGD。其次，我们提出了一个解释，即投石机如何通过增加与真实预测器的平均梯度外积（AGOP）对齐来促进特征学习，从而实现更好的泛化。此外，我们证明，在SGD中，更小的批量大小会导致更多的投石机出现，从而提高AGOP对齐和测试性能。

    In this paper, we first present an explanation regarding the common occurrence of spikes in the training loss when neural networks are trained with stochastic gradient descent (SGD). We provide evidence that the spikes in the training loss of SGD are "catapults", an optimization phenomenon originally observed in GD with large learning rates in [Lewkowycz et al. 2020]. We empirically show that these catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel, for both GD and SGD. Second, we posit an explanation for how catapults lead to better generalization by demonstrating that catapults promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP) of the true predictor. Furthermore, we demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance.
    
[^69]: 使用反事实预测集设计决策支持系统

    Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])

    [http://arxiv.org/abs/2306.03928](http://arxiv.org/abs/2306.03928)

    本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。

    

    分类任务的决策支持系统通常被设计用于预测地面实况标签的值。然而，由于它们的预测并不完美，这些系统还需要让人类专家了解何时以及如何使用这些预测来更新自己的预测。不幸的是，这被证明是具有挑战性的。最近有人认为，另一种类型的决策支持系统可能会避开这个挑战。这些系统不是提供单个标签预测，而是使用符合预测器构建一组标签预测值，即预测集，并强制要求专家从预测集中预测一个标签值。然而，这些系统的设计和评估迄今仍依赖于样式化的专家模型，这引发了人们对它们的承诺的质疑。本文从在线学习的角度重新审视了这种系统的设计，并开发了一种不需要。

    Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
    
[^70]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^71]: 关于神经网络作为无限树状概率图模型的论文研究

    On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])

    [http://arxiv.org/abs/2305.17583](http://arxiv.org/abs/2305.17583)

    本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    

    深度神经网络(DNNs)缺乏概率图模型(PGMs)的精确语义和明确定义的概率解释。本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决这个问题。我们的研究揭示了DNNs在前向传播期间确实执行PGM推断的近似，这与曾经的神经网络描述为核机器或无限大小的高斯过程的现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似。潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
    
[^72]: 从未知奖励的演示中学习安全约束

    Learning Safety Constraints from Demonstrations with Unknown Rewards. (arXiv:2305.16147v1 [cs.LG])

    [http://arxiv.org/abs/2305.16147](http://arxiv.org/abs/2305.16147)

    CoCoRL是一种从不知道奖励的已知安全演示中推断约束的方法，可以用于Constrained Markov Decision Process（CMDP），并且对于几乎最优演示能够无误差收敛于真实的安全集。

    

    本文提出了一种新方法，Convex Constraint Learning for Reinforcement Learning (CoCoRL)，用于从一组已知安全演示中推断Constrained Markov Decision Process (CMDP)的共享约束。与以前的方法只限于已知奖励或完全已知环境动态的演示相比，CoCoRL可以从具有不同未知奖励的演示中学习约束，而无需了解环境动态。CoCoRL基于演示构建了一个凸安全集，即使是潜在的次优演示也能保证安全。对于几乎最优演示，CoCoRL能够无误差收敛于真实的安全集。我们在表格环境和一个包含多个约束的连续驾驶仿真中评估CoCoRL。CoCoRL学习到的限制导致了安全的驾驶行为，并可以转移到不同的任务和环境。与之相反，基于学习已知回报的替代方法无法推广到具有不同回报的新环境，突显了CoCoRL在不知道回报函数的情况下学习约束的重要性。

    We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a novel approach for inferring shared constraints in a Constrained Markov Decision Process (CMDP) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL converges to the true safe set with no policy regret. We evaluate CoCoRL in tabular environments and a continuous driving simulation with multiple constraints. CoCoRL learns constraints that lead to safe driving behavior and that can be transferred to different tasks and environments. In contrast, alternative methods based 
    
[^73]: 使用神经薛定谔桥实现非配对图像转换

    Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])

    [http://arxiv.org/abs/2305.15086](http://arxiv.org/abs/2305.15086)

    本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。

    

    扩散模型是一类生成模型，它通过模拟随机微分方程（SDE）从噪声生成数据。尽管扩散模型在最近取得了显著进展，但由于高斯先验假设，它们在非配对的图像转换任务中存在局限性。薛定谔桥是一种学习 SDE 以在两个任意分布之间转换的方法，被视为解决这个问题的一种有吸引力的解决方案。然而，迄今为止，薛定谔桥模型在高分辨率图像之间的非配对转换方面并不成功。在这项工作中，我们提出了非配对神经薛定谔桥（UNSB），它将薛定谔桥与对抗性训练和正则化相结合，以学习非配对数据之间的 SDE。我们证明了 UNSB 是可伸缩的，并且成功解决了各种非配对图像转换任务。

    Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
    
[^74]: 带有Fisher线性判别分析的近似最优领域自适应

    Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis. (arXiv:2302.14186v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.14186](http://arxiv.org/abs/2302.14186)

    本文提出了一种基于Fisher线性判别的领域自适应模型，该模型是两个假设的凸组合，可以在不访问任何单个源任务的直接信息的情况下计算最优分类器，并在基于EEG和ECG的分类设置中展示了其有效性。

    

    我们提出了一类基于Fisher线性判别（FLD）的模型，用于领域自适应。该类模型是两个假设的凸组合：i）代表先前看到的源任务的平均假设和ii）在新的目标任务上训练的假设。对于特定的生成设置，我们在0-1损失下导出了两种模型的最优凸组合，提出了一种可计算的逼近，并研究了各种参数设置对最优假设、假设i）和假设ii）之间相对风险的影响。我们展示了所提出的最优分类器在基于EEG和ECG的分类设置中的有效性，并认为可以在不访问任何单个源任务的直接信息的情况下计算最优分类器。最后我们讨论了进一步的应用、限制和可能的未来方向。

    We propose a class of models based on Fisher's Linear Discriminant (FLD) in the context of domain adaptation. The class is the convex combination of two hypotheses: i) an average hypothesis representing previously seen source tasks and ii) a hypothesis trained on a new target task. For a particular generative setting we derive the optimal convex combination of the two models under 0-1 loss, propose a computable approximation, and study the effect of various parameter settings on the relative risks between the optimal hypothesis, hypothesis i), and hypothesis ii). We demonstrate the effectiveness of the proposed optimal classifier in the context of EEG- and ECG-based classification settings and argue that the optimal classifier can be computed without access to direct information from any of the individual source tasks. We conclude by discussing further applications, limitations, and possible future directions.
    
[^75]: 论几何图神经网络表现力的研究

    On the Expressive Power of Geometric Graph Neural Networks. (arXiv:2301.09308v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09308](http://arxiv.org/abs/2301.09308)

    本文提出了几何版本的Weisfeiler-Leman测试(GWL)，可以区分几何图形，揭示了关键设计选择如何影响几何GNN的表现力

    

    通过 Weisfeiler-Leman (WL) 图同构测试，已经广泛研究了图神经网络 (GNNs) 的表现力。然而，标准的 GNNs 和 WL 框架不适用于嵌入欧几里得空间的几何图形，例如生物分子、材料和其他物理系统。在本文中，我们提出了 WL 测试的几何版本 (GWL)，以区分几何图形，同时尊重底层物理对称性：排列、旋转、反射和平移。我们使用 GWL 来表征具有不变或等变于物理对称性的几何 GNN 的表现力，以区分几何图形。GWL 揭示了关键设计选择如何影响几何 GNN 的表现力：(1) 不变层表现力有限，因为它们无法区分一跳相同的几何图形；(2) 等变层通过传播局部邻域之外的几何信息，区分更大类别的图形；(3)

    The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3)
    

