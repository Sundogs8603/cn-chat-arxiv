# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FastAMI -- a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics.](http://arxiv.org/abs/2305.03022) | FastAMI是一种用于大型数据集的聚类比较的快速方法，通过蒙特卡罗模拟来实现偶然性调整，相比于传统的基于排列的方法更准确。 |
| [^2] | [When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](http://arxiv.org/abs/2305.02997) | 这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。 |
| [^3] | [Majorizing Measures, Codes, and Information.](http://arxiv.org/abs/2305.02960) | 本文介绍了一种基于信息论的趋势测度定理视角，该视角将随机过程的有限性与索引度量空间元素的有效可变长度编码的存在性相关联。 |
| [^4] | [Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality.](http://arxiv.org/abs/2305.02955) | 该论文提出了一种受权重影响的计数赌博机(WTB)设置，通过Repeated Exposure Optimality(REO)来研究它。他们提出了一个算法来满足REO，并提供了最优的遗憾边界。 |
| [^5] | [Piecewise Normalizing Flows.](http://arxiv.org/abs/2305.02930) | 介绍了一种分段归一化流方法，将目标分布分成集群，并通过训练模拟复杂的多模态目标。这种方法可以更好地匹配标准正态基础分布的拓扑结构。 |
| [^6] | [FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization.](http://arxiv.org/abs/2305.02894) | 本文基于一致性优化（CBO）的思想，提出了一种新的解决方案，通过互动粒子系统实现对于聚类联邦学习中各个群组的有效模型训练. |
| [^7] | [Trainability barriers and opportunities in quantum generative modeling.](http://arxiv.org/abs/2305.02881) | 本文研究了量子生成模型的可训练性障碍，如荒芜高原和指数损失集中，使用隐式生成模型和明确损失会产生一种新的荒芜高原现象。最大均值差可以是低秩且可训练的或全局性且不可训练的。但是，可训练性所需的低秩损失通常不能区分高频和低频特征。 |
| [^8] | [Maximizing Submodular Functions for Recommendation in the Presence of Biases.](http://arxiv.org/abs/2305.02806) | 该论文研究了如何在存在偏见的情况下，通过最大化子模函数来优化推荐系统。先前研究指出，基于公平性约束的干预可以确保比例代表性，并在存在偏见时获得接近最优的效用。而本文则探讨了一组能够捕捉这种目的的子模函数。 |
| [^9] | [Interpretable Regional Descriptors: Hyperbox-Based Local Explanations.](http://arxiv.org/abs/2305.02780) | 本文介绍了一种可解释的区域描述符，它是一种模型无关的局部解释方法，通过描述超立方体来预测特征值可更改但不影响预测结果，并提供"即使是"参数，揭示决策的特征和偏差。 |
| [^10] | [Using interpretable boosting algorithms for modeling environmental and agricultural data.](http://arxiv.org/abs/2305.02699) | 本文介绍了如何使用可解释的提升算法来分析高维环境和农业数据。通过考虑组结构和使用两步提升方法，我们预测了农民在面对气候灾害时的财务脆弱性。重要的预测变量包括自然资产、灌溉类型和附近农场的作物损坏。交互作用也提高了预测能力。 |
| [^11] | [Impact Study of Numerical Discretization Accuracy on Parameter Reconstructions and Model Parameter Distributions.](http://arxiv.org/abs/2305.02663) | 本论文研究了数值离散化参数对光学纳米计量领域参数重建和模型参数分布的影响，确定了数值参数可以允许高精度重建。 |
| [^12] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^13] | [Variations on a Theme by Blahut and Arimoto.](http://arxiv.org/abs/2305.02650) | 本文提出了BA算法的一种新的修改，通过让乘数在每次迭代中通过一维求根来更新，这使得算法能够直接计算所需失真的RD函数，而无需像原始算法一样探索整个RD曲线。 |
| [^14] | [Joint Graph Learning and Model Fitting in Laplacian Regularized Stratified Models.](http://arxiv.org/abs/2305.02573) | 本文提出了一种联合图学习和模型拟合的通用方法，适用于广泛的LRSM问题，并在合成和真实世界数据集上实现了最先进的性能。 |
| [^15] | [Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA.](http://arxiv.org/abs/2305.02544) | 该研究开发出了一种近似线性时间的鲁棒PCA算法，具有接近最优的误差保证，并且还开发了一种单遍流式PCA算法，具有几乎线性的内存使用。 |
| [^16] | [Correcting for Interference in Experiments: A Case Study at Douyin.](http://arxiv.org/abs/2305.02542) | 该研究提出了一种新型的Monte-Carlo估计器，它能够修正在抖音双向内容市场平台上实验时的干扰问题，并在现场实验中将实验吞吐量提高了一倍。 |
| [^17] | [AutoML-GPT: Automatic Machine Learning with GPT.](http://arxiv.org/abs/2305.02499) | AutoML-GPT 是一种基于 GPT 的自动机器学习方法，利用大型语言模型动态地利用各种人工智能模型，自动化训练管道，节约了选择模型架构、优化算法和调整超参数的人力和时间成本。 |
| [^18] | [Semisupervised regression in latent structure networks on unknown manifolds.](http://arxiv.org/abs/2305.02473) | 本文提出了一种基于半监督回归的潜在结构网络模型，在未知机流形上使用流形学习和图嵌入技术进行响应预测，并为这些响应建立了收敛保证。 |
| [^19] | [Streaming PCA for Markovian Data.](http://arxiv.org/abs/2305.02456) | 本文提出了一种面向马尔可夫数据采样的流式PCA算法，并获得了该算法在整个数据集上的第一个尖锐率，提高了算法的效率。同时，本文提出的自适应方案在模拟和真实数据示例中表现良好。 |
| [^20] | [Reward Teaching for Federated Multi-armed Bandits.](http://arxiv.org/abs/2305.02441) | 本论文提出了一种基于奖励教学思想的联邦多臂老虎机设计，通过隐式本地奖励调整来指导客户端朝着全局最优性，队服务端提出了老虎机学习和目标教学任务进行了优化。 |
| [^21] | [Discovering Communication Pattern Shifts in Large-Scale Networks using Encoder Embedding and Vertex Dynamics.](http://arxiv.org/abs/2305.02381) | 本文提出了一种新的方法称为“时间编码嵌入”，可以以线性复杂度有效地嵌入大量图数据，并利用此方法在大型组织的通信网络中检测出了个体顶点、顶点社区和整体图结构的通信模式变化。 |
| [^22] | [Efficient estimation of weighted cumulative treatment effects by double/debiased machine learning.](http://arxiv.org/abs/2305.02373) | 提出了一种双重/去偏机器学习估计器，用于对时间至事件结果的加权平均治疗效应进行估计，解决了覆盖度不足的挑战，并通过证明其一致性和渐近正常性来验证其有效性。 |
| [^23] | [Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration.](http://arxiv.org/abs/2302.04250) | 本论文介绍了一种通过学习部分马尔可夫决策过程来进行上下文适应和探索的新方法，其使用变压器进行推理过程学习，考虑了模型假设空间，假设表示为小的马尔可夫决策过程，可以在性价比高的情况下进行动态规划。该方法在Symbolic Alchemy基准测试中表现出与精确后验抽样相近的适应速度和探索利用平衡。 |
| [^24] | [Combinatorial Inference on the Optimal Assortment in Multinomial Logit Models.](http://arxiv.org/abs/2301.12254) | 本文提出了一种基于多项式logit模型的推断框架，可以测试最优产品组合是否具有特定性质。 |
| [^25] | [Mathematical analysis of singularities in the diffusion model under the submanifold assumption.](http://arxiv.org/abs/2301.07882) | 本文提供了扩散模型中漂移项的数学分析。通过次流形假设，提出一种新的目标函数和相关的损失函数，可处理低维流形上的奇异数据分布，解决了均值漂移函数和得分函数渐近发散的问题。 |
| [^26] | [A Stochastic Proximal Polyak Step Size.](http://arxiv.org/abs/2301.04935) | 本文开发了一种正则化的随机梯度下降ProxSPS算法，相比随机Polyak步长（SPS）更稳定易调整，同时在图像分类任务中表现良好，可导致网络具有更小的权重参数。 |
| [^27] | [Unbiased Supervised Contrastive Learning.](http://arxiv.org/abs/2211.05568) | 本文提出了一种新的监督对比损失形式（epsilon-SupInfoNCE）以及一种新的去偏正则化损失（FairKL），旨在解决从有偏数据中学习无偏模型的问题。 |
| [^28] | [Domain Adaptation under Missingness Shift.](http://arxiv.org/abs/2211.02093) | 本文解决了领域自适应问题中缺失数据转移的情况，提出了DAMS方法。针对缺失数据指标不可用的情况，提供了理论结果，包括协变量转移被违反、最优源预测器可能比总是预测均值表现更差、最优目标预测器可被识别等。 |
| [^29] | [Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning.](http://arxiv.org/abs/2206.01162) | 本文提出一种通过核化Stein距离构建后验Coreset的MBRL方法，在放松转移模型高斯或Lipschitz的限制下表现出优异的性能，并且可以应用于大规模训练。 |
| [^30] | [A Cross Validation Framework for Signal Denoising with Applications to Trend Filtering, Dyadic CART and Beyond.](http://arxiv.org/abs/2201.02654) | 本论文提出了一种通用的交叉验证框架，可应用于信号去噪和回归分析中，其中经过交叉验证的版本可以达到与最佳调节版本几乎相同的收敛速率，具有一定的普适性和应用价值。 |
| [^31] | [Non-linear Functional Modeling using Neural Networks.](http://arxiv.org/abs/2104.09371) | 本文提出了一种基于神经网络的、适用于函数数据的新型非线性模型。我们提出了两种变体，旨在显式利用函数数据中固有的结构，并通过全面的模拟研究和实际数据示例证明了该方法的有效性。 |
| [^32] | [GTEA: Inductive Representation Learning on Temporal Interaction Graphs via Temporal Edge Aggregation.](http://arxiv.org/abs/2009.05266) | 本文提出了 GTEA框架，用于在时序交互图上进行归纳学习，结合了时间动态建模和图嵌入，通过聚合相邻节点和边嵌入的特征，共同学习了 TIG 的拓扑和时间依赖关系，而且引入了一种稀疏感知的自注意机制，在多个时间序列预测任务中表现出有效性。 |
| [^33] | [Vertex Nomination in Richly Attributed Networks.](http://arxiv.org/abs/2005.02151) | 本文探讨了富有属性网络中顶点提名的双重作用，并提出了一种新颖的基于内容感知的网络嵌入方法，证明该方法优于现有的不利用内容和上下文的顶点提名方法。 |
| [^34] | [Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions.](http://arxiv.org/abs/2004.08318) | 本文研究了基于结果抽样的因果推断，发现强无偏性不总是像随机抽样下那样强大，并且某些单调性假设在锐利识别间隔方面产生可比较的结果。 通过算法推断出参数并用实证例子证明了方法的贡献。 |

# 详细

[^1]: FastAMI -- 一种蒙特卡罗方法用于聚类比较度量中的偶然性调整

    FastAMI -- a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics. (arXiv:2305.03022v1 [cs.LG])

    [http://arxiv.org/abs/2305.03022](http://arxiv.org/abs/2305.03022)

    FastAMI是一种用于大型数据集的聚类比较的快速方法，通过蒙特卡罗模拟来实现偶然性调整，相比于传统的基于排列的方法更准确。

    

    聚类是机器学习的核心，随着数据可用性的增加，其应用日益增多。然而，随着数据集的增长，带有偶然性调整的聚类比较变得计算困难，导致没有偏见的真实比较和解决方案选择。我们提出了FastAMI，一种基于蒙特卡罗的方法，用于高效地估计经过调整的互信息（AMI）并将其扩展到标准化互信息（SMI）。与准确计算相比，我们的方法足够快，可以为大型数据集启用这些带有调整的信息论比较，同时保持比配对方法更准确的结果。

    Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach.
    
[^2]: 神经网络何时在表格数据上胜过增强树？

    When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])

    [http://arxiv.org/abs/2305.02997](http://arxiv.org/abs/2305.02997)

    这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。

    

    表格数据是机器学习中最常用的数据类型之一。尽管神经网络（NN）在表格数据上取得了最近的进展，但人们仍在积极讨论NN是否通常优于梯度提升决策树（GBDT）在表格数据上的表现，一些最近的工作要么认为GBDT在表格数据上一贯优于NN，要么认为NN优于GBDT。在这项工作中，我们退一步问：'这重要吗？'我们通过对176个数据集比较19种算法，进行了迄今为止最大的表格数据分析，并发现'NN vs. GBDT'争论被过分强调：令人惊讶的是，在相当多的数据集中，GBDT和NN之间的性能差异要么可以忽略不计，要么GBDT的轻微超参数调整比选择最佳算法更重要。接下来，我们分析了965个元特征，以确定数据集的哪些特性使NN或GBDT更适合表现良好。例如，我们发现GBDT要比NN在高维稀疏数据上表现更好。

    Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
    
[^3]: Majorizing Measures, Codes, and Information（测度主导、码和信息）

    Majorizing Measures, Codes, and Information. (arXiv:2305.02960v1 [cs.IT])

    [http://arxiv.org/abs/2305.02960](http://arxiv.org/abs/2305.02960)

    本文介绍了一种基于信息论的趋势测度定理视角，该视角将随机过程的有限性与索引度量空间元素的有效可变长度编码的存在性相关联。

    

    Fernique和Talagrand的趋势测度定理是随机过程理论中的一个基本结果。它将度量空间中元素索引的随机过程的有限性与来自某些多尺度组合结构（如填充和覆盖树）的复杂性度量相关联。本文在Andreas Maurer的一份鲜为人知的预印本中首次概述的思路上构建一种基于信息论的趋势测度定理视角，根据该视角，随机过程的有限性是用索引度量空间元素的有效可变长度编码的存在性来表述的。

    The majorizing measure theorem of Fernique and Talagrand is a fundamental result in the theory of random processes. It relates the boundedness of random processes indexed by elements of a metric space to complexity measures arising from certain multiscale combinatorial structures, such as packing and covering trees. This paper builds on the ideas first outlined in a little-noticed preprint of Andreas Maurer to present an information-theoretic perspective on the majorizing measure theorem, according to which the boundedness of random processes is phrased in terms of the existence of efficient variable-length codes for the elements of the indexing metric space.
    
[^4]: 受权重影响的计数赌博机: 通过重复暴露来克服不可解性

    Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality. (arXiv:2305.02955v1 [stat.ML])

    [http://arxiv.org/abs/2305.02955](http://arxiv.org/abs/2305.02955)

    该论文提出了一种受权重影响的计数赌博机(WTB)设置，通过Repeated Exposure Optimality(REO)来研究它。他们提出了一个算法来满足REO，并提供了最优的遗憾边界。

    

    在在线学习的推荐系统或众包应用中，人类的偏好或能力通常是算法最近行动的一个函数。 相关工作已经形式化了设置，在这些设置中，行动的损失是最近$m$个时间步中该行动的播放次数的函数，其中$m$对应于人类记忆能力的上限。 为了更忠实地反映人类记忆随时间的衰减，我们引入了受权重影响的计数赌博机(WTB)，它通过要求行动损失是最近$m$个时间步中该臂被玩的次数的加权总和的函数来概括这个设置。除非进一步假设，否则WTB设置是不可解的。因此，我们在Repeated Exposure Optimality(REO)下研究了它，该条件是受人体生理学文献的启发，它要求存在一种行动，当反复播放时，最终将产生比任何其他行动更小的损失。 我们提出了一种算法，满足WTB设置下的REO，并提供了最优地缩放$m$和行动集大小的遗憾边界。 我们的证明技术要求在一种解耦形式下进行新颖的浓度结果，这可能是独立感兴趣的。

    In recommender system or crowdsourcing applications of online learning, a human's preferences or abilities are often a function of the algorithm's recent actions. Motivated by this, a significant line of work has formalized settings where an action's loss is a function of the number of times that action was recently played in the prior $m$ timesteps, where $m$ corresponds to a bound on human memory capacity. To more faithfully capture decay of human memory with time, we introduce the Weighted Tallying Bandit (WTB), which generalizes this setting by requiring that an action's loss is a function of a \emph{weighted} summation of the number of times that arm was played in the last $m$ timesteps. This WTB setting is intractable without further assumption. So we study it under Repeated Exposure Optimality (REO), a condition motivated by the literature on human physiology, which requires the existence of an action that when repetitively played will eventually yield smaller loss than any othe
    
[^5]: 分段归一化流

    Piecewise Normalizing Flows. (arXiv:2305.02930v1 [stat.ML])

    [http://arxiv.org/abs/2305.02930](http://arxiv.org/abs/2305.02930)

    介绍了一种分段归一化流方法，将目标分布分成集群，并通过训练模拟复杂的多模态目标。这种方法可以更好地匹配标准正态基础分布的拓扑结构。

    

    归一化流是一种通过从基础分布进行可逆转换来对复杂概率密度进行建模的成熟方法。然而，目标分布能否精确地被归一化流所捕捉，强烈受到基础分布的拓扑结构的影响。目标和基础分布之间的拓扑不匹配可能导致性能差，如对于多模态问题。一些不同的工作试图通过使用高斯混合模型 [Izmailov et al., 2020、Ardizzone et al., 2020、Hagemann and Neumayer, 2021] 或学习接受/拒绝采样 [Stimper et al., 2022] 来修改基础分布的拓扑结构以更好地匹配目标分布。我们引入了分段归一化流，将目标分布分成集群，并训练一系列流来模拟复杂的多模态目标。

    Normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. However, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. A mismatch between the topology of the target and the base can result in a poor performance, as is the case for multi-modal problems. A number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of Gaussian Mixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann and Neumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. We introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. The piecewise nat
    
[^6]: 利用一致性优化实现集群联邦学习的组共识

    FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization. (arXiv:2305.02894v1 [cs.LG])

    [http://arxiv.org/abs/2305.02894](http://arxiv.org/abs/2305.02894)

    本文基于一致性优化（CBO）的思想，提出了一种新的解决方案，通过互动粒子系统实现对于聚类联邦学习中各个群组的有效模型训练.

    

    联邦学习是现代机器学习中的重要框架，旨在整合来自多个用户的学习模型的训练，每个用户都有自己的本地数据集，以满足数据隐私和通信丢失约束。在聚类联邦学习中，假设用户之间存在附加的未知群组结构，并且目标是训练对每个群组有用的模型，而不仅仅是为所有用户训练一个单一的全局模型。本文提出了一种新的解决方案，通过一致性优化（CBO）的思想，解决了聚类联邦学习的问题。我们的新型CBO类型方法基于一个互动粒子系统，忽略了群组成员资格。我们的模型得到了严格的数学推理支持，包括描述我们的粒子系统大量粒子极限的平均场分析，以及同时全局优化的收敛保证。

    Federated learning is an important framework in modern machine learning that seeks to integrate the training of learning models from multiple users, each user having their own local data set, in a way that is sensitive to data privacy and to communication loss constraints. In clustered federated learning, one assumes an additional unknown group structure among users, and the goal is to train models that are useful for each group, rather than simply training a single global model for all users. In this paper, we propose a novel solution to the problem of clustered federated learning that is inspired by ideas in consensus-based optimization (CBO). Our new CBO-type method is based on a system of interacting particles that is oblivious to group memberships. Our model is motivated by rigorous mathematical reasoning, including a mean field analysis describing the large number of particles limit of our particle system, as well as convergence guarantees for the simultaneous global optimization
    
[^7]: 量子生成建模中的可训练性障碍和机遇

    Trainability barriers and opportunities in quantum generative modeling. (arXiv:2305.02881v1 [quant-ph])

    [http://arxiv.org/abs/2305.02881](http://arxiv.org/abs/2305.02881)

    本文研究了量子生成模型的可训练性障碍，如荒芜高原和指数损失集中，使用隐式生成模型和明确损失会产生一种新的荒芜高原现象。最大均值差可以是低秩且可训练的或全局性且不可训练的。但是，可训练性所需的低秩损失通常不能区分高频和低频特征。

    

    量子生成模型提供了本质高效的采样策略，因此在量子硬件上实现近期优势有很大的潜力。然而，它们的可扩展性仍存在重要问题。本文研究量子生成模型的可训练性障碍，如荒芜高原和指数损失集中。我们探索了明确和隐含模型、损失之间的相互作用，并表明使用隐式生成模型（如基于量子电路的模型）和明确损失（如KL散度）会产生一种新的荒芜高原现象。相比之下，最大均值差（MMD），作为隐式损失的一个流行例子，可以看作是一个观测量的期望值，该观测量可能是低秩且可训练的，也可能是全局性且不可训练的，具体取决于核函数的选择。然而，我们同时强调，可训练性所需的低秩损失通常不能区分高频和低频特征。

    Quantum generative models, in providing inherently efficient sampling strategies, show promise for achieving a near-term advantage on quantum hardware. Nonetheless, important questions remain regarding their scalability. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using implicit generative models (such as quantum circuit-based models) with explicit losses (such as the KL divergence) leads to a new flavour of barren plateau. In contrast, the Maximum Mean Discrepancy (MMD), which is a popular example of an implicit loss, can be viewed as the expectation value of an observable that is either low-bodied and trainable, or global and untrainable depending on the choice of kernel. However, in parallel, we highlight that the low-bodied losses required for trainability cannot in general distinguish hig
    
[^8]: 在存在偏见的情况下，最大化子模函数用于推荐系统

    Maximizing Submodular Functions for Recommendation in the Presence of Biases. (arXiv:2305.02806v1 [cs.LG])

    [http://arxiv.org/abs/2305.02806](http://arxiv.org/abs/2305.02806)

    该论文研究了如何在存在偏见的情况下，通过最大化子模函数来优化推荐系统。先前研究指出，基于公平性约束的干预可以确保比例代表性，并在存在偏见时获得接近最优的效用。而本文则探讨了一组能够捕捉这种目的的子模函数。

    

    子集选择任务在推荐系统和搜索引擎中经常出现，要求选择一些最大化用户价值的物品子集。子集的价值往往呈现出递减的回报，因此，使用子模函数来建模。然而，在许多应用中，发现输入具有社会偏见，会降低输出子集的效用，因此需要干预以提高其效用。本文研究了一组子模函数的最大化，这些函数涵盖了上述应用中出现的函数。

    Subset selection tasks, arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user. The values of subsets often display diminishing returns, and hence, submodular functions have been used to model them. If the inputs defining the submodular function are known, then existing algorithms can be used. In many applications, however, inputs have been observed to have social biases that reduce the utility of the output subset. Hence, interventions to improve the utility are desired. Prior works focus on maximizing linear functions -- a special case of submodular functions -- and show that fairness constraint-based interventions can not only ensure proportional representation but also achieve near-optimal utility in the presence of biases. We study the maximization of a family of submodular functions that capture functions arising in the aforementioned applications. Our first result is that, unlike linear functions, constraint-ba
    
[^9]: 可解释的区域描述符：基于超立方体的局部解释

    Interpretable Regional Descriptors: Hyperbox-Based Local Explanations. (arXiv:2305.02780v1 [stat.ML])

    [http://arxiv.org/abs/2305.02780](http://arxiv.org/abs/2305.02780)

    本文介绍了一种可解释的区域描述符，它是一种模型无关的局部解释方法，通过描述超立方体来预测特征值可更改但不影响预测结果，并提供"即使是"参数，揭示决策的特征和偏差。

    

    本文介绍了一种用于模型无关的局部解释的可解释的区域描述符（IRDs），它们是描述观测值特征值可更改而不影响其预测的超立方体。通过提供一组“即使是”参数（半事实的解释），它们证明了一个预测，并指出哪些特征影响了预测以及是否存在点偏差或不可信。一个具体的用例展示了它对于机器学习模型的构建者和决策受影响人员都是有价值的。我们将IRDs的搜索形式化为一个优化问题，并引入了一个计算IRDs的统一框架，包括期望、初始化技术和后处理方法。我们展示了如何将现有的超立方体方法适应到这个统一框架中。一项基准研究比较了基于多个质量指标的方法，并确定了两种改进IRDs的策略。

    This work introduces interpretable regional descriptors, or IRDs, for local, model-agnostic interpretations. IRDs are hyperboxes that describe how an observation's feature values can be changed without affecting its prediction. They justify a prediction by providing a set of "even if" arguments (semi-factual explanations), and they indicate which features affect a prediction and whether pointwise biases or implausibilities exist. A concrete use case shows that this is valuable for both machine learning modelers and persons subject to a decision. We formalize the search for IRDs as an optimization problem and introduce a unifying framework for computing IRDs that covers desiderata, initialization techniques, and a post-processing method. We show how existing hyperbox methods can be adapted to fit into this unified framework. A benchmark study compares the methods based on several quality measures and identifies two strategies to improve IRDs.
    
[^10]: 使用可解释的提升算法建模环境和农业数据

    Using interpretable boosting algorithms for modeling environmental and agricultural data. (arXiv:2305.02699v1 [stat.ML])

    [http://arxiv.org/abs/2305.02699](http://arxiv.org/abs/2305.02699)

    本文介绍了如何使用可解释的提升算法来分析高维环境和农业数据。通过考虑组结构和使用两步提升方法，我们预测了农民在面对气候灾害时的财务脆弱性。重要的预测变量包括自然资产、灌溉类型和附近农场的作物损坏。交互作用也提高了预测能力。

    

    我们阐述了如何使用基于岭正则化广义线性模型的解释性提升算法来分析高维环境数据。我们以智利和突尼斯的农民在面对气候灾害时的财务脆弱性为例，使用环境、社会、人类和生物物理数据进行预测。我们展示了如何考虑组结构以及如何在高维数据集中找到交互作用，使用一种新的两步提升方法。所提出方法的优点和功效都得到了实证和讨论。结果表明，在两步提升中引入交互作用可以提高预测能力。在预测所有类型的脆弱性方面，最重要的变量是自然资产。其他重要变量包括灌溉类型、经济资产和附近农场作物损坏的存在。

    We describe how interpretable boosting algorithms based on ridge-regularized generalized linear models can be used to analyze high-dimensional environmental data. We illustrate this by using environmental, social, human and biophysical data to predict the financial vulnerability of farmers in Chile and Tunisia against climate hazards. We show how group structures can be considered and how interactions can be found in high-dimensional datasets using a novel 2-step boosting approach. The advantages and efficacy of the proposed method are shown and discussed. Results indicate that the presence of interaction effects only improves predictive power when included in two-step boosting. The most important variable in predicting all types of vulnerabilities are natural assets. Other important variables are the type of irrigation, economic assets and the presence of crop damage of near farms.
    
[^11]: 数值离散化精度对参数重建和模型参数分布的影响研究

    Impact Study of Numerical Discretization Accuracy on Parameter Reconstructions and Model Parameter Distributions. (arXiv:2305.02663v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.02663](http://arxiv.org/abs/2305.02663)

    本论文研究了数值离散化参数对光学纳米计量领域参数重建和模型参数分布的影响，确定了数值参数可以允许高精度重建。

    

    数值模型在光学纳米计量领域的参数重建中得到广泛应用。通过使用贝叶斯目标向量优化方法将有限元数值模型拟合到实验数据集，可以获得纳米结构线光栅的几何参数。在重建过程中，使用高斯过程代理模型进行训练。然后，我们利用马尔可夫链蒙特卡罗抽样器对代理模型进行抽样，以确定重建模型参数的完整模型参数分布。数值离散化参数的选择，如有限元解答函数的多项式阶数，影响正演模型的数值离散化误差。在本研究中，我们研究了正演问题的数值离散化参数对重构参数以及模型参数分布的影响。我们表明这样的收敛研究可以确定允许高精度重建的数值参数。

    Numerical models are used widely for parameter reconstructions in the field of optical nano metrology. To obtain geometrical parameters of a nano structured line grating, we fit a finite element numerical model to an experimental data set by using the Bayesian target vector optimization method. Gaussian process surrogate models are trained during the reconstruction. Afterwards, we employ a Markov chain Monte Carlo sampler on the surrogate models to determine the full model parameter distribution for the reconstructed model parameters. The choice of numerical discretization parameters, like the polynomial order of the finite element ansatz functions, impacts the numerical discretization error of the forward model. In this study we investigate the impact of numerical discretization parameters of the forward problem on the reconstructed parameters as well as on the model parameter distributions. We show that such a convergence study allows to determine numerical parameters which allow for
    
[^12]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^13]: Blahut和Arimoto的主题变体

    Variations on a Theme by Blahut and Arimoto. (arXiv:2305.02650v1 [cs.IT])

    [http://arxiv.org/abs/2305.02650](http://arxiv.org/abs/2305.02650)

    本文提出了BA算法的一种新的修改，通过让乘数在每次迭代中通过一维求根来更新，这使得算法能够直接计算所需失真的RD函数，而无需像原始算法一样探索整个RD曲线。

    

    Blahut-Arimoto（BA）算法在计算速率失真（RD）函数方面起着基础性作用，该算法通过交替最小化带有固定乘数的Lagrangian具有理想的单调收敛属性。在本文中，我们提出了BA算法的新颖修改，使乘数每次迭代通过相对于单调单变量函数的一维求根步骤更新，这可以通过牛顿法有效实现。这允许以灵活和高效的方式更新乘数，克服了原始BA算法的一个主要缺点，其中乘数在整个迭代过程中都是固定的。因此，修改后的算法能够直接计算所需失真的RD函数，而不像原始BA算法一样探索整个RD曲线。理论分析表明，修改后的算法仍会收敛到RD函数。

    The Blahut-Arimoto (BA) algorithm has played a fundamental role in the numerical computation of rate-distortion (RD) functions. This algorithm possesses a desirable monotonic convergence property by alternatively minimizing its Lagrangian with a fixed multiplier. In this paper, we propose a novel modification of the BA algorithm, letting the multiplier be updated in each iteration via a one-dimensional root-finding step with respect to a monotonic univariate function, which can be efficiently implemented by Newton's method. This allows the multiplier to be updated in a flexible and efficient manner, overcoming a major drawback of the original BA algorithm wherein the multiplier is fixed throughout iterations. Consequently, the modified algorithm is capable of directly computing the RD function for a given target distortion, without exploring the entire RD curve as in the original BA algorithm. A theoretical analysis shows that the modified algorithm still converges to the RD function a
    
[^14]: 拉普拉斯正则化分层模型中的联合图学习和模型拟合

    Joint Graph Learning and Model Fitting in Laplacian Regularized Stratified Models. (arXiv:2305.02573v1 [stat.ML])

    [http://arxiv.org/abs/2305.02573](http://arxiv.org/abs/2305.02573)

    本文提出了一种联合图学习和模型拟合的通用方法，适用于广泛的LRSM问题，并在合成和真实世界数据集上实现了最先进的性能。

    

    拉普拉斯正则化分层模型（LRSM）是利用子问题的显式或隐式网络结构，由分类特征称为层（例如年龄、区域、时间、预测时间、等），并从相邻层中获取数据以增强每个子问题的参数学习。它们已广泛应用于机器学习和信号处理问题，包括但不限于时间序列预测，表示学习，图聚类，最大间隔分类和一般少量样本学习。然而，现有的LRSM研究要么假设已知图形，要么仅限于特定应用。在本文中，我们首先展示了LRSM中图权重的重要性和敏感性，并证明了当节点之间参数比例和样本量不平衡时，敏感性可能会任意增大。然后，我们提出了一种通用方法，在拟合模型的同时联合学习图，适用于广泛的LRSM问题。具体而言，我们制定了联合图学习和模型拟合问题，并通过交替学习图（通过稀疏逆协方差估计）和拟合模型（通过近端梯度下降）来解决它。我们的方法不仅在合成和真实世界数据集上实现了最先进的性能，还揭示了有关问题的有趣图案和结构。

    Laplacian regularized stratified models (LRSM) are models that utilize the explicit or implicit network structure of the sub-problems as defined by the categorical features called strata (e.g., age, region, time, forecast horizon, etc.), and draw upon data from neighboring strata to enhance the parameter learning of each sub-problem. They have been widely applied in machine learning and signal processing problems, including but not limited to time series forecasting, representation learning, graph clustering, max-margin classification, and general few-shot learning. Nevertheless, existing works on LRSM have either assumed a known graph or are restricted to specific applications. In this paper, we start by showing the importance and sensitivity of graph weights in LRSM, and provably show that the sensitivity can be arbitrarily large when the parameter scales and sample sizes are heavily imbalanced across nodes. We then propose a generic approach to jointly learn the graph while fitting 
    
[^15]: 异常值鲁棒主成分分析的近似线性时间和流式算法

    Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA. (arXiv:2305.02544v1 [cs.LG])

    [http://arxiv.org/abs/2305.02544](http://arxiv.org/abs/2305.02544)

    该研究开发出了一种近似线性时间的鲁棒PCA算法，具有接近最优的误差保证，并且还开发了一种单遍流式PCA算法，具有几乎线性的内存使用。

    

    我们研究主成分分析，其中给定来自分布的$\mathbb{R}^d$的数据集，任务是找到一个单位向量$v$，在沿$v$投影后，近似地最大化分布的方差。尽管是一个经典的任务，但如果数据包含即使是少量的异常值，标准估计器也会严重失败，这激发了鲁棒主成分分析的问题。最近的工作已经开发出计算效率较高的鲁棒PCA算法，但要么需要超线性时间，要么具有次优的误差保证。我们的主要贡献是开发出一种近似线性时间的鲁棒PCA算法，并具有接近最优的误差保证。我们还开发了一种单遍流式PCA算法，其内存使用几乎与维数成线性比。

    We study principal component analysis (PCA), where given a dataset in $\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly-linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage nearly-linear in the dimension.
    
[^16]: 修正干扰问题：以抖音为例的案例研究

    Correcting for Interference in Experiments: A Case Study at Douyin. (arXiv:2305.02542v1 [stat.ME])

    [http://arxiv.org/abs/2305.02542](http://arxiv.org/abs/2305.02542)

    该研究提出了一种新型的Monte-Carlo估计器，它能够修正在抖音双向内容市场平台上实验时的干扰问题，并在现场实验中将实验吞吐量提高了一倍。

    

    干扰是在双向内容市场平台上进行实验时的普遍问题，例如中国版的TikTok——抖音。在许多情况下，创作者是实验的自然单位，但是创作者通过争夺观众有限的时间和注意力互相干扰。目前实践中使用的“朴素”估计器简单地忽略了干扰，但这样做会产生治疗效果的误差。我们将这样的实验推断问题形式化为政策评估中的问题。虽然偏差很小，但形式上行为策略不够实用。我们引入了一种基于“Q值差异”的(DQ)技术的新型Monte-Carlo估计器，它达到了治疗效果在二阶的偏差，同时保持了样本效率。在理论方面，我们的贡献是发展了一个通用的政策评估泰勒展开理论，将DQ理论扩展到所有主要的MDP公式。在实践方面，我们的估计器被部署在抖音上，并在现场实验中将实验吞吐量提高了一倍。

    Interference is a ubiquitous problem in experiments conducted on two-sided content marketplaces, such as Douyin (China's analog of TikTok). In many cases, creators are the natural unit of experimentation, but creators interfere with each other through competition for viewers' limited time and attention. "Naive" estimators currently used in practice simply ignore the interference, but in doing so incur bias on the order of the treatment effect. We formalize the problem of inference in such experiments as one of policy evaluation. Off-policy estimators, while unbiased, are impractically high variance. We introduce a novel Monte-Carlo estimator, based on "Differences-in-Qs" (DQ) techniques, which achieves bias that is second-order in the treatment effect, while remaining sample-efficient to estimate. On the theoretical side, our contribution is to develop a generalized theory of Taylor expansions for policy evaluation, which extends DQ theory to all major MDP formulations. On the practica
    
[^17]: AutoML-GPT: 基于 GPT 的自动机器学习

    AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])

    [http://arxiv.org/abs/2305.02499](http://arxiv.org/abs/2305.02499)

    AutoML-GPT 是一种基于 GPT 的自动机器学习方法，利用大型语言模型动态地利用各种人工智能模型，自动化训练管道，节约了选择模型架构、优化算法和调整超参数的人力和时间成本。

    

    AI 任务涵盖了广泛的领域和领域。虽然为特定任务和应用程序设计了众多 AI 模型，但它们通常需要大量的人力投入来查找正确的模型架构、优化算法和超参数。最近，像 ChatGPT 这样的大型语言模型 (LLM) 在推理、理解和交互的各个方面展现出了卓越的能力。因此，我们提出了开发面向任务的提示并自动利用 LLM 自动化训练管道的想法。为了实现这个概念，我们推出了 AutoML-GPT，它采用 GPT 作为连接多种 AI 模型的桥梁，并动态地使用优化超参数训练模型。AutoML-GPT 从模型和数据卡中动态获取用户请求，并组成相应的提示段落。最终，通过这个提示段落，AutoML-GPT 将自动从数据处理到模型架构、超参数调整进行实验。

    AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning,
    
[^18]: 未知机流形上的潜在结构网络中的半监督回归。

    Semisupervised regression in latent structure networks on unknown manifolds. (arXiv:2305.02473v1 [stat.ML])

    [http://arxiv.org/abs/2305.02473](http://arxiv.org/abs/2305.02473)

    本文提出了一种基于半监督回归的潜在结构网络模型，在未知机流形上使用流形学习和图嵌入技术进行响应预测，并为这些响应建立了收敛保证。

    

    随机图在建模各种应用中的网络越来越受到关注。潜在位置随机图模型认为每个节点都与潜在位置向量相关联，并且这些向量在潜在空间中遵循某些几何结构。本文考虑随机点积图，其中在其各自的潜在位置的内积给定的概率下形成两个节点之间的边缘。我们假设潜在位置向量位于未知的一维曲线上，并通过回归模型与响应协变量耦合。利用潜在位置向量的底层几何结构，我们提出了一种流形学习和图嵌入技术，以预测样本外节点上的响应变量，并为这些响应建立了收敛保证。我们的理论结果得到模拟和对Drosophila大脑数据的应用的支持。

    Random graphs are increasingly becoming objects of interest for modeling networks in a wide range of applications. Latent position random graph models posit that each node is associated with a latent position vector, and that these vectors follow some geometric structure in the latent space. In this paper, we consider random dot product graphs, in which an edge is formed between two nodes with probability given by the inner product of their respective latent positions. We assume that the latent position vectors lie on an unknown one-dimensional curve and are coupled with a response covariate via a regression model. Using the geometry of the underlying latent position vectors, we propose a manifold learning and graph embedding technique to predict the response variable on out-of-sample nodes, and we establish convergence guarantees for these responses. Our theoretical results are supported by simulations and an application to Drosophila brain data.
    
[^19]: 面向马尔可夫数据的流式PCA算法

    Streaming PCA for Markovian Data. (arXiv:2305.02456v1 [math.ST])

    [http://arxiv.org/abs/2305.02456](http://arxiv.org/abs/2305.02456)

    本文提出了一种面向马尔可夫数据采样的流式PCA算法，并获得了该算法在整个数据集上的第一个尖锐率，提高了算法的效率。同时，本文提出的自适应方案在模拟和真实数据示例中表现良好。

    

    自从Oja在1982年的经典论文中首次提出以来，Oja算法已成为流式主成分分析(PCA)的一种常用方法。本文研究了流式PCA问题，其中数据点从一个不可约、无周期、可逆的马尔可夫链中采样。我们的目标是估计平稳分布的未知协方差矩阵的前一个特征向量。这种情况适用于只能从马尔可夫链蒙特卡罗(MCMC)类型的算法中采样数据，并且目标是对该链的平稳分布的参数进行推断的情况。现有文献中大多数Oja算法的收敛保证都假定数据点是IID采样的。对于具有马尔可夫依赖关系的数据流，人们通常对数据进行下采样以获得"几乎"独立的数据流。在本文中，我们获得了Oja算法在整个数据集上的第一个尖锐率，其中去掉了$n$的对数依赖性，结果是$\mathcal{O}(n^{-1})$的速率。我们还提出了一种自适应方案来调整算法的步长，它在模拟和真实数据示例中都表现更好。

    Since its inception in Erikki Oja's seminal paper in 1982, Oja's algorithm has become an established method for streaming principle component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in situations where data can only be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the goal is to do inference for parameters of the stationary distribution of this chain. Most convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a "nearly" independent data stream. In this paper, we obtain the first sharp rate for Oja's algorithm on the entire data, where we remove the logarithmic dependence on $n$ resulti
    
[^20]: 基于奖励教学的联邦多臂老虎机设计

    Reward Teaching for Federated Multi-armed Bandits. (arXiv:2305.02441v1 [stat.ML])

    [http://arxiv.org/abs/2305.02441](http://arxiv.org/abs/2305.02441)

    本论文提出了一种基于奖励教学思想的联邦多臂老虎机设计，通过隐式本地奖励调整来指导客户端朝着全局最优性，队服务端提出了老虎机学习和目标教学任务进行了优化。

    

    目前大部分已有的联邦多臂老虎机（FMAB）设计都基于假设客户端会实现指定的设计来与服务器协作。但实际上，可能无法修改客户端现有的协议。为了应对这一挑战，该工作关注始终最大化其个体累积奖励的客户端，并引入了“奖励教学”的新思想，即通过隐式的本地奖励调整指导客户端朝着全局最优性。在这个框架下，服务器面临两个密切耦合的任务，即老虎机学习和目标教学，它们的结合非常复杂和具有挑战性。首先设计了一个名为 “Teaching-After-Learning（TAL）” 的分阶段方法，分别鼓励和限制客户端的探索。当客户端策略满足一定的温和要求时，建立了TAL的综合性能分析。通过开发新的技术方法来分析TAL的热启动和算法，我们展示了TAL可以比现有的FMAB设计带来显著的改进。

    Most of the existing federated multi-armed bandits (FMAB) designs are based on the presumption that clients will implement the specified design to collaborate with the server. In reality, however, it may not be possible to modify the client's existing protocols. To address this challenge, this work focuses on clients who always maximize their individual cumulative rewards, and introduces a novel idea of "reward teaching", where the server guides the clients towards global optimality through implicit local reward adjustments. Under this framework, the server faces two tightly coupled tasks of bandit learning and target teaching, whose combination is non-trivial and challenging. A phased approach, called Teaching-After-Learning (TAL), is first designed to encourage and discourage clients' explorations separately. General performance analyses of TAL are established when the clients' strategies satisfy certain mild requirements. With novel technical approaches developed to analyze the warm
    
[^21]: 利用编码嵌入和顶点动态发现大规模网络中的通信模式变化

    Discovering Communication Pattern Shifts in Large-Scale Networks using Encoder Embedding and Vertex Dynamics. (arXiv:2305.02381v1 [cs.SI])

    [http://arxiv.org/abs/2305.02381](http://arxiv.org/abs/2305.02381)

    本文提出了一种新的方法称为“时间编码嵌入”，可以以线性复杂度有效地嵌入大量图数据，并利用此方法在大型组织的通信网络中检测出了个体顶点、顶点社区和整体图结构的通信模式变化。

    

    分析大规模时间序列网络数据（如社交媒体和电子邮件通信）仍然是图分析方法学面临的重大挑战。本文介绍了一种称为“时间编码嵌入”的新方法，可以以线性复杂度有效地嵌入大量的图数据。我们将该方法应用于一家大型机构跨越2019年至2020年的匿名时间序列通信网络，由超过10万个顶点和8000万个边组成。我们的方法在标准计算机上仅需10秒即可嵌入数据，并能够检测个体顶点、顶点社区和整体图结构的通信模式变化。通过支持理论和综合研究，我们证明了我们的方法在随机图模型下的理论健全性和数值效果。

    The analysis of large-scale time-series network data, such as social media and email communications, remains a significant challenge for graph analysis methodology. In particular, the scalability of graph analysis is a critical issue hindering further progress in large-scale downstream inference. In this paper, we introduce a novel approach called "temporal encoder embedding" that can efficiently embed large amounts of graph data with linear complexity. We apply this method to an anonymized time-series communication network from a large organization spanning 2019-2020, consisting of over 100 thousand vertices and 80 million edges. Our method embeds the data within 10 seconds on a standard computer and enables the detection of communication pattern shifts for individual vertices, vertex communities, and the overall graph structure. Through supporting theory and synthesis studies, we demonstrate the theoretical soundness of our approach under random graph models and its numerical effecti
    
[^22]: 双重/去偏机器学习的加权累积治疗效应的有效估计

    Efficient estimation of weighted cumulative treatment effects by double/debiased machine learning. (arXiv:2305.02373v1 [stat.ME])

    [http://arxiv.org/abs/2305.02373](http://arxiv.org/abs/2305.02373)

    提出了一种双重/去偏机器学习估计器，用于对时间至事件结果的加权平均治疗效应进行估计，解决了覆盖度不足的挑战，并通过证明其一致性和渐近正常性来验证其有效性。

    

    在具有时间至事件结果的经验研究中，研究人员常常利用观察数据，在随机对照试验数据不适用的情况下进行曝光的因果推断。模型错配和缺乏重叠是观察研究中常见的问题，它们经常导致平均治疗效应的不一致和低效的估计器。为解决覆盖度不足的挑战，已经提出了针对叠加加权效应的估计器，而使辅助模型能够进行灵活的机器学习的方法可以解决模型错配的问题。但是，当存在覆盖度不足时，允许辅助模型进行机器学习的方法尚未扩展到时间至事件结果的加权平均治疗效应的情景中。在本文中，我们提出了一类一步交叉适应的双重/去偏机器学习估计器，用于作为限制时间的加权累积因果效应的函数。我们证明了拟议方法的一致性和渐近正常性。

    In empirical studies with time-to-event outcomes, investigators often leverage observational data to conduct causal inference on the effect of exposure when randomized controlled trial data is unavailable. Model misspecification and lack of overlap are common issues in observational studies, and they often lead to inconsistent and inefficient estimators of the average treatment effect. Estimators targeting overlap weighted effects have been proposed to address the challenge of poor overlap, and methods enabling flexible machine learning for nuisance models address model misspecification. However, the approaches that allow machine learning for nuisance models have not been extended to the setting of weighted average treatment effects for time-to-event outcomes when there is poor overlap. In this work, we propose a class of one-step cross-fitted double/debiased machine learning estimators for the weighted cumulative causal effect as a function of restriction time. We prove that the propo
    
[^23]: 学习如何推断部分马尔可夫决策过程进行上下文适应和探索

    Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration. (arXiv:2302.04250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04250](http://arxiv.org/abs/2302.04250)

    本论文介绍了一种通过学习部分马尔可夫决策过程来进行上下文适应和探索的新方法，其使用变压器进行推理过程学习，考虑了模型假设空间，假设表示为小的马尔可夫决策过程，可以在性价比高的情况下进行动态规划。该方法在Symbolic Alchemy基准测试中表现出与精确后验抽样相近的适应速度和探索利用平衡。

    

    为了在任务间进行泛化，智能体应该从过去的任务中获取知识，以促进未来任务中的适应和探索。我们关注上下文适应和探索问题，其中一个智能体只依赖于上下文，即状态、动作和/或奖励的历史记录，而不是梯度更新。后验抽样是一种有前途的方法，但它需要贝叶斯推理和动态规划，通常涉及未知量（例如，先验）和昂贵的计算。为了解决这些困难，我们使用一个变压器来从训练任务中学习推理过程，并考虑一个假设空间的部分模型，表示为小的马尔可夫决策过程，这对于动态规划来说是廉价的。在我们版本的Symbolic Alchemy基准测试中，我们的方法的适应速度和探索利用平衡接近于精确的后验抽样神谕。我们还展示了即使部分模型排除了r

    To generalize across tasks, an agent should acquire knowledge from past tasks that facilitate adaptation and exploration in future tasks. We focus on the problem of in-context adaptation and exploration, where an agent only relies on context, i.e., history of states, actions and/or rewards, rather than gradient-based updates. Posterior sampling (extension of Thompson sampling) is a promising approach, but it requires Bayesian inference and dynamic programming, which often involve unknowns (e.g., a prior) and costly computations. To address these difficulties, we use a transformer to learn an inference process from training tasks and consider a hypothesis space of partial models, represented as small Markov decision processes that are cheap for dynamic programming. In our version of the Symbolic Alchemy benchmark, our method's adaptation speed and exploration-exploitation balance approach those of an exact posterior sampling oracle. We also show that even though partial models exclude r
    
[^24]: 多项式Logit模型中最优产品组合的组合推断

    Combinatorial Inference on the Optimal Assortment in Multinomial Logit Models. (arXiv:2301.12254v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12254](http://arxiv.org/abs/2301.12254)

    本文提出了一种基于多项式logit模型的推断框架，可以测试最优产品组合是否具有特定性质。

    

    最优的产品组合优化已经成为实践中的重要问题。本文提出了一种新的推断框架，用于测试最优产品组合是否具有特定性质。我们考虑了广泛采用的多项式logit（MNL）模型，并将其用于研究最优组合问题。

    Assortment optimization has received active explorations in the past few decades due to its practical importance. Despite the extensive literature dealing with optimization algorithms and latent score estimation, uncertainty quantification for the optimal assortment still needs to be explored and is of great practical significance. Instead of estimating and recovering the complete optimal offer set, decision-makers may only be interested in testing whether a given property holds true for the optimal assortment, such as whether they should include several products of interest in the optimal set, or how many categories of products the optimal set should include. This paper proposes a novel inferential framework for testing such properties. We consider the widely adopted multinomial logit (MNL) model, where we assume that each customer will purchase an item within the offered products with a probability proportional to the underlying preference score associated with the product. We reduce
    
[^25]: 基于次流形假设下扩散模型奇异性的数学分析

    Mathematical analysis of singularities in the diffusion model under the submanifold assumption. (arXiv:2301.07882v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07882](http://arxiv.org/abs/2301.07882)

    本文提供了扩散模型中漂移项的数学分析。通过次流形假设，提出一种新的目标函数和相关的损失函数，可处理低维流形上的奇异数据分布，解决了均值漂移函数和得分函数渐近发散的问题。

    

    本文提供了机器学习中扩散模型的数学分析。以条件期望表示反向采样流程的漂移项，其中涉及数据分布和前向扩散。训练过程旨在通过最小化与条件期望相关的均方残差来寻找此类漂移函数。使用前向扩散的Green函数的小时间近似，我们证明了DDPM中的解析均值漂移函数和SGM中的得分函数在采样过程的最后阶段，对于像那些集中在低维流形上的奇异数据分布而言，渐近地发散，因此难以通过网络进行逼近。为了克服这个困难，我们推导出了一个新的目标函数和相关的损失函数，即使在处理奇异数据分布时仍然保持有界。我们通过几个数值实验来说明理论发现。

    This paper provide several mathematical analyses of the diffusion model in machine learning. The drift term of the backwards sampling process is represented as a conditional expectation involving the data distribution and the forward diffusion. The training process aims to find such a drift function by minimizing the mean-squared residue related to the conditional expectation. Using small-time approximations of the Green's function of the forward diffusion, we show that the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for singular data distributions such as those concentrated on lower-dimensional manifolds, and is therefore difficult to approximate by a network. To overcome this difficulty, we derive a new target function and associated loss, which remains bounded even for singular data distributions. We illustrate the theoretical findings with several numerical examples.
    
[^26]: 一种随机Proximal Polyak步长方法

    A Stochastic Proximal Polyak Step Size. (arXiv:2301.04935v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.04935](http://arxiv.org/abs/2301.04935)

    本文开发了一种正则化的随机梯度下降ProxSPS算法，相比随机Polyak步长（SPS）更稳定易调整，同时在图像分类任务中表现良好，可导致网络具有更小的权重参数。

    

    最近，随机Polyak步长（SPS）已成为随机梯度下降的竞争性自适应步长方案。在本文中，我们开发了ProxSPS，这是SPS的proximal变体，可以处理正则化项。开发SPS的proximal变体特别重要，因为SPS需要目标函数的下界才能发挥良好的作用。当目标函数是损失和正则化项的总和时，可用的总和下界估计可能不准确。相比之下，ProxSPS只需要对损失进行下界估计，而这通常很容易得到。因此，我们展示了ProxSPS更易于调整，在正则化的情况下更稳定。此外，对于图像分类任务，ProxSPS表现与AdamW一样好，几乎不需要调整，并且导致具有更小权重参数的网络。我们还为ProxSPS提供了广泛的收敛性分析，包括非光滑、光滑、弱凸和强凸设置。

    Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.
    
[^27]: 无偏的监督对比学习

    Unbiased Supervised Contrastive Learning. (arXiv:2211.05568v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05568](http://arxiv.org/abs/2211.05568)

    本文提出了一种新的监督对比损失形式（epsilon-SupInfoNCE）以及一种新的去偏正则化损失（FairKL），旨在解决从有偏数据中学习无偏模型的问题。

    

    许多数据集存在偏差，即它们包含仅在数据集中与目标类高度相关的易于学习的特征，但不在真实的数据分布中。因此，从有偏数据中学习无偏模型已成为近年来非常相关的研究课题。在这项工作中，我们解决了学习对偏差具有鲁棒性的表征的问题。我们首先提出了一种基于边缘的理论框架，可以帮助我们澄清为什么最近的对比损失（InfoNCE，SupCon等）在处理偏差数据时可能失败。基于此，我们推导出了一种新的监督对比损失形式（epsilon-SupInfoNCE），提供了更准确的对正负样本之间最小距离的控制。此外，由于我们的理论框架，我们还提出了FairKL，一种新的去偏正则化损失，即使在极度偏差的数据情况下也可以很好地工作。我们在标准的视觉数据集上验证了所提出的损失。

    Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets inc
    
[^28]: 缺失数据转移下的领域自适应

    Domain Adaptation under Missingness Shift. (arXiv:2211.02093v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02093](http://arxiv.org/abs/2211.02093)

    本文解决了领域自适应问题中缺失数据转移的情况，提出了DAMS方法。针对缺失数据指标不可用的情况，提供了理论结果，包括协变量转移被违反、最优源预测器可能比总是预测均值表现更差、最优目标预测器可被识别等。

    

    缺失数据的比率通常会因记录政策而变化，即使基础特征相对稳定，这种情况可能在不同的时间和地点发生变化。本文介绍了缺失数据转移下的领域自适应问题(DAMS)。在这里，(有标签的)源数据和(无标签的)目标数据可以互换，但存在不同的缺失数据机制。我们展示了，如果缺失数据指标是可用的，DAMS就会归结为协变量转移。针对这种指标不可用的情况，我们为基于完全随机下报告的讨论提供了以下理论结果：(i)协变量转移被违反了(需要适应)；(ii)与总是预测均值相比，最优的线性源预测器在目标域上的表现可能会变得更糟；(iii)即使缺失率本身无法确定，也能够识别出最优的目标预测器；(iv)对于线性模型，一个简单的分析调整可以得到最优参数的一致估计。

    Rates of missing data often depend on record-keeping policies and thus may change across times and locations, even when the underlying features are comparatively stable. In this paper, we introduce the problem of Domain Adaptation under Missingness Shift (DAMS). Here, (labeled) source data and (unlabeled) target data would be exchangeable but for different missing data mechanisms. We show that if missing data indicators are available, DAMS reduces to covariate shift. Addressing cases where such indicators are absent, we establish the following theoretical results for underreporting completely at random: (i) covariate shift is violated (adaptation is required); (ii) the optimal linear source predictor can perform arbitrarily worse on the target domain than always predicting the mean; (iii) the optimal target predictor can be identified, even when the missingness rates themselves are not; and (iv) for linear models, a simple analytic adjustment yields consistent estimates of the optimal 
    
[^29]: 带有核化Stein距离的后验Coreset构建用于基于模型的强化学习

    Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning. (arXiv:2206.01162v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01162](http://arxiv.org/abs/2206.01162)

    本文提出一种通过核化Stein距离构建后验Coreset的MBRL方法，在放松转移模型高斯或Lipschitz的限制下表现出优异的性能，并且可以应用于大规模训练。

    

    模型为基础的强化学习方法在实践中表现出优异的性能，但在大空间的理论保证大多数限于转移模型为高斯或Lipschitz的情况下，并且需要后验估计其表示复杂度随时间增长而无界。 在本文中，我们开发了一种新的MBRL方法，(i) 放松目标转移模型属于通用混合模型族的假设；(ii) 通过包含压缩步骤以仅由统计显着的过去状态 - 操作对的贝叶斯Coreset组成，适用于大规模训练；(iii) 表现出亚线性的贝叶斯遗憾。为了实现这些结果，我们采用一种基于Stein方法的方法，该方法在构造后验和目标上满足平滑性条件的情况下，允许以核Stein距离（KSD）的形式封闭地评估分布距离。前面提到的后验Coreset构建是通过最小化这个KSD来完成的，其中混合组件的位置取决于l-inf预算，以限制中心的数量。我们在一系列模拟机器人控制任务中展示了该方法的有效性。

    Model-based approaches to reinforcement learning (MBRL) exhibit favorable performance in practice, but their theoretical guarantees in large spaces are mostly restricted to the setting when transition model is Gaussian or Lipschitz, and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a Bayesian coreset of only statistically significant past state-action pairs; and (iii) exhibits a sublinear Bayesian regret. To achieve these results, we adopt an approach based upon Stein's method, which, under a smoothness condition on the constructed posterior and target, allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). The afor
    
[^30]: 一种用于信号去噪的交叉验证框架及其在趋势滤波、二级决策树等算法中的应用

    A Cross Validation Framework for Signal Denoising with Applications to Trend Filtering, Dyadic CART and Beyond. (arXiv:2201.02654v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2201.02654](http://arxiv.org/abs/2201.02654)

    本论文提出了一种通用的交叉验证框架，可应用于信号去噪和回归分析中，其中经过交叉验证的版本可以达到与最佳调节版本几乎相同的收敛速率，具有一定的普适性和应用价值。

    

    本文提出了一个通用的信号去噪交叉验证框架，并将其应用于非参数回归方法，如趋势滤波和二级决策树。结果显示，这些经过交叉验证的版本可以达到与最佳调节版本几乎相同的收敛速率。此外，我们还展示了lasso和奇异值阈值法的经过交叉验证的版本，以证明此框架的普遍适用性。我们的通用框架灵感来自Chatterjee和Jafarov（2015）的思想，并可能适用于使用调节参数的广泛估计方法。

    This paper formulates a general cross validation framework for signal denoising. The general framework is then applied to nonparametric regression methods such as Trend Filtering and Dyadic CART. The resulting cross validated versions are then shown to attain nearly the same rates of convergence as are known for the optimally tuned analogues. There did not exist any previous theoretical analyses of cross validated versions of Trend Filtering or Dyadic CART. To illustrate the generality of the framework we also propose and study cross validated versions of two fundamental estimators; lasso for high dimensional linear regression and singular value thresholding for matrix estimation. Our general framework is inspired by the ideas in Chatterjee and Jafarov (2015) and is potentially applicable to a wide range of estimation methods which use tuning parameters.
    
[^31]: 基于神经网络的非线性函数建模

    Non-linear Functional Modeling using Neural Networks. (arXiv:2104.09371v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.09371](http://arxiv.org/abs/2104.09371)

    本文提出了一种基于神经网络的、适用于函数数据的新型非线性模型。我们提出了两种变体，旨在显式利用函数数据中固有的结构，并通过全面的模拟研究和实际数据示例证明了该方法的有效性。

    

    本文介绍了一种基于神经网络的新型非线性函数数据模型。深度学习在非线性建模方面非常成功，但在函数数据设置方面却很少有研究。我们提出了两种变体：一种是具有连续隐藏层的函数神经网络，称为函数直接神经网络（FDNN），另一种则利用基扩展和连续隐藏层，称为基函数神经网络（FBNN）。两种变体都是设计用来显式利用函数数据中固有的结构。为了拟合这些模型，我们导出了一种基于函数梯度的优化算法。我们通过全面的模拟研究和实际数据示例展示了所提出方法在处理复杂函数模型方面的有效性。

    We introduce a new class of non-linear models for functional data based on neural networks. Deep learning has been very successful in non-linear modeling, but there has been little work done in the functional data setting. We propose two variations of our framework: a functional neural network with continuous hidden layers, called the Functional Direct Neural Network (FDNN), and a second version that utilizes basis expansions and continuous hidden layers, called the Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data. To fit these models we derive a functional gradient based optimization algorithm. The effectiveness of the proposed methods in handling complex functional models is demonstrated by comprehensive simulation studies and real data examples.
    
[^32]: GTEA: 通过时间边聚合在时序交互图上进行归纳表征学习

    GTEA: Inductive Representation Learning on Temporal Interaction Graphs via Temporal Edge Aggregation. (arXiv:2009.05266v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.05266](http://arxiv.org/abs/2009.05266)

    本文提出了 GTEA框架，用于在时序交互图上进行归纳学习，结合了时间动态建模和图嵌入，通过聚合相邻节点和边嵌入的特征，共同学习了 TIG 的拓扑和时间依赖关系，而且引入了一种稀疏感知的自注意机制，在多个时间序列预测任务中表现出有效性。

    

    本文提出了Graph Temporal Edge Aggregation (GTEA)框架，用于在时序交互图（TIGs）上进行归纳学习。不同于以往工作，GTEA将交互序列的时间动态建模在连续时间空间中，并同时利用图中丰富的节点和边/交互属性。具体来说，我们将序列模型与时间编码器相结合，学习相邻节点之间的成对交互动力学。这有助于捕捉节点对沿历史的复杂时间交互模式，生成边嵌入可以输入到GNN骨干。通过聚合相邻节点和相应的边嵌入的特征，GTEA共同学习TIG的拓扑和时间依赖关系。此外，还采用了一种稀疏感知自注意机制进行邻居聚合，突出更重要的邻居并抑制GTEA的琐碎噪声。通过联合优化整个框架，我们验证了其在多个时间序列预测任务中的有效性。

    In this paper, we propose the Graph Temporal Edge Aggregation (GTEA) framework for inductive learning on Temporal Interaction Graphs (TIGs). Different from previous works, GTEA models the temporal dynamics of interaction sequences in the continuous-time space and simultaneously takes advantage of both rich node and edge/ interaction attributes in the graph. Concretely, we integrate a sequence model with a time encoder to learn pairwise interactional dynamics between two adjacent nodes.This helps capture complex temporal interactional patterns of a node pair along the history, which generates edge embeddings that can be fed into a GNN backbone. By aggregating features of neighboring nodes and the corresponding edge embeddings, GTEA jointly learns both topological and temporal dependencies of a TIG. In addition, a sparsity-inducing self-attention scheme is incorporated for neighbor aggregation, which highlights more important neighbors and suppresses trivial noises for GTEA. By jointly o
    
[^33]: 富有属性网络中的顶点提名

    Vertex Nomination in Richly Attributed Networks. (arXiv:2005.02151v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2005.02151](http://arxiv.org/abs/2005.02151)

    本文探讨了富有属性网络中顶点提名的双重作用，并提出了一种新颖的基于内容感知的网络嵌入方法，证明该方法优于现有的不利用内容和上下文的顶点提名方法。

    

    顶点提名是一项轻度监督的网络信息检索任务，在这个任务中，感兴趣的一张图的顶点被用来查询第二张图以发现感兴趣的第二张图的顶点。与其他信息检索任务类似，顶点提名方案的输出是第二张图中顶点的排序列表，理想情况下，未知的感兴趣的顶点应该集中在列表的顶部。顶点提名方案为高效地挖掘复杂网络中的相关信息提供了有用的工具。在本文中，我们从理论和实践两方面探讨了内容（即边缘和顶点属性）和上下文（即网络拓扑结构）在顶点提名中的双重作用。我们提供了必要和充分的条件，证明了利用内容和上下文的顶点提名方案能够超越仅利用内容或上下文的方案。虽然内容和上下文的联合效用在其他网络分析任务中已经得到证实，但我们证明在顶点提名的背景下，这种联合效用也是成立的。此外，我们提出了一种新颖的基于内容感知的网络嵌入方法，用于顶点提名，可以有效地结合局部和全局网络属性信息。我们在真实的社交和引用网络上进行了实验，证明了我们提出的方法优于不利用内容和上下文的现有的顶点提名方法。

    Vertex nomination is a lightly-supervised network information retrieval task in which vertices of interest in one graph are used to query a second graph to discover vertices of interest in the second graph. Similar to other information retrieval tasks, the output of a vertex nomination scheme is a ranked list of the vertices in the second graph, with the heretofore unknown vertices of interest ideally concentrating at the top of the list. Vertex nomination schemes provide a useful suite of tools for efficiently mining complex networks for pertinent information. In this paper, we explore, both theoretically and practically, the dual roles of content (i.e., edge and vertex attributes) and context (i.e., network topology) in vertex nomination. We provide necessary and sufficient conditions under which vertex nomination schemes that leverage both content and context outperform schemes that leverage only content or context separately. While the joint utility of both content and context has 
    
[^34]: 基于结果抽样的因果推断和单调性假设

    Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions. (arXiv:2004.08318v5 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2004.08318](http://arxiv.org/abs/2004.08318)

    本文研究了基于结果抽样的因果推断，发现强无偏性不总是像随机抽样下那样强大，并且某些单调性假设在锐利识别间隔方面产生可比较的结果。 通过算法推断出参数并用实证例子证明了方法的贡献。

    

    本文研究了根据案例控制和案例群体取样进行因果推断的方法。具体而言，我们关注二元结果和二元治疗的情况，其中感兴趣的参数是通过潜在结果框架定义的因果相对和可归因风险。我们发现强无偏性不总是像随机抽样下那样强大，并且某些单调性假设在锐利识别间隔方面产生可比较的结果。具体而言，通常的比值比在单调治疗反应和单调治疗选择假设下被证明是因果相对风险的锐利识别上界。我们提供了聚合在协变量的真实人口分布上的因果参数推断算法。我们通过研究三个实证例子展示了我们方法的有用性：在巴基斯坦进入著名大学的私立学校受益问题上；在留在学校和放弃学校早期的关系问题上；在预测岗位培训效果问题上。

    We study causal inference under case-control and case-population sampling. Specifically, we focus on the binary-outcome and binary-treatment case, where the parameters of interest are causal relative and attributable risks defined via the potential outcome framework. It is shown that strong ignorability is not always as powerful as it is under random sampling and that certain monotonicity assumptions yield comparable results in terms of sharp identified intervals. Specifically, the usual odds ratio is shown to be a sharp identified upper bound on causal relative risk under the monotone treatment response and monotone treatment selection assumptions. We offer algorithms for inference on the causal parameters that are aggregated over the true population distribution of the covariates. We show the usefulness of our approach by studying three empirical examples: the benefit of attending private school for entering a prestigious university in Pakistan; the relationship between staying in sc
    

