# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fairer and More Accurate Tabular Models Through NAS.](http://arxiv.org/abs/2310.12145) | 通过使用多目标神经架构搜索(NAS)和超参数优化(HPO)，我们在表格数据领域首次提出了一种更新模型架构和超参数的策略，以寻找更公平和准确的模型。我们发现，仅针对准确性进行优化可能会导致公平性的降低，因此需要同时考虑准确性和公平性。 |
| [^2] | [Simple Mechanisms for Representing, Indexing and Manipulating Concepts.](http://arxiv.org/abs/2310.12143) | 通过查看概念的矩阵统计量，生成一个概念的具体表示或签名，可以用于发现概念之间的结构并递归产生更高级的概念，同时可以通过概念的签名来找到相关的共同主题。 |
| [^3] | [Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data.](http://arxiv.org/abs/2310.12140) | 本研究提出了一种在线估计方法，通过加权滚动验证过程来提高基本估计器的自适应收敛速度，并证明了这种方法的重要性和敏感性 |
| [^4] | [MMD-based Variable Importance for Distributional Random Forest.](http://arxiv.org/abs/2310.12115) | 本文介绍了基于MMD距离和经典的drop and relearn原理的变量重要性算法，可以在分布随机森林中检测影响输出分布的变量，并且在实证性能上超越了竞争对手。 |
| [^5] | [Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks.](http://arxiv.org/abs/2310.12079) | 最近的研究发现，对于具有形状激活函数的神经网络，其缩放极限可以由微分方程描述。然而，关于未经形状处理的神经网络的信息尚不明确。本文研究了两种未经形状处理的网络，发现它们也可以由类似的微分方程来描述，并给出了它们的一些特征。 |
| [^6] | [Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems.](http://arxiv.org/abs/2310.12046) | 本文探讨了在贝叶斯逆问题中，使用机器学习为基础的代理模型，以提高计算效率和处理数值挑战问题。 |
| [^7] | [A General Theoretical Paradigm to Understand Learning from Human Preferences.](http://arxiv.org/abs/2310.12036) | 本文研究了学习从人类偏好中学习的实际算法的理论基础，推导出一个新的一般目标，绕过了两个重要的近似。这种方法允许直接从收集的数据中学习策略而无需奖励模型的训练。 |
| [^8] | [Conformal Drug Property Prediction with Density Estimation under Covariate Shift.](http://arxiv.org/abs/2310.12033) | 提出了一种名为CoDrug的方法，该方法利用基于能量的模型和核密度估计来解决协变量偏移问题，从而实现一致性药物属性预测。 |
| [^9] | [Exact and efficient solutions of the LMC Multitask Gaussian Process model.](http://arxiv.org/abs/2310.12032) | LMC多任务高斯过程模型的精确解决方案表明，只需对噪声模型进行温和假设，即可实现高效计算。通过引入完整参数化的“投影LMC”模型和边缘似然函数表达式，展示了该方法相对于未经处理的方法的优异性能。 |
| [^10] | [Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design.](http://arxiv.org/abs/2310.12026) | 提出了无参数离散选择实验与机器学习引导的自适应设计方法GBS，通过自适应构建配对比较问题来满足消费者的偏好，不需要参数化效用模型，可以扩展到具有数百个属性的产品，并在准确性和样本效率方面具有优势。 |
| [^11] | [Bayesian Flow Networks in Continual Learning.](http://arxiv.org/abs/2310.12001) | 连续学习中的贝叶斯流网络（BFNs）是一种具有通用生成建模能力的方法，通过结合神经网络和贝叶斯推断，在非稳态数据上取得了良好的实验结果。 |
| [^12] | [Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models.](http://arxiv.org/abs/2310.12000) | 这篇文章介绍了用于潜在高斯过程模型中的Vecchia-Laplace近似法的迭代方法，相比于传统的Cholesky分解方法，可以显著加快计算速度。 |
| [^13] | [Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation.](http://arxiv.org/abs/2310.11991) | 该论文提出了一种通过联合子空间估计从神经网络表示中消除错误概念的迭代算法，并在计算机视觉和自然语言处理的基准数据集上展示了其优越性能。 |
| [^14] | [A Finite-Horizon Approach to Active Level Set Estimation.](http://arxiv.org/abs/2310.11985) | 这项研究提出了一种有限时间视角下的主动水平集估计方法，在一维和高维情况下均表现出较高的效果，能够平衡估计误差和已移动距离，具有推广性。 |
| [^15] | [Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?.](http://arxiv.org/abs/2310.11978) | 本文研究了分组缩放方法（BVS）的几种改进方法，探索了使用替代损失函数和基于输入特征的分组方案来提高机器学习回归的预测不确定性的一致性和适应性。 |
| [^16] | [Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering.](http://arxiv.org/abs/2310.11940) | 引入了一个可解释的瓶颈，称为滤波器组（FB），用于时间序列聚类的光谱变分自编码器（ISVAE）。通过约束VAE的辨识能力，这个模型能够学习到具有增强可解释性和聚类能力的新编码f_0，并呈现为一个动态的分层树。同时，还提出了与滤波器组结构对称对齐的定制解码器结构，用于处理复杂的数据配置。 |
| [^17] | [A connection between Tempering and Entropic Mirror Descent.](http://arxiv.org/abs/2310.11914) | 本论文研究了退火（针对顺序蒙特卡罗; SMC）和熵镜像下降之间的联系，证明了退火SMC是应用于Kullback-Leibler（KL）散度的熵镜像下降的数值近似，并提出了改进方案。 |
| [^18] | [Online Convex Optimization with Switching Cost and Delayed Gradients.](http://arxiv.org/abs/2310.11880) | 提出了一种在线多梯度下降（OMGD）算法用于解决具有二次和线性开关成本的在线凸优化问题，证明了其竞争比率上界，并在有限信息设置下达到了最优（按顺序）的动态后悔。 |
| [^19] | [SQ Lower Bounds for Learning Mixtures of Linear Classifiers.](http://arxiv.org/abs/2310.11876) | 本文研究了学习线性分类器混合的问题，证明了该问题的统计查询（SQ）算法的复杂度下界是$n^{\mathrm{poly}(1/\Delta) \log(r)}$，同时提出了一种可能具有独立兴趣的新球面设计方法。 |
| [^20] | [Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems.](http://arxiv.org/abs/2310.11838) | 本文提出了一种新的不确定性量化方法，利用参数引导算法的等变形式，可以在成像反问题中量化重构图像的不确定性，并且可以与任何图像重建技术结合使用。 |
| [^21] | [Optimising Distributions with Natural Gradient Surrogates.](http://arxiv.org/abs/2310.11837) | 本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。 |
| [^22] | [Kernel Learning in Ridge Regression "Automatically" Yields Exact Low Rank Solution.](http://arxiv.org/abs/2310.11736) | 该论文研究了核岭回归问题中核学习的低秩解的性质。在只有低维子空间对响应变量有解释能力的情况下，通过该方法可以自动得到精确的低秩解，无需额外的正则化。 |
| [^23] | [Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features.](http://arxiv.org/abs/2310.11654) | 本文提出了一种针对计数数据的主题专用深度神经网络，通过引入伽玛随机效应来提高预测性能，并同时获得了固定参数的最大似然估计和随机效应的最佳无偏预测器。该方法可以快速处理高基数分类特征的聚类计数数据，并且可以轻松实现最先进的网络架构。 |
| [^24] | [Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback.](http://arxiv.org/abs/2310.11550) | 研究带有对抗损失和强盗反馈的线性MDPs问题，提出了两种算法，分别达到了$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$和$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$的遗憾性能。 |
| [^25] | [Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach.](http://arxiv.org/abs/2310.11531) | 本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。 |
| [^26] | [Thin and Deep Gaussian Processes.](http://arxiv.org/abs/2310.11527) | 本文提出了一种薄而深的高斯过程方法，克服了传统方法的局限性，并在学习低维嵌入和解释性之间取得了平衡。 |
| [^27] | [On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction.](http://arxiv.org/abs/2310.11479) | 该论文探讨了将温度参数纳入贝叶斯图神经网络在一致预测中的优势，以提供有效的不确定性量化。 |
| [^28] | [Identifying Interpretable Visual Features in Artificial and Biological Neural Systems.](http://arxiv.org/abs/2310.11431) | 本文提出了一种量化视觉可解释性的自动化方法，并找到了卷积神经网络中有意义的方向。 |
| [^29] | [Surrogate Active Subspaces for Jump-Discontinuous Functions.](http://arxiv.org/abs/2310.10907) | 该论文提出了一种针对不连续函数的替代主动子空间方法，扩展了活跃子空间的应用范围，并通过数值实验验证了该方法的有效性。 |
| [^30] | [Adaptive maximization of social welfare.](http://arxiv.org/abs/2310.09597) | 论文研究了通过适应性策略选择最大化社会福利的问题，并提供了关于遗憾的下界和算法的匹配上界。研究发现福利最大化比多臂老虎机问题更困难，但该算法达到了最优增长速率。 |
| [^31] | [Mixed Variational Flows for Discrete Variables.](http://arxiv.org/abs/2308.15613) | 本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。 |
| [^32] | [Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness.](http://arxiv.org/abs/2308.03666) | 该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。 |
| [^33] | [Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate.](http://arxiv.org/abs/2306.15444) | 有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。 |
| [^34] | [Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards.](http://arxiv.org/abs/2306.05579) | 本文提出了一种处理多智能体多臂赌博机问题的算法框架，通过稳健模拟生成随机图并将加权技术结合UCB算法，以协作方式减小整个系统的总遗憾。 |
| [^35] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^36] | [Generalized equivalences between subsampling and ridge regularization.](http://arxiv.org/abs/2305.18496) | 此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。 |
| [^37] | [Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms.](http://arxiv.org/abs/2305.18409) | 本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。 |
| [^38] | [Bi-fidelity Variational Auto-encoder for Uncertainty Quantification.](http://arxiv.org/abs/2305.16530) | 本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。 |
| [^39] | [When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](http://arxiv.org/abs/2305.02997) | 这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。 |
| [^40] | [Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data.](http://arxiv.org/abs/2302.08466) | 本研究设计了一种黑盒模型提取攻击方法Marich，它使用公共数据集中的最小数量查询来创建一个与目标模型具有信息丰富度和分布等价性的副本，实验结果表明Marich能提取具有60-95%真实模型准确性的模型。 |
| [^41] | [Estimating the Contamination Factor's Distribution in Unsupervised Anomaly Detection.](http://arxiv.org/abs/2210.10487) | 该论文提出了一种从贝叶斯的角度估计无监督异常检测中污染因子的后验分布的方法，并使用多个异常检测器的输出作为表示，并通过特定的混合形式进行估计。在22个数据集的实证研究中，该方法表现良好。 |
| [^42] | [Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information.](http://arxiv.org/abs/2209.07481) | 本文提供了对准算术混合、散度最小化和Bregman信息的全面分析。通过在密度函数的单调嵌入下使用Bregman散度，我们将常见的散度函数与退火路径上的中间密度关联起来。 |
| [^43] | [On the Selection of Tuning Parameters for Patch-Stitching Embedding Methods.](http://arxiv.org/abs/2207.07218) | 本文研究了拼接嵌入方法中调参参数的选择问题，提出了一种简单的方法来监督选择参数，并发现了新的偏差-方差权衡现象。 |
| [^44] | [Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}.](http://arxiv.org/abs/2010.02968) | 该论文提出了一种结合Fréchet均值和形状不变模型的方法，用于检测功能性轮廓中的形状变化，并构建了功能性数据的控制图，可解释性强且能识别潜在变化。 |

# 详细

[^1]: 通过NAS实现更公平和准确的表格模型

    Fairer and More Accurate Tabular Models Through NAS. (arXiv:2310.12145v1 [cs.LG])

    [http://arxiv.org/abs/2310.12145](http://arxiv.org/abs/2310.12145)

    通过使用多目标神经架构搜索(NAS)和超参数优化(HPO)，我们在表格数据领域首次提出了一种更新模型架构和超参数的策略，以寻找更公平和准确的模型。我们发现，仅针对准确性进行优化可能会导致公平性的降低，因此需要同时考虑准确性和公平性。

    

    长期以来，通过算法使得表格数据的模型更加公平一直是研究的课题。现有的技术通常针对存在不可取的结果的神经模型，通过改变数据的摄入方式、模型权重或输出处理方式来修复。我们采用了新的策略，在去偏过程中考虑更新模型的架构和训练超参数，以找到一个从一开始在预测结果上更好的新模型。在这项工作中，我们首次将多目标神经架构搜索(NAS)和超参数优化(HPO)应用于具有挑战性的表格数据领域。我们在不同数据集上对MLP、ResNet和FT-Transformer等不同架构和超参数空间进行了广泛的探索，展示了模型预测的准确性和公平性指标对超参数组合的依赖关系。我们表明，仅针对准确性进行优化的模型可能会导致公平性的降低，因此需要在优化过程中同时考虑准确性和公平性。

    Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for ac
    
[^2]: 简单机制用于表示、索引和操作概念

    Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])

    [http://arxiv.org/abs/2310.12143](http://arxiv.org/abs/2310.12143)

    通过查看概念的矩阵统计量，生成一个概念的具体表示或签名，可以用于发现概念之间的结构并递归产生更高级的概念，同时可以通过概念的签名来找到相关的共同主题。

    

    深度网络通常通过分类器学习概念，这涉及设置模型并通过梯度下降训练它以适应具有标记概念的数据。我们将提出一个不同的观点，即可以通过查看概念的矩阵矩阵统计量来生成概念的具体表示或签名。这些签名可以用于发现一组概念的结构，并且可以通过从这些签名中学习该结构来递归地产生更高级的概念。当概念"相交"时，概念的签名可以用于在一些相关的"相交"概念中找到一个共同的主题。这个过程可以用于保持一个概念字典，以便输入能够正确识别并被路由到与输入的(潜在)生成相关的概念集合中。

    Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
    
[^3]: 在线估计与滚动验证：适应性非参数估计与数据流

    Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data. (arXiv:2310.12140v1 [math.ST])

    [http://arxiv.org/abs/2310.12140](http://arxiv.org/abs/2310.12140)

    本研究提出了一种在线估计方法，通过加权滚动验证过程来提高基本估计器的自适应收敛速度，并证明了这种方法的重要性和敏感性

    

    由于其高效计算和竞争性的泛化能力，在线非参数估计器越来越受欢迎。一个重要的例子是随机梯度下降的变体。这些算法通常一次只取一个样本点，并立即更新感兴趣的参数估计。在这项工作中，我们考虑了这些在线算法的模型选择和超参数调整。我们提出了一种加权滚动验证过程，一种在线的留一交叉验证变体，对于许多典型的随机梯度下降估计器来说，额外的计算成本最小。类似于批量交叉验证，它可以提升基本估计器的自适应收敛速度。我们的理论分析很简单，主要依赖于一些一般的统计稳定性假设。模拟研究强调了滚动验证中发散权重在实践中的重要性，并证明了即使只有一个很小的偏差，它的敏感性也很高

    Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and instantly update the parameter estimate of interest. In this work we consider model selection and hyperparameter tuning for such online algorithms. We propose a weighted rolling-validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators. Similar to batch cross-validation, it can boost base estimators to achieve a better, adaptive convergence rate. Our theoretical analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in rolling validation in practice and demonstrates its sensitivity even when there is only a slim
    
[^4]: 基于MMD的分布随机森林的变量重要性

    MMD-based Variable Importance for Distributional Random Forest. (arXiv:2310.12115v1 [stat.ML])

    [http://arxiv.org/abs/2310.12115](http://arxiv.org/abs/2310.12115)

    本文介绍了基于MMD距离和经典的drop and relearn原理的变量重要性算法，可以在分布随机森林中检测影响输出分布的变量，并且在实证性能上超越了竞争对手。

    

    分布随机森林（DRF）是一种灵活的基于森林的方法，用于估计给定输入变量的多元输出的全条件分布。在本文中，我们介绍了一种基于经典的drop and relearn原理和MMD距离的DRF变量重要性算法。传统的重要性度量只能发现对输出均值有影响的变量，而我们的算法可以更普遍地发现影响输出分布的变量。我们展示了引入的重要性度量是一致的，在真实数据和模拟数据上具有较高的实证性能，并且超越了竞争对手。特别地，我们的算法通过递归特征消除高效地选择变量，因此可以提供小型变量集合来构建准确的条件输出分布估计。

    Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
    
[^5]: 形状和非形状神经网络的微分方程缩放极限

    Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks. (arXiv:2310.12079v1 [stat.ML])

    [http://arxiv.org/abs/2310.12079](http://arxiv.org/abs/2310.12079)

    最近的研究发现，对于具有形状激活函数的神经网络，其缩放极限可以由微分方程描述。然而，关于未经形状处理的神经网络的信息尚不明确。本文研究了两种未经形状处理的网络，发现它们也可以由类似的微分方程来描述，并给出了它们的一些特征。

    

    最近对具有形状激活函数（即随着网络规模增大而缩放的激活函数）的神经网络进行的分析表明，它们具有由微分方程描述的缩放极限。然而，这些结果不预先告诉我们关于“普通”非形状网络的任何信息，其中激活函数在网络规模增大时保持不变。在本文中，我们针对两种类型的非形状网络找到了类似的基于微分方程的渐近特征描述。首先，我们证明以下两种架构在初始化时会收敛到相同的无限深度和宽度极限：（i）带有残差分支上的 $d^{-1/2}$ 因子的全连接 ResNet，其中 $d$ 是网络的深度；（ii）带有深度 $d \ll$ 宽度 $n$ 和形状 ReLU 激活函数 (activation) 的多层感知机 (MLP)，以 $d^{-1/2}$ 的速率。其次，对于初始化的非形状 MLP，我们推导了层间相关性的一阶渐近修正。特别地，如果 $\rho_\ell$ 是第 $\ell$ 层的相关性，则...

    Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.  Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.  Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer
    
[^6]: 以机器学习为基础的代理模型在贝叶斯逆问题方法中的应用

    Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems. (arXiv:2310.12046v1 [stat.ML])

    [http://arxiv.org/abs/2310.12046](http://arxiv.org/abs/2310.12046)

    本文探讨了在贝叶斯逆问题中，使用机器学习为基础的代理模型，以提高计算效率和处理数值挑战问题。

    

    神经网络已成为一种强大的工具，作为代理模型，在增加计算效率的同时为科学问题提供数值解。这种效率在时间至关重要的数值挑战问题或需要评估许多类似分析方案的情况下具有优势。一个特定的科学兴趣领域是逆问题的设置，其中我们知道系统的正向动态由偏微分方程描述，任务是根据这些动态的（潜在有噪声的）观测来推断系统的性质。我们考虑推断给定2D声波方程的嘈杂解的方块域中波源的位置的逆问题。在假设为高斯噪声的情况下，可以构造源位置的似然函数，每个评估都需要对系统进行一次正向模拟。使用标准的神经网络作为代理模型

    Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model make
    
[^7]: 一个理论框架来理解从人类偏好中学习的一般方法

    A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])

    [http://arxiv.org/abs/2310.12036](http://arxiv.org/abs/2310.12036)

    本文研究了学习从人类偏好中学习的实际算法的理论基础，推导出一个新的一般目标，绕过了两个重要的近似。这种方法允许直接从收集的数据中学习策略而无需奖励模型的训练。

    

    目前从人类偏好中学习的流行方法依赖于两个重要的近似：第一假设可以用逐点奖励替代成对偏好。第二个假设是在这些逐点奖励上训练的奖励模型可以从收集到的数据泛化到策略采样的超出分布的数据。最近，提出了一种称为直接偏好优化(DPO)的方法，该方法绕过了第二个近似，并直接从收集的数据中学习策略而无需奖励模型阶段。然而，这种方法仍然严重依赖于第一个近似。在本文中，我们试图对这些实际算法进行更深入的理论理解。特别地，我们推导出了一个新的一般目标，称为ΨPO，用于从人类偏好中学习，该目标以成对偏好的形式表达，因此绕过了这两个近似。这个新的一般目标使我们能够进行一种新的从训练数据直接学习策略的方法而无需进行奖励模型的训练。

    The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
    
[^8]: 在协变量偏移下使用密度估计进行一致性药物属性预测

    Conformal Drug Property Prediction with Density Estimation under Covariate Shift. (arXiv:2310.12033v1 [cs.LG])

    [http://arxiv.org/abs/2310.12033](http://arxiv.org/abs/2310.12033)

    提出了一种名为CoDrug的方法，该方法利用基于能量的模型和核密度估计来解决协变量偏移问题，从而实现一致性药物属性预测。

    

    在药物发现中，通过计算模型确认药品性质的预测需要进行昂贵的湿实验。因此，获取可靠的不确定性估计对于优先选择药物分子进行后续实验验证至关重要。一致性预测是一种有保证覆盖率的为分子性质创建预测集的有希望的工具。然而，在药物发现任务中，一致性预测的交换性假设往往会受到协变量偏移的挑战：大多数数据集包含有限的标记数据，这些数据可能不足以代表从中提取分子的庞大化学空间。为解决这个问题，我们提出了一种方法称为CoDrug，该方法使用基于能量的模型利用训练数据和无标记数据，并使用核密度估计来评估分子集合的密度。然后，利用估计的密度来加权分子样本以构建预测集并修正以下的

    In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying fo
    
[^9]: LMC多任务高斯过程模型的精确和高效解决方案

    Exact and efficient solutions of the LMC Multitask Gaussian Process model. (arXiv:2310.12032v1 [cs.LG])

    [http://arxiv.org/abs/2310.12032](http://arxiv.org/abs/2310.12032)

    LMC多任务高斯过程模型的精确解决方案表明，只需对噪声模型进行温和假设，即可实现高效计算。通过引入完整参数化的“投影LMC”模型和边缘似然函数表达式，展示了该方法相对于未经处理的方法的优异性能。

    

    线性共同关联模型（LMC）是一种非常通用的多任务高斯过程模型，用于回归或分类。虽然其表达能力和概念简单性很有吸引力，但朴素实现在数据点数量和任务数量方面具有立方复杂度，使得对大多数应用来说，必须进行近似处理。然而，最近的研究表明，在某些条件下，该模型的潜在过程可以解耦，导致仅与所述过程数量呈线性复杂度。我们在这里扩展了这些结果，从最一般的假设中展示了在LMC的高效精确计算所需的唯一条件是对噪声模型进行温和假设。我们引入了结果的完整参数化“投影LMC”模型，并给出了边缘似然函数的表达式，以实现高效的优化。我们对合成数据进行了参数研究，展示了我们方法相对于未经处理的方法的优异性能。

    The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unr
    
[^10]: 无参数离散选择实验与机器学习引导的自适应设计

    Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design. (arXiv:2310.12026v1 [stat.ML])

    [http://arxiv.org/abs/2310.12026](http://arxiv.org/abs/2310.12026)

    提出了无参数离散选择实验与机器学习引导的自适应设计方法GBS，通过自适应构建配对比较问题来满足消费者的偏好，不需要参数化效用模型，可以扩展到具有数百个属性的产品，并在准确性和样本效率方面具有优势。

    

    为了满足消费者的偏好，设计产品对于企业的成功至关重要。我们提出了基于梯度的调查（GBS），这是一种用于多属性产品设计的离散选择实验。该实验通过一系列针对部分配置进行的配对比较来获取消费者的偏好。GBS根据受访者的先前选择自适应地构建配对比较问题。与传统的随机效用最大化范式不同，GBS不需要参数化效用模型，因此对模型规范错误具有鲁棒性。通过将机器学习和实验设计相结合，GBS可扩展到具有数百个属性的产品，并且可以为异质消费者设计个性化产品。我们通过模拟实验证明了GBS相比于现有的参数化和非参数化方法在准确性和样本效率方面的优势。

    Designing products to meet consumers' preferences is essential for a business's success. We propose the Gradient-based Survey (GBS), a discrete choice experiment for multiattribute product design. The experiment elicits consumer preferences through a sequence of paired comparisons for partial profiles. GBS adaptively constructs paired comparison questions based on the respondents' previous choices. Unlike the traditional random utility maximization paradigm, GBS is robust to model misspecification by not requiring a parametric utility model. Cross-pollinating the machine learning and experiment design, GBS is scalable to products with hundreds of attributes and can design personalized products for heterogeneous consumers. We demonstrate the advantage of GBS in accuracy and sample efficiency compared to the existing parametric and nonparametric methods in simulations.
    
[^11]: 连续学习中的贝叶斯流网络

    Bayesian Flow Networks in Continual Learning. (arXiv:2310.12001v1 [cs.LG])

    [http://arxiv.org/abs/2310.12001](http://arxiv.org/abs/2310.12001)

    连续学习中的贝叶斯流网络（BFNs）是一种具有通用生成建模能力的方法，通过结合神经网络和贝叶斯推断，在非稳态数据上取得了良好的实验结果。

    

    最近，贝叶斯流网络（BFNs）被提出作为通用生成建模中非常有前景的方向之一，具有学习任何数据类型的能力。他们的强大之处在于神经网络和贝叶斯推断的表达能力，使它们适用于连续学习的背景下。我们深入研究了BFNs的机制，并进行实验证实了它在非稳态数据上的生成能力。

    Bayesian Flow Networks (BFNs) has been recently proposed as one of the most promising direction to universal generative modelling, having ability to learn any of the data type. Their power comes from the expressiveness of neural networks and Bayesian inference which make them suitable in the context of continual learning. We delve into the mechanics behind BFNs and conduct the experiments to empirically verify the generative capabilities on non-stationary data.
    
[^12]: Vecchia-Laplace近似法在潜在高斯过程模型中的迭代方法

    Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])

    [http://arxiv.org/abs/2310.12000](http://arxiv.org/abs/2310.12000)

    这篇文章介绍了用于潜在高斯过程模型中的Vecchia-Laplace近似法的迭代方法，相比于传统的Cholesky分解方法，可以显著加快计算速度。

    

    潜在高斯过程（GP）模型是灵活的概率非参数函数模型。Vecchia近似是用于克服大数据计算瓶颈的准确近似方法，Laplace近似是一种快速方法，可以近似非高斯似然函数的边缘似然和后验预测分布，并具有渐近收敛保证。然而，当与直接求解方法（如Cholesky分解）结合使用时，Vecchia-Laplace近似的计算复杂度增长超线性地随样本大小增加。因此，与Vecchia-Laplace近似计算相关的运算在通常情况下是最准确的大型数据集时会变得非常缓慢。在本文中，我们提出了几种用于Vecchia-Laplace近似推断的迭代方法，相比于基于Cholesky的计算，可以大大加快计算速度。我们对我们的方法进行了分析。

    Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
    
[^13]: 通过联合子空间估计消除神经网络表示中的错误概念

    Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation. (arXiv:2310.11991v1 [cs.LG])

    [http://arxiv.org/abs/2310.11991](http://arxiv.org/abs/2310.11991)

    该论文提出了一种通过联合子空间估计从神经网络表示中消除错误概念的迭代算法，并在计算机视觉和自然语言处理的基准数据集上展示了其优越性能。

    

    神经网络中的错误相关性经常会影响到模型在样本外的泛化能力。常见的策略是通过从神经网络表示中消除错误概念来缓解这个问题。现有的错误概念消除方法往往过于激进，不经意间会消除与模型主要任务相关的特征，从而影响模型性能。我们提出了一种迭代算法，通过共同识别神经网络表示中的两个低维正交子空间来分离错误和主要任务的概念。我们在计算机视觉（Waterbirds，CelebA）和自然语言处理（MultiNLI）的基准数据集上评估了该算法，并表明它优于现有的概念消除方法。

    Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
    
[^14]: 一种有限时间视角下的主动水平集估计方法

    A Finite-Horizon Approach to Active Level Set Estimation. (arXiv:2310.11985v1 [cs.LG])

    [http://arxiv.org/abs/2310.11985](http://arxiv.org/abs/2310.11985)

    这项研究提出了一种有限时间视角下的主动水平集估计方法，在一维和高维情况下均表现出较高的效果，能够平衡估计误差和已移动距离，具有推广性。

    

    我们考虑了在空间采样中进行主动学习的问题，用于水平集估计（LSE）。该问题的目标是尽快定位所有符合给定阈值上/下的感兴趣函数的区域。我们提出了一种有限时间的搜索过程，以在一维中执行LSE，同时在固定数量样本的情况下，最优地平衡最终估计误差和已移动的距离。通过调整参数来权衡估计准确性和已移动的距离。我们证明了得出的优化问题可以闭式求解，并且所得策略推广了现有的解决方法。然后，我们展示了如何在流行的高斯过程模型下使用这种方法来进行更高维度的水平集估计。对合成数据的实证结果表明，随着旅行成本的增加，我们的方法以非迭代距离处理的能力使其能够显著改善已有方法的能力。

    We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the s
    
[^15]: 可以通过分组缩放提高机器学习回归的预测不确定性的一致性和适应性吗？

    Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?. (arXiv:2310.11978v1 [stat.ML])

    [http://arxiv.org/abs/2310.11978](http://arxiv.org/abs/2310.11978)

    本文研究了分组缩放方法（BVS）的几种改进方法，探索了使用替代损失函数和基于输入特征的分组方案来提高机器学习回归的预测不确定性的一致性和适应性。

    

    最近，分组方差缩放（BVS）被提出作为一种用于机器学习回归问题的预测不确定性的事后校准方法，能够比统一方差（或温度）缩放更有效地进行校正。原始版本的BVS使用基于不确定性的分组，旨在提高条件上的校准性，即一致性。本文探讨了BVS的几种改进方法，特别是在损失函数和基于输入特征（X）的分组方案上进行改进，以提高适应性，即在给定X的条件下进行校准性。将BVS及其改进方案在预测原子化能的基准数据集上进行了性能测试，并与保序回归的结果进行了比较。

    Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
    
[^16]: 可解释的光谱变分自编码器（ISVAE）用于时间序列聚类

    Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering. (arXiv:2310.11940v1 [stat.ML])

    [http://arxiv.org/abs/2310.11940](http://arxiv.org/abs/2310.11940)

    引入了一个可解释的瓶颈，称为滤波器组（FB），用于时间序列聚类的光谱变分自编码器（ISVAE）。通过约束VAE的辨识能力，这个模型能够学习到具有增强可解释性和聚类能力的新编码f_0，并呈现为一个动态的分层树。同时，还提出了与滤波器组结构对称对齐的定制解码器结构，用于处理复杂的数据配置。

    

    最好的编码是具有可解释性的。在本研究中，我们引入了一种新颖的模型，在变分自编码器（VAE）的初期引入了一个可解释的瓶颈，称为滤波器组（FB）。这种安排迫使VAE关注输入信号中最有信息的片段，促进了新编码f_0的学习，使其在传统的潜变量空间上具有增强的可解释性和聚类能力。通过有意地约束VAE使用这个滤波器组，我们有意地限制了它访问广泛输入域信息的能力，促进了一个可辨识、可分离且降低维度的编码的发展。f_0的进化学习轨迹进一步表现为一个动态的分层树，提供了对聚类相似性的深刻洞察。此外，为了处理复杂的数据配置，我们提出了一个与滤波器组结构对称对齐的定制解码器结构。

    The best encoding is the one that is interpretable in nature. In this work, we introduce a novel model that incorporates an interpretable bottleneck-termed the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This arrangement compels the VAE to attend on the most informative segments of the input signal, fostering the learning of a novel encoding ${f_0}$ which boasts enhanced interpretability and clusterability over traditional latent spaces. By deliberately constraining the VAE with this FB, we intentionally constrict its capacity to access broad input domain information, promoting the development of an encoding that is discernible, separable, and of reduced dimensionality. The evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic hierarchical tree, offering profound insights into cluster similarities. Additionally, for handling intricate data configurations, we propose a tailored decoder structure that is symmetrically aligned with FB's architec
    
[^17]: 退火和熵镜像下降之间的联系

    A connection between Tempering and Entropic Mirror Descent. (arXiv:2310.11914v1 [stat.CO])

    [http://arxiv.org/abs/2310.11914](http://arxiv.org/abs/2310.11914)

    本论文研究了退火（针对顺序蒙特卡罗; SMC）和熵镜像下降之间的联系，证明了退火SMC是应用于Kullback-Leibler（KL）散度的熵镜像下降的数值近似，并提出了改进方案。

    

    本论文探讨了退火（针对顺序蒙特卡罗; SMC）和熵镜像下降之间的联系，以从已知未归一化概率密度的目标概率分布中采样。我们证明了退火SMC是应用于Kullback-Leibler（KL）散度的熵镜像下降的数值近似，并获得了退火迭代的收敛速度。我们的结果从优化角度推动了退火迭代，表明退火可以用作Langevin算法的替代选择，以最小化KL散度。我们利用退火和镜像下降迭代之间的联系来证明SMC中常见的做法，并提出了文献中算法的改进方案。

    This paper explores the connections between tempering (for Sequential Monte Carlo; SMC) and entropic mirror descent to sample from a target probability distribution whose unnormalized density is known.  We establish that tempering SMC is a numerical approximation of entropic mirror descent applied to the Kullback-Leibler (KL) divergence and obtain convergence rates for the tempering iterates.  Our result motivates the tempering iterates from an optimization point of view, showing that tempering can be used as an alternative to Langevin-based algorithms to minimize the KL divergence.  We exploit the connection between tempering and mirror descent iterates to justify common practices in SMC and propose improvements to algorithms in literature.
    
[^18]: 具有开关成本和延迟梯度的在线凸优化问题

    Online Convex Optimization with Switching Cost and Delayed Gradients. (arXiv:2310.11880v1 [cs.LG])

    [http://arxiv.org/abs/2310.11880](http://arxiv.org/abs/2310.11880)

    提出了一种在线多梯度下降（OMGD）算法用于解决具有二次和线性开关成本的在线凸优化问题，证明了其竞争比率上界，并在有限信息设置下达到了最优（按顺序）的动态后悔。

    

    我们考虑了在有限信息设置下具有二次和线性开关成本的在线凸优化问题，在这里在线算法仅能利用先前目标函数的梯度信息进行动作选择。对于$L$-光滑和$\mu$-强凸目标函数，我们提出了一种在线多梯度下降（OMGD）算法，并证明该算法在具有二次开关成本的在线凸优化问题上的竞争比率最多为$4(L+5)+\frac{16(L+5)}{\mu}$。对于OMGD的竞争比率上界也被证明在$L$和$\mu$方面是紧致的。此外，当开关成本为二次时，我们证明了任何在线算法的竞争比率是$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$。我们还证明了在有限信息设置下，OMGD算法实现了最优（按顺序）的动态后悔。对于线性开关成本，

    We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In addition, we show that the competitive ratio of any online algorithm is $\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of
    
[^19]: 学习线性分类器混合的SQ下界

    SQ Lower Bounds for Learning Mixtures of Linear Classifiers. (arXiv:2310.11876v1 [cs.LG])

    [http://arxiv.org/abs/2310.11876](http://arxiv.org/abs/2310.11876)

    本文研究了学习线性分类器混合的问题，证明了该问题的统计查询（SQ）算法的复杂度下界是$n^{\mathrm{poly}(1/\Delta) \log(r)}$，同时提出了一种可能具有独立兴趣的新球面设计方法。

    

    我们研究了在高斯协变量下学习线性分类器混合的问题。给定对形式为$(\mathbf{x},y_{\ell})$的$n$维高斯分布的$r$个混合分布样本访问权限，其中$\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$，$y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$，目标是以总变异距离学习潜在的分布。我们的主要结果是统计查询（SQ）的下界，表明对于这个问题的已知算法实际上是最好的，即使对于均匀混合的特殊情况也是如此。特别地，我们证明了对于该问题的任何SQ算法的复杂度都是$n^{\mathrm{poly}(1/\Delta) \log(r)}$，其中$\Delta$是$\mathbf{v}_\ell$之间的两两$\ell_2$-分离的下界。我们结果的关键技术构建是一种可能具有独立兴趣的新球面设计。

    We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and $y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.
    
[^20]: 等变引导法在成像反问题不确定性量化中的应用

    Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems. (arXiv:2310.11838v1 [eess.IV])

    [http://arxiv.org/abs/2310.11838](http://arxiv.org/abs/2310.11838)

    本文提出了一种新的不确定性量化方法，利用参数引导算法的等变形式，可以在成像反问题中量化重构图像的不确定性，并且可以与任何图像重建技术结合使用。

    

    科学成像问题通常存在严重的不适定性，因此具有重要的内在不确定性。准确量化解决方案的不确定性对于严格解释实验结果以及可靠地使用重构图像作为科学证据至关重要。不幸的是，现有的成像方法无法以对实验重复具有鲁棒性的方式量化重构图像中的不确定性。本文提出了一种新的不确定性量化方法，基于参数引导算法的等变形式，利用在成像问题中常见的对称性和不变性特性。此外，所提出的方法是通用的，可以轻松地与任何图像重建技术结合使用，包括只能从观察数据中进行训练的无监督训练策略，从而实现了不确定性量化在只有观测数据的情况下的应用。

    Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where 
    
[^21]: 使用自然梯度替代品优化分布

    Optimising Distributions with Natural Gradient Surrogates. (arXiv:2310.11837v1 [stat.ML])

    [http://arxiv.org/abs/2310.11837](http://arxiv.org/abs/2310.11837)

    本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。

    

    自然梯度方法已经被用于优化各种情况下的概率分布参数，通常能得到快速收敛的过程。然而，对于许多感兴趣的分布，计算自然梯度存在一些挑战。在这项工作中，我们提出了一种新的技术来解决这些问题，这涉及将优化重新定义为关于替代分布参数的优化，计算自然梯度很容易。我们给出了几个可以解释为应用这种技术的现有方法的例子，并提出了一种新的方法，可以将其应用于各种问题。我们的方法扩展了可以有效使用自然梯度的分布集合。此外，它快速、易于理解，可以使用标准的自动微分软件进行简单实现，并且不需要冗长的模型特定导数计算。我们在最大似然估计和变分推断上演示了我们的方法。

    Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and varia
    
[^22]: 在Ridge回归中，核学习“自动”给出精确的低秩解

    Kernel Learning in Ridge Regression "Automatically" Yields Exact Low Rank Solution. (arXiv:2310.11736v1 [math.ST])

    [http://arxiv.org/abs/2310.11736](http://arxiv.org/abs/2310.11736)

    该论文研究了核岭回归问题中核学习的低秩解的性质。在只有低维子空间对响应变量有解释能力的情况下，通过该方法可以自动得到精确的低秩解，无需额外的正则化。

    

    我们考虑形式为$(x,x') \mapsto \phi(\|x-x'\|^2_\Sigma)$且由参数$\Sigma$参数化的核函数。对于这样的核函数，我们研究了核岭回归问题的变体，它同时优化了预测函数和再现核希尔伯特空间的参数$\Sigma$。从这个核岭回归问题中学到的$\Sigma$的特征空间可以告诉我们协变量空间中哪些方向对预测是重要的。假设协变量只通过低维子空间（中心均值子空间）对响应变量有非零的解释能力，我们发现有很高的概率下有限样本核学习目标的全局最小化者也是低秩的。更具体地说，最小化$\Sigma$的秩有很高的概率被中心均值子空间的维度所限制。这个现象很有趣，因为低秩特性是在没有使用任何对$\Sigma$的显式正则化的情况下实现的，例如核范数正则化等。

    We consider kernels of the form $(x,x') \mapsto \phi(\|x-x'\|^2_\Sigma)$ parametrized by $\Sigma$. For such kernels, we study a variant of the kernel ridge regression problem which simultaneously optimizes the prediction function and the parameter $\Sigma$ of the reproducing kernel Hilbert space. The eigenspace of the $\Sigma$ learned from this kernel ridge regression problem can inform us which directions in covariate space are important for prediction.  Assuming that the covariates have nonzero explanatory power for the response only through a low dimensional subspace (central mean subspace), we find that the global minimizer of the finite sample kernel learning objective is also low rank with high probability. More precisely, the rank of the minimizing $\Sigma$ is with high probability bounded by the dimension of the central mean subspace. This phenomenon is interesting because the low rankness property is achieved without using any explicit regularization of $\Sigma$, e.g., nuclear
    
[^23]: 针对具有高基数分类特征的计数数据的主题专用深度神经网络

    Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features. (arXiv:2310.11654v1 [cs.LG])

    [http://arxiv.org/abs/2310.11654](http://arxiv.org/abs/2310.11654)

    本文提出了一种针对计数数据的主题专用深度神经网络，通过引入伽玛随机效应来提高预测性能，并同时获得了固定参数的最大似然估计和随机效应的最佳无偏预测器。该方法可以快速处理高基数分类特征的聚类计数数据，并且可以轻松实现最先进的网络架构。

    

    由于实际数据通常呈现出相关性，针对主题特定的预测使用深度神经网络（DNNs）的兴趣日益增长，但传统DNN框架通常忽视了这一点。在本文中，我们提出了一种新颖的层次似然学习框架，将伽玛随机效应引入到泊松DNN中，以通过捕捉输入变量的非线性效应和主题特定的聚类效应来提高预测性能。所提出的方法通过优化单个目标函数同时获得了固定参数的最大似然估计和随机效应的最佳无偏预测器。该方法能够快速处理涉及高基数分类特征的聚类计数数据的端到端算法。此外，最先进的网络架构可以轻松实现到所提出的h-likelihood框架当中。例如，我们引入了多头注意力层和一个稀疏层

    There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a spars
    
[^24]: 面向带有强对抗损失和强盗反馈的对抗性线性MDPs的最优遗憾

    Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback. (arXiv:2310.11550v1 [cs.LG])

    [http://arxiv.org/abs/2310.11550](http://arxiv.org/abs/2310.11550)

    研究带有对抗损失和强盗反馈的线性MDPs问题，提出了两种算法，分别达到了$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$和$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$的遗憾性能。

    

    我们研究了在线强化学习中的线性马尔可夫决策过程，并考虑了对抗性损失和强盗反馈，没有事先了解转换或访问模拟器的先验知识。我们引入了两种算法，相较于现有方法，它们都能取得更好的遗憾性能。第一种算法虽然计算效率低，但能保证$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$的遗憾性能，其中$K$是回合数。这是该设置下第一个具有最佳$K$依赖性的结果。第二种算法基于策略优化框架，能保证$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$的遗憾性能，并且计算效率高。我们的两个结果都显著改善了现有最先进方法：Kong等人[2023]的计算效率低的算法，其遗憾性能为$\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$，其中$\lambda_{\min}$是问题相关常数。

    We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$ regret, for some problem-dependent constant $\lam
    
[^25]: 在无限时域的马尔可夫决策过程中，利用离线数据集进行高效在线学习：一种贝叶斯方法

    Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])

    [http://arxiv.org/abs/2310.11531](http://arxiv.org/abs/2310.11531)

    本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。

    

    本文研究了当存在一个离线数据集时，如何在无限时域设置下进行高效的在线强化学习问题。我们假设离线数据集是由一个专家生成的，但其能力水平未知，即它不是完美的，也不一定使用最优策略。我们展示了如果学习代理模拟专家使用的行为策略（由能力参数参数化），在累积遗憾最小化方面能取得明显更好的结果。我们建立了一个以 $\tilde{O}(\sqrt{T})$ 为缩放的精确有用PSRL算法遗憾的上界。这需要对贝叶斯在线学习算法在无限时域设置下进行新颖的先验相关遗憾分析。然后，我们提出了一种近似的Informed RLSVI算法，可以理解为使用离线数据集进行模仿学习，然后进行在线学习。

    In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
    
[^26]: 薄而深的高斯过程

    Thin and Deep Gaussian Processes. (arXiv:2310.11527v1 [stat.ML])

    [http://arxiv.org/abs/2310.11527](http://arxiv.org/abs/2310.11527)

    本文提出了一种薄而深的高斯过程方法，克服了传统方法的局限性，并在学习低维嵌入和解释性之间取得了平衡。

    

    高斯过程（GPs）可以提供一种可靠的方法来量化不确定性，具有易于解释的内核超参数，如长度尺度，可以控制函数值的相关距离。然而，选择合适的内核可能具有挑战性。深度高斯过程通过逐层参数化GP层的内核，避免了手动内核工程，使其能够学习解释输出数据的低维嵌入方法。沿用深度神经网络的架构，最常见的深度高斯过程逐层变形输入空间，但失去了浅层高斯过程的所有解释性。另一种构建方法是逐层参数化内核的长度尺度，提高了解释性，但最终放弃了学习低维嵌入的概念。不幸的是，这两种方法都容易受到特定的病态影响，可能会阻碍拟合并限制其可解释性。本文提出了一种新的综合方法

    Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis
    
[^27]: 关于贝叶斯图神经网络在一致预测中的温度问题

    On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v1 [cs.LG])

    [http://arxiv.org/abs/2310.11479](http://arxiv.org/abs/2310.11479)

    该论文探讨了将温度参数纳入贝叶斯图神经网络在一致预测中的优势，以提供有效的不确定性量化。

    

    准确的不确定性量化对于图神经网络(GNNs)至关重要，特别是在高风险领域中经常使用GNNs的情况下。一致预测(CP)为任何黑盒模型提供了一个量化不确定性的有前途的框架。CP保证了一个预测集以所需的概率包含真实标签的形式的官方概率保证。然而，预测集的大小，即"低效率"，受到底层模型和数据生成过程的影响。另一方面，贝叶斯学习还基于估计的后验分布提供一个可信区域，但只有在模型正确指定的情况下，这个区域才是"良好校准"的。在一个最近的工作的基础上，该工作引入了一个缩放参数，用于从后验估计中构建有效的可信区域，我们的研究探讨了在CP框架中将一个温度参数纳入贝叶斯GNNs中的优势。

    Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP fra
    
[^28]: 在人工和生物神经系统中识别可解释的视觉特征

    Identifying Interpretable Visual Features in Artificial and Biological Neural Systems. (arXiv:2310.11431v1 [stat.ML])

    [http://arxiv.org/abs/2310.11431](http://arxiv.org/abs/2310.11431)

    本文提出了一种量化视觉可解释性的自动化方法，并找到了卷积神经网络中有意义的方向。

    

    神经网络中的单个神经元通常是“可解释的”，因为它们代表个别直观有意义的特征。然而，许多神经元表现出“混合选择性”，即它们代表多个不相关的特征。最近的假设认为，深度网络中的特征可能以“叠加”的方式表示，即由多个神经元沿非正交轴表示，因为自然数据中可解释的特征数通常大于给定网络中的神经元数。因此，我们应该能够在激活空间中找到与个别神经元不对齐的有意义的方向。在这里，我们提出了（1）一种自动化的方法来量化视觉可解释性，并通过与大量人类心理物理学对神经元可解释性的判断进行验证，以及（2）一种在网络激活空间中寻找有意义方向的方法。我们利用这些方法来发现卷积神经网络中的方向。

    Single neurons in neural networks are often ``interpretable'' in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural
    
[^29]: 针对跳跃不连续函数的替代主动子空间

    Surrogate Active Subspaces for Jump-Discontinuous Functions. (arXiv:2310.10907v1 [stat.ML])

    [http://arxiv.org/abs/2310.10907](http://arxiv.org/abs/2310.10907)

    该论文提出了一种针对不连续函数的替代主动子空间方法，扩展了活跃子空间的应用范围，并通过数值实验验证了该方法的有效性。

    

    替代建模和活跃子空间已经成为计算科学和工程领域的强大范例。将这些技术应用于社会科学中的计算模型，突显了它们在处理离散输出的Agent-Based模型等不连续模拟器时的局限性。然而，之前的应用研究已经表明，对于这类估计器，替代计算的活跃子空间可以产生有趣的结果。但是，由于活跃子空间是通过梯度定义的，当将该方法应用于不连续模拟器时，估计的是什么量还不清楚。本文首先展示了进行此类分析时可能出现的一些病态情况。这促使我们将活跃子空间扩展到不连续函数上，澄清了在此类分析中实际估计的内容。我们还对合成测试函数进行了数值实验，比较了活跃子空间的高斯过程估计。

    Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active sub
    
[^30]: 自适应最大化社会福利

    Adaptive maximization of social welfare. (arXiv:2310.09597v1 [econ.EM])

    [http://arxiv.org/abs/2310.09597](http://arxiv.org/abs/2310.09597)

    论文研究了通过适应性策略选择最大化社会福利的问题，并提供了关于遗憾的下界和算法的匹配上界。研究发现福利最大化比多臂老虎机问题更困难，但该算法达到了最优增长速率。

    

    我们考虑了重复选择政策以最大化社会福利的问题。福利是个人效用和公共收入的加权和。早期的结果影响后续的政策选择。效用不可观测，但可以间接推断。响应函数通过实验学习获得。我们推导出了一个关于遗憾的下界，并且对于一种Exp3算法的匹配对策对立上界。累积遗憾以$T^{2/3}$的速率增长。这意味着(i)福利最大化比多臂老虎机问题更困难（对于有限的政策集来说，增长速率为$T^{1/2}$），和(ii)我们的算法实现了最优增长速率。对于随机设置，如果社会福利是凹的，我们可以使用二分搜索算法在连续政策集上实现$T^{1/2}$的速率。我们分析了非线性收入税扩展，并概述了商品税扩展。我们将我们的设置与垄断定价（更容易）和双边交易的定价进行了比较。

    We consider the problem of repeatedly choosing policies to maximize social welfare. Welfare is a weighted sum of private utility and public revenue. Earlier outcomes inform later policies. Utility is not observed, but indirectly inferred. Response functions are learned through experimentation.  We derive a lower bound on regret, and a matching adversarial upper bound for a variant of the Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This implies that (i) welfare maximization is harder than the multi-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our algorithm achieves the optimal rate. For the stochastic setting, if social welfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy sets), using a dyadic search algorithm.  We analyze an extension to nonlinear income taxation, and sketch an extension to commodity taxation. We compare our setting to monopoly pricing (which is easier), and price setting for bilateral tra
    
[^31]: 混合方差流用于离散变量

    Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])

    [http://arxiv.org/abs/2308.15613](http://arxiv.org/abs/2308.15613)

    本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。

    

    变分流允许从事者学习复杂的连续分布，但是近似离散分布仍然是一个挑战。目前的方法通常将离散目标嵌入连续空间中-通常是通过连续松弛或去量化-然后应用连续流动。这些方法涉及一个可能无法捕捉到原始离散目标的替代目标，可能具有偏倚或不稳定的梯度，并且可能会创建一个困难的优化问题。在这项工作中，我们开发了一种针对离散分布的变分流族，而不需要任何连续嵌入。首先，我们开发了一个保持度量的离散可逆映射，使离散目标保持不变，然后基于该映射创建了一个混合变分流(MAD Mix)。我们还开发了一个扩展，用于处理联合离散和连续模型。我们的实验表明，MAD Mix产生了比连续嵌入流更可靠的近似。

    Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
    
[^32]: 架起可信度与开放世界学习的桥梁：一种探索性神经方法，用于增强可解释性、泛化性和鲁棒性

    Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])

    [http://arxiv.org/abs/2308.03666](http://arxiv.org/abs/2308.03666)

    该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。

    

    随着研究人员努力缩小机器智能与人类之间的差距，通过发展人工智能技术，我们必须认识到可信度在开放世界中的关键重要性，在日常生活的各个方面对每个人都已经无处不在。然而，目前的人工智能系统存在几个挑战，可能会导致信任危机：1）对预测结果的解释不足；2）学习模型的泛化性不足；3）对不确定环境的适应能力差。因此，我们探索了一种神经程序，用于架起可信度与开放世界学习之间的桥梁，从单模态扩展到多模态场景，以供读者使用。1）为了增强设计级可解释性，我们首先定制了具有特定物理含义的可信网络；2）然后，通过灵活的学习正则化器设计环境福祉任务接口，以改善可信网络的泛化性能。

    As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
    
[^33]: 有限内存贪婪拟牛顿方法与非渐进超线性收敛速率

    Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate. (arXiv:2306.15444v1 [math.OC])

    [http://arxiv.org/abs/2306.15444](http://arxiv.org/abs/2306.15444)

    有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。

    

    非渐进收敛分析表明，拟牛顿方法的显式超线性速率为O$((1/\sqrt{t})^t)$。然而，获得这一速率的方法存在一个众所周知的缺点：它们需要存储先前的黑塞近似矩阵，或者存储所有过去的曲率信息以形成当前的黑塞逆近似。有限内存的拟牛顿方法（如著名的L-BFGS）通过利用有限窗口的过去曲率信息来构造黑塞逆近似，从而缓解了这个问题。因此，它们的每次迭代复杂度和存储需求为O$(\tau d)$，其中$\tau \le d$ 是窗口的大小，$d$ 是问题的维数，从而降低了标准拟牛顿方法的O$(d^2)$ 计算成本和内存需求。然而，据我们所知，没有结果表明有限内存拟牛顿方法存在非渐进超线性收敛速率。

    Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit superlinear rate of O$((1/\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is O$(\tau d)$ where $\tau \le d$ is the size of the window and $d$ is the problem dimension reducing the O$(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for a
    
[^34]: 分布式随机分布的异构奖励多智能体多臂赌博机

    Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards. (arXiv:2306.05579v1 [cs.LG])

    [http://arxiv.org/abs/2306.05579](http://arxiv.org/abs/2306.05579)

    本文提出了一种处理多智能体多臂赌博机问题的算法框架，通过稳健模拟生成随机图并将加权技术结合UCB算法，以协作方式减小整个系统的总遗憾。

    

    本文研究了分布式多智能体多臂赌博机问题，多个客户端通过由环境提供的时间依赖性随机图进行连接。每个臂的奖励分布因客户而异，并且奖励是根据包括亚指数和亚高斯分布在内的分布，由环境独立地随时间生成的。每个客户端都会拉动一个臂，并根据由环境提供的图与邻居进行通信。我们的目标是通过协作来减小整个系统的总遗憾。为此，我们引入了一种新的算法框架，该框架首先提供了使用快速混合马尔可夫链或随机图模型生成随机图的稳健仿真方法，然后将基于平均一致性方法和新提出的加权技术以及上置信限结合起来，提供了一种UCB类型的解决方案。我们的算法考虑到了图形中的随机性，消除了限制条件。

    We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing Markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the con
    
[^35]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^36]: 子采样与岭回归的广义等价性研究

    Generalized equivalences between subsampling and ridge regularization. (arXiv:2305.18496v1 [math.ST])

    [http://arxiv.org/abs/2305.18496](http://arxiv.org/abs/2305.18496)

    此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。

    

    我们针对举集岭估计器，建立了子采样和岭回归之间的精确结构和风险等价性。具体而言，我们证明了，当用不同的岭正则化水平$\lambda$和子采样比例$\psi$拟合子样岭估计器的线性和二次泛函，在$(\lambda,\psi)$-平面上沿着特定路径渐近等价（其中$\psi$是特征维度与子采样大小的比率）。我们的结果仅要求特征和响应分布具有有界矩，并允许任意联合分布。此外，我们提供了一种数据相关的方法来确定$(\lambda,\psi)$的等价路径。我们结果的间接含义是，在数据方面比例中，调优的岭回归呈现出单调预测风险。这解决了Nakkiran等人提出的一个近期未解决的开放性问题，在一般数据分布和温和的正则条件下。

    We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda, \psi )$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a datadependent method to determine the equivalent paths of $(\lambda, \psi )$. An indirect implication of our equivalences is that optimally-tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. under general data distributions and a mild regularity condition that
    
[^37]: 面向方向的多目标学习：简单且可证明的随机算法

    Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])

    [http://arxiv.org/abs/2305.18409](http://arxiv.org/abs/2305.18409)

    本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。

    

    多目标优化（MOO）已成为许多与多个目标相关的机器学习问题（如多标准学习和多任务学习（MTL））中一个有影响力的框架。本文提出了一种新的面向方向的多目标问题，通过在一个方向的邻域内限制公共下降方向来规范线性组合目标的最优方向，例如MTL中的平均损失。 这个公式包括GD和MGDA作为特殊情况，享受像CAGrad中的面向方向的好处，以及有利于随机算法的设计。为了解决这个问题，我们提出了随机方向导向多目标梯度下降（SDMGrad），它使用简单的SGD类型的更新算法，以及在目标数量较多的情况下，使用高效的目标采样的SDMGrad-OS算法。 对于恒定的正则化参数λ，我们证明SDMGrad和SDMGrad-OS确实收敛到帕累托稳定点。

    Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
    
[^38]: 双保真度变分自编码器用于不确定性量化

    Bi-fidelity Variational Auto-encoder for Uncertainty Quantification. (arXiv:2305.16530v1 [stat.ML])

    [http://arxiv.org/abs/2305.16530](http://arxiv.org/abs/2305.16530)

    本文提出了一种双保真度变分自编码器方法，用于在低、高保真度样本中估计物理系统中量的不确定性，平衡了计算效率和数值精度之间的需求。

    

    在模型验证中，量化物理系统感兴趣的量的不确定性是一个主要目标。然而，实现这一目标需要平衡计算效率和数值精度之间的需求。为了解决这个问题，我们提出了一种新颖的双保真度变分自编码器（BF-VAE）公式，旨在从物理系统中低、高保真度样本中估计与量感兴趣的量有关的不确定性。该模型通过利用从低保真度样本得出的信息来逼近高保真度量的统计信息。具体而言，我们设计了一个在潜在空间中的双保真度自回归模型，将其整合到VAE的概率编码-解码结构中。我们提出了一种有效的算法，以在存在有限高保真度数据的情况下，最大化高保真度对数似然的变分下界，从而以较低的计算成本合成高保真度的实现。此外，我们在各种数值示例中证明了我们方法的有效性，包括非线性随机系统和计算流体动力学模拟。

    Quantifying the uncertainty of quantities of interest (QoIs) from physical systems is a primary objective in model validation. However, achieving this goal entails balancing the need for computational efficiency with the requirement for numerical accuracy. To address this trade-off, we propose a novel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to estimate the uncertainty associated with a QoI from low-fidelity (LF) and high-fidelity (HF) samples of the QoI. This model allows for the approximation of the statistics of the HF QoI by leveraging information derived from its LF counterpart. Specifically, we design a bi-fidelity auto-regressive model in the latent space that is integrated within the VAE's probabilistic encoder-decoder structure. An effective algorithm is proposed to maximize the variational lower bound of the HF log-likelihood in the presence of limited HF data, resulting in the synthesis of HF realizations with a reduced computational cost. Addit
    
[^39]: 神经网络何时在表格数据上胜过增强树？

    When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])

    [http://arxiv.org/abs/2305.02997](http://arxiv.org/abs/2305.02997)

    这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。

    

    表格数据是机器学习中最常用的数据类型之一。尽管神经网络（NN）在表格数据上取得了最近的进展，但人们仍在积极讨论NN是否通常优于梯度提升决策树（GBDT）在表格数据上的表现，一些最近的工作要么认为GBDT在表格数据上一贯优于NN，要么认为NN优于GBDT。在这项工作中，我们退一步问：'这重要吗？'我们通过对176个数据集比较19种算法，进行了迄今为止最大的表格数据分析，并发现'NN vs. GBDT'争论被过分强调：令人惊讶的是，在相当多的数据集中，GBDT和NN之间的性能差异要么可以忽略不计，要么GBDT的轻微超参数调整比选择最佳算法更重要。接下来，我们分析了965个元特征，以确定数据集的哪些特性使NN或GBDT更适合表现良好。例如，我们发现GBDT要比NN在高维稀疏数据上表现更好。

    Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
    
[^40]: Marich：一种使用公共数据的查询效率高的分布等价模型提取攻击

    Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data. (arXiv:2302.08466v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08466](http://arxiv.org/abs/2302.08466)

    本研究设计了一种黑盒模型提取攻击方法Marich，它使用公共数据集中的最小数量查询来创建一个与目标模型具有信息丰富度和分布等价性的副本，实验结果表明Marich能提取具有60-95%真实模型准确性的模型。

    

    我们研究设计黑盒模型提取攻击，该攻击可以通过一个预测API从一个公开可用的数据集向目标ML模型发送最小数量的查询，以创建一个具有信息丰富度和分布等价性的目标副本。首先，我们定义了分布等价和最大信息模型提取攻击，并将它们简化为一个变分优化问题。攻击者顺序解决这个优化问题，选择最具信息量的查询，同时最大化熵和降低目标和盗窃模型之间的不匹配。这导致了一种基于主动抽样的查询选择算法Marich，它是模型无关的。然后，我们在不同的文本和图像数据集以及不同的模型上评估了Marich。Marich提取的模型实现了真实模型准确性的60-95％，并使用了来自公开可用数据集的1,000-8,500个查询。

    We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\sim 60-95\%$ of true model's accuracy and uses $\sim 1,000 - 8,500$ queries from the publicly available datasets, which are differen
    
[^41]: 无监督异常检测中污染因子分布的估计

    Estimating the Contamination Factor's Distribution in Unsupervised Anomaly Detection. (arXiv:2210.10487v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10487](http://arxiv.org/abs/2210.10487)

    该论文提出了一种从贝叶斯的角度估计无监督异常检测中污染因子的后验分布的方法，并使用多个异常检测器的输出作为表示，并通过特定的混合形式进行估计。在22个数据集的实证研究中，该方法表现良好。

    

    异常检测方法在无监督情况下通过为示例分配基于各种启发式规则的实值异常分数来识别不符合预期行为的示例。这些分数需要通过阈值化转换为实际预测，从而使被标记为异常的示例比例等于预期的异常比例，称为污染因子。不幸的是，目前没有好的方法来估计污染因子本身。我们从贝叶斯的角度来解决这个问题，引入了一种估计给定无标签数据集的污染因子的后验分布的方法。我们利用多个异常检测器的输出作为已经捕捉到异常性的表示，并使用特定的混合形式来估计污染。在22个数据集的实证研究中，我们表明估计的分布是良好校准的，并且通过设置阈值使用这种方法可以取得良好的性能。

    Anomaly detection methods identify examples that do not follow the expected behaviour, typically in an unsupervised fashion, by assigning real-valued anomaly scores to the examples based on various heuristics. These scores need to be transformed into actual predictions by thresholding, so that the proportion of examples marked as anomalies equals the expected proportion of anomalies, called contamination factor. Unfortunately, there are no good methods for estimating the contamination factor itself. We address this need from a Bayesian perspective, introducing a method for estimating the posterior distribution of the contamination factor of a given unlabeled dataset. We leverage on outputs of several anomaly detectors as a representation that already captures the basic notion of anomalousness and estimate the contamination using a specific mixture formulation. Empirically on 22 datasets, we show that the estimated distribution is well-calibrated and that setting the threshold using the
    
[^42]: 准算术混合、散度最小化和Bregman信息

    Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information. (arXiv:2209.07481v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07481](http://arxiv.org/abs/2209.07481)

    本文提供了对准算术混合、散度最小化和Bregman信息的全面分析。通过在密度函数的单调嵌入下使用Bregman散度，我们将常见的散度函数与退火路径上的中间密度关联起来。

    

    马尔可夫链蒙特卡洛方法用于从复杂分布中采样和估计归一化常数通常模拟沿着连接可跟踪初始分布和目标密度的退火路径的中间分布的样本。先前的工作使用准算术平均构建了退火路径，并解释了由此产生的中间密度是最小化期望散度到端点的。我们通过在密度函数的单调嵌入下使用Bregman散度对这个“质心”性质进行了全面分析，从而将常见的散度（如Amari和Renyi的alpha散度、（alpha，beta）散度和Jensen-Shannon散度）与沿着退火路径的中间密度关联起来。我们的分析突出了参数化族、准算术平均和散度函数之间的相互作用，使用了Zhang的rho-tau Bregman散度框架。

    Markov Chain Monte Carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. Prior work has constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. We provide a comprehensive analysis of this 'centroid' property using Bregman divergences under a monotonic embedding of the density function, thereby associating common divergences such as Amari's and Renyi's ${\alpha}$-divergences, ${(\alpha,\beta)}$-divergences, and the Jensen-Shannon divergence with intermediate densities along an annealing path. Our analysis highlights the interplay between parametric families, quasi-arithmetic means, and divergence functions using the rho-tau Bregman divergence framework of Zhang
    
[^43]: 关于拼接嵌入方法调参参数选择的研究

    On the Selection of Tuning Parameters for Patch-Stitching Embedding Methods. (arXiv:2207.07218v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2207.07218](http://arxiv.org/abs/2207.07218)

    本文研究了拼接嵌入方法中调参参数的选择问题，提出了一种简单的方法来监督选择参数，并发现了新的偏差-方差权衡现象。

    

    尽管经典的缩放方法和主成分分析都不需要调参，但其他嵌入多元数据的方法则需要选择一个或多个调参参数。由于无监督的特性，这个调参过程可能会非常困难。我们提出了一个简单且近乎显而易见的方法来监督选择调参参数：最小化一种应力概念。我们将这种方法应用于典型的拼接嵌入方法中的补丁大小选择，无论是在多维缩放（又称网络定位）设置中还是在降维（又称流形学习）设置中。在我们的研究中，我们发现了一种新的偏差-方差权衡现象。

    While classical scaling, just like principal component analysis, is parameter-free, other methods for embedding multivariate data require the selection of one or several tuning parameters. This tuning can be difficult due to the unsupervised nature of the situation. We propose a simple, almost obvious, approach to supervise the choice of tuning parameter(s): minimize a notion of stress. We apply this approach to the selection of the patch size in a prototypical patch-stitching embedding method, both in the multidimensional scaling (aka network localization) setting and in the dimensionality reduction (aka manifold learning) setting. In our study, we uncover a new bias--variance tradeoff phenomenon.
    
[^44]: 功能性轮廓建模和可解释形状变化检测：结合Fréchet均值与形状不变模型的方法

    Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}. (arXiv:2010.02968v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2010.02968](http://arxiv.org/abs/2010.02968)

    该论文提出了一种结合Fréchet均值和形状不变模型的方法，用于检测功能性轮廓中的形状变化，并构建了功能性数据的控制图，可解释性强且能识别潜在变化。

    

    提出了一种适用于检测功能性轮廓中形状变化的建模框架，结合了Fréchet均值概念和变形模型的概念。利用Fréchet均值提供的广义均值感知能够捕捉研究对象轮廓的典型模式，而变形模型的概念，特别是形状不变模型，允许对轮廓与典型形状之间的偏差进行可解释的参数化。构建和提出了与数据的功能性特性和所采用的变形模型相兼容的EWMA类型控制图，利用研究对象的轮廓在广义均值感知下的某些形状特征，实现对形状和/或变形过程潜在变化的识别。进一步将形状变形过程的潜在变化区分为与幅度和/或相位相关的显著变化。

    A modelling framework suitable for detecting shape shifts in functional profiles combining the notion of Fr\'echet mean and the concept of deformation models is developed and proposed. The generalized mean sense offerred by the Fr\'echet mean notion is employed to capture the typical pattern of the profiles under study, while the concept of deformation models, and in particular of the shape invariant model, allows for interpretable parameterizations of profile's deviations from the typical shape. EWMA-type control charts compatible with the functional nature of data and the employed deformation model are built and proposed, exploiting certain shape characteristics of the profiles under study with respect to the generalised mean sense, allowing for the identification of potential shifts concerning the shape and/or the deformation process. Potential shifts in the shape deformation process, are further distingu\-ished to significant shifts with respect to amplitude and/or the phase of the
    

