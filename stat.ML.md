# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bayesian deep learning for cosmic volumes with modified gravity.](http://arxiv.org/abs/2309.00612) | 该研究利用贝叶斯深度学习的方法，从修正引力模拟中提取宇宙学参数，并对不确定性进行了评估。 |
| [^2] | [Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms.](http://arxiv.org/abs/2309.00591) | 本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。 |
| [^3] | [Mechanism of feature learning in convolutional neural networks.](http://arxiv.org/abs/2309.00570) | 本论文研究了卷积神经网络从图像数据中学习特征的机制，并提出了卷积神经特征假设。通过实证分析和理论证明，论文展示了滤波器的协方差与输入补丁的平均梯度外积之间的高度相关性，以及通过基于补丁的平均梯度外积实现深度特征学习的普遍性。 |
| [^4] | [Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data.](http://arxiv.org/abs/2309.00564) | 本文研究了高维线性回归在解释方面的挑战，发现空间零值和正则化对回归系数产生重要影响，并提出了一种优化公式来比较回归系数与物理工程知识得到的系数，从而实现解释性的回归结果。 |
| [^5] | [Interactive and Concentrated Differential Privacy for Bandits.](http://arxiv.org/abs/2309.00557) | 本文研究了在交互学习和推荐系统中隐私保护的Bandit问题，并引入了集中差分隐私的概念。通过提供关于有限臂和线性Bandit问题遗憾的下界，我们揭示了不同隐私预算下的难度区域，并发现集中差分隐私可以比全局差分隐私更有效地保护隐私，我们提出了两种相应的算法。 |
| [^6] | [Area-norm COBRA on Conditional Survival Prediction.](http://arxiv.org/abs/2309.00417) | 本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。 |
| [^7] | [Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond.](http://arxiv.org/abs/2309.00416) | 本研究在个性化、隐私保证和公平性之间解决了联邦学习模型的三元交互作用。差分隐私及其变体被应用为提供正式隐私保证的前沿标准。在多样化的数据集中寻求公平性变得重要。 |
| [^8] | [Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds.](http://arxiv.org/abs/2309.00380) | 本文提出了一种用于多模态数据的深度潜变量模型，并开发了更灵活的编码特征聚合方案，能够紧密地下界数据对数似然。 |
| [^9] | [Differentially Private Functional Summaries via the Independent Component Laplace Process.](http://arxiv.org/abs/2309.00125) | 本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。 |
| [^10] | [On the Implicit Bias of Adam.](http://arxiv.org/abs/2309.00079) | 本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。 |
| [^11] | [Calibrated Explanations for Regression.](http://arxiv.org/abs/2308.16245) | 本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。 |
| [^12] | [Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning.](http://arxiv.org/abs/2308.14705) | 本文提出了一个新的自助培训体制，利用独立子网络的集成和新的损失函数来提高自助的鲁棒性表示学习的效率和多样性。 |
| [^13] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^14] | [SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics.](http://arxiv.org/abs/2302.11055) | 该论文研究了随机梯度下降(SGD)算法在神经网络上的学习时间复杂度，提出了一种复杂度度量称为跃迁，证明了在高斯各向同性数据和二层神经网络上的研究结果，并展示了训练过程中函数支持的动态学习方法。 |
| [^15] | [Scalable and adaptive variational Bayes methods for Hawkes processes.](http://arxiv.org/abs/2212.00293) | 这项研究提出了一种可扩展和自适应的变分贝叶斯方法，用于处理Hawkes过程的依赖关系和非参数推断。通过分析先验、变分类和非线性模型的条件，我们统一了现有的方法，并证明了其渐近性质。 |
| [^16] | [Towards solving model bias in cosmic shear forward modeling.](http://arxiv.org/abs/2210.16243) | 这项研究通过混合物理和深度学习的分层贝叶斯模型，能够在真实星系上恢复出无偏估计的剪切，解决了余辉前向建模中的模型偏差问题。 |
| [^17] | [Generalised Bayesian Inference for Discrete Intractable Likelihood.](http://arxiv.org/abs/2206.08420) | 该论文提出了一种适用于离散难处理似然的广义贝叶斯推断方法，通过使用离散Fisher散度更新参数信念来避免计算困难，并得到一个可以用标准计算工具进行采样的广义后验分布。 |
| [^18] | [The Rich Get Richer: Disparate Impact of Semi-Supervised Learning.](http://arxiv.org/abs/2110.06282) | 本文研究了半监督学习的不平等影响，发现那些在不使用SSL时具有更高基准准确性的子族群更容易从SSL中获益，而那些基准准确性较低的子族群在添加SSL模块后可能会观察到性能下降。 |
| [^19] | [Simulation comparisons between Bayesian and de-biased estimators in low-rank matrix completion.](http://arxiv.org/abs/2103.11749) | 本文通过模拟比较了贝叶斯方法和去偏估计器在低秩矩阵补全问题中的性能。结果表明去偏估计器与贝叶斯估计器一样好，在样本较小的情况下贝叶斯方法更稳定。 |
| [^20] | [Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge.](http://arxiv.org/abs/2012.05465) | 本文提出对抗元学习方法，用于计算在一组与可用知识相容的先验分布中最小化最坏情况的 Bayes 风险的 Gamma-Minimax 估计器，文中还提出了一种神经网络类用于提供估计器类，以及两个实验环节用于说明该方法的应用。 |

# 详细

[^1]: 基于贝叶斯深度学习的宇宙尺度中的修正引力研究

    Bayesian deep learning for cosmic volumes with modified gravity. (arXiv:2309.00612v1 [astro-ph.CO])

    [http://arxiv.org/abs/2309.00612](http://arxiv.org/abs/2309.00612)

    该研究利用贝叶斯深度学习的方法，从修正引力模拟中提取宇宙学参数，并对不确定性进行了评估。

    

    新一代的星系调查将提供前所未有的数据，使我们能够在宇宙尺度上测试引力。对大尺度结构的健壮宇宙学分析需要利用编码在宇宙网中的非线性信息。机器学习技术提供了这样的工具，然而却不能提供先验的不确定性评估。本研究旨在通过具有不确定性估计的深度神经网络从修正引力（MG）模拟中提取宇宙学参数。我们使用贝叶斯神经网络（BNNs）实现了一个丰富的近似后验分布，分别考虑了一个带有单一贝叶斯最后一层（BLL）的情况，和一个在所有层面上都具有贝叶斯层（FullB）的情况。我们使用实空间密度场和一套2000个仅包含暗物质粒子网格$ N $-体模拟的功率谱对这两个BNN进行训练，这些模拟包括基于MG-PICOLA的修正引力模型，覆盖了边长为256 $h^{-1}$ Mpc的立方体体积，其中包含128$。

    The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$
    
[^2]: 快速和遗憾最小的最佳臂识别：基本限制和低复杂度算法

    Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])

    [http://arxiv.org/abs/2309.00591](http://arxiv.org/abs/2309.00591)

    本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。

    

    本文考虑具有双重目标的随机多臂老虎机(MAB)问题：(i) 快速识别并选择最佳臂，以及(ii) 在一系列T个连续回合中最大化奖励。尽管每个目标都已经得到了独立的深入研究，即(i)的最佳臂识别和(ii)的遗憾最小化，但是同时实现这两个目标仍然是一个开放的问题，尽管它在实践中非常重要。本文引入了“遗憾最小化的最佳臂识别”(ROBAI)，旨在实现这两个双重目标。为了解决具有预定停止时间和自适应停止时间要求的ROBAI，我们分别提出了$\mathsf{EOCP}$算法及其变体，不仅在高斯老虎机和一般老虎机中达到了渐进最优遗憾，而且在预定停止时间下，在$\mathcal{O}(\log T)$回合内选择了最佳臂，在自适应停止时间下，选择了最佳臂在$\mathcal{O}(\log^2 T)$回合内。

    This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
    
[^3]: 卷积神经网络中特征学习的机制

    Mechanism of feature learning in convolutional neural networks. (arXiv:2309.00570v1 [stat.ML])

    [http://arxiv.org/abs/2309.00570](http://arxiv.org/abs/2309.00570)

    本论文研究了卷积神经网络从图像数据中学习特征的机制，并提出了卷积神经特征假设。通过实证分析和理论证明，论文展示了滤波器的协方差与输入补丁的平均梯度外积之间的高度相关性，以及通过基于补丁的平均梯度外积实现深度特征学习的普遍性。

    

    理解卷积神经网络如何从图像数据中学习特征是机器学习和计算机视觉中的一个基本问题。在这项工作中，我们确定了这样一个机制。我们提出了卷积神经特征假设，它指出任何卷积层中滤波器的协方差与该层输入的补丁的平均梯度外积成正比。我们对我们的假设提供了大量的实证证据，包括在标准神经网络架构（如AlexNet，VGG和在ImageNet上预训练的ResNets）的卷积层中，发现滤波器的协方差与基于补丁的平均梯度外积之间存在高度相关性。我们还提供了支持理论的证据。然后，我们通过使用基于补丁的平均梯度外积来展示我们结果的普遍性，实现了在卷积核机器中的深度特征学习。我们将得到的算法称为(Deep) ConvRFM，并展示了我们的算法能够恢复出类似的特征。

    Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers simil
    
[^4]: 高维线性回归的解释：空间零值和正则化在电池数据上的影响的演示

    Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data. (arXiv:2309.00564v1 [stat.ML])

    [http://arxiv.org/abs/2309.00564](http://arxiv.org/abs/2309.00564)

    本文研究了高维线性回归在解释方面的挑战，发现空间零值和正则化对回归系数产生重要影响，并提出了一种优化公式来比较回归系数与物理工程知识得到的系数，从而实现解释性的回归结果。

    

    高维线性回归在许多科学领域中非常重要。本文考虑到从化学或生物系统中经常得到的基础平滑潜在过程的离散测量数据。在高维度中解释是具有挑战性的，因为空间零值及其与正则化的相互作用会塑造回归系数。数据的空间零值包含所有满足$\mathbf{Xw}=\mathbf{0}$的系数，从而允许非常不同的系数产生相同的预测。我们开发了一种优化公式来比较回归系数和通过物理工程知识得到的系数，以了解系数差异的哪些部分接近于空间零值。这种空间零值方法在一个合成示例和锂离子电池数据上进行了测试。案例研究表明，如果根据先前的物理知识选择合适的正则化和z-score处理，可以得到可解释的回归结果。

    High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. 
    
[^5]: 交互式和集中式差分隐私在Bandit问题中的应用

    Interactive and Concentrated Differential Privacy for Bandits. (arXiv:2309.00557v1 [stat.ML])

    [http://arxiv.org/abs/2309.00557](http://arxiv.org/abs/2309.00557)

    本文研究了在交互学习和推荐系统中隐私保护的Bandit问题，并引入了集中差分隐私的概念。通过提供关于有限臂和线性Bandit问题遗憾的下界，我们揭示了不同隐私预算下的难度区域，并发现集中差分隐私可以比全局差分隐私更有效地保护隐私，我们提出了两种相应的算法。

    

    Bandit问题在交互式学习方案和现代推荐系统中起着至关重要的作用。然而，这些系统通常依赖于敏感的用户数据，因此隐私是一个重要问题。本文通过交互式差分隐私的视角研究了基于可信集中式决策者的Bandit问题的隐私性。虽然已经对纯ε-全局差分隐私的Bandit问题进行了广泛研究，但我们在理解零集中差分隐私(zCDP)的Bandit问题方面做出了贡献。针对有限臂和线性Bandit问题，我们提供了关于遗憾的最小最大和问题相关下界，从而量化了这些情况下ρ-全局zCDP的代价。这些下界揭示了基于隐私预算ρ的两个困难区域，并表明ρ-全局zCDP比纯ε-全局差分隐私产生的遗憾更小。我们提出了两种有限臂和线性Bandit问题的ρ-全局zCDP算法，即AdaC-UCB和AdaC-GOPE。这两个算法都使用了高斯机制的共同策略。

    Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism 
    
[^6]: 条件生存预测中的面积规范COBRA

    Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])

    [http://arxiv.org/abs/2309.00417](http://arxiv.org/abs/2309.00417)

    本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。

    

    本文探讨了一种不同的组合回归策略来计算条件生存函数。我们使用基于回归的弱学习器来创建所提出的集成技术。所提出的组合回归策略使用相似度度量作为两个生存曲线之间的面积。所提出的模型表明其表现优于随机生存森林。本文讨论了一种在组合回归设置中选择最重要变量的新技术。我们进行了一项模拟研究，表明我们对变量相关性的提议效果很好。我们还使用了三个真实数据集来说明该模型。

    The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
    
[^7]: 推进个性化联邦学习：团体隐私、公平性等方面的突破

    Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond. (arXiv:2309.00416v1 [cs.LG])

    [http://arxiv.org/abs/2309.00416](http://arxiv.org/abs/2309.00416)

    本研究在个性化、隐私保证和公平性之间解决了联邦学习模型的三元交互作用。差分隐私及其变体被应用为提供正式隐私保证的前沿标准。在多样化的数据集中寻求公平性变得重要。

    

    联邦学习 (FL) 是一种在分布式和协作方式下训练机器学习模型的框架。在训练过程中，一组参与的客户端处理本地存储的数据，仅共享通过最小化其本地输入的成本函数获得的模型更新。FL被提出作为隐私保护机器学习的一种途径，但已被证明易受私人信息泄露、模型个性化缺失以及可能导致某些群体比其他群体更公平的训练模型等问题的影响。在本文中，我们解决了在FL框架中训练的模型在个性化、隐私保证和公平性之间的三元交互作用。差分隐私及其变体已被研究和应用为提供正式隐私保证的前沿标准。然而，FL中的客户端往往拥有非常多样化的数据集，代表着异质的社区，这使得保证公平性变得重要。

    Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only the model updates obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learning, but it has been shown vulnerable to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups than to others. In this paper, we address the triadic interaction among personalization, privacy guarantees, and fairness attained by models trained within the FL framework. Differential privacy and its variants have been studied and applied as cutting-edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important 
    
[^8]: 用排序不变的编码器和更紧的变分边界学习多模态生成模型

    Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds. (arXiv:2309.00380v1 [stat.ML])

    [http://arxiv.org/abs/2309.00380](http://arxiv.org/abs/2309.00380)

    本文提出了一种用于多模态数据的深度潜变量模型，并开发了更灵活的编码特征聚合方案，能够紧密地下界数据对数似然。

    

    设计用于多模态数据的深度潜变量模型一直是机器学习研究中的一个重要主题。多模态变分自编码器 (VAE) 是一种常用的生成模型类别，它学习能够共同解释多种模态的潜在表示。各种客观函数已被提出用于这样的模型，往往以多模态数据对数似然的下界以及信息论方面的考虑为动机。为了对不同模态子集进行编码，我们经常使用并展示了产品型专家 (PoE) 或者混合型专家 (MoE) 聚合方案，这些方案在生成质量或者多模态一致性等方面具有不同的权衡。在本研究中，我们考虑了一个能够紧密地下界数据对数似然的变分边界。我们通过将不同模态的编码特征组合起来，开发了更灵活的聚合方案，这些方案推广了 PoE 或者 MoE 方法。

    Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modali
    
[^9]: 通过独立分量拉普拉斯过程实现差分隐私的函数性摘要

    Differentially Private Functional Summaries via the Independent Component Laplace Process. (arXiv:2309.00125v1 [stat.ML])

    [http://arxiv.org/abs/2309.00125](http://arxiv.org/abs/2309.00125)

    本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。

    

    在这项工作中，我们提出了一种称为独立分量拉普拉斯过程（ICLP）机制的差分隐私函数性摘要的新机制。通过将感兴趣的函数性摘要视为真正无限维对象，并使用ICLP噪声来扰动它们，该新机制放宽了关于数据轨迹的假设，并相对于文献中的经典有限维子空间嵌入方法保留了更高的效用。我们在多个函数空间中验证了所提出机制的可行性。我们考虑了几个统计估计问题，并通过轻微过平滑摘要来证明隐私成本不会主导统计误差，并且在渐近情况下可以忽略。对合成和真实数据集的数值实验证明了所提出机制的有效性。

    In this work, we propose a new mechanism for releasing differentially private functional summaries called the Independent Component Laplace Process, or ICLP, mechanism. By treating the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over-smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism.
    
[^10]: 关于Adam的隐式偏差

    On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])

    [http://arxiv.org/abs/2309.00079](http://arxiv.org/abs/2309.00079)

    本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。

    

    在以前的文献中，后向误差分析被用来找到近似梯度下降轨迹的常微分方程（ODEs）。发现有限步长会隐式地规范化解决方案，因为出现在ODE中的项会惩罚损失梯度的二范数。我们证明了RMSProp和Adam中是否存在类似的隐式规范化取决于它们的超参数和训练阶段，但涉及的“范数”不同：对应的ODE项要么惩罚（扰动的）损失梯度的一范数，要么相反地阻止其减小（后一种情况是典型的）。我们还进行了数值实验，并讨论了这些证明事实如何影响泛化。

    In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
    
[^11]: 回归问题的校准解释

    Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])

    [http://arxiv.org/abs/2308.16245](http://arxiv.org/abs/2308.16245)

    本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。

    

    人工智能（AI）通常是现代决策支持系统（DSS）的一部分。在基于AI的DSS中使用的最佳预测模型缺乏透明度。可解释的人工智能（XAI）旨在创建能够向人类用户解释其理由的AI系统。XAI中的局部解释可以提供关于个别预测的原因的信息，即特征重要性。然而，现有局部解释方法的一个关键缺点是无法量化与特征重要性相关的不确定性。本文介绍了特征重要性解释方法Calibrated Explanations（CE）的扩展，之前只支持分类，现在支持标准回归和概率回归，即目标超过任意阈值的概率。回归问题的扩展保留了CE的所有优点，例如将底层模型的预测与置信度校准。

    Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
    
[^12]: 自助的鲁棒性表示学习的独立子网络多样化集成

    Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning. (arXiv:2308.14705v1 [stat.ML])

    [http://arxiv.org/abs/2308.14705](http://arxiv.org/abs/2308.14705)

    本文提出了一个新的自助培训体制，利用独立子网络的集成和新的损失函数来提高自助的鲁棒性表示学习的效率和多样性。

    

    集成神经网络是提高模型性能、估计不确定性和改善深度有监督学习鲁棒性的广泛承认方法。然而，深层集成通常具有高计算成本和内存需求。此外，深度集成的效率与集成成员之间的多样性有关，这对于大型的过参数化深度神经网络来说是具有挑战性的。而且，集成学习尚未得到如此广泛的采用，并且对于自助的或无监督的表示学习来说仍然是一项具有挑战性的工作。在这些挑战的推动下，我们提出了一个新颖的自助培训体制，利用独立子网络的集成，辅以一个旨在鼓励多样性的新损失函数。我们的方法以高多样性实现了高效的子模型集成，从而获得了良好校准的模型不确定性估计，并与传统方法相比，计算开销最小。

    Ensembling a neural network is a widely recognized approach to enhance model performance, estimate uncertainty, and improve robustness in deep supervised learning. However, deep ensembles often come with high computational costs and memory demands. In addition, the efficiency of a deep ensemble is related to diversity among the ensemble members which is challenging for large, over-parameterized deep neural networks. Moreover, ensemble learning has not yet seen such widespread adoption, and it remains a challenging endeavor for self-supervised or unsupervised representation learning. Motivated by these challenges, we present a novel self-supervised training regime that leverages an ensemble of independent sub-networks, complemented by a new loss function designed to encourage diversity. Our method efficiently builds a sub-model ensemble with high diversity, leading to well-calibrated estimates of model uncertainty, all achieved with minimal computational overhead compared to traditional
    
[^13]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^14]: SGD学习神经网络: 跃迁复杂度与鞍到鞍动力学

    SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. (arXiv:2302.11055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11055](http://arxiv.org/abs/2302.11055)

    该论文研究了随机梯度下降(SGD)算法在神经网络上的学习时间复杂度，提出了一种复杂度度量称为跃迁，证明了在高斯各向同性数据和二层神经网络上的研究结果，并展示了训练过程中函数支持的动态学习方法。

    

    我们研究了随机梯度下降(SGD)学习具有各向同性数据的全连接神经网络的时间复杂度。我们提出了一种复杂度度量——跃迁，用来度量目标函数的"层级"程度。对于$d$维均匀布尔或各向同性高斯数据，我们的主要猜想是学习一个低维支持函数$f$的时间复杂度为$\tilde\Theta (d^{\max(\mathrm{Leap}(f),2)})$。我们在额外对SGD的技术假设下，证明了这个猜想在高斯各向同性数据和二层神经网络上的一个版本。我们展示出训练过程以鞍点到鞍点的动态方式逐渐学习了函数的支持。与[Abbe et al. 2022]不同，我们的结果超越了跃迁1(合并阶梯函数)的限制，并超越了均场和梯度流近似，在这里可以获得完全的复杂度控制。最后，我们指出这给出了完整训练的SGD复杂度。

    We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure -- the leap -- which measures how "hierarchical" target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $\tilde\Theta (d^{\max(\mathrm{Leap}(f),2)})$. We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al. 2022] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity for the full train
    
[^15]: 可扩展和自适应的Hawkes过程变分贝叶斯方法

    Scalable and adaptive variational Bayes methods for Hawkes processes. (arXiv:2212.00293v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.00293](http://arxiv.org/abs/2212.00293)

    这项研究提出了一种可扩展和自适应的变分贝叶斯方法，用于处理Hawkes过程的依赖关系和非参数推断。通过分析先验、变分类和非线性模型的条件，我们统一了现有的方法，并证明了其渐近性质。

    

    Hawkes过程常用于建模多元事件数据集中的依赖和相互作用现象，如神经尖峰序列、社交互动和金融交易。在非参数设置中，学习Hawkes过程的时间依赖结构通常是一项计算成本高昂的任务，尤其是对于贝叶斯估计方法。特别是对于广义非线性Hawkes过程，应用于计算难以处理的后验分布的马尔可夫链蒙特卡罗方法在实践中无法扩展到高维过程。最近，提出了针对后验分布平均场变分逼近的高效算法。在这项工作中，我们首先在一个通用的非参数推断框架下统一了现有的变分贝叶斯方法，并分析了这些方法在先验、变分类和非线性模型上的易于验证的渐近性质。

    Hawkes processes are often applied to model dependence and interaction phenomena in multivariate event data sets, such as neuronal spike trains, social interactions, and financial transactions. In the nonparametric setting, learning the temporal dependence structure of Hawkes processes is generally a computationally expensive task, all the more with Bayesian estimation methods. In particular, for generalised nonlinear Hawkes processes, Monte-Carlo Markov Chain methods applied to compute the doubly intractable posterior distribution are not scalable to high-dimensional processes in practice. Recently, efficient algorithms targeting a mean-field variational approximation of the posterior distribution have been proposed. In this work, we first unify existing variational Bayes approaches under a general nonparametric inference framework, and analyse the asymptotic properties of these methods under easily verifiable conditions on the prior, the variational class, and the nonlinear model. Se
    
[^16]: 解决余辉前向建模中的模型偏差问题

    Towards solving model bias in cosmic shear forward modeling. (arXiv:2210.16243v2 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/2210.16243](http://arxiv.org/abs/2210.16243)

    这项研究通过混合物理和深度学习的分层贝叶斯模型，能够在真实星系上恢复出无偏估计的剪切，解决了余辉前向建模中的模型偏差问题。

    

    随着现代星系调查的规模和质量增加，测量嵌入星系形状中的宇宙学信号的难度也增加。由宇宙中最庞大结构引发的弱引力透镜效应会产生微小的星系形态剪切，称为余辉，是研究宇宙模型的重要探测手段。基于椭圆度测量统计的现代剪切估计技术存在一个问题，即椭圆度对于任意星系光度轮廓并不是一个明确定义的量，从而产生剪切估计偏差。我们展示了一种混合物理和深度学习的分层贝叶斯模型，在这种模型中，生成模型捕捉了星系的形态，使我们能够在真实星系上恢复出无偏估计的剪切，从而解决了模型偏差问题。

    As the volume and quality of modern galaxy surveys increase, so does the difficulty of measuring the cosmological signal imprinted in galaxy shapes. Weak gravitational lensing sourced by the most massive structures in the Universe generates a slight shearing of galaxy morphologies called cosmic shear, key probe for cosmological models. Modern techniques of shear estimation based on statistics of ellipticity measurements suffer from the fact that the ellipticity is not a well-defined quantity for arbitrary galaxy light profiles, biasing the shear estimation. We show that a hybrid physical and deep learning Hierarchical Bayesian Model, where a generative model captures the galaxy morphology, enables us to recover an unbiased estimate of the shear on realistic galaxies, thus solving the model bias.
    
[^17]: 适用于离散难处理似然的广义贝叶斯推断

    Generalised Bayesian Inference for Discrete Intractable Likelihood. (arXiv:2206.08420v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.08420](http://arxiv.org/abs/2206.08420)

    该论文提出了一种适用于离散难处理似然的广义贝叶斯推断方法，通过使用离散Fisher散度更新参数信念来避免计算困难，并得到一个可以用标准计算工具进行采样的广义后验分布。

    

    离散状态空间对统计推断来说是一个重要的计算挑战，因为归一化常数的计算需要对大型或可能无限的集合求和，这可能是不可行的。本文通过开发一种新颖的广义贝叶斯推断方法来应对这一计算挑战，该方法适用于离散难处理似然。受到近期连续数据方法的方法进展的启发，主要思想是使用离散Fisher散度更新对模型参数的信念，而不是使用有问题的难处理似然。结果是得到一个广义的后验分布，可以使用标准的计算工具（如马尔可夫链蒙特卡洛）来进行采样，从而避免了难处理的归一化常数。对广义后验的统计性质进行了分析，建立了后验一致性和渐近正态性的充分条件。此外，提出了一种新颖且通用的校准方法。

    Discrete state spaces represent a major computational challenge to statistical inference, since the computation of normalisation constants requires summation over large or possibly infinite sets, which can be impractical. This paper addresses this computational challenge through the development of a novel generalised Bayesian inference procedure suitable for discrete intractable likelihood. Inspired by recent methodological advances for continuous data, the main idea is to update beliefs about model parameters using a discrete Fisher divergence, in lieu of the problematic intractable likelihood. The result is a generalised posterior that can be sampled from using standard computational tools, such as Markov chain Monte Carlo, circumventing the intractable normalising constant. The statistical properties of the generalised posterior are analysed, with sufficient conditions for posterior consistency and asymptotic normality established. In addition, a novel and general approach to calibr
    
[^18]: 富者愈富: 半监督学习的不平等影响

    The Rich Get Richer: Disparate Impact of Semi-Supervised Learning. (arXiv:2110.06282v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06282](http://arxiv.org/abs/2110.06282)

    本文研究了半监督学习的不平等影响，发现那些在不使用SSL时具有更高基准准确性的子族群更容易从SSL中获益，而那些基准准确性较低的子族群在添加SSL模块后可能会观察到性能下降。

    

    半监督学习（SSL）在高质量监督数据严重有限的情况下，已经展示了提高模型准确性的潜力。尽管往往可以确立整个数据族群的平均准确性得到改善，但SSL在不同子族群中的表现尚不清楚。当我们希望公平对待的人口群体由人口统计学分组定义时，理解上述问题具有重要的公平性影响。在本文中，我们揭示了部署SSL的不平等影响：那些在不使用SSL时具有更高基准准确性的子族群（"富裕"子族群）往往从SSL中获益更多；而那些在基准准确性较低（"贫穷"）的子族群在添加SSL模块后甚至可能观察到性能下降。我们在一类广泛的SSL算法上在理论上和实证上证实了上述观察，这些算法明确或隐含地使用了一个...

    Semi-supervised learning (SSL) has demonstrated its potential to improve the model accuracy for a variety of learning tasks when the high-quality supervised data is severely limited. Although it is often established that the average accuracy for the entire population of data is improved, it is unclear how SSL fares with different sub-populations. Understanding the above question has substantial fairness implications when different sub-populations are defined by the demographic groups that we aim to treat fairly. In this paper, we reveal the disparate impacts of deploying SSL: the sub-population who has a higher baseline accuracy without using SSL (the "rich" one) tends to benefit more from SSL; while the sub-population who suffers from a low baseline accuracy (the "poor" one) might even observe a performance drop after adding the SSL module. We theoretically and empirically establish the above observation for a broad family of SSL algorithms, which either explicitly or implicitly use a
    
[^19]: 贝叶斯估计器与去偏估计器在低秩矩阵补全中的模拟比较

    Simulation comparisons between Bayesian and de-biased estimators in low-rank matrix completion. (arXiv:2103.11749v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.11749](http://arxiv.org/abs/2103.11749)

    本文通过模拟比较了贝叶斯方法和去偏估计器在低秩矩阵补全问题中的性能。结果表明去偏估计器与贝叶斯估计器一样好，在样本较小的情况下贝叶斯方法更稳定。

    

    本文研究低秩矩阵补全问题，这是一类机器学习问题，旨在预测部分观察矩阵中的缺失条目。这类问题在协同过滤、图像处理和基因型插补等多个具有挑战性的应用中出现。我们比较了贝叶斯方法和最近引入的去偏估计器，该估计器提供了一种有用的方式来构建感兴趣的置信区间。从理论角度来看，去偏估计器具有尖锐的极小最优估计误差速率，而贝叶斯方法通过额外的对数因子达到了这一速率。我们的模拟研究显示了有趣的结果，即去偏估计器与贝叶斯估计器一样好。此外，贝叶斯方法更加稳定，在样本较小的情况下可以优于去偏估计器。此外，我们还发现置信区间的经验覆盖率

    In this paper, we study the low-rank matrix completion problem, a class of machine learning problems, that aims at the prediction of missing entries in a partially observed matrix. Such problems appear in several challenging applications such as collaborative filtering, image processing, and genotype imputation. We compare the Bayesian approaches and a recently introduced de-biased estimator which provides a useful way to build confidence intervals of interest. From a theoretical viewpoint, the de-biased estimator comes with a sharp minimax-optimal rate of estimation error whereas the Bayesian approach reaches this rate with an additional logarithmic factor. Our simulation studies show originally interesting results that the de-biased estimator is just as good as the Bayesian estimators. Moreover, Bayesian approaches are much more stable and can outperform the de-biased estimator in the case of small samples. In addition, we also find that the empirical coverage rate of the confidence 
    
[^20]: 利用先验知识的 Gamma-Minimax 估计器的对抗元学习

    Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge. (arXiv:2012.05465v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2012.05465](http://arxiv.org/abs/2012.05465)

    本文提出对抗元学习方法，用于计算在一组与可用知识相容的先验分布中最小化最坏情况的 Bayes 风险的 Gamma-Minimax 估计器，文中还提出了一种神经网络类用于提供估计器类，以及两个实验环节用于说明该方法的应用。

    

    贝叶斯估计提供了一种将能够以单个先验分布的形式表达的先验知识结合起来的方式。然而，当这种知识太模糊，无法用单个先验表示时，就需要另一种方法。Gamma-minimax 估计器提供了这样一种方法。这些估计器将在与可用知识相容的一组先验分布 $\Gamma$ 上最小化最坏情况的 Bayes 风险。传统上，Gamma-minimax 性质是为参数模型定义的。在本文中，我们为一般模型定义 Gamma-minimax 估计器，并提出了利用一般化矩限制的对抗元学习算法来计算它们。我们还提出了一种神经网络类，它提供了一种丰富但有限维度的估计器类，可以从中选择 Gamma-minimax 估计器。我们在两个环节中说明了我们的方法，即估计未知支持分布的样本熵和后分层估计。

    Bayes estimators are well known to provide a means to incorporate prior knowledge that can be expressed in terms of a single prior distribution. However, when this knowledge is too vague to express with a single prior, an alternative approach is needed. Gamma-minimax estimators provide such an approach. These estimators minimize the worst-case Bayes risk over a set $\Gamma$ of prior distributions that are compatible with the available knowledge. Traditionally, Gamma-minimaxity is defined for parametric models. In this work, we define Gamma-minimax estimators for general models and propose adversarial meta-learning algorithms to compute them when the set of prior distributions is constrained by generalized moments. Accompanying convergence guarantees are also provided. We also introduce a neural network class that provides a rich, but finite-dimensional, class of estimators from which a Gamma-minimax estimator can be selected. We illustrate our method in two settings, namely entropy est
    

