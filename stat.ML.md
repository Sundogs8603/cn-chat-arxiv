# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multi-unit soft sensing permits few-shot learning.](http://arxiv.org/abs/2309.15828) | 多单元软测量是利用可转移性学习算法改进软测量的一种方法，能够通过解决多个任务来增强软测量的性能，并且特别适用于具有多个实现的进程。 |
| [^2] | [Fair Canonical Correlation Analysis.](http://arxiv.org/abs/2309.15809) | 本文研究了典范相关分析中的公平性和偏见问题，并提出了一种通过最小化相关性差异误差来减轻不公平现象的方法。该方法在保持CCA准确性的同时，减少了相关性差异误差。 |
| [^3] | [Targeting Relative Risk Heterogeneity with Causal Forests.](http://arxiv.org/abs/2309.15793) | 本研究提出了一种通过修改因果森林方法，以相对风险为目标，从而捕捉到治疗效应异质性的潜在来源。 |
| [^4] | [Importance-Weighted Offline Learning Done Right.](http://arxiv.org/abs/2309.15771) | 本文研究了随机上下文赌博问题中的离线策略优化问题，并提出了一种替代方法，通过使用“隐式探索”估计器来计算策略价值的权重重要估计。与之前的结果相比，在几乎所有情况下都具有更好的性能保证，同时消除了之前所做的非常苛刻的“均匀覆盖”假设。 |
| [^5] | [Entropic Matching for Expectation Propagation of Markov Jump Processes.](http://arxiv.org/abs/2309.15604) | 本文提出了一个基于熵匹配框架的新的可处理的推断方案，可以嵌入到期望传播算法中，对于描述离散状态空间过程的Markov跳跃过程的统计推断问题具有重要意义。我们展示了我们方法的有效性，并通过提供一类近似分布的闭式结果以及应用于化学反应网络的一般类别来加以论证。此外，我们通过一个近似的期望最大化程序导出了潜在参数的点估计的闭式表达式，并在各种化学反应网络示例中评估了我们的方法的性能。我们还讨论了该方法的局限性和未来的潜力。 |
| [^6] | [Uncertainty Quantification via Neural Posterior Principal Components.](http://arxiv.org/abs/2309.15533) | 本论文提出了一种使用神经网络在单次前向传递中预测任意输入图像后验分布的主成分的方法，以实现不确定性量化。 |
| [^7] | [Monitoring Machine Learning Models: Online Detection of Relevant Deviations.](http://arxiv.org/abs/2309.15187) | 本论文提出了一种用于监测机器学习模型的在线检测方案，通过考虑模型质量的时间依赖性，可以减少不必要的警报并优化对相关变化的检测。 |
| [^8] | [Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms.](http://arxiv.org/abs/2309.14506) | 本论文提出了一种用于黎曼流形上随机优化的零阶黎曼平均随机逼近算法（Zo-RASA），通过使用黎曼移动平均随机梯度估计器和新颖的黎曼-李雅普诺夫分析技术，实现了生成ε-近似的一阶稳定解的最优样本复杂度，同时通过使用回缩和向量传输替代指数映射和平行传输降低了算法的每次迭代复杂度。 |
| [^9] | [Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures.](http://arxiv.org/abs/2309.14298) | 本研究提出了一种改进的随机线性Bandit算法，利用鞍点边界的马丁格尔混合构建了适用于随机Bandit的置信序列，并证明该算法能够以竞争性的最坏情况下遗憾保证实现更好的性能。 |
| [^10] | [The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance.](http://arxiv.org/abs/2309.13775) | 提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。 |
| [^11] | [Error Reduction from Stacked Regressions.](http://arxiv.org/abs/2309.09880) | 本文提出了一种新的堆叠回归方法，通过最小化总体风险并受非负性约束，成功降低了误差。实验证明，堆叠估计器相比其中最佳的单个估计器具有更小的总体风险。 |
| [^12] | [Simultaneous inference for generalized linear models with unmeasured confounders.](http://arxiv.org/abs/2309.07261) | 本文研究了存在混淆效应时的广义线性模型的大规模假设检验问题，并提出了一种利用正交结构和线性投影的统计估计和推断框架，解决了由于未测混淆因素引起的偏差问题。 |
| [^13] | [Distributionally Robust Machine Learning with Multi-source Data.](http://arxiv.org/abs/2309.02211) | 本文提出了一种基于多源数据的分布鲁棒机器学习方法，通过引入组分布鲁棒预测模型来提高具有分布偏移的目标人群的预测准确性。 |
| [^14] | [Quantum-Noise-driven Generative Diffusion Models.](http://arxiv.org/abs/2308.12013) | 该论文提出了三种量子噪声驱动的生成扩散模型，利用了量子特性以克服传统模型的主要计算困难，并建议将量子噪声视为可利用的特性而非问题。 |
| [^15] | [Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness.](http://arxiv.org/abs/2308.03666) | 该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。 |
| [^16] | [Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks.](http://arxiv.org/abs/2307.15285) | 本论文解决了Zonoid的最优逼近和浅层神经网络的均匀逼近两个问题。对于Zonoid的逼近，我们填补了在$d=2,3$时的对数差距，实现了在所有维度上的解决方案。对于神经网络的逼近，我们的技术在$k \geq 1$时显著提高了目前的逼近率，并能够均匀逼近目标函数及其导数。 |
| [^17] | [Correcting Underrepresentation and Intersectional Bias for Fair Classification.](http://arxiv.org/abs/2306.11112) | 本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。 |
| [^18] | [Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers.](http://arxiv.org/abs/2305.18974) | 该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。 |
| [^19] | [On progressive sharpening, flat minima and generalisation.](http://arxiv.org/abs/2305.14683) | 本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。 |
| [^20] | [Orthogonal polynomial approximation and Extended Dynamic Mode Decomposition in chaos.](http://arxiv.org/abs/2305.08074) | 本文在简单的混沌映射上证明了扩展动态模态分解（EDMD）对于多项式可观测字典有指数效率，从而有效处理了混沌动力学中的正则函数问题，并展示了在这种情况下使用EDMD产生的预测和Koopman谱数据收敛至物理上有意义的极限。 |
| [^21] | [Meaningful Causal Aggregation and Paradoxical Confounding.](http://arxiv.org/abs/2304.11625) | 聚合变量上的因果性不确定性可能会使得原本不混淆的因果关系变得混淆，在实际应用中，我们需要接受宏观因果关系通常只与微观状态相关的事实。 |
| [^22] | [Ellipsoid fitting with the Cayley transform.](http://arxiv.org/abs/2304.10630) | 介绍了一种使用Cayley变换在任意维度上将椭球拟合到嘈杂数据中的新算法CTEF，可以拟合任意的椭球，并且能提取其他方法无法识别的数据中的非线性特征，可用于降维、数据可视化和聚类，相比其他方法更优。 |
| [^23] | [Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics.](http://arxiv.org/abs/2304.09123) | 本文提供了有限时间界限，用于被动随机梯度 Langevin 动力学算法，该算法可用于逆强化学习。该算法充当随机采样器，恢复用外部过程优化而来的成本函数。 |
| [^24] | [One-Step Estimation of Differentiable Hilbert-Valued Parameters.](http://arxiv.org/abs/2303.16711) | 本文介绍了一种针对Hilbert-Valued参数进行一步估计的方法，并提供一种适用于缺乏重现核的Hilbert空间的解决方案。 |
| [^25] | [An Asymptotically Optimal Algorithm for the Convex Hull Membership Problem.](http://arxiv.org/abs/2302.02033) | 本研究提出了一个名为Thompson-CHM的渐近最优算法，用于解决凸包成员问题，且将算法扩展到了一维和多维环境中。该算法基于模块化设计，包括停止规则和采样规则，并通过数值实验验证了理论结果的准确性。 |
| [^26] | [GeONet: a neural operator for learning the Wasserstein geodesic.](http://arxiv.org/abs/2209.14440) | GeONet是一个不受网格影响的深度神经算子网络，学习了从初始和终端分布到连接两个端点分布的Wasserstein测地的非线性映射。通过学习鞍点优化条件，GeONet可以快速进行实时预测，并在仿真示例和测试数据上取得了与标准OT求解器相当的准确性。 |
| [^27] | [Robust leave-one-out cross-validation for high-dimensional Bayesian models.](http://arxiv.org/abs/2209.09190) | 提出了一个计算贝叶斯LOO-CV准则的新的混合估计量，保持了经典方法的简便性和计算方便性，并确保了结果估计量的渐近方差有限。在高维问题中尤为显著，可以应用于更广泛的模型和具有高度影响的观测数据集。 |
| [^28] | [Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions.](http://arxiv.org/abs/2105.13913) | 本论文介绍了一种简单的Frank-Wolfe算法变体，利用广义自协调函数的特性，在不需要使用二阶信息或估计局部平滑度参数的情况下，以$\mathcal{O}(1/t)$的收敛速度达到了优化目标。 |
| [^29] | [Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling.](http://arxiv.org/abs/2009.08267) | 本论文提出了一种新颖灵活的参数估计方法，用于机械建模中的数据一致反演。该方法解决了贝叶斯分析中无信息先验引入的偏差问题，并在随机逆问题框架下推断参数。使用拒绝采样、马尔科夫链蒙特卡洛和生成对抗网络等新方法解决了数据一致反演的限制，并通过约束优化和先验逆问题分析进一步优化了结果。 |
| [^30] | [Improving the convergence of SGD through adaptive batch sizes.](http://arxiv.org/abs/1910.08222) | 通过自适应批大小，本研究提出了一种改善SGD收敛性的方法，既减少了高方差梯度估计的问题，又保持了较高精度的梯度估计。 |
| [^31] | [Optimal Sparse Decision Trees.](http://arxiv.org/abs/1904.12847) | 这篇论文介绍了第一个针对二进制变量的最优决策树的实用算法，通过分析界限和现代系统技术的结合来解决决策树优化的困难，实验证明了其在可扩展性、速度和最优性证明方面的优势。 |

# 详细

[^1]: 多单元软测量允许少样本学习

    Multi-unit soft sensing permits few-shot learning. (arXiv:2309.15828v1 [stat.ML])

    [http://arxiv.org/abs/2309.15828](http://arxiv.org/abs/2309.15828)

    多单元软测量是利用可转移性学习算法改进软测量的一种方法，能够通过解决多个任务来增强软测量的性能，并且特别适用于具有多个实现的进程。

    

    近期的研究探索了利用具有可转移性的学习算法来改进软测量的各种方法。总体来说，当一个软测量通过解决多个任务来学习时，其性能可以得到加强。可转移性的有用性取决于所设计的学习任务的相关性。在软测量要应用于有多个实现的进程（例如，有多个可用数据的系统或设备）时，尤其相关。然后，每个实现都提供一个软测量学习任务，并且合理地期望这些不同任务之间具有强相关性。在这种设置中应用可转移性导致了我们所称的多单元软测量，其中软测量通过从所有实现的数据中学习来建模一个进程。本文探讨了多单元软测量的学习能力，它被构建为一个分层模型，并使用...

    Recent literature has explored various ways to improve soft sensors using learning algorithms with transferability. Broadly put, the performance of a soft sensor may be strengthened when it is learned by solving multiple tasks. The usefulness of transferability depends on how strongly related the devised learning tasks are. A particularly relevant case for transferability, is when a soft sensor is to be developed for a process of which there are many realizations, e.g. system or device with many implementations from which data is available. Then, each realization presents a soft sensor learning task, and it is reasonable to expect that the different tasks are strongly related. Applying transferability in this setting leads to what we call multi-unit soft sensing, where a soft sensor models a process by learning from data from all of its realizations.  This paper explores the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented usin
    
[^2]: 公平的典范相关分析

    Fair Canonical Correlation Analysis. (arXiv:2309.15809v1 [cs.LG])

    [http://arxiv.org/abs/2309.15809](http://arxiv.org/abs/2309.15809)

    本文研究了典范相关分析中的公平性和偏见问题，并提出了一种通过最小化相关性差异误差来减轻不公平现象的方法。该方法在保持CCA准确性的同时，减少了相关性差异误差。

    

    本文研究了典范相关分析（CCA）中的公平性和偏见问题，CCA是一种广泛应用的统计技术，用于研究两组变量之间的关系。我们提出了一个框架，通过最小化与受保护属性相关的相关性差异误差，来减轻不公平现象。我们的方法使得CCA能够从所有数据点中学习到全局投影矩阵，同时确保这些矩阵产生与组特定投影矩阵相当的相关性水平。对合成和真实数据集的实验评估表明，我们的方法在减少相关性差异误差的同时不会影响CCA的准确性。

    This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables CCA to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing correlation disparity error without compromising CCA accuracy.
    
[^3]: 用因果森林针对相对风险异质性进行目标化

    Targeting Relative Risk Heterogeneity with Causal Forests. (arXiv:2309.15793v1 [stat.ME])

    [http://arxiv.org/abs/2309.15793](http://arxiv.org/abs/2309.15793)

    本研究提出了一种通过修改因果森林方法，以相对风险为目标，从而捕捉到治疗效应异质性的潜在来源。

    

    在临床试验分析中，治疗效应异质性（TEH）即种群中不同亚群的治疗效应的变异性是非常重要的。因果森林（Wager和Athey，2018）是解决这个问题的一种非常流行的方法，但像许多其他发现TEH的方法一样，它用于分离亚群的标准侧重于绝对风险的差异。这可能会削弱统计功效，掩盖了相对风险中的细微差别，而相对风险通常是临床关注的更合适的数量。在这项工作中，我们提出并实现了一种修改因果森林以针对相对风险的方法，使用基于广义线性模型（GLM）比较的新颖节点分割过程。我们在模拟和真实数据上展示了结果，表明相对风险的因果森林可以捕捉到其他未观察到的异质性源。

    Treatment effect heterogeneity (TEH), or variability in treatment effect for different subgroups within a population, is of significant interest in clinical trial analysis. Causal forests (Wager and Athey, 2018) is a highly popular method for this problem, but like many other methods for detecting TEH, its criterion for separating subgroups focuses on differences in absolute risk. This can dilute statistical power by masking nuance in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison. We present results on simulated and real-world data that suggest relative risk causal forests can capture otherwise unobserved sources of heterogeneity.
    
[^4]: 权重重要的离线学习正确地完成

    Importance-Weighted Offline Learning Done Right. (arXiv:2309.15771v1 [cs.LG])

    [http://arxiv.org/abs/2309.15771](http://arxiv.org/abs/2309.15771)

    本文研究了随机上下文赌博问题中的离线策略优化问题，并提出了一种替代方法，通过使用“隐式探索”估计器来计算策略价值的权重重要估计。与之前的结果相比，在几乎所有情况下都具有更好的性能保证，同时消除了之前所做的非常苛刻的“均匀覆盖”假设。

    

    我们研究了随机上下文赌博问题中的离线策略优化问题，目标是基于由次优行为策略收集的决策数据集学习一个近似最优的策略。我们不对奖励函数做任何结构性假设，而是假设可以访问给定的策略类，并且旨在与该类中的最佳比较器策略竞争。在这种情况下，标准方法是计算每个策略价值的权重重要估计，并选择一个最小化估计值的策略，减去估计值中的“悲观”调整以减少其随机波动。在本文中，我们展示了一种基于 \citet{Neu2015} 的“隐式探索”估计器的简单替代方法，其性能保证在几乎所有可能的情况下都优于之前的结果。尤其值得注意的是，我们消除了之前所有工作中非常苛刻的“均匀覆盖”假设。

    We study the problem of offline policy optimization in stochastic contextual bandit problems, where the goal is to learn a near-optimal policy based on a dataset of decision data collected by a suboptimal behavior policy. Rather than making any structural assumptions on the reward function, we assume access to a given policy class and aim to compete with the best comparator policy within this class. In this setting, a standard approach is to compute importance-weighted estimators of the value of each policy, and select a policy that minimizes the estimated value up to a "pessimistic" adjustment subtracted from the estimates to reduce their random fluctuations. In this paper, we show that a simple alternative approach based on the "implicit exploration" estimator of \citet{Neu2015} yields performance guarantees that are superior in nearly all possible terms to all previous results. Most notably, we remove an extremely restrictive "uniform coverage" assumption made in all previous works.
    
[^5]: Entropic Matching用于Markov跳跃过程的期望传播的熵匹配

    Entropic Matching for Expectation Propagation of Markov Jump Processes. (arXiv:2309.15604v1 [cs.LG])

    [http://arxiv.org/abs/2309.15604](http://arxiv.org/abs/2309.15604)

    本文提出了一个基于熵匹配框架的新的可处理的推断方案，可以嵌入到期望传播算法中，对于描述离散状态空间过程的Markov跳跃过程的统计推断问题具有重要意义。我们展示了我们方法的有效性，并通过提供一类近似分布的闭式结果以及应用于化学反应网络的一般类别来加以论证。此外，我们通过一个近似的期望最大化程序导出了潜在参数的点估计的闭式表达式，并在各种化学反应网络示例中评估了我们的方法的性能。我们还讨论了该方法的局限性和未来的潜力。

    

    本文解决了潜在连续时间随机过程的统计推断问题，该问题通常难以处理，特别是对于由Markov跳跃过程描述的离散状态空间过程。为了克服这个问题，我们提出了一种新的可处理的推断方案，基于熵匹配框架，可以嵌入到众所周知的期望传播算法中。我们通过为一类简单的近似分布提供闭式结果，并将其应用于化学反应网络的一般类别，该类别是系统生物学建模的重要工具，来证明我们方法的有效性。此外，我们使用近似的期望最大化程序导出了潜在参数的点估计的闭式表达式。我们评估了我们方法在各种化学反应网络示例中的性能，包括随机的Lotka-Voltera示例，并讨论了它的局限性和未来的潜力。

    This paper addresses the problem of statistical inference for latent continuous-time stochastic processes, which is often intractable, particularly for discrete state space processes described by Markov jump processes. To overcome this issue, we propose a new tractable inference scheme based on an entropic matching framework that can be embedded into the well-known expectation propagation algorithm. We demonstrate the effectiveness of our method by providing closed-form results for a simple family of approximate distributions and apply it to the general class of chemical reaction networks, which are a crucial tool for modeling in systems biology. Moreover, we derive closed form expressions for point estimation of the underlying parameters using an approximate expectation maximization procedure. We evaluate the performance of our method on various chemical reaction network instantiations, including a stochastic Lotka-Voltera example, and discuss its limitations and potential for future 
    
[^6]: 通过神经后验主成分进行不确定性量化

    Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])

    [http://arxiv.org/abs/2309.15533](http://arxiv.org/abs/2309.15533)

    本论文提出了一种使用神经网络在单次前向传递中预测任意输入图像后验分布的主成分的方法，以实现不确定性量化。

    

    不确定性量化对于在自动驾驶和生物成像等安全关键领域中部署图像恢复模型至关重要。迄今为止，关于不确定性可视化的方法主要集中在每像素估计上。然而，每像素方差的热图通常在实际中用途有限，因为它无法捕捉像素之间的强相关性。更自然的不确定性度量对应于后验分布的主成分（PCs）上的方差。理论上，可以通过对输入图像的条件生成模型生成的样本应用PCA来计算PCs。然而，这需要在测试时生成大量的样本，而在目前的最先进（扩散）模型下非常缓慢。在该工作中，我们提出了一种方法来在神经网络的单次前向传递中预测后验分布的PCs，适用于任意输入图像。

    Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap
    
[^7]: 监测机器学习模型：在线检测相关偏差

    Monitoring Machine Learning Models: Online Detection of Relevant Deviations. (arXiv:2309.15187v1 [cs.LG])

    [http://arxiv.org/abs/2309.15187](http://arxiv.org/abs/2309.15187)

    本论文提出了一种用于监测机器学习模型的在线检测方案，通过考虑模型质量的时间依赖性，可以减少不必要的警报并优化对相关变化的检测。

    

    机器学习模型是各个领域中重要的工具，但其性能可能会随时间的推移而降低，原因是数据分布的变化或其他因素。一方面，检测和解决这种降级对于保持模型的可靠性至关重要。另一方面，给定足够的数据，可以检测到任意小的质量变化。由于模型重新训练或替换等干预措施可能代价高昂，我们认为仅当变化超过给定阈值时才应该进行这些干预措施。我们提出了一种顺序监测方案来检测这些相关变化。所提出的方法通过考虑所测量模型质量的时间依赖性来减少不必要的警报并克服多重测试问题。文中提供了一致性和指定渐近水平的条件。使用模拟和真实数据进行的实证验证证明了我们的方法在检测模型质量相关变化方面的优越性，相比基准方法

    Machine learning models are essential tools in various domains, but their performance can degrade over time due to changes in data distribution or other factors. On one hand, detecting and addressing such degradations is crucial for maintaining the models' reliability. On the other hand, given enough data, any arbitrary small change of quality can be detected. As interventions, such as model re-training or replacement, can be expensive, we argue that they should only be carried out when changes exceed a given threshold. We propose a sequential monitoring scheme to detect these relevant changes. The proposed method reduces unnecessary alerts and overcomes the multiple testing problem by accounting for temporal dependence of the measured model quality. Conditions for consistency and specified asymptotic levels are provided. Empirical validation using simulated and real data demonstrates the superiority of our approach in detecting relevant changes in model quality compared to benchmark m
    
[^8]: 零阶黎曼平均随机逼近算法

    Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms. (arXiv:2309.14506v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2309.14506](http://arxiv.org/abs/2309.14506)

    本论文提出了一种用于黎曼流形上随机优化的零阶黎曼平均随机逼近算法（Zo-RASA），通过使用黎曼移动平均随机梯度估计器和新颖的黎曼-李雅普诺夫分析技术，实现了生成ε-近似的一阶稳定解的最优样本复杂度，同时通过使用回缩和向量传输替代指数映射和平行传输降低了算法的每次迭代复杂度。

    

    我们提出了一种用于黎曼流形上随机优化的零阶黎曼平均随机逼近（Zo-RASA）算法。我们证明Zo-RASA仅使用每次迭代中的一个样本或常数阶的批处理就能实现生成ε-近似的一阶稳定解的最优样本复杂度。我们的方法采用了黎曼移动平均随机梯度估计器，并利用了一种新颖的黎曼-李雅普诺夫分析技术进行收敛分析。通过使用回缩和向量传输代替指数映射和平行传输，我们改进了算法的可行性，从而降低了每次迭代的复杂度。此外，我们还引入了一个新颖的几何条件，满足有有界第二基本形式的流形，从而为用向量传输逼近平行传输提供新的误差界。

    We present Zeroth-order Riemannian Averaging Stochastic Approximation (\texttt{Zo-RASA}) algorithms for stochastic optimization on Riemannian manifolds. We show that \texttt{Zo-RASA} achieves optimal sample complexities for generating $\epsilon$-approximation first-order stationary solutions using only one-sample or constant-order batches in each iteration. Our approach employs Riemannian moving-average stochastic gradient estimators, and a novel Riemannian-Lyapunov analysis technique for convergence analysis. We improve the algorithm's practicality by using retractions and vector transport, instead of exponential mappings and parallel transports, thereby reducing per-iteration complexity. Additionally, we introduce a novel geometric condition, satisfied by manifolds with bounded second fundamental form, which enables new error bounds for approximating parallel transport with vector transport.
    
[^9]: 使用鞍点边界的马丁格尔混合改进随机线性Bandit算法

    Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures. (arXiv:2309.14298v1 [stat.ML])

    [http://arxiv.org/abs/2309.14298](http://arxiv.org/abs/2309.14298)

    本研究提出了一种改进的随机线性Bandit算法，利用鞍点边界的马丁格尔混合构建了适用于随机Bandit的置信序列，并证明该算法能够以竞争性的最坏情况下遗憾保证实现更好的性能。

    

    我们提出了一种对随机线性Bandit问题具有最坏情况下遗憾保证的改进算法。广泛使用的"面对不确定性时的乐观原则"可以将随机Bandit问题转化为对未知奖励函数构建置信序列的问题。结果算法的性能取决于置信序列的大小，置信集较小可提供更好的经验性能和更强的遗憾保证。本研究中，我们使用了一种对自适应马丁格尔混合的尾部边界来构建适用于随机Bandit的置信序列。这些置信序列允许通过凸规划进行高效的动作选择。我们证明了基于我们的置信序列的线性Bandit算法能够保证达到具有竞争力的最坏情况下遗憾。我们实证和理论上证明了我们的置信序列比竞争对手更紧致。最后，我们证明了我们的紧致置信序列可以提供和置信集比较容易配置的更好的性能。

    We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used "optimism in the face of uncertainty" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confi
    
[^10]: 论文标题：The Rashomon Importance Distribution: 摆脱不稳定的基于单一模型的变量重要性

    The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])

    [http://arxiv.org/abs/2309.13775](http://arxiv.org/abs/2309.13775)

    提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。

    

    量化变量重要性对于回答遗传学、公共政策和医学等领域的重大问题至关重要。当前的方法通常计算给定数据集上训练的给定模型的变量重要性。然而，对于给定数据集，可能有许多模型同样能解释目标结果;如果不考虑所有可能的解释，不同的研究者可能会得出许多冲突但同样有效的结论。此外，即使考虑了给定数据集的所有可能解释，这些洞察力可能不具有普适性，因为并非所有好的解释在合理的数据扰动下都是稳定的。我们提出了一种新的变量重要性框架，该框架量化了在所有好的模型集合中的变量重要性，并且在数据分布上是稳定的。我们的框架非常灵活，可以与大多数现有的模型类和全局变量重要性指标结合使用。

    Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
    
[^11]: 由堆叠回归减少误差

    Error Reduction from Stacked Regressions. (arXiv:2309.09880v1 [stat.ML])

    [http://arxiv.org/abs/2309.09880](http://arxiv.org/abs/2309.09880)

    本文提出了一种新的堆叠回归方法，通过最小化总体风险并受非负性约束，成功降低了误差。实验证明，堆叠估计器相比其中最佳的单个估计器具有更小的总体风险。

    

    堆叠回归是一种集成技术，它通过形成不同回归估计器的线性组合来提高预测准确性。传统方法使用交叉验证数据来生成由构成估计器预测，并使用带非负性约束的最小二乘法学习组合权重。在本文中，我们类似地通过最小化一种估计的总体风险来学习这些权重，并受到非负性约束。当构成的估计器是通过至少三个维度分隔的嵌套子空间的线性最小二乘投影时，我们证明由于收缩效应，所得到的堆叠估计器的总体风险严格小于其中最佳的单个估计器。这里的“最佳”是指最小化选择准则如AIC或BIC的模型。换句话说，在这种情况下，最佳的单个估计器是不可接受的。因为优化问题可以重构为同信息回归，所以...

    Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, t
    
[^12]: 具有未测混淆因素的广义线性模型的同时推断

    Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])

    [http://arxiv.org/abs/2309.07261](http://arxiv.org/abs/2309.07261)

    本文研究了存在混淆效应时的广义线性模型的大规模假设检验问题，并提出了一种利用正交结构和线性投影的统计估计和推断框架，解决了由于未测混淆因素引起的偏差问题。

    

    在基因组研究中，常常进行成千上万个同时假设检验，以确定差异表达的基因。然而，由于存在未测混淆因素，许多标准统计方法可能存在严重的偏差。本文研究了存在混淆效应时的多元广义线性模型的大规模假设检验问题。在任意混淆机制下，我们提出了一个统一的统计估计和推断方法，利用正交结构并将线性投影整合到三个关键阶段中。首先，利用多元响应变量分离边际和不相关的混淆效应，恢复混淆系数的列空间。随后，利用$\ell_1$正则化进行稀疏性估计，并强加正交性限制于混淆系数，联合估计潜在因子和主要效应。最后，我们结合投影和加权偏差校正步骤。

    Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
    
[^13]: 基于多源数据的分布鲁棒机器学习

    Distributionally Robust Machine Learning with Multi-source Data. (arXiv:2309.02211v1 [stat.ML])

    [http://arxiv.org/abs/2309.02211](http://arxiv.org/abs/2309.02211)

    本文提出了一种基于多源数据的分布鲁棒机器学习方法，通过引入组分布鲁棒预测模型来提高具有分布偏移的目标人群的预测准确性。

    

    当目标分布与源数据集不同时，传统的机器学习方法可能导致较差的预测性能。本文利用多个数据源，并引入了一种基于组分布鲁棒预测模型来优化关于目标分布类的可解释方差的对抗性奖励。与传统的经验风险最小化相比，所提出的鲁棒预测模型改善了具有分布偏移的目标人群的预测准确性。我们证明了组分布鲁棒预测模型是源数据集条件结果模型的加权平均。我们利用这一关键鉴别结果来提高任意机器学习算法的鲁棒性，包括随机森林和神经网络等。我们设计了一种新的偏差校正估计器来估计通用机器学习算法的最优聚合权重，并展示了其在c方面的改进。

    Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the c
    
[^14]: 量子噪声驱动的生成扩散模型

    Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])

    [http://arxiv.org/abs/2308.12013](http://arxiv.org/abs/2308.12013)

    该论文提出了三种量子噪声驱动的生成扩散模型，利用了量子特性以克服传统模型的主要计算困难，并建议将量子噪声视为可利用的特性而非问题。

    

    通过机器学习技术实现的生成模型是从有限的训练样本中推断出复杂和未知数据分布并产生新的合成数据的强大工具。扩散模型是一种新兴的框架，最近在创建合成文本和高质量图像方面已经超越了生成对抗性网络的性能。在这里，我们提出并讨论了扩散模型的量子推generalization，即三种可能在实际量子系统上进行实验的量子噪声驱动的生成扩散模型。我们的想法是利用独特的量子特性，特别是目前可用的有噪声量子处理器不可避免地受到的相干性、纠缠性和噪声之间的非平凡相互作用，以克服传统扩散模型在推断过程中的主要计算负担。因此，我们建议将量子噪声不作为需要检测和解决的问题，而是作为一种可利用的特性，使得扩散模型能够更好地工作。

    Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
    
[^15]: 架起可信度与开放世界学习的桥梁：一种探索性神经方法，用于增强可解释性、泛化性和鲁棒性

    Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])

    [http://arxiv.org/abs/2308.03666](http://arxiv.org/abs/2308.03666)

    该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。

    

    随着研究人员努力缩小机器智能与人类之间的差距，通过发展人工智能技术，我们必须认识到可信度在开放世界中的关键重要性，在日常生活的各个方面对每个人都已经无处不在。然而，目前的人工智能系统存在几个挑战，可能会导致信任危机：1）对预测结果的解释不足；2）学习模型的泛化性不足；3）对不确定环境的适应能力差。因此，我们探索了一种神经程序，用于架起可信度与开放世界学习之间的桥梁，从单模态扩展到多模态场景，以供读者使用。1）为了增强设计级可解释性，我们首先定制了具有特定物理含义的可信网络；2）然后，通过灵活的学习正则化器设计环境福祉任务接口，以改善可信网络的泛化性能。

    As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
    
[^16]: Zonoid的最优逼近和浅层神经网络的均匀逼近

    Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])

    [http://arxiv.org/abs/2307.15285](http://arxiv.org/abs/2307.15285)

    本论文解决了Zonoid的最优逼近和浅层神经网络的均匀逼近两个问题。对于Zonoid的逼近，我们填补了在$d=2,3$时的对数差距，实现了在所有维度上的解决方案。对于神经网络的逼近，我们的技术在$k \geq 1$时显著提高了目前的逼近率，并能够均匀逼近目标函数及其导数。

    

    我们研究了以下两个相关问题。第一个问题是确定一个任意的在$\mathbb{R}^{d+1}$空间中的Zonoid可以通过$n$个线段的Hausdorff距离来逼近的误差。第二个问题是确定浅层ReLU$^k$神经网络在其变分空间中的均匀范数的最优逼近率。第一个问题已经在$d \neq 2, 3$时得到解决，但当$d = 2, 3$时，最优上界和最优下界之间仍存在一个对数差距。我们填补了这个差距，完成了所有维度上的解决方案。对于第二个问题，我们的技术在$k \geq 1$时显著提高了现有的逼近率，并实现了目标函数及其导数的均匀逼近。

    We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
    
[^17]: 纠正公平分类中的低估偏差和交叉偏差

    Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])

    [http://arxiv.org/abs/2306.11112](http://arxiv.org/abs/2306.11112)

    本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。

    

    我们考虑学习被低估偏差损坏的数据的问题，其中正例在固定数量的敏感组中以不同的未知速率从数据中过滤掉。我们表明，在有少量无偏数据的情况下，我们可以有效地估计每个组的减少参数，即使在交叉组成员资格使得学习每个交叉率变得计算上不可行的情况下。利用这个分组丢失率的估计，我们构造了一个重新加权方案，可以使我们近似评估任何假设在真实分布上的损失，即使我们只能在一个有偏样本上观察到经验误差。最后，我们提出了一个封装了这个学习和重新加权过程的算法，并提供了强PAC风格的保证，即有很高的概率我们对假设在真实分布上的风险的估计将与真实风险任意接近。

    We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
    
[^18]: 异常点存在时健壮经验风险最小化性能的渐进特性研究

    Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers. (arXiv:2305.18974v1 [stat.ML])

    [http://arxiv.org/abs/2305.18974](http://arxiv.org/abs/2305.18974)

    该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。

    

    我们研究了高维健壮线性回归问题，当维度$d$和数据点数量$n$以固定比率$\alpha=n/d$发散，并研究了包括异常点在内的数据模型。我们对使用$\ell_2$ -正则化$\ell_2$，$\ell_1$，和 Huber 损失的经验风险最小化（ERM）性能提供了精确的渐近特性，这是解决这类问题的标准方法。我们关注性能的两个指标：具有异常点的相似数据集的泛化误差和原始无污染函数的估计误差。我们将结果与信息论贝叶斯最优估计界进行了比较。对于泛化误差，我们发现如果进行简单的校准并计算收敛速率，则最优正则化ERM在大样本复杂度限制下是渐近一致的。然而，对于估计误差，由于范数校准不匹配，我们表明估计器的一致性需要一个较强的收敛假设，这对问题的解决还需要进一步的研究。

    We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\ell_2$-regularised $\ell_2$, $\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an
    
[^19]: 论进化磨锋、平坦极小和泛化

    On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])

    [http://arxiv.org/abs/2305.14683](http://arxiv.org/abs/2305.14683)

    本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。

    

    我们提出了一种新的方法来理解深度学习中损失曲率与泛化之间的关系。具体来说，我们利用现有的深度网络损失黑塞矩阵频谱经验分析，提出了一个将损失黑塞矩阵和深度神经网络的输入-输出雅克比矩阵联系起来的假设。然后，我们证明了一系列理论结果，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界。我们利用我们的假设和理论结果，给出了关于最近观察到的进化磨锋现象以及平坦极小的泛化性质的新描述。实验证据验证了我们的主张。

    We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
    
[^20]: 正交多项式逼近和扩展动态模态分解在混沌中的应用

    Orthogonal polynomial approximation and Extended Dynamic Mode Decomposition in chaos. (arXiv:2305.08074v1 [math.NA])

    [http://arxiv.org/abs/2305.08074](http://arxiv.org/abs/2305.08074)

    本文在简单的混沌映射上证明了扩展动态模态分解（EDMD）对于多项式可观测字典有指数效率，从而有效处理了混沌动力学中的正则函数问题，并展示了在这种情况下使用EDMD产生的预测和Koopman谱数据收敛至物理上有意义的极限。

    

    扩展动态模态分解（EDMD）是一种数据驱动的工具，用于动态的预测和模型简化，在物理科学领域得到广泛应用。虽然这种方法在概念上很简单，但在确定性混沌中，它的性质或者它的收敛性还不清楚。特别是，EDMD的最小二乘逼近如何处理需要描绘混沌动力学含义的正则函数的类别，这也是不清楚的。本文在分析上简单的一个圆环展开映射的最简单例子上，发展了关于EDMD的一般的、严格的理论。证明了一个新的关于在单位圆上的正交多项式（OPUC）的理论结果，我们证明在无限数据极限时，针对多项式的可观测字典的最小二乘投影具有指数效率。因此，我们展示了在这种情况下使用EDMD产生的预测和Koopman谱数据收敛到物理上有意义的极限的指数速率。

    Extended Dynamic Mode Decomposition (EDMD) is a data-driven tool for forecasting and model reduction of dynamics, which has been extensively taken up in the physical sciences. While the method is conceptually simple, in deterministic chaos it is unclear what its properties are or even what it converges to. In particular, it is not clear how EDMD's least-squares approximation treats the classes of regular functions needed to make sense of chaotic dynamics.  In this paper we develop a general, rigorous theory of EDMD on the simplest examples of chaotic maps: analytic expanding maps of the circle. Proving a new result in the theory of orthogonal polynomials on the unit circle (OPUC), we show that in the infinite-data limit, the least-squares projection is exponentially efficient for polynomial observable dictionaries. As a result, we show that the forecasts and Koopman spectral data produced using EDMD in this setting converge to the physically meaningful limits, at an exponential rate.  
    
[^21]: 有意义的因果聚合和悖论性混淆

    Meaningful Causal Aggregation and Paradoxical Confounding. (arXiv:2304.11625v1 [cs.AI])

    [http://arxiv.org/abs/2304.11625](http://arxiv.org/abs/2304.11625)

    聚合变量上的因果性不确定性可能会使得原本不混淆的因果关系变得混淆，在实际应用中，我们需要接受宏观因果关系通常只与微观状态相关的事实。

    

    在聚合变量中，干预的影响通常是不确定的，因为相同的宏观干预的不同微观实现可能会导致下游宏观变量的不同变化。我们表明，对于聚合变量，因果性的不确定性可以使得原本不混淆的因果关系变得混淆，并且反之亦然，这一点取决于相应的微观实现。我们认为，只有在聚合因果系统没有这种不确定性的情况下，我们才可以实际应用这种方法。否则，我们需要接受一点，就是宏观因果关系通常只与微观状态相关。在积极方面，我们表明当宏观干预的分布与观测分布中微观状态的分布相同时，因果关系可以进行聚合，并讨论了此观察的概括。

    In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. On the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution and also discuss generalizations of this observation.
    
[^22]: 用Cayley变换拟合椭球

    Ellipsoid fitting with the Cayley transform. (arXiv:2304.10630v1 [stat.ML])

    [http://arxiv.org/abs/2304.10630](http://arxiv.org/abs/2304.10630)

    介绍了一种使用Cayley变换在任意维度上将椭球拟合到嘈杂数据中的新算法CTEF，可以拟合任意的椭球，并且能提取其他方法无法识别的数据中的非线性特征，可用于降维、数据可视化和聚类，相比其他方法更优。

    

    我们引入了一种算法，Cayley变换椭球拟合(CTEF)，它使用Cayley变换在任意维度上将椭球拟合到嘈杂的数据中。与许多椭球拟合方法不同，CTEF是椭球特定的——意味着它总是返回椭圆解——并且可以拟合任意的椭球。当数据不均匀地分布在椭球表面上时，它也优于其他拟合方法。受机器学习中可解释和可重复方法的呼吁启发，我们将CTEF应用于降维、数据可视化和聚类。由于CTEF捕捉全局曲率，因此它能够提取其他方法无法识别的数据中的非线性特征。这在人类细胞周期数据的降维和在经典玩具例子的聚类的背景下得到了说明。在后一种情况下，CTEF优于10种流行的聚类算法。

    We introduce an algorithm, Cayley transform ellipsoid fitting (CTEF), that uses the Cayley transform to fit ellipsoids to noisy data in any dimension. Unlike many ellipsoid fitting methods, CTEF is ellipsoid specific -- meaning it always returns elliptic solutions -- and can fit arbitrary ellipsoids. It also outperforms other fitting methods when data are not uniformly distributed over the surface of an ellipsoid. Inspired by calls for interpretable and reproducible methods in machine learning, we apply CTEF to dimension reduction, data visualization, and clustering. Since CTEF captures global curvature, it is able to extract nonlinear features in data that other methods fail to identify. This is illustrated in the context of dimension reduction on human cell cycle data, and in the context of clustering on classical toy examples. In the latter case, CTEF outperforms 10 popular clustering algorithms.
    
[^23]: 使用被动 Langevin 动力学的自适应逆强化学习的有限样本界限

    Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics. (arXiv:2304.09123v1 [cs.LG])

    [http://arxiv.org/abs/2304.09123](http://arxiv.org/abs/2304.09123)

    本文提供了有限时间界限，用于被动随机梯度 Langevin 动力学算法，该算法可用于逆强化学习。该算法充当随机采样器，恢复用外部过程优化而来的成本函数。

    

    随机梯度 Langevin 动力学 (SGLD) 是从概率分布采样的有用方法。本文提供了一个被动随机梯度 Langevin 动力学算法 (PSGLD) 的有限样本分析，旨在实现逆强化学习。此处的“被动”是指 PSGLD 算法(逆学习过程)可用的噪声渐变是由外部随机梯度算法(正向学习器)在随机选择的点上评估的。PSGLD 算法因此充当一个随机采样器，可恢复正在被此外部过程优化的成本函数。以前的工作使用随机逼近技术分析了这个被动算法的渐近性能；在本文中，我们分析了它的有限时间性能。具体而言，我们提供了在被动算法和其稳定测度之间的 2-Wasserstein 距离上的有限时间界限，从中可以获得重建的成本函数。

    Stochastic gradient Langevin dynamics (SGLD) are a useful methodology for sampling from probability distributions. This paper provides a finite sample analysis of a passive stochastic gradient Langevin dynamics algorithm (PSGLD) designed to achieve inverse reinforcement learning. By "passive", we mean that the noisy gradients available to the PSGLD algorithm (inverse learning process) are evaluated at randomly chosen points by an external stochastic gradient algorithm (forward learner). The PSGLD algorithm thus acts as a randomized sampler which recovers the cost function being optimized by this external process. Previous work has analyzed the asymptotic performance of this passive algorithm using stochastic approximation techniques; in this work we analyze the non-asymptotic performance. Specifically, we provide finite-time bounds on the 2-Wasserstein distance between the passive algorithm and its stationary measure, from which the reconstructed cost function is obtained.
    
[^24]: Hilbert-Valued参数的一步估计

    One-Step Estimation of Differentiable Hilbert-Valued Parameters. (arXiv:2303.16711v1 [math.ST])

    [http://arxiv.org/abs/2303.16711](http://arxiv.org/abs/2303.16711)

    本文介绍了一种针对Hilbert-Valued参数进行一步估计的方法，并提供一种适用于缺乏重现核的Hilbert空间的解决方案。

    

    本文提出了对光滑Hilbert-Valued参数的估计器，其中光滑性由逐路径可微条件表征。当参数空间是重现核Hilbert空间时，我们提供了一种获取高效和相关置信区间的方法。这些估计器对应于基于Hilbert-Valued有效影响函数的交叉一步估计器的概括。我们提供了理论保证，即使使用任意的辅助函数估计器，包括基于机器学习技术的估计器。我们表明，即使缺乏重现核的Hilbert空间，只要参数具有高效的影响函数，这些结果自然地可以扩展到该空间。然而，我们也揭示了不幸的事实，当不存在重现核时，许多有趣的参数即使它们在路径上是可微的，也缺乏有效的影响函数。为了处理这些情况，我们提出了一种正则化的方法。

    We present estimators for smooth Hilbert-valued parameters, where smoothness is characterized by a pathwise differentiability condition. When the parameter space is a reproducing kernel Hilbert space, we provide a means to obtain efficient, root-n rate estimators and corresponding confidence sets. These estimators correspond to generalizations of cross-fitted one-step estimators based on Hilbert-valued efficient influence functions. We give theoretical guarantees even when arbitrary estimators of nuisance functions are used, including those based on machine learning techniques. We show that these results naturally extend to Hilbert spaces that lack a reproducing kernel, as long as the parameter has an efficient influence function. However, we also uncover the unfortunate fact that, when there is no reproducing kernel, many interesting parameters fail to have an efficient influence function, even though they are pathwise differentiable. To handle these cases, we propose a regularized on
    
[^25]: 一个渐近最优的凸包成员问题算法

    An Asymptotically Optimal Algorithm for the Convex Hull Membership Problem. (arXiv:2302.02033v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02033](http://arxiv.org/abs/2302.02033)

    本研究提出了一个名为Thompson-CHM的渐近最优算法，用于解决凸包成员问题，且将算法扩展到了一维和多维环境中。该算法基于模块化设计，包括停止规则和采样规则，并通过数值实验验证了理论结果的准确性。

    

    本研究将凸包成员问题的纯探索设置与凸包均值的有限分布集合中有效准确地确定给定点是否在凸包中相关。我们在一维环境中完全刻画了凸包成员问题的样本复杂性。我们提出了第一个渐近最优算法，名为Thompson-CHM，其模块化设计包括停止规则和采样规则。此外，我们将算法扩展到了一些在多臂赌博机文献中广义的重要问题。此外，我们还讨论了Thompson-CHM在高维情况下的扩展。最后，我们进行了数值实验，以展示算法的经验行为与我们在实际时间范围内的理论结果相匹配。

    This work studies the pure-exploration setting for the convex hull membership (CHM) problem where one aims to efficiently and accurately determine if a given point lies in the convex hull of means of a finite set of distributions. We give a complete characterization of the sample complexity of the CHM problem in the one-dimensional setting. We present the first asymptotically optimal algorithm called Thompson-CHM, whose modular design consists of a stopping rule and a sampling rule. In addition, we extend the algorithm to settings that generalize several important problems in the multi-armed bandit literature. Furthermore, we discuss the extension of Thompson-CHM to higher dimensions. Finally, we provide numerical experiments to demonstrate the empirical behavior of the algorithm matches our theoretical results for realistic time horizons.
    
[^26]: GeONet：一种学习Wasserstein测地的神经算子

    GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14440](http://arxiv.org/abs/2209.14440)

    GeONet是一个不受网格影响的深度神经算子网络，学习了从初始和终端分布到连接两个端点分布的Wasserstein测地的非线性映射。通过学习鞍点优化条件，GeONet可以快速进行实时预测，并在仿真示例和测试数据上取得了与标准OT求解器相当的准确性。

    

    最优传输(OT)提供了一种将复杂数据分布进行几何上有意义比较的通用框架。传统的计算概率测度的Wasserstein距离和测地的方法需要网格依赖的域离散化，同时受到维度灾难的影响。我们提出了GeONet，一种不受网格影响的深度神经算子网络，它学习了将输入的初始和终端分布映射到连接两个端点分布的Wasserstein测地的非线性映射。在脱机训练阶段，GeONet通过耦合的PDE系统表征的原始和对偶空间中的动态最优条件学习了OT问题的鞍点优化条件。后续的推理阶段是瞬时完成的，并可以在在线学习设置中用于实时预测。我们证明了GeONet在仿真示例和...

    Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
    
[^27]: 高维贝叶斯模型的鲁棒离群值交叉验证

    Robust leave-one-out cross-validation for high-dimensional Bayesian models. (arXiv:2209.09190v2 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2209.09190](http://arxiv.org/abs/2209.09190)

    提出了一个计算贝叶斯LOO-CV准则的新的混合估计量，保持了经典方法的简便性和计算方便性，并确保了结果估计量的渐近方差有限。在高维问题中尤为显著，可以应用于更广泛的模型和具有高度影响的观测数据集。

    

    离群值交叉验证（LOO-CV）是估计样本外预测准确性的一种常用方法。然而，由于需要多次拟合模型，计算LOO-CV准则可能非常耗时。在贝叶斯背景下，重要性抽样提供了一种可能的解决方案，但经典方法很容易产生渐近方差为无穷大的估计量，使其潜在地不可靠。在这里，我们提出并分析了一种新的混合估计量来计算贝叶斯LOO-CV准则。我们的方法保持了经典方法的简单性和计算方便性，同时保证了所得估计量的渐近方差有限。我们提供了理论和数值结果来说明改进的鲁棒性和效率。在高维问题中，这种计算优势尤为显著，可以在更广泛的模型和具有高度影响性的观测数据集上进行贝叶斯LOO-CV。

    Leave-one-out cross-validation (LOO-CV) is a popular method for estimating out-of-sample predictive accuracy. However, computing LOO-CV criteria can be computationally expensive due to the need to fit the model multiple times. In the Bayesian context, importance sampling provides a possible solution but classical approaches can easily produce estimators whose asymptotic variance is infinite, making them potentially unreliable. Here we propose and analyze a novel mixture estimator to compute Bayesian LOO-CV criteria. Our method retains the simplicity and computational convenience of classical approaches, while guaranteeing finite asymptotic variance of the resulting estimators. Both theoretical and numerical results are provided to illustrate the improved robustness and efficiency. The computational benefits are particularly significant in high-dimensional problems, allowing to perform Bayesian LOO-CV for a broader range of models, and datasets with highly influential observations. The 
    
[^28]: 只需简单步骤：Frank-Wolfe算法和广义自协调函数

    Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v6 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.13913](http://arxiv.org/abs/2105.13913)

    本论文介绍了一种简单的Frank-Wolfe算法变体，利用广义自协调函数的特性，在不需要使用二阶信息或估计局部平滑度参数的情况下，以$\mathcal{O}(1/t)$的收敛速度达到了优化目标。

    

    广义自协调是许多重要学习问题的目标函数中存在的一个关键特性。我们建立了一个简单的Frank-Wolfe变体的收敛速度，该变体使用了开环步长策略$\gamma_t=2/(t+2)$，对于这类函数在原始间隙和Frank-Wolfe间隙方面获得了$\mathcal{O}(1/t)$的收敛速度，其中$t$是迭代次数。这避免了使用二阶信息或需要估计先前工作的局部平滑度参数。我们还展示了不同常见情况下的改进收敛速度，例如，当所考虑的可行域是均匀凸的或者是多面体的时候。

    Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
    
[^29]: 机械建模中数据一致反演的新颖灵活参数估计方法

    Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling. (arXiv:2009.08267v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2009.08267](http://arxiv.org/abs/2009.08267)

    本论文提出了一种新颖灵活的参数估计方法，用于机械建模中的数据一致反演。该方法解决了贝叶斯分析中无信息先验引入的偏差问题，并在随机逆问题框架下推断参数。使用拒绝采样、马尔科夫链蒙特卡洛和生成对抗网络等新方法解决了数据一致反演的限制，并通过约束优化和先验逆问题分析进一步优化了结果。

    

    物理系统的预测通常依赖于从模拟集合中获得的知识，例如生物科学中的细胞集合。为了定性和定量分析，这些集合使用参数化的机械模型（MM）进行模拟。基于贝叶斯推断和模型族方法是目前物理系统参数估计的两类主流方法。然而，在贝叶斯分析中，对MM参数使用无信息先验会引入不可取的偏差。在这里，我们提出了如何在随机逆问题（SIP）框架中推断参数，该框架也被称为数据一致反演，其中先验只关注由于MM不可逆造成的不确定性。为了演示，我们引入了基于拒绝采样、马尔科夫链蒙特卡洛和生成对抗网络（GANs）的新方法来解决SIP。此外，为了克服SIP的局限性，我们基于约束优化和预

    Predictions for physical systems often rely upon knowledge acquired from ensembles of entities, e.g., ensembles of cells in biological sciences. For qualitative and quantitative analysis, these ensembles are simulated with parametric families of mechanistic models (MM). Two classes of methodologies, based on Bayesian inference and Population of Models, currently prevail in parameter estimation for physical systems. However, in Bayesian analysis, uninformative priors for MM parameters introduce undesirable bias. Here, we propose how to infer parameters within the framework of stochastic inverse problems (SIP), also termed data-consistent inversion, wherein the prior targets only uncertainties that arise due to MM non-invertibility. To demonstrate, we introduce new methods to solve SIP based on rejection sampling, Markov chain Monte Carlo, and generative adversarial networks (GANs). In addition, to overcome limitations of SIP, we reformulate SIP based on constrained optimization and pres
    
[^30]: 通过自适应批大小改善SGD的收敛性

    Improving the convergence of SGD through adaptive batch sizes. (arXiv:1910.08222v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.08222](http://arxiv.org/abs/1910.08222)

    通过自适应批大小，本研究提出了一种改善SGD收敛性的方法，既减少了高方差梯度估计的问题，又保持了较高精度的梯度估计。

    

    小批量随机梯度下降（SGD）及其变种使用少量训练样本来近似目标函数的梯度，也就是批大小。小批量大小在每个模型更新时需要较少的计算量，但可能导致高方差的梯度估计，这对优化来说是一些挑战。相反，大批量需要更多计算量，但可能产生更高精度的梯度估计。本文提出了一种将批大小调整到模型训练损失的方法。对于各种函数类，我们证明了我们的方法对于模型更新来说需要与梯度下降相同数量的次数，同时对于梯度计算来说需要与SGD相同数量的次数。该方法需要在每个模型更新时计算整个数据集上的损失，但通过近似训练损失可以大大减少所需的计算量。我们提供了实验证明我们的方法需要更少的模型更新而不增加总计算量。

    Mini-batch stochastic gradient descent (SGD) and variants thereof approximate the objective function's gradient with a small number of training examples, aka the batch size. Small batch sizes require little computation for each model update but can yield high-variance gradient estimates, which poses some challenges for optimization. Conversely, large batches require more computation but can yield higher precision gradient estimates. This work presents a method to adapt the batch size to the model's training loss. For various function classes, we show that our method requires the same order of model updates as gradient descent while requiring the same order of gradient computations as SGD. This method requires evaluating the model's loss on the entire dataset every model update. However, the required computation is greatly reduced by approximating the training loss. We provide experiments that illustrate our methods require fewer model updates without increasing the total amount of comp
    
[^31]: 最优稀疏决策树

    Optimal Sparse Decision Trees. (arXiv:1904.12847v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1904.12847](http://arxiv.org/abs/1904.12847)

    这篇论文介绍了第一个针对二进制变量的最优决策树的实用算法，通过分析界限和现代系统技术的结合来解决决策树优化的困难，实验证明了其在可扩展性、速度和最优性证明方面的优势。

    

    决策树算法自从1980年代初以来就一直是可解释（透明）机器学习中最受欢迎的算法之一。然而，自从它们问世以来，困扰决策树算法的问题就是它们的非最优性，或者说缺乏接近最优的保证：决策树算法往往是贪婪的或者目光短浅的，有时会产生明显非最优的模型。决策树优化的困难既是一个理论上的障碍，也是一个实际上的障碍，即使是仔细的数学规划方法也无法高效地解决这些问题。本文提出了第一个针对二进制变量的最优决策树的实用算法。该算法通过分析界限减小搜索空间，并利用现代系统技术，包括数据结构和自定义位向量库。我们的实验证明了在可扩展性、速度和最优性证明方面的优势。代码可在https://github.com/xi获得。

    Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality. The code is available at https://github.com/xi
    

