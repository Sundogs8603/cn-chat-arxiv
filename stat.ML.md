# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces.](http://arxiv.org/abs/2310.20060) | AdaSub是一种使用低维子空间中的二阶信息进行随机优化的算法，通过选择搜索的子空间维度来管理计算开销和算法效率。初步数值结果显示，AdaSub在时间和迭代次数方面优于其他随机优化器。 |
| [^2] | [Flow-based Distributionally Robust Optimization.](http://arxiv.org/abs/2310.19253) | 这项研究提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化问题，通过使用流模型和Wasserstein近端梯度流类型的算法，实现了对具有更大样本大小的问题的可扩展性和更好的泛化能力。 |
| [^3] | [Looping in the Human: Collaborative and Explainable Bayesian Optimization.](http://arxiv.org/abs/2310.17273) | 协作和可解释的贝叶斯优化框架(CoExBO)在贝叶斯优化中引入了循环，平衡了人工智能和人类的合作关系。它利用偏好学习将用户见解融合到优化中，解释每次迭代的候选选择，从而增强用户对优化过程的信任，并提供无害保证。 |
| [^4] | [Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods.](http://arxiv.org/abs/2310.05309) | 本文提出了一种新颖的理论框架，为深度神经网络和增强学习方法在解决组合问题方面的有效性提供了肯定的答案。这个框架对于解决包括最大割和最小割、最大$k$约束问题、最大权重二分图匹配和旅行商问题在内的广泛的组合问题有重要的意义。 |
| [^5] | [A Corrected Expected Improvement Acquisition Function Under Noisy Observations.](http://arxiv.org/abs/2310.05166) | 这个论文提出了一个修正的期望改善采集函数，在贝叶斯优化中解决了对于有噪声观测的情况下忽略候选解不确定性的问题。 |
| [^6] | [Scalable neural network models and terascale datasets for particle-flow reconstruction.](http://arxiv.org/abs/2309.06782) | 本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。 |
| [^7] | [Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test.](http://arxiv.org/abs/2309.02422) | 本文将最大均差相似度应用于神经网络，并提出了一种称为Radon-Kolmogorov-Smirnov（RKS）检验的方法，该方法将样本均值差异最大化的问题推广到多维空间和更高平滑度顺序，同时与神经网络密切相关。 |
| [^8] | [How to Scale Your EMA.](http://arxiv.org/abs/2307.13813) | 本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。 |
| [^9] | [Dynamics of Temporal Difference Reinforcement Learning.](http://arxiv.org/abs/2307.04841) | 我们使用统计物理学的概念，研究了时间差分学习在线性函数逼近器下的典型学习曲线。我们发现由于子采样可能的轨迹空间而产生的随机半梯度噪声会导致值误差出现显著的平台。 |
| [^10] | [Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges.](http://arxiv.org/abs/2307.01050) | 本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。 |
| [^11] | [Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions.](http://arxiv.org/abs/2306.14351) | 本文比较了因果框架中的潜在结果模型(RCM)和结构因果模型(SCM)，并阐明了RCM成为SCM可表达的条件，以及每个RCM作为某些可表达的RCM的抽象。作者介绍了SCM原则在RCM经典应用中的重要作用，并提出了由图表示的代数约束的特征，有助于进一步比较两个框架。 |
| [^12] | [Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions.](http://arxiv.org/abs/2306.00904) | 本文提出了一种用于高阶相互作用的测量方法和基于核的测试方法，并与格理论建立了数学联系，为增强相互作用模型提供了方法。 |
| [^13] | [Birth of a Transformer: A Memory Viewpoint.](http://arxiv.org/abs/2306.00802) | 本文研究了transformers如何平衡全局分布和上下文特定分布的两种知识类型，并提供了有关权值矩阵作为联想记忆的作用及梯度如何实现权重学习的理论见解。 |
| [^14] | [Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach.](http://arxiv.org/abs/2305.17058) | 该论文提出一种精确推理离散统计模型的贝叶斯方法，支持离散采样、连续采样、离散观测、仿射函数、（随机）分支和事件条件。通过概率生成函数实现后验概率、期望、方差和高阶矩的精确计算。该方法性能优于近似蒙特卡洛方法，并避免了近似误差。 |
| [^15] | [Convergence Analysis of Mean Shift.](http://arxiv.org/abs/2305.08463) | 本研究提出了均值漂移算法的模估计序列的收敛保证，并扩展了现有的涵盖解析核和Epanechnikov核的发现，意义在于涵盖了在基于KDE的模估计的渐近统计效率方面最优的非负核——双重核。 |
| [^16] | [Convergence of Adam Under Relaxed Assumptions.](http://arxiv.org/abs/2304.13972) | 本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。 |
| [^17] | [Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks.](http://arxiv.org/abs/2304.03408) | 本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。 |
| [^18] | [Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States.](http://arxiv.org/abs/2303.17963) | 本文提出了一种面向未知具有潜在状态系统的学习优化控制方法，并给出了概率性能保证，同时提出了一种验证任意控制律性能的方法。 |
| [^19] | [Offline Policy Evaluation and Optimization under Confounding.](http://arxiv.org/abs/2211.16583) | 该论文致力于解决离线强化学习中混淆变量导致策略评估和优化存在挑战的问题，包括无法获得一致价值估计和样本复杂度的保证，作者提出了具有保证的下限算法和局部收敛的改进算法。 |
| [^20] | [Block majorization-minimization with diminishing radius for constrained nonconvex optimization.](http://arxiv.org/abs/2012.03503) | 这个论文介绍了一种用于非凸约束优化问题的块主导极小化算法，并证明了在使用强凸替代函数的情况下，该算法可以在一定迭代次数内收敛到一个稳定点，并提出了一个信任域变体，可以处理只具备凸性的替代函数。 |

# 详细

[^1]: AdaSub：使用低维子空间中的二阶信息进行随机优化

    AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces. (arXiv:2310.20060v1 [math.OC])

    [http://arxiv.org/abs/2310.20060](http://arxiv.org/abs/2310.20060)

    AdaSub是一种使用低维子空间中的二阶信息进行随机优化的算法，通过选择搜索的子空间维度来管理计算开销和算法效率。初步数值结果显示，AdaSub在时间和迭代次数方面优于其他随机优化器。

    

    我们介绍了AdaSub，一种基于低维自适应定义的二阶信息的随机优化算法。与一阶方法相比，二阶方法具有更好的收敛特性，但在每次迭代中计算Hessian矩阵会导致过高的计算开销，使其不实用。为解决这个问题，我们的方法通过选择搜索的子空间维度来管理计算开销和算法效率。我们的代码在GitHub上免费提供，我们的初步数值结果表明，AdaSub在达到给定精度所需的时间和迭代次数方面超过了流行的随机优化器。

    We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
    
[^2]: 基于流的分布鲁棒优化

    Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])

    [http://arxiv.org/abs/2310.19253](http://arxiv.org/abs/2310.19253)

    这项研究提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化问题，通过使用流模型和Wasserstein近端梯度流类型的算法，实现了对具有更大样本大小的问题的可扩展性和更好的泛化能力。

    

    我们提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化（DRO）问题，其中要求最坏情况分布（也称为最不利分布，LFD）是连续的，从而使得算法能够可扩展到具有更大样本大小的问题，并实现对诱导的鲁棒算法的更好泛化能力。为了解决计算上具有挑战性的无限维优化问题，我们利用基于流的模型，在数据分布和目标分布之间进行连续时间可逆传输映射，并开发了一种Wasserstein近端梯度流类型的算法。在实践中，我们通过梯度下降逐步训练块内的神经网络序列来参数化传输映射。我们的计算框架通用，能够处理高维数据和大样本大小，并可用于各种应用。

    We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
    
[^3]: 将循环引入人类：协作和可解释的贝叶斯优化

    Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])

    [http://arxiv.org/abs/2310.17273](http://arxiv.org/abs/2310.17273)

    协作和可解释的贝叶斯优化框架(CoExBO)在贝叶斯优化中引入了循环，平衡了人工智能和人类的合作关系。它利用偏好学习将用户见解融合到优化中，解释每次迭代的候选选择，从而增强用户对优化过程的信任，并提供无害保证。

    

    像许多优化器一样，贝叶斯优化在获得用户信任方面常常存在不足，因为其不透明性。虽然已经尝试开发面向人类的优化器，但它们通常假设用户知识是明确且无误的，并主要将用户作为优化过程的监督者。我们放宽了这些假设，提出了一种更平衡的人工智能和人类合作伙伴关系，即我们的协作和可解释的贝叶斯优化（CoExBO）框架。CoExBO使用偏好学习来无缝地将人类见解整合到优化中，从而产生与用户使用偏好一致的算法建议。CoExBO解释其每次迭代的候选选择，以培养信任，使用户更清楚地掌握优化的过程。此外，CoExBO提供无害保证，允许用户犯错误；即使在极端对抗性干扰下，算法也会渐进地收敛。

    Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
    
[^4]: 优化组合问题的解采样器：策略梯度方法的梯度方向

    Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v1 [cs.LG])

    [http://arxiv.org/abs/2310.05309](http://arxiv.org/abs/2310.05309)

    本文提出了一种新颖的理论框架，为深度神经网络和增强学习方法在解决组合问题方面的有效性提供了肯定的答案。这个框架对于解决包括最大割和最小割、最大$k$约束问题、最大权重二分图匹配和旅行商问题在内的广泛的组合问题有重要的意义。

    

    深度神经网络和增强学习方法在解决复杂的组合问题方面具有很高的实用价值。在这些方法中，深度神经网络被用作解决方案生成器，然后通过梯度下降等方法进行训练，以逐步获得更好的解决方案分布。在这项工作中，我们引入了一种新颖的理论框架来分析这些方法的有效性。我们的主要贡献是对这个问题的积极回答。我们的结果适用于包括最大割和最小割、最大$k$约束问题、最大权重二分图匹配和旅行商问题在内的广泛的组合问题。

    Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. A
    
[^5]: 一个在有噪声观测下修正的期望改善采集函数

    A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v1 [cs.LG])

    [http://arxiv.org/abs/2310.05166](http://arxiv.org/abs/2310.05166)

    这个论文提出了一个修正的期望改善采集函数，在贝叶斯优化中解决了对于有噪声观测的情况下忽略候选解不确定性的问题。

    

    序列最大化期望改善(EI)是贝叶斯优化中最常用的策略之一，因其简单性和处理噪声观测的能力而广泛应用。特别是，在噪声环境中，改善函数通常使用最佳后验均值作为最佳候选解。然而，在许多解析的EI类型方法中，常常忽略与候选解相关的不确定性：在无噪声的情况下导出了一个闭合形式的采集函数，然后应用于有噪声观测的情况。为了解决这个限制，我们提出了一种修正EI的方法，将高斯过程(GP)模型提供的协方差信息纳入其闭合形式表达式中。这个采集函数与经典的无噪声结果相吻合，我们认为它应该取代贝叶斯优化软件包、教程和教材中的那个公式。这个改进的采集函数为有噪声和无噪声的解提供了良好的适用性。

    Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless s
    
[^6]: 可扩展的神经网络模型和千兆级数据集用于粒子流重建

    Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])

    [http://arxiv.org/abs/2309.06782](http://arxiv.org/abs/2309.06782)

    本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。

    

    本研究针对高能电子-正电子碰撞中基于高度粒度探测器模拟的完整事件重建，研究了可扩展的机器学习模型。粒子流（PF）重建可通过跟踪和量能器团簇或击中来构建监督学习任务。我们比较了图神经网络和基于内核的变换器，并证明两者都避免了二次内存分配和计算成本，同时实现了真实的粒子流重建。我们展示了在超级计算机上进行的超参数调优显著提高了模型的物理性能。我们还展示了所得模型在硬件处理器上具有高度可移植性，支持NVIDIA, AMD和英特尔 Habana卡。最后，我们证明了模型可以在由跟踪和量能器击中组成的高粒度输入上进行训练，从而获得与基准相竞争的物理性能。有关复现研究的数据集和软件已发布。

    We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
    
[^7]: 最大均差相似度遇上神经网络：Radon-Kolmogorov-Smirnov检验

    Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v1 [stat.ML])

    [http://arxiv.org/abs/2309.02422](http://arxiv.org/abs/2309.02422)

    本文将最大均差相似度应用于神经网络，并提出了一种称为Radon-Kolmogorov-Smirnov（RKS）检验的方法，该方法将样本均值差异最大化的问题推广到多维空间和更高平滑度顺序，同时与神经网络密切相关。

    

    最大均差相似度（MMD）是一类基于最大化两个分布$P$和$Q$之间样本均值差异的非参数双样本检验，其中考虑了所有在某个函数空间$\mathcal{F}$中的数据变换$f$的选择。受到最近将所谓的Radon有界变差函数（RBV）和神经网络联系起来的工作的启发（Parhi和Nowak, 2021, 2023），我们研究了将$\mathcal{F}$取为给定平滑度顺序$k \geq 0$下的RBV空间中的单位球的MMD。这个检验被称为Radon-Kolmogorov-Smirnov（RKS）检验，可以看作是对多维空间和更高平滑度顺序的经典Kolmogorov-Smirnov（KS）检验的一般化。它还与神经网络密切相关：我们证明RKS检验中的证据函数$f$，即达到最大均差的函数，总是一个二次样条函数。

    Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree
    
[^8]: 如何扩展您的EMA（arXiv:2307.13813v1 [stat.ML]）

    How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])

    [http://arxiv.org/abs/2307.13813](http://arxiv.org/abs/2307.13813)

    本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。

    

    在实际机器学习中，保持训练动态在批量大小之间的一致性是一种重要工具，它能够在批量大小和墙钟时间之间进行权衡。这种权衡通常通过一个缩放规则来实现，例如，在随机梯度下降中，应该将学习率与批量大小呈线性关系。另一个实际机器学习的重要工具是模型指数移动平均（EMA），它是一个不接收梯度信息的模型副本，而是以一定的动量跟随其目标模型。这个模型EMA可以提高监督学习的稳健性和泛化性能，稳定伪标记，为自监督学习提供学习信号。之前的研究将模型EMA与优化分开处理，导致批量大小之间存在不同的训练动态和较低的模型性能。在这项工作中，我们提供了在存在模型EMA的情况下进行优化的缩放规则，并展示了其效果。

    Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
    
[^9]: 时间差分强化学习的动态

    Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])

    [http://arxiv.org/abs/2307.04841](http://arxiv.org/abs/2307.04841)

    我们使用统计物理学的概念，研究了时间差分学习在线性函数逼近器下的典型学习曲线。我们发现由于子采样可能的轨迹空间而产生的随机半梯度噪声会导致值误差出现显著的平台。

    

    强化学习在需要学习在反馈有限的环境中行动的多个应用中取得了成功。然而，尽管有这种经验上的成功，仍然没有对强化学习模型的参数和用于表示状态的特征如何相互作用控制学习动态的理论理解。在这项工作中，我们使用统计物理学的概念，研究线性函数逼近器下时间差分学习价值函数的典型学习曲线。我们的理论是在一个高斯等效假设下推导出来的，其中对随机轨迹的平均值被替换为时态相关的高斯特征平均值，并且我们在小规模马尔可夫决策过程上验证了我们的假设。我们发现，由于对可能的轨迹空间进行子采样而产生的随机半梯度噪声导致值误差出现显著的平台，这与传统的梯度下降不同。

    Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
    
[^10]: 运输、变分推断和扩散：应用于回火流和薛定谔桥的论文研究

    Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])

    [http://arxiv.org/abs/2307.01050](http://arxiv.org/abs/2307.01050)

    本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。

    

    本文探讨了最优运输与变分推断之间的联系，重点研究了正向和反向随机微分方程以及Girsanov变换。我们提出了一个基于路径空间散度的采样和生成建模的原则性和系统性框架。我们的工作最终发展出一个新颖的基于得分的回火流技术（与统计物理中的Jarzynski和Crooks恒等式有关）和一个正则化的迭代比例拟合（IPF）型目标，不同于标准IPF的顺序性。通过一系列的生成建模示例和基于双井的稀有事件任务，我们展示了所提方法的潜力。

    This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
    
[^11]: 比较因果框架：潜在结果、结构模型、图和抽象

    Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions. (arXiv:2306.14351v1 [stat.ME])

    [http://arxiv.org/abs/2306.14351](http://arxiv.org/abs/2306.14351)

    本文比较了因果框架中的潜在结果模型(RCM)和结构因果模型(SCM)，并阐明了RCM成为SCM可表达的条件，以及每个RCM作为某些可表达的RCM的抽象。作者介绍了SCM原则在RCM经典应用中的重要作用，并提出了由图表示的代数约束的特征，有助于进一步比较两个框架。

    

    本文旨在阐明潜在结果模型（RCM）与结构因果模型（SCM）框架在因果推断中的关系。采用中立的逻辑视角，借鉴以前的研究成果，我们展示了RCM成为SCM可表达的条件。一个关键结果显示，每个RCM -- 包括那些违反SCM框架中暗示的代数原则的RCM -- 作为某些可表达的RCM的抽象而出现。最后，我们通过准确定位SCM原则在RCM经典应用中的重要作用，阐明了这种改进性视角的优势；反之，我们提供了由图表示的代数约束的特征，有助于进一步比较两个框架。

    The aim of this paper is to make clear and precise the relationship between the Rubin causal model (RCM) and structural causal model (SCM) frameworks for causal inference. Adopting a neutral logical perspective, and drawing on previous work, we show what is required for an RCM to be representable by an SCM. A key result then shows that every RCM -- including those that violate algebraic principles implied by the SCM framework -- emerges as an abstraction of some representable RCM. Finally, we illustrate the power of this ameliorative perspective by pinpointing an important role for SCM principles in classic applications of RCMs; conversely, we offer a characterization of the algebraic constraints implied by a graph, helping to substantiate further comparisons between the two frameworks.
    
[^12]: 相互作用测量，分区格和核测试用于高阶相互作用

    Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions. (arXiv:2306.00904v1 [stat.ML])

    [http://arxiv.org/abs/2306.00904](http://arxiv.org/abs/2306.00904)

    本文提出了一种用于高阶相互作用的测量方法和基于核的测试方法，并与格理论建立了数学联系，为增强相互作用模型提供了方法。

    

    仅依赖于成对关系的模型往往无法捕捉到各种领域（如社会经济、生态或生物医学系统）中找到的复杂多变量数据的完整统计结构。两个以上变量组之间的非平凡依赖关系在这些系统的分析和建模中可以发挥重要作用，但从数据中提取这样的高阶相互作用仍然具有挑战性。本文引入了一系列$d$-order ($d \geq 2$)相互作用测量，依次包括可能的联合概率分布分解，并定义了非参数、基于核的测试，以系统地确定$d$-order相互作用的统计显着性。同时，我们建立了与格理论的数学联系，阐明了相互作用度量的导出及其复合排列测试的涵义；澄清了单纯复合体与核矩阵中心化的联系；并提供了一种增强相互作用模型的方法。

    Models that rely solely on pairwise relationships often fail to capture the complete statistical structure of the complex multivariate data found in diverse domains, such as socio-economic, ecological, or biomedical systems. Non-trivial dependencies between groups of more than two variables can play a significant role in the analysis and modelling of such systems, yet extracting such high-order interactions from data remains challenging. Here, we introduce a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly inclusive of possible factorisations of the joint probability distribution, and define non-parametric, kernel-based tests to establish systematically the statistical significance of $d$-order interactions. We also establish mathematical links with lattice theory, which elucidate the derivation of the interaction measures and their composite permutation tests; clarify the connection of simplicial complexes with kernel matrix centring; and provide a means to enhan
    
[^13]: 一种记忆视角下的Transformer生成模型

    Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v1 [stat.ML])

    [http://arxiv.org/abs/2306.00802](http://arxiv.org/abs/2306.00802)

    本文研究了transformers如何平衡全局分布和上下文特定分布的两种知识类型，并提供了有关权值矩阵作为联想记忆的作用及梯度如何实现权重学习的理论见解。

    

    基于Transformer的大型语言模型取得了巨大的实证成功。然而，随着它们被广泛部署，越来越需要更好地理解它们的内部机制以使它们更加可靠。我们研究了transformers如何通过考虑一个合成的设置来平衡存储于它们之中的两种知识类型——全局分布和上下文特定的二元分布。通过对简化的两层Transformer的训练过程进行仔细的实证分析，我们阐述了对全局二元分布的快速学习以及对上下文中的二元分布的"归纳头"机制的较慢发展。我们强调了权值矩阵作为联想记忆的作用，提供了理论上的见解，说明了梯度如何在训练过程中实现权重的学习，并研究了数据分布的作用。

    Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributio
    
[^14]: 通过概率生成函数的贝叶斯离散模型精确推理：概率编程方法

    Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v1 [cs.PL])

    [http://arxiv.org/abs/2305.17058](http://arxiv.org/abs/2305.17058)

    该论文提出一种精确推理离散统计模型的贝叶斯方法，支持离散采样、连续采样、离散观测、仿射函数、（随机）分支和事件条件。通过概率生成函数实现后验概率、期望、方差和高阶矩的精确计算。该方法性能优于近似蒙特卡洛方法，并避免了近似误差。

    

    我们提出了一种离散统计模型的精确贝叶斯推理方法，即使是对于无限支持和连续先验也可以找到准确的解决方案。为了表达这样的模型，我们引入了一种支持离散和连续采样、离散观测、仿射函数、（随机）分支和事件条件的概率编程语言。我们的关键工具是概率生成函数：它们提供了定义程序的分布的紧凑闭合形式表示，从而实现了后验概率、期望、方差和高阶矩的精确计算。我们的推理方法是可证明正确的、完全自动化的，使用自动微分（特别是泰勒多项式），但不需要计算机代数。我们的实验表明，它在一系列真实世界的例子中的性能与近似蒙特卡洛方法竞争，同时避免了近似误差。

    We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to many discrete inference problems, even with infinite support and continuous priors. To express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on events. Our key tool is probability generating functions: they provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments. Our inference method is provably correct, fully automated and uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra. Our experiments show that its performance on a range of real-world examples is competitive with approximate Monte Carlo methods, while avoiding approximation errors
    
[^15]: 均值漂移的收敛性分析

    Convergence Analysis of Mean Shift. (arXiv:2305.08463v1 [stat.ML])

    [http://arxiv.org/abs/2305.08463](http://arxiv.org/abs/2305.08463)

    本研究提出了均值漂移算法的模估计序列的收敛保证，并扩展了现有的涵盖解析核和Epanechnikov核的发现，意义在于涵盖了在基于KDE的模估计的渐近统计效率方面最优的非负核——双重核。

    

    均值漂移（MS）算法寻找核密度估计（KDE）的模。本研究提出了一种由MS算法产生的模估计序列的收敛保证，并在相当温和的条件下，借助于关于{\L}ojasiewicz不等式的论证，评估了收敛速度。我们的发现扩展了现有的涵盖解析核和Epanechnikov核的发现，意义在于涵盖了在基于KDE的模估计的渐近统计效率方面最优的非负核——双重核。

    The mean shift (MS) algorithm seeks a mode of the kernel density estimate (KDE). This study presents a convergence guarantee of the mode estimate sequence generated by the MS algorithm and an evaluation of the convergence rate, under fairly mild conditions, with the help of the argument concerning the {\L}ojasiewicz inequality. Our findings, which extend existing ones covering analytic kernels and the Epanechnikov kernel, are significant in that they cover the biweight kernel that is optimal among non-negative kernels in terms of the asymptotic statistical efficiency for the KDE-based mode estimation.
    
[^16]: 松弛假设下Adam收敛性的证明

    Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])

    [http://arxiv.org/abs/2304.13972](http://arxiv.org/abs/2304.13972)

    本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。

    

    本文针对一类广泛的优化目标，对自适应矩估计（Adam）算法的收敛性进行了严格证明。虽然Adam算法在训练深度神经网络中的流行度和效率很高，但其理论性质尚未完全理解，现有的收敛性证明需要过于强的假设，如全局梯度有界，以证明收敛到稳定点。本文证明了在更为现实的条件下，Adam能以$\mathcal{O}(\epsilon^{-4})$梯度复杂度收敛到$\epsilon$-稳定点。我们分析的关键是根据一种广义光滑性假设给出的，沿着优化轨迹的梯度有界的新证明。根据该假设，局部光滑性(即存在时的Hessian norm)受梯度范数的次平方函数限制。此外，我们提出了一种方差约减版本的Adam与加速Gradient。

    In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien
    
[^17]: 有限宽度核和平均场神经网络中的预测波动动力学分析

    Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])

    [http://arxiv.org/abs/2304.03408](http://arxiv.org/abs/2304.03408)

    本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。

    

    我们分析了宽但有限的特征学习神经网络中有限宽度效应的动力学。与许多先前的分析不同，我们的结果是针对特征学习强度的非微扰有限宽度的结果。从无限宽深度神经网络核和预测动力学的动力学平均场理论（DMFT）描述开始，我们提供了对网络权重的随机初始化下DMFT序参数$\mathcal{O}(1/\sqrt{\text{width}})$波动的表征。在网络训练的懒惰极限中，所有核都是随机的但在时间上静止的，预测方差具有通用形式。然而，在富有特征学习的区域，核和预测的波动是动态耦合且方差可以被自洽计算。在两层网络中，我们展示了特征学习如何动态地减少最终NTK和最终网络预测的方差。我们还展示了如何进行初始化。

    We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
    
[^18]: 面向未知具有潜在状态系统的学习优化控制方法

    Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])

    [http://arxiv.org/abs/2303.17963](http://arxiv.org/abs/2303.17963)

    本文提出了一种面向未知具有潜在状态系统的学习优化控制方法，并给出了概率性能保证，同时提出了一种验证任意控制律性能的方法。

    

    随着控制工程方法应用于越来越复杂的系统，数据驱动的系统辨识方法成为物理建模的有希望的替代方法。然而，许多这些方法依赖于状态测量的可用性，而复杂系统的状态通常不是直接可测量的。因此，可能需要同时估计动力学和潜在状态，从而更加具有挑战性地设计具有性能保证的控制器。本文提出了一种新方法，用于计算具有潜在状态的未知非线性系统的最优输入轨迹。对结果输入轨迹进行了概率性能保证，并提出了一种验证任意控制律性能的方法。本文在数值模拟中展示了所提出方法的有效性。

    As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
    
[^19]: 在混淆下的离线策略评估和优化。

    Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16583](http://arxiv.org/abs/2211.16583)

    该论文致力于解决离线强化学习中混淆变量导致策略评估和优化存在挑战的问题，包括无法获得一致价值估计和样本复杂度的保证，作者提出了具有保证的下限算法和局部收敛的改进算法。

    

    在离线强化学习中，评估和优化策略在存在未观察到的混淆变量时是一个备受关注的问题。使用传统的离线强化学习方法来处理混淆问题不仅可能导致糟糕的决策和策略，而且在关键应用领域如医疗和教育中可能会产生灾难性的影响。我们勾勒了混淆的 MDP 离线策略评估的面貌，并根据混淆对数据收集策略的时间演变和影响来区分混淆的假设。在一些情况下，我们确定了一些无法获得一致价值估计的情况，并提供和讨论了计算具有保证的下限的算法。当一致的估计可行时，我们提供了样本复杂度的保证。我们还提出了新的离线策略改进算法，并证明了局部收敛的保证。最后，我们在格子世界和模拟医疗场景中对算法进行了实验评估。

    Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but can also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on their time-evolution and effect on the data-collection policies. We determine when consistent value estimates are not achievable, providing and discussing algorithms to estimate lower bounds with guarantees in those cases. When consistent estimates are achievable, we provide sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on gridworld and a simulated healthcare settin
    
[^20]: 带有递减半径的块主导极小化方法用于约束非凸优化

    Block majorization-minimization with diminishing radius for constrained nonconvex optimization. (arXiv:2012.03503v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2012.03503](http://arxiv.org/abs/2012.03503)

    这个论文介绍了一种用于非凸约束优化问题的块主导极小化算法，并证明了在使用强凸替代函数的情况下，该算法可以在一定迭代次数内收敛到一个稳定点，并提出了一个信任域变体，可以处理只具备凸性的替代函数。

    

    块主导极小化（BMM）是一种简单的迭代算法，用于非凸约束优化，在每个块坐标上顺序最小化目标函数的主导替代函数，而其他坐标保持固定。BMM包括一大类优化算法，如块坐标下降及其近端点变体，期望最大化和块投影梯度下降。我们证明，对于一般约束非凸优化，使用强凸替代函数的BMM可以在$O(\epsilon^{-2}(\log \epsilon^{-1})^{2})$次迭代中产生一个$\epsilon$-稳定点，并渐近收敛于稳定点集。此外，我们提出了一种信任域变体的BMM，可以处理只具备凸性的替代函数，并仍然获得相同的迭代复杂度和渐近稳定性。这些结果具有鲁棒性，即使凸子问题的求解是非精确的，只要最优间隙满足要求。

    Block majorization-minimization (BMM) is a simple iterative algorithm for nonconvex constrained optimization that sequentially minimizes majorizing surrogates of the objective function in each block coordinate while the other coordinates are held fixed. BMM entails a large class of optimization algorithms such as block coordinate descent and its proximal-point variant, expectation-minimization, and block projected gradient descent. We establish that for general constrained nonconvex optimization, BMM with strongly convex surrogates can produce an $\epsilon$-stationary point within $O(\epsilon^{-2}(\log \epsilon^{-1})^{2})$ iterations and asymptotically converges to the set of stationary points. Furthermore, we propose a trust-region variant of BMM that can handle surrogates that are only convex and still obtain the same iteration complexity and asymptotic stationarity. These results hold robustly even when the convex sub-problems are inexactly solved as long as the optimality gaps are 
    

