# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Using Property Elicitation to Understand the Impacts of Fairness Constraints.](http://arxiv.org/abs/2309.11343) | 这项研究使用属性引导方法来探索损失函数和正则化函数与最优决策之间的关系，特别是在公平机器学习中的应用。它提供了损失函数和正则化函数成对时属性改变的必要和充分条件，并通过实验证明了算法决策与数据分布变化和约束难度的相关性。 |
| [^2] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^3] | [Will More Expressive Graph Neural Networks do Better on Generative Tasks?.](http://arxiv.org/abs/2308.11978) | 本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。 |
| [^4] | [On the Expected Size of Conformal Prediction Sets.](http://arxiv.org/abs/2306.07254) | 该论文研究了适应性预测集的期望大小问题，提出了一种理论量化方法以及点估计和高概率区间，并在真实数据集上验证了其实用性。 |
| [^5] | [Learning Linear Causal Representations from Interventions under General Nonlinear Mixing.](http://arxiv.org/abs/2306.02235) | 本文针对先前工作弱一类问题进行推广，提出了一种用于在非线性混合下的干预中学习线性因果表示的强可识别性算法，证明了其有效性，并提出了在实践中识别潜在变量的对比算法。 |
| [^6] | [Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts.](http://arxiv.org/abs/2305.19951) | 本研究通过将NeSy模型中出现的推理快捷方式定义为学习目标的意外最优解，并确定其发生的四个关键条件，提出了几种可行的缓解策略并对其进行了分析，显示推理快捷方式难以处理。 |
| [^7] | [Learning Two-Layer Neural Networks, One (Giant) Step at a Time.](http://arxiv.org/abs/2305.18270) | 本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。 |
| [^8] | [GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations.](http://arxiv.org/abs/2305.17021) | GLOBE-CE是一种用于全球因果解释的机器学习方法，它可以超越局部解释，提供更有效和交互式的解释工具。 |
| [^9] | [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding.](http://arxiv.org/abs/2305.14196) | ZeroSCROLLS是一个用于长文本自然语言理解的零Shot基准测试，包括六个任务和四个数据集，能够评估大型语言模型的性能。当前，GPT-4的平均得分最高，但在聚合任务等多个挑战上，仍有改进的空间。 |
| [^10] | [Optimality of Message-Passing Architectures for Sparse Graphs.](http://arxiv.org/abs/2305.10391) | 本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。 |
| [^11] | [On the connections between optimization algorithms, Lyapunov functions, and differential equations: theory and insights.](http://arxiv.org/abs/2305.08658) | 本研究在推广线性矩阵不等式框架的基础上，研究了微分方程和优化算法的联系，提出了针对一个两参数Nesterov优化方法家族新的李亚普诺夫函数并表征其收敛速度，在此基础上证明了有显著改进的Nesterov方法的收敛速度，并确定出产生最佳速度的系数选择。 |
| [^12] | [Low-complexity subspace-descent over symmetric positive definite manifold.](http://arxiv.org/abs/2305.02041) | 本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。 |
| [^13] | [Continuous-Time Functional Diffusion Processes.](http://arxiv.org/abs/2303.00800) | 连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。 |
| [^14] | [Extrapolated cross-validation for randomized ensembles.](http://arxiv.org/abs/2302.13511) | 本论文提出了一种名为ECV的交叉验证方法，用于调整随机集成中的集成和子样本大小。该方法基于Out-of-Bag错误和利用预测风险分解结构的新型风险外推技术，能够产生$\delta$-最优（关于Oracle调整风险）的集成。 |
| [^15] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^16] | [Direct Uncertainty Quantification.](http://arxiv.org/abs/2302.02420) | 本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。 |
| [^17] | [Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models.](http://arxiv.org/abs/2207.06950) | 本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。 |
| [^18] | [Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks.](http://arxiv.org/abs/2204.01682) | 本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。 |

# 详细

[^1]: 使用属性引导方法来理解公平性约束的影响

    Using Property Elicitation to Understand the Impacts of Fairness Constraints. (arXiv:2309.11343v1 [cs.LG])

    [http://arxiv.org/abs/2309.11343](http://arxiv.org/abs/2309.11343)

    这项研究使用属性引导方法来探索损失函数和正则化函数与最优决策之间的关系，特别是在公平机器学习中的应用。它提供了损失函数和正则化函数成对时属性改变的必要和充分条件，并通过实验证明了算法决策与数据分布变化和约束难度的相关性。

    

    预测算法通常通过优化某个损失函数进行训练，并添加正则化函数来施加违反约束的惩罚。预期地，添加这样的正则化函数可以改变目标函数的最小化值。目前还不清楚哪些正则化函数会改变损失函数的最小化值，以及当最小化值发生变化时，它会如何变化。我们使用属性引导方法来初步了解损失函数和正则化函数与给定问题实例的最优决策之间的联合关系。具体而言，我们给出了损失函数和正则化函数成对时，属性改变的必要和充分条件，并研究了一些满足这个条件的正则化函数在公平机器学习文献中的标准。我们通过实验证明了算法决策如何随着数据分布的变化和约束的难度而改变。

    Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraint
    
[^2]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^3]: 更具表现力的图神经网络在生成任务中是否更好？

    Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])

    [http://arxiv.org/abs/2308.11978](http://arxiv.org/abs/2308.11978)

    本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。

    

    图生成是一个重要的挑战，它涉及根据给定的标签预测一个完整的具有多个节点和边的图。这个任务对许多实际应用非常重要，包括药物和分子设计。近年来，在图生成领域出现了几种成功的方法。然而，这些方法存在两个重大问题：(1) 这些方法中使用的基础图神经网络（GNN）架构往往未经深入探索；(2) 这些方法往往只在有限的指标上进行评估。为填补这个空白，我们通过将图生成模型的基础GNN替换为更具表现力的GNN，研究了GNN在分子图生成任务中的表现能力。具体而言，我们分析了两种不同生成框架（GCPN和GraphAF）中六种GNN在六个不同的分子生成目标上的性能。

    Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
    
[^4]: 关于适应性预测集期望大小的研究

    On the Expected Size of Conformal Prediction Sets. (arXiv:2306.07254v1 [stat.ML])

    [http://arxiv.org/abs/2306.07254](http://arxiv.org/abs/2306.07254)

    该论文研究了适应性预测集的期望大小问题，提出了一种理论量化方法以及点估计和高概率区间，并在真实数据集上验证了其实用性。

    

    虽然适应性预测器在误差频率方面具有严格的统计保证，但其预测集大小对其实际效用至关重要。不幸的是，目前缺乏有限样本分析和预测集大小的保证。为了解决这个问题，我们在分裂适应性预测框架下理论量化预测集的期望大小。因为这种精确的计算通常无法直接计算，我们进一步推导出可轻松计算的点估计和高概率区间，提供了一种描述测试和校准数据不同可能实现的期望预测集大小的实用方法。此外，我们通过对回归和分类问题的真实世界数据集进行实验证实了我们结果的实用性。

    While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data. Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems.
    
[^5]: 从非线性混合下的干预中学习线性因果表示

    Learning Linear Causal Representations from Interventions under General Nonlinear Mixing. (arXiv:2306.02235v1 [cs.LG])

    [http://arxiv.org/abs/2306.02235](http://arxiv.org/abs/2306.02235)

    本文针对先前工作弱一类问题进行推广，提出了一种用于在非线性混合下的干预中学习线性因果表示的强可识别性算法，证明了其有效性，并提出了在实践中识别潜在变量的对比算法。

    

    我们研究了在混合函数完全通用的一般设置下，从未知的潜在干预中学习因果表示的问题，其中潜在分布是高斯分布。 我们证明了在单节点未知干预（即没有干预目标的情况下）给出强可识别性结果。这推广了先前的工作，先前的工作着重于更弱的类别，例如线性映射或成对的反事实数据。这也是首次从非配对干预的深度神经网络嵌入中获得因果可识别性。我们的证明依赖于仔细揭示经过非线性密度转换后数据分布中存在的高维几何结构，我们通过分析潜在分布的精度矩阵的二次形式来捕捉这种结构。最后，我们提出了一种对比算法来实际识别潜在变量，并评估其在各种任务上的性能。

    We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of causal identifiability from non-paired interventions for deep neural network embeddings. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks.
    
[^6]: 不是所有神经符号概念都是平等的： 推理快捷方式的分析和缓解

    Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts. (arXiv:2305.19951v1 [cs.LG])

    [http://arxiv.org/abs/2305.19951](http://arxiv.org/abs/2305.19951)

    本研究通过将NeSy模型中出现的推理快捷方式定义为学习目标的意外最优解，并确定其发生的四个关键条件，提出了几种可行的缓解策略并对其进行了分析，显示推理快捷方式难以处理。

    

    神经符号（NeSy）预测模型承诺具有改进的约束遵从性，系统化泛化和可解释性，因为它们允许通过对从子符号输入中提取出的高级概念进行推理来推断与某些先验知识一致的标签。最近显示NeSy预测器受到推理快捷方式的影响：它们可以通过利用具有意外语义的概念达到高精度，从而短于其承诺的优势。但是，缺少对推理快捷方式及其潜在缓解策略的系统描述。本文通过将其表征为学习目标的意外最优解，并确定其发生背后的四个关键条件来填补这一空白。基于此，我们推导出几种自然的缓解策略，并从理论和实证角度分析它们的功效。我们的分析显示，推理快捷方式很难处理，这对于信任它们的合理性产生了疑问。

    Neuro-Symbolic (NeSy) predictive models hold the promise of improved compliance with given constraints, systematic generalization, and interpretability, as they allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. It was recently shown that NeSy predictors are affected by reasoning shortcuts: they can attain high accuracy but by leveraging concepts with unintended semantics, thus coming short of their promised advantages. Yet, a systematic characterization of reasoning shortcuts and of potential mitigation strategies is missing. This work fills this gap by characterizing them as unintended optima of the learning objective and identifying four key conditions behind their occurrence. Based on this, we derive several natural mitigation strategies, and analyze their efficacy both theoretically and empirically. Our analysis shows reasoning shortcuts are difficult to deal with, casting doubts on the trus
    
[^7]: 学习两层神经网络：一次(巨大)的步骤。

    Learning Two-Layer Neural Networks, One (Giant) Step at a Time. (arXiv:2305.18270v1 [stat.ML])

    [http://arxiv.org/abs/2305.18270](http://arxiv.org/abs/2305.18270)

    本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。

    

    我们研究了浅层神经网络的训练动态，研究了有限数量的大批量梯度下降步骤有助于在核心范围之外促进特征学习的条件。我们比较了批量大小和多个(但有限的)步骤的影响。我们分析了单步骤过程，发现批量大小为$n=O(d)$可以促进特征学习，但只适合学习单一方向或单索引模型。相比之下，$n=O(d^2)$对于学习多个方向和专业化至关重要。此外，我们证明“硬”方向缺乏前$\ell$个Hermite系数，仍未被发现，并且需要批量大小为$n=O(d^\ell)$才能被梯度下降捕获。经过几次迭代，情况发生变化：批量大小为$n=O(d)$足以学习新的目标方向，这些方向在Hermite基础上线性连接到之前学习的方向所涵盖的子空间。

    We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions,
    
[^8]: GLOBE-CE：一种用于全球因果解释的基于翻译的方法

    GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v1 [cs.LG])

    [http://arxiv.org/abs/2305.17021](http://arxiv.org/abs/2305.17021)

    GLOBE-CE是一种用于全球因果解释的机器学习方法，它可以超越局部解释，提供更有效和交互式的解释工具。

    

    因果解释在可解释性方面得到了广泛研究，公平性、追索权和模型理解等领域的应用依赖于一系列方法。然而，这些方法最大的缺点是无法提供超越局部或实例级别的解释。尽管许多作品涉及全局解释的概念，通常建议聚合大量局部解释以确定全局属性，但很少提供可靠且计算可行的框架。同时，实践者需要更有效和交互式的可解释性工具。我们借此机会提出了全局且有效的反事实解释框架(GLOBE-CE)，这是一个灵活的框架，解决了当前最先进框架在高维数据集和连续特征存在的可靠性和可扩展性问题。此外，我们提供了一个独特的数学模型。

    Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global & Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathemati
    
[^9]: ZeroSCROLLS：一个用于长文本理解的零Shot基准测试

    ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])

    [http://arxiv.org/abs/2305.14196](http://arxiv.org/abs/2305.14196)

    ZeroSCROLLS是一个用于长文本自然语言理解的零Shot基准测试，包括六个任务和四个数据集，能够评估大型语言模型的性能。当前，GPT-4的平均得分最高，但在聚合任务等多个挑战上，仍有改进的空间。

    

    我们介绍了 ZeroSCROLLS，这是一个用于长文本自然语言理解的零Shot基准测试，仅包含测试集而没有训练或开发数据。我们从SCROLLS基准测试中适应了六个任务，并添加了四个新数据集，包括两个新的信息融合任务，例如聚合正面评价的百分比。使用ZeroSCROLLS，我们对开源和闭源大型语言模型进行了全面评估，发现Claude优于ChatGPT，并且GPT-4获得了最高的平均分数。然而，在ZeroSCROLLS的多个开放挑战方面（例如，聚合任务），还有改进的空间，因为模型很难通过朴素的基准测试。由于最先进的技术还在不断更新，我们邀请研究人员在实时的ZeroSCROLLS排行榜上评估他们的想法。

    We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
    
[^10]: 稀疏图的消息传递架构的最优性

    Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])

    [http://arxiv.org/abs/2305.10391](http://arxiv.org/abs/2305.10391)

    本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。

    

    我们研究了特征装饰图上的节点分类问题，在稀疏设置下，即节点的预期度数为节点数的O(1)时。这样的图通常被称为本地树状图。我们引入了一种叫做渐近本地贝叶斯最优性的节点分类任务的贝叶斯最优性概念，并根据这个标准计算了具有任意节点特征和边连接分布的相当一般的统计数据模型的最优分类器。该最优分类器可以使用消息传递图神经网络架构实现。然后我们计算了该分类器的泛化误差，并在一个已经研究充分的统计模型上从理论上与现有的学习方法进行比较。我们发现，在低图信号的情况下，最佳消息传递架构插值于标准MLP和一种典型的c架构之间。

    We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
    
[^11]: 关于优化算法、李亚普诺夫函数和微分方程的联系：理论与洞见研究

    On the connections between optimization algorithms, Lyapunov functions, and differential equations: theory and insights. (arXiv:2305.08658v1 [math.OC])

    [http://arxiv.org/abs/2305.08658](http://arxiv.org/abs/2305.08658)

    本研究在推广线性矩阵不等式框架的基础上，研究了微分方程和优化算法的联系，提出了针对一个两参数Nesterov优化方法家族新的李亚普诺夫函数并表征其收敛速度，在此基础上证明了有显著改进的Nesterov方法的收敛速度，并确定出产生最佳速度的系数选择。

    

    本研究通过推广Fazylab等人在2018年发展的线性矩阵不等式框架，研究了用李亚普诺夫函数研究$m$-强凸和$L$-光滑函数的微分方程和优化算法之间的联系。使用新框架，我们针对一个两参数Nesterov优化方法家族的新型（离散）李亚普诺夫函数进行了解析推导，并表征了其收敛速度。这使得我们能够证明对于标准系数的Nesterov方法的先前证明速度有了明显改进，并且表征了产生最佳速度的系数选择。我们为Polyak ODE获得了新的李亚普诺夫函数，并重新审视了此ODE与Nesterov算法之间的联系。此外，讨论了将Nesterov方法解释为加性Runge-Kutta离散化的新方法，并解释了离散化Polyak方程的结构条件。

    We study connections between differential equations and optimization algorithms for $m$-strongly and $L$-smooth convex functions through the use of Lyapunov functions by generalizing the Linear Matrix Inequality framework developed by Fazylab et al. in 2018. Using the new framework we derive analytically a new (discrete) Lyapunov function for a two-parameter family of Nesterov optimization methods and characterize their convergence rate. This allows us to prove a convergence rate that improves substantially on the previously proven rate of Nesterov's method for the standard choice of coefficients, as well as to characterize the choice of coefficients that yields the optimal rate. We obtain a new Lyapunov function for the Polyak ODE and revisit the connection between this ODE and the Nesterov's algorithms. In addition discuss a new interpretation of Nesterov method as an additive Runge-Kutta discretization and explain the structural conditions that discretizations of the Polyak equation
    
[^12]: 对称正定流形上低复杂度的子空间下降算法

    Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])

    [http://arxiv.org/abs/2305.02041](http://arxiv.org/abs/2305.02041)

    本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。

    

    本文提出了一种低复杂度的黎曼子空间下降算法，用于在对称正定（SPD）流形上对函数进行最小化。与现有的黎曼梯度下降变体不同的是，所提出的方法利用 carefully chosen 的子空间，使得更新可以写成迭代的 Cholesky 因子和一个稀疏矩阵的乘积形式。由此产生的更新避免了昂贵的矩阵操作，如矩阵指数和密集矩阵乘法，这些操作通常在几乎所有其他 Riemannian 优化算法中都是必需的。

    This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
    
[^13]: 连续时间功能扩散过程

    Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00800](http://arxiv.org/abs/2303.00800)

    连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。

    

    我们引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。 FDPs需要一种新的数学框架来描述前向和反向动力学，并进行多个扩展以得出实际的训练目标。这些扩展包括Girsanov定理的无限维版本，以便能够计算ELBO，以及采样定理的无限维版本，以确保可数个点上的函数评估等价于无限维函数。我们使用FDPs在函数空间中构建了一种新型生成模型，不需要专门的网络架构，并且可以处理任何类型的连续数据。我们在真实数据上的结果显示，使用简单的多层感知机（MLP）结构，FDPs实现了高质量的图像生成，所需的参数数量比现有的扩散模型低几个数量级。

    We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.
    
[^14]: 针对随机集成的外推交叉验证方法

    Extrapolated cross-validation for randomized ensembles. (arXiv:2302.13511v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.13511](http://arxiv.org/abs/2302.13511)

    本论文提出了一种名为ECV的交叉验证方法，用于调整随机集成中的集成和子样本大小。该方法基于Out-of-Bag错误和利用预测风险分解结构的新型风险外推技术，能够产生$\delta$-最优（关于Oracle调整风险）的集成。

    

    集成方法，如Bagging和随机森林在各个领域中随处可见，从金融到基因组学。尽管它们很常见，但关于有效调整集成参数的问题却受到相对较少的关注。本文介绍了一种交叉验证方法，即ECV（外推交叉验证），用于调整随机集成中的集成和子样本大小。我们的方法基于两个主要因素: 使用Out-of-Bag错误来获取小型集成的初步估计值和一种利用预测风险分解结构的新型风险外推技术。通过证明我们的风险外推技术在集成和子样本大小上具有统一的一致性，我们表明ECV对于平方预测风险能够产生$\delta$-最优（关于Oracle调整风险）。我们的理论适用于一般的集成预测器，只需要温和的矩假设，并允许具有高维度特征的情况。

    Ensemble methods such as bagging and random forests are ubiquitous in various fields, from finance to genomics. Despite their prevalence, the question of the efficient tuning of ensemble parameters has received relatively little attention. This paper introduces a cross-validation method, ECV (Extrapolated Cross-Validation), for tuning the ensemble and subsample sizes in randomized ensembles. Our method builds on two primary ingredients: initial estimators for small ensemble sizes using out-of-bag errors and a novel risk extrapolation technique that leverages the structure of prediction risk decomposition. By establishing uniform consistency of our risk extrapolation technique over ensemble and subsample sizes, we show that ECV yields $\delta$-optimal (with respect to the oracle-tuned risk) ensembles for squared prediction risk. Our theory accommodates general ensemble predictors, only requires mild moment assumptions, and allows for high-dimensional regimes where the feature dimension 
    
[^15]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^16]: 直接不确定量化

    Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02420](http://arxiv.org/abs/2302.02420)

    本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。

    

    传统神经网络易于训练，但会产生过于自信的预测；而贝叶斯神经网络提供了良好的不确定量化，但优化它们需要耗费时间。本文介绍了一种新的方法——“直接不确定量化”（DirectUQ），它结合了它们的优点，其中神经网络直接输出最后一层的均值和方差。DirectUQ可以导出为一个替代的变分下界，因此从落单变分推理中获益，提供了改进的正则化器。另一方面，像非概率模型一样，DirectUQ具有简单的训练方法，可以使用Rademacher复杂性为模型提供风险边界。实验表明，DirectUQ和DirectUQ集成提供了时间和不确定性量化方面的良好平衡，特别是对于分布之外的数据。

    Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
    
[^17]: 使用基于模型的树和提升方法拟合低阶函数ANOVA模型

    Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.06950](http://arxiv.org/abs/2207.06950)

    本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。

    

    低阶函数ANOVA模型已经被机器学习社区重新发现，并称之为内在可解释的机器学习。本文提出了一种新算法GAMI-Tree，类似于EBM，但具有一些趋向更好性能的特性。我们采用模型为基础的树，并融入一种新的交互过滤方法，提高了对底层交互的捕捉。此外，我们的迭代训练方法收敛于具有更好预测性能的模型，并确保相互作用在分层意义上正交于主效应。该算法不需要广泛的调整，并且实现快速高效。我们使用模拟和真实数据集进行比较。

    Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
    
[^18]: 深度特征筛选：通过深度神经网络进行超高维数据的特征选择

    Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.01682](http://arxiv.org/abs/2204.01682)

    本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。

    This paper proposes a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome the challenges of high-dimensional, low-sample-size data and identify significant features with high precision for ultra high-dimensional, low-sample-size data.

    传统的统计特征选择方法在高维、低样本数据上的应用经常遇到困难和挑战，如过拟合、维数灾难、计算不可行和强模型假设。本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服这些问题，并对超高维、低样本数据进行高精度的特征筛选。该方法首先提取输入数据的低维表示，然后应用基于Deb和Sen（2021）最近开发的多元秩距相关性的特征筛选。该方法结合了深度神经网络和特征筛选的优点，除了处理具有少量样本的超高维数据的能力外，还具有以下吸引人的特点：（1）它是模型自由和分布自由的；（2）它可以

    The applications of traditional statistical feature selection methods to high-dimension, low sample-size data often struggle and encounter challenging problems, such as overfitting, curse of dimensionality, computational infeasibility, and strong model assumption. In this paper, we propose a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome these problems and identify significant features with high precision for ultra high-dimensional, low-sample-size data. This approach first extracts a low-dimensional representation of input data and then applies feature screening based on multivariate rank distance correlation recently developed by Deb and Sen (2021). This approach combines the strengths of both deep neural networks and feature screening, and thereby has the following appealing features in addition to its ability of handling ultra high-dimensional data with small number of samples: (1) it is model free and distribution free; (2) it can be
    

