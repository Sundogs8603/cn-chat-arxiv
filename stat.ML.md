# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Gotta match 'em all: Solution diversification in graph matching matched filters.](http://arxiv.org/abs/2308.13451) | 本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。 |
| [^2] | [Six Lectures on Linearized Neural Networks.](http://arxiv.org/abs/2308.13431) | 这篇论文通过分析线性模型，探讨了多层神经网络行为的学习。研究使用了线性回归、核岭回归、随机特征模型和神经切线模型等四个线性化神经网络模型。研究还讨论了线性理论的局限性和其他方法如何克服这些局限性。 |
| [^3] | [Enhanced Mortality Prediction In Patients With Subarachnoid Haemorrhage Using A Deep Learning Model Based On The Initial CT Scan.](http://arxiv.org/abs/2308.13373) | 这项研究使用深度学习模型基于初始CT扫描，提高了对颅内动脉瘤出血患者的死亡率预测能力。 |
| [^4] | [A topological model for partial equivariance in deep learning and data analysis.](http://arxiv.org/abs/2308.13357) | 本文提出了一种拓扑模型，用于在神经网络中编码部分等变性，并研究了相应的测量空间和P-GENEO空间的性质。 |
| [^5] | [Bayesian Reasoning for Physics Informed Neural Networks.](http://arxiv.org/abs/2308.13222) | 本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。 |
| [^6] | [Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery.](http://arxiv.org/abs/2308.13135) | 该论文提出了一种非参数可加模型，用于估计可解释的值函数，并在强化学习中具有应用价值。该方法能够克服传统模型的线性假设限制，同时提供较强的决策建议解释性。 |
| [^7] | [Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology.](http://arxiv.org/abs/2308.13068) | 多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。 |
| [^8] | [Quantum Kernel Mixtures for Probabilistic Deep Learning.](http://arxiv.org/abs/2305.18204) | 本文提出了一种量子核混合方法，可以用于表示连续和离散随机变量的联合概率分布。该框架允许构建可微分的模型，适用于密度估计、推理和采样，以及各种机器学习任务，包括生成建模和判别学习。 |
| [^9] | [Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box.](http://arxiv.org/abs/2304.05527) | 本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。 |
| [^10] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^11] | [PDSketch: Integrated Planning Domain Programming and Learning.](http://arxiv.org/abs/2303.05501) | 本文通过PDSketch语言和可训练的神经网络，实现了模型的学习和在线规划，加速了机器人的灵活性和通用性。 |
| [^12] | [Differentially Private Diffusion Models.](http://arxiv.org/abs/2210.09929) | 本研究提出了一种差分隐私扩散模型(DPDMs)，通过差分隐私训练生成模型，实现对隐私的保护，在图像生成基准测试中表现优越，能够在标准测试中与特定任务的DP-SGD训练的分类器相媲美。 |
| [^13] | [The Curse of Unrolling: Rate of Differentiating Through Optimization.](http://arxiv.org/abs/2209.13271) | 优化问题中的展开求导是机器学习中的一个重要问题，本文对其在梯度下降和 Chebyshev 方法中的二次目标提供了非渐进收敛速率分析，发现我们要确保雅可比矩阵的收敛，就必须面临展开求导的诅咒，即要么选择大的学习率导致快速渐进收敛但算法有较长的初始阶段，要么选择较小的学习率导致即时但较慢的收敛。 |
| [^14] | [Dynamics of Local Elasticity During Training of Neural Nets.](http://arxiv.org/abs/2111.01166) | 本论文研究了神经网络训练过程中的局部弹性动力学。通过对现有$S_{\rm rel}$定义的全面研究并提出新的定义，我们发现新的定义能更敏锐地检测出权重更新更偏向于在与样本数据同一类别内进行预测变化的特性。 |
| [^15] | [Block majorization-minimization with diminishing radius for constrained nonconvex optimization.](http://arxiv.org/abs/2012.03503) | 这个论文介绍了一种用于非凸约束优化问题的块主导极小化算法，并证明了在使用强凸替代函数的情况下，该算法可以在一定迭代次数内收敛到一个稳定点，并提出了一个信任域变体，可以处理只具备凸性的替代函数。 |
| [^16] | [On Model Identification and Out-of-Sample Prediction of Principal Component Regression: Applications to Synthetic Controls.](http://arxiv.org/abs/2010.14449) | 该论文在高维度的误差变量固定设计环境中分析了主成分回归模型识别和样本外预测问题，并提出了优于已知最佳速率的非渐进预测保证。在分析过程中，引入了一种自然的线性代数条件来避免样本外预测的分布假设，并构建了一个假设检验来检查该条件在实践中的可行性。同时，该论文的结果也对合成对照研究提供了新的发现。 |
| [^17] | [Frequentist Regret Bounds for Randomized Least-Squares Value Iteration.](http://arxiv.org/abs/1911.00567) | 本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。 |

# 详细

[^1]: 抓住它们：图匹配匹配滤波中的解决方案多样化

    Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])

    [http://arxiv.org/abs/2308.13451](http://arxiv.org/abs/2308.13451)

    本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。

    

    我们提出了一种在非常大的背景图中查找多个嵌入在其中的模板图的新方法。我们的方法基于Sussman等人提出的图匹配匹配滤波技术，通过在匹配滤波算法中迭代地惩罚合适的节点对相似度矩阵来实现多样化匹配的发现。此外，我们提出了算法加速，极大地提高了我们的匹配滤波方法的可扩展性。我们在相关的Erdos-Renyi图设置中对我们的方法进行了理论上的验证，显示其在温和的模型条件下能够顺序地发现多个模板。我们还通过使用模拟模型和真实世界数据集（包括人脑连接组和大型交易知识库）进行了大量实验证明了我们方法的实用性。

    We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
    
[^2]: 线性化神经网络的六个讲座

    Six Lectures on Linearized Neural Networks. (arXiv:2308.13431v1 [stat.ML])

    [http://arxiv.org/abs/2308.13431](http://arxiv.org/abs/2308.13431)

    这篇论文通过分析线性模型，探讨了多层神经网络行为的学习。研究使用了线性回归、核岭回归、随机特征模型和神经切线模型等四个线性化神经网络模型。研究还讨论了线性理论的局限性和其他方法如何克服这些局限性。

    

    在这六个讲座中，我们从线性模型的分析中探讨了对多层神经网络行为的学习。我们首先回顾了通过所谓的懒惰模式将神经网络与线性模型相对应的情况。然后，我们回顾了线性化神经网络的四个模型：带有集中特征的线性回归，核岭回归，随机特征模型和神经切线模型。最后，我们强调了线性理论的局限性，并讨论了其他方法如何克服这些局限性。

    In these six lectures, we examine what can be learnt about the behavior of multi-layer neural networks from the analysis of linear models. We first recall the correspondence between neural networks and linear models via the so-called lazy regime. We then review four models for linearized neural networks: linear regression with concentrated features, kernel ridge regression, random feature model and neural tangent model. Finally, we highlight the limitations of the linear theory and discuss how other approaches can overcome them.
    
[^3]: 基于初始CT扫描的深度学习模型，提高了颅内动脉瘤出血患者的死亡率预测能力

    Enhanced Mortality Prediction In Patients With Subarachnoid Haemorrhage Using A Deep Learning Model Based On The Initial CT Scan. (arXiv:2308.13373v1 [cs.CV])

    [http://arxiv.org/abs/2308.13373](http://arxiv.org/abs/2308.13373)

    这项研究使用深度学习模型基于初始CT扫描，提高了对颅内动脉瘤出血患者的死亡率预测能力。

    

    目的：颅内动脉瘤出血（SAH）具有较高的发病率和死亡率。卷积神经网络（CNN）是一种深度学习模型，能够从影像数据中生成非常准确的预测。本研究旨在通过对初始CT扫描进行CNN算法处理，预测SAH患者的死亡率。

    PURPOSE: Subarachnoid hemorrhage (SAH) entails high morbidity and mortality rates. Convolutional neural networks (CNN), a form of deep learning, are capable of generating highly accurate predictions from imaging data. Our objective was to predict mortality in SAH patients by processing the initial CT scan on a CNN based algorithm.  METHODS: Retrospective multicentric study of a consecutive cohort of patients with SAH between 2011-2022. Demographic, clinical and radiological variables were analyzed. Pre-processed baseline CT scan images were used as the input for training a CNN using AUCMEDI Framework. Our model's architecture leverages the DenseNet-121 structure, employing transfer learning principles. The output variable was mortality in the first three months. Performance of the model was evaluated by statistical parameters conventionally used in studies involving artificial intelligence methods.  RESULTS: Images from 219 patients were processed, 175 for training and validation of th
    
[^4]: 深度学习和数据分析中的部分等变性的拓扑模型

    A topological model for partial equivariance in deep learning and data analysis. (arXiv:2308.13357v1 [stat.ML])

    [http://arxiv.org/abs/2308.13357](http://arxiv.org/abs/2308.13357)

    本文提出了一种拓扑模型，用于在神经网络中编码部分等变性，并研究了相应的测量空间和P-GENEO空间的性质。

    

    本文提出了一种拓扑模型，用于在神经网络中编码部分等变性。为此，我们引入了一类称为P-GENEO的运算符，以非扩张的方式改变通过测量表示的数据，并遵循一定集合的变换作用。如果变换作用的集合是一个群，则得到所谓的GENEO。然后，我们研究了受某些自映射作用的测量空间以及这些空间之间的P-GENEO空间。我们在它们上定义了伪度量，并展示了一些结果空间的性质。特别地，我们展示了这些空间具有便利的逼近和凸性质。

    In this article, we propose a topological model to encode partial equivariance in neural networks. To this end, we introduce a class of operators, called P-GENEOs, that change data expressed by measurements, respecting the action of certain sets of transformations, in a non-expansive way. If the set of transformations acting is a group, then we obtain the so-called GENEOs. We then study the spaces of measurements, whose domains are subject to the action of certain self-maps, and the space of P-GENEOs between these spaces. We define pseudo-metrics on them and show some properties of the resulting spaces. In particular, we show how such spaces have convenient approximation and convexity properties.
    
[^5]: 用于物理信息神经网络的贝叶斯推理

    Bayesian Reasoning for Physics Informed Neural Networks. (arXiv:2308.13222v1 [physics.comp-ph])

    [http://arxiv.org/abs/2308.13222](http://arxiv.org/abs/2308.13222)

    本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。

    

    本文提出了一种基于贝叶斯公式的物理信息神经网络（PINN）方法。我们采用了MacKay在Neural Computation（1992年）中提出的贝叶斯神经网络框架。通过拉普拉斯近似法，得到后验密度。对于每个模型（拟合），计算所谓的证据。它是一种分类假设的度量。最优解具有最大的证据值。贝叶斯框架使我们能够控制边界对总损失的影响。事实上，贝叶斯算法通过微调损失组件的相对权重。我们解决了热力学、波动和Burger方程。所得结果与精确解基本一致。所有解都提供了在贝叶斯框架内计算的不确定性。

    Physics informed neural network (PINN) approach in Bayesian formulation is presented. We adopt the Bayesian neural network framework formulated by MacKay (Neural Computation 4 (3) (1992) 448). The posterior densities are obtained from Laplace approximation. For each model (fit), the so-called evidence is computed. It is a measure that classifies the hypothesis. The most optimal solution has the maximal value of the evidence. The Bayesian framework allows us to control the impact of the boundary contribution to the total loss. Indeed, the relative weights of loss components are fine-tuned by the Bayesian algorithm. We solve heat, wave, and Burger's equations. The obtained results are in good agreement with the exact solutions. All solutions are provided with the uncertainties computed within the Bayesian framework.
    
[^6]: 非参数可加值函数：具有可解释性的强化学习方法及其在外科手术恢复中的应用

    Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery. (arXiv:2308.13135v1 [stat.ML])

    [http://arxiv.org/abs/2308.13135](http://arxiv.org/abs/2308.13135)

    该论文提出了一种非参数可加模型，用于估计可解释的值函数，并在强化学习中具有应用价值。该方法能够克服传统模型的线性假设限制，同时提供较强的决策建议解释性。

    

    我们提出了一种非参数可加模型，用于在强化学习中估计可解释的值函数。学习依靠数字表型特征的有效自适应临床干预是医务人员重视的问题。在脊柱手术方面，关于患者运动能力恢复的不同术后恢复建议可能会导致患者恢复程度的显著变化。虽然强化学习在游戏等领域取得了广泛成功，但最近的方法严重依赖于黑盒方法，如神经网络。不幸的是，这些方法阻碍了考察每个特征对于产生最终建议决策的贡献。虽然在经典算法（如最小二乘策略迭代）中可以轻松提供这样的解释，但基本的线性假设阻止了学习特征之间的高阶灵活交互作用。在本文中，我们提出了一种新颖的方法，提供了一种灵活的技术来克服这些限制，并能够得到解释性强的决策建议模型。

    We propose a nonparametric additive model for estimating interpretable value functions in reinforcement learning. Learning effective adaptive clinical interventions that rely on digital phenotyping features is a major for concern medical practitioners. With respect to spine surgery, different post-operative recovery recommendations concerning patient mobilization can lead to significant variation in patient recovery. While reinforcement learning has achieved widespread success in domains such as games, recent methods heavily rely on black-box methods, such neural networks. Unfortunately, these methods hinder the ability of examining the contribution each feature makes in producing the final suggested decision. While such interpretations are easily provided in classical algorithms such as Least Squares Policy Iteration, basic linearity assumptions prevent learning higher-order flexible interactions between features. In this paper, we present a novel method that offers a flexible techniq
    
[^7]: 多元时间序列异常检测: 炫酷算法和有缺陷的评估方法

    Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])

    [http://arxiv.org/abs/2308.13068](http://arxiv.org/abs/2308.13068)

    多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。

    

    多元时间序列（MVTS）的异常检测是一个长期存在且具有挑战性的研究课题，近年来吸引了工业界和学术界的大量研究努力。然而，对文献的仔细研究让我们意识到：1）该领域的社区活跃，但并不像计算机视觉（CV）和自然语言处理（NLP）等其他机器学习领域那样组织有序；2）大多数提出的解决方案使用不合适或存在明显缺陷的评估协议进行评估，缺乏科学基础。其中一个非常流行的协议，即所谓的 \pa 协议，是如此有缺陷，以至于随机猜测可以显示系统地优于迄今为止开发的\emph{所有}算法。在本文中，我们使用更健壮的协议对许多最近的算法进行回顾和评估，并讨论在MVTS异常检测的背景下，一个本来很好的协议可能存在的问题以及如何减轻这些问题。我们还对基准数据集表达了关切。

    Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
    
[^8]: 概率深度学习的量子核混合方法

    Quantum Kernel Mixtures for Probabilistic Deep Learning. (arXiv:2305.18204v1 [cs.LG])

    [http://arxiv.org/abs/2305.18204](http://arxiv.org/abs/2305.18204)

    本文提出了一种量子核混合方法，可以用于表示连续和离散随机变量的联合概率分布。该框架允许构建可微分的模型，适用于密度估计、推理和采样，以及各种机器学习任务，包括生成建模和判别学习。

    

    本文提出了一种新的概率深度学习方法——量子核混合，它是从量子密度矩阵的数学形式中推导出来的。该方法提供了一种简单而有效的机制，用于表示连续和离散随机变量的联合概率分布。该框架允许构建可微分的模型，用于密度估计、推理和采样，从而能够整合到端到端的深度神经模型中。通过这样做，我们提供了一种多功能的边际和联合概率分布表示，可以开发一种可微分的、组合的和可逆的推理过程，涵盖了广泛的机器学习任务，包括密度估计、判别学习和生成建模。我们通过两个示例来说明该框架的广泛适用性：一个图像分类模型，它可以自然地转化为条件生成模型，得益于量子核混合的表示能力。

    This paper presents a novel approach to probabilistic deep learning (PDL), quantum kernel mixtures, derived from the mathematical formalism of quantum density matrices, which provides a simpler yet effective mechanism for representing joint probability distributions of both continuous and discrete random variables. The framework allows for the construction of differentiable models for density estimation, inference, and sampling, enabling integration into end-to-end deep neural models. In doing so, we provide a versatile representation of marginal and joint probability distributions that allows us to develop a differentiable, compositional, and reversible inference procedure that covers a wide range of machine learning tasks, including density estimation, discriminative learning, and generative modeling. We illustrate the broad applicability of the framework with two examples: an image classification model, which can be naturally transformed into a conditional generative model thanks to
    
[^9]: 一种使用确定性目标的黑匣子变分推断：更快，更精确，更黑。

    Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])

    [http://arxiv.org/abs/2304.05527](http://arxiv.org/abs/2304.05527)

    本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。

    

    自动微分变分推断（ADVI）提供了多种现代概率编程语言中快速易用的后验近似方法。然而它的随机优化器缺乏明确的收敛标准，并且需要调整参数。此外，ADVI继承了均值场变分贝叶斯（MFVB）的较差后验不确定性估计。我们引入了“确定性ADVI”（DADVI）来解决这些问题。DADVI用固定的蒙特卡罗近似替换了MFVB的不可解目标，这一技术在随机优化文献中被称为“样本平均近似”（SAA）。通过优化近似但确定的目标，DADVI可以使用现成的二阶优化，而且与标准均值场ADVI不同的是，可以适用于更准确的后验线性响应（LR）协方差估计。与现有的最坏情况理论相反，我们表明，在某些常见的统计问题类别上，DADVI和SAA可以表现得更好。

    Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
    
[^10]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^11]: PDSketch: 集成规划领域编程和学习

    PDSketch: Integrated Planning Domain Programming and Learning. (arXiv:2303.05501v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.05501](http://arxiv.org/abs/2303.05501)

    本文通过PDSketch语言和可训练的神经网络，实现了模型的学习和在线规划，加速了机器人的灵活性和通用性。

    

    本文研究了一种模型学习和在线规划方法，以构建灵活和通用的机器人。具体而言，我们研究了如何利用底层环境转换模型中的局部性和稀疏结构，以提高模型泛化能力、数据效率和运行效率。我们提出了一种新的域定义语言，名为PDSketch。它允许用户灵活地定义转换模型中的高级结构，例如对象和特征的依赖关系，类似于程序员使用TensorFlow或PyTorch指定卷积神经网络的核大小和隐藏维度的方式。转换模型的细节将由可训练的神经网络填充。基于定义的结构和学习参数，PDSketch自动生成与域无关的规划启发式算法，无需额外的训练。衍生的启发式算法加速了对新目标的规划性能。

    This paper studies a model learning and online planning approach towards building flexible and general robots. Specifically, we investigate how to exploit the locality and sparsity structures in the underlying environmental transition model to improve model generalization, data-efficiency, and runtime-efficiency. We present a new domain definition language, named PDSketch. It allows users to flexibly define high-level structures in the transition models, such as object and feature dependencies, in a way similar to how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden dimensions of a convolutional neural network. The details of the transition model will be filled in by trainable neural networks. Based on the defined structures and learned parameters, PDSketch automatically generates domain-independent planning heuristics without additional training. The derived heuristics accelerate the performance-time planning for novel goals.
    
[^12]: 差分隐私扩散模型

    Differentially Private Diffusion Models. (arXiv:2210.09929v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09929](http://arxiv.org/abs/2210.09929)

    本研究提出了一种差分隐私扩散模型(DPDMs)，通过差分隐私训练生成模型，实现对隐私的保护，在图像生成基准测试中表现优越，能够在标准测试中与特定任务的DP-SGD训练的分类器相媲美。

    

    现代机器学习模型依赖于越来越大的训练数据集，然而在涉及隐私的领域，数据通常是有限的。通过差分隐私训练的生成模型可以绕过这一挑战，提供对合成数据的访问。本文在扩散模型的最新成功基础上，引入了差分隐私扩散模型(DPDMs)，使用差分隐私随机梯度下降(DP-SGD)来实现隐私保护。我们研究了DPDM中的参数化和采样算法，并提出了噪声多样性，这是一个针对DM训练的强大改进。我们在图像生成基准测试中验证了我们的新颖DPDMs，并在所有实验证明了最先进的性能。此外，在标准基准测试中，使用DPDM生成的合成数据训练的分类器表现与特定任务的DP-SGD训练的分类器相当，这在以往的研究中没有被证明。

    While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been dem
    
[^13]: 展开的诅咒：系统优化的不同化速率

    The Curse of Unrolling: Rate of Differentiating Through Optimization. (arXiv:2209.13271v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.13271](http://arxiv.org/abs/2209.13271)

    优化问题中的展开求导是机器学习中的一个重要问题，本文对其在梯度下降和 Chebyshev 方法中的二次目标提供了非渐进收敛速率分析，发现我们要确保雅可比矩阵的收敛，就必须面临展开求导的诅咒，即要么选择大的学习率导致快速渐进收敛但算法有较长的初始阶段，要么选择较小的学习率导致即时但较慢的收敛。

    

    计算优化问题解的雅可比矩阵是机器学习中的一个中心问题，在超参数优化、元学习、优化作为一种层以及数据集蒸馏等方面有着广泛应用。展开求导是一种流行的启发式方法，它使用迭代求解器近似求解解，并通过计算路径进行不同化。本文针对梯度下降和Chebyshev方法中二次目标提供了该方法的非渐进收敛速率分析。我们表明，为了确保雅可比矩阵的收敛，我们可以选择1）选择大的学习率，导致快速的渐进收敛，但可能会接受算法具有任意长的初始阶段，或者2）选择较小的学习率，导致即时但较慢的收敛。我们称之为展开的诅咒。最后，我们讨论了与此方法相关的开放问题，例如导出实用的更新规则。

    Computing the Jacobian of the solution of an optimization problem is a central problem in machine learning, with applications in hyperparameter optimization, meta-learning, optimization as a layer, and dataset distillation, to name a few. Unrolled differentiation is a popular heuristic that approximates the solution using an iterative solver and differentiates it through the computational path. This work provides a non-asymptotic convergence-rate analysis of this approach on quadratic objectives for gradient descent and the Chebyshev method. We show that to ensure convergence of the Jacobian, we can either 1) choose a large learning rate leading to a fast asymptotic convergence but accept that the algorithm may have an arbitrarily long burn-in phase or 2) choose a smaller learning rate leading to an immediate but slower convergence. We refer to this phenomenon as the curse of unrolling. Finally, we discuss open problems relative to this approach, such as deriving a practical update rul
    
[^14]: 训练神经网络时的局部弹性动力学。

    Dynamics of Local Elasticity During Training of Neural Nets. (arXiv:2111.01166v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.01166](http://arxiv.org/abs/2111.01166)

    本论文研究了神经网络训练过程中的局部弹性动力学。通过对现有$S_{\rm rel}$定义的全面研究并提出新的定义，我们发现新的定义能更敏锐地检测出权重更新更偏向于在与样本数据同一类别内进行预测变化的特性。

    

    在最近的研究中，我们发现了神经网络训练轨迹在权重空间中的一个属性，即“局部弹性”（表示为$S_{\rm rel}$）。局部弹性试图量化样本数据点对预测结果在其他数据点上的影响传播。在本研究中，我们对已有的$S_{\rm rel}$定义进行了全面研究，并提出了一个新的定义，解决了原始定义在分类设置中的局限性。通过在SVHN，CIFAR-10和CIFAR-100上进行各种最先进的神经网络训练实验，我们展示了我们新的$S_{\rm rel}$定义相比于原始定义更加敏锐地检测出权重更新更偏向于在与样本数据同一类别内进行预测变化的特性。在神经回归实验中，我们展示了原始$S_{\rm rel}$显示出一个2阶段行为--通过初始弹性阶段进行训练。

    In the recent past, a property of neural training trajectories in weight-space had been isolated, that of "local elasticity" (denoted as $S_{\rm rel}$). Local elasticity attempts to quantify the propagation of the influence of a sampled data point on the prediction at another data. In this work, we embark on a comprehensive study of the existing notion of $S_{\rm rel}$ and also propose a new definition that addresses the limitations that we point out for the original definition in the classification setting. On various state-of-the-art neural network training on SVHN, CIFAR-10 and CIFAR-100 we demonstrate how our new proposal of $S_{\rm rel}$, as opposed to the original definition, much more sharply detects the property of the weight updates preferring to make prediction changes within the same class as the sampled data.  In neural regression experiments we demonstrate that the original $S_{\rm rel}$ reveals a $2-$phase behavior -- that the training proceeds via an initial elastic phas
    
[^15]: 带有递减半径的块主导极小化方法用于约束非凸优化

    Block majorization-minimization with diminishing radius for constrained nonconvex optimization. (arXiv:2012.03503v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2012.03503](http://arxiv.org/abs/2012.03503)

    这个论文介绍了一种用于非凸约束优化问题的块主导极小化算法，并证明了在使用强凸替代函数的情况下，该算法可以在一定迭代次数内收敛到一个稳定点，并提出了一个信任域变体，可以处理只具备凸性的替代函数。

    

    块主导极小化（BMM）是一种简单的迭代算法，用于非凸约束优化，在每个块坐标上顺序最小化目标函数的主导替代函数，而其他坐标保持固定。BMM包括一大类优化算法，如块坐标下降及其近端点变体，期望最大化和块投影梯度下降。我们证明，对于一般约束非凸优化，使用强凸替代函数的BMM可以在$O(\epsilon^{-2}(\log \epsilon^{-1})^{2})$次迭代中产生一个$\epsilon$-稳定点，并渐近收敛于稳定点集。此外，我们提出了一种信任域变体的BMM，可以处理只具备凸性的替代函数，并仍然获得相同的迭代复杂度和渐近稳定性。这些结果具有鲁棒性，即使凸子问题的求解是非精确的，只要最优间隙满足要求。

    Block majorization-minimization (BMM) is a simple iterative algorithm for nonconvex constrained optimization that sequentially minimizes majorizing surrogates of the objective function in each block coordinate while the other coordinates are held fixed. BMM entails a large class of optimization algorithms such as block coordinate descent and its proximal-point variant, expectation-minimization, and block projected gradient descent. We establish that for general constrained nonconvex optimization, BMM with strongly convex surrogates can produce an $\epsilon$-stationary point within $O(\epsilon^{-2}(\log \epsilon^{-1})^{2})$ iterations and asymptotically converges to the set of stationary points. Furthermore, we propose a trust-region variant of BMM that can handle surrogates that are only convex and still obtain the same iteration complexity and asymptotic stationarity. These results hold robustly even when the convex sub-problems are inexactly solved as long as the optimality gaps are 
    
[^16]: 关于主成分回归模型识别和样本外预测的研究: 应用于合成对照研究

    On Model Identification and Out-of-Sample Prediction of Principal Component Regression: Applications to Synthetic Controls. (arXiv:2010.14449v5 [math.ST] UPDATED)

    [http://arxiv.org/abs/2010.14449](http://arxiv.org/abs/2010.14449)

    该论文在高维度的误差变量固定设计环境中分析了主成分回归模型识别和样本外预测问题，并提出了优于已知最佳速率的非渐进预测保证。在分析过程中，引入了一种自然的线性代数条件来避免样本外预测的分布假设，并构建了一个假设检验来检查该条件在实践中的可行性。同时，该论文的结果也对合成对照研究提供了新的发现。

    

    我们在高维度的误差变量固定设计环境中分析了主成分回归(PCR)。在适当的条件下，我们证明了PCR能够一致地识别出具有最小L2范数的唯一模型。这些结果使我们能够建立起优于已知最佳速率的非渐进的样本外预测保证。在我们的分析过程中，我们引入了一种自然的线性代数条件，联系了样本内和样本外的协变量，从而避免了对样本外预测的分布假设。我们的模拟实验证明了此条件在泛化方面的重要性，即使在协变量漂移的情况下也是如此。因此，我们构建了一个假设检验来检查这个条件在实践中是否成立。此外，我们的结果还为合成对照文献提供了新的发现，合成对照是一种主要的政策评估方法。据我们所知，我们在固定设计环境中的预测保证尚未被研究过。

    We analyze principal component regression (PCR) in a high-dimensional error-in-variables setting with fixed design. Under suitable conditions, we show that PCR consistently identifies the unique model with minimum $\ell_2$-norm. These results enable us to establish non-asymptotic out-of-sample prediction guarantees that improve upon the best known rates. In the course of our analysis, we introduce a natural linear algebraic condition between the in- and out-of-sample covariates, which allows us to avoid distributional assumptions for out-of-sample predictions. Our simulations illustrate the importance of this condition for generalization, even under covariate shifts. Accordingly, we construct a hypothesis test to check when this conditions holds in practice. As a byproduct, our results also lead to novel results for the synthetic controls literature, a leading approach for policy evaluation. To the best of our knowledge, our prediction guarantees for the fixed design setting have been 
    
[^17]: 随机最小二乘值迭代的频率后悔界限

    Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.00567](http://arxiv.org/abs/1911.00567)

    本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。

    

    我们考虑有限时间域强化学习中的探索-利用困境。当状态空间很大或连续时，传统的表格方法不可行，必须采用函数逼近的形式。在本文中，我们引入了一种乐观初始化的改进版本的随机最小二乘值迭代（RLSVI）算法，该算法是一种无模型算法，其中探索是通过扰动行动值函数的最小二乘逼近来诱导的。在假设马尔可夫决策过程具有低秩转移动态的情况下，我们证明了RLSVI的频率后悔将上界为$\widetilde O(d^2 H^2 \sqrt{T})$，其中$ d $是特征维度，$ H $是时间限制，$ T $是总步数。据我们所知，这是对于带有函数逼近的随机探索的第一个频率后悔分析。

    We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.
    

