# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SQ Lower Bounds for Learning Bounded Covariance GMMs.](http://arxiv.org/abs/2306.13057) | 本文研究了在有界协方差情况下学习分离高斯混合模型问题的复杂性，并证明了使用SQ算法的复杂度下限为$d^{\Omega(1/\epsilon)}$。 |
| [^2] | [Inferring the finest pattern of mutual independence from data.](http://arxiv.org/abs/2306.12984) | 通过引入二分独立性，利用i.i.d.正态分布数据估计随机变量最精细的互相独立模式，并在模拟数据和实验数据上进行测试。 |
| [^3] | [Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model.](http://arxiv.org/abs/2306.12968) | 本论文提出了一种算法，名为实例自适应聚类（IAC），它能够在标记随机块模型（LSBM）中恢复隐藏的群集。IAC包括一次谱聚类和一个迭代的基于似然的簇分配改进，不需要任何模型参数，是高效的。 |
| [^4] | [On the use of the Gram matrix for multivariate functional principal components analysis.](http://arxiv.org/abs/2306.12949) | 本文提出使用内积来估计多元和多维函数数据集的特征向量，为函数主成分分析提供了新的有效方法。 |
| [^5] | [Evolving Computation Graphs.](http://arxiv.org/abs/2306.12943) | 本论文提出了一种进化计算图算法（ECGs），用于提高针对异质性数据的图神经网络（GNN）性能，通过重连GNN的计算图增加连接同一类节点的边缘以提升性能。 |
| [^6] | [AudioPaLM: A Large Language Model That Can Speak and Listen.](http://arxiv.org/abs/2306.12925) | AudioPaLM是一款能够更好地处理语音任务的大型语言模型，它继承了AudioLM的语音身份和语调信息保留能力以及PaLM-2的语言知识，可用于语音识别、语音翻译等应用。 |
| [^7] | [Mitigating Discrimination in Insurance with Wasserstein Barycenters.](http://arxiv.org/abs/2306.12912) | 本研究针对保险业依赖个人敏感特征预测风险容易造成歧视的问题，提出使用Wasserstein重心缓解偏见的方法。 |
| [^8] | [Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement.](http://arxiv.org/abs/2306.12803) | 本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。 |
| [^9] | [Fitted Value Iteration Methods for Bicausal Optimal Transport.](http://arxiv.org/abs/2306.12658) | 本文提出了一种适用于双因果最优传输问题的拟合值迭代方法，能够在保证精度的同时具有良好的可扩展性，数值实验结果也证明了该方法的优越性。 |
| [^10] | [Targeted collapse regularized autoencoder for anomaly detection: black hole at the center.](http://arxiv.org/abs/2306.12627) | 本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。 |
| [^11] | [Communication-Efficient Federated Learning through Importance Sampling.](http://arxiv.org/abs/2306.12625) | 本文提出了一种通过重要性抽样实现有效通信的联邦学习方法，大大降低了发送模型更新的高通信成本，利用服务器端客户端分布和附加信息的接近关系，只需要较少的通信量即可实现。 |
| [^12] | [Hierarchical Neural Simulation-Based Inference Over Event Ensembles.](http://arxiv.org/abs/2306.12584) | 本文介绍了一种基于层级神经模拟的方法，可以在似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断，着重考虑了模型的层级结构，可以导致更紧凑的参数约束。 |
| [^13] | [Adversarial Training with Generated Data in High-Dimensional Regression: An Asymptotic Study.](http://arxiv.org/abs/2306.12582) | 该论文研究了在高维回归中将生成数据与对抗训练相结合的方法，发现该方法可通过两阶段训练实现更好的性能表现。 |
| [^14] | [Finite-time Lyapunov exponents of deep neural networks.](http://arxiv.org/abs/2306.12548) | 本文研究了深度神经网络的有限时间李雅普诺夫指数，发现正指数的脊线将输入空间分成不同区域，并揭示了深度网络学习能力的机制。 |
| [^15] | [Memory-Query Tradeoffs for Randomized Convex Optimization.](http://arxiv.org/abs/2306.12534) | 随机凸优化需要在内存和查询之间权衡，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法是最优的。 |
| [^16] | [Structured Learning in Time-dependent Cox Models.](http://arxiv.org/abs/2306.12528) | 本文提出了一个灵活的框架，用于在时间相关的Cox模型中进行变量选择，并适应各种复杂的分组结构，具有较低的误警率和快速的计算。 |
| [^17] | [Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms.](http://arxiv.org/abs/2306.12383) | 本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。 |
| [^18] | [Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion.](http://arxiv.org/abs/2306.02957) | 本研究探讨了离散扩散核如何影响图的扩散模型的性能，结果表明选择正确的收敛先验对于扩散模型的生成性能至关重要。 |
| [^19] | [Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification.](http://arxiv.org/abs/2306.01726) | 本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。 |
| [^20] | [Balanced Training of Energy-Based Models with Adaptive Flow Sampling.](http://arxiv.org/abs/2306.00684) | 本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。 |
| [^21] | [Classification Tree Pruning Under Covariate Shift.](http://arxiv.org/abs/2305.04335) | 本文提出了一种基于协变量转移的分类树剪枝方法，可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。使用的优化标准是一个关于分布 $P_{X} \to Q_{X}$ 的 \emph{平均差异}，该标准可以显著放宽最近提出的 \emph{转移指数}，最终可以得到最优的剪枝结果。 |
| [^22] | [Sharp analysis of EM for learning mixtures of pairwise differences.](http://arxiv.org/abs/2302.10066) | 该论文研究了使用成对比较设计的随机样本的线性回归的对称混合，通过分析EM算法的序列收敛性和极限值，得出了$\ell_\infty$范数和$\ell_2$范数中的估计尖锐度。研究表明EM算法可以展现出多个独特的行为。 |
| [^23] | [A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization.](http://arxiv.org/abs/2302.09766) | 本文提出了两种单时间规模的算法：Prox-DASA和Prox-DASA-GT，它们可以用常量批量大小找到复合目标函数的$\epsilon$-静止点，并且不需要大批量大小、更复杂的操作或更强的假设。 |
| [^24] | [Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation.](http://arxiv.org/abs/2302.03673) | 本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。 |
| [^25] | [scikit-fda: A Python Package for Functional Data Analysis.](http://arxiv.org/abs/2211.02566) | scikit-fda是一个用于函数数据分析的Python包，提供了全面的工具，并于scikit-learn兼容，采用三条款BSD许可证发布，对FDA社区贡献开放。 |
| [^26] | [Convergence of Dirichlet Forms for MCMC Optimal Scaling with Dependent Target Distributions on Large Graphs.](http://arxiv.org/abs/2210.17042) | 本文利用Dirichlet形式的Mosco收敛性分析了在大图上的随机游走Metropolis（RWM）算法，证明了RWM算法的最优比例缩放具有收敛性，将已知的几个结果推广到了大图上的依赖目标分布的情况，并为大图上的MCMC算法开辟了许多新的可能性。 |
| [^27] | [Scrutinizing XAI using linear ground-truth data with suppressor variables.](http://arxiv.org/abs/2111.07473) | 研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。 |

# 详细

[^1]: 用于学习有界协方差高斯混合模型的SQ下限

    SQ Lower Bounds for Learning Bounded Covariance GMMs. (arXiv:2306.13057v1 [cs.LG])

    [http://arxiv.org/abs/2306.13057](http://arxiv.org/abs/2306.13057)

    本文研究了在有界协方差情况下学习分离高斯混合模型问题的复杂性，并证明了使用SQ算法的复杂度下限为$d^{\Omega(1/\epsilon)}$。

    

    本文研究了具有相同未知有界协方差矩阵的分离高斯混合模型的复杂性。 具体来说，我们关注形式为$P = \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$的$\mathbb{R}^d$上的高斯混合模型（GMMs），其中$\mathbf \Sigma_i = \mathbf \Sigma \preceq \mathbf I$且$\min_{i \neq j} \| \boldsymbol \mu_i \boldsymbol \mu_j\|_2 \geq k^\epsilon$对于某些$\epsilon> 0$。已知的学习算法的复杂度为$(dk)^{O(1/\epsilon)}$。在本文中，我们证明了任何用于此问题的统计查询（SQ）算法的复杂度至少需要$d^{\Omega(1/\epsilon)}$。当分离在$k^{1/2}$数量级上时，我们另外获得了具有正确指数的细粒度SQ下限。我们的SQ下限意味着低次多项式测试的类似下限。从概念上讲，我们的结果表明已知算法几乎是最优的。

    We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i = \mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i \boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon>0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly be
    
[^2]: 从数据中推断最精细的互相独立模式

    Inferring the finest pattern of mutual independence from data. (arXiv:2306.12984v1 [stat.ML])

    [http://arxiv.org/abs/2306.12984](http://arxiv.org/abs/2306.12984)

    通过引入二分独立性，利用i.i.d.正态分布数据估计随机变量最精细的互相独立模式，并在模拟数据和实验数据上进行测试。

    

    对于随机变量X，我们对其最精细的互相独立模式μ(X)的盲目提取感兴趣。我们引入了一种特定的独立性，称为二分的独立性。如果Δ(X)代表所有适用于X的二分独立性模式集，则我们证明μ(X)可以作为Δ(X)的所有元素的交集来获得。然后，我们提出了一种方法，在数据独立且服从多元正态分布的条件下，估计Δ(X)。如果^Δ(X)是有效的二分独立模式的估计集，则我们将μ(X)估计为^Δ(X)的所有模式的交集。该方法在模拟数据上进行了测试，表明了其优点和局限性。我们还考虑了一个玩具例子和实验数据的应用。

    For a random variable $X$, we are interested in the blind extraction of its finest mutual independence pattern $\mu ( X )$. We introduce a specific kind of independence that we call dichotomic. If $\Delta ( X )$ stands for the set of all patterns of dichotomic independence that hold for $X$, we show that $\mu ( X )$ can be obtained as the intersection of all elements of $\Delta ( X )$. We then propose a method to estimate $\Delta ( X )$ when the data are independent and identically (i.i.d.) realizations of a multivariate normal distribution. If $\hat{\Delta} ( X )$ is the estimated set of valid patterns of dichotomic independence, we estimate $\mu ( X )$ as the intersection of all patterns of $\hat{\Delta} ( X )$. The method is tested on simulated data, showing its advantages and limits. We also consider an application to a toy example as well as to experimental data.
    
[^3]: 标记随机块模型中的最优簇恢复问题

    Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])

    [http://arxiv.org/abs/2306.12968](http://arxiv.org/abs/2306.12968)

    本论文提出了一种算法，名为实例自适应聚类（IAC），它能够在标记随机块模型（LSBM）中恢复隐藏的群集。IAC包括一次谱聚类和一个迭代的基于似然的簇分配改进，不需要任何模型参数，是高效的。

    

    本文考虑在有限数量的簇的情况下，用标记随机块模型（LSBM）恢复隐藏的社群，其中簇大小随着物品总数$n$的增长而线性增长。在LSBM中，为每对物品（独立地）观测到一个标签。我们的目标是设计一种有效的算法，利用观测到的标签来恢复簇。为此，我们重新审视了关于期望被任何聚类算法误分类的物品数量的实例特定下界。我们提出了实例自适应聚类（IAC），这是第一个在期望和高概率下都能匹配这些下界表现的算法。IAC由一次谱聚类算法和一个迭代的基于似然的簇分配改进组成。这种方法基于实例特定的下界，不需要任何模型参数，包括簇的数量。通过仅执行一次谱聚类，IAC在计算和存储方面都是高效的。

    We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
    
[^4]: 关于使用格拉姆矩阵进行多元函数主成分分析的研究

    On the use of the Gram matrix for multivariate functional principal components analysis. (arXiv:2306.12949v1 [stat.ME])

    [http://arxiv.org/abs/2306.12949](http://arxiv.org/abs/2306.12949)

    本文提出使用内积来估计多元和多维函数数据集的特征向量，为函数主成分分析提供了新的有效方法。

    

    在函数数据分析中，降维是至关重要的。降维的关键工具是函数主成分分析。现有的函数主成分分析方法通常涉及协方差矩阵的对角化。随着函数数据集的规模和复杂性增加，协方差矩阵的估计变得更加具有挑战性。因此，需要有效的方法来估计特征向量。基于观测空间和函数特征空间的对偶性，我们提出使用曲线之间的内积来估计多元和多维函数数据集的特征向量。建立了协方差矩阵特征向量和内积矩阵特征向量之间的关系。我们探讨了这些方法在几个函数数据分析设置中的应用，并提供了它们的通用指导。

    Dimension reduction is crucial in functional data analysis (FDA). The key tool to reduce the dimension of the data is functional principal component analysis. Existing approaches for functional principal component analysis usually involve the diagonalization of the covariance operator. With the increasing size and complexity of functional datasets, estimating the covariance operator has become more challenging. Therefore, there is a growing need for efficient methodologies to estimate the eigencomponents. Using the duality of the space of observations and the space of functional features, we propose to use the inner-product between the curves to estimate the eigenelements of multivariate and multidimensional functional datasets. The relationship between the eigenelements of the covariance operator and those of the inner-product matrix is established. We explore the application of these methodologies in several FDA settings and provide general guidance on their usability.
    
[^5]: 进化计算图算法

    Evolving Computation Graphs. (arXiv:2306.12943v1 [cs.LG])

    [http://arxiv.org/abs/2306.12943](http://arxiv.org/abs/2306.12943)

    本论文提出了一种进化计算图算法（ECGs），用于提高针对异质性数据的图神经网络（GNN）性能，通过重连GNN的计算图增加连接同一类节点的边缘以提升性能。

    

    在关系数据建模方面，图神经网络（GNN）已经展现出了成功，尤其是对于那些表现出同质性的数据：当节点之间的连接往往暗示它们属于同一类时，这一点更为明显。然而，虽然这个假设在许多相关情况下是成立的，但在重要的现实场景中也有违反这个假设的情况，这促进了对GNN在这些情况下进行改进的研究。在这项工作中，我们提出了一种新颖的方法——进化计算图算法（ECGs），用于增强针对异质性数据的GNN。我们的方法建立在先前的理论洞见之上，将节点度、高同质性和内部与类间嵌入相似性联系在一起，通过重连GNN的计算图来增加连接同一类节点的边缘。我们利用较弱的分类器来识别这些边缘，从而最终提高了GNN对非同质性数据的性能。我们评估ECGs在一组多样化的最近提出的异质数据集上的表现。

    Graph neural networks (GNNs) have demonstrated success in modeling relational data, especially for data that exhibits homophily: when a connection between nodes tends to imply that they belong to the same class. However, while this assumption is true in many relevant situations, there are important real-world scenarios that violate this assumption, and this has spurred research into improving GNNs for these cases. In this work, we propose Evolving Computation Graphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Our approach builds on prior theoretical insights linking node degree, high homophily, and inter vs intra-class embedding similarity by rewiring the GNNs' computation graph towards adding edges that connect nodes that are likely to be in the same class. We utilise weaker classifiers to identify these edges, ultimately improving GNN performance on non-homophilic data as a result. We evaluate ECGs on a diverse set of recently-proposed heterophilous datasets a
    
[^6]: AudioPaLM：一款能说会听的大型语言模型

    AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])

    [http://arxiv.org/abs/2306.12925](http://arxiv.org/abs/2306.12925)

    AudioPaLM是一款能够更好地处理语音任务的大型语言模型，它继承了AudioLM的语音身份和语调信息保留能力以及PaLM-2的语言知识，可用于语音识别、语音翻译等应用。

    

    本文介绍了一种用于语音理解和生成的大型语言模型——AudioPaLM。它将基于文本的语言模型PaLM-2[Anil等人，2023]和基于音频的语言模型AudioLM[Borsos等人，2022]结合成一个统一的多模态结构，可以处理和生成文本和语音，包括语音识别和语音翻译等应用。AudioPaLM继承了从AudioLM中保留语音发音者身份和语调等的能力，以及只存在于文本大型语言模型PaLM-2中的语言知识。我们表明，通过使用文本大型语言模型的权重进行初始化，可以改善语音处理，成功地利用了预训练中使用的更大量的文本训练数据来协助语音任务。最终得到的模型在语音翻译任务中明显优于现有系统，并且具有进行零-shot言语文本转换的能力。

    We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for ma
    
[^7]: 使用Wasserstein重心缓解保险中的歧视

    Mitigating Discrimination in Insurance with Wasserstein Barycenters. (arXiv:2306.12912v1 [stat.ML])

    [http://arxiv.org/abs/2306.12912](http://arxiv.org/abs/2306.12912)

    本研究针对保险业依赖个人敏感特征预测风险容易造成歧视的问题，提出使用Wasserstein重心缓解偏见的方法。

    

    保险业严重依赖于根据潜在客户的特征预测风险的模型。虽然这样的模型很常见，但研究人员长期以来一直指出，这种做法会因基于敏感特征（如性别或种族）而产生歧视。鉴于这种歧视通常可以归因于历史数据偏见，因此消除或至少缓解歧视是可取的。随着从更传统的模型转向基于机器学习的预测，对更大的缓解呼声也在增加，因为仅仅排除价格过程中的敏感变量被证明是无效的。在本文中，我们首先研究为什么预测在行业内是必要的，以及为什么纠正偏见并不像简单地识别敏感变量那么直接。然后，我们提出使用Wasserstein重心来缓解偏见，而不是简单地缩放。为了展示这种方法的效果和有效性

    The insurance industry is heavily reliant on predictions of risks based on characteristics of potential customers. Although the use of said models is common, researchers have long pointed out that such practices perpetuate discrimination based on sensitive features such as gender or race. Given that such discrimination can often be attributed to historical data biases, an elimination or at least mitigation is desirable. With the shift from more traditional models to machine-learning based predictions, calls for greater mitigation have grown anew, as simply excluding sensitive variables in the pricing process can be shown to be ineffective. In this article, we first investigate why predictions are a necessity within the industry and why correcting biases is not as straightforward as simply identifying a sensitive variable. We then propose to ease the biases through the use of Wasserstein barycenters instead of simple scaling. To demonstrate the effects and effectiveness of the approach 
    
[^8]: 具有局部可变测量尺度的随机变量的鲁棒统计比较

    Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement. (arXiv:2306.12803v1 [stat.ML])

    [http://arxiv.org/abs/2306.12803](http://arxiv.org/abs/2306.12803)

    本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。

    

    具有局部可变测量尺度的空间，在统计学和机器学习中是相当普遍的，比如说，具有不同缩放维度的多维结构。然而，如何正确地利用这些空间中编码的全部信息，仍然被认为是一个开放性问题。我们通过考虑一个基于随机变量期望的（集合）偏序关系来解决这个问题，这些随机变量映射到这些非标准空间中。当没有或完全的基数结构时，这个偏序关系包含随机优势和期望顺序作为极端情况。我们通过线性优化导出了一个适用于我们提出的广义随机优势（GSD）顺序的（正则化的）统计检验，并通过不精确概率模型使其更为鲁棒。我们的发现用多维贫困度量、金融和医学数据进行说明。

    Spaces with locally varying scale of measurement, like multidimensional structures with differently scaled dimensions, are pretty common in statistics and machine learning. Nevertheless, it is still understood as an open question how to exploit the entire information encoded in them properly. We address this problem by considering an order based on (sets of) expectations of random variables mapping into such non-standard spaces. This order contains stochastic dominance and expectation order as extreme cases when no, or respectively perfect, cardinal structure is given. We derive a (regularized) statistical test for our proposed generalized stochastic dominance (GSD) order, operationalize it by linear optimization, and robustify it by imprecise probability models. Our findings are illustrated with data from multidimensional poverty measurement, finance, and medicine.
    
[^9]: 拟合值迭代方法求解适应结构双因果最优传输问题

    Fitted Value Iteration Methods for Bicausal Optimal Transport. (arXiv:2306.12658v1 [stat.ML])

    [http://arxiv.org/abs/2306.12658](http://arxiv.org/abs/2306.12658)

    本文提出了一种适用于双因果最优传输问题的拟合值迭代方法，能够在保证精度的同时具有良好的可扩展性，数值实验结果也证明了该方法的优越性。

    

    本文提出一种拟合值迭代方法(FVI)用于计算具有适应结构的双因果最优传输(OT)。基于动态规划的形式化表述，FVI采用函数类用于近似双因果OT中的值函数。在可集中条件和近似完备性假设下，我们使用（局部）Rademacher复杂度证明了样本复杂度。此外，我们证明了深度多层神经网络具有适当结构，满足样本复杂度证明所需的关键假设条件。数值实验表明，FVI在时间跨度增加时优于线性规划和适应性Sinkhorn方法，在保持可接受精度的同时具有很好的可扩展性。

    We develop a fitted value iteration (FVI) method to compute bicausal optimal transport (OT) where couplings have an adapted structure. Based on the dynamic programming formulation, FVI adopts a function class to approximate the value functions in bicausal OT. Under the concentrability condition and approximate completeness assumption, we prove the sample complexity using (local) Rademacher complexity. Furthermore, we demonstrate that multilayer neural networks with appropriate structures satisfy the crucial assumptions required in sample complexity proofs. Numerical experiments reveal that FVI outperforms linear programming and adapted Sinkhorn methods in scalability as the time horizon increases, while still maintaining acceptable accuracy.
    
[^10]: 针对异常检测的目标塌缩正则化自编码器：中心的黑洞

    Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])

    [http://arxiv.org/abs/2306.12627](http://arxiv.org/abs/2306.12627)

    本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。

    

    自编码器已广泛用于最近的异常检测技术开发中。它们的应用前提是在正常训练数据上训练自编码器后，异常输入将表现出显著的重构误差。因此，这使得正常和异常样本之间有了明显的区别。然而，在实践中观察到，自编码器可以一定程度上泛化到正常类之外，并在一些异常样本上实现较小的重构误差。为了改善性能，各种技术提出了其他组件和更复杂的训练程序。在这项工作中，我们提出了一个非常简单的替代方法：不是添加神经网络组件、涉及计算和繁琐的训练，而是通过在潜在空间中调节表示的范数，用一个计算简单的项来补充重构损失。我们方法的简单性最小化了复杂性。

    Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
    
[^11]: 通过重要性抽样实现有效通信的联邦学习

    Communication-Efficient Federated Learning through Importance Sampling. (arXiv:2306.12625v1 [cs.LG])

    [http://arxiv.org/abs/2306.12625](http://arxiv.org/abs/2306.12625)

    本文提出了一种通过重要性抽样实现有效通信的联邦学习方法，大大降低了发送模型更新的高通信成本，利用服务器端客户端分布和附加信息的接近关系，只需要较少的通信量即可实现。

    

    客户端向服务器发送模型更新的高通信成本是可扩展联邦学习（FL）的重要瓶颈。现有方法中，使用随机压缩方法实现了最先进的比特率-准确性折衷——其中客户端n发送来自仅为该客户端的概率分布qφ（n）的样本，服务器使用这些样本估计客户端分布的平均值。然而，这种方法没有充分利用FL的设置，其中服务器在整个训练过程中具有预数据分布pθ的附加信息，该分布与客户端分布qφ（n）在Kullback-Leibler（KL）发散方面接近。在本文中，我们利用服务器端客户端分布qφ（n)与附加信息pθ之间的这种接近关系，并提出了一种框架，该框架需要大约Dkl（qφ（n）|| pθ）位的通信量。

    The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a pre-data distribution $p_{\theta}$ that is close to the client's distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of com
    
[^12]: 基于层级神经模拟的事件集推断

    Hierarchical Neural Simulation-Based Inference Over Event Ensembles. (arXiv:2306.12584v1 [stat.ML])

    [http://arxiv.org/abs/2306.12584](http://arxiv.org/abs/2306.12584)

    本文介绍了一种基于层级神经模拟的方法，可以在似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断，着重考虑了模型的层级结构，可以导致更紧凑的参数约束。

    

    在实际数据分析中，事件集是常见的观测值集合，它们共同约束了感兴趣的模型参数。这些模型通常具有层级结构，其中“局部”参数影响单个事件，“全局”参数影响整个数据集。我们引入了实用的方法，用于处理似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断。我们构建了似然函数（比）或后验概率的神经估计器，并展示了明确考虑模型层级结构可以导致更紧凑的参数约束。我们以物理科学为例研究了本文讨论的内容，着重于粒子物理学（粒子对撞机数据）和天体物理学（强引力透镜观测）的案例。

    When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where "local" parameters impact individual events and "global" parameters influence the entire dataset. We introduce practical approaches for optimal dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via forward modeling. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics (particle collider data) and astrophysics (strong gravitational lensing observations).
    
[^13]: 生成数据在高维回归中的对抗训练：一项渐近研究

    Adversarial Training with Generated Data in High-Dimensional Regression: An Asymptotic Study. (arXiv:2306.12582v1 [stat.ML])

    [http://arxiv.org/abs/2306.12582](http://arxiv.org/abs/2306.12582)

    该论文研究了在高维回归中将生成数据与对抗训练相结合的方法，发现该方法可通过两阶段训练实现更好的性能表现。

    

    近年来，许多研究（例如\cite{carmon2019unlabeled,gowal2021improving,xing2022artificial}）表明通过两阶段训练方法，在对抗训练中加入带伪标签的额外真实或生成数据可以增强模型性能。本文在高维线性回归模型中对该方法的渐近行为进行了理论分析。我们发现，虽然在无岭训练中存在双峰现象，但在适当的$\mathcal{L}_2$正则化中，两阶段对抗训练可以实现更好的性能。最后，我们导出了一个特别针对两阶段训练方法的快速交叉验证公式。

    In recent years, studies such as \cite{carmon2019unlabeled,gowal2021improving,xing2022artificial} have demonstrated that incorporating additional real or generated data with pseudo-labels can enhance adversarial training through a two-stage training approach. In this paper, we perform a theoretical analysis of the asymptotic behavior of this method in high-dimensional linear regression. While a double-descent phenomenon can be observed in ridgeless training, with an appropriate $\mathcal{L}_2$ regularization, the two-stage adversarial training achieves a better performance. Finally, we derive a shortcut cross-validation formula specifically tailored for the two-stage training method.
    
[^14]: 深度神经网络的有限时间李雅普诺夫指数

    Finite-time Lyapunov exponents of deep neural networks. (arXiv:2306.12548v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2306.12548](http://arxiv.org/abs/2306.12548)

    本文研究了深度神经网络的有限时间李雅普诺夫指数，发现正指数的脊线将输入空间分成不同区域，并揭示了深度网络学习能力的机制。

    

    我们计算了小的输入扰动如何影响深度神经网络的输出，探索深度网络与动力系统之间的类比，其中局部扰动的增长或衰减由有限时间李雅普诺夫指数来描述。我们显示最大指数在输入空间中形成几何结构，类似于动力系统中的相干结构。大正指数的脊线将输入空间分成网络将其与不同类别相关联的不同区域。这些脊线可视化深度网络在输入空间中构建的几何形状，揭示了其学习能力背后的基本机制。

    We compute how small input perturbations affect the output of deep neural networks, exploring an analogy between deep networks and dynamical systems, where the growth or decay of local perturbations is characterised by finite-time Lyapunov exponents. We show that the maximal exponent forms geometrical structures in input space, akin to coherent structures in dynamical systems. Ridges of large positive exponents divide input space into different regions that the network associates with different classes. These ridges visualise the geometry that deep networks construct in input space, shedding light on the fundamental mechanisms underlying their learning capabilities.
    
[^15]: 随机凸优化的内存和查询权衡

    Memory-Query Tradeoffs for Randomized Convex Optimization. (arXiv:2306.12534v1 [cs.DS])

    [http://arxiv.org/abs/2306.12534](http://arxiv.org/abs/2306.12534)

    随机凸优化需要在内存和查询之间权衡，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法是最优的。

    

    我们证明了任何随机一阶算法，用于在单位球上最小化一个 $d$ 维、$1$-Lipschitz 凸函数，必须使用 $\Omega(d^{2-\delta})$ 比特的内存或者进行 $\Omega(d^{1+\delta/6-o(1)})$ 次查询，其中 $\delta\in (0,1)$ 是任意常数，精度 $\epsilon$ 在 $d$ 中是准多项式小的。我们的结果意味着，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法，在随机一阶算法中是帕累托最优，而对于凸优化，需要二次内存才能实现最佳查询复杂度。

    We show that any randomized first-order algorithm which minimizes a $d$-dimensional, $1$-Lipschitz convex function over the unit ball must either use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$ queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$ is quasipolynomially small in $d$. Our result implies that cutting plane methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries, are Pareto-optimal among randomized first-order algorithms, and quadratic memory is required to achieve optimal query complexity for convex optimization.
    
[^16]: 时间相关的Cox模型中的结构化学习

    Structured Learning in Time-dependent Cox Models. (arXiv:2306.12528v1 [stat.ME])

    [http://arxiv.org/abs/2306.12528](http://arxiv.org/abs/2306.12528)

    本文提出了一个灵活的框架，用于在时间相关的Cox模型中进行变量选择，并适应各种复杂的分组结构，具有较低的误警率和快速的计算。

    

    具有时间相关系数和协变量的Cox模型在生存分析中得到广泛应用。在高维环境中，采用稀疏正则化技术进行变量选择，但现有的时间相关的Cox模型方法缺乏在强制特定稀疏模式（即协变量结构）方面的灵活性。我们提出了一个灵活的框架，用于在时间相关的Cox模型中进行变量选择，可适应复杂的选择规则。我们的方法可以适应任意分组结构，包括交互选择，时间性，空间性，树和有向无环图结构。它可以通过降低误警率实现准确的估计。我们开发了sox软件包，实现了一种网络流算法，用于高效地解决具有复杂协变量结构的模型。Sox提供了一个用户友好的接口，用于指定分组结构，并提供快速的计算。通过案例研究，包括一个用于确定所有致死时间预测因素的案例研究。

    Cox models with time-dependent coefficients and covariates are widely used in survival analysis. In high-dimensional settings, sparse regularization techniques are employed for variable selection, but existing methods for time-dependent Cox models lack flexibility in enforcing specific sparsity patterns (i.e., covariate structures). We propose a flexible framework for variable selection in time-dependent Cox models, accommodating complex selection rules. Our method can adapt to arbitrary grouping structures, including interaction selection, temporal, spatial, tree, and directed acyclic graph structures. It achieves accurate estimation with low false alarm rates. We develop the sox package, implementing a network flow algorithm for efficiently solving models with complex covariate structures. Sox offers a user-friendly interface for specifying grouping structures and delivers fast computation. Through examples, including a case study on identifying predictors of time to all-cause death 
    
[^17]: 二次型赌臂机的样本复杂度：Hessian相关性界限和最优算法

    Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])

    [http://arxiv.org/abs/2306.12383](http://arxiv.org/abs/2306.12383)

    本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。

    

    在随机零阶优化中，了解如何充分利用底层目标函数的局部几何结构是一个实际相关的问题。我们考虑一种基本情况，即目标函数是二次型的，并且提供了最优Hessian相关样本复杂度的第一个紧密刻画。我们的贡献具有双重性质。首先，从信息论的角度出发，通过引入一种称为能量分配的概念来捕捉搜索算法和目标函数几何结构之间的交互，证明了Hessian相关复杂度的紧密下界。通过解决最优能量谱，得到了配套的上限。其次，算法方面，我们展示了存在一种Hessian无关的算法，能够普遍实现所有Hessian实例的渐近最优样本复杂度。我们算法能够实现的渐近最优样本复杂度对于重尾噪声分布仍然有效。

    In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
    
[^18]: 离散图扩散中不同收敛先验的复杂偏好

    Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion. (arXiv:2306.02957v1 [cs.LG])

    [http://arxiv.org/abs/2306.02957](http://arxiv.org/abs/2306.02957)

    本研究探讨了离散扩散核如何影响图的扩散模型的性能，结果表明选择正确的收敛先验对于扩散模型的生成性能至关重要。

    

    扩散模型已经取得了在生成许多不同类型的数据，包括图像、文本和视频方面的最先进表现。尽管它们很成功，但对于基础扩散过程和最终收敛先验如何影响生成的性能进行的研究有限；此研究也仅限于连续数据类型和基于分数的扩散框架。我们探讨了不同离散扩散核（收敛到不同的先验分布）如何影响图的扩散模型的性能。为此，我们开发了一种新的离散扩散核系列公式，可以轻松调整以收敛到不同的伯努利先验，并研究这些不同的核对生成性能的影响。我们表明，生成的图的质量对使用的先验很敏感，最优选择不能用明显的统计数据或指标来解释，这挑战了扩散模型的直觉假设。我们的结果表明，在离散数据上，选择正确的收敛先验对于扩散模型的生成性能至关重要。

    Diffusion models have achieved state-of-the-art performance in generating many different kinds of data, including images, text, and videos. Despite their success, there has been limited research on how the underlying diffusion process and the final convergent prior can affect generative performance; this research has also been limited to continuous data types and a score-based diffusion framework. To fill this gap, we explore how different discrete diffusion kernels (which converge to different prior distributions) affect the performance of diffusion models for graphs. To this end, we developed a novel formulation of a family of discrete diffusion kernels which are easily adjustable to converge to different Bernoulli priors, and we study the effect of these different kernels on generative performance. We show that the quality of generated graphs is sensitive to the prior used, and that the optimal choice cannot be explained by obvious statistics or metrics, which challenges the intuiti
    
[^19]: 评估有噪声判别器对未标记数据的流式算法 -- 二元分类

    Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])

    [http://arxiv.org/abs/2306.01726](http://arxiv.org/abs/2306.01726)

    本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。

    

    本文将对未标记数据中的有噪声二元分类器的评估作为流式任务进行研究: 给定一个分类器决策的数据草图，估计标签的真实流行度以及每个分类器对它们的准确度。本文构建了两种完全代数化的评估器来实现这一目标。两种评估器都基于分类器产生独立错误的假设。第一种是基于多数投票的。而第二种则是本文的主要贡献，并被保证是正确的。但是如何确保分类器在任何给定的测试中是独立的呢？本文通过利用独立评估器无法返回合理估计的失败来缓解这个委托/代理监控悖论。通过利用代数故障模式来拒绝太相关的评估集合，使用 \texttt{adult}，\texttt{mushroom} 和 \texttt{two-norm} 数据集对一组几乎无误差三元组进行了实证搜索。这些搜索通过构建评估空间中的表面来进行精细化。

    The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
    
[^20]: 使用自适应流采样平衡训练能量基模型

    Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])

    [http://arxiv.org/abs/2306.00684](http://arxiv.org/abs/2306.00684)

    本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。

    

    能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。

    Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
    
[^21]: 基于协变量转移的分类树剪枝

    Classification Tree Pruning Under Covariate Shift. (arXiv:2305.04335v1 [stat.ML])

    [http://arxiv.org/abs/2305.04335](http://arxiv.org/abs/2305.04335)

    本文提出了一种基于协变量转移的分类树剪枝方法，可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。使用的优化标准是一个关于分布 $P_{X} \to Q_{X}$ 的 \emph{平均差异}，该标准可以显著放宽最近提出的 \emph{转移指数}，最终可以得到最优的剪枝结果。

    

    本文考虑在训练数据不均匀的情况下，选择适当的子树以平衡偏差和方差的分类树剪枝问题。我们提出了一种针对这种情况的最优剪枝的高效程序，该程序可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。在基本交叉验证和其他进行惩罚的变体，如基于信息度量的剪枝方法非常不理想的情况下，我们提供了一种最优剪枝的方法。使用的优化标准是一个关于分布 $P_{X} \to Q_{X}$ 的 \emph{平均差异}（在 $X$ 空间上平均），该标准可以显著放宽最近提出的 \emph{转移指数} 这一统计学概念，该概念被证明能够紧密地捕捉这种分布转移情况下分类的极限限制。我们放宽的标准可以被看作是分布之间的\emph{相对维度}度量，因为它涉及到信息的现有度量概念，例如闵可夫斯基和Rényi维度。

    We consider the problem of \emph{pruning} a classification tree, that is, selecting a suitable subtree that balances bias and variance, in common situations with inhomogeneous training data. Namely, assuming access to mostly data from a distribution $P_{X, Y}$, but little data from a desired distribution $Q_{X, Y}$ with different $X$-marginals, we present the first efficient procedure for optimal pruning in such situations, when cross-validation and other penalized variants are grossly inadequate. Optimality is derived with respect to a notion of \emph{average discrepancy} $P_{X} \to Q_{X}$ (averaged over $X$ space) which significantly relaxes a recent notion -- termed \emph{transfer-exponent} -- shown to tightly capture the limits of classification under such a distribution shift. Our relaxed notion can be viewed as a measure of \emph{relative dimension} between distributions, as it relates to existing notions of information such as the Minkowski and Renyi dimensions.
    
[^22]: 学习成对差分混合的EM算法的尖锐分析。

    Sharp analysis of EM for learning mixtures of pairwise differences. (arXiv:2302.10066v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2302.10066](http://arxiv.org/abs/2302.10066)

    该论文研究了使用成对比较设计的随机样本的线性回归的对称混合，通过分析EM算法的序列收敛性和极限值，得出了$\ell_\infty$范数和$\ell_2$范数中的估计尖锐度。研究表明EM算法可以展现出多个独特的行为。

    

    我们考虑使用成对比较设计的随机样本的线性回归的对称混合，这可以看作是一种欧几里得距离几何问题的噪声版本。我们在真实值周围局部分析期望最大化（EM）算法，并建立起它的序列线性收敛性，从而为迭代估计误差提供一个$\ell_\infty$-范数保证。此外，我们表明，EM序列的极限实现了在$\ell_2$-范数中的估计尖锐度，匹配信息理论上最优的常数。我们还通过模拟论证了在这种情况下从随机初始化收敛的问题更为微妙，通常不会发生。我们的结果表明，当协变量分布被适当地结构化时，EM算法可以表现出几个独特的行为。

    We consider a symmetric mixture of linear regressions with random samples from the pairwise comparison design, which can be seen as a noisy version of a type of Euclidean distance geometry problem. We analyze the expectation-maximization (EM) algorithm locally around the ground truth and establish that the sequence converges linearly, providing an $\ell_\infty$-norm guarantee on the estimation error of the iterates. Furthermore, we show that the limit of the EM sequence achieves the sharp rate of estimation in the $\ell_2$-norm, matching the information-theoretically optimal constant. We also argue through simulation that convergence from a random initialization is much more delicate in this setting, and does not appear to occur in general. Our results show that the EM algorithm can exhibit several unique behaviors when the covariate distribution is suitably structured.
    
[^23]: 一种单样本去中心化近端算法用于非凸随机复合优化

    A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization. (arXiv:2302.09766v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.09766](http://arxiv.org/abs/2302.09766)

    本文提出了两种单时间规模的算法：Prox-DASA和Prox-DASA-GT，它们可以用常量批量大小找到复合目标函数的$\epsilon$-静止点，并且不需要大批量大小、更复杂的操作或更强的假设。

    

    本文研究了去中心化随机非凸优化问题，其中$n$个代理共同优化由光滑项和非光滑凸项相加的复合目标函数。为了解决这个问题，我们提出了两个单时间规模算法：Prox-DASA和Prox-DASA-GT。这些算法可以使用常量批量大小（即$\mathcal{O}(1)$）在$\mathcal{O}(n^{-1}\epsilon^{-2})$次迭代中找到$\epsilon$-静止点。与以前的工作不同，我们的算法在不需要大批量大小、更复杂的每次迭代操作（如双重循环）或更强的假设的情况下实现了可比拟的复杂度。我们的理论发现得到了广泛的数值实验支持，这些实验证明了我们的算法优于以前的方法。我们的代码可在https://github.com/xuxingc/ProxDASA找到。

    We focus on decentralized stochastic non-convex optimization, where $n$ agents work together to optimize a composite objective function which is a sum of a smooth term and a non-smooth convex term. To solve this problem, we propose two single-time scale algorithms: Prox-DASA and Prox-DASA-GT. These algorithms can find $\epsilon$-stationary points in $\mathcal{O}(n^{-1}\epsilon^{-2})$ iterations using constant batch sizes (i.e., $\mathcal{O}(1)$). Unlike prior work, our algorithms achieve comparable complexity without requiring large batch sizes, more complex per-iteration operations (such as double loops), or stronger assumptions. Our theoretical findings are supported by extensive numerical experiments, which demonstrate the superiority of our algorithms over previous approaches. Our code is available at https://github.com/xuxingc/ProxDASA.
    
[^24]: 在大状态空间中打破多智体的诅咒：带独立线性函数逼近的Markov博弈中的强化学习

    Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03673](http://arxiv.org/abs/2302.03673)

    本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。

    

    我们提出了一种新的模型——独立线性Markov博弈，用于具有大状态空间和大量代理的多智体强化学习。这是一类带有独立线性函数逼近的Markov博弈，每个代理都有自己的函数逼近，用于被其他玩家的策略边缘化的状态-动作值函数。我们设计了学习Markov粗略相关均衡和Markov相关均衡的新算法，并提供了样本复杂度界限，这些界限仅与每个代理自己的函数类复杂度成多项式比例，从而打破了多智体的诅咒。相比之下，现有的用于函数逼近的Markov博弈的研究，在特化于标准表格状况的Markov博弈设置时，其样本复杂度界限会随着\emph{联合行动空间}的大小成指数级增长，而该联合行动空间在代理的数量上呈指数级增长。我们的算法依赖于两个关键的技术创新：(1) 利用策略重放来降低样本复杂度；(2) 利用独立线性函数逼近来获得计算上的有效性和统计上的高精度。

    We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac
    
[^25]: scikit-fda：用于函数数据分析的Python包

    scikit-fda: A Python Package for Functional Data Analysis. (arXiv:2211.02566v2 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2211.02566](http://arxiv.org/abs/2211.02566)

    scikit-fda是一个用于函数数据分析的Python包，提供了全面的工具，并于scikit-learn兼容，采用三条款BSD许可证发布，对FDA社区贡献开放。

    

    库scikit-fda是用于函数数据分析（FDA）的Python软件包。它提供了一套全面的工具，用于函数数据的表示、预处理和探索性分析。该库建立在Python科学计算生态系统之上，特别是采用了scikit-learn应用程序接口，以利用该软件包提供的机器学习功能：包括管道、模型选择和超参数调整等。这个scikit-fda软件包已经以三条款BSD许可证的形式发布为自由和开源软件，并对FDA社区的贡献持开放态度。该库的广泛文档包括逐步教程和详细的使用示例。

    The library scikit-fda is a Python package for Functional Data Analysis (FDA). It provides a comprehensive set of tools for representation, preprocessing, and exploratory analysis of functional data. The library is built upon and integrated in Python's scientific ecosystem. In particular, it conforms to the scikit-learn application programming interface so as to take advantage of the functionality for machine learning provided by this package: pipelines, model selection, and hyperparameter tuning, among others. The scikit-fda package has been released as free and open-source software under a 3-Clause BSD license and is open to contributions from the FDA community. The library's extensive documentation includes step-by-step tutorials and detailed examples of use.
    
[^26]: 依赖于大图的MCMC最优比例缩放的Dirichlet形式的收敛性

    Convergence of Dirichlet Forms for MCMC Optimal Scaling with Dependent Target Distributions on Large Graphs. (arXiv:2210.17042v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2210.17042](http://arxiv.org/abs/2210.17042)

    本文利用Dirichlet形式的Mosco收敛性分析了在大图上的随机游走Metropolis（RWM）算法，证明了RWM算法的最优比例缩放具有收敛性，将已知的几个结果推广到了大图上的依赖目标分布的情况，并为大图上的MCMC算法开辟了许多新的可能性。

    

    Markov Chain Monte Carlo (MCMC)算法在统计学、物理学、机器学习等方面发挥了重要作用，并且对于一些高维问题，它们是唯一已知的通用和有效的方法。本文利用Dirichlet形式的Mosco收敛性分析了在大图上的随机游走Metropolis（RWM）算法，其目标分布是包括任何满足Markov性质的概率测度的Gibbs测度。Dirichlet形式的抽象且强大的理论使我们能够直接和自然地在无限维空间上工作，我们的Mosco收敛性概念允许与RWM链相关联的Dirichlet形式位于变化的图序列上，其中图的大小可以是无界的，图可以是相关的。我们证明了在强空间依赖性存在的情况下，RWM算法的最优比例缩放具有收敛性。我们的结果将已知的几个结果推广到了大图上的依赖目标分布的情况，并为大图上的MCMC算法开辟了许多新的可能性。

    Markov chain Monte Carlo (MCMC) algorithms have played a significant role in statistics, physics, machine learning and others, and they are the only known general and efficient approach for some high-dimensional problems. The random walk Metropolis (RWM) algorithm as the most classical MCMC algorithm, has had a great influence on the development and practice of science and engineering. The behavior of the RWM algorithm in high-dimensional problems is typically investigated through a weak convergence result of diffusion processes. In this paper, we utilize the Mosco convergence of Dirichlet forms in analyzing the RWM algorithm on large graphs, whose target distribution is the Gibbs measure that includes any probability measure satisfying a Markov property. The abstract and powerful theory of Dirichlet forms allows us to work directly and naturally on the infinite-dimensional space, and our notion of Mosco convergence allows Dirichlet forms associated with the RWM chains to lie on changi
    
[^27]: 利用抑制变量的线性基础数据审查可解释人工智能(XAI)

    Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.07473](http://arxiv.org/abs/2111.07473)

    研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。

    

    机器学习(ML)越来越常用于高风险决策中。由于复杂的ML模型(如深度神经网络)通常被认为是黑匣子，因此已经开发出大量程序来阐明其内部运作和预测方式，定义了"可解释人工智能"(XAI)领域。显著性方法根据某种"重要性"度量对输入特征进行排序。由于迄今为止缺乏特征重要性的正式定义，这些方法很难验证。已经证实，一些显著性方法可以突出显示与预测目标没有统计联系的特征(抑制变量)。为了避免由于这种行为引起的错误解读，我们提出了确保此类联系存在是特征重要性的必要条件和客观初步定义。我们精心制作了一个基础数据集，其中所有统计依赖关系都是明确定义的和线性的，

    Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se
    

