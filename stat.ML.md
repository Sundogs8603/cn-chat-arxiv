# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [List Online Classification.](http://arxiv.org/abs/2303.15383) | 本文研究了多标签列表的在线预测问题，提出了 $b$-ary Littlestone 维度可学习模型，并且在懵懂的情况下探索不同的情况。可以使用改编自 Littlestone 的 SOA 和 Rosenblatt 的感知器等算法进行预测，同时还建立了列表可学习的组合结果。 |
| [^2] | [Exploring Continual Learning of Diffusion Models.](http://arxiv.org/abs/2303.15342) | 扩散模型需要进行计算密集型的从头开始训练，因此探索在数据分布发生变化时重新使用计算的迭代训练是有必要的。经验重放与减少排练系数的性能强大。使用每维比特数评估CL存在某些缺陷。 |
| [^3] | [Manifold Learning by Mixture Models of VAEs for Inverse Problems.](http://arxiv.org/abs/2303.15244) | 本文提出了一种用混合VAE模型学习流形的方法，并将其用于解决逆问题，结果表现出良好的性能，可用于模糊和电阻抗层析成像。 |
| [^4] | [Asynchronous Online Federated Learning with Reduced Communication Requirements.](http://arxiv.org/abs/2303.15226) | 提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，能够处理异构和延迟设备，实现参与学习任务的可访问性和效率。 |
| [^5] | [On the Connection between $L_p$ and Risk Consistency and its Implications on Regularized Kernel Methods.](http://arxiv.org/abs/2303.15210) | 本文阐明了$L_p$-一致性和风险一致性之间的密切关系，特别在移位损失函数中发现了新的规律，这对于正则化内核方法具有重要意义。 |
| [^6] | [Learning linear dynamical systems under convex constraints.](http://arxiv.org/abs/2303.15121) | 本文考虑在给定凸约束下学习线性动态系统，通过解出受约束的最小二乘估计，提出新的非渐进误差界，并应用于稀疏矩阵等情境，改进了现有统计方法。 |
| [^7] | [Conjunction Data Messages for Space Collision Behave as a Poisson Process.](http://arxiv.org/abs/2303.15074) | 本研究提出了一个统计学习模型，研究碰撞数据信息到达的泊松过程性质，并能够回答卫星运营商关心的两个问提：（1）下一个指定时间间隔内是否会有新的消息？（2）下一个消息将在何时，并带有怎样的不确定性？ |
| [^8] | [Towards black-box parameter estimation.](http://arxiv.org/abs/2303.15041) | 本文提出了一种基于弱参数结构假设的黑盒程序，用于估计统计模型参数。该程序可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。 |
| [^9] | [Regularized EM algorithm.](http://arxiv.org/abs/2303.14989) | 该论文提出了一种正则化EM算法，用于处理低样本支持情况下GMM的协方差矩阵更新问题，并且可以利用预先提供的先验知识来实现更高效的计算。 |
| [^10] | [Learning Generative Models with Goal-conditioned Reinforcement Learning.](http://arxiv.org/abs/2303.14811) | 研究提出了用目标条件的强化学习来学习生成模型的框架，能够在训练集中生成多样性和高质量的样本。 |
| [^11] | [FAStEN: an efficient adaptive method for feature selection and estimation in high-dimensional functional regressions.](http://arxiv.org/abs/2303.14801) | 提出了一种新的自适应方法FAStEN，用于在高维函数回归问题中执行特征选择和参数估计，通过利用函数主成分和对偶增广Lagrangian问题的稀疏性质，具有显著的计算效率和选择准确性。 |
| [^12] | [On the tightness of information-theoretic bounds on generalization error of learning algorithms.](http://arxiv.org/abs/2303.14658) | 本文研究了学习算法泛化误差信息理论界限的紧密性。研究表明，通过适当的假设，可以在快速收敛速度下使用信息理论量$O(\lambda/n)$来上界估计泛化误差。 |
| [^13] | [Complexity-calibrated Benchmarks for Machine Learning Reveal When Next-Generation Reservoir Computer Predictions Succeed and Mislead.](http://arxiv.org/abs/2303.14553) | 本文讨论了有限历史记忆痕迹在下一代水库计算机中存在的固有限制，并发现在高度非马尔可夫过程中，流行的循环神经网络远远无法实现最佳预测这种复杂过程的目标。这些结果凸显了一种新一代优化循环神经网络的需求。 |
| [^14] | [Learning with Explanation Constraints.](http://arxiv.org/abs/2303.14496) | 本文研究了解释约束下的学习问题，提出了EPAC模型，探讨了使用这些解释时模型的益处，并提供了一种基于变分近似的算法解决方案。 |
| [^15] | [Autoregressive Conditional Neural Processes.](http://arxiv.org/abs/2303.14468) | 本论文提出了自回归条件神经过程模型，能够建模高度相关的非高斯预测分布。 |
| [^16] | [Hybrid Fuzzy-Crisp Clustering Algorithm: Theory and Experiments.](http://arxiv.org/abs/2303.14366) | 本文提出了一种基于目标函数的混合模糊-清晰聚类算法，能够解决传统模糊C均值聚类算法在聚类大小差异巨大时的不平衡影响问题，同时在实验中表现出更好的聚类质量和鲁棒性。 |
| [^17] | [repliclust: Synthetic Data for Cluster Analysis.](http://arxiv.org/abs/2303.14301) | repliclust 是一个 Python 包，用于生成具有聚类的合成数据集，基于数据集的原型，提供了放置集群中心、采样集群形状、选择每个集群的数据点数量以及为集群分配概率分布的算法。 |
| [^18] | [Efficient Lipschitzian Global Optimization of H\"older Continuous Multivariate Functions.](http://arxiv.org/abs/2303.14293) | 本研究提出了一种高效的全局优化技术，通过预定的查询创建规则实现了对Hölder连续多元函数的计算优势，可在给定时间段内获得 minimax 最优的结果。 |
| [^19] | [Applications of Gaussian Processes at Extreme Lengthscales: From Molecules to Black Holes.](http://arxiv.org/abs/2303.14291) | 高斯过程是一种适合拟合少量数据且充分考虑不确定性的模型，可用于从分子到黑洞等多个领域的数据预测和推断。 |
| [^20] | [Feature Space Sketching for Logistic Regression.](http://arxiv.org/abs/2303.14284) | 该论文提出了基于草图的逻辑回归coreset构建、特征选择和降维的新界限，并解决了之前工作中存在的问题，并提出了可扩展到广义线性模型的前向误差界限。 |
| [^21] | [Sequential Knockoffs for Variable Selection in Reinforcement Learning.](http://arxiv.org/abs/2303.14281) | 本论文介绍了一种新颖的序列 Knockoffs (SEEK)算法，用于在强化学习系统中实现变量选择，该算法估计了最小充分状态，确保学习进程良好而不会减缓。 |
| [^22] | [Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing.](http://arxiv.org/abs/2303.14244) | 本论文研究了过参数化低秩矩阵感知问题，证明了通过因子化方法训练的过参数化模型可以收敛，并且隐式平衡和正则化可以促进泛化。 |
| [^23] | [Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions.](http://arxiv.org/abs/2303.14226) | 提出了一种在组合干预下进行因果推断的模型，通过施加潜在结构跨越单位和组合，在降低实验数量和处理混杂问题方面有着良好表现。 |
| [^24] | [Variational Inference for Longitudinal Data Using Normalizing Flows.](http://arxiv.org/abs/2303.14220) | 该文提出一种基于标准化流的高维纵向数据生成模型，能够进行好的缺失数据插补操作。 |
| [^25] | [Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression.](http://arxiv.org/abs/2211.07484) | 该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。 |
| [^26] | [Average-Case Complexity of Tensor Decomposition for Low-Degree Polynomials.](http://arxiv.org/abs/2211.05274) | 该论文研究了低次多项式张量分解问题的平均计算复杂度，发现在$r \ll n^{3/2}$时存在多项式时间算法，但当$r \lesssim n^2$时，该问题只能在原则上恢复秩-1分量，是一个计算困难问题。 |
| [^27] | [A Characterization of List Learnability.](http://arxiv.org/abs/2211.04956) | 本文通过引入$k$-DS维度，完全表征了$k$-列表学习性，并指出当且仅当该假设类的$k$-DS维度有限，该假设类才$k$-列表可学习。 |
| [^28] | [LOT: Layer-wise Orthogonal Training on Improving $\ell_2$ Certified Robustness.](http://arxiv.org/abs/2210.11620) | 本文提出了一种基于层内正交训练的方法(LOT)，通过使用一个无限制矩阵来参数化正交矩阵来有效训练1-Lipschitz卷积层，并证明了半监督训练可以进一步提高利普希茨约束模型的可证明鲁棒性。在确定性l2可证明鲁棒性方面，LOT显著优于基线，并能够扩展到更深的神经网络。 |
| [^29] | [Krylov-Bellman boosting: Super-linear policy evaluation in general state spaces.](http://arxiv.org/abs/2210.11377) | 该论文提出了Krylov-Bellman Boosting (KBB)算法，用于在一般状态空间中进行策略评估，该算法具有超线性的收敛速度。 |
| [^30] | [A New Family of Generalization Bounds Using Samplewise Evaluated CMI.](http://arxiv.org/abs/2210.06422) | 本文提出了一类新的信息论泛化界限，通过逐个样本评估的条件互信息，限制联合凸函数上界，与先前的界限相比在某些情况下更紧密地刻画了深度神经网络的总体损失。 |
| [^31] | [Machine learning and invariant theory.](http://arxiv.org/abs/2209.14991) | 本文介绍了等变机器学习，其中函数将与某个群作用相关，使用不可约表示或不变量理论来参数化这些函数的空间。 Malgrange的一般过程用来表达群$G$作用下所有多项式映射。 |
| [^32] | [Risk-aware linear bandits with convex loss.](http://arxiv.org/abs/2209.07154) | 本论文研究了带上下文的风险感知线性赌博机问题，提出了一种用于估计风险度量的置信序列，并提出了一种乐观的UCB算法以学习最佳的风险感知行为。 |
| [^33] | [clusterBMA: Bayesian model averaging for clustering.](http://arxiv.org/abs/2209.04117) | clusterBMA是一种结合多个非监督聚类算法结果的贝叶斯模型平均法，提供了组合聚类结构的概率解释和模型不确定性的量化。 |
| [^34] | [Optimal Activation Functions for the Random Features Regression Model.](http://arxiv.org/abs/2206.01332) | 本文确定了随机特征回归模型的最佳激活函数。这些函数可能是线性的、饱和线性函数或基于Hermite多项式的函数。使用最佳激活函数会影响RFR模型的重要特性，如双峰曲线和最佳正则化参数与噪声水平的相关性。 |
| [^35] | [An Information-Theoretic Framework for Supervised Learning.](http://arxiv.org/abs/2203.00246) | 本文提出了一种信息论框架，分析机器学习的数据需求，研究了由具有ReLU激活单元的深度神经网络生成的数据的学习样本复杂度，提出了样本复杂度边界。 |
| [^36] | [Optimal Online Generalized Linear Regression with Stochastic Noise and Its Application to Heteroscedastic Bandits.](http://arxiv.org/abs/2202.13603) | 论文提出了一种具有随机噪声的在线广义线性回归问题的最优解，获得了接近最优的遗憾上限。同时，针对具有异方差噪声的广义线性Bandits问题，提出了一种基于FTRL的算法实现第一个方差感知的遗憾上限。 |
| [^37] | [PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX.](http://arxiv.org/abs/2202.04110) | PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。 |
| [^38] | [Anticorrelated Noise Injection for Improved Generalization.](http://arxiv.org/abs/2202.02831) | 本文发现，在一些目标函数中，抗相关噪声的梯度下降方法比传统的梯度下降和常规扰动梯度下降有更好的泛化性能。理论分析证明了这是因为 Anti-PGD 能够移动到更宽的最小值点，而 GD 和 PGD 会停滞在次优区域甚至发散。 |
| [^39] | [Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers.](http://arxiv.org/abs/2109.12772) | 本研究提出了一种适用于多类逻辑回归的分布鲁棒优化方法，在深度图像分类器中得到了应用。这种方法可使图像分类器对随机和对抗攻击具有鲁棒性。在使用MNIST和CIFAR-10数据集时，相比于基准方法，通过采用新的随机训练方法，测试错误率的降低高达83.5%，损失降低高达91.3%。 |
| [^40] | [Variance Reduction for Matrix Computations with Applications to Gaussian Processes.](http://arxiv.org/abs/2106.14565) | 本文提出了一种通过矩阵分解来进行矩阵计算的方差缩减方法，并展示了在某些问题上可以实现任意更好的随机性能。此外，本文提出的矩阵乘积迹的分解估计器与对半正定矩阵行列式的对数估计器均有显著的效率提高，可以在实际的贝叶斯优化问题中得到应用。 |
| [^41] | [Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers.](http://arxiv.org/abs/2106.13870) | 本文提出了一种方法来估计确定性图像分类器在给定输入数据上结果的不确定性，定义了惠灵顿后验作为分布以表明可能由生成给定图像的相同场景生成的数据的响应。 |
| [^42] | [Kernel Two-Sample Tests for Manifold Data.](http://arxiv.org/abs/2105.03425) | 本文研究了与最大均值差异（MMD）相关的基于核的双样本检验统计量在测量流形数据时的应用。文章展示了检验水平和功率与核带宽、样本数量和流形内在维度之间的关系，并在特定条件下建立了测试功率下界。 |
| [^43] | [A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit.](http://arxiv.org/abs/2011.14033) | 本文提供了一个新颖的、不需要调整指数参数的MNL-Contextual Bandit问题的简便在线学习算法。算法具有与该问题的最佳理论界限匹配的遗憾上界。 |
| [^44] | [DeepTopPush: Simple and Scalable Method for Accuracy at the Top.](http://arxiv.org/abs/2006.12293) | DeepTopPush是一种用于解决Accuracy at the Top问题的简单而可扩展的方法，能有效地选择少量重要的样本，并在不同领域取得了优异的性能表现 |

# 详细

[^1]: 基于列表的在线分类

    List Online Classification. (arXiv:2303.15383v1 [cs.LG])

    [http://arxiv.org/abs/2303.15383](http://arxiv.org/abs/2303.15383)

    本文研究了多标签列表的在线预测问题，提出了 $b$-ary Littlestone 维度可学习模型，并且在懵懂的情况下探索不同的情况。可以使用改编自 Littlestone 的 SOA 和 Rosenblatt 的感知器等算法进行预测，同时还建立了列表可学习的组合结果。

    

    我们研究多分类在线预测，其中学习者可以使用多个标签的列表进行预测（与传统设置中仅使用一种标签不同）。我们使用 $b$-ary Littlestone 维度表征了该模型中的可学习性。该维度是经典 Littlestone 维度的变体，其中二进制错误树被替换为 $(k+1)$-ary 错误树，其中 k 是列表中标签的数量。在懵懂的场景中，我们根据比较类中是否包含单标签或多标签函数以及它与算法使用的列表大小之间的权衡来探索不同的情况。我们发现在某些情况下可以实现负悔，同时提供了什么情况下实现负悔的完整特性化。作为我们工作的一部分，我们改编了经典算法，如 Littlestone 的 SOA 和 Rosenblatt 的感知器，以使用标签列表进行预测。我们还为可以进行列表学习的组合结果建立了基础。

    We study multiclass online prediction where the learner can predict using a list of multiple labels (as opposed to just one label in the traditional setting). We characterize learnability in this model using the $b$-ary Littlestone dimension. This dimension is a variation of the classical Littlestone dimension with the difference that binary mistake trees are replaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in the list. In the agnostic setting, we explore different scenarios depending on whether the comparator class consists of single-labeled or multi-labeled functions and its tradeoff with the size of the lists the algorithm uses. We find that it is possible to achieve negative regret in some cases and provide a complete characterization of when this is possible. As part of our work, we adapt classical algorithms such as Littlestone's SOA and Rosenblatt's Perceptron to predict using lists of labels. We also establish combinatorial results for list-learnable c
    
[^2]: 探索扩散模型的持续学习

    Exploring Continual Learning of Diffusion Models. (arXiv:2303.15342v1 [cs.LG])

    [http://arxiv.org/abs/2303.15342](http://arxiv.org/abs/2303.15342)

    扩散模型需要进行计算密集型的从头开始训练，因此探索在数据分布发生变化时重新使用计算的迭代训练是有必要的。经验重放与减少排练系数的性能强大。使用每维比特数评估CL存在某些缺陷。

    

    由于其新颖的训练程序应用于大量数据，扩散模型在生成高质量图像方面取得了显著的成功。然而，从头开始训练扩散模型是计算密集型的。这突出了需要研究在数据分布发生变化时重新使用计算的迭代训练可能性。在此研究中，我们迈出了这一方向的第一步，并评估了扩散模型的持续学习（CL）属性。我们首先对应用于去噪扩散概率模型（DDPM）的最常见CL方法进行基准测试，其中我们注意到经验重放与减少排练系数的性能强大。此外，我们提供了对遗忘动态的见解，它们表现出扩散时间步长的不同行为。我们还揭示了使用每维比特数评估CL的某些缺陷。

    Diffusion models have achieved remarkable success in generating high-quality images thanks to their novel training procedures applied to unprecedented amounts of data. However, training a diffusion model from scratch is computationally expensive. This highlights the need to investigate the possibility of training these models iteratively, reusing computation while the data distribution changes. In this study, we take the first step in this direction and evaluate the continual learning (CL) properties of diffusion models. We begin by benchmarking the most common CL methods applied to Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong performance of the experience replay with the reduced rehearsal coefficient. Furthermore, we provide insights into the dynamics of forgetting, which exhibit diverse behavior across diffusion timesteps. We also uncover certain pitfalls of using the bits-per-dimension metric for evaluating CL.
    
[^3]: 用混合VAE模型学习流形来解决逆问题

    Manifold Learning by Mixture Models of VAEs for Inverse Problems. (arXiv:2303.15244v1 [cs.LG])

    [http://arxiv.org/abs/2303.15244](http://arxiv.org/abs/2303.15244)

    本文提出了一种用混合VAE模型学习流形的方法，并将其用于解决逆问题，结果表现出良好的性能，可用于模糊和电阻抗层析成像。

    

    在实践中，使用生成模型表示高维数据的流形已被证明具有计算效率。然而，这要求数据流形具有全局参数化。为了表示任意拓扑的流形，我们提出了学习变分自编码器的混合模型。这里，每个编码器-解码器对表示流形的一个图表。我们提出了一种损失函数来最大化似然估计模型权重，并选择一个架构，为我们提供图表及其逆的解析表达式。一旦学习了流形，我们将其用于通过将数据拟合项限制在学习的流形上来解决逆问题。为了解决所产生的最小化问题，我们在学习的流形上提出了一种黎曼梯度下降算法。我们展示了我们的方法在低维玩具例子以及模糊和电阻抗层析成像方面的性能。

    Representing a manifold of very high-dimensional data with generative models has been shown to be computationally efficient in practice. However, this requires that the data manifold admits a global parameterization. In order to represent manifolds of arbitrary topology, we propose to learn a mixture model of variational autoencoders. Here, every encoder-decoder pair represents one chart of a manifold. We propose a loss function for maximum likelihood estimation of the model weights and choose an architecture that provides us the analytical expression of the charts and of their inverses. Once the manifold is learned, we use it for solving inverse problems by minimizing a data fidelity term restricted to the learned manifold. To solve the arising minimization problem we propose a Riemannian gradient descent algorithm on the learned manifold. We demonstrate the performance of our method for low-dimensional toy examples as well as for deblurring and electrical impedance tomography on cert
    
[^4]: 带有降低通信要求的异步在线联邦学习

    Asynchronous Online Federated Learning with Reduced Communication Requirements. (arXiv:2303.15226v1 [cs.LG])

    [http://arxiv.org/abs/2303.15226](http://arxiv.org/abs/2303.15226)

    提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，能够处理异构和延迟设备，实现参与学习任务的可访问性和效率。

    

    在线联邦学习（FL）使得地理分布的设备可以从本地的流数据中学习到全局共享模型。大多数关于在线FL的文献都考虑了最佳情况下的参与客户端和通信渠道。然而，这些假设在实际应用中通常无法满足。异步设置可以反映出更现实的环境，例如由于可用的计算能力和电池限制而发生的异构客户端参与，以及由通信渠道或落后设备引起的延迟。此外，在大多数应用中，必须考虑能源效率。我们提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，通过减少参与者的通信开销，提高了参与学习任务的可访问性和效率。此外，该方法能够处理异构和延迟设备，使其更适用于实际应用。

    Online federated learning (FL) enables geographically distributed devices to learn a global shared model from locally available streaming data. Most online FL literature considers a best-case scenario regarding the participating clients and the communication channels. However, these assumptions are often not met in real-world applications. Asynchronous settings can reflect a more realistic environment, such as heterogeneous client participation due to available computational power and battery constraints, as well as delays caused by communication channels or straggler devices. Further, in most applications, energy efficiency must be taken into consideration. Using the principles of partial-sharing-based communications, we propose a communication-efficient asynchronous online federated learning (PAO-Fed) strategy. By reducing the communication overhead of the participants, the proposed method renders participation in the learning task more accessible and efficient. In addition, the prop
    
[^5]: $L_p$与风险一致性之间的联系及其对正则化内核方法的影响

    On the Connection between $L_p$ and Risk Consistency and its Implications on Regularized Kernel Methods. (arXiv:2303.15210v1 [stat.ML])

    [http://arxiv.org/abs/2303.15210](http://arxiv.org/abs/2303.15210)

    本文阐明了$L_p$-一致性和风险一致性之间的密切关系，特别在移位损失函数中发现了新的规律，这对于正则化内核方法具有重要意义。

    

    由于预测质量经常通过其风险进行评估，因此将风险一致性视为学习方法的理想性质是很自然的，确实已经证明了许多这样的方法具有风险一致性。本文的第一个目的是在比以前更广泛的损失函数类别中建立$L_p$一致性与风险一致性之间的密切联系。尝试将此联系转移到移位损失函数，惊人地发现，与其他许多结果相比，这种移位并不能减少必须对基础概率度量做出的假设。结果应用于正则化内核方法，例如支持向量机。

    As a predictor's quality is often assessed by means of its risk, it is natural to regard risk consistency as a desirable property of learning methods, and many such methods have indeed been shown to be risk consistent. The first aim of this paper is to establish the close connection between risk consistency and $L_p$-consistency for a considerably wider class of loss functions than has been done before. The attempt to transfer this connection to shifted loss functions surprisingly reveals that this shift does not reduce the assumptions needed on the underlying probability measure to the same extent as it does for many other results. The results are applied to regularized kernel methods such as support vector machines.
    
[^6]: 在凸约束下学习线性动态系统

    Learning linear dynamical systems under convex constraints. (arXiv:2303.15121v1 [math.ST])

    [http://arxiv.org/abs/2303.15121](http://arxiv.org/abs/2303.15121)

    本文考虑在给定凸约束下学习线性动态系统，通过解出受约束的最小二乘估计，提出新的非渐进误差界，并应用于稀疏矩阵等情境，改进了现有统计方法。

    

    我们考虑从单个轨迹中识别线性动态系统的问题。最近的研究主要关注未对系统矩阵 $A^* \in \mathbb{R}^{n \times n}$ 进行结构假设的情况，并对普通最小二乘 (OLS) 估计器进行了详细分析。我们假设可用先前的 $A^*$ 的结构信息，可以在包含 $A^*$ 的凸集 $\mathcal{K}$ 中捕获。对于随后的受约束最小二乘估计的解，我们推导出 Frobenius 范数下依赖于 $\mathcal{K}$ 在 $A^*$ 处切锥的局部大小的非渐进误差界。为了说明这一结果的有用性，我们将其实例化为以下设置：(i) $\mathcal{K}$ 是 $\mathbb{R}^{n \times n}$ 中的 $d$ 维子空间，或者 (ii) $A^*$ 是 $k$ 稀疏的，$\mathcal{K}$ 是适当缩放的 $\ell_1$ 球。在 $d, k \ll n^2$ 的区域中，我们的误差界对于相同的统计和噪声假设比 OLS 估计器获得了改进。

    We consider the problem of identification of linear dynamical systems from a single trajectory. Recent results have predominantly focused on the setup where no structural assumption is made on the system matrix $A^* \in \mathbb{R}^{n \times n}$, and have consequently analyzed the ordinary least squares (OLS) estimator in detail. We assume prior structural information on $A^*$ is available, which can be captured in the form of a convex set $\mathcal{K}$ containing $A^*$. For the solution of the ensuing constrained least squares estimator, we derive non-asymptotic error bounds in the Frobenius norm which depend on the local size of the tangent cone of $\mathcal{K}$ at $A^*$. To illustrate the usefulness of this result, we instantiate it for the settings where, (i) $\mathcal{K}$ is a $d$ dimensional subspace of $\mathbb{R}^{n \times n}$, or (ii) $A^*$ is $k$-sparse and $\mathcal{K}$ is a suitably scaled $\ell_1$ ball. In the regimes where $d, k \ll n^2$, our bounds improve upon those obta
    
[^7]: 空间碰撞共面数据信息的泊松过程特性

    Conjunction Data Messages for Space Collision Behave as a Poisson Process. (arXiv:2303.15074v1 [stat.ML])

    [http://arxiv.org/abs/2303.15074](http://arxiv.org/abs/2303.15074)

    本研究提出了一个统计学习模型，研究碰撞数据信息到达的泊松过程性质，并能够回答卫星运营商关心的两个问提：（1）下一个指定时间间隔内是否会有新的消息？（2）下一个消息将在何时，并带有怎样的不确定性？

    

    空间碎片是空间探索中的一个重大问题。国际机构不断监测大量的轨道物体数据库并发出共面数据信息形式的警告。对于卫星运营商来说，一个重要的问题是估计何时会有新的信息到达，以便他们可以及时而节俭地进行卫星机动。我们提出了一个信息到达过程的统计学习模型，允许我们回答两个重要的问题：（1）下一个指定时间间隔内是否会有新的消息？（2）下一个消息将在何时，并带有怎样的不确定性？我们的贝叶斯泊松过程模型对于问题（2）的平均预测误差在一个包含50,000个紧密相遇事件测试集中比基线小了4个小时以上。

    Space debris is a major problem in space exploration. International bodies continuously monitor a large database of orbiting objects and emit warnings in the form of conjunction data messages. An important question for satellite operators is to estimate when fresh information will arrive so that they can react timely but sparingly with satellite maneuvers. We propose a statistical learning model of the message arrival process, allowing us to answer two important questions: (1) Will there be any new message in the next specified time interval? (2) When exactly and with what uncertainty will the next message arrive? The average prediction error for question (2) of our Bayesian Poisson process model is smaller than the baseline in more than 4 hours in a test set of 50k close encounter events.
    
[^8]: 朝黑盒参数估计迈进

    Towards black-box parameter estimation. (arXiv:2303.15041v1 [stat.ML])

    [http://arxiv.org/abs/2303.15041](http://arxiv.org/abs/2303.15041)

    本文提出了一种基于弱参数结构假设的黑盒程序，用于估计统计模型参数。该程序可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。

    

    深度学习算法最近已经被证明是估计统计模型参数的成功工具，模拟容易但似然计算具有挑战性。但这些方法的成功取决于模拟出可以充分复制观察数据的参数，并且目前缺乏有效的方法来产生这些模拟数据。我们开发了基于弱参数结构假设估计统计模型参数的新的黑盒程序。对于似然函数有较频繁出现的良好结构的情况，如时间序列，这是通过在广泛的模拟数据库上预训练深度神经网络来实现的，该数据库涵盖了各种数据大小的范围。对于其他类型的复杂依赖关系，则需要一个迭代的算法来指导多轮正确参数区域的模拟。这些方法可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。

    Deep learning algorithms have recently shown to be a successful tool in estimating parameters of statistical models for which simulation is easy, but likelihood computation is challenging. But the success of these approaches depends on simulating parameters that sufficiently reproduce the observed data, and, at present, there is a lack of efficient methods to produce these simulations. We develop new black-box procedures to estimate parameters of statistical models based only on weak parameter structure assumptions. For well-structured likelihoods with frequent occurrences, such as in time series, this is achieved by pre-training a deep neural network on an extensive simulated database that covers a wide range of data sizes. For other types of complex dependencies, an iterative algorithm guides simulations to the correct parameter region in multiple rounds. These approaches can successfully estimate and quantify the uncertainty of parameters from non-Gaussian models with complex spatia
    
[^9]: 正则化EM算法

    Regularized EM algorithm. (arXiv:2303.14989v1 [stat.ML])

    [http://arxiv.org/abs/2303.14989](http://arxiv.org/abs/2303.14989)

    该论文提出了一种正则化EM算法，用于处理低样本支持情况下GMM的协方差矩阵更新问题，并且可以利用预先提供的先验知识来实现更高效的计算。

    

    期望最大化（EM）算法是一种广泛用于计算局部最大似然估计（MLE）的迭代算法。它可以应用于广泛的问题，包括基于高斯混合模型（GMM）进行数据聚类。当样本大小不大于数据维数时，可能会出现数值不稳定和收敛问题。在这种低样本支持（LSS）设置下，EM-GMM算法中的协方差矩阵更新可能会变得奇异或病态，从而导致算法崩溃。另一方面，在许多信号处理问题中，预先可用的先验信息可以指示不同聚类协方差矩阵的某些结构。在本文中，我们提出了一种正则化EM算法，用于GMM，可以有效利用这样的先验知识以及应对LSS情况。该方法旨在最大化惩罚的GMM似然，其中可以使用正则化估计来确保正定的协方差矩阵。

    Expectation-Maximization (EM) algorithm is a widely used iterative algorithm for computing (local) maximum likelihood estimate (MLE). It can be used in an extensive range of problems, including the clustering of data based on the Gaussian mixture model (GMM). Numerical instability and convergence problems may arise in situations where the sample size is not much larger than the data dimensionality. In such low sample support (LSS) settings, the covariance matrix update in the EM-GMM algorithm may become singular or poorly conditioned, causing the algorithm to crash. On the other hand, in many signal processing problems, a priori information can be available indicating certain structures for different cluster covariance matrices. In this paper, we present a regularized EM algorithm for GMM-s that can make efficient use of such prior knowledge as well as cope with LSS situations. The method aims to maximize a penalized GMM likelihood where regularized estimation may be used to ensure pos
    
[^10]: 用目标条件的强化学习学习生成模型

    Learning Generative Models with Goal-conditioned Reinforcement Learning. (arXiv:2303.14811v1 [cs.LG])

    [http://arxiv.org/abs/2303.14811](http://arxiv.org/abs/2303.14811)

    研究提出了用目标条件的强化学习来学习生成模型的框架，能够在训练集中生成多样性和高质量的样本。

    

    我们提出了一种新颖的、用目标条件的强化学习来学习生成模型的框架。我们定义了两个代理，一个是目标条件代理（GC-agent），另一个是监督代理（S-agent）。在给定用户输入的初始状态后，GC-agent学习重构训练集。在此情况下，训练集中的元素是目标。在训练过程中，S-agent学习在不知道目标的情况下模仿GC-agent。在推理阶段，我们使用S-agent生成新的样本。类似于变分自编码器，我们通过推导出一个上限来衡量负对数似然，它由重构项和GC-agent策略与（目标无关的）S-agent策略之间的差异组成。我们在图像合成任务中经验证明，我们的方法能够生成多样性和高质量的样本。

    We present a novel, alternative framework for learning generative models with goal-conditioned reinforcement learning. We define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent). Given a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals. During training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals. At inference we generate new samples with the S-agent. Following a similar route as in variational auto-encoders, we derive an upper bound on the negative log-likelihood that consists of a reconstruction term and a divergence between the GC-agent policy and the (goal-agnostic) S-agent policy. We empirically demonstrate that our method is able to generate diverse and high quality samples in the task of image synthesis.
    
[^11]: 高维函数回归中特征选择和估计的一种高效自适应方法--FAStEN

    FAStEN: an efficient adaptive method for feature selection and estimation in high-dimensional functional regressions. (arXiv:2303.14801v1 [stat.ME])

    [http://arxiv.org/abs/2303.14801](http://arxiv.org/abs/2303.14801)

    提出了一种新的自适应方法FAStEN，用于在高维函数回归问题中执行特征选择和参数估计，通过利用函数主成分和对偶增广Lagrangian问题的稀疏性质，具有显著的计算效率和选择准确性。

    

    函数回归分析是许多当代科学应用的已建立工具。涉及大规模和复杂数据集的回归问题是普遍存在的，特征选择对于避免过度拟合和实现准确预测至关重要。我们提出了一种新的、灵活的、超高效的方法，用于在稀疏高维函数回归问题中执行特征选择，并展示了如何将其扩展到标量对函数框架中。我们的方法将函数数据、优化和机器学习技术相结合，以同时执行特征选择和参数估计。我们利用函数主成分的特性以及对偶增广Lagrangian问题的稀疏性质，显著降低了计算成本，并引入了自适应方案来提高选择准确性。通过广泛的模拟研究，我们将我们的方法与最佳现有竞争对手进行了基准测试，并证明了我们的方法的高效性。

    Functional regression analysis is an established tool for many contemporary scientific applications. Regression problems involving large and complex data sets are ubiquitous, and feature selection is crucial for avoiding overfitting and achieving accurate predictions. We propose a new, flexible, and ultra-efficient approach to perform feature selection in a sparse high dimensional function-on-function regression problem, and we show how to extend it to the scalar-on-function framework. Our method combines functional data, optimization, and machine learning techniques to perform feature selection and parameter estimation simultaneously. We exploit the properties of Functional Principal Components, and the sparsity inherent to the Dual Augmented Lagrangian problem to significantly reduce computational cost, and we introduce an adaptive scheme to improve selection accuracy. Through an extensive simulation study, we benchmark our approach to the best existing competitors and demonstrate a 
    
[^12]: 关于学习算法泛化误差信息理论界限的紧密性研究

    On the tightness of information-theoretic bounds on generalization error of learning algorithms. (arXiv:2303.14658v1 [cs.IT])

    [http://arxiv.org/abs/2303.14658](http://arxiv.org/abs/2303.14658)

    本文研究了学习算法泛化误差信息理论界限的紧密性。研究表明，通过适当的假设，可以在快速收敛速度下使用信息理论量$O(\lambda/n)$来上界估计泛化误差。

    

    Russo和Xu提出了一种方法来证明学习算法的泛化误差可以通过信息度量进行上界估计。然而，这种收敛速度通常被认为是“慢”的，因为它的期望收敛速度的形式为$O(\sqrt{\lambda/n})$，其中$\lambda$是一些信息理论量。在本文中我们证明了根号并不一定意味着收敛速度慢，可以在适当的假设下使用这个界限来得到$O(\lambda/n)$的快速收敛速度。此外，我们确定了达到快速收敛速度的关键条件，即所谓的$(\eta,c)$-中心条件。在这个条件下，我们给出了学习算法泛化误差的信息理论界限。

    A recent line of works, initiated by Russo and Xu, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generaliz
    
[^13]: 机器学习的复杂性校准基准揭示了下一代水库计算机预测成功和误导的时机。

    Complexity-calibrated Benchmarks for Machine Learning Reveal When Next-Generation Reservoir Computer Predictions Succeed and Mislead. (arXiv:2303.14553v1 [cs.LG])

    [http://arxiv.org/abs/2303.14553](http://arxiv.org/abs/2303.14553)

    本文讨论了有限历史记忆痕迹在下一代水库计算机中存在的固有限制，并发现在高度非马尔可夫过程中，流行的循环神经网络远远无法实现最佳预测这种复杂过程的目标。这些结果凸显了一种新一代优化循环神经网络的需求。

    

    循环神经网络被用于预测金融、气候、语言和其他领域的时间序列。水库计算机是一种特别易于训练的循环神经网络形式。最近引入了“下一代”水库计算机，其中内存跟踪仅涉及有限数量的先前符号。我们探讨了这种有趣提议中有限历史记忆痕迹的固有限制。从范诺不等式的下界可以看出，在由大型概率状态机生成的高度非马尔可夫过程中，具有相当长存储痕迹的下一代水库计算机的错误概率比预测下一个观察的最小可达错误概率高至少~60%。总的来说，似乎流行的循环神经网络远远无法实现最佳预测这种复杂过程的目标。这些结果凸显了一种新一代优化循环神经网络的需求。

    Recurrent neural networks are used to forecast time series in finance, climate, language, and from many other domains. Reservoir computers are a particularly easily trainable form of recurrent neural network. Recently, a "next-generation" reservoir computer was introduced in which the memory trace involves only a finite number of previous symbols. We explore the inherent limitations of finite-past memory traces in this intriguing proposal. A lower bound from Fano's inequality shows that, on highly non-Markovian processes generated by large probabilistic state machines, next-generation reservoir computers with reasonably long memory traces have an error probability that is at least ~ 60% higher than the minimal attainable error probability in predicting the next observation. More generally, it appears that popular recurrent neural networks fall far short of optimally predicting such complex processes. These results highlight the need for a new generation of optimized recurrent neural ne
    
[^14]: 解释约束下的学习

    Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])

    [http://arxiv.org/abs/2303.14496](http://arxiv.org/abs/2303.14496)

    本文研究了解释约束下的学习问题，提出了EPAC模型，探讨了使用这些解释时模型的益处，并提供了一种基于变分近似的算法解决方案。

    

    尽管监督学习假设存在标注数据，但我们可能有关于模型应如何运行的先验信息。本文将其形式化为从解释约束中学习，并提供了一个学习理论框架，分析了这些解释如何提高模型的学习能力。本文的第一项关键贡献是通过定义我们称之为EPAC模型（在新数据期望中满足这些约束的模型）来回答哪些模型会受益于解释这一问题。我们使用标准的学习理论工具分析了这类模型。第二个关键贡献是对于由线性模型和两层神经网络的梯度信息给出的规范解释的限制（以其Rademacher复杂度为衡量标准）进行了表征。最后，我们通过一种变分近似提供了我们的框架的算法解决方案，它能够实现更好的性能并满足这些约束。

    While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
    
[^15]: 自回归条件神经过程

    Autoregressive Conditional Neural Processes. (arXiv:2303.14468v1 [stat.ML])

    [http://arxiv.org/abs/2303.14468](http://arxiv.org/abs/2303.14468)

    本论文提出了自回归条件神经过程模型，能够建模高度相关的非高斯预测分布。

    

    有条件的神经过程是一种有吸引力元学习模型，能够产生良好校准的预测，并且可以通过简单的最大似然过程进行训练。本研究提出了一种改变有条件的神经过程在测试时部署方式的方法，从而能够建模高度相关的非高斯预测分布。

    Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an e
    
[^16]: 混合模糊-清晰聚类算法：理论与实验

    Hybrid Fuzzy-Crisp Clustering Algorithm: Theory and Experiments. (arXiv:2303.14366v1 [stat.ML])

    [http://arxiv.org/abs/2303.14366](http://arxiv.org/abs/2303.14366)

    本文提出了一种基于目标函数的混合模糊-清晰聚类算法，能够解决传统模糊C均值聚类算法在聚类大小差异巨大时的不平衡影响问题，同时在实验中表现出更好的聚类质量和鲁棒性。

    

    在传统模糊C均值聚类算法中，由于隶属函数始终为正，当聚类大小差异巨大时，会导致不平衡的影响。即，一个明显更大的聚类将所有其他聚类坐标点吸引到其中心，无论它们有多远。为了解决这个问题，本文提出了一种基于隶属度函数线性和二次项的目标函数的混合模糊-清晰聚类算法。在该算法中，如果数据点距离聚类中心“足够”远，则将其隶属度精确地设置为零。本文介绍了一种新的混合模糊-清晰聚类算法及其几何解释。该算法在二十个模拟的数据集和五个来自UCI数据仓库的真实数据集上进行了测试，并与传统的模糊和清晰聚类方法进行了比较。实验结果表明，所提出的算法在聚类质量和鲁棒性方面优于传统方法。

    With the membership function being strictly positive, the conventional fuzzy c-means clustering method sometimes causes imbalanced influence when clusters of vastly different sizes exist. That is, an outstandingly large cluster drags to its center all the other clusters, however far they are separated. To solve this problem, we propose a hybrid fuzzy-crisp clustering algorithm based on a target function combining linear and quadratic terms of the membership function. In this algorithm, the membership of a data point to a cluster is automatically set to exactly zero if the data point is ``sufficiently'' far from the cluster center. In this paper, we present a new algorithm for hybrid fuzzy-crisp clustering along with its geometric interpretation. The algorithm is tested on twenty simulated data generated and five real-world datasets from the UCI repository and compared with conventional fuzzy and crisp clustering methods. The proposed algorithm is demonstrated to outperform the conventi
    
[^17]: repliclust：聚类分析的合成数据

    repliclust: Synthetic Data for Cluster Analysis. (arXiv:2303.14301v1 [cs.LG])

    [http://arxiv.org/abs/2303.14301](http://arxiv.org/abs/2303.14301)

    repliclust 是一个 Python 包，用于生成具有聚类的合成数据集，基于数据集的原型，提供了放置集群中心、采样集群形状、选择每个集群的数据点数量以及为集群分配概率分布的算法。

    

    我们介绍了 repliclust（来自于 repli-cate 和 clust-er），这是一个用于生成具有聚类的合成数据集的 Python 包。我们的方法基于数据集的原型，即高级几何描述，用户可以从中创建许多不同的数据集，并具有所需的几何特性。我们软件的架构是模块化和面向对象的，将数据生成分解成放置集群中心的算法、采样集群形状的算法、选择每个集群的数据点数量的算法以及为集群分配概率分布的算法。repliclust.org 项目网页提供了简明的用户指南和全面的文档。

    We present repliclust (from repli-cate and clust-er), a Python package for generating synthetic data sets with clusters. Our approach is based on data set archetypes, high-level geometric descriptions from which the user can create many different data sets, each possessing the desired geometric characteristics. The architecture of our software is modular and object-oriented, decomposing data generation into algorithms for placing cluster centers, sampling cluster shapes, selecting the number of data points for each cluster, and assigning probability distributions to clusters. The project webpage, repliclust.org, provides a concise user guide and thorough documentation.
    
[^18]: 针对Hölder连续多元函数的高效Lipschitzian全局优化方法

    Efficient Lipschitzian Global Optimization of H\"older Continuous Multivariate Functions. (arXiv:2303.14293v1 [cs.LG])

    [http://arxiv.org/abs/2303.14293](http://arxiv.org/abs/2303.14293)

    本研究提出了一种高效的全局优化技术，通过预定的查询创建规则实现了对Hölder连续多元函数的计算优势，可在给定时间段内获得 minimax 最优的结果。

    

    本研究提出了一种有效的全局优化技术，专门针对Hölder连续的多元函数。与构造下界代理函数的传统方法不同，这个算法采用了预定的查询创建规则，使其在计算上更具优势。算法的性能使用平均或累积遗憾进行评估，这也意味着简单遗憾的界限，反映了该方法的整体有效性。结果表明，使用适当的参数，算法在给定时间段$T$内针对Hölder连续的Hölder指数为$\alpha$的目标函数在$n$维空间中获得了$O(T^{-\frac{\alpha}{n}})$的平均遗憾界限。我们证明了这一界限是极小化最优的。

    This study presents an effective global optimization technique designed for multivariate functions that are H\"older continuous. Unlike traditional methods that construct lower bounding proxy functions, this algorithm employs a predetermined query creation rule that makes it computationally superior. The algorithm's performance is assessed using the average or cumulative regret, which also implies a bound for the simple regret and reflects the overall effectiveness of the approach. The results show that with appropriate parameters the algorithm attains an average regret bound of $O(T^{-\frac{\alpha}{n}})$ for optimizing a H\"older continuous target function with H\"older exponent $\alpha$ in an $n$-dimensional space within a given time horizon $T$. We demonstrate that this bound is minimax optimal.
    
[^19]: 高斯过程在极端长度尺度上的应用：从分子到黑洞。

    Applications of Gaussian Processes at Extreme Lengthscales: From Molecules to Black Holes. (arXiv:2303.14291v1 [stat.ML])

    [http://arxiv.org/abs/2303.14291](http://arxiv.org/abs/2303.14291)

    高斯过程是一种适合拟合少量数据且充分考虑不确定性的模型，可用于从分子到黑洞等多个领域的数据预测和推断。

    

    在许多观测和实验科学领域，数据非常稀缺。在高能天体物理学中，受到天体遮挡和有限的望远镜时间的影响，数据观测受到干扰。而在合成化学和材料科学的实验室实验中得出的数据，耗时和成本都非常高昂。然而，在科学领域中通常可以获得有关数据产生机制的知识，例如实验装置的测量误差等。这两个特征，即数据量小和对基本物理原理的了解，使得高斯过程（GPs）成为适合拟合此类数据集的理想候选。GPs 能够考虑到不确定性，例如在分子和材料的虚拟筛选中进行预测，并且还可以对不完整的数据进行推断，例如从黑洞吸积盘的潜在发射特征。此外，GPs目前是贝叶斯优化的工作模型，这是一种预计将成为引导学习的方法。

    In many areas of the observational and experimental sciences data is scarce. Data observation in high-energy astrophysics is disrupted by celestial occlusions and limited telescope time while data derived from laboratory experiments in synthetic chemistry and materials science is time and cost-intensive to collect. On the other hand, knowledge about the data-generation mechanism is often available in the sciences, such as the measurement error of a piece of laboratory apparatus. Both characteristics, small data and knowledge of the underlying physics, make Gaussian processes (GPs) ideal candidates for fitting such datasets. GPs can make predictions with consideration of uncertainty, for example in the virtual screening of molecules and materials, and can also make inferences about incomplete data such as the latent emission signature from a black hole accretion disc. Furthermore, GPs are currently the workhorse model for Bayesian optimisation, a methodology foreseen to be a guide for l
    
[^20]: 逻辑回归特征空间草图

    Feature Space Sketching for Logistic Regression. (arXiv:2303.14284v1 [cs.LG])

    [http://arxiv.org/abs/2303.14284](http://arxiv.org/abs/2303.14284)

    该论文提出了基于草图的逻辑回归coreset构建、特征选择和降维的新界限，并解决了之前工作中存在的问题，并提出了可扩展到广义线性模型的前向误差界限。

    

    我们提出了针对逻辑回归的coreset构建、特征选择和降维的新界限。这三种方法都可以视为逻辑回归输入的草图。在coreset构建方面，我们解决了之前工作中的问题，并提出了coreset构造方法的复杂度新界限。在特征选择和降维方面，我们开始研究逻辑回归的前向误差界限。我们的界限可以收紧，直到确定的因素为止，并且前向误差界限可以扩展到广义线性模型。

    We present novel bounds for coreset construction, feature selection, and dimensionality reduction for logistic regression. All three approaches can be thought of as sketching the logistic regression inputs. On the coreset construction front, we resolve open problems from prior work and present novel bounds for the complexity of coreset construction methods. On the feature selection and dimensionality reduction front, we initiate the study of forward error bounds for logistic regression. Our bounds are tight up to constant factors and our forward error bounds can be extended to Generalized Linear Models.
    
[^21]: 基于序列 Knockoffs 的强化学习变量选择

    Sequential Knockoffs for Variable Selection in Reinforcement Learning. (arXiv:2303.14281v1 [stat.ML])

    [http://arxiv.org/abs/2303.14281](http://arxiv.org/abs/2303.14281)

    本论文介绍了一种新颖的序列 Knockoffs (SEEK)算法，用于在强化学习系统中实现变量选择，该算法估计了最小充分状态，确保学习进程良好而不会减缓。

    

    在强化学习的实际应用中，通常很难获得一个既简洁又满足马尔可夫属性的状态表示，而不需要使用先验知识。因此，常规做法是构造一个比必要的要大的状态，例如将连续时间点上的测量串联起来。然而，增加状态的维数可能会减缓学习进程并使学习策略模糊不清。我们引入了一个在马尔可夫决策过程(MDP)中的最小充分状态的概念，作为原始状态下最小的子向量，使该过程仍然是MDP，并且与原始过程共享相同的最优策略。我们提出了一种新颖的序列 Knockoffs (SEEK)算法，用于估计高维复杂非线性动力学系统中的最小充分状态。在大样本中，所提出的方法控制了假发现率，并且选择所有充分的变量的概率趋近于1。

    In real-world applications of reinforcement learning, it is often challenging to obtain a state representation that is parsimonious and satisfies the Markov property without prior knowledge. Consequently, it is common practice to construct a state which is larger than necessary, e.g., by concatenating measurements over contiguous time points. However, needlessly increasing the dimension of the state can slow learning and obfuscate the learned policy. We introduce the notion of a minimal sufficient state in a Markov decision process (MDP) as the smallest subvector of the original state under which the process remains an MDP and shares the same optimal policy as the original process. We propose a novel sequential knockoffs (SEEK) algorithm that estimates the minimal sufficient state in a system with high-dimensional complex nonlinear dynamics. In large samples, the proposed method controls the false discovery rate, and selects all sufficient variables with probability approaching one. As
    
[^22]: 隐式平衡和正则化：过参数化非对称矩阵感知中的泛化和收敛保证

    Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing. (arXiv:2303.14244v1 [cs.LG])

    [http://arxiv.org/abs/2303.14244](http://arxiv.org/abs/2303.14244)

    本论文研究了过参数化低秩矩阵感知问题，证明了通过因子化方法训练的过参数化模型可以收敛，并且隐式平衡和正则化可以促进泛化。

    

    最近，对于训练过参数化学习模型的基于梯度的方法的收敛和泛化属性有了重要进展。然而，其中许多方面，包括小随机初始化的角色以及模型的各种参数在梯度更新中如何耦合以促进良好的泛化，仍然是很神秘的。最近一系列的论文已经开始研究非凸对称半正定（PSD）矩阵感知问题的形式，在这个问题中需要从几个线性测量中重建一个低秩PSD矩阵。这种底层的对称性/PSD性对于现有的这个问题的收敛和泛化保证是至关重要的。在本文中，我们研究了一个一般的过参数化的低秩矩阵感知问题，其中希望从少量的线性测量中重建一个非对称矩形低秩矩阵。我们证明了通过因子化来训练的过参数化模型在这个问题上可以收敛，而隐式平衡和正则化可以促进泛化。

    Recently, there has been significant progress in understanding the convergence and generalization properties of gradient-based methods for training overparameterized learning models. However, many aspects including the role of small random initialization and how the various parameters of the model are coupled during gradient-based updates to facilitate good generalization remain largely mysterious. A series of recent papers have begun to study this role for non-convex formulations of symmetric Positive Semi-Definite (PSD) matrix sensing problems which involve reconstructing a low-rank PSD matrix from a few linear measurements. The underlying symmetry/PSDness is crucial to existing convergence and generalization guarantees for this problem. In this paper, we study a general overparameterized low-rank matrix sensing problem where one wishes to reconstruct an asymmetric rectangular low-rank matrix from a few linear measurements. We prove that an overparameterized model trained via factori
    
[^23]: 组合干预的因果推断框架:合成组合

    Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions. (arXiv:2303.14226v1 [stat.ME])

    [http://arxiv.org/abs/2303.14226](http://arxiv.org/abs/2303.14226)

    提出了一种在组合干预下进行因果推断的模型，通过施加潜在结构跨越单位和组合，在降低实验数量和处理混杂问题方面有着良好表现。

    

    我们考虑一个包含N个异质单位和p个干预的设置。 我们的目标是学习任意组合的单位特定潜在结果，即N×2 ^ p个因果参数。在许多应用程序中自然出现了选择干预组合的问题，例如因子设计试验，推荐引擎(例如，为用户显示最大程度的参与度的一组电影)，医学中的组合疗法，选择ML模型的重要特征等等。当N和p增长时，进行N×2 ^ p个实验来估计各种参数是不可行的。而且，观测数据很可能存在混杂，即单位是否在组合下出现与其在该组合下的潜在结果相关。为了解决这些问题，我们提出了一种新颖的模型，它在单位和组合之间都施加了潜在结构。我们假设单位之间存在潜在的相似性(即类似单位的潜在结果是相似的)，并且组合之间也存在潜在的相似性(即类似组合的效果是相似的)。我们使用层次贝叶斯非参数模型来形式化这一点，该模型联合聚类单元和组合，并且足够灵活，可以模拟连续或离散结果。我们在模拟和实际数据上演示了所提出的方法，并表明它可以显着减少学习因果参数所需的实验数量。

    We consider a setting with $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \times 2^p$ causal parameters. Choosing combinations of interventions is a problem that naturally arises in many applications such as factorial design experiments, recommendation engines (e.g., showing a set of movies that maximizes engagement for users), combination therapies in medicine, selecting important features for ML models, etc. Running $N \times 2^p$ experiments to estimate the various parameters is infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. To address these challenges, we propose a novel model that imposes latent structure across both units and combinations. We assume latent similarity across units (i.e., the potential outco
    
[^24]: 使用标准化流的变分推理处理纵向数据

    Variational Inference for Longitudinal Data Using Normalizing Flows. (arXiv:2303.14220v1 [stat.ML])

    [http://arxiv.org/abs/2303.14220](http://arxiv.org/abs/2303.14220)

    该文提出一种基于标准化流的高维纵向数据生成模型，能够进行好的缺失数据插补操作。

    

    本文引入了一种新的潜变量生成模型，能够处理高维纵向数据，并依赖于变分推理。使用标准化流来对相关的潜变量的时间依赖关系进行建模。该方法可以用于生成完全合成的纵向序列，也可以用于生成在序列中与多个数据有关的轨迹，并且在缺失数据方面表现出良好的鲁棒性。我们在6个不同复杂度的数据集上测试了该模型，并显示出它可以实现更好的似然估计，以及更可靠的缺失数据插补。代码可在\url{https://github.com/clementchadebec/variational_inference_for_longitudinal_data}上获得。

    This paper introduces a new latent variable generative model able to handle high dimensional longitudinal data and relying on variational inference. The time dependency between the observations of an input sequence is modelled using normalizing flows over the associated latent variables. The proposed method can be used to generate either fully synthetic longitudinal sequences or trajectories that are conditioned on several data in a sequence and demonstrates good robustness properties to missing data. We test the model on 6 datasets of different complexity and show that it can achieve better likelihood estimates than some competitors as well as more reliable missing data imputation. A code is made available at \url{https://github.com/clementchadebec/variational_inference_for_longitudinal_data}.
    
[^25]: 带装载和覆盖约束的上下文幸存者问题：基于回归的模块化Lagrangian方法

    Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07484](http://arxiv.org/abs/2211.07484)

    该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。

    

    我们考虑一种上下文幸存者问题的变种，其中算法在总消费的线性约束下使用多个资源。这个问题推广了带背包的上下文幸存者问题(CBwK)，允许装载和覆盖约束，以及正负资源消耗。我们提出了一种新算法，简单、计算效率高，能够实现退化的后悔。当某些约束被违反时，对于CBwK，它在统计上是最优的。我们的算法基于LagrangianBwK(Immorlica等人，FOCS 2019)，这是一种面向CBwK的Lagrangian技术，以及SquareCB(Foster和Rakhlin，ICML 2020)，这是一种面向上下文幸存者的回归技术。我们的分析利用了两种技术本质上的模块化。

    We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
    
[^26]: 低次多项式张量分解的平均复杂度

    Average-Case Complexity of Tensor Decomposition for Low-Degree Polynomials. (arXiv:2211.05274v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2211.05274](http://arxiv.org/abs/2211.05274)

    该论文研究了低次多项式张量分解问题的平均计算复杂度，发现在$r \ll n^{3/2}$时存在多项式时间算法，但当$r \lesssim n^2$时，该问题只能在原则上恢复秩-1分量，是一个计算困难问题。

    

    假设我们给定一个由$r$个随机秩-1项组成的$n$维三阶对称张量$T \in (\mathbb{R}^n)^{\otimes 3}$，则当$r \lesssim n^2$时，可以在原则上恢复秩-1分量，但是仅在$r \ll n^{3/2}$的情况下才知道存在多项式时间算法。许多高维推断任务中存在类似的“统计计算差距”，近年来，研究人员通过针对统计查询（SQ），平方和（SoS）和低次多项式（LDP）等受限（但强大）的计算模型证明下限，以解释这些问题的计算难度。然而，在张量分解中不存在类似“植入对空”的测试问题来解释其难度，这也是目前不存在任何此类工作的原因之一。我们考虑了一个随机三阶张量分解模型，其中一个分量的范数略大于其余分量（以破坏对称性）。

    Suppose we are given an $n$-dimensional order-3 symmetric tensor $T \in (\mathbb{R}^n)^{\otimes 3}$ that is the sum of $r$ random rank-1 terms. The problem of recovering the rank-1 components is possible in principle when $r \lesssim n^2$ but polynomial-time algorithms are only known in the regime $r \ll n^{3/2}$. Similar "statistical-computational gaps" occur in many high-dimensional inference tasks, and in recent years there has been a flurry of work on explaining the apparent computational hardness in these problems by proving lower bounds against restricted (yet powerful) models of computation such as statistical queries (SQ), sum-of-squares (SoS), and low-degree polynomials (LDP). However, no such prior work exists for tensor decomposition, largely because its hardness does not appear to be explained by a "planted versus null" testing problem.  We consider a model for random order-3 tensor decomposition where one component is slightly larger in norm than the rest (to break symmetr
    
[^27]: 列表可学习性的表征

    A Characterization of List Learnability. (arXiv:2211.04956v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.04956](http://arxiv.org/abs/2211.04956)

    本文通过引入$k$-DS维度，完全表征了$k$-列表学习性，并指出当且仅当该假设类的$k$-DS维度有限，该假设类才$k$-列表可学习。

    

    学习理论中的一个经典结果表明，二元假设类的PAC可学习性与VC维度的有限性等效。将其扩展到多类别设置是一个待解决的问题，近期通过早期由丹尼尔和沙列夫-施瓦茨引入的DS维度，解决了这个问题。本文考虑列表PAC学习，其目标是输出k个预测结果。在多种设置下已经开发了列表学习算法，事实上，在最近的多类学习的表征中，列表学习扮演了重要的角色。本文旨在探讨以下问题：何时可以用列表学习算法学习假设类？我们通过一个名为$k$-DS维度的DS维度泛化完全表征$k$-列表学习性。通过对多类学习的最近表征进行泛化，我们表明，假设类$k$-列表可学习，当且仅当...

    A classical result in learning theory shows the equivalence of PAC learnability of binary hypothesis classes and the finiteness of VC dimension. Extending this to the multiclass setting was an open problem, which was settled in a recent breakthrough result characterizing multiclass PAC learnability via the DS dimension introduced earlier by Daniely and Shalev-Shwartz. In this work we consider list PAC learning where the goal is to output a list of $k$ predictions. List learning algorithms have been developed in several settings before and indeed, list learning played an important role in the recent characterization of multiclass learnability. In this work we ask: when is it possible to $k$-list learn a hypothesis class? We completely characterize $k$-list learnability in terms of a generalization of DS dimension that we call the $k$-DS dimension. Generalizing the recent characterization of multiclass learnability, we show that a hypothesis class is $k$-list learnable if and only if the
    
[^28]: LOT: 基于层内正交训练来提高$\ell_2$ 保护的鲁棒性。

    LOT: Layer-wise Orthogonal Training on Improving $\ell_2$ Certified Robustness. (arXiv:2210.11620v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11620](http://arxiv.org/abs/2210.11620)

    本文提出了一种基于层内正交训练的方法(LOT)，通过使用一个无限制矩阵来参数化正交矩阵来有效训练1-Lipschitz卷积层，并证明了半监督训练可以进一步提高利普希茨约束模型的可证明鲁棒性。在确定性l2可证明鲁棒性方面，LOT显著优于基线，并能够扩展到更深的神经网络。

    

    最近的研究表明，使用利普希茨约束训练深度神经网络（DNN）能够增强对抗性鲁棒性和其他模型特性，例如稳定性。在本文中，我们提出了一种基于层内正交训练方法（LOT）来有效训练1-Lipschitz卷积层，通过使用一个无限制矩阵来参数化一个正交矩阵。然后，我们通过将输入域转换为傅里叶频域来高效计算卷积核的平方根的逆。另一方面，由于现有研究表明半监督训练有助于提高经验上的鲁棒性，我们旨在弥合差距，并证明半监督学习也会提高利普希茨约束模型的可证明鲁棒性。我们在不同设置下对LOT进行了全面评估，并展示了LOT在确定性l2可证明鲁棒性方面显著优于基线，并能够扩展到更深的神经网络。在监督情况下，我们发现与纯监督训练相比，半监督训练可以进一步提高保护的鲁棒性。

    Recent studies show that training deep neural networks (DNNs) with Lipschitz constraints are able to enhance adversarial robustness and other model properties such as stability. In this paper, we propose a layer-wise orthogonal training method (LOT) to effectively train 1-Lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix. We then efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the Fourier frequency domain. On the other hand, as existing works show that semi-supervised training helps improve empirical robustness, we aim to bridge the gap and prove that semi-supervised learning also improves the certified robustness of Lipschitz-bounded models. We conduct comprehensive evaluations for LOT under different settings. We show that LOT significantly outperforms baselines regarding deterministic l2 certified robustness, and scales to deeper neural networks. Under the supervised scenario, we i
    
[^29]: Krylov-Bellman提升：在一般状态空间中超线性策略评估

    Krylov-Bellman boosting: Super-linear policy evaluation in general state spaces. (arXiv:2210.11377v1 [stat.ML] CROSS LISTED)

    [http://arxiv.org/abs/2210.11377](http://arxiv.org/abs/2210.11377)

    该论文提出了Krylov-Bellman Boosting (KBB)算法，用于在一般状态空间中进行策略评估，该算法具有超线性的收敛速度。

    

    我们提出并分析了Krylov-Bellman Boosting（KBB）算法，用于在一般状态空间中进行策略评估。该算法交替使用非参数回归（如提升）拟合Bellman残差，并使用随着时间增长而自适应增长的特征集合应用最小二乘时序差分（LSTD）过程来估计值函数。通过利用与Krylov方法的联系，我们为该方法提供了两种有吸引力的保证。首先，我们提供了一个通用的收敛界限，允许在残差拟合和LSTD计算中分别进行估计误差。与我们的数值实验一致，这个界限表明收敛速度取决于受限谱结构，通常是超线性的。其次，通过将这个元结果与残差拟合和LSTD计算的样本大小依赖保证相结合，我们得到了依赖于样本大小以及函数类复杂度的具体统计保证。

    We present and analyze the Krylov-Bellman Boosting (KBB) algorithm for policy evaluation in general state spaces. It alternates between fitting the Bellman residual using non-parametric regression (as in boosting), and estimating the value function via the least-squares temporal difference (LSTD) procedure applied with a feature set that grows adaptively over time. By exploiting the connection to Krylov methods, we equip this method with two attractive guarantees. First, we provide a general convergence bound that allows for separate estimation errors in residual fitting and LSTD computation. Consistent with our numerical experiments, this bound shows that convergence rates depend on the restricted spectral structure, and are typically super-linear. Second, by combining this meta-result with sample-size dependent guarantees for residual fitting and LSTD computation, we obtain concrete statistical guarantees that depend on the sample size along with the complexity of the function class 
    
[^30]: 使用逐个样本评估的条件互信息提出一类新的泛化界限

    A New Family of Generalization Bounds Using Samplewise Evaluated CMI. (arXiv:2210.06422v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06422](http://arxiv.org/abs/2210.06422)

    本文提出了一类新的信息论泛化界限，通过逐个样本评估的条件互信息，限制联合凸函数上界，与先前的界限相比在某些情况下更紧密地刻画了深度神经网络的总体损失。

    

    我们提出了一类新的信息论泛化界限，其中通过一个联合凸函数比较训练损失和总体损失。这个函数的上界通过分解、逐个样本评估的条件互信息（CMI）加以限制，这是一种与所选假设产生的损失有关而不是假设本身有关的信息度量，这在大多数近似正确性（PAC）- 贝叶斯结果中很常见。通过恢复和扩展之前已知的信息论界限，我们展示了这种框架的普适性。此外，使用评估的CMI，我们导出了Seeger的PAC-Bayesian界限的逐个样本的平均版本，其中凸函数是二元KL散度。在某些情况下，这种新的界限比先前的界限更紧密地刻画了深度神经网络的总体损失。最后，我们导出了一些这些平均界限的高概率版本。

    We present a new family of information-theoretic generalization bounds, in which the training loss and the population loss are compared through a jointly convex function. This function is upper-bounded in terms of the disintegrated, samplewise, evaluated conditional mutual information (CMI), an information measure that depends on the losses incurred by the selected hypothesis, rather than on the hypothesis itself, as is common in probably approximately correct (PAC)-Bayesian results. We demonstrate the generality of this framework by recovering and extending previously known information-theoretic bounds. Furthermore, using the evaluated CMI, we derive a samplewise, average version of Seeger's PAC-Bayesian bound, where the convex function is the binary KL divergence. In some scenarios, this novel bound results in a tighter characterization of the population loss of deep neural networks than previous bounds. Finally, we derive high-probability versions of some of these average bounds. We
    
[^31]: 机器学习与不变量理论

    Machine learning and invariant theory. (arXiv:2209.14991v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14991](http://arxiv.org/abs/2209.14991)

    本文介绍了等变机器学习，其中函数将与某个群作用相关，使用不可约表示或不变量理论来参数化这些函数的空间。 Malgrange的一般过程用来表达群$G$作用下所有多项式映射。

    

    在物理定律的启发下，等变机器学习将学习限制在假设空间中，其中所有函数都关于某个群作用等变。通常使用不可约表示或不变量理论来参数化这些函数的空间。本文介绍了这一主题，并解释了一些用于明确参数化等变函数的方法，这些方法在机器学习应用中被使用。特别是，我们详细说明了Malgrange的一般过程，给定较大空间上不变多项式的表征，表达群$G$作用下所有多项式映射，该方法还在$G$是紧Lie群的情况下参数化了光顺等变映射。

    Inspired by constraints from physical law, equivariant machine learning restricts the learning to a hypothesis class where all the functions are equivariant with respect to some group action. Irreducible representations or invariant theory are typically used to parameterize the space of such functions. In this article, we introduce the topic and explain a couple of methods to explicitly parameterize equivariant functions that are being used in machine learning applications. In particular, we explicate a general procedure, attributed to Malgrange, to express all polynomial maps between linear spaces that are equivariant under the action of a group $G$, given a characterization of the invariant polynomials on a bigger space. The method also parametrizes smooth equivariant maps in the case that $G$ is a compact Lie group.
    
[^32]: 带凸损失的风险感知线性赌博机

    Risk-aware linear bandits with convex loss. (arXiv:2209.07154v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.07154](http://arxiv.org/abs/2209.07154)

    本论文研究了带上下文的风险感知线性赌博机问题，提出了一种用于估计风险度量的置信序列，并提出了一种乐观的UCB算法以学习最佳的风险感知行为。

    

    在多臂赌博机等决策问题中，代理通过优化某种反馈进行顺序学习。虽然平均回报标准已被广泛研究，但反映对不良结果的厌恶的其他度量，例如方差、条件风险价值（CVaR），可能对关键应用（医疗保健、农业）有用。在没有上下文信息的赌博反馈下提出了用于此类风险感知度量的算法。在这项工作中，我们研究了具有上下文的赌徒，在这些赌博机反馈下，可以通过凸损失的最小化来提取这种风险度量作为上下文的线性函数。符合此框架的典型示例是expectile度量，它是通过不对称最小二乘问题的解得到的。使用卡曼超融合方法，我们导出了用于估计此类风险度量的置信序列。然后，我们提出了一种乐观的UCB算法，以学习最佳的风险感知行为。

    In decision-making problems such as the multi-armed bandit, an agent learns sequentially by optimizing a certain feedback. While the mean reward criterion has been extensively studied, other measures that reflect an aversion to adverse outcomes, such as mean-variance or conditional value-at-risk (CVaR), can be of interest for critical applications (healthcare, agriculture). Algorithms have been proposed for such risk-aware measures under bandit feedback without contextual information. In this work, we study contextual bandits where such risk measures can be elicited as linear functions of the contexts through the minimization of a convex loss. A typical example that fits within this framework is the expectile measure, which is obtained as the solution of an asymmetric least-square problem. Using the method of mixtures for supermartingales, we derive confidence sequences for the estimation of such risk measures. We then propose an optimistic UCB algorithm to learn optimal risk-aware act
    
[^33]: clusterBMA：用于聚类的贝叶斯模型平均法

    clusterBMA: Bayesian model averaging for clustering. (arXiv:2209.04117v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2209.04117](http://arxiv.org/abs/2209.04117)

    clusterBMA是一种结合多个非监督聚类算法结果的贝叶斯模型平均法，提供了组合聚类结构的概率解释和模型不确定性的量化。

    

    在组合聚类文献中，已经开发了各种方法来组合跨多个结果集的推断，以进行非监督聚类。通过从多个候选聚类模型中报告单个“最佳”模型的结果的方法通常忽略了由模型选择带来的不确定性，并且会产生对特定模型和参数敏感的推断。贝叶斯模型平均（BMA）是一种流行的方法，可用于在多个模型之间组合结果，并且在这种设置中提供了一些有吸引力的优点，包括组合聚类结构的概率解释和基于模型的不确定性的量化。在这项工作中，我们介绍了clusterBMA，一种使多个非监督聚类算法的结果进行加权模型平均的方法。我们使用聚类内部验证标准开发了后验模型概率的近似，用于加权每个模型的结果。

    Various methods have been developed to combine inference across multiple sets of results for unsupervised clustering, within the ensemble clustering literature. The approach of reporting results from one `best' model out of several candidate clustering models generally ignores the uncertainty that arises from model selection, and results in inferences that are sensitive to the particular model and parameters chosen. Bayesian model averaging (BMA) is a popular approach for combining results across multiple models that offers some attractive benefits in this setting, including probabilistic interpretation of the combined cluster structure and quantification of model-based uncertainty.  In this work we introduce clusterBMA, a method that enables weighted model averaging across results from multiple unsupervised clustering algorithms. We use clustering internal validation criteria to develop an approximation of the posterior model probability, used for weighting the results from each model
    
[^34]: 随机特征回归模型的最佳激活函数

    Optimal Activation Functions for the Random Features Regression Model. (arXiv:2206.01332v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.01332](http://arxiv.org/abs/2206.01332)

    本文确定了随机特征回归模型的最佳激活函数。这些函数可能是线性的、饱和线性函数或基于Hermite多项式的函数。使用最佳激活函数会影响RFR模型的重要特性，如双峰曲线和最佳正则化参数与噪声水平的相关性。

    

    近期已研究了随机特征回归模型(RFR)的渐近均方测试误差和灵敏度。我们在此基础上，通过不同的函数简洁概念，确定了在闭合形式下极小化RFR测试误差和灵敏度组合的激活函数(AF)族群。我们发现在某些场景下，最佳AF可以是线性的、饱和线性函数或基于Hermite多项式表示的函数。最后，我们展示了使用最佳AF如何影响RFR模型的重要特性，比如双峰曲线和其最佳正则化参数与观察噪声水平的相关性。

    The asymptotic mean squared test error and sensitivity of the Random Features Regression model (RFR) have been recently studied. We build on this work and identify in closed-form the family of Activation Functions (AFs) that minimize a combination of the test error and sensitivity of the RFR under different notions of functional parsimony. We find scenarios under which the optimal AFs are linear, saturated linear functions, or expressible in terms of Hermite polynomials. Finally, we show how using optimal AFs impacts well-established properties of the RFR model, such as its double descent curve, and the dependency of its optimal regularization parameter on the observation noise level.
    
[^35]: 监督学习的信息论框架

    An Information-Theoretic Framework for Supervised Learning. (arXiv:2203.00246v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00246](http://arxiv.org/abs/2203.00246)

    本文提出了一种信息论框架，分析机器学习的数据需求，研究了由具有ReLU激活单元的深度神经网络生成的数据的学习样本复杂度，提出了样本复杂度边界。

    

    每年，深度学习展示出更加新颖和优秀的经验结果，其中采用更深和更广的神经网络。同时，利用现有的理论框架，分析超过两层的神经网络是困难的，除非诉诸于计数参数或遭遇深度指数样本复杂度边界。因此，在不同的角度下分析现代机器学习可能是有成果的。本文提出了一种新的信息论框架，具有自己的遗憾和样本复杂度概念，用于分析机器学习的数据需求。我们首先通过一些经典案例，例如标量估计和线性回归，使用我们的框架建立直觉和介绍一般技术。然后，我们利用该框架研究了由具有ReLU激活单元的深度神经网络生成的数据的学习样本复杂度。对于权重的特定先验分布，我们建立了学习的样本复杂度边界。

    Each year, deep learning demonstrates new and improved empirical results with deeper and wider neural networks. Meanwhile, with existing theoretical frameworks, it is difficult to analyze networks deeper than two layers without resorting to counting parameters or encountering sample complexity bounds that are exponential in depth. Perhaps it may be fruitful to try to analyze modern machine learning under a different lens. In this paper, we propose a novel information-theoretic framework with its own notions of regret and sample complexity for analyzing the data requirements of machine learning. With our framework, we first work through some classical examples such as scalar estimation and linear regression to build intuition and introduce general techniques. Then, we use the framework to study the sample complexity of learning from data generated by deep neural networks with ReLU activation units. For a particular prior distribution on weights, we establish sample complexity bounds tha
    
[^36]: 具有随机噪声的在线广义线性回归的最优解及其在异方差Bandits中的应用

    Optimal Online Generalized Linear Regression with Stochastic Noise and Its Application to Heteroscedastic Bandits. (arXiv:2202.13603v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.13603](http://arxiv.org/abs/2202.13603)

    论文提出了一种具有随机噪声的在线广义线性回归问题的最优解，获得了接近最优的遗憾上限。同时，针对具有异方差噪声的广义线性Bandits问题，提出了一种基于FTRL的算法实现第一个方差感知的遗憾上限。

    

    我们研究了随机背景下在线广义线性回归问题，其中标签是由可能具有无界加性噪声的广义线性模型生成的。我们对经典的跟随正则化领袖（FTRL）算法进行了尖锐的分析，以应对标签噪声。具体而言，对于$\sigma$-子高斯标签噪声，我们的分析提供了一个遗憾的上限$O(\sigma^2 d \log T) + o(\log T)$，其中$d$是输入向量的维数, $T$ 是总回合数。我们还证明了随机在线线性回归的$\Omega(\sigma^2d\log(T/d))$下限，这表明我们的上限几乎是最优的。此外，我们将我们的分析扩展到了更精细的伯恩斯坦噪声条件。作为一个应用，我们研究了具有异方差噪声的广义线性Bandits，提出了一种基于FTRL的算法实现第一个方差感知的遗憾上限。

    We study the problem of online generalized linear regression in the stochastic setting, where the label is generated from a generalized linear model with possibly unbounded additive noise. We provide a sharp analysis of the classical follow-the-regularized-leader (FTRL) algorithm to cope with the label noise. More specifically, for $\sigma$-sub-Gaussian label noise, our analysis provides a regret upper bound of $O(\sigma^2 d \log T) + o(\log T)$, where $d$ is the dimension of the input vector, $T$ is the total number of rounds. We also prove a $\Omega(\sigma^2d\log(T/d))$ lower bound for stochastic online linear regression, which indicates that our upper bound is nearly optimal. In addition, we extend our analysis to a more refined Bernstein noise condition. As an application, we study generalized linear bandits with heteroscedastic noise and propose an algorithm based on FTRL to achieve the first variance-aware regret bound.
    
[^37]: PGMax: 用于离散概率图模型和JAX中的循环置信传播的因子图

    PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04110](http://arxiv.org/abs/2202.04110)

    PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。

    PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.

    PGMax是一个开源的Python包，用于轻松指定离散概率图模型（PGMs）作为因子图，并在JAX中自动运行高效且可扩展的循环置信传播（LBP）。PGMax支持具有可处理因子的一般因子图，并利用现代加速器（如GPU）进行推理。与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。PGMax还与快速增长的JAX生态系统无缝交互，开启了新的研究可能性。我们的源代码、示例和文档可在https://github.com/deepmind/PGMax上获得。

    PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
    
[^38]: 抗相关噪声注入用于提高泛化性能

    Anticorrelated Noise Injection for Improved Generalization. (arXiv:2202.02831v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.02831](http://arxiv.org/abs/2202.02831)

    本文发现，在一些目标函数中，抗相关噪声的梯度下降方法比传统的梯度下降和常规扰动梯度下降有更好的泛化性能。理论分析证明了这是因为 Anti-PGD 能够移动到更宽的最小值点，而 GD 和 PGD 会停滞在次优区域甚至发散。

    

    将人工噪声注入梯度下降常常被用于改善机器学习模型的性能。通常，这种扰动的梯度下降方法使用的是不相关的噪声。然而，目前尚不清楚是否使用不同类型的噪声能够提供更好的泛化性能。本文聚焦于相关的扰动。我们研究了各种目标函数，发现带有抗相关扰动的梯度下降（"Anti-PGD"）比传统的梯度下降和常规的（不相关的）扰动梯度下降有着更好的泛化性能。为了支持这些实验结果，我们还进行了理论分析，证明了 Anti-PGD 能够移动到更宽的最小值点，而 GD 和 PGD 会停滞在次优区域甚至发散。这一新颖的抗相关噪声与泛化性能的联系为训练机器学习模型提供了新的方法。

    Injecting artificial noise into gradient descent (GD) is commonly employed to improve the performance of machine learning models. Usually, uncorrelated noise is used in such perturbed gradient descent (PGD) methods. It is, however, not known if this is optimal or whether other types of noise could provide better generalization performance. In this paper, we zoom in on the problem of correlating the perturbations of consecutive PGD steps. We consider a variety of objective functions for which we find that GD with anticorrelated perturbations ("Anti-PGD") generalizes significantly better than GD and standard (uncorrelated) PGD. To support these experimental findings, we also derive a theoretical analysis that demonstrates that Anti-PGD moves to wider minima, while GD and PGD remain stuck in suboptimal regions or even diverge. This new connection between anticorrelated noise and generalization opens the field to novel ways to exploit noise for training machine learning models.
    
[^39]: 分布鲁棒的多类分类及其在深度图像分类器中的应用

    Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2109.12772v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.12772](http://arxiv.org/abs/2109.12772)

    本研究提出了一种适用于多类逻辑回归的分布鲁棒优化方法，在深度图像分类器中得到了应用。这种方法可使图像分类器对随机和对抗攻击具有鲁棒性。在使用MNIST和CIFAR-10数据集时，相比于基准方法，通过采用新的随机训练方法，测试错误率的降低高达83.5%，损失降低高达91.3%。

    

    我们提出了一种适用于多类逻辑回归的分布鲁棒优化（DRO）方法，可以容忍数据受到离群值的干扰。该DRO框架使用具有接近Wasserstein距离意义下的经验分布的分布球的概率模糊集来定义。我们将DRO形式化简为一个正则化的学习问题，其中正则化项是系数矩阵的范数。我们为我们模型的解决方案建立了样外性能保证，为我们控制预测误差的正则化器的作用提供了见解。我们将提出的方法应用于使基于深度Vision Transformer（ViT）的图像分类器对随机和对抗攻击具有鲁棒性。具体而言，我们使用MNIST和CIFAR-10数据集，通过采用一种新颖的随机训练方法，证明了测试错误率降低了高达83.5%，损失降低了高达91.3%与基线方法相比。

    We develop a Distributionally Robust Optimization (DRO) formulation for Multiclass Logistic Regression (MLR), which could tolerate data contaminated by outliers. The DRO framework uses a probabilistic ambiguity set defined as a ball of distributions that are close to the empirical distribution of the training set in the sense of the Wasserstein metric. We relax the DRO formulation into a regularized learning problem whose regularizer is a norm of the coefficient matrix. We establish out-of-sample performance guarantees for the solutions to our model, offering insights on the role of the regularizer in controlling the prediction error. We apply the proposed method in rendering deep Vision Transformer (ViT)-based image classifiers robust to random and adversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, we demonstrate reductions in test error rate by up to 83.5% and loss by up to 91.3% compared with baseline methods, by adopting a novel random training method.
    
[^40]: 矩阵计算的方差缩减及其在高斯过程中的应用

    Variance Reduction for Matrix Computations with Applications to Gaussian Processes. (arXiv:2106.14565v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.14565](http://arxiv.org/abs/2106.14565)

    本文提出了一种通过矩阵分解来进行矩阵计算的方差缩减方法，并展示了在某些问题上可以实现任意更好的随机性能。此外，本文提出的矩阵乘积迹的分解估计器与对半正定矩阵行列式的对数估计器均有显著的效率提高，可以在实际的贝叶斯优化问题中得到应用。

    

    随着计算速度和内存容量的不断提高，方法学上的进步也为随机模拟的性能带来了显著提高。本文聚焦于通过矩阵分解来进行矩阵计算的方差缩减。我们提供了现有方差缩减方法在估计大型矩阵元素时未能利用矩阵分解所带来的方差缩减，而如何通过计算矩阵的平方根分解，在某些情况下能够实现任意更好的随机性能。此外，我们提出了一种矩阵乘积迹的分解估计器，并在数值上证明，在某些高斯过程对数似然函数估计的问题上，该估计器的效率可以提高1000倍。此外，我们提供了一种对半正定矩阵行列式的对数估计器，其方差比文献中使用的标准估计器快速衰减。我们在实际的贝叶斯优化问题中说明了我们提出的方法的实用性。

    In addition to recent developments in computing speed and memory, methodological advances have contributed to significant gains in the performance of stochastic simulation. In this paper, we focus on variance reduction for matrix computations via matrix factorization. We provide insights into existing variance reduction methods for estimating the entries of large matrices. Popular methods do not exploit the reduction in variance that is possible when the matrix is factorized. We show how computing the square root factorization of the matrix can achieve in some important cases arbitrarily better stochastic performance. In addition, we propose a factorized estimator for the trace of a product of matrices and numerically demonstrate that the estimator can be up to 1,000 times more efficient on certain problems of estimating the log-likelihood of a Gaussian process. Additionally, we provide a new estimator of the log-determinant of a positive semi-definite matrix where the log-determinant 
    
[^41]: 场景不确定性与确定性图像分类器的惠灵顿后验

    Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.13870](http://arxiv.org/abs/2106.13870)

    本文提出了一种方法来估计确定性图像分类器在给定输入数据上结果的不确定性，定义了惠灵顿后验作为分布以表明可能由生成给定图像的相同场景生成的数据的响应。

    

    我们提出了一种方法来估计在给定输入数据上图像分类器结果的不确定性。用于图像分类的深度神经网络是从输入图像到输出类别的确定性映射。因此，它们在给定数据上的结果不涉及不确定性，因此必须在定义、测量和解释不确定性，并将“置信度”归因于结果时指定所引用的变化性。为此，我们介绍了惠灵顿后验，它是在可能由生成给定图像的相同场景生成的数据的响应中获得的结果分布。由于可以生成任何给定图像的无限多个场景，因此惠灵顿后验涉及来自除所描绘的场景之外的场景的归纳传递。我们探索了使用数据增强、丢弃、集成、单视图重建和模型线性化来计算惠灵顿后验。

    We propose a method to estimate the uncertainty of the outcome of an image classifier on a given input datum. Deep neural networks commonly used for image classification are deterministic maps from an input image to an output class. As such, their outcome on a given datum involves no uncertainty, so we must specify what variability we are referring to when defining, measuring and interpreting uncertainty, and attributing "confidence" to the outcome. To this end, we introduce the Wellington Posterior, which is the distribution of outcomes that would have been obtained in response to data that could have been generated by the same scene that produced the given image. Since there are infinitely many scenes that could have generated any given image, the Wellington Posterior involves inductive transfer from scenes other than the one portrayed. We explore the use of data augmentation, dropout, ensembling, single-view reconstruction, and model linearization to compute a Wellington Posterior. 
    
[^42]: 测量流形数据的核双样本检验

    Kernel Two-Sample Tests for Manifold Data. (arXiv:2105.03425v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.03425](http://arxiv.org/abs/2105.03425)

    本文研究了与最大均值差异（MMD）相关的基于核的双样本检验统计量在测量流形数据时的应用。文章展示了检验水平和功率与核带宽、样本数量和流形内在维度之间的关系，并在特定条件下建立了测试功率下界。

    

    我们在流形数据设置下研究了与最大均值差异（MMD）相关的基于核的双样本检验统计量，假设高维观测数据接近于低维流形。我们表征了测试水平和功率与核带宽、样本数量和流形的内在维度之间的关系。具体地，我们表明，当数据密度支持在一个嵌入到$m$维空间中的$d$维子流形$\mathcal{M}$上时，从服从于一对分布$p$和$q$抽取的数据进行核双样本检验，这对分布$ p $和$q$是具有H\"older阶$\beta$（最高2），样本数量$n$足够大，使得$\Delta_2\gtrsim n^{- {2\beta/(d+4\beta)}}$，其中$\Delta_2$是流形上$p$和$q$之间的平方$L^2$-差异。我们建立了一个足够大且有限$n$的测试功率下界，其中核带宽参数$\gamma$的比例尺度为$n^ {-1/(d+4\beta)}$。

    We present a study of a kernel-based two-sample test statistic related to the Maximum Mean Discrepancy (MMD) in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $p$ and $q$ that are H\"older with order $\beta$ (up to 2) is powerful when the number of samples $n$ is large such that $\Delta_2 \gtrsim n^{- { 2 \beta/( d + 4 \beta ) }}$, where $\Delta_2$ is the squared $L^2$-divergence between $p$ and $q$ on manifold. We establish a lower bound on the test power for finite $n$ that is sufficiently large, where the kernel bandwidth parameter $\gamma$ scales as $n^{
    
[^43]: MNL上下文Bandit问题的简便在线学习算法

    A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit. (arXiv:2011.14033v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.14033](http://arxiv.org/abs/2011.14033)

    本文提供了一个新颖的、不需要调整指数参数的MNL-Contextual Bandit问题的简便在线学习算法。算法具有与该问题的最佳理论界限匹配的遗憾上界。

    

    本文考虑了MNL-Bandit问题的上下文变体。更具体地说，我们考虑了一个动态集合优化问题，其中决策者向消费者提供一组产品（购物清单），并在每个回合观察响应。消费者购买产品以最大化他们的效用。我们假设一组属性描述了产品，产品的平均效用与这些属性的值呈线性关系。我们使用广泛使用的Multinomial Logit（MNL）模型建模消费者选择行为，并考虑在优化销售周期$T$内累积收益的同时动态学习模型参数的决策者问题。尽管这个问题近来引起了相当大的关注，但许多现有方法通常涉及解决一个难以处理的非凸优化问题。他们的理论性能保证取决于一个可能非常大的问题相关参数。特别地，现有方法需要调整随着属性集规模指数增长的调整参数。本文提供了一种新颖的MNL-Contextual Bandit问题的简便在线学习算法，它不需要调整此类指数参数。我们展示我们的算法具有与该问题的最佳理论界限匹配的遗憾上界。我们还通过模拟和真实世界实验证明了我们算法的有效性。

    In this paper, we consider the contextual variant of the MNL-Bandit problem. More specifically, we consider a dynamic set optimization problem, where a decision-maker offers a subset (assortment) of products to a consumer and observes the response in every round. Consumers purchase products to maximize their utility. We assume that a set of attributes describe the products, and the mean utility of a product is linear in the values of these attributes. We model consumer choice behavior using the widely used Multinomial Logit (MNL) model and consider the decision maker problem of dynamically learning the model parameters while optimizing cumulative revenue over the selling horizon $T$. Though this problem has attracted considerable attention in recent times, many existing methods often involve solving an intractable non-convex optimization problem. Their theoretical performance guarantees depend on a problem-dependent parameter which could be prohibitively large. In particular, existing 
    
[^44]: DeepTopPush: 一种简单可扩展的Accuracy at the Top方法

    DeepTopPush: Simple and Scalable Method for Accuracy at the Top. (arXiv:2006.12293v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.12293](http://arxiv.org/abs/2006.12293)

    DeepTopPush是一种用于解决Accuracy at the Top问题的简单而可扩展的方法，能有效地选择少量重要的样本，并在不同领域取得了优异的性能表现

    

    Accuracy at the top是一类特殊的二分类问题，其性能仅在少数相关（顶部）样本上评估。应用包括信息检索系统或需要手动（昂贵）后处理的工艺。这导致最小化超过阈值的无关样本数量。我们考虑以任意（深度）网络的形式构建分类器，并提出了一种新的DeepTopPush方法来最小化顶部的损失函数。由于阈值取决于所有样本，因此问题是不可分解的。我们修改了随机梯度下降以处理非可分解性，并提出了一种从当前迷你批次值和一个延迟值估计阈值的方法。我们展示了DeepTopPush在视觉识别数据集和两个真实应用中的优异性能。第一个应用程序选择少量分子进行进一步的药物测试。第二个应用程序使用了

    Accuracy at the top is a special class of binary classification problems where the performance is evaluated only on a small number of relevant (top) samples. Applications include information retrieval systems or processes with manual (expensive) postprocessing. This leads to minimizing the number of irrelevant samples above a threshold. We consider classifiers in the form of an arbitrary (deep) network and propose a new method DeepTopPush for minimizing the loss function at the top. Since the threshold depends on all samples, the problem is non-decomposable. We modify the stochastic gradient descent to handle the non-decomposability in an end-to-end training manner and propose a way to estimate the threshold only from values on the current minibatch and one delayed value. We demonstrate the excellent performance of DeepTopPush on visual recognition datasets and two real-world applications. The first one selects a small number of molecules for further drug testing. The second one uses r
    

