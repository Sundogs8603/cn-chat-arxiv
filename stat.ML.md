# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Understanding the Double Descent Phenomenon in Deep Learning](https://arxiv.org/abs/2403.10459) | 在现代深度学习中，庞大的过参数化模型通过增加模型复杂度来降低测试误差，这就是双下降现象。 |
| [^2] | [Structured Evaluation of Synthetic Tabular Data](https://arxiv.org/abs/2403.10424) | 提出了一个具有单一数学目标的评估框架，用于确定合成数据应该从与观测数据相同的分布中提取，并且推理了任何一组指标的完整性，统一了现有的指标，并鼓励新的模型无关基线和指标。 |
| [^3] | [Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination](https://arxiv.org/abs/2403.10416) | 我们提出了在Huber污染模型下进行高斯稀疏估计任务的鲁棒估计器，为均值估计、主成分分析和线性回归提供了具有最优误差保证的高效算法，同时引入了一种新颖的多维滤波方法。 |
| [^4] | [Conformal Predictions for Probabilistically Robust Scalable Machine Learning Classification](https://arxiv.org/abs/2403.10368) | 通过引入得分函数和定义适应性安全集，将可扩展分类器和适应性预测相结合，定义了一种可靠的学习框架，能够在设计初期就为分类提供稳健的算法。 |
| [^5] | [Rough Transformers for Continuous and Efficient Time-Series Modelling](https://arxiv.org/abs/2403.10288) | 提出了粗糙Transformer，用于在连续时间表示的输入序列上进行操作，大大降低了计算成本，对于处理医疗情境中的长程依赖性至关重要。 |
| [^6] | [Interpretable Machine Learning for Survival Analysis](https://arxiv.org/abs/2403.10250) | 可解释的机器学习在生存分析中的应用促进了透明度和公平性，揭示了模型的潜在偏见和限制，并提供了更符合数学原理的特征影响和风险因素预测方法。 |
| [^7] | [Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification](https://arxiv.org/abs/2403.10182) | 研究在工业零部件分类中探讨了利用更便宜的神经网络集成实现可靠的不确定性估计的方法 |
| [^8] | [A Short Survey on Importance Weighting for Machine Learning](https://arxiv.org/abs/2403.10175) | 重要性加权是统计学和机器学习中的基本程序，通过对目标函数或概率分布进行加权，可以保证监督学习在训练和测试分布之间差异的情况下具有统计上期望的性质 |
| [^9] | [Explainability through uncertainty: Trustworthy decision-making with neural networks](https://arxiv.org/abs/2403.10168) | 本文提出了一个通用的不确定性框架，将机器学习模型中的不确定性估计定位为XAI技术，并提供了解释输出结果时是否应该信任的方法。 |
| [^10] | [A Structure-Preserving Kernel Method for Learning Hamiltonian Systems](https://arxiv.org/abs/2403.10070) | 提出了一种保结构的核岭回归方法，可以从噪声观测数据中恢复哈密顿函数，拓展了核回归方法，并具有出色的数值性能和收敛速度。 |
| [^11] | [Multivariate Gaussian Approximation for Random Forest via Region-based Stabilization](https://arxiv.org/abs/2403.09960) | 该论文通过基于区域稳定性的方法，推导出了随机森林预测的高斯逼近界限，并建立了适用于各种相关统计问题的概率结果。 |
| [^12] | [Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors](https://arxiv.org/abs/2403.09869) | 开发了一族组感知先验分布，可以改进神经网络模型在数据分布的亚群体偏移下的泛化能力，并展示了即使只重新训练非鲁棒模型的最后一层，使用这种先验进行训练也能获得最先进的性能。 |
| [^13] | [Estimating the history of a random recursive tree](https://arxiv.org/abs/2403.09755) | 本文研究了估计随机递归树中顶点到达顺序的问题，提出了基于Jordan中心性度量的顺序估计器，并证明其几乎是最优的。 |
| [^14] | [A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage](https://arxiv.org/abs/2403.09701) | 混合强化学习算法中，通过将离线数据集包含在在线算法的经验重放缓冲区中进行启动，可以实现类似于基于离线数据分布引导在线探索的可证明收益，即使离线数据集没有单一策略可集中性。 |
| [^15] | [Variational Inference with Sequential Sample-Average Approximations](https://arxiv.org/abs/2403.09429) | VISA方法通过顺序样本均值逼近在计算密集型模型中实现近似推断，能够在保守选择学习率的情况下以较小的计算成本达到与标准方法相当的逼近精度。 |
| [^16] | [Max-sliced 2-Wasserstein distance](https://arxiv.org/abs/2403.02142) | 使用相同技术获得了紧支撑对称概率测度与其对称经验分布之间的期望最大切片2-瓦塞斯坦距离的上界 |
| [^17] | [Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms](https://arxiv.org/abs/2402.04952) | 本文提出了三个新的距离度量指标（s/c距离、马尔科夫距离和忠实度距离），用于评估因果推断算法的输出图与真实情况的分离/连接程度。 |
| [^18] | [Learning Markov State Abstractions for Deep Reinforcement Learning](https://arxiv.org/abs/2106.04379) | 引入了一组新颖条件，证明了学习马尔可夫抽象状态表示的充分性，并提出了结合逆模型估计和时间对比学习的实用训练过程，该方法适用于在线和离线训练，不依赖奖励信号但可以利用奖励信息。 |
| [^19] | [Debiasing Algorithm through Model Adaptation.](http://arxiv.org/abs/2310.18913) | 本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。 |
| [^20] | [How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?.](http://arxiv.org/abs/2310.08391) | 本文研究了在线性回归中的上下文学习，并发现有效的预训练只需要少量独立任务，预训练模型与贝叶斯最优算法接近。这些理论发现对ICL的统计基础提供了启示。 |
| [^21] | [Post-hoc Bias Scoring Is Optimal For Fair Classification.](http://arxiv.org/abs/2310.05725) | 本研究提出了一种后验偏差评分的方法，在满足公平性约束的情况下保持高准确性，并给出了基于偏差分数的修改规则。该方法适用于各种类型的公平性约束问题。 |
| [^22] | [Self-Compatibility: Evaluating Causal Discovery without Ground Truth.](http://arxiv.org/abs/2307.09552) | 本论文提出了一种在没有基准数据的情况下评估因果发现方法的新方法，通过在不同变量子集上学习的因果图之间的兼容性检测，来伪证因果关系的推断正确性。 |
| [^23] | [Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs.](http://arxiv.org/abs/2305.18702) | 本研究提出了一种新的深度生成模型来调整训练集中的随机样本，以使PDE解的残余在最小化时能保持平滑的轮廓，并通过引入对抗性损失项优化PINN模型，从而使神经网络学习稳定的解。同时本文还展示了该方法可以扩展到纳入最优传输约束，从而形成了将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。 |
| [^24] | [Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning.](http://arxiv.org/abs/2305.17297) | 本论文研究了具有低秩结构但非独立同分布数据的情况，在分离训练和测试分布的假设下，解决了分布偏移问题，实验结果表明，在分布偏移的情况下，本方法显著提高了泛化误差的性能。 |
| [^25] | [Goodness of fit by Neyman-Pearson testing.](http://arxiv.org/abs/2305.14137) | 本研究介绍了Neyman-Pearson检验在拟合优度研究中的应用，实现了一种名为NPLM的实用实现。和基于分类器方法相比，在探测到数据与期望分布的小偏差时，NPLM更灵敏且不会偏向任何类型的异常，比较适用于对撞机实验中对于新物理的不可知搜索。 future work 需要研究它在其他环境中的使用。 |
| [^26] | [MARS via LASSO.](http://arxiv.org/abs/2111.11694) | 本文提出了一种自然lasso变体的MARS方法，通过减少对维度的依赖来获得收敛率，并与使用平滑性约束的非参数估计技术联系在一起。 |

# 详细

[^1]: 深度学习中的双下降现象探究

    Understanding the Double Descent Phenomenon in Deep Learning

    [https://arxiv.org/abs/2403.10459](https://arxiv.org/abs/2403.10459)

    在现代深度学习中，庞大的过参数化模型通过增加模型复杂度来降低测试误差，这就是双下降现象。

    

    将经验风险最小化与容量控制相结合是机器学习中经典的策略，用于控制泛化差距并避免过拟合，因为模型类容量变大。然而，在现代深度学习实践中，非常庞大的过参数化模型（如神经网络）被优化以完美拟合训练数据，并且仍然可以获得良好的泛化性能。超越插值点后，增加模型复杂度似乎实际上会降低测试误差。在本教程中，我们解释了双下降的概念及其机制。第一部分建立了经典的统计学习框架并介绍了双下降现象。通过观察多个示例，第二部分介绍了归纳偏差，在双下降中选择平滑的经验风险最小化器起着关键作用。最后，第三部分探讨了t

    arXiv:2403.10459v1 Announce Type: new  Abstract: Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.   In this tutorial, we explain the concept of double descent and its mechanisms. The first section sets the classical statistical learning framework and introduces the double descent phenomenon. By looking at a number of examples, section 2 introduces inductive biases that appear to have a key role in double descent by selecting, among the multiple interpolating solutions, a smooth empirical risk minimizer. Finally, section 3 explores t
    
[^2]: 结构化评估合成表格数据

    Structured Evaluation of Synthetic Tabular Data

    [https://arxiv.org/abs/2403.10424](https://arxiv.org/abs/2403.10424)

    提出了一个具有单一数学目标的评估框架，用于确定合成数据应该从与观测数据相同的分布中提取，并且推理了任何一组指标的完整性，统一了现有的指标，并鼓励新的模型无关基线和指标。

    

    表格数据通常存在但往往不完整，数据量较小，并且由于隐私原因受限于访问。合成数据生成提供了潜在解决方案。存在许多用于评估合成表格式数据质量的指标；然而，我们缺乏对这些指标的客观、连贯的解释。为解决这一问题，我们提出了一个具有单一数学目标的评估框架，认为合成数据应该从与观测数据相同的分布中提取。通过对目标的各种结构分解，该框架首次允许我们推理任何一组指标的完整性，并统一现有的指标，包括源自忠实性考虑、下游应用和基于模型方法的指标。此外，该框架激励了无模型基线和一系列新的指标。我们评估了结构化信息合成器和合成器。

    arXiv:2403.10424v1 Announce Type: new  Abstract: Tabular data is common yet typically incomplete, small in volume, and access-restricted due to privacy concerns. Synthetic data generation offers potential solutions. Many metrics exist for evaluating the quality of synthetic tabular data; however, we lack an objective, coherent interpretation of the many metrics. To address this issue, we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data. Through various structural decomposition of the objective, this framework allows us to reason for the first time the completeness of any set of metrics, as well as unifies existing metrics, including those that stem from fidelity considerations, downstream application, and model-based approaches. Moreover, the framework motivates model-free baselines and a new spectrum of metrics. We evaluate structurally informed synthesizers and syn
    
[^3]: 在Huber污染模型下具有最优误差的高斯稀疏估计的鲁棒估计

    Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination

    [https://arxiv.org/abs/2403.10416](https://arxiv.org/abs/2403.10416)

    我们提出了在Huber污染模型下进行高斯稀疏估计任务的鲁棒估计器，为均值估计、主成分分析和线性回归提供了具有最优误差保证的高效算法，同时引入了一种新颖的多维滤波方法。

    

    我们在Huber的污染模型中研究了高斯稀疏估计任务，重点关注均值估计、主成分分析和线性回归。针对这些任务，我们提供了第一个样本和计算高效的鲁棒估计器，具有最优的误差保证，且在常数因子内。所有先前用于这些任务的高效算法都导致量化的次优误差。具体来说，对于在$\mathbb{R}^d$上带有污染率$\epsilon>0$的高斯稳健$k$-稀疏均值估计，我们的算法具有样本复杂度$(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$，在样本多项式时间内运行，并在$\ell_2$-误差为$O(\epsilon)$的情况下逼近目标均值。先前的高效算法固有地产生误差为$\Omega(\epsilon \sqrt{\log(1/\epsilon)})$。在技术层面上，我们开发了一种在稀疏领域中可能具有其他应用的新颖的多维滤波方法。

    arXiv:2403.10416v1 Announce Type: new  Abstract: We study Gaussian sparse estimation tasks in Huber's contamination model with a focus on mean estimation, PCA, and linear regression. For each of these tasks, we give the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors. All prior efficient algorithms for these tasks incur quantitatively suboptimal error. Concretely, for Gaussian robust $k$-sparse mean estimation on $\mathbb{R}^d$ with corruption rate $\epsilon>0$, our algorithm has sample complexity $(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$, runs in sample polynomial time, and approximates the target mean within $\ell_2$-error $O(\epsilon)$. Previous efficient algorithms inherently incur error $\Omega(\epsilon \sqrt{\log(1/\epsilon)})$. At the technical level, we develop a novel multidimensional filtering method in the sparse regime that may find other applications.
    
[^4]: 面向概率鲁棒可扩展机器学习分类的适应性预测

    Conformal Predictions for Probabilistically Robust Scalable Machine Learning Classification

    [https://arxiv.org/abs/2403.10368](https://arxiv.org/abs/2403.10368)

    通过引入得分函数和定义适应性安全集，将可扩展分类器和适应性预测相结合，定义了一种可靠的学习框架，能够在设计初期就为分类提供稳健的算法。

    

    适应性预测可以定义可靠且稳健的学习算法，从设计初期就为分类定义了可靠的学习框架，通过将可扩展分类器的概念与统计排序理论和概率学习理论联系起来。本文通过引入得分函数的新定义和定义一组特殊的输入变量，即适应性安全集，来分析可扩展分类器和适应性预测之间的相似之处，该安全集能够识别在输入空间中满足误差覆盖保证的模式，即对于属于该集合的点观察到错误（可能不安全）标签的概率受到预定义的$\varepsilon$错误水平的限制。

    arXiv:2403.10368v1 Announce Type: cross  Abstract: Conformal predictions make it possible to define reliable and robust learning algorithms. But they are essentially a method for evaluating whether an algorithm is good enough to be used in practice. To define a reliable learning framework for classification from the very beginning of its design, the concept of scalable classifier was introduced to generalize the concept of classical classifier by linking it to statistical order theory and probabilistic learning theory. In this paper, we analyze the similarities between scalable classifiers and conformal predictions by introducing a new definition of a score function and defining a special set of input variables, the conformal safety set, which can identify patterns in the input space that satisfy the error coverage guarantee, i.e., that the probability of observing the wrong (possibly unsafe) label for points belonging to this set is bounded by a predefined $\varepsilon$ error level. W
    
[^5]: 用于连续和高效时间序列建模的粗糙Transformer

    Rough Transformers for Continuous and Efficient Time-Series Modelling

    [https://arxiv.org/abs/2403.10288](https://arxiv.org/abs/2403.10288)

    提出了粗糙Transformer，用于在连续时间表示的输入序列上进行操作，大大降低了计算成本，对于处理医疗情境中的长程依赖性至关重要。

    

    在真实世界的医疗环境中，时间序列数据通常表现出长程依赖性，并且以不均匀间隔观察到。在这种情况下，传统的基于序列的循环模型很难处理。为了克服这一问题，研究人员用基于神经ODE的模型替换循环架构来建模非均匀采样的数据，并使用Transformer架构来考虑长程依赖。尽管这两种方法取得了成功，但对于中等长度及更长输入序列，两者都需要非常高的计算成本。为了缓解这一问题，我们引入了粗糙Transformer，这是Transformer模型的一种变体，其在输入序列的连续时间表示上运行，并且减少了计算成本，对于处理医疗情境中常见的长程依赖性至关重要。特别地，我们提出了多视图签名注意力，利用路径签名来增强传统的注意力。

    arXiv:2403.10288v1 Announce Type: cross  Abstract: Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attent
    
[^6]: 可解释的机器学习用于生存分析

    Interpretable Machine Learning for Survival Analysis

    [https://arxiv.org/abs/2403.10250](https://arxiv.org/abs/2403.10250)

    可解释的机器学习在生存分析中的应用促进了透明度和公平性，揭示了模型的潜在偏见和限制，并提供了更符合数学原理的特征影响和风险因素预测方法。

    

    随着黑盒机器学习模型的传播和快速进步，可解释的机器学习（IML）领域或可解释的人工智能（XAI）在过去十年中变得越来越重要。 这在生存分析领域尤为重要，其中采用IML技术促进了透明度、问责制和公平性，特别是在临床决策过程、有针对性疗法的开发、干预或其他医学或与医疗保健相关的环境中。 具体来说，可解释性可以揭示生存模型的潜在偏见和局限性，并提供更符合数学原理的方法来理解哪些特征对预测有影响或构成风险因素。 然而，缺乏即时可用的IML方法可能已经阻碍了医学从业者和公共卫生政策制定者充分利用机器学习的潜力。

    arXiv:2403.10250v1 Announce Type: cross  Abstract: With the spread and rapid advancement of black box machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade. This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and fairness in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts. More specifically, explainability can uncover a survival model's potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors. However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine lea
    
[^7]: 用更便宜的神经网络集成实现可靠的不确定性：工业零部件分类案例研究

    Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification

    [https://arxiv.org/abs/2403.10182](https://arxiv.org/abs/2403.10182)

    研究在工业零部件分类中探讨了利用更便宜的神经网络集成实现可靠的不确定性估计的方法

    

    在运筹学(OR)中，预测模型经常会遇到数据分布与训练数据分布不同的场景。近年来，神经网络(NNs)在图像分类等领域的出色性能使其在OR中备受关注。然而，当面对OOD数据时，NNs往往会做出自信但不正确的预测。不确定性估计为自信的模型提供了一个解决方案，当输出应(不应)被信任时进行通信。因此，在OR领域中，NNs中的可靠不确定性量化至关重要。由多个独立NNs组成的深度集合已经成为一种有前景的方法，不仅提供强大的预测准确性，还能可靠地估计不确定性。然而，它们的部署由于较大的计算需求而具有挑战性。最近的基础研究提出了更高效的NN集成，即sna

    arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
    
[^8]: 机器学习中的重要性加权简要调查

    A Short Survey on Importance Weighting for Machine Learning

    [https://arxiv.org/abs/2403.10175](https://arxiv.org/abs/2403.10175)

    重要性加权是统计学和机器学习中的基本程序，通过对目标函数或概率分布进行加权，可以保证监督学习在训练和测试分布之间差异的情况下具有统计上期望的性质

    

    重要性加权是统计学和机器学习中的一项基本程序，根据某种意义上实例的重要性对目标函数或概率分布进行加权。这一简单而有用的思想的广泛应用导致了许多重要性加权的应用。例如，据知，在关于训练和测试分布之间差异的假设下的监督学习，通过密度比的重要性加权可以保证统计上期望的性质。这项调查总结了机器学习和相关研究中重要性加权的广泛应用。

    arXiv:2403.10175v1 Announce Type: cross  Abstract: Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.
    
[^9]: 通过不确定性实现可解释性：神经网络中可信赖的决策制定

    Explainability through uncertainty: Trustworthy decision-making with neural networks

    [https://arxiv.org/abs/2403.10168](https://arxiv.org/abs/2403.10168)

    本文提出了一个通用的不确定性框架，将机器学习模型中的不确定性估计定位为XAI技术，并提供了解释输出结果时是否应该信任的方法。

    

    不确定性是任何机器学习模型的一个关键特征，尤其在神经网络中尤为重要，因为神经网络往往过于自信。不确定性在数据分布发生变化时尤为令人担忧，当数据分布偏离训练数据分布时，模型性能会悄无声息地下降。不确定性估计提供了解决过于自信模型的方法，指示何时应该（不应该）信任输出结果。虽然关于不确定性估计的方法已经得到发展，但尚未明确定义与可解释人工智能（XAI）领域的关系。此外，运筹学领域的文献忽略了不确定性估计的可操作性组成部分，并且未考虑到数据分布的变化。本文提出了一个通用的不确定性框架，贡献主要体现在三个方面：（i）将机器学习模型中的不确定性估计定位为XAI技术，提供局部的，模型特定的解释

    arXiv:2403.10168v1 Announce Type: new  Abstract: Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident. This overconfidence is worrying under distribution shifts, where the model performance silently degrades as the data distribution diverges from the training data distribution. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI). Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider distribution shifts. This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific expla
    
[^10]: 用于学习哈密顿系统的保结构核方法

    A Structure-Preserving Kernel Method for Learning Hamiltonian Systems

    [https://arxiv.org/abs/2403.10070](https://arxiv.org/abs/2403.10070)

    提出了一种保结构的核岭回归方法，可以从噪声观测数据中恢复哈密顿函数，拓展了核回归方法，并具有出色的数值性能和收敛速度。

    

    提出了一种保结构的核岭回归方法，允许从包含哈密顿向量场的噪声观测数据集中恢复潜在的高维非线性哈密顿函数。该方法提出了一个闭式解，在这一设置中表现出优秀的数值性能，超越了文献中提出的其他技术。从方法论的角度看，该论文扩展了核回归方法，解决需要包含梯度线性函数的损失函数的问题，特别地，在这一背景下证明了微分再现属性和表示定理。分析了保结构核估计器和高斯后验均值估计器之间的关系。进行了完整的误差分析，提供使用固定和自适应正则化参数的收敛速度。所提出方法的优良性能得到了确认。

    arXiv:2403.10070v1 Announce Type: cross  Abstract: A structure-preserving kernel ridge regression method is presented that allows the recovery of potentially high-dimensional and nonlinear Hamiltonian functions out of datasets made of noisy observations of Hamiltonian vector fields. The method proposes a closed-form solution that yields excellent numerical performances that surpass other techniques proposed in the literature in this setup. From the methodological point of view, the paper extends kernel regression methods to problems in which loss functions involving linear functions of gradients are required and, in particular, a differential reproducing property and a Representer Theorem are proved in this context. The relation between the structure-preserving kernel estimator and the Gaussian posterior mean estimator is analyzed. A full error analysis is conducted that provides convergence rates using fixed and adaptive regularization parameters. The good performance of the proposed 
    
[^11]: 通过基于区域稳定性的多元高斯逼近改进随机森林

    Multivariate Gaussian Approximation for Random Forest via Region-based Stabilization

    [https://arxiv.org/abs/2403.09960](https://arxiv.org/abs/2403.09960)

    该论文通过基于区域稳定性的方法，推导出了随机森林预测的高斯逼近界限，并建立了适用于各种相关统计问题的概率结果。

    

    我们在给定由泊松过程产生的一组训练点的情况下，推导了随机森林预测的高斯逼近界限，假设数据生成过程存在相当温和的正则性假设。我们的方法基于一个关键观察：随机森林的预测满足一定的称为基于区域稳定性的几何属性。在为随机森林开发结果的过程中，我们还为基于区域稳定的泊松过程的一般泛函建立了一个概率结果，这可能是独立感兴趣的。这一普遍结果利用了Malliavin-Stein方法，并且可能适用于各种相关的统计问题。

    arXiv:2403.09960v1 Announce Type: cross  Abstract: We derive Gaussian approximation bounds for random forest predictions based on a set of training points given by a Poisson process, under fairly mild regularity assumptions on the data generating process. Our approach is based on the key observation that the random forest predictions satisfy a certain geometric property called region-based stabilization. In the process of developing our results for the random forest, we also establish a probabilistic result, which might be of independent interest, on multivariate Gaussian approximation bounds for general functionals of Poisson process that are region-based stabilizing. This general result makes use of the Malliavin-Stein method, and is potentially applicable to various related statistical problems.
    
[^12]: 对亚群体偏移的鲁棒性改进：使用组感知先验

    Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors

    [https://arxiv.org/abs/2403.09869](https://arxiv.org/abs/2403.09869)

    开发了一族组感知先验分布，可以改进神经网络模型在数据分布的亚群体偏移下的泛化能力，并展示了即使只重新训练非鲁棒模型的最后一层，使用这种先验进行训练也能获得最先进的性能。

    

    机器学习模型在数据分布的亚群体偏移下往往表现不佳。开发能够让机器学习模型更好地泛化到这种偏移的方法对于在现实世界中安全部署至关重要。在这篇论文中，我们提出了一族针对神经网络参数的组感知先验（GAP）分布，明确支持在数据分布的亚群体偏移下泛化良好的模型。我们设计了一个简单的组感知先验，只需要访问一小部分包含组信息的数据，证明了在此先验下训练会获得最先进的性能——即使只重新训练先前训练的非鲁棒模型的最后一层。组感知先验在概念上简单，与现有方法（如属性伪标记和数据重新加权）互补，为利用贝叶斯推断以实现对亚群体偏移的鲁棒性开辟了有前景的新途径。

    arXiv:2403.09869v1 Announce Type: cross  Abstract: Machine learning models often perform poorly under subpopulation shifts in the data distribution. Developing methods that allow machine learning models to better generalize to such shifts is crucial for safe deployment in real-world settings. In this paper, we develop a family of group-aware prior (GAP) distributions over neural network parameters that explicitly favor models that generalize well under subpopulation shifts. We design a simple group-aware prior that only requires access to a small set of data with group information and demonstrate that training with this prior yields state-of-the-art performance -- even when only retraining the final layer of a previously trained non-robust model. Group aware-priors are conceptually simple, complementary to existing approaches, such as attribute pseudo labeling and data reweighting, and open up promising new avenues for harnessing Bayesian inference to enable robustness to subpopulation
    
[^13]: 估计随机递归树的历史

    Estimating the history of a random recursive tree

    [https://arxiv.org/abs/2403.09755](https://arxiv.org/abs/2403.09755)

    本文研究了估计随机递归树中顶点到达顺序的问题，提出了基于Jordan中心性度量的顺序估计器，并证明其几乎是最优的。

    

    本文研究了估计随机递归树中顶点到达顺序的问题。具体来说，我们研究了两个基本模型：均匀连接模型和线性优先连接模型。我们提出了一种基于Jordan中心性度量的顺序估计器，并定义了一族风险度量来量化排序过程的质量。此外，我们为这个问题建立了极小-最大下界，并证明所提出的估计器几乎是最优的。最后，我们通过数值实验表明所提出的估计器优于基于度数和谱排序程序。

    arXiv:2403.09755v1 Announce Type: cross  Abstract: This paper studies the problem of estimating the order of arrival of the vertices in a random recursive tree. Specifically, we study two fundamental models: the uniform attachment model and the linear preferential attachment model. We propose an order estimator based on the Jordan centrality measure and define a family of risk measures to quantify the quality of the ordering procedure. Moreover, we establish a minimax lower bound for this problem, and prove that the proposed estimator is nearly optimal. Finally, we numerically demonstrate that the proposed estimator outperforms degree-based and spectral ordering procedures.
    
[^14]: 一种对有限覆盖的混合RL在线算法的自然扩展

    A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage

    [https://arxiv.org/abs/2403.09701](https://arxiv.org/abs/2403.09701)

    混合强化学习算法中，通过将离线数据集包含在在线算法的经验重放缓冲区中进行启动，可以实现类似于基于离线数据分布引导在线探索的可证明收益，即使离线数据集没有单一策略可集中性。

    

    混合强化学习（RL）结合在线和离线数据，近年来引起了广泛关注，但关于其可证明益处的研究仍然很少。许多现有的混合RL算法对离线数据集施加覆盖假设，但我们表明这是不必要的。一个设计良好的在线算法应该在离线数据集中“填补空白”，探索行为策略未探索的状态和动作。与先前侧重于估计离线数据分布以引导在线探索的方法不同，我们表明对标准乐观在线算法的一个自然扩展——通过将离线数据集包含在经验重放缓冲区中来启动它们——即使离线数据集没有单一策略可集中性，也可实现混合数据的类似可证明收益。我们完成

    arXiv:2403.09701v1 Announce Type: new  Abstract: Hybrid Reinforcement Learning (RL), leveraging both online and offline data, has garnered recent interest, yet research on its provable benefits remains sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on the offline dataset, but we show that this is unnecessary. A well-designed online algorithm should "fill in the gaps" in the offline dataset, exploring states and actions that the behavior policy did not explore. Unlike previous approaches that focus on estimating the offline data distribution to guide online exploration (Li et al., 2023b), we show that a natural extension to standard optimistic online algorithms -- warm-starting them by including the offline dataset in the experience replay buffer -- achieves similar provable gains from hybrid data even when the offline dataset does not have single-policy concentrability. We accomplish
    
[^15]: 具有顺序样本均值逼近的变分推断

    Variational Inference with Sequential Sample-Average Approximations

    [https://arxiv.org/abs/2403.09429](https://arxiv.org/abs/2403.09429)

    VISA方法通过顺序样本均值逼近在计算密集型模型中实现近似推断，能够在保守选择学习率的情况下以较小的计算成本达到与标准方法相当的逼近精度。

    

    我们提出了一种具有顺序样本均值逼近（VISA）的变分推断方法，用于在计算密集型模型中进行近似推断，例如基于数值模拟的模型。VISA通过采用一系列样本均值逼近来扩展重要性加权的前向KL变分推断，这些逼近在信任区域内被视为有效。这使得可以在多个梯度步骤中重复使用模型评估，从而降低计算成本。我们在高维高斯分布、Lotka-Volterra动力学和Pickover吸引子上进行实验，结果表明，VISA可以在选择保守的学习率的情况下，以两倍或更高的计算节约达到与标准重要性加权前向KL变分推断相当的逼近精度。

    arXiv:2403.09429v1 Announce Type: cross  Abstract: We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.
    
[^16]: 最大切片2-瓦塞斯坦距离

    Max-sliced 2-Wasserstein distance

    [https://arxiv.org/abs/2403.02142](https://arxiv.org/abs/2403.02142)

    使用相同技术获得了紧支撑对称概率测度与其对称经验分布之间的期望最大切片2-瓦塞斯坦距离的上界

    

    这个笔记是作者在“最大切片瓦塞斯坦距离的尖锐界限”方面之前工作的延续。我们使用相同的技术获得紧支撑对称概率测度与其对称经验分布之间的期望最大切片2-瓦塞斯坦距离上界。

    arXiv:2403.02142v1 Announce Type: cross  Abstract: This note is a continuation of the author's previous work on ``Sharp bounds for the max-sliced Wasserstein distance." We use the same technique to obtain an upper bound for the expected max-sliced 2-Wasserstein distance between a compactly supported symmetric probability measure on a Euclidean space and its symmetrized empirical distribution.
    
[^17]: 评估因果推断算法的马尔科夫等价类指标

    Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms

    [https://arxiv.org/abs/2402.04952](https://arxiv.org/abs/2402.04952)

    本文提出了三个新的距离度量指标（s/c距离、马尔科夫距离和忠实度距离），用于评估因果推断算法的输出图与真实情况的分离/连接程度。

    

    许多最先进的因果推断方法旨在生成一个输出图，该图编码了生成数据过程的因果图的图形分离和连接陈述。在本文中，我们认为，对合成数据的因果推断方法进行评估应该包括分析该方法的输出与真实情况的分离/连接程度，以衡量这一明确目标的实现情况。我们证明现有的评估指标不能准确捕捉到两个因果图的分离/连接差异，并引入了三个新的距离度量指标，即s/c距离、马尔科夫距离和忠实度距离，以解决这个问题。我们通过玩具示例、实证实验和伪代码来补充我们的理论分析。

    Many state-of-the-art causal discovery methods aim to generate an output graph that encodes the graphical separation and connection statements of the causal graph that underlies the data-generating process. In this work, we argue that an evaluation of a causal discovery method against synthetic data should include an analysis of how well this explicit goal is achieved by measuring how closely the separations/connections of the method's output align with those of the ground truth. We show that established evaluation measures do not accurately capture the difference in separations/connections of two causal graphs, and we introduce three new measures of distance called s/c-distance, Markov distance and Faithfulness distance that address this shortcoming. We complement our theoretical analysis with toy examples, empirical experiments and pseudocode.
    
[^18]: 学习马尔可夫状态抽象以用于深度强化学习

    Learning Markov State Abstractions for Deep Reinforcement Learning

    [https://arxiv.org/abs/2106.04379](https://arxiv.org/abs/2106.04379)

    引入了一组新颖条件，证明了学习马尔可夫抽象状态表示的充分性，并提出了结合逆模型估计和时间对比学习的实用训练过程，该方法适用于在线和离线训练，不依赖奖励信号但可以利用奖励信息。

    

    强化学习在马尔可夫决策过程（MDPs）中的一个基本假设是，相关的决策过程实际上是马尔可夫的。然而，当MDPs具有丰富的观测时，代理通常通过抽象状态表示学习，这种表示未必能保持马尔可夫性质。我们引入了一组新颖的条件，并证明它们足以学习马尔可夫抽象状态表示。然后，我们描述了一个实用的训练过程，结合了逆模型估计和时间对比学习，以学习一个近似满足这些条件的抽象。我们的新颖训练目标适用于在线和离线训练：它不需要奖励信号，但当可用时，代理可以利用奖励信息。我们在一个视觉格子世界域和一组连续控制基准任务上对我们的方法进行了实证评估。

    arXiv:2106.04379v4 Announce Type: replace-cross  Abstract: A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmar
    
[^19]: 通过模型适应来去除偏见算法

    Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])

    [http://arxiv.org/abs/2310.18913](http://arxiv.org/abs/2310.18913)

    本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。

    

    大型语言模型正在成为各种语言任务的首选解决方案。然而，随着容量的增长，模型很容易依赖训练数据中存在的偏见和刻板印象所产生的虚假相关性。本研究提出了一种新颖的方法来检测和减轻语言模型中的性别偏见。我们进行因果分析，以识别问题模型组件，并发现中上层前馈层最容易传递偏见。根据分析结果，我们通过线性投影将这些层乘以模型进行适应。我们的方法DAMA通过各种度量指标明显减少了偏见，同时保持模型在后续任务中的性能。我们发布了我们的方法和模型的代码，通过重新训练，保持了LLaMA的最先进性能，同时偏见显著减少。

    Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
    
[^20]: 多少个预训练任务需要用于线性回归的上下文学习？

    How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?. (arXiv:2310.08391v1 [stat.ML])

    [http://arxiv.org/abs/2310.08391](http://arxiv.org/abs/2310.08391)

    本文研究了在线性回归中的上下文学习，并发现有效的预训练只需要少量独立任务，预训练模型与贝叶斯最优算法接近。这些理论发现对ICL的统计基础提供了启示。

    

    在多样任务上进行预训练的Transformer展现了非凡的上下文学习（ICL）能力，使其能够仅基于输入上下文解决未见任务，而无需调整模型参数。本文研究了其中最简单设置的ICL：预训练线性参数化的单层线性注意力模型，用于具有高斯先验的线性回归。我们为注意力模型预训练建立了一个统计任务复杂度界，表明有效的预训练只需要少量独立任务。此外，我们证明了预训练模型与贝叶斯最优算法非常接近，即几乎实现了固定上下文长度下未见任务的贝叶斯最优风险。这些理论发现对之前的实验研究进行了补充，并为ICL的统计基础提供了启示。

    Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.
    
[^21]: 后验偏差评分对公平分类最优

    Post-hoc Bias Scoring Is Optimal For Fair Classification. (arXiv:2310.05725v1 [stat.ML])

    [http://arxiv.org/abs/2310.05725](http://arxiv.org/abs/2310.05725)

    本研究提出了一种后验偏差评分的方法，在满足公平性约束的情况下保持高准确性，并给出了基于偏差分数的修改规则。该方法适用于各种类型的公平性约束问题。

    

    我们考虑了一个在群体公平性约束下的二元分类问题，该问题可以是人口统计学公平性（DP），机会均等（EOp）或等概率（EO）之一。我们提出了在公平性约束下贝叶斯最优分类器的明确特征化，结果是不受约束分类器的简单修改规则。即，我们引入了一种新的实例级别的偏差度量，称为偏差分数，而修改规则则是在有限量的偏差分数之上的简单线性规则。基于这个特征化，我们开发了一种后验方法，使我们能够适应公平性约束同时保持较高的准确性。在DP和EOp约束的情况下，修改规则是基于单个偏差分数的阈值选择，而在EO约束的情况下，我们需要调整具有2个参数的线性修改规则。该方法还可以用于包含多个敏感属性的复合群体公平性标准的情况。

    We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive att
    
[^22]: 自我兼容性：在没有基准数据的情况下评估因果发现的方法。

    Self-Compatibility: Evaluating Causal Discovery without Ground Truth. (arXiv:2307.09552v1 [cs.LG])

    [http://arxiv.org/abs/2307.09552](http://arxiv.org/abs/2307.09552)

    本论文提出了一种在没有基准数据的情况下评估因果发现方法的新方法，通过在不同变量子集上学习的因果图之间的兼容性检测，来伪证因果关系的推断正确性。

    

    鉴于因果基本事实非常罕见，因果发现算法通常只在模拟数据上进行评估。这令人担忧，因为模拟反映了关于噪声分布、模型类别等生成过程的常见假设。在这项工作中，我们提出了一种新的方法，用于在没有基准数据的情况下对因果发现算法的输出进行伪证。我们的关键见解是，尽管统计学习寻求数据点子集之间的稳定性，但因果学习应该寻求变量子集之间的稳定性。基于这个见解，我们的方法依赖于在不同变量子集上学习的因果图之间的兼容性概念。我们证明了检测不兼容性可以伪证因果关系被错误推断的原因，这是因为假设违反或有限样本效应带来的错误。虽然通过这种兼容性测试只是对良好性能的必要条件，但我们认为它提供了强有力的证据。

    As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidenc
    
[^23]: 对抗式自适应采样：将PINN和最优传输统一用于PDE近似

    Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs. (arXiv:2305.18702v1 [stat.ML])

    [http://arxiv.org/abs/2305.18702](http://arxiv.org/abs/2305.18702)

    本研究提出了一种新的深度生成模型来调整训练集中的随机样本，以使PDE解的残余在最小化时能保持平滑的轮廓，并通过引入对抗性损失项优化PINN模型，从而使神经网络学习稳定的解。同时本文还展示了该方法可以扩展到纳入最优传输约束，从而形成了将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。

    

    求解偏微分方程（PDE）是科学计算的一个核心任务。近年来，使用神经网络逼近PDE引起了越来越多的关注，具有无网格离散的灵活性和解决高维问题的潜力。一个基本的计算困难是训练集中的随机样本引入了统计错误，可能成为最终逼近中占主导的误差，从而掩盖了神经网络的建模能力。本文提出了一种新的minmax公式，同时优化近似的解和由深度生成模型提供的训练集中的随机样本。关键思想是使用深度生成模型调整训练集中的随机样本，使近似PDE解引起的残余在最小化时能保持平滑的轮廓。这种想法是通过在PINN优化过程中引入对抗性损失项来实现的，该损失项鼓励神经网络学习稳定的解，即使训练集中的样本有限或输入含噪声。我们进一步展示，所提出的方法可以自然地扩展到纳入最优传输约束，从而形成将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。

    Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust random samples in the training set such that the residual induced by the approximate PDE solution can maintain a smooth profile when it is being minimized. Such an idea is ach
    
[^24]: 无独立性的泛化误差：去噪、线性回归和迁移学习

    Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning. (arXiv:2305.17297v1 [cs.LG])

    [http://arxiv.org/abs/2305.17297](http://arxiv.org/abs/2305.17297)

    本论文研究了具有低秩结构但非独立同分布数据的情况，在分离训练和测试分布的假设下，解决了分布偏移问题，实验结果表明，在分布偏移的情况下，本方法显著提高了泛化误差的性能。

    

    研究线性模型在真实数据中的泛化能力是统计学习中的一个核心问题。先前的一些重要工作验证了理论工作与真实数据的相关性，但这些工作由于技术假设存在限制，这些假设包括具有良好条件的协方差矩阵以及具有独立同分布数据，这些假设在真实数据中并不一定成立。此外，以前的一些关于分布偏移的工作通常对训练和测试数据的联合分布进行技术假设，并且不在真实数据上进行测试。为了解决这些问题并更好地对真实数据进行建模，我们研究了具有低秩结构但非独立同分布数据的情况，同时通过分离训练和测试分布的假设来解决分布偏移问题。我们还在这些松弛的假设下，研究了去噪问题、线性回归和迁移学习。我们的实验结果表明，相比以前的方法，在分布偏移的情况下，我们的方法显著提高了泛化误差的性能。

    Studying the generalization abilities of linear models with real data is a central question in statistical learning. While there exist a limited number of prior important works (Loureiro et al. (2021A, 2021B), Wei et al. 2022) that do validate theoretical work with real data, these works have limitations due to technical assumptions. These assumptions include having a well-conditioned covariance matrix and having independent and identically distributed data. These assumptions are not necessarily valid for real data. Additionally, prior works that do address distributional shifts usually make technical assumptions on the joint distribution of the train and test data (Tripuraneni et al. 2021, Wu and Xu 2020), and do not test on real data.  In an attempt to address these issues and better model real data, we look at data that is not I.I.D. but has a low-rank structure. Further, we address distributional shift by decoupling assumptions on the training and test distribution. We provide anal
    
[^25]: Neyman-Pearson检验的拟合优度研究

    Goodness of fit by Neyman-Pearson testing. (arXiv:2305.14137v1 [hep-ph])

    [http://arxiv.org/abs/2305.14137](http://arxiv.org/abs/2305.14137)

    本研究介绍了Neyman-Pearson检验在拟合优度研究中的应用，实现了一种名为NPLM的实用实现。和基于分类器方法相比，在探测到数据与期望分布的小偏差时，NPLM更灵敏且不会偏向任何类型的异常，比较适用于对撞机实验中对于新物理的不可知搜索。 future work 需要研究它在其他环境中的使用。

    

    当备选假设$H_1$足够通用，既不引入重大偏差又避免过度拟合时，Neyman-Pearson策略可以用于拟合优度检验。在高能物理的背景下，一种名为NPLM的实用实现已被开发，旨在探测标准模型未预料到的新物理效应，我们在本文中将该方法与其他拟合优度方法进行了比较。

    The Neyman-Pearson strategy for hypothesis testing can be employed for goodness of fit if the alternative hypothesis $\rm H_1$ is generic enough not to introduce a significant bias while at the same time avoiding overfitting. A practical implementation of this idea (dubbed NPLM) has been developed in the context of high energy physics, targeting the detection in collider data of new physical effects not foreseen by the Standard Model. In this paper we initiate a comparison of this methodology with other approaches to goodness of fit, and in particular with classifier-based strategies that share strong similarities with NPLM. NPLM emerges from our comparison as more sensitive to small departures of the data from the expected distribution and not biased towards detecting specific types of anomalies while being blind to others. These features make it more suited for agnostic searches for new physics at collider experiments. Its deployment in other contexts should be investigated.
    
[^26]: MARS via LASSO.（arXiv:2111.11694v2 [math.ST] 已更新）

    MARS via LASSO. (arXiv:2111.11694v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2111.11694](http://arxiv.org/abs/2111.11694)

    本文提出了一种自然lasso变体的MARS方法，通过减少对维度的依赖来获得收敛率，并与使用平滑性约束的非参数估计技术联系在一起。

    

    多元自适应回归样条（Multivariate Adaptive Regression Splines，MARS）是Friedman在1991年提出的一种非参数回归方法。MARS将简单的非线性和非加性函数拟合到回归数据上。本文提出并研究了MARS方法的一种自然lasso变体。我们的方法是基于最小二乘估计，通过考虑MARS基础函数的无限维线性组合并强加基于变分的复杂度约束条件来获得函数的凸类。虽然我们的估计是定义为无限维优化问题的解，但其可以通过有限维凸优化来计算。在一些标准设计假设下，我们证明了我们的估计器仅在维度上对数收敛，因此在一定程度上避免了通常的维度灾难。我们还表明，我们的方法自然地与基于平滑性约束的非参数估计技术相联系。

    Multivariate adaptive regression splines (MARS) is a popular method for nonparametric regression introduced by Friedman in 1991. MARS fits simple nonlinear and non-additive functions to regression data. We propose and study a natural lasso variant of the MARS method. Our method is based on least squares estimation over a convex class of functions obtained by considering infinite-dimensional linear combinations of functions in the MARS basis and imposing a variation based complexity constraint. Our estimator can be computed via finite-dimensional convex optimization, although it is defined as a solution to an infinite-dimensional optimization problem. Under a few standard design assumptions, we prove that our estimator achieves a rate of convergence that depends only logarithmically on dimension and thus avoids the usual curse of dimensionality to some extent. We also show that our method is naturally connected to nonparametric estimation techniques based on smoothness constraints. We i
    

