# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning.](http://arxiv.org/abs/2310.01380) | 本论文提出了一种悲观非线性最小二乘值迭代算法（PNLSVI），用于离线强化学习中的非线性函数逼近问题。该算法具有创新的方差加权回归方案、方差估计子程序和悲观值迭代方法的规划阶段。 |
| [^2] | [Corrected generalized cross-validation for finite ensembles of penalized estimators.](http://arxiv.org/abs/2310.01374) | 本文研究了广义交叉验证（GCV）在有限惩罚估计器集合中估计预测风险的一致性问题，并提出了一种修正方法（CGCV）来解决这个问题。 |
| [^3] | [TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series.](http://arxiv.org/abs/2310.01327) | TACTiS-2是一种改进的多变量时间序列关注联合分布模型，采用了简化的目标函数和线性参数数量，具有更好的训练动态和最先进的性能。 |
| [^4] | [Optimal Estimator for Linear Regression with Shuffled Labels.](http://arxiv.org/abs/2310.01326) | 本文提出了一种用于解决具有打乱标签的线性回归问题的最优估计器。该估计器的复杂度不超过线性分配算法和最小二乘算法，并对信噪比要求进行了细分。 |
| [^5] | [Automated regime detection in multidimensional time series data using sliced Wasserstein k-means clustering.](http://arxiv.org/abs/2310.01285) | 本文通过研究使用Wasserstein k-means聚类算法在一维时间序列数据中的行为，并将其扩展到多维时间序列数据，通过切片Wasserstein距离进行多维制度检测。 |
| [^6] | [Non-Exchangeable Conformal Risk Control.](http://arxiv.org/abs/2310.01262) | 本文提出了一种非交换式共形风险控制的框架，可以在数据不可交换的情况下控制任何单调损失函数的期望值。 |
| [^7] | [Mirror Diffusion Models for Constrained and Watermarked Generation.](http://arxiv.org/abs/2310.01236) | 提出了一种新的镜像扩散模型（MDM），可以在受限制集合上生成数据而不丧失可追溯性。这通过在一个标准的欧几里得空间中学习扩散过程，并利用镜像映射来实现。 |
| [^8] | [A path-norm toolkit for modern networks: consequences, promises and challenges.](http://arxiv.org/abs/2310.01225) | 本文介绍了适用于现代神经网络的路径范数工具包，可以包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络。这个工具包恢复或超越了已知的路径范数界限，并挑战了基于路径范数的一些具体承诺。 |
| [^9] | [Unified Uncertainty Calibration.](http://arxiv.org/abs/2310.01202) | 该论文提出了一种统一的不确定性校准（U2C）框架，用于合并可知和认知不确定性，实现了面对困难样例时的准确预测和校准。 |
| [^10] | [SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping.](http://arxiv.org/abs/2310.01201) | SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。 |
| [^11] | [If there is no underfitting, there is no Cold Posterior Effect.](http://arxiv.org/abs/2310.01189) | 研究发现，当贝叶斯后验欠拟合时，模型失配会导致冷后验效应(CPE)在贝叶斯深度学习中出现。 |
| [^12] | [Parallel-in-Time Probabilistic Numerical ODE Solvers.](http://arxiv.org/abs/2310.01145) | 本文提出了一种并行时间概率数值ODE求解器，通过将数值模拟问题视为贝叶斯状态估计问题，并利用贝叶斯滤波和平滑的框架，实现了在并行处理所有时间步骤的同时将时间开销降低到对数级别。 |
| [^13] | [Prompt-tuning latent diffusion models for inverse problems.](http://arxiv.org/abs/2310.01110) | 本论文提出了一种使用文本到图像潜在扩散模型作为通用先验解决成像逆问题的方法。通过Prompt-tuning将文本嵌入进行实时优化，同时通过投影保持潜在变量的演化在编码器的范围空间内，使生成图像更符合扩散先验。这种综合方法，在超分辨率、去模糊和修复等各种任务中，优于基于图像和基于潜在扩散模型的逆问题求解器。 |
| [^14] | [Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models.](http://arxiv.org/abs/2310.01107) | 本论文提出了一种名为 Ground-A-Video 的基于引导的视频到视频转换框架，用于多属性视频编辑。该方法在没有训练的情况下实现了输入视频的时间一致的多属性编辑，并且解决了其他方法存在的问题。 |
| [^15] | [Energy-Guided Continuous Entropic Barycenter Estimation for General Costs.](http://arxiv.org/abs/2310.01105) | 本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。 |
| [^16] | [The Fisher-Rao geometry of CES distributions.](http://arxiv.org/abs/2310.01032) | 这篇论文介绍了费舍尔-拉奥信息几何在椭圆分布中的应用，包括协方差矩阵估计、内蕴Cram\'er-Rao界限和使用黎曼距离进行分类。 |
| [^17] | [Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients.](http://arxiv.org/abs/2310.01012) | 本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。 |
| [^18] | [A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression.](http://arxiv.org/abs/2310.00987) | 本研究通过推导任意有限秩核岭回归模型的尖锐的非渐近性上界和下界，填补了有限秩核岭回归保证的空白。 |
| [^19] | [Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits.](http://arxiv.org/abs/2310.00968) | 本文提出了一种新的算法，用于解决对决争夺中固有不确定性的问题，算法具有计算效率和方差感知遗憾界限。 |
| [^20] | [Improved Variational Bayesian Phylogenetic Inference using Mixtures.](http://arxiv.org/abs/2310.00941) | VBPI-Mixtures是一种改进的变分贝叶斯系统发育推断算法，通过使用混合学习技术来提高系统发育后验分布的准确性，并能够捕捉到传统算法未能建模的树形拓扑分布。 |
| [^21] | [Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP.](http://arxiv.org/abs/2310.00927) | 本文研究了CLIP中的可转移表示学习和零样本传递，提出了一个新的CLIP类型方法，在基准数据集上取得了更好的性能。 |
| [^22] | [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models.](http://arxiv.org/abs/2310.00902) | DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。 |
| [^23] | [Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models.](http://arxiv.org/abs/2310.00839) | 本文提出了一种使用深度生成模型和集成方法的井下特性表征方法。通过结合WGAN-GP和ES-MDA技术，实现了准确且高效的K场估计。这种方法在几个井下实例中得到了验证，并展示了未知K字段的主要特征。 |
| [^24] | [Learning to Make Adherence-Aware Advice.](http://arxiv.org/abs/2310.00817) | 本文提出了一种顺序决策模型，考虑了人的依从程度和机器提供建议的时机，并提供了学习算法来学习最佳的建议策略。 |
| [^25] | [Towards Causal Foundation Model: on Duality between Causal Inference and Attention.](http://arxiv.org/abs/2310.00809) | 该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。 |
| [^26] | [Identifying Copeland Winners in Dueling Bandits with Indifferences.](http://arxiv.org/abs/2310.00750) | 本文解决了决斗对比中的 Copeland 获胜者识别问题，利用三元反馈解决了传统决斗对比问题中未被充分研究的变体。我们提供了样本复杂度的下界，并提出了一种算法 POCOWISTA，该算法几乎达到了下界，展现出很好的实证性能。 |
| [^27] | [Spectral Neural Networks: Approximation Theory and Optimization Landscape.](http://arxiv.org/abs/2310.00729) | 这项研究探索了光谱神经网络（SNN）的关键理论方面，包括神经网络学习光谱几何信息的权衡和SNN优化路径的理论探索。 |
| [^28] | [Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems.](http://arxiv.org/abs/2310.00728) | 提出了一种基于物理信息的图神经网络（GNN）框架GraPhyR，用于解决电力系统的动态重构（DyR）问题。该框架将运营和连接约束直接融入GNN框架中，并进行端到端的训练，能够有效地优化DyR任务。 |
| [^29] | [Improving Length-Generalization in Transformers via Task Hinting.](http://arxiv.org/abs/2310.00726) | 该论文提出了一种基于任务提示的方法，在变压器模型中改善了长度泛化能力。通过同时训练模型处理简单但相关的辅助任务，可以显著提高模型在长序列数据上的性能。 |
| [^30] | [The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization.](http://arxiv.org/abs/2310.00692) | 本文对随机梯度下降（SGD）中的噪声几何进行了全面的理论研究，发现噪声与损失函数的局部几何特征有利的一致性。通过实验证明，SGD在逃脱尖锐极小值时与GD形成鲜明对比，逃脱方向在平坦方向上有显著分量。 |
| [^31] | [WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data.](http://arxiv.org/abs/2310.00646) | 本文提出了一种基于水印的框架WASA，通过允许大型语言模型生成带有嵌入源信息的合成文本水印来解决源归属和数据来源的问题。 |
| [^32] | [Discrete Choice Multi-Armed Bandits.](http://arxiv.org/abs/2310.00562) | 本文在离散选择模型和在线学习和多臂赌博算法之间建立了联系，提出了具有次线性遗憾界限的算法，并引入了一种灵活调整模型的新颖对抗性多臂赌博算法家族。 |
| [^33] | [Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks.](http://arxiv.org/abs/2310.00541) | 该论文提出了一种基于鲁棒非参数假设检验框架的新指标，用于衡量分类模型之间的相似度，该指标能够适用于从训练模型中派生的其他量。 |
| [^34] | [Thompson Exploration with Best Challenger Rule in Best Arm Identification.](http://arxiv.org/abs/2310.00539) | 本文提出了一种新的策略，将Thompson采样与最佳候选规则相结合，用于解决最佳臂识别问题。该策略在渐近情况下是最优的，并在一般的多臂赌博机问题中达到接近最优的性能。 |
| [^35] | [The objective function equality property of infoGAN for two-layer network.](http://arxiv.org/abs/2310.00443) | 这项研究证明了在infoGAN中，辨别器和生成器的样本数量趋向无穷时，两个目标函数变得等价。 |
| [^36] | [On the Stability of Iterative Retraining of Generative Models on their own Data.](http://arxiv.org/abs/2310.00429) | 本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。 |
| [^37] | [An Efficient Algorithm for Clustered Multi-Task Compressive Sensing.](http://arxiv.org/abs/2310.00420) | 本文提出了一种用于聚类多任务压缩感知的高效的算法，通过避免重复计算协方差矩阵，我们的算法在性能上显著优于现有算法，速度可提升上千倍，内存效率更高一个数量级。 |
| [^38] | [Linear Convergence of Pre-Conditioned PI Consensus Algorithm under Restricted Strong Convexity.](http://arxiv.org/abs/2310.00419) | 本文在点对点多智能体网络中提出了一种使用比例积分（PI）控制策略的预条件PI共识算法，保证了其在受限强凸函数下的线性收敛性，无需个体局部代价函数的凸性，并且通过引入局部预条件进一步加速算法。 |
| [^39] | [Order-Preserving GFlowNets.](http://arxiv.org/abs/2310.00386) | 本研究提出了保序GFlowNets（OP-GFNs），通过学习奖励函数与候选者的排序相一致的概率进行采样，解决了使用预定义标量奖励的局限性，同时提供了证明训练过程稀疏奖励景观的理论支持。 |
| [^40] | [Anomaly Detection in Power Generation Plants with Generative Adversarial Networks.](http://arxiv.org/abs/2310.00335) | 本研究探索了使用生成对抗网络（GANs）进行电力发电厂异常检测，结果表明GANs在这一领域具有有效性，并能帮助识别燃料消耗模式中的异常情况。 |
| [^41] | [Memorization with neural nets: going beyond the worst case.](http://arxiv.org/abs/2310.00327) | 本文研究了神经网络的插值问题，提出了一种简单的随机算法，在给定的数据集和两个类的情况下，能够以很高的概率构建一个插值的神经网络。这些结果与训练数据规模无关。 |
| [^42] | [Universality of max-margin classifiers.](http://arxiv.org/abs/2310.00176) | 该论文研究了最大间隔分类器在非高斯特征映射和高维渐近条件下的普适性问题。 |
| [^43] | [On the Disconnect Between Theory and Practice of Overparametrized Neural Networks.](http://arxiv.org/abs/2310.00137) | 本文研究了神经网络在无穷宽度极限下的行为，并与核方法建立了联系。虽然在合成架构中展示了一些优势，如更快的优化和可靠的不确定性量化，但实际相关的架构需要比深度大很多倍的宽度才能实现这些优势。 |
| [^44] | [Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit.](http://arxiv.org/abs/2310.00110) | GUESS is a new sampling strategy for global fit that combines predictive posterior uncertainty and higher-order Taylor expansion values to reduce the number of samples needed for accurate surrogate modeling. - GUESS 是一种新的全局拟合采样策略，结合了预测后验不确定性和高阶泰勒展开值，可以减少准确的代理模型所需的样本数量。 |
| [^45] | [Federated Learning with Differential Privacy for End-to-End Speech Recognition.](http://arxiv.org/abs/2310.00098) | 本文提出了一种基于联邦学习和差分隐私的端到端语音识别方法，探索了大型Transformer模型的不同方面，并建立了基线结果。 |
| [^46] | [Pointwise uncertainty quantification for sparse variational Gaussian process regression with a Brownian motion prior.](http://arxiv.org/abs/2310.00097) | 本文研究了稀疏变分高斯过程回归中点估计和不确定性量化的方法，通过使用布朗运动先验和特征向量引导变量，推导了频率派可信区间的理论保证和限制，并在足够多的引导变量的情况下精确定义了渐近频率派覆盖，从而推断出这个变分方法的可信区间何时保守，何时过于自信/误导。数值实验证明了这些结果的适用性，并讨论了与其他常见高斯过程先验的相关性。 |
| [^47] | [Unlabeled Out-Of-Domain Data Improves Generalization.](http://arxiv.org/abs/2310.00027) | 这个论文提出了一种新的框架，可以将无标记的域外数据纳入半监督分类问题，从而改善泛化能力。该框架结合了分布鲁棒优化与自监督训练，并利用了高效的多项式时间算法。在理论上，该框架在高斯混合分类问题中得到了验证。 |
| [^48] | [Stackelberg Batch Policy Learning.](http://arxiv.org/abs/2309.16188) | Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。 |
| [^49] | [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.](http://arxiv.org/abs/2309.15111) | 本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。 |
| [^50] | [A Model-Agnostic Graph Neural Network for Integrating Local and Global Information.](http://arxiv.org/abs/2309.13459) | MaGNet是一种模型无关的图神经网络框架，能够顺序地整合不同顺序的信息，并通过识别有影响力的紧凑图结构提供有意义且可解释的结果。 |
| [^51] | [Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework.](http://arxiv.org/abs/2309.13278) | 本研究提出了一个对于强化学习非同策略评估问题的统一误差量化框架，并解决了分布偏移的挑战。通过在一个单一的区间内共同量化两个估计误差源，该框架揭示了之前隐藏的误差权衡，从而提高了置信区间的准确性。 |
| [^52] | [Statistical Hypothesis Testing for Information Value (IV).](http://arxiv.org/abs/2309.13183) | 该论文提出了信息价值（IV）的统计假设检验方法，为模型建立前的特征选择提供了理论框架，并通过实验证明了该方法的有效性。 |
| [^53] | [Sharpness-Aware Minimization and the Edge of Stability.](http://arxiv.org/abs/2309.12488) | 本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。 |
| [^54] | [$\mathbb{T}$-Stochastic Graphs.](http://arxiv.org/abs/2309.01301) | 本文研究了社交网络中层次聚类的统计方法，并提出了一种新的概率模型$\mathbb{T}$-随机图，用于解决现有方法中存在的不稳定性问题。 |
| [^55] | [On the Implicit Bias of Adam.](http://arxiv.org/abs/2309.00079) | 本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。 |
| [^56] | [Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals.](http://arxiv.org/abs/2308.14945) | 本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。 |
| [^57] | [Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach.](http://arxiv.org/abs/2307.02037) | 本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。 |
| [^58] | [The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets.](http://arxiv.org/abs/2306.14975) | 本文研究了复杂数据集中的底层缩放定律和普适统计结构。通过将数据类比为物理系统，并应用统计物理学和随机矩阵理论的方法，揭示了特征-特征协方差矩阵的局部和全局特征值统计量的规律。研究发现，在无关随机数据和真实数据之间存在显著差异，并且可以通过引入长程相关性完全恢复缩放行为。同时，生成的数据和真实世界数据都属于混沌系统，并在较小的数据集大小上即可体现随机矩阵理论的统计行为。 |
| [^59] | [Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models.](http://arxiv.org/abs/2306.09251) | 该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。 |
| [^60] | [Noise Stability Optimization for Flat Minima with Optimal Convergence Rates.](http://arxiv.org/abs/2306.08553) | 本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。 |
| [^61] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^62] | [Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation.](http://arxiv.org/abs/2306.00788) | 本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。 |
| [^63] | [A Geometric Perspective on Diffusion Models.](http://arxiv.org/abs/2305.19947) | 本文研究了扩散模型的几何结构，发现通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接了数据分布和噪声分布，建立了基于ODE的最优采样和经典的均值漂移算法之间的理论关系。 |
| [^64] | [Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks.](http://arxiv.org/abs/2305.19243) | 通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。 |
| [^65] | [How to Query Human Feedback Efficiently in RL?.](http://arxiv.org/abs/2305.18505) | 该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。 |
| [^66] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^67] | [Learning Two-Layer Neural Networks, One (Giant) Step at a Time.](http://arxiv.org/abs/2305.18270) | 本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。 |
| [^68] | [The minimax risk in testing the histogram of discrete distributions for uniformity under missing ball alternatives.](http://arxiv.org/abs/2305.18111) | 研究了离散分布样本对于类别间的均匀分布拟合问题下的极小极大风险，在缺少球形替代方案的情况下进行了讨论，通过离散直方图进行检验，获得了一种具有精确刻画的检验方法，并在实证研究中表现出了显著性。 |
| [^69] | [Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching.](http://arxiv.org/abs/2305.17884) | 本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。 |
| [^70] | [Provable Offline Reinforcement Learning with Human Feedback.](http://arxiv.org/abs/2305.14816) | 本文提出了一种具有人类反馈的离线强化学习算法，解决了如何估计隐式奖励以及在置信集周围解决规划问题的方法。此外，作者提出了一个能够使用多项式数量的样本学习任何目标策略的新保证，同时引入了一个新的单策略集中系数来衡量目标策略的覆盖范围。 |
| [^71] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^72] | [A Variational Perspective on Solving Inverse Problems with Diffusion Models.](http://arxiv.org/abs/2305.04391) | 该论文提出了一种通过去噪扩散过程自然地导致正则化的变分方法（RED-Diff），可用于解决不同反问题。加权机制可以衡量不同时间步长的去噪器的贡献。 |
| [^73] | [Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling.](http://arxiv.org/abs/2304.07665) | 本文提出了一个新方法，利用贝叶斯分层建模，动态平衡探索-开发权衡，以更好地查询数据点。 |
| [^74] | [OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems.](http://arxiv.org/abs/2304.06686) | 本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。 |
| [^75] | [Energy-guided Entropic Neural Optimal Transport.](http://arxiv.org/abs/2304.06094) | 本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。 |
| [^76] | [ChemCrow: Augmenting large-language models with chemistry tools.](http://arxiv.org/abs/2304.05376) | 本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。 |
| [^77] | [A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation.](http://arxiv.org/abs/2304.02858) | 本文研究了集成学习和数据增强方法的应用，针对类别不平衡问题，通过计算评估，找到了最有效的组合。 |
| [^78] | [Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization.](http://arxiv.org/abs/2303.16721) | 本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。 |
| [^79] | [Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients.](http://arxiv.org/abs/2303.05341) | 通过引入罚函数，我们提出了一种创新的深度部分线性Cox模型，用于在肺癌患者的CT扫描中分析死亡风险。该模型能有效地整合已知和新兴的风险因素，解决了参数维度超出样本大小和非参数建模中维度灾难的问题。 |
| [^80] | [Efficient Explorative Key-term Selection Strategies for Conversational Contextual Bandits.](http://arxiv.org/abs/2303.00315) | 本研究提出了一种通用框架“ConLinUCB”来解决对话式情境马尔可夫决策过程中信息整合和探索性关键词选择的问题，以加速用户偏好估计的收敛速度。 |
| [^81] | [mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization.](http://arxiv.org/abs/2302.09693) | mSAM是一种深度学习优化方法，通过在训练过程中聚合对抗性扰动得到的更新，从理论上证明了比传统方法更平的极小值点，实验证实了其在各种任务上的优越性能。 |
| [^82] | [Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes.](http://arxiv.org/abs/2302.07477) | 这篇论文研究了对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度理论。作者发现，在混合的情况下，最优样本复杂度依赖于总变异混合时间、折扣因子和解误差容忍度。 |
| [^83] | [Quasi-optimal Reinforcement Learning with Continuous Actions.](http://arxiv.org/abs/2301.08940) | 本研究提出了一种准最优学习算法，用于解决强化学习中连续动作环境下的决策问题，特别适用于医疗应用中确定最佳剂量水平的问题。 |
| [^84] | [Support Vector Regression: Risk Quadrangle Framework.](http://arxiv.org/abs/2212.09178) | 本文结合风险四方理论，研究了支持向量回归（SVR）。研究结果发现，SVR的两种形式对应于等效误差度量的最小化，同时加上正则化惩罚项。通过构造基本风险四方框，我们证明了SVR是对两个对称条件分位数的平均数的渐近无偏估计量。此外，我们证明了$\varepsilon$-SVR和$\nu$-SVR在一般随机环境下的等价性。 |
| [^85] | [Physics-informed neural networks with unknown measurement noise.](http://arxiv.org/abs/2211.15498) | 这篇论文提出了一种解决物理信息神经网络在存在非高斯噪声情况下失效的问题的方法，即通过同时训练一个能量模型来学习正确的噪声分布。通过多个例子的实验证明了该方法的改进性能。 |
| [^86] | [Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology.](http://arxiv.org/abs/2211.08939) | 本文提出了增强型物理知识编码神经网络(APINN)，采用软领域分解和参数共享，通过门控网络初始化和一般领域和函数分解来改进了扩展物理知识编码神经网络(XPINN)和基本物理知识编码神经网络(PINN)的泛化能力。 |
| [^87] | [Latent Multimodal Functional Graphical Model Estimation.](http://arxiv.org/abs/2210.17237) | 本研究提出了一个潜在多模态功能图模型估计的新框架，通过同时估计转换算子和潜在图来填补当前科学方法在估计多模态功能数据图模型方面的空白 |
| [^88] | [Continuous-in-time Limit for Bayesian Bandits.](http://arxiv.org/abs/2210.07513) | 本文提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略，并且其计算成本不包括依赖于时间长度的项。 |
| [^89] | [Normalised clustering accuracy: An asymmetric external cluster validity measure.](http://arxiv.org/abs/2209.02935) | 本文提出了一种非对称的外部聚类有效度量方法，旨在区分不同任务类型上表现良好和系统性表现不佳的聚类算法。与传统的内部度量不同，该方法利用参考真实分组进行评估，并弥补了现有方法在最坏情况下的误差。 |
| [^90] | [Federated Learning with Uncertainty via Distilled Predictive Distributions.](http://arxiv.org/abs/2206.07562) | 本论文提出了一种联邦学习的不确定性框架，每个客户端在每轮中推断其参数的后验分布和后验预测分布，并将其蒸馏为单一的深度神经网络发送给服务器。这种方法可以解决现有联邦学习方法无法估计模型不确定性的问题，并在有限数据环境下取得更准确的预测。 |
| [^91] | [Superiority of GNN over NN in generalizing bandlimited functions.](http://arxiv.org/abs/2206.05904) | 本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。 |
| [^92] | [Decoupled Self-supervised Learning for Non-Homophilous Graphs.](http://arxiv.org/abs/2206.03601) | 本文提出了一种用于非同态图的解耦自监督学习（DSSL）框架。通过模拟节点和链接的生成过程，将不同邻域之间的不同潜在语义解耦到自监督学习过程中。该框架对编码器不敏感，并且不需要预制的增强，对不同的图具有灵活性。 |
| [^93] | [Minimising the Expected Posterior Entropy Yields Optimal Summary Statistics.](http://arxiv.org/abs/2206.02340) | 该论文介绍了从大型数据集中提取低维摘要统计量的重要性，提出了通过最小化后验熵来获取最优摘要统计量的方法，并提供了实践建议和示例验证。 |
| [^94] | [A PAC-Bayes oracle inequality for sparse neural networks.](http://arxiv.org/abs/2204.12392) | 这篇论文研究了在非参数回归设置中利用稀疏深度神经网络的Gibbs后验分布，通过Metropolis-adjusted Langevin算法可以访问后验分布。通过对网络权重的稀疏集合进行统一先验的混合，证明了该方法能够适应未知的正则性和层次结构的回归函数，并达到了极小化最优收敛速率（除了对数因子）。 |
| [^95] | [Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records.](http://arxiv.org/abs/2110.09680) | 本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。 |
| [^96] | [Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach.](http://arxiv.org/abs/2109.12701) | 本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。 |
| [^97] | [Predicting Inflation with Recurrent Neural Networks.](http://arxiv.org/abs/2104.03757) | 本文应用LSTM循环神经网络模型，研究了预测通货膨胀的能力，并发现在长期预测和宏观经济不确定性加剧期间表现良好。有趣的是，LSTM所涉及的因素与商业周期指标高度相关，这说明这些信号作为通胀预测因子的实用性。 |
| [^98] | [Revisiting minimum description length complexity in overparameterized models.](http://arxiv.org/abs/2006.10189) | 本文重审了超参数模型中的最小描述长度复杂度。通过定义一个新的基于MDL的复杂度度量，我们发现复杂度不仅取决于参数数量，还与设计矩阵或核矩阵的奇异值和信噪比有关。 |
| [^99] | [Understanding the Difficulty of Training Transformers.](http://arxiv.org/abs/2004.08249) | 该论文研究了Transformer训练的困难。他们发现不平衡的梯度不是训练不稳定的根本原因，而是每一层的放大效应导致训练不稳定。他们观察到轻量级的依赖限制了模型潜力，导致表现较差的训练模型。 |
| [^100] | [Distributed Multivariate Regression Modeling For Selecting Biomarkers Under Data Protection Constraints.](http://arxiv.org/abs/1803.00422) | 该论文提出了一种基于分布式多元回归建模的选择生物标志物的方法，解决了数据保护约束的问题，并克服了传统方法在处理大量生物标志物时丢失信息的局限性。 |

# 详细

[^1]: 悲观非线性最小二乘值迭代算法用于离线强化学习

    Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning. (arXiv:2310.01380v1 [cs.LG])

    [http://arxiv.org/abs/2310.01380](http://arxiv.org/abs/2310.01380)

    本论文提出了一种悲观非线性最小二乘值迭代算法（PNLSVI），用于离线强化学习中的非线性函数逼近问题。该算法具有创新的方差加权回归方案、方差估计子程序和悲观值迭代方法的规划阶段。

    

    离线强化学习（Offline RL）是指智能体根据由行为策略收集的数据学习最优策略的任务，近年来引起了越来越多的关注。虽然在线性函数逼近下的离线强化学习已经得到了广泛研究，并在一定假设下取得了最优结果，但很多研究将兴趣转向了非线性函数逼近下的离线强化学习。然而，关于非线性函数逼近下的离线强化学习的具有实例依赖后悔保证的研究工作却很有限。在本文中，我们提出了一种名为悲观非线性最小二乘值迭代（PNLSVI）的高效算法，用于非线性函数逼近下的离线强化学习。我们的算法设计包括三个创新的组成部分：（1）一种基于方差加权回归的方案，适用于广泛的函数类；（2）一种方差估计子程序；和（3）一个利用悲观值迭代方法的规划阶段。

    Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. O
    
[^2]: 有限惩罚估计器集合的修正广义交叉验证

    Corrected generalized cross-validation for finite ensembles of penalized estimators. (arXiv:2310.01374v1 [math.ST])

    [http://arxiv.org/abs/2310.01374](http://arxiv.org/abs/2310.01374)

    本文研究了广义交叉验证（GCV）在有限惩罚估计器集合中估计预测风险的一致性问题，并提出了一种修正方法（CGCV）来解决这个问题。

    

    广义交叉验证（GCV）是一种广泛使用的方法，用于估计在样本外进行预测的风险平方，并采用标量自由度调整（以乘法增加）来调整训练误差的平方。本文研究了GCV一致估计任意惩罚最小二乘估计器集合预测风险的能力。我们发现，对于任何大于一的有限大小的估计器集合，GCV是不一致的。为了弥补这个缺点，我们提出了一个纠正，它涉及到对每个集合成分的自由度调整训练误差的额外标量修正（以加法增加）。所提出的估计器（称为CGCV）保持了GCV的计算优势，既不需要样本分裂，模型重拟，也不需要包外风险估计。该估计器源自对集合风险分解的细致检查和该分解中各个成分的两种中间风险估计器。

    Generalized cross-validation (GCV) is a widely-used method for estimating the squared out-of-sample prediction risk that employs a scalar degrees of freedom adjustment (in a multiplicative sense) to the squared training error. In this paper, we examine the consistency of GCV for estimating the prediction risk of arbitrary ensembles of penalized least squares estimators. We show that GCV is inconsistent for any finite ensemble of size greater than one. Towards repairing this shortcoming, we identify a correction that involves an additional scalar correction (in an additive sense) based on degrees of freedom adjusted training errors from each ensemble component. The proposed estimator (termed CGCV) maintains the computational advantages of GCV and requires neither sample splitting, model refitting, or out-of-bag risk estimation. The estimator stems from a finer inspection of ensemble risk decomposition and two intermediate risk estimators for the components in this decomposition. We prov
    
[^3]: TACTiS-2：更好、更快、更简单的多变量时间序列关注联合分布模型

    TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series. (arXiv:2310.01327v1 [cs.LG])

    [http://arxiv.org/abs/2310.01327](http://arxiv.org/abs/2310.01327)

    TACTiS-2是一种改进的多变量时间序列关注联合分布模型，采用了简化的目标函数和线性参数数量，具有更好的训练动态和最先进的性能。

    

    我们引入了一种新的模型用于多变量概率时间序列预测，旨在灵活地处理包括预测、插值和它们的组合等一系列任务。基于联合分布理论，我们提出了一种简化的目标函数，用于最近引入的基于Transformer的关注联合分布模型（TACTiS）。新的目标函数的分布参数数量与变量数量呈线性而非阶乘关系。新的目标函数需要引入一种训练课程，并且需要对原始架构进行必要的改动。我们展示了得到的模型具有显著改善的训练动态，并在多样的真实世界预测任务中实现了最先进的性能，同时保持了先前工作的灵活性，如无缝处理不对齐和采样不均匀的时间序列。

    We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
    
[^4]: 具有打乱标签的线性回归的最优估计器

    Optimal Estimator for Linear Regression with Shuffled Labels. (arXiv:2310.01326v1 [stat.ML])

    [http://arxiv.org/abs/2310.01326](http://arxiv.org/abs/2310.01326)

    本文提出了一种用于解决具有打乱标签的线性回归问题的最优估计器。该估计器的复杂度不超过线性分配算法和最小二乘算法，并对信噪比要求进行了细分。

    

    本文考虑了具有打乱标签的线性回归问题，即 $\mathbf Y = \mathbf \Pi \mathbf X \mathbf B + \mathbf W$，其中 $\mathbf Y \in \mathbb R^{n\times m}, \mathbf Pi \in \mathbb R^{n\times n}, \mathbf X\in \mathbb R^{n\times p}, \mathbf B \in \mathbb R^{p\times m}$，和 $\mathbf W\in \mathbb R^{n\times m}$ 分别表示传感结果，（未知或缺失的）对应信息，传感矩阵，感兴趣的信号和附加的感知噪声。给定观测值 $\mathbf Y$ 和感知矩阵 $\mathbf X$，我们提出了一种一步估计器来重构 $(\mathbf \Pi, \mathbf B)$。从计算的角度来看，我们估计器的复杂度为 $O(n^3 + np^2m)$，不大于线性分配算法（例如 $O(n^3)$）和最小二乘算法（例如 $O(np^2 m)$）的最大复杂度。从统计学的角度来看，我们将最小信噪比要求分为四个区间，即未知、困难、中等和简单区间。

    This paper considers the task of linear regression with shuffled labels, i.e., $\mathbf Y = \mathbf \Pi \mathbf X \mathbf B + \mathbf W$, where $\mathbf Y \in \mathbb R^{n\times m}, \mathbf Pi \in \mathbb R^{n\times n}, \mathbf X\in \mathbb R^{n\times p}, \mathbf B \in \mathbb R^{p\times m}$, and $\mathbf W\in \mathbb R^{n\times m}$, respectively, represent the sensing results, (unknown or missing) corresponding information, sensing matrix, signal of interest, and additive sensing noise. Given the observation $\mathbf Y$ and sensing matrix $\mathbf X$, we propose a one-step estimator to reconstruct $(\mathbf \Pi, \mathbf B)$. From the computational perspective, our estimator's complexity is $O(n^3 + np^2m)$, which is no greater than the maximum complexity of a linear assignment algorithm (e.g., $O(n^3)$) and a least square algorithm (e.g., $O(np^2 m)$). From the statistical perspective, we divide the minimum $snr$ requirement into four regimes, e.g., unknown, hard, medium, and easy reg
    
[^5]: 使用切片Wasserstein k-means聚类在多维时间序列数据中自动检测制度

    Automated regime detection in multidimensional time series data using sliced Wasserstein k-means clustering. (arXiv:2310.01285v1 [q-fin.CP])

    [http://arxiv.org/abs/2310.01285](http://arxiv.org/abs/2310.01285)

    本文通过研究使用Wasserstein k-means聚类算法在一维时间序列数据中的行为，并将其扩展到多维时间序列数据，通过切片Wasserstein距离进行多维制度检测。

    

    最近的研究提出了Wasserstein k-means (Wk-means)聚类作为一种强大的方法来识别时间序列数据中的制度，尤其是一维资产收益。本文首先详细研究了应用于合成一维时间序列数据的Wasserstein k-means聚类算法的行为。我们研究了算法的动态性，并调查了不同超参数对不同随机初始化下聚类算法性能的影响。我们计算了一些简单的指标，发现它们对于识别高质量的聚类是有用的。然后，我们通过将多维Wasserstein距离近似为切片Wasserstein距离，将Wasserstein k-means聚类技术扩展到多维时间序列数据，得到一种我们称之为`sliced Wasserstein k-means (sWk-means)聚类'的方法。我们将sWk-means聚类方法应用于多维时间序列数据的自动制度检测问题。

    Recent work has proposed Wasserstein k-means (Wk-means) clustering as a powerful method to identify regimes in time series data, and one-dimensional asset returns in particular. In this paper, we begin by studying in detail the behaviour of the Wasserstein k-means clustering algorithm applied to synthetic one-dimensional time series data. We study the dynamics of the algorithm and investigate how varying different hyperparameters impacts the performance of the clustering algorithm for different random initialisations. We compute simple metrics that we find are useful in identifying high-quality clusterings. Then, we extend the technique of Wasserstein k-means clustering to multidimensional time series data by approximating the multidimensional Wasserstein distance as a sliced Wasserstein distance, resulting in a method we call `sliced Wasserstein k-means (sWk-means) clustering'. We apply the sWk-means clustering method to the problem of automated regime detection in multidimensional ti
    
[^6]: 非交换式共形风险控制

    Non-Exchangeable Conformal Risk Control. (arXiv:2310.01262v1 [cs.LG])

    [http://arxiv.org/abs/2310.01262](http://arxiv.org/abs/2310.01262)

    本文提出了一种非交换式共形风险控制的框架，可以在数据不可交换的情况下控制任何单调损失函数的期望值。

    

    最近，由于其能够为黑匣子神经模型的预测提供形式上保证的不确定性集合或区间，确保包含实际真实值的预定义概率，拆分共形预测引发了极大的兴趣。虽然最初的公式假设数据可交换，但一些扩展处理不可交换的数据，在许多现实世界的场景中经常发生。同时，一些进展已经在共形方法中取得，这些方法对更广泛的目标提供统计保证，例如限制最佳F1分数或以期望最小化误报率。在本文中，我们利用和扩展这两个工作线路，提出了非交换式共形风险控制，可以在数据不可交换的情况下控制任何单调损失函数的期望值。我们的框架灵活，假设很少，并允许根据数据的统计相似性进行加权处理。

    Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its statistical similarity with t
    
[^7]: 受限制和带水印生成的镜像扩散模型

    Mirror Diffusion Models for Constrained and Watermarked Generation. (arXiv:2310.01236v1 [stat.ML])

    [http://arxiv.org/abs/2310.01236](http://arxiv.org/abs/2310.01236)

    提出了一种新的镜像扩散模型（MDM），可以在受限制集合上生成数据而不丧失可追溯性。这通过在一个标准的欧几里得空间中学习扩散过程，并利用镜像映射来实现。

    

    现代扩散模型在学习复杂的高维数据分布方面取得了成功，这部分归功于其能够构建具有解析转移核函数和评分函数的扩散过程。这种可追溯性结果在不需要模拟的框架中具有稳定的回归损失，从而可以学习到可以扩展的逆向生成过程。然而，当数据被限制在受限制集合而不是标准的欧几里得空间中时，根据之前的尝试，这些理想的特性似乎丧失了。在这项工作中，我们提出了镜像扩散模型（MDM），一种新的扩散模型类，可以在凸约束集合上生成数据而不丧失任何可追溯性。这是通过在从镜像映射构建的对偶空间中学习扩散过程来实现的，关键的是，这是一个标准的欧几里得空间。我们推导了流行的约束集合（如单纯形和$\ell_2$-球）的镜像映射的有效计算，显示明显的提升。

    Modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. The tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. However, when data is confined to a constrained set as opposed to a standard Euclidean space, these desirable characteristics appear to be lost based on prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new class of diffusion models that generate data on convex constrained sets without losing any tractability. This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space. We derive efficient computation of mirror maps for popular constrained sets, such as simplices and $\ell_2$-balls, showing significantly im
    
[^8]: 一种适用于现代网络的路径范数工具包：影响、前景和挑战

    A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])

    [http://arxiv.org/abs/2310.01225](http://arxiv.org/abs/2310.01225)

    本文介绍了适用于现代神经网络的路径范数工具包，可以包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络。这个工具包恢复或超越了已知的路径范数界限，并挑战了基于路径范数的一些具体承诺。

    

    本文介绍了第一个完全能够包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络的路径范数工具包。这个工具包不仅适用于最广泛的基于路径范数的现代神经网络，还可以恢复或超越已知的此类范数的最尖锐界限。这些扩展的路径范数还享有路径范数的常规优点：计算简便、对网络的对称性具有不变性，在前馈网络上比操作符范数的乘积（另一种常用的复杂度度量）具有更好的锐度。工具包的多功能性和易于实施使我们能够通过数值评估在ImageNet上对ResNet的最尖锐界限来挑战基于路径范数的具体承诺。

    This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
    
[^9]: 统一的不确定性校准

    Unified Uncertainty Calibration. (arXiv:2310.01202v1 [stat.ML])

    [http://arxiv.org/abs/2310.01202](http://arxiv.org/abs/2310.01202)

    该论文提出了一种统一的不确定性校准（U2C）框架，用于合并可知和认知不确定性，实现了面对困难样例时的准确预测和校准。

    

    为了构建健壮，公平和安全的人工智能系统，我们希望在面对困难或超出训练类别的测试样例时，分类器能够说“我不知道”。普遍的预测不确定性策略是简单的“拒绝或分类”规则：如果认知不确定性高，则放弃预测，否则进行分类。然而，这种方法不允许不同的不确定性来源相互通信，会产生未校准的预测，并且不能纠正不确定性估计中的错误。为了解决这三个问题，我们引入了统一的不确定性校准（U2C）的整体框架，用于合并可知和认知不确定性。U2C能够进行清晰的学习理论分析不确定性估计，并且在各种ImageNet基准测试中优于拒绝或分类方法。

    To build robust, fair, and safe AI systems, we would like our classifiers to say ``I don't know'' when facing test examples that are difficult or fall outside of the training classes.The ubiquitous strategy to predict under uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify otherwise.Unfortunately, this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. To address these three issues, we introduce \emph{unified uncertainty calibration (U2C)}, a holistic framework to combine aleatoric and epistemic uncertainties. U2C enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of ImageNet benchmarks.
    
[^10]: SWoTTeD:张量分解在时间表征中的扩展

    SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping. (arXiv:2310.01201v1 [cs.LG])

    [http://arxiv.org/abs/2310.01201](http://arxiv.org/abs/2310.01201)

    SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。

    

    张量分解最近在机器学习领域对于个体追踪数据的分析，如电子健康记录(EHR)，引起了人们的关注。然而，当数据遵循复杂的时间模式时，这个任务变得更加困难。本文引入了时间表征的概念，即一组随时间变化的特征，并提出了SWoTTeD (Sliding Window for Temporal Tensor Decomposition)方法，一种发现隐藏时间模式的新方法。SWoTTeD集成了多种约束和正则化方法，以增强提取到的表征的可解释性。我们使用合成数据集和真实世界数据集进行验证，并提供了使用巴黎大学医院的数据的原始用例。结果表明，SWoTTeD能够至少与最新的基于张量分解的模型一样准确地重建数据，并提取到对临床医生有意义的时间表征。

    Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
    
[^11]: 如果没有欠拟合，就没有冷后验效应

    If there is no underfitting, there is no Cold Posterior Effect. (arXiv:2310.01189v1 [stat.ML])

    [http://arxiv.org/abs/2310.01189](http://arxiv.org/abs/2310.01189)

    研究发现，当贝叶斯后验欠拟合时，模型失配会导致冷后验效应(CPE)在贝叶斯深度学习中出现。

    

    贝叶斯深度学习中的冷后验效应(CPE)表明，对于温度$T<1$的后验，得到的后验预测性能可能优于贝叶斯后验($T=1$)。由于贝叶斯后验在完美模型规范下被认为是最优的，许多最近的研究将CPE的存在视为模型规范错误的问题，来自先验或似然函数。在这项工作中，我们提供了对CPE更细致的理解，我们证明了当得到的贝叶斯后验欠拟合时，模型失配才会导致CPE。事实上，我们理论上证明没有欠拟合则没有CPE。

    The cold posterior effect (CPE) (Wenzel et al., 2020) in Bayesian deep learning shows that, for posteriors with a temperature $T<1$, the resulting posterior predictive could have better performances than the Bayesian posterior ($T=1$). As the Bayesian posterior is known to be optimal under perfect model specification, many recent works have studied the presence of CPE as a model misspecification problem, arising from the prior and/or from the likelihood function. In this work, we provide a more nuanced understanding of the CPE as we show that misspecification leads to CPE only when the resulting Bayesian posterior underfits. In fact, we theoretically show that if there is no underfitting, there is no CPE.
    
[^12]: 并行时间概率数值ODE求解器

    Parallel-in-Time Probabilistic Numerical ODE Solvers. (arXiv:2310.01145v1 [math.NA])

    [http://arxiv.org/abs/2310.01145](http://arxiv.org/abs/2310.01145)

    本文提出了一种并行时间概率数值ODE求解器，通过将数值模拟问题视为贝叶斯状态估计问题，并利用贝叶斯滤波和平滑的框架，实现了在并行处理所有时间步骤的同时将时间开销降低到对数级别。

    

    针对常微分方程(ODE)的概率数值求解器将动力系统的数值仿真问题视为贝叶斯状态估计问题。除了生成ODE解的后验分布并因此量化方法本身的数值逼近误差之外，这种形式化方法的一个不常被注意到的优势是通过在贝叶斯滤波和平滑的框架中进行数值模拟而获得的算法灵活性。在本文中，我们利用这种灵活性，基于时间并行迭代扩展卡尔曼平滑器的公式化，提出了一种并行时间概率数值ODE求解器。与当前的概率求解器依次按时间顺序模拟动力系统不同，所提出的方法以并行方式处理所有时间步骤，从而将时间开销从线性降低到对数级别的时间步骤数。我们通过在多种问题上展示了我们方法的有效性。

    Probabilistic numerical solvers for ordinary differential equations (ODEs) treat the numerical simulation of dynamical systems as problems of Bayesian state estimation. Aside from producing posterior distributions over ODE solutions and thereby quantifying the numerical approximation error of the method itself, one less-often noted advantage of this formalism is the algorithmic flexibility gained by formulating numerical simulation in the framework of Bayesian filtering and smoothing. In this paper, we leverage this flexibility and build on the time-parallel formulation of iterated extended Kalman smoothers to formulate a parallel-in-time probabilistic numerical ODE solver. Instead of simulating the dynamical system sequentially in time, as done by current probabilistic solvers, the proposed method processes all time steps in parallel and thereby reduces the span cost from linear to logarithmic in the number of time steps. We demonstrate the effectiveness of our approach on a variety o
    
[^13]: 用于逆问题的Prompt-tuning潜在扩散模型

    Prompt-tuning latent diffusion models for inverse problems. (arXiv:2310.01110v1 [cs.LG])

    [http://arxiv.org/abs/2310.01110](http://arxiv.org/abs/2310.01110)

    本论文提出了一种使用文本到图像潜在扩散模型作为通用先验解决成像逆问题的方法。通过Prompt-tuning将文本嵌入进行实时优化，同时通过投影保持潜在变量的演化在编码器的范围空间内，使生成图像更符合扩散先验。这种综合方法，在超分辨率、去模糊和修复等各种任务中，优于基于图像和基于潜在扩散模型的逆问题求解器。

    

    我们提出了一种使用文本到图像潜在扩散模型作为通用先验解决成像逆问题的新方法。现有的使用潜在扩散模型解决逆问题的方法通常依赖简单的空文本提示，这可能导致性能不佳。为了解决这个限制，我们引入了一种Prompt-tuning方法，在运行反向扩散过程时实时优化文本嵌入。这使我们能够生成更符合扩散先验的图像。此外，我们提出了一种通过投影将潜在变量的演化保持在编码器的范围空间内的方法。这有助于减少使用潜在扩散模型而不是基于像素的扩散模型时产生的图像伪影问题。我们提出的综合方法称为P2L，在各种任务（如超分辨率、去模糊和修复）中优于基于图像和基于潜在扩散模型的逆问题求解器。

    We propose a new method for solving imaging inverse problems using text-to-image latent diffusion models as general priors. Existing methods using latent diffusion models for inverse problems typically rely on simple null text prompts, which can lead to suboptimal performance. To address this limitation, we introduce a method for prompt tuning, which jointly optimizes the text embedding on-the-fly while running the reverse diffusion process. This allows us to generate images that are more faithful to the diffusion prior. In addition, we propose a method to keep the evolution of latent variables within the range space of the encoder, by projection. This helps to reduce image artifacts, a major problem when using latent diffusion models instead of pixel-based diffusion models. Our combined method, called P2L, outperforms both image- and latent-diffusion model-based inverse problem solvers on a variety of tasks, such as super-resolution, deblurring, and inpainting.
    
[^14]: Ground-A-Video: 使用文本到图像扩散模型的零样本视频编辑

    Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models. (arXiv:2310.01107v1 [cs.CV])

    [http://arxiv.org/abs/2310.01107](http://arxiv.org/abs/2310.01107)

    本论文提出了一种名为 Ground-A-Video 的基于引导的视频到视频转换框架，用于多属性视频编辑。该方法在没有训练的情况下实现了输入视频的时间一致的多属性编辑，并且解决了其他方法存在的问题。

    

    最近在视频编辑领域取得了令人期待的成果，实现了单属性编辑或风格传递的任务，不论通过在文本-视频数据上训练文本到视频（T2V）模型还是采用无需训练的方法。然而，当面对多属性编辑情景的复杂性时，它们存在一些缺点，比如忽略或忽视所期望的属性变化，修改输入视频的错误元素，以及无法保留应该保持原样的输入视频区域。为解决这个问题，我们提出了一种新颖的基于引导的视频到视频转换框架，名为 Ground-A-Video，用于多属性视频编辑。Ground-A-Video以无需训练的方式实现了输入视频的时间一致的多属性编辑，并且没有上述缺点。我们方法的核心是引入了交叉帧门控注意力，以一种时间上一致的方式将定位信息融入到潜在表示中。

    Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video (T2V) models on text-video data or adopting training-free methods. However, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. To address this, here we present a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing. Ground-A-Video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings. Central to our method is the introduction of Cross-Frame Gated Attention which incorporates groundings information into the latent representations in a temporally consistent fashion,
    
[^15]: 基于能量导向的连续熵巴氏中心估计方法及其在一般成本问题中的应用

    Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])

    [http://arxiv.org/abs/2310.01105](http://arxiv.org/abs/2310.01105)

    本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。

    

    优化输运（OT）巴氏中心是一种在捕捉概率分布几何特性的同时对其进行平均的数学方法。本文提出了一种新颖的算法，用于近似计算任意OT成本函数的连续熵OT巴氏中心。我们的方法基于最近在机器学习社区中受到关注的基于弱OT的连续熵最优输运问题的对偶重构。除了创新性之外，我们的方法还具有以下若干优势特点：（i）我们建立了对恢复解的质量界限；（ii）该方法与基于能量的模型（EBMs）学习过程无缝连接，可以使用经过良好调整的算法解决感兴趣的问题；（iii）它提供了一种直观的优化方案，避免使用极小-极大、强化等复杂技巧。为了验证我们的方法，我们考虑了s

    Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
    
[^16]: CES分布的费舍尔-拉奥（Fisher-Rao）几何

    The Fisher-Rao geometry of CES distributions. (arXiv:2310.01032v1 [stat.ML])

    [http://arxiv.org/abs/2310.01032](http://arxiv.org/abs/2310.01032)

    这篇论文介绍了费舍尔-拉奥信息几何在椭圆分布中的应用，包括协方差矩阵估计、内蕴Cram\'er-Rao界限和使用黎曼距离进行分类。

    

    在处理参数统计模型时，自然会在参数空间上赋予费舍尔信息度量，从而产生一个黎曼流形。这个度量诱导在参数上的几何称为费舍尔-拉奥信息几何。有趣的是，这种观点可以利用微分几何中的许多工具。在简要介绍这些概念后，我们将介绍这些几何工具在椭圆分布框架中的一些实际应用。这个阐述的第二部分分为三个主要方面：协方差矩阵估计的黎曼优化、内蕴Cram\'er-Rao界限和使用黎曼距离进行分类。

    When dealing with a parametric statistical model, a Riemannian manifold can naturally appear by endowing the parameter space with the Fisher information metric. The geometry induced on the parameters by this metric is then referred to as the Fisher-Rao information geometry. Interestingly, this yields a point of view that allows for leveragingmany tools from differential geometry. After a brief introduction about these concepts, we will present some practical uses of these geometric tools in the framework of elliptical distributions. This second part of the exposition is divided into three main axes: Riemannian optimization for covariance matrix estimation, Intrinsic Cram\'er-Rao bounds, and classification using Riemannian distances.
    
[^17]: CCA家族的高效算法：无约束目标与无偏梯度

    Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])

    [http://arxiv.org/abs/2310.01012](http://arxiv.org/abs/2310.01012)

    本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。

    

    典型相关分析（CCA）方法在多视角学习中具有基础性作用。正则化线性CCA方法可以看作是偏最小二乘（PLS）的推广，并与广义特征值问题（GEP）框架统一。然而，这些线性方法的传统算法在大规模数据上计算上是不可行的。深度CCA的扩展显示出很大的潜力，但目前的训练过程缓慢且复杂。我们首先提出了一个描述GEPs的顶级子空间的新颖无约束目标。我们的核心贡献是一系列快速算法，用随机梯度下降（SGD）应用于相应的CCA目标，从而获得随机PLS、随机CCA和深度CCA。这些方法在所有标准CCA和深度CCA基准测试中显示出比先前最先进方法更快的收敛速度和更高的相关性恢复。这样的速度使我们能够首次进行大规模生物数据的PLS分析。

    The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
    
[^18]: 有限秩核岭回归的测试误差的理论分析

    A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression. (arXiv:2310.00987v1 [cs.LG])

    [http://arxiv.org/abs/2310.00987](http://arxiv.org/abs/2310.00987)

    本研究通过推导任意有限秩核岭回归模型的尖锐的非渐近性上界和下界，填补了有限秩核岭回归保证的空白。

    

    现有的对于一般核回归模型的统计学学习保证在使用有限秩核时往往会得到宽松的边界。然而，在几个机器学习问题中，如在执行迁移学习时，将预训练的深度神经网络的最后一层微调以适应新任务时，有限秩核会自然地出现。本文通过推导任意有限秩核岭回归模型的尖锐的非渐近性上界和下界，填补了有限秩核岭回归保证的空白。我们的边界比之前针对有限秩核岭回归模型推导的边界更紧，并且与类似结果不同的是，它们也适用于任何正则化参数。

    Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in several machine learning problems, e.g.\ when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters.
    
[^19]: 随机情境对决争夺决策的方差感知遗憾界限

    Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits. (arXiv:2310.00968v1 [cs.LG])

    [http://arxiv.org/abs/2310.00968](http://arxiv.org/abs/2310.00968)

    本文提出了一种新的算法，用于解决对决争夺中固有不确定性的问题，算法具有计算效率和方差感知遗憾界限。

    

    对决争夺是一个重要的决策框架，涉及到偏好反馈的决策，这是一个适用于人机交互的各种应用场景的有价值特性，例如排名、信息检索和推荐系统。虽然在对决争夺中已经做出了大量的努力来最小化累计遗憾，但目前研究中存在一个明显的空白，即遗憾界限未考虑到对决手表间成对比较的固有不确定性。直观地说，更大的不确定性意味着问题的难度更高。为了填补这个空白，本文研究了情境对决争夺的问题，其中决策手表的二元对比是由广义线性模型（GLM）生成的。我们提出了一种新的SupLinUCB类型的算法，这个算法具有计算效率和一个感知方差遗憾界限$\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$，其中$\sigma_t$是每轮成对比较的方差。

    Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$, where $\sigma_t$ is the variance of the pairwise comparison in round
    
[^20]: 改进的变分贝叶斯系统发育推断算法——混合算法

    Improved Variational Bayesian Phylogenetic Inference using Mixtures. (arXiv:2310.00941v1 [cs.LG])

    [http://arxiv.org/abs/2310.00941](http://arxiv.org/abs/2310.00941)

    VBPI-Mixtures是一种改进的变分贝叶斯系统发育推断算法，通过使用混合学习技术来提高系统发育后验分布的准确性，并能够捕捉到传统算法未能建模的树形拓扑分布。

    

    我们提出了VBPI-Mixtures算法，旨在提高系统发育后验分布的准确性，特别是在树形拓扑和分支长度近似方面。尽管变分贝叶斯系统发育推断（VBPI）作为一个领先的黑盒变分推理（BBVI）框架，在这些分布的近似中取得了显著的成果，但树形拓扑后验的多模性对于样本采用学习技术（如BBVI）来说是一个严峻的挑战。先进的深度学习方法，如标准流和图神经网络，已经被应用于改善分支长度后验的近似，但在改进树形拓扑后验的方面的努力还不足。我们的新颖算法VBPI-Mixtures填补了这一空白，将BBVI域内混合学习的最新突破应用其中。因此，VBPI-Mixtures能够捕捉VBPI未能建模的树形拓扑分布。

    We present VBPI-Mixtures, an algorithm designed to enhance the accuracy of phylogenetic posterior distributions, particularly for tree-topology and branch-length approximations. Despite the Variational Bayesian Phylogenetic Inference (VBPI), a leading-edge black-box variational inference (BBVI) framework, achieving remarkable approximations of these distributions, the multimodality of the tree-topology posterior presents a formidable challenge to sampling-based learning techniques such as BBVI. Advanced deep learning methodologies such as normalizing flows and graph neural networks have been explored to refine the branch-length posterior approximation, yet efforts to ameliorate the posterior approximation over tree topologies have been lacking. Our novel VBPI-Mixtures algorithm bridges this gap by harnessing the latest breakthroughs in mixture learning within the BBVI domain. As a result, VBPI-Mixtures is capable of capturing distributions over tree-topologies that VBPI fails to model.
    
[^21]: 理解CLIP中的可转移表示学习和零样本传递

    Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP. (arXiv:2310.00927v1 [cs.LG])

    [http://arxiv.org/abs/2310.00927](http://arxiv.org/abs/2310.00927)

    本文研究了CLIP中的可转移表示学习和零样本传递，提出了一个新的CLIP类型方法，在基准数据集上取得了更好的性能。

    

    多模态学习因其能够利用不同数据源（例如文本和图像）的信息来提高模型性能而日益受到关注。近年来，CLIP作为一种有效的方法，采用视觉-语言对比预训练来学习联合图像和文本表示，并在零样本学习和文本引导的自然图像生成方面表现出非凡的性能。尽管CLIP在实践中取得了巨大的成功，但其理论理解仍然困难。在本文中，我们正式研究了CLIP中的可转移表示学习，并展示了不同模态的特征如何对齐。我们还分析了其在下游任务中的零样本传递性能。受到我们分析的启发，我们提出了一种新的CLIP类型方法，在基准数据集上实现了比CLIP和其他最先进方法更好的性能。

    Multi-modal learning has become increasingly popular due to its ability to leverage information from different data sources (e.g., text and images) to improve the model performance. Recently, CLIP has emerged as an effective approach that employs vision-language contrastive pretraining to learn joint image and text representations and exhibits remarkable performance in zero-shot learning and text-guided natural image generation. Despite the huge practical success of CLIP, its theoretical understanding remains elusive. In this paper, we formally study transferrable representation learning underlying CLIP and demonstrate how features from different modalities get aligned. We also analyze its zero-shot transfer performance on the downstream tasks. Inspired by our analysis, we propose a new CLIP-type approach, which achieves better performance than CLIP and other state-of-the-art methods on benchmark datasets.
    
[^22]: DataInf：在LLMs和扩散模型中高效估计数据影响力

    DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])

    [http://arxiv.org/abs/2310.00902](http://arxiv.org/abs/2310.00902)

    DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。

    

    量化训练数据点的影响力对于理解机器学习模型的输出和提高AI管道的透明度至关重要。影响函数是一种原则性和流行的数据归属方法，但其计算成本使其难以使用。这个问题在大型语言模型和文本到图像模型的设置中更加突出。在这项工作中，我们提出了DataInf，一种高效的影响力近似方法，适用于大规模生成型AI模型。通过利用易于计算的闭式表达式，DataInf在计算和内存效率方面优于现有的影响计算算法。我们的理论分析表明，DataInf特别适用于诸如LoRA的参数有效微调技术。通过系统的实证评估，我们展示了DataInf能够准确地近似影响分数，并且比现有方法快几个数量级。

    Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
    
[^23]: 使用深度生成模型的基于集成方法的井下特性表征

    Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models. (arXiv:2310.00839v1 [cs.LG])

    [http://arxiv.org/abs/2310.00839](http://arxiv.org/abs/2310.00839)

    本文提出了一种使用深度生成模型和集成方法的井下特性表征方法。通过结合WGAN-GP和ES-MDA技术，实现了准确且高效的K场估计。这种方法在几个井下实例中得到了验证，并展示了未知K字段的主要特征。

    

    估计如水力传导率（K）等空间分布属性是井下特性表征中的重大挑战。然而，由于计算成本高和稀疏数据集的预测精度低，逆向建模在不适定的高维应用中受限。本文将Wasserstein生成对抗网络与梯度惩罚（WGAN-GP）和基于集成的多元数据同化（ES-MDA）技术相结合，实现了准确且高效的井下特性表征。WGAN-GP通过训练从低维潜变量空间生成高维K场，ES-MDA通过同化可用测量结果来更新潜变量。利用几个井下实例评估了所提出方法的准确性和效率，以及未知K字段的主要特征。

    Estimating spatially distributed properties such as hydraulic conductivity (K) from available sparse measurements is a great challenge in subsurface characterization. However, the use of inverse modeling is limited for ill-posed, high-dimensional applications due to computational costs and poor prediction accuracy with sparse datasets. In this paper, we combine Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), a deep generative model that can accurately capture complex subsurface structure, and Ensemble Smoother with Multiple Data Assimilation (ES-MDA), an ensemble-based inversion method, for accurate and accelerated subsurface characterization. WGAN-GP is trained to generate high-dimensional K fields from a low-dimensional latent space and ES-MDA then updates the latent variables by assimilating available measurements. Several subsurface examples are used to evaluate the accuracy and efficiency of the proposed method and the main features of the unknown K fie
    
[^24]: 学习如何提供注重依从性的建议

    Learning to Make Adherence-Aware Advice. (arXiv:2310.00817v1 [stat.ML])

    [http://arxiv.org/abs/2310.00817](http://arxiv.org/abs/2310.00817)

    本文提出了一种顺序决策模型，考虑了人的依从程度和机器提供建议的时机，并提供了学习算法来学习最佳的建议策略。

    

    随着人工智能系统在人类决策中扮演越来越重要的角色，人工智能与人类之间的交互存在挑战。由于没有充分考虑到人类忽视人工智能建议和人工智能选择性提供建议的需求，一个挑战就来自于底层人工智能策略的不佳表现。本文提出了一个顺序决策模型，该模型考虑了人类的依从程度（即人类遵循/拒绝机器建议的概率），并引入了一个推迟选项，使得机器在最合适的时候可以暂时不提供建议。我们提供了学习算法，可以学习最佳的建议策略，并仅在关键时刻提供建议。与问题不可知的强化学习算法相比，我们的专门化学习算法不仅具有更好的理论收敛性能，而且在实证性能上表现出色。

    As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.
    
[^25]: 指向因果基础模型: 因果推断与注意力的对偶关系

    Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])

    [http://arxiv.org/abs/2310.00809](http://arxiv.org/abs/2310.00809)

    该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。

    

    基于因果推断和注意力之间的对偶连接，我们提出了一种名为Causal Inference with Attention (CInA)的理论上完备的方法，利用多个无标签数据集进行自监督因果学习，并在新数据的未见任务上实现零样本因果推断。我们的实证结果表明了我们的方法在复杂任务中的有效性。

    Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
    
[^26]: 用于决斗对比中的 Copeland 获胜者的识别

    Identifying Copeland Winners in Dueling Bandits with Indifferences. (arXiv:2310.00750v1 [cs.LG])

    [http://arxiv.org/abs/2310.00750](http://arxiv.org/abs/2310.00750)

    本文解决了决斗对比中的 Copeland 获胜者识别问题，利用三元反馈解决了传统决斗对比问题中未被充分研究的变体。我们提供了样本复杂度的下界，并提出了一种算法 POCOWISTA，该算法几乎达到了下界，展现出很好的实证性能。

    

    我们考虑使用三元反馈来识别决斗对比问题中的 Copeland 获胜者。这是传统决斗对比问题的一个未被充分研究但在实际中有意义的变体，在这个问题中，除了在两个选项之间有明确的偏好之外，我们还可以观察到因为漠不关心而形成的反馈。我们给出了任何学习算法在固定错误概率下找到 Copeland 获胜者的样本复杂度的下界。此外，我们提出了一种名为 POCOWISTA 的算法，其样本复杂度几乎与这个下界相匹配，并且在传统决斗对比问题中展现了出色的实证性能。对于偏好概率满足特定类型的随机传递性的情况，我们提供了改进后的精炼版本，具有改进的最坏情况样本复杂度。

    We consider the task of identifying the Copeland winner(s) in a dueling bandits problem with ternary feedback. This is an underexplored but practically relevant variant of the conventional dueling bandits problem, in which, in addition to strict preference between two arms, one may observe feedback in the form of an indifference. We provide a lower bound on the sample complexity for any learning algorithm finding the Copeland winner(s) with a fixed error probability. Moreover, we propose POCOWISTA, an algorithm with a sample complexity that almost matches this lower bound, and which shows excellent empirical performance, even for the conventional dueling bandits problem. For the case where the preference probabilities satisfy a specific type of stochastic transitivity, we provide a refined version with an improved worst case sample complexity.
    
[^27]: 光谱神经网络：逼近理论和优化路径的研究

    Spectral Neural Networks: Approximation Theory and Optimization Landscape. (arXiv:2310.00729v1 [cs.LG])

    [http://arxiv.org/abs/2310.00729](http://arxiv.org/abs/2310.00729)

    这项研究探索了光谱神经网络（SNN）的关键理论方面，包括神经网络学习光谱几何信息的权衡和SNN优化路径的理论探索。

    

    有许多基于从数据中提取光谱几何信息的机器学习方法。然而，许多这些方法的实现往往依赖于传统的特征求解器，在实际的在线大数据场景中存在一些限制。为了解决这些挑战，研究人员提出了不同的策略来训练神经网络作为传统特征求解器的替代方案，其中一种方法被称为光谱神经网络（SNN）。在本文中，我们研究了SNN的关键理论方面。首先，我们对神经网络学习光谱几何信息的神经元数量和量之间的权衡提出了定量洞察。其次，我们对SNN目标的优化路径进行了理论探索，以揭示SNN的训练动态。与典型的神经网络训练动态收敛到全局解的研究不同，SNN呈现了某种程度上的保守。

    There is a large variety of machine learning methodologies that are based on the extraction of spectral geometric information from data. However, the implementations of many of these methods often depend on traditional eigensolvers, which present limitations when applied in practical online big data scenarios. To address some of these challenges, researchers have proposed different strategies for training neural networks as alternatives to traditional eigensolvers, with one such approach known as Spectral Neural Network (SNN). In this paper, we investigate key theoretical aspects of SNN. First, we present quantitative insights into the tradeoff between the number of neurons and the amount of spectral geometric information a neural network learns. Second, we initiate a theoretical exploration of the optimization landscape of SNN's objective to shed light on the training dynamics of SNN. Unlike typical studies of convergence to global solutions of NN training dynamics, SNN presents an ad
    
[^28]: 基于物理信息的图神经网络用于电力系统的动态重构

    Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems. (arXiv:2310.00728v1 [cs.LG])

    [http://arxiv.org/abs/2310.00728](http://arxiv.org/abs/2310.00728)

    提出了一种基于物理信息的图神经网络（GNN）框架GraPhyR，用于解决电力系统的动态重构（DyR）问题。该框架将运营和连接约束直接融入GNN框架中，并进行端到端的训练，能够有效地优化DyR任务。

    

    为了保持可靠的电网，我们需要快速的决策算法来解决动态重构（DyR）等复杂问题。DyR实时优化配电网开关设置，以最小化电网损耗，并分派资源以满足可用发电量的负载需求。DyR是一个混合整数问题，对于大规模电网和快速时间尺度来说，可能计算难以解决。我们提出了GraPhyR，一种专为DyR而设计的基于物理信息的图神经网络（GNN）框架。我们直接将基本的运营和连接约束融入到GNN框架中，并进行端到端的训练。我们的结果表明，GraPhyR能够学习优化DyR任务。

    To maintain a reliable grid we need fast decision-making algorithms for complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution grid switch settings in real-time to minimize grid losses and dispatches resources to supply loads with available generation. DyR is a mixed-integer problem and can be computationally intractable to solve for large grids and at fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network (GNNs) framework tailored for DyR. We incorporate essential operational and connectivity constraints directly within the GNN framework and train it end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR task.
    
[^29]: 通过任务提示在变压器中改善长度泛化能力

    Improving Length-Generalization in Transformers via Task Hinting. (arXiv:2310.00726v1 [cs.LG])

    [http://arxiv.org/abs/2310.00726](http://arxiv.org/abs/2310.00726)

    该论文提出了一种基于任务提示的方法，在变压器模型中改善了长度泛化能力。通过同时训练模型处理简单但相关的辅助任务，可以显著提高模型在长序列数据上的性能。

    

    近年来观察到，对于某些类型的推理和算术任务，变压器在长度泛化方面存在问题。特别是，在任务（例如加法）的训练中，当应用于相同问题的更长实例时，变压器模型的性能会急剧下降。本文提出了一种基于任务提示的方法来解决长度泛化问题。我们的关键思想是，在训练模型处理特定任务的数据时，同时训练模型处理一个更简单但相关的辅助任务。我们以经典的排序问题为例来评估我们的方法。我们设计了一个多任务训练框架，并展示了任务提示在改善长度泛化能力方面的显著效果。对于排序问题，我们展示了可以训练模型处理长度最多为20的序列数据，并将在长度为100的序列上的测试准确率提高至

    It has been observed in recent years that transformers have problems with length generalization for certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied to longer instances of the same problem. This work proposes an approach based on task hinting towards addressing length generalization. Our key idea is that while training the model on task-specific data, it is helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.  We study the classical sorting problem as a canonical example to evaluate our approach. We design a multitask training framework and show that task hinting significantly improve length generalization. For sorting we show that it is possible to train models on data consisting of sequences having length at most $20$, and improve the test accuracy on sequences of length $100$ from l
    
[^30]: 随机梯度下降的噪声几何：定量和分析特征的研究

    The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])

    [http://arxiv.org/abs/2310.00692](http://arxiv.org/abs/2310.00692)

    本文对随机梯度下降（SGD）中的噪声几何进行了全面的理论研究，发现噪声与损失函数的局部几何特征有利的一致性。通过实验证明，SGD在逃脱尖锐极小值时与GD形成鲜明对比，逃脱方向在平坦方向上有显著分量。

    

    实证研究表明，随机梯度下降（SGD）中的噪声与损失函数的局部几何特征有利的一致性。然而，对于这种现象的理论和定量解释仍然不足。本文对过参数化线性模型和两层神经网络的上述“噪声几何”进行了全面的理论研究。我们细致地研究了平均和方向的一致性，特别关注样本大小和输入数据退化对一致性强度的影响。作为特定应用，我们利用噪声几何特征研究了SGD如何从尖锐极小值中逃脱，发现逃脱方向在平坦方向上有显著分量，这与只在最尖锐方向逃脱的梯度下降方法GD形成鲜明对比。为了验证我们的理论发现，我们进行了合成和真实世界的实验。

    Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
    
[^31]: WASA：基于水印的大型语言模型生成数据的源归属

    WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data. (arXiv:2310.00646v1 [cs.LG])

    [http://arxiv.org/abs/2310.00646](http://arxiv.org/abs/2310.00646)

    本文提出了一种基于水印的框架WASA，通过允许大型语言模型生成带有嵌入源信息的合成文本水印来解决源归属和数据来源的问题。

    

    大型语言模型（LLM）的出色性能和其商业化的巨大潜力引发了对其训练数据知识产权（IP）的严重关注。特别是，LLM生成的合成文本可能侵犯被用于训练LLM的数据的知识产权。为此，我们能够（a）通过水印识别出对LLM生成的合成文本做出贡献的数据提供者（源归属）；以及（b）验证文本数据是否来自于某个数据提供者对LLM进行了训练（数据来源）。在本文中，我们展示了通过水印技术可以解决这两个问题，即通过让LLM生成具有嵌入源信息的合成文本水印来实现。我们确定了这种水印技术框架的关键特性（例如源归属准确性、抵抗对手攻击的鲁棒性），并提出了一个满足这些要求的WAtermarking for Source Attribution（WASA）框架.

    The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies
    
[^32]: 离散选择的多臂赌博机

    Discrete Choice Multi-Armed Bandits. (arXiv:2310.00562v1 [stat.ML])

    [http://arxiv.org/abs/2310.00562](http://arxiv.org/abs/2310.00562)

    本文在离散选择模型和在线学习和多臂赌博算法之间建立了联系，提出了具有次线性遗憾界限的算法，并引入了一种灵活调整模型的新颖对抗性多臂赌博算法家族。

    

    本文建立了离散选择模型与在线学习和多臂赌博算法领域的联系。我们的贡献可以总结为两个关键方面。首先，我们给出了一系列算法的次线性遗憾界限，其中包括了Exp3算法作为一个特例。其次，我们引入了一种新颖的对抗性多臂赌博算法家族，受到wen等人引入的广义嵌套对数模型的启发。这些算法通过闭式采样分布概率，为用户提供了广泛调整模型的灵活性，并能够高效地实现。为了展示我们算法的实际应用，我们进行了数值实验，重点关注随机赌博机情况。

    This paper establishes a connection between a category of discrete choice models and the realms of online learning and multiarmed bandit algorithms. Our contributions can be summarized in two key aspects. Firstly, we furnish sublinear regret bounds for a comprehensive family of algorithms, encompassing the Exp3 algorithm as a particular case. Secondly, we introduce a novel family of adversarial multiarmed bandit algorithms, drawing inspiration from the generalized nested logit models initially introduced by \citet{wen:2001}. These algorithms offer users the flexibility to fine-tune the model extensively, as they can be implemented efficiently due to their closed-form sampling distribution probabilities. To demonstrate the practical implementation of our algorithms, we present numerical experiments, focusing on the stochastic bandit case.
    
[^33]: 训练神经网络中的可变性的鲁棒非参数假设检验

    Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks. (arXiv:2310.00541v1 [stat.ML])

    [http://arxiv.org/abs/2310.00541](http://arxiv.org/abs/2310.00541)

    该论文提出了一种基于鲁棒非参数假设检验框架的新指标，用于衡量分类模型之间的相似度，该指标能够适用于从训练模型中派生的其他量。

    

    训练深度神经网络（DNN）通常涉及随机优化，这意味着每次运行将产生不同的模型。几项研究表明，当模型具有相同的性能时，这种可变性是可以忽略的，在分类问题中即测试准确度。然而，具有类似测试准确度的模型可能没有计算相同的函数。我们提出了一种基于网络输出在阈值化之前的分类模型之间相似度的新指标。我们的指标基于一个鲁棒的假设检验框架，并可适用于从训练模型中派生的其他量。

    Training a deep neural network (DNN) often involves stochastic optimization, which means each run will produce a different model. Several works suggest this variability is negligible when models have the same performance, which in the case of classification is test accuracy. However, models with similar test accuracy may not be computing the same function. We propose a new measure of closeness between classification models based on the output of the network before thresholding. Our measure is based on a robust hypothesis-testing framework and can be adapted to other quantities derived from trained models.
    
[^34]: 最佳候选规则下的Thompson探索在最佳臂识别中的应用

    Thompson Exploration with Best Challenger Rule in Best Arm Identification. (arXiv:2310.00539v1 [stat.ML])

    [http://arxiv.org/abs/2310.00539](http://arxiv.org/abs/2310.00539)

    本文提出了一种新的策略，将Thompson采样与最佳候选规则相结合，用于解决最佳臂识别问题。该策略在渐近情况下是最优的，并在一般的多臂赌博机问题中达到接近最优的性能。

    

    本文研究了在经典单参数指数模型下，固定置信度下的最佳臂识别（BAI）问题。针对这个问题，目前已有很多策略被提出，但大多数需要在每一轮解决一个最优化问题和/或者需要探索一个臂至少一定次数，除非是针对高斯模型的限制。为了解决这些限制，我们提出了一种新的策略，将Thompson采样与一个计算效率高的方法——最佳候选规则相结合。虽然Thompson采样最初被考虑用于最大化累积奖励，但我们证明它也可以自然地用于在BAI中探索臂而不强迫最大化奖励。我们证明了我们的策略在任意两臂赌博机问题上是渐近最优的，并且在一般的$K$臂赌博机问题上（$K\geq 3$）达到接近最优的性能。然而，在数值实验中，我们的策略与现有方法相比表现出了竞争性的性能。

    This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to as
    
[^35]: infoGAN的两层网络的目标函数等式属性

    The objective function equality property of infoGAN for two-layer network. (arXiv:2310.00443v1 [stat.ML])

    [http://arxiv.org/abs/2310.00443](http://arxiv.org/abs/2310.00443)

    这项研究证明了在infoGAN中，辨别器和生成器的样本数量趋向无穷时，两个目标函数变得等价。

    

    信息最大化生成对抗网络(infoGAN)可以理解为涉及两个网络(辨别器和生成器)的极小化极大问题，其中包含了互信息函数。infoGAN包括多种组件，包括潜在变量、互信息和目标函数。本研究证明，在辨别器和生成器样本数量趋向无穷时，infoGAN中的两个目标函数变得等价。这种等价关系是通过考虑目标函数的经验版本和总体版本之间的差异来建立的。这个差异的界限由辨别器和生成器函数类的Rademacher复杂度决定。此外，使用具有Lipschitz和非递减激活函数的两层网络来验证这个等式。

    Information Maximizing Generative Adversarial Network (infoGAN) can be understood as a minimax problem involving two networks: discriminators and generators with mutual information functions. The infoGAN incorporates various components, including latent variables, mutual information, and objective function. This research demonstrates that the two objective functions in infoGAN become equivalent as the discriminator and generator sample size approaches infinity. This equivalence is established by considering the disparity between the empirical and population versions of the objective function. The bound on this difference is determined by the Rademacher complexity of the discriminator and generator function class. Furthermore, the utilization of a two-layer network for both the discriminator and generator, featuring Lipschitz and non-decreasing activation functions, validates this equality
    
[^36]: 关于生成模型在其自己的数据上迭代训练的稳定性研究

    On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])

    [http://arxiv.org/abs/2310.00429](http://arxiv.org/abs/2310.00429)

    本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。

    

    深度生成模型在建模复杂数据方面取得了巨大的进展，往往展现出超过典型人类能力的样本真实性辨别能力。这一成功的关键驱动力无疑是这些模型消耗海量网络规模数据的结果。由于这些模型惊人的性能和易得性，网络上将不可避免地出现越来越多的合成内容。这个事实直接意味着生成模型的未来迭代必须面对一个现实：它们的训练数据由清洁数据和先前模型生成的人工数据组成。在本文中，我们开发了一个框架来对混合数据集（包括真实数据和合成数据）上训练生成模型对稳定性的影响进行严格研究。我们首先证明了在初始生成模型足够好地近似数据分布并且真实数据与合成数据的比例适当的情况下，迭代训练的稳定性。

    Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
    
[^37]: 一种用于聚类多任务压缩感知的高效算法

    An Efficient Algorithm for Clustered Multi-Task Compressive Sensing. (arXiv:2310.00420v1 [eess.SP])

    [http://arxiv.org/abs/2310.00420](http://arxiv.org/abs/2310.00420)

    本文提出了一种用于聚类多任务压缩感知的高效的算法，通过避免重复计算协方差矩阵，我们的算法在性能上显著优于现有算法，速度可提升上千倍，内存效率更高一个数量级。

    

    本文研究了聚类多任务压缩感知，这是一种通过找到利用共享信息相互改进信号重建的任务集群来解决多个压缩感知任务的层次模型。现有的推断算法在高维情况下计算代价很大，并且无法很好地扩展。主要瓶颈是涉及到多个大协方差矩阵的重复矩阵求逆和对数行列式计算。我们提出了一种新的算法，通过避免显式计算这些协方差矩阵，显著加速了模型推断过程。我们的方法结合了蒙特卡罗采样和迭代线性求解器。实验证明，与现有基线相比，我们的算法可以快上数千倍，且内存效率更高一个数量级。

    This paper considers clustered multi-task compressive sensing, a hierarchical model that solves multiple compressive sensing tasks by finding clusters of tasks that leverage shared information to mutually improve signal reconstruction. The existing inference algorithm for this model is computationally expensive and does not scale well in high dimensions. The main bottleneck involves repeated matrix inversion and log-determinant computation for multiple large covariance matrices. We propose a new algorithm that substantially accelerates model inference by avoiding the need to explicitly compute these covariance matrices. Our approach combines Monte Carlo sampling with iterative linear solvers. Our experiments reveal that compared to the existing baseline, our algorithm can be up to thousands of times faster and an order of magnitude more memory-efficient.
    
[^38]: 受限强凸性下的预条件PI共识算法的线性收敛性

    Linear Convergence of Pre-Conditioned PI Consensus Algorithm under Restricted Strong Convexity. (arXiv:2310.00419v1 [math.OC])

    [http://arxiv.org/abs/2310.00419](http://arxiv.org/abs/2310.00419)

    本文在点对点多智能体网络中提出了一种使用比例积分（PI）控制策略的预条件PI共识算法，保证了其在受限强凸函数下的线性收敛性，无需个体局部代价函数的凸性，并且通过引入局部预条件进一步加速算法。

    

    本文考虑在点对点多智能体网络中解决分布式凸优化问题。网络被假定为同步和连通的。采用比例积分（PI）控制策略，开发了多种具有固定步长的算法，其中最早的是PI共识算法。利用李雅普诺夫理论，我们首次保证了具有速率匹配离散化的受限强凸函数的PI共识算法的指数收敛性，而不需要个体局部代价函数的凸性。为了加速PI共识算法，我们采用了局部预条件的形式，即常数正定矩阵，并通过数值验证其相比于突出的分布式凸优化算法的效率。

    This paper considers solving distributed convex optimization problems in peer-to-peer multi-agent networks. The network is assumed to be synchronous and connected. By using the proportional-integral (PI) control strategy, various algorithms with fixed stepsize have been developed. The earliest among them is the PI consensus algorithm. Using Lyapunov theory, we guarantee exponential convergence of the PI consensus algorithm for restricted strongly convex functions with rate-matching discretization, without requiring convexity of individual local cost functions, for the first time. In order to accelerate the PI consensus algorithm, we incorporate local pre-conditioning in the form of constant positive definite matrices and numerically validate its efficiency compared to the prominent distributed convex optimization algorithms. Unlike classical pre-conditioning, where only the gradients are multiplied by a pre-conditioner, the proposed pre-conditioning modifies both the gradients and the 
    
[^39]: 保序GFlowNets

    Order-Preserving GFlowNets. (arXiv:2310.00386v1 [cs.LG])

    [http://arxiv.org/abs/2310.00386](http://arxiv.org/abs/2310.00386)

    本研究提出了保序GFlowNets（OP-GFNs），通过学习奖励函数与候选者的排序相一致的概率进行采样，解决了使用预定义标量奖励的局限性，同时提供了证明训练过程稀疏奖励景观的理论支持。

    

    生成流网络（GFlowNets）被引入作为一种根据给定奖励概率采样多样化的候选集的方法。然而，GFlowNets只能与预定义的标量奖励一起使用，在多目标优化（MOO）任务中，这可能是计算昂贵的或者直接不可访问的。此外，为了优先识别高奖励候选者，传统做法是将奖励提高到更高的指数，而这个最优选择在不同环境下可能会有所不同。为了解决这些问题，我们提出了保序GFlowNets（OP-GFNs），它们以与提供的（部分）候选者排序一致的学习奖励函数的概率进行采样，从而消除了对奖励函数的显式表达的需求。我们在理论上证明了OP-GFNs的训练过程逐渐稀疏了学习到的奖励景观。

    Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective max
    
[^40]: 用生成对抗网络进行电力发电厂异常检测

    Anomaly Detection in Power Generation Plants with Generative Adversarial Networks. (arXiv:2310.00335v1 [cs.LG])

    [http://arxiv.org/abs/2310.00335](http://arxiv.org/abs/2310.00335)

    本研究探索了使用生成对抗网络（GANs）进行电力发电厂异常检测，结果表明GANs在这一领域具有有效性，并能帮助识别燃料消耗模式中的异常情况。

    

    异常检测是一项关键任务，涉及对与预定义模式偏离的数据点的识别，对于欺诈检测和相关活动非常有用。各种技术被用于异常检测，但最近的研究表明，深度学习方法凭借其辨别复杂数据模式的能力，非常适合这项任务。本研究探索了在电力发电厂中使用生成对抗网络（GANs）进行异常检测。本次研究使用的数据集包括来自电信公司运营的发电厂的燃料消耗记录。最初，这些数据是基于公司基站上的发电机组燃料消耗模式中观察到的不规则性而采集的。数据集根据特定变量被分为正常和异常数据点，其中64.88%被分类为正常，35.12%被分类为异常。特征重要性的分析结果表明了GANs在电力发电厂异常检测中的有效性。

    Anomaly detection is a critical task that involves the identification of data points that deviate from a predefined pattern, useful for fraud detection and related activities. Various techniques are employed for anomaly detection, but recent research indicates that deep learning methods, with their ability to discern intricate data patterns, are well-suited for this task. This study explores the use of Generative Adversarial Networks (GANs) for anomaly detection in power generation plants. The dataset used in this investigation comprises fuel consumption records obtained from power generation plants operated by a telecommunications company. The data was initially collected in response to observed irregularities in the fuel consumption patterns of the generating sets situated at the company's base stations. The dataset was divided into anomalous and normal data points based on specific variables, with 64.88% classified as normal and 35.12% as anomalous. An analysis of feature importance
    
[^41]: 神经网络的记忆化：超越最坏情况

    Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v1 [stat.ML])

    [http://arxiv.org/abs/2310.00327](http://arxiv.org/abs/2310.00327)

    本文研究了神经网络的插值问题，提出了一种简单的随机算法，在给定的数据集和两个类的情况下，能够以很高的概率构建一个插值的神经网络。这些结果与训练数据规模无关。

    

    在实践中，深度神经网络通常能够轻松地插值其训练数据。为了理解这一现象，许多研究都旨在量化神经网络架构的记忆能力：即在任意放置这些点并任意分配标签的情况下，架构能够插值的最大点数。然而，对于实际数据，人们直觉地期望存在一种良性结构，使得插值在比记忆能力建议的较小网络尺寸上已经发生。在本文中，我们通过采用实例特定的观点来研究插值。我们引入了一个简单的随机算法，它可以在多项式时间内给定一个固定的有限数据集和两个类的情况下，以很高的概率构建出一个插值三层神经网络。所需的参数数量与这两个类的几何特性及其相互排列有关。因此，我们获得了与训练数据规模无关的保证。

    In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of t
    
[^42]: 最大间隔分类器的普适性

    Universality of max-margin classifiers. (arXiv:2310.00176v1 [math.ST])

    [http://arxiv.org/abs/2310.00176](http://arxiv.org/abs/2310.00176)

    该论文研究了最大间隔分类器在非高斯特征映射和高维渐近条件下的普适性问题。

    

    最大间隔二元分类是机器学习中最基础的算法之一，然而对于非高斯特征的映射函数和高维渐近条件下的误分类错误的作用仍然知之甚少。我们考虑观测到二元标签 $y_i$，通过随机映射函数 ${\boldsymbol \phi}:\mathbb{R}^d \to\mathbb{R}^p$，将 $d$ 维协变量 ${\boldsymbol z}_i$ 映射到 $p$ 维空间，或者对于非高斯独立的 $p$ 维特征。在这个背景下，我们研究了两个基本问题：$(i)$ 数据在过参数化比率 $p/n$ 下何时变为线性可分？$(ii)$ 最大间隔分类器的泛化误差是多少？在高维条件下，特征数 $p$、样本数 $n$ 和输入维度 $d$（非线性特征化设置下）发散的情况下，我们证明了一个普适的结果。

    Maximum margin binary classification is one of the most fundamental algorithms in machine learning, yet the role of featurization maps and the high-dimensional asymptotics of the misclassification error for non-Gaussian features are still poorly understood. We consider settings in which we observe binary labels $y_i$ and either $d$-dimensional covariates ${\boldsymbol z}_i$ that are mapped to a $p$-dimension space via a randomized featurization map ${\boldsymbol \phi}:\mathbb{R}^d \to\mathbb{R}^p$, or $p$-dimensional features of non-Gaussian independent entries. In this context, we study two fundamental questions: $(i)$ At what overparametrization ratio $p/n$ do the data become linearly separable? $(ii)$ What is the generalization error of the max-margin classifier?  Working in the high-dimensional regime in which the number of features $p$, the number of samples $n$ and the input dimension $d$ (in the nonlinear featurization setting) diverge, with ratios of order one, we prove a unive
    
[^43]: 关于过参数化神经网络理论与实践的脱节

    On the Disconnect Between Theory and Practice of Overparametrized Neural Networks. (arXiv:2310.00137v1 [cs.LG])

    [http://arxiv.org/abs/2310.00137](http://arxiv.org/abs/2310.00137)

    本文研究了神经网络在无穷宽度极限下的行为，并与核方法建立了联系。虽然在合成架构中展示了一些优势，如更快的优化和可靠的不确定性量化，但实际相关的架构需要比深度大很多倍的宽度才能实现这些优势。

    

    神经网络（NNs）的无穷宽度极限作为分析大规模、过参数化网络行为的理论框架已经引起了重要关注。通过接近无限宽度，NNs可以有效地收敛到一个具有由神经切线核(NTK)特征化的线性模型。这建立了NNs和核方法之间的联系，后者是被充分理解的。基于这种联系，已经假设并在合成架构中从理论上和算法上验证了一些优势。这些优势包括更快的优化、可靠的不确定性量化和改进的持续学习能力。然而，目前量化向核心领域收敛速度的结果表明，利用这些优势需要比深度大几个数量级的架构。这个假设引发了对实际相关架构是否表现如预测的担忧。

    The infinite-width limit of neural networks (NNs) has garnered significant attention as a theoretical framework for analyzing the behavior of large-scale, overparametrized networks. By approaching infinite width, NNs effectively converge to a linear model with features characterized by the neural tangent kernel (NTK). This establishes a connection between NNs and kernel methods, the latter of which are well understood. Based on this link, theoretical benefits and algorithmic improvements have been hypothesized and empirically demonstrated in synthetic architectures. These advantages include faster optimization, reliable uncertainty quantification and improved continual learning. However, current results quantifying the rate of convergence to the kernel regime suggest that exploiting these benefits requires architectures that are orders of magnitude wider than they are deep. This assumption raises concerns that practically relevant architectures do not exhibit behavior as predicted via 
    
[^44]: Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit. (arXiv:2310.00110v1 [stat.ML]) - 全局拟合中的梯度和不确定性增强的顺序采样

    Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit. (arXiv:2310.00110v1 [stat.ML])

    [http://arxiv.org/abs/2310.00110](http://arxiv.org/abs/2310.00110)

    GUESS is a new sampling strategy for global fit that combines predictive posterior uncertainty and higher-order Taylor expansion values to reduce the number of samples needed for accurate surrogate modeling. - GUESS 是一种新的全局拟合采样策略，结合了预测后验不确定性和高阶泰勒展开值，可以减少准确的代理模型所需的样本数量。

    

    基于机器学习方法的代理模型已成为现代工程的重要组成部分，用以取代昂贵的计算机模拟。创建代理模型所使用的数据对于模型的准确性至关重要，但往往受到成本和时间限制的限制。自适应采样策略已被证明可以减少创建准确模型所需的样本数量。本文提出了一种新的全局拟合采样策略，称为Gradient and Uncertainty Enhanced Sequential Sampling (GUESS)。采用两个术语的收购功能：用于探索未见区域的代理模型的预测后验不确定性及用于开发的二阶及更高阶泰勒展开值的加权逼近值。尽管迄今为止已提出了各种采样策略，但选择合适的方法并不容易。因此，我们将我们提出的策略与基于26个不同统计学和机器学习方法的9种自适应采样策略进行了比较。

    Surrogate models based on machine learning methods have become an important part of modern engineering to replace costly computer simulations. The data used for creating a surrogate model are essential for the model accuracy and often restricted due to cost and time constraints. Adaptive sampling strategies have been shown to reduce the number of samples needed to create an accurate model. This paper proposes a new sampling strategy for global fit called Gradient and Uncertainty Enhanced Sequential Sampling (GUESS). The acquisition function uses two terms: the predictive posterior uncertainty of the surrogate model for exploration of unseen regions and a weighted approximation of the second and higher-order Taylor expansion values for exploitation. Although various sampling strategies have been proposed so far, the selection of a suitable method is not trivial. Therefore, we compared our proposed strategy to 9 adaptive sampling strategies for global surrogate modeling, based on 26 diff
    
[^45]: 使用差分隐私的联邦学习进行端到端语音识别

    Federated Learning with Differential Privacy for End-to-End Speech Recognition. (arXiv:2310.00098v1 [cs.LG])

    [http://arxiv.org/abs/2310.00098](http://arxiv.org/abs/2310.00098)

    本文提出了一种基于联邦学习和差分隐私的端到端语音识别方法，探索了大型Transformer模型的不同方面，并建立了基线结果。

    

    联邦学习是一种有前景的训练机器学习模型的方法，但在自动语音识别领域仅限于初步探索。此外，联邦学习不能本质上保证用户隐私，并需要差分隐私来提供稳健的隐私保证。然而，我们还不清楚在自动语音识别中应用差分隐私的先前工作。本文旨在通过为联邦学习提供差分隐私的自动语音识别基准，并建立第一个基线来填补这一研究空白。我们扩展了现有的联邦学习自动语音识别研究，探索了最新的大型端到端Transformer模型的不同方面：架构设计，种子模型，数据异质性，领域转移，以及cohort大小的影响。通过合理的中央聚合数量，我们能够训练出即使在异构数据、来自另一个领域的种子模型或无预先训练的情况下仍然接近最优的联邦学习模型。

    While federated learning (FL) has recently emerged as a promising approach to train machine learning models, it is limited to only preliminary explorations in the domain of automatic speech recognition (ASR). Moreover, FL does not inherently guarantee user privacy and requires the use of differential privacy (DP) for robust privacy guarantees. However, we are not aware of prior work on applying DP to FL for ASR. In this paper, we aim to bridge this research gap by formulating an ASR benchmark for FL with DP and establishing the first baselines. First, we extend the existing research on FL for ASR by exploring different aspects of recent $\textit{large end-to-end transformer models}$: architecture design, seed models, data heterogeneity, domain shift, and impact of cohort size. With a $\textit{practical}$ number of central aggregations we are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even with heterogeneous data, a seed model from another domain, or no pre-trai
    
[^46]: 稀疏变分高斯过程回归的点估计和不确定性量化与布朗运动先验的研究

    Pointwise uncertainty quantification for sparse variational Gaussian process regression with a Brownian motion prior. (arXiv:2310.00097v1 [math.ST])

    [http://arxiv.org/abs/2310.00097](http://arxiv.org/abs/2310.00097)

    本文研究了稀疏变分高斯过程回归中点估计和不确定性量化的方法，通过使用布朗运动先验和特征向量引导变量，推导了频率派可信区间的理论保证和限制，并在足够多的引导变量的情况下精确定义了渐近频率派覆盖，从而推断出这个变分方法的可信区间何时保守，何时过于自信/误导。数值实验证明了这些结果的适用性，并讨论了与其他常见高斯过程先验的相关性。

    

    本文研究了使用特征向量引导变量的稀疏变分高斯过程方法的点估计和不确定性量化。对于具有重标定布朗运动先验的情况，我们推导了点化可信区间的频率派大小和覆盖的理论保证和限制。通过充分的引导变量，我们精确地描述了渐近频率派覆盖，推断了这个变分方法的可信区间何时保守，何时过于自信/误导。我们通过数值实验说明了我们的结果的适用性，并讨论了与其他常见高斯过程先验的联系。

    We study pointwise estimation and uncertainty quantification for a sparse variational Gaussian process method with eigenvector inducing variables. For a rescaled Brownian motion prior, we derive theoretical guarantees and limitations for the frequentist size and coverage of pointwise credible sets. For sufficiently many inducing variables, we precisely characterize the asymptotic frequentist coverage, deducing when credible sets from this variational method are conservative and when overconfident/misleading. We numerically illustrate the applicability of our results and discuss connections with other common Gaussian process priors.
    
[^47]: 无标记的域外数据改善了泛化能力

    Unlabeled Out-Of-Domain Data Improves Generalization. (arXiv:2310.00027v1 [stat.ML])

    [http://arxiv.org/abs/2310.00027](http://arxiv.org/abs/2310.00027)

    这个论文提出了一种新的框架，可以将无标记的域外数据纳入半监督分类问题，从而改善泛化能力。该框架结合了分布鲁棒优化与自监督训练，并利用了高效的多项式时间算法。在理论上，该框架在高斯混合分类问题中得到了验证。

    

    我们提出了一种将无标记数据纳入半监督分类问题的新框架，其中考虑了最小化鲁棒性损失函数或非鲁棒性损失函数的情景。值得注意的是，我们允许无标记样本在总变差意义上略微偏离域内分布。我们的框架的核心思想是将分布鲁棒优化（DRO）与自监督训练相结合。因此，我们还利用了训练阶段的高效多项式时间算法。从理论上讲，我们将我们的框架应用于在$\mathbb{R}^d$中的两个高斯混合分类问题，除了来自真实分布的$m$个独立标记样本之外，还给出了一组$n$个（通常$n\gg m$）域外和无标记样本。已知仅使用标记数据，泛化误差可以通过$\propto\left(d/m\right)$进行界定。

    We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\propto\left(d/m\right
    
[^48]: Stackelberg批量策略学习

    Stackelberg Batch Policy Learning. (arXiv:2309.16188v1 [stat.ML])

    [http://arxiv.org/abs/2309.16188](http://arxiv.org/abs/2309.16188)

    Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。

    

    批量强化学习定义了从固定的数据批次中进行学习，缺乏详尽的探索。最坏情况下的最优算法使用经验数据来校准价值函数模型，并在学习模型下执行某种悲观评估，已经成为批量强化学习中一种有前景的范式。然而，对于这个流派的现代研究通常忽视了优化景观中隐藏的分层决策结构。在本文中，我们采用博弈论的观点，将策略学习图表建模为具有领导者-跟随者结构的两人零和博弈。我们提出了一种新颖的基于随机梯度的学习算法：StackelbergLearner，领导者根据其目标的全导数进行更新，而不是通常的个体梯度，而跟随者进行个体更新并确保过渡一致的悲观推理。推导出的学习动力

    Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynam
    
[^49]: SGD在具有接近最优样本复杂度的双层神经网络中寻找并调整特征：以XOR问题为案例研究

    SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])

    [http://arxiv.org/abs/2309.15111](http://arxiv.org/abs/2309.15111)

    本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。

    

    本文研究了小批量随机梯度下降（SGD）在具有二次真实函数分隔数据的双层神经网络上的优化过程。我们证明，对于从$d$维布尔超立方体中由二次“XOR”函数$y = -x_ix_j$标记的数据，可以通过标准小批量SGD在逻辑损失上同时训练两层ReLU激活的双层神经网络，用$d \:\text{polylog}(d)$个样本将其训练到人口误差为$o(1)$的程度。据我们所知，这是首次给出了在标准神经网络上以及标准训练下，对于在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。我们的主要技术是展示网络演化有两个阶段：一个”信号发现“阶段，在此网络规模较小且许多神经元独立演化以寻找特征，以及一个”信号密集“阶段，其中许多神经元相互作用以优化预测。

    In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
    
[^50]: 模型无关的图神经网络用于整合局部和全局信息的研究

    A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])

    [http://arxiv.org/abs/2309.13459](http://arxiv.org/abs/2309.13459)

    MaGNet是一种模型无关的图神经网络框架，能够顺序地整合不同顺序的信息，并通过识别有影响力的紧凑图结构提供有意义且可解释的结果。

    

    图神经网络（GNNs）在各种以图为重点的任务中取得了令人满意的性能。尽管取得了成功，但现有的GNN存在两个重要限制：由于黑盒特性，结果缺乏可解释性；无法学习不同顺序的表示。为了解决这些问题，我们提出了一种新的模型无关的图神经网络（MaGNet）框架，能够顺序地整合不同顺序的信息，从高阶邻居中提取知识，并通过识别有影响力的紧凑图结构提供有意义且可解释的结果。特别地，MaGNet由两个组件组成：图拓扑下复杂关系的潜在表示的估计模型和识别有影响力的节点、边和重要节点特征的解释模型。从理论上，我们通过经验Rademacher复杂度建立了MaGNet的泛化误差界，并展示了其强大的能力。

    Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
    
[^51]: 分布偏移感知的强化学习非同策略区间估计方法：一个统一的误差量化框架

    Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework. (arXiv:2309.13278v1 [stat.ML])

    [http://arxiv.org/abs/2309.13278](http://arxiv.org/abs/2309.13278)

    本研究提出了一个对于强化学习非同策略评估问题的统一误差量化框架，并解决了分布偏移的挑战。通过在一个单一的区间内共同量化两个估计误差源，该框架揭示了之前隐藏的误差权衡，从而提高了置信区间的准确性。

    

    本文研究了在无限时间马尔可夫决策过程中的高置信度非同策略评估问题，其目标是仅利用从未知行为策略预先收集的离线数据为目标策略的值建立一个置信区间（CI）。该任务面临两个主要挑战：在CI估计中提供全面且严格的误差量化，并解决由目标策略产生的分布偏移问题，该分布与离线数据生成过程之间存在差异。受到创新的统一误差分析的启发，我们在一个单一的区间内共同量化两个估计误差来源：在建模边际化重要性权重时的规范不准确误差和抽样导致的统计不确定性。这一统一的框架揭示了误差之间以前隐藏的权衡，从而削弱了CI的紧密性。通过依靠精心设计的判别函数，提出了一种新的解决方案来克服分布偏移问题。

    We study high-confidence off-policy evaluation in the context of infinite-horizon Markov decision processes, where the objective is to establish a confidence interval (CI) for the target policy value using only offline data pre-collected from unknown behavior policies. This task faces two primary challenges: providing a comprehensive and rigorous error quantification in CI estimation, and addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process. Motivated by an innovative unified error analysis, we jointly quantify the two sources of estimation errors: the misspecification error on modeling marginalized importance weights and the statistical uncertainty due to sampling, within a single interval. This unified framework reveals a previously hidden tradeoff between the errors, which undermines the tightness of the CI. Relying on a carefully designed discriminator function, the proposed
    
[^52]: 信息价值（IV）的统计假设检验

    Statistical Hypothesis Testing for Information Value (IV). (arXiv:2309.13183v1 [math.ST])

    [http://arxiv.org/abs/2309.13183](http://arxiv.org/abs/2309.13183)

    该论文提出了信息价值（IV）的统计假设检验方法，为模型建立前的特征选择提供了理论框架，并通过实验证明了该方法的有效性。

    

    信息价值（IV）是模型建立前进行特征选择的一种常用技术。目前存在一些实际标准，但基于IV的判断是否一个预测因子具有足够的预测能力的理论依据依然神秘且缺乏。然而，关于该技术的数学发展和统计推断方法在文献中几乎没有提及。在本研究中，我们提出了一个关于IV的理论框架，并提出了一种非参数假设检验方法来测试预测能力。我们展示了如何高效计算检验统计量，并在模拟数据上研究其表现。此外，我们将这一方法应用于银行欺诈数据，并提供了一个实现我们结果的Python库。

    Information value (IV) is a quite popular technique for feature selection prior to the modeling phase. There are practical criteria, but at the same time mysterious and lacking theoretical arguments, based on the IV, to decide if a predictor has sufficient predictive power to be considered in the modeling phase. However, the mathematical development and statistical inference methods for this technique is almost non-existent in the literature. In this work we present a theoretical framework for the IV and propose a non-parametric hypothesis test to test the predictive power. We show how to efficiently calculate the test statistic and study its performance on simulated data. Additionally, we apply our test on bank fraud data and provide a Python library where we implement our results.
    
[^53]: 锐度感知最小化和稳定性边界。

    Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])

    [http://arxiv.org/abs/2309.12488](http://arxiv.org/abs/2309.12488)

    本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。

    

    最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。

    Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
    
[^54]: $\mathbb{T}$-随机图

    $\mathbb{T}$-Stochastic Graphs. (arXiv:2309.01301v1 [stat.AP])

    [http://arxiv.org/abs/2309.01301](http://arxiv.org/abs/2309.01301)

    本文研究了社交网络中层次聚类的统计方法，并提出了一种新的概率模型$\mathbb{T}$-随机图，用于解决现有方法中存在的不稳定性问题。

    

    先前的社交网络分析中关于层次聚类的统计方法都构建了一个“超度量”的层次结构。虽然超度量性的假设在系统发生学文献中已经得到了讨论和研究，但在社交网络文献中尚未得到承认。我们展示了网络中的“非超度量结构”引入了现有自上而下恢复算法的显著不稳定性。为了解决这个问题，我们引入了一种不稳定性诊断图并使用它来检查一系列经验网络。这些网络似乎违反了“超度量”假设。我们提出了一种看似简单但又很通用的概率模型类，称为$\mathbb{T}$-随机图，它对潜在层次结构不施加拓扑限制。为了说明这个模型，我们提出了六种替代性的层次网络模型，然后证明了所有六种模型都等价于$\mathbb{T}$-随机图模型。

    Previous statistical approaches to hierarchical clustering for social network analysis all construct an "ultrametric" hierarchy. While the assumption of ultrametricity has been discussed and studied in the phylogenetics literature, it has not yet been acknowledged in the social network literature. We show that "non-ultrametric structure" in the network introduces significant instabilities in the existing top-down recovery algorithms. To address this issue, we introduce an instability diagnostic plot and use it to examine a collection of empirical networks. These networks appear to violate the "ultrametric" assumption. We propose a deceptively simple and yet general class of probabilistic models called $\mathbb{T}$-Stochastic Graphs which impose no topological restrictions on the latent hierarchy. To illustrate this model, we propose six alternative forms of hierarchical network models and then show that all six are equivalent to the $\mathbb{T}$-Stochastic Graph model. These alternativ
    
[^55]: 关于Adam的隐式偏差

    On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])

    [http://arxiv.org/abs/2309.00079](http://arxiv.org/abs/2309.00079)

    本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。

    

    在以前的文献中，后向误差分析被用来找到近似梯度下降轨迹的常微分方程（ODEs）。发现有限步长会隐式地规范化解决方案，因为出现在ODE中的项会惩罚损失梯度的二范数。我们证明了RMSProp和Adam中是否存在类似的隐式规范化取决于它们的超参数和训练阶段，但涉及的“范数”不同：对应的ODE项要么惩罚（扰动的）损失梯度的一范数，要么相反地阻止其减小（后一种情况是典型的）。我们还进行了数值实验，并讨论了这些证明事实如何影响泛化。

    In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
    
[^56]: 通过正则化Wasserstein Proximals实现无噪声的抽样算法

    Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])

    [http://arxiv.org/abs/2308.14945](http://arxiv.org/abs/2308.14945)

    本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。

    

    本文考虑由潜势函数控制的分布抽样问题。本文提出了一种显式的基于评分的确定性马尔科夫链蒙特卡洛方法，使得粒子的演化变为确定性的，而不是随机微分方程的演化。评分项由正则化的Wasserstein proximal以闭合形式给出，使用采样来近似核卷积。我们在不同问题上展示了快速收敛，并且与未调整Langevin算法和Metropolis调整Langevin算法相比，显示了高斯分布的混合时间边界的改善维度依赖性。我们还推导了二次潜势函数每次迭代的分布的闭合形式表达式，表征了方差降低。实证结果表明，粒子的行为是有组织的，位于潜势的等值线上。此外，后验均值估计结果显示了该方法的有效性。

    We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
    
[^57]: 无等渗性的蒙特卡洛采样：一种逆扩散方法

    Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])

    [http://arxiv.org/abs/2307.02037](http://arxiv.org/abs/2307.02037)

    本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。

    

    现代生成模型的有效性通常取决于扩散路径上得分估计的精度，重点关注扩散模型及其生成高质量数据样本的能力。本研究深入探讨了通过逆扩散进行后验采样的潜力。通过对采样文献进行研究，发现可以通过转移核的分解将得分估计转化为均值估计问题。通过估计辅助分布的均值，逆扩散过程可以产生一种新颖的后验采样算法，该算法与传统的基于梯度的马尔科夫链蒙特卡洛（MCMC）方法不同。我们提供了总变差距离下的收敛分析，并证明了所提算法的等渗性依赖性相对较低，比传统的MCMC技术表现出更高的高维采样性能。

    The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
    
[^58]: 复杂数据集的底层缩放定律和普适统计结构

    The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets. (arXiv:2306.14975v1 [cs.LG])

    [http://arxiv.org/abs/2306.14975](http://arxiv.org/abs/2306.14975)

    本文研究了复杂数据集中的底层缩放定律和普适统计结构。通过将数据类比为物理系统，并应用统计物理学和随机矩阵理论的方法，揭示了特征-特征协方差矩阵的局部和全局特征值统计量的规律。研究发现，在无关随机数据和真实数据之间存在显著差异，并且可以通过引入长程相关性完全恢复缩放行为。同时，生成的数据和真实世界数据都属于混沌系统，并在较小的数据集大小上即可体现随机矩阵理论的统计行为。

    

    我们研究了在真实世界的复杂数据集和人工生成的数据集中都出现的普遍特征。我们将数据类比为物理系统，并利用统计物理学和随机矩阵理论的工具揭示其底层结构。我们重点分析了特征-特征协方差矩阵，分析了其局部和全局特征值统计量。我们的主要观察结果是：(i) 大部分特征值呈现的幂律缩放在无相关随机数据和真实数据之间存在显著差异，(ii) 通过简单地引入长程相关性，可以完全恢复这种缩放行为到合成数据中，(iii) 从随机矩阵理论的角度看，生成的数据集和真实世界数据集属于同一个普适性类别，都是混沌系统而非可积系统，(iv) 预期的随机矩阵理论统计行为在相对较小的数据集大小上就已经在经验协方差矩阵中得到体现。

    We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated random data compared to real-world data, (ii) this scaling behavior can be completely recovered by introducing long range correlations in a simple way to the synthetic data, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conv
    
[^59]: 面向扩散式生成模型的非渐进快速收敛方法

    Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models. (arXiv:2306.09251v1 [stat.ML])

    [http://arxiv.org/abs/2306.09251](http://arxiv.org/abs/2306.09251)

    该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。

    

    扩散模型通过学习反转马尔可夫扩散过程将噪音转化为新数据实例，在当代生成建模领域中已成为基石。虽然它们的实用性现在已被广泛认可，但其理论基础仍然不够成熟。在这项工作中，我们开发了一套非渐进理论，以理解离散时间下扩散模型的数据生成过程，假设可以获得（Stein）得分函数的可靠估计。针对一种流行的确定性采样器（基于概率流ODE），我们建立了一个与 $T$（总步数）成比例的收敛速度，改进了过去的结果；对于另一种主流的随机采样器（即一种去噪扩散概率模型（DDPM）），我们导出了一个与 $1/\sqrt{T}$ 成比例的收敛速度，与最先进的理论相匹配。我们的理论对目标数据分布只作出最小的假设（例如，没有平滑）。

    Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoot
    
[^60]: 噪声稳定优化对于具有最优收敛率的平坦极小值的影响

    Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])

    [http://arxiv.org/abs/2306.08553](http://arxiv.org/abs/2306.08553)

    本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。

    

    本文研究通过加入加权扰动来找到平坦的极小值。给定一个非凸函数$f:\mathbb{R}^d\rightarrow \mathbb{R}$和一个$d$维分布$\mathcal{P}$，我们扰动$f$的权重，并定义$F(W)=\mathbb{E}[f({W+U})]$，其中$U$是一个从$\mathcal{P}$中随机抽取的样本。这个过程通过$f$的海森矩阵的迹来诱导正则化，以适应于小的、各向同性的高斯扰动。因此，加权扰动的函数偏向于带有低海森矩阵迹的极小值。本文提出了一种类似于SGD的算法，在计算梯度之前注入随机噪声，同时利用$\mathcal{P}$的对称性来减少方差。我们还提供了严格的分析，证明了...

    We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing
    
[^61]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^62]: 通过RKHS逼近理解基于增广的自监督表示学习

    Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation. (arXiv:2306.00788v1 [cs.LG])

    [http://arxiv.org/abs/2306.00788](http://arxiv.org/abs/2306.00788)

    本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。

    

    好的数据增强是自监督表示学习（如对比学习和掩码语言建模）实现经验成功的关键因素之一，但其在学习好的表示方面的理论理解仍然有限。最近的工作建立了自监督学习和逼近图拉普拉斯算子的顶部特征空间之间的联系。在这项工作中，我们利用这一洞察力对基于增广的预训练进行统计分析。我们从保持等距的属性出发，这是由增强给出的目标函数的关键几何特征。我们的第一主要定理为任意编码器提供了接近紧密的上限，用于估计通过在编码器之上拟合线性探测器而产生的估计误差和编码器学习的RKHS的逼近误差。我们的第二个主要定理表明，在温和条件下，可以通过RKHS函数任意精确地逼近增广函数。这个结果意味着，随着增广函数质量的提高，学习的编码器的表示能力也会提高。我们的分析还揭示了自监督学习中批量归一化和数据增强的作用。

    Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main
    
[^63]: 扩散模型的几何视角

    A Geometric Perspective on Diffusion Models. (arXiv:2305.19947v1 [cs.CV])

    [http://arxiv.org/abs/2305.19947](http://arxiv.org/abs/2305.19947)

    本文研究了扩散模型的几何结构，发现通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接了数据分布和噪声分布，建立了基于ODE的最优采样和经典的均值漂移算法之间的理论关系。

    

    近年来，针对扩散模型的高效训练和快速采样方法取得了显著进展。最近的一个重要进展是使用随机微分方程（SDE）来描述数据扰动和生成建模，以实现统一的数学框架。本文揭示了扩散模型的几个有趣的几何结构，并为其采样动力学提供了简单而强大的解释。通过仔细检查一种流行的方差爆炸SDE及其保持边际的普通微分方程（ODE）用于采样，我们发现数据分布和噪声分布通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接，即使在视觉质量方面也收敛更快。我们还建立起基于ODE的最优采样和经典的均值漂移（寻找模式）算法之间的理论关系。

    Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with w
    
[^64]: Auto-tune: 神经网络的先验与后验PAC-Bayes优化

    Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])

    [http://arxiv.org/abs/2305.19243](http://arxiv.org/abs/2305.19243)

    通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。

    

    通过精心设计训练过程，可以显著提高神经网络的泛化能力。目前最先进的训练方法涉及使用随机梯度下降或Adam优化算法，以及额外的正则化技术，如权重衰减、Dropout或噪声注入。通过网格搜索调整数量众多的超参数才能达到最优泛化，这可能耗时，并需要额外的验证数据集。为解决这个问题，我们引入了一个切实可行的PAC-Bayes训练框架，几乎是无需调整，也不需要额外的正则化，而在完成网格搜索和加入额外正则化后，达到了与SGD/Adam可比较的测试性能。我们提出的算法展示了PAC训练在深度神经网络上实现最先进性能的显著潜力。

    It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit
    
[^65]: 如何有效地在强化学习中进行人类反馈查询？

    How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])

    [http://arxiv.org/abs/2305.18505](http://arxiv.org/abs/2305.18505)

    该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。

    

    人类反馈强化学习（RLHF）是一种范例，在此范例下，RL代理学习使用对轨迹的成对优先级反馈来最优化任务，而不是使用明确的奖励信号。尽管RLHF在微调语言模型方面已经取得了实用成功，但现有的实证研究并未解决如何高效采样轨迹对以查询人类反馈的挑战。在本研究中，我们提出了一种有效的采样方法，用于获取探索性轨迹，在收集任何人类反馈之前，使学习隐藏的奖励函数更加准确。理论分析表明，与现有文献相比，我们的算法在线性参数化和未知过渡的基于偏好模型下学习最优策略所需的人类反馈更少。具体而言，我们的框架可以纳入线性和低秩MDPs。此外，我们研究了使用基于行动比较的反馈的RLHF，并介绍了一种高效的采样方法，以在优化具有有限反馈的任务时获得探索性轨迹。

    Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
    
[^66]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^67]: 学习两层神经网络：一次(巨大)的步骤。

    Learning Two-Layer Neural Networks, One (Giant) Step at a Time. (arXiv:2305.18270v1 [stat.ML])

    [http://arxiv.org/abs/2305.18270](http://arxiv.org/abs/2305.18270)

    本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。

    

    我们研究了浅层神经网络的训练动态，研究了有限数量的大批量梯度下降步骤有助于在核心范围之外促进特征学习的条件。我们比较了批量大小和多个(但有限的)步骤的影响。我们分析了单步骤过程，发现批量大小为$n=O(d)$可以促进特征学习，但只适合学习单一方向或单索引模型。相比之下，$n=O(d^2)$对于学习多个方向和专业化至关重要。此外，我们证明“硬”方向缺乏前$\ell$个Hermite系数，仍未被发现，并且需要批量大小为$n=O(d^\ell)$才能被梯度下降捕获。经过几次迭代，情况发生变化：批量大小为$n=O(d)$足以学习新的目标方向，这些方向在Hermite基础上线性连接到之前学习的方向所涵盖的子空间。

    We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions,
    
[^68]: 在缺少球形替代方案下测试离散分布直方图均匀性的极小极大风险

    The minimax risk in testing the histogram of discrete distributions for uniformity under missing ball alternatives. (arXiv:2305.18111v1 [math.ST])

    [http://arxiv.org/abs/2305.18111](http://arxiv.org/abs/2305.18111)

    研究了离散分布样本对于类别间的均匀分布拟合问题下的极小极大风险，在缺少球形替代方案的情况下进行了讨论，通过离散直方图进行检验，获得了一种具有精确刻画的检验方法，并在实证研究中表现出了显著性。

    

    我们考虑测试一个来自许多类别的离散样本对于类别间的均匀分布拟合的问题。作为另一类替代假设，我们考虑去除半径为$\epsilon$的$\ell_p$球形替代方案，其中$p\leq 2$。我们给出了基于直方图（缺失类别、单例、碰撞的数量）的检验在样本数和维数趋向无穷大，$\epsilon\to0$时，渐进极小极大风险的一个精确刻画。例如，当$p=1$且期望样本数$n$与类别数$N$的比值很小（也称为“次线性”区域）时，渐进极小极大风险$R^*_\epsilon$趋近于$2\bar{\Phi}\left(n\epsilon^2/\sqrt{8N}\right)$，其中$\bar{\Phi}(x)$是正态残存函数。在一系列问题参数范围内的实证研究表明，这个估计在有限样本中很精确，并且我们的检验显著。

    We consider the problem of testing the fit of a discrete sample of items from many categories to the uniform distribution over the categories. As a class of alternative hypotheses, we consider the removal of an $\ell_p$ ball of radius $\epsilon$ around the uniform rate sequence for $p \leq 2$. We deliver a sharp characterization of the asymptotic minimax risk when $\epsilon \to 0$ as the number of samples and number of dimensions go to infinity, for testing based on the occurrences' histogram (number of absent categories, singletons, collisions, ...). For example, for $p=1$ and in the limit of a small expected number of samples $n$ compared to the number of categories $N$ (aka "sub-linear" regime), the minimax risk $R^*_\epsilon$ asymptotes to $2 \bar{\Phi}\left(n \epsilon^2/\sqrt{8N}\right) $, with $\bar{\Phi}(x)$ the normal survival function. Empirical studies over a range of problem parameters show that this estimate is accurate in finite samples, and that our test is significantly 
    
[^69]: 通过草图技术，将粒子方法和张量网络方法结合用于偏微分方程求解

    Combining Particle and Tensor-network Methods for Partial Differential Equations via Sketching. (arXiv:2305.17884v1 [math.NA])

    [http://arxiv.org/abs/2305.17884](http://arxiv.org/abs/2305.17884)

    本文提出了通过草图技术将粒子方法和张量网络方法结合的方法用于解决高维偏微分方程。这种方法包括粒子模拟和张量网络重新估计，并可用作粒子数控制的可替代方法。在模拟Fokker-Planck方程和量子虚时间演化方面，该方法表现出通用性和灵活性。

    

    本文提出了一种解决高维偏微分方程的张量网络框架，其中我们采用粒子模拟更新解决方案，并使用最近提出的张量列车草图技术将新解决方案重新估计为张量网络。我们的方法还可以被解释为通过假设粒子来自底层张量网络来执行粒子数控制的可替代方法。我们通过将其应用于两种特定的情景来展示我们方法的通用性和灵活性：通过Langevin动力学模拟Fokker-Planck方程和通过辅助场量子蒙特卡罗模拟量子虚时间演化。

    In this paper, we propose a general framework for solving high-dimensional partial differential equations with tensor networks. Our approach offers a comprehensive solution methodology, wherein we employ a combination of particle simulations to update the solution and re-estimations of the new solution as a tensor-network using a recently proposed tensor train sketching technique. Our method can also be interpreted as an alternative approach for performing particle number control by assuming the particles originate from an underlying tensor network. We demonstrate the versatility and flexibility of our approach by applying it to two specific scenarios: simulating the Fokker-Planck equation through Langevin dynamics and quantum imaginary time evolution via auxiliary-field quantum Monte Carlo.
    
[^70]: 具有人类反馈的可证明的离线强化学习

    Provable Offline Reinforcement Learning with Human Feedback. (arXiv:2305.14816v1 [cs.LG])

    [http://arxiv.org/abs/2305.14816](http://arxiv.org/abs/2305.14816)

    本文提出了一种具有人类反馈的离线强化学习算法，解决了如何估计隐式奖励以及在置信集周围解决规划问题的方法。此外，作者提出了一个能够使用多项式数量的样本学习任何目标策略的新保证，同时引入了一个新的单策略集中系数来衡量目标策略的覆盖范围。

    

    本文研究了离线强化学习的问题，其中反馈是以轨迹对之间的偏好形式提供的。我们提出的算法包括两个主要步骤：（1）使用通用函数逼近从离线数据估计隐式奖励，和（2）在MLE周围的置信集上解决分布鲁棒的规划问题。我们考虑了通用的奖励设置，其中奖励可以在整个轨迹上定义，并提供了一个新的保证，只要目标策略被离线数据覆盖，我们就可以使用多项式数量的样本来学习任何目标策略。为了衡量目标策略的覆盖范围，我们引入了一个新的单策略集中系数，可以通过每个轨迹的集中系数上界来上界化。

    In this paper, we investigate the problem of offline reinforcement learning with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coe
    
[^71]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^72]: 用扩散模型解决反问题的变分视角

    A Variational Perspective on Solving Inverse Problems with Diffusion Models. (arXiv:2305.04391v1 [cs.LG])

    [http://arxiv.org/abs/2305.04391](http://arxiv.org/abs/2305.04391)

    该论文提出了一种通过去噪扩散过程自然地导致正则化的变分方法（RED-Diff），可用于解决不同反问题。加权机制可以衡量不同时间步长的去噪器的贡献。

    

    扩散模型已成为视觉领域基础模型的关键支柱之一。其中一个关键应用是通过单个扩散先验普遍解决不同的反问题，而无需为每个任务重新训练。然而，由于扩散过程的非线性和迭代性质使得后验难以处理，因此我们提出了一种变分方法，旨在近似真实后验分布。我们展示了我们的方法通过去噪扩散过程（RED-Diff）自然地导致正则化，其中来自不同时间步长的去噪器同时对图像施加不同的结构约束。为了衡量不同时间步长的去噪器的贡献，我们提出了一种基于信号-t的加权机制。

    Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-t
    
[^73]: 贝叶斯分层建模中主动学习回归中的动态探索-开发权衡

    Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling. (arXiv:2304.07665v1 [cs.LG])

    [http://arxiv.org/abs/2304.07665](http://arxiv.org/abs/2304.07665)

    本文提出了一个新方法，利用贝叶斯分层建模，动态平衡探索-开发权衡，以更好地查询数据点。

    

    主动学习提供了一种自适应采样最具信息的实验以学习未知的黑盒函数的框架。本文提出了一种贝叶斯分层方法来动态平衡探索-开发权衡，以更好地查询数据点。

    Active learning provides a framework to adaptively sample the most informative experiments towards learning an unknown black-box function. Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space. Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal. In this paper, we develop a Bayesian hierarchical approach to dynamically balance the exploration-exploitation trade-off as more data points are queried. We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of data samples in the feature space to sample from the posterior distribution of the trade-off parameter obtained from the Bayesian hierarchical model. Simulated and real-world examples show the proposed approach achieves at least 6% and 11% average improvement when compared to pure exploration and ex
    
[^74]: OKRidge: 用于学习动态系统的可扩展 k 稀疏岭回归

    OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])

    [http://arxiv.org/abs/2304.06686](http://arxiv.org/abs/2304.06686)

    本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。

    

    本文解决了科学发现中的一个重要问题，即，确定非线性动态系统的稀疏控制方程，通过求解稀疏岭回归问题可以证明最优性，以确定驱动基础动态的项。我们提出了一种称为 OKRidge 的快速算法，用一种新颖的下界计算方法，涉及鞍点公式，然后使用线性系统或基于 ADMM 的方法来解决，其中可以通过解决另一个线性系统和单调回归问题来有效地计算近端算子。我们还提出了一种启动我们求解器的方法，利用了波束搜索。在实验中，我们的方法达到可证明的最优性，并且运行时间比商业求解器 Gurobi 解决的现有 MIP公式运行时间快几个数量级。

    We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
    
[^75]: 能量引导的熵神经最优输运

    Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])

    [http://arxiv.org/abs/2304.06094](http://arxiv.org/abs/2304.06094)

    本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。

    

    能量基础模型（EBMs）在机器学习社区已经有数十年的历史。自两千年代起，一直有很多高效的方法通过能量势（非归一化的似然函数）来解决生成建模问题。相比之下，最优输运（OT）领域，尤其是神经OT求解器，受到的探索要少得多，仅有一些近期的研究（不包括利用OT作为损失函数来解决问题的WGAN方法）。在本研究中，我们弥合了EBMs和熵正则化OT之间的差距，提出了一种新的方法，允许利用前者的最新发展和技术改进来丰富后者。我们在2D情景和标准的图像到图像翻译问题中验证了我们方法的适用性。为简单起见，我们选择了简短和长跑的EBMs。

    Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
    
[^76]: ChemCrow:用化学工具增强大型语言模型

    ChemCrow: Augmenting large-language models with chemistry tools. (arXiv:2304.05376v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.05376](http://arxiv.org/abs/2304.05376)

    本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。

    

    近期大型语言模型(LLMs)在跨领域的任务表现出一定的优势，但在化学相关问题上却表现不佳。此外，这些模型缺乏访问外部知识源，限制了它们在科学应用中的有用性。在本研究中，我们介绍了ChemCrow，一种LLM化学代理，旨在完成有机合成、药物发现和材料设计等任务。通过整合13个专家设计的工具，ChemCrow提高了LLM在化学中的性能，并产生了新的能力。我们的评估，包括LLM和人类专家评估，证明了ChemCrow在自动化各种化学任务方面的有效性。令人惊讶的是，我们发现GPT-4作为评估器无法区分明显错误的GPT-4完成和GPT-4 + ChemCrow性能。这种工具的滥用有很大的风险，我们讨论了它们的潜在危害。在负责任的情况下，ChemCrow不仅可以帮助专业化学家并降低成本。

    Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers ba
    
[^77]: 面向类别不均问题的集成学习和数据增强模型综述：组合、实现和评估

    A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])

    [http://arxiv.org/abs/2304.02858](http://arxiv.org/abs/2304.02858)

    本文研究了集成学习和数据增强方法的应用，针对类别不平衡问题，通过计算评估，找到了最有效的组合。

    

    分类问题中的类别不平衡（CI）是指属于一个类的观测值数量低于其他类的数量。集成学习结合数据增强方法已被广泛应用于解决类别不平衡问题。在过去的十年里，一些策略已经被应用于增强集成学习和数据增强方法，同时还开发了一些新方法，如生成对抗网络（GAN）。本文对用于解决基准CI问题的数据增强和集成学习方法进行计算评估。我们提出了一个评估CI问题的10个数据增强方法和10个集成学习方法的通用框架。我们的目标是识别提高分类效果最有效的组合。

    Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
    
[^78]: 最大似然方法再探：Kullback - Leibler 散度中的规范对称性和性能保证的正则化

    Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization. (arXiv:2303.16721v1 [stat.ML])

    [http://arxiv.org/abs/2303.16721](http://arxiv.org/abs/2303.16721)

    本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。

    

    最大似然方法是估计数据背后概率的最知名方法。然而，传统方法获得与经验分布最接近的概率模型，导致过度拟合。然后，正则化方法可以防止模型过度接近错误的概率，但是对它们的性能知之甚少。正则化的思想类似于纠错代码，通过将次优解与错误接收到的代码混合，获得最优解码。纠错代码中的最优解码是基于规范对称性实现的。我们通过关注 Kullback - Leibler 散度中的规范对称性，提出了最大似然方法中的理论上保证的正则化。在我们的方法中，我们可以获得最优的模型，而无需频繁搜索正则化中经常出现的超参数。

    The maximum likelihood method is the best-known method for estimating the probabilities behind the data. However, the conventional method obtains the probability model closest to the empirical distribution, resulting in overfitting. Then regularization methods prevent the model from being excessively close to the wrong probability, but little is known systematically about their performance. The idea of regularization is similar to error-correcting codes, which obtain optimal decoding by mixing suboptimal solutions with an incorrectly received code. The optimal decoding in error-correcting codes is achieved based on gauge symmetry. We propose a theoretically guaranteed regularization in the maximum likelihood method by focusing on a gauge symmetry in Kullback -- Leibler divergence. In our approach, we obtain the optimal model without the need to search for hyperparameters frequently appearing in regularization.
    
[^79]: 利用罚函数的深度部分线性Cox模型及其在肺癌患者CT扫描中的应用

    Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients. (arXiv:2303.05341v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05341](http://arxiv.org/abs/2303.05341)

    通过引入罚函数，我们提出了一种创新的深度部分线性Cox模型，用于在肺癌患者的CT扫描中分析死亡风险。该模型能有效地整合已知和新兴的风险因素，解决了参数维度超出样本大小和非参数建模中维度灾难的问题。

    

    肺癌是全球癌症死亡率的主要原因，突出了理解其死亡风险对设计有效的以患者为中心的治疗的重要性。国家肺部筛查试验（NLST）采用了计算机断层扫描纹理分析，提供了CT扫描上纹理模式的客观测量，用于量化肺癌患者的死亡风险。部分线性Cox模型通过将风险函数分解为参数和非参数分量，成为生存分析中备受青睐的方法，可以有效地将已知风险因素（如年龄和临床变量）和新兴风险因素（如图像特征）整合在一个统一的框架中。然而，当参数分量的维度超过样本大小时，模型拟合变得困难，而非参数建模则面临维度灾难的问题。我们提出了一种新颖的罚函数深度部分线性Cox模型（Penali

    Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (e.g., image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penali
    
[^80]: 对话式情境马尔可夫决策过程的高效探索性关键词选择策略

    Efficient Explorative Key-term Selection Strategies for Conversational Contextual Bandits. (arXiv:2303.00315v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00315](http://arxiv.org/abs/2303.00315)

    本研究提出了一种通用框架“ConLinUCB”来解决对话式情境马尔可夫决策过程中信息整合和探索性关键词选择的问题，以加速用户偏好估计的收敛速度。

    

    对话式情境马尔可夫决策过程通过偶尔询问显式反馈中的关键词来加快学习过程。然而，现有方法存在一些局限性。首先，关于关键词层面的对话和臂级推荐的信息没有被妥善地结合起来以加速学习。其次，重要的是问一些探索性的关键词，以迅速了解用户在各个领域的潜在兴趣，从而加速用户偏好估计的收敛速度，而这在现有研究中从未被考虑过。为了解决这些问题，我们首先提出了“ConLinUCB”这一对话式决策过程的通用框架，它能够更好地将关键词层面和臂级反馈结合起来，在每个时间步骤上一步估计用户偏好。基于这个框架，我们进一步设计了两种带有探索性关键词选择策略的决策过程算法，即ConLinUCB-BS和ConLinUCB-MCR。

    Conversational contextual bandits elicit user preferences by occasionally querying for explicit feedback on key-terms to accelerate learning. However, there are aspects of existing approaches which limit their performance. First, information gained from key-term-level conversations and arm-level recommendations is not appropriately incorporated to speed up learning. Second, it is important to ask explorative key-terms to quickly elicit the user's potential interests in various domains to accelerate the convergence of user preference estimation, which has never been considered in existing works. To tackle these issues, we first propose ``ConLinUCB", a general framework for conversational bandits with better information incorporation, combining arm-level and key-term-level feedback to estimate user preference in one step at each time. Based on this framework, we further design two bandit algorithms with explorative key-term selection strategies, ConLinUCB-BS and ConLinUCB-MCR. We prove t
    
[^81]: mSAM: 微批量平均锐度感知最小化

    mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization. (arXiv:2302.09693v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09693](http://arxiv.org/abs/2302.09693)

    mSAM是一种深度学习优化方法，通过在训练过程中聚合对抗性扰动得到的更新，从理论上证明了比传统方法更平的极小值点，实验证实了其在各种任务上的优越性能。

    

    现代深度学习模型是过参数化的，不同的极值可能导致广泛变化的泛化性能。锐度感知最小化（Sharpness-Aware Minimization，SAM）技术修改了基本的损失函数，使随机梯度下降方法朝着更平的极小值点前进，这被认为能够展现出增强的泛化能力。我们的研究深入探讨了一种特定的SAM变体，即微批量SAM（mSAM）。这种变体在训练过程中通过对来自多个分片（微批量）的对抗性扰动得到的更新进行聚合。我们将最近开发和研究的用于平坦性分析的通用框架扩展到理论上证明SAM实现了比随机梯度下降更平的极小值点，而mSAM比SAM实现了更加平坦的极小值点。我们对各种图像分类和自然语言处理任务进行了彻底的实证评估以验证这一理论进展。我们还表明，与以前的工作相反，mSAM可以被实现。

    Modern deep learning models are over-parameterized, where different optima can result in widely varying generalization performance. The Sharpness-Aware Minimization (SAM) technique modifies the fundamental loss function that steers gradient descent methods toward flatter minima, which are believed to exhibit enhanced generalization prowess. Our study delves into a specific variant of SAM known as micro-batch SAM (mSAM). This variation involves aggregating updates derived from adversarial perturbations across multiple shards (micro-batches) of a mini-batch during training. We extend a recently developed and well-studied general framework for flatness analysis to theoretically show that SAM achieves flatter minima than SGD, and mSAM achieves even flatter minima than SAM. We provide a thorough empirical evaluation of various image classification and natural language processing tasks to substantiate this theoretical advancement. We also show that contrary to previous work, mSAM can be impl
    
[^82]: 对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度研究

    Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes. (arXiv:2302.07477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07477](http://arxiv.org/abs/2302.07477)

    这篇论文研究了对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度理论。作者发现，在混合的情况下，最优样本复杂度依赖于总变异混合时间、折扣因子和解误差容忍度。

    

    我们考虑了表格型强化学习（RL）对于在马尔可夫决策过程（MDP）中最大化无穷时间折扣奖励的最优样本复杂度理论。在这种设定下，已经为表格型问题开发了最优最坏情况复杂度结果，导致样本复杂度依赖于折扣系数$\gamma$和解误差容忍度$\epsilon$的形式为$\tilde \Theta((1-\gamma)^{-3}\epsilon^{-2})$，其中$\gamma$表示折扣因子，$\epsilon$为解误差容忍度。然而，在许多感兴趣的应用中，最优策略（或所有策略）会产生混合。我们确定，在这种情况下，最优样本复杂度的依赖关系为$\tilde \Theta(t_{\text{mix}}(1-\gamma)^{-2}\epsilon^{-2})$，其中$t_{\text{mix}}$是总变异混合时间。我们的分析基于再生型思想，我们认为这些思想对于研究一般状态空间MDPs的RL问题具有独立的兴趣。

    We consider the optimal sample complexity theory of tabular reinforcement learning (RL) for maximizing the infinite horizon discounted reward in a Markov decision process (MDP). Optimal worst-case complexity results have been developed for tabular RL problems in this setting, leading to a sample complexity dependence on $\gamma$ and $\epsilon$ of the form $\tilde \Theta((1-\gamma)^{-3}\epsilon^{-2})$, where $\gamma$ denotes the discount factor and $\epsilon$ is the solution error tolerance. However, in many applications of interest, the optimal policy (or all policies) induces mixing. We establish that in such settings, the optimal sample complexity dependence is $\tilde \Theta(t_{\text{mix}}(1-\gamma)^{-2}\epsilon^{-2})$, where $t_{\text{mix}}$ is the total variation mixing time. Our analysis is grounded in regeneration-type ideas, which we believe are of independent interest, as they can be used to study RL problems for general state space MDPs.
    
[^83]: 具有连续动作的准最优强化学习

    Quasi-optimal Reinforcement Learning with Continuous Actions. (arXiv:2301.08940v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.08940](http://arxiv.org/abs/2301.08940)

    本研究提出了一种准最优学习算法，用于解决强化学习中连续动作环境下的决策问题，特别适用于医疗应用中确定最佳剂量水平的问题。

    

    强化学习在许多现实应用中需要在连续动作环境中做出决策。在医疗治疗方案的开发中，确定最佳剂量水平起着至关重要的作用。然而，将现有的强化学习算法应用于医疗应用中的一个挑战是，流行的无穷支持随机策略（例如高斯策略）可能会分配过高的剂量，严重危害患者。因此，引导一个支持仅包含近似最优动作的策略类别，并缩小效果和可靠性的动作搜索区域是很重要的。为了实现这一点，我们开发了一种新的“准最优学习算法”，该算法在离线策略设置下可以轻松优化，并在一般函数逼近下保证收敛。在理论上，我们分析了所提出算法的一致性、样本复杂度、适应性和收敛性。我们通过全面的模拟实验评估了我们的算法。

    Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel \emph{quasi-optimal learning algorithm}, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated expe
    
[^84]: 支持向量回归: 风险四方框架

    Support Vector Regression: Risk Quadrangle Framework. (arXiv:2212.09178v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.09178](http://arxiv.org/abs/2212.09178)

    本文结合风险四方理论，研究了支持向量回归（SVR）。研究结果发现，SVR的两种形式对应于等效误差度量的最小化，同时加上正则化惩罚项。通过构造基本风险四方框，我们证明了SVR是对两个对称条件分位数的平均数的渐近无偏估计量。此外，我们证明了$\varepsilon$-SVR和$\nu$-SVR在一般随机环境下的等价性。

    

    本文在基本的风险四方理论的背景下研究了支持向量回归（SVR），该理论将优化、风险管理和统计估计联系起来。研究结果表明，SVR的两种形式，$\varepsilon$-SVR和$\nu$-SVR，都对应于等效误差度量（分别为Vapnik误差和CVaR范数）的最小化，同时加上正则化惩罚项。这些误差度量又定义了相应的风险四方框。通过构造与SVR对应的基本风险四方框，我们证明了SVR是两个对称条件分位数的平均数的渐近无偏估计量。此外，我们在一般随机环境中证明了$\varepsilon$-SVR和$\nu$-SVR的等价性。此外，SVR被表述为带有正则化惩罚项的正则偏离最小化问题。最后，推导了在风险四方框架中的SVR的对偶形式。

    This paper investigates Support Vector Regression (SVR) in the context of the fundamental risk quadrangle theory, which links optimization, risk management, and statistical estimation. It is shown that both formulations of SVR, $\varepsilon$-SVR and $\nu$-SVR, correspond to the minimization of equivalent error measures (Vapnik error and CVaR norm, respectively) with a regularization penalty. These error measures, in turn, define the corresponding risk quadrangles. By constructing the fundamental risk quadrangle, which corresponds to SVR, we show that SVR is the asymptotically unbiased estimator of the average of two symmetric conditional quantiles. Further, we prove the equivalence of the $\varepsilon$-SVR and $\nu$-SVR in a general stochastic setting. Additionally, SVR is formulated as a regular deviation minimization problem with a regularization penalty. Finally, the dual formulation of SVR in the risk quadrangle framework is derived.
    
[^85]: 具有未知测量噪声的物理信息神经网络

    Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.15498](http://arxiv.org/abs/2211.15498)

    这篇论文提出了一种解决物理信息神经网络在存在非高斯噪声情况下失效的问题的方法，即通过同时训练一个能量模型来学习正确的噪声分布。通过多个例子的实验证明了该方法的改进性能。

    

    物理信息神经网络(PINNs)是一种既能找到解决方案又能识别偏微分方程参数的灵活方法。大多数相关的研究都假设数据是无噪声的，或者是受弱高斯噪声污染的。我们展示了标准PINN框架在非高斯噪声情况下失效的问题，并提出了一种解决这个根本性问题的方法，即同时训练一个能量模型(Energy-Based Model, EBM)来学习正确的噪声分布。我们通过多个例子展示了我们方法的改进性能。

    Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
    
[^86]: 增强型物理知识编码神经网络 (APINNs)：基于门控网络的软领域分解方法

    Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology. (arXiv:2211.08939v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08939](http://arxiv.org/abs/2211.08939)

    本文提出了增强型物理知识编码神经网络(APINN)，采用软领域分解和参数共享，通过门控网络初始化和一般领域和函数分解来改进了扩展物理知识编码神经网络(XPINN)和基本物理知识编码神经网络(PINN)的泛化能力。

    

    本文提出了增强型物理知识编码神经网络 (APINN)，采用软可训练的领域分解和灵活的参数共享以进一步改进扩展物理知识编码神经网络 (XPINN) 和基本物理知识编码神经网络 (PINN) 方法。具体而言，使用可训练的门控网络来模拟 XPINN 的硬分解，可以灵活地微调以发现更好的分区。APINN的输出是几个子网络的权重平均值。APINN不需要复杂的界面条件，并且其子网络可以利用所有训练样本，而不仅仅是其子域中的一部分训练数据。最后，每个子网络共享一部分共同参数，以捕捉每个分解函数中的相似组件。此外，根据胡等人[2021]的PINN泛化理论，我们展示了APINN可以通过适当的门控网络初始化和一般领域和函数分解来改善泛化能力。大量实验证明了APINN方法的有效性和优越性。

    In this paper, we propose the augmented physics-informed neural network (APINN), which adopts soft and trainable domain decomposition and flexible parameter sharing to further improve the extended PINN (XPINN) as well as the vanilla PINN methods. In particular, a trainable gate network is employed to mimic the hard decomposition of XPINN, which can be flexibly fine-tuned for discovering a potentially better partition. It weight-averages several sub-nets as the output of APINN. APINN does not require complex interface conditions, and its sub-nets can take advantage of all training samples rather than just part of the training data in their subdomains. Lastly, each sub-net shares part of the common parameters to capture the similar components in each decomposed function. Furthermore, following the PINN generalization theory in Hu et al. [2021], we show that APINN can improve generalization by proper gate network initialization and general domain & function decomposition. Extensive experi
    
[^87]: 潜在多模态功能图模型估计

    Latent Multimodal Functional Graphical Model Estimation. (arXiv:2210.17237v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.17237](http://arxiv.org/abs/2210.17237)

    本研究提出了一个潜在多模态功能图模型估计的新框架，通过同时估计转换算子和潜在图来填补当前科学方法在估计多模态功能数据图模型方面的空白

    

    共同多模态功能数据采集是一种现代的方法，通过最近在神经学和生物科学中的工程突破，可以同时从同一主体中测量来自多种模式的功能数据。获取这样的数据的一个重要动机是通过结合多模态信号来发现潜在的连接性。尽管存在科学兴趣，但在估计多模态功能数据下的图模型方面仍存在差距。为此，我们提出了一个新的综合框架，对数据生成过程进行建模，并识别从观测空间到潜在空间的算子映射。然后，我们开发了一个估计器，可以同时估计转换算子和潜在图。这个估计器基于偏相关算子，我们从多元到功能设置中严格推广了它。我们的程序是pr封闭的

    Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is pr
    
[^88]: 贝叶斯赌博机问题的连续时间极限

    Continuous-in-time Limit for Bayesian Bandits. (arXiv:2210.07513v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.07513](http://arxiv.org/abs/2210.07513)

    本文提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略，并且其计算成本不包括依赖于时间长度的项。

    

    本文重新审视了贝叶斯设置下的赌博机问题。贝叶斯方法将赌博机问题制定为一个优化问题，旨在寻找最优策略以最小化贝叶斯遗憾。面对的主要挑战之一是，当问题的时间长度或臂数较大时，计算最优策略通常是不可行的。我们首先展示了在适当的重缩放下，贝叶斯赌博机问题收敛于一个连续的哈密尔顿 - 雅各比 - 贝尔曼（HJB）方程。对于常见的一些赌博机问题，可以明确获得极限HJB方程的最优策略，并且在无法明确解决方案的情况下，我们提供了解决HJB方程的数字方法。基于这些结果，我们提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略。我们的方法的计算成本不包括依赖于时间长度的项，这与现有方法不同。数值模拟表明了我们方法的有效性。

    This paper revisits the bandit problem in the Bayesian setting. The Bayesian approach formulates the bandit problem as an optimization problem, and the goal is to find the optimal policy which minimizes the Bayesian regret. One of the main challenges facing the Bayesian approach is that computation of the optimal policy is often intractable, especially when the length of the problem horizon or the number of arms is large. In this paper, we first show that under a suitable rescaling, the Bayesian bandit problem converges toward a continuous Hamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJB equation can be explicitly obtained for several common bandit problems, and we give numerical methods to solve the HJB equation when an explicit solution is not available. Based on these results, we propose an approximate Bayes-optimal policy for solving Bayesian bandit problems with large horizons. Our method has the added benefit that its computational cost does not inc
    
[^89]: 规范化聚类准确度：一种非对称的外部聚类有效度量

    Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.02935](http://arxiv.org/abs/2209.02935)

    本文提出了一种非对称的外部聚类有效度量方法，旨在区分不同任务类型上表现良好和系统性表现不佳的聚类算法。与传统的内部度量不同，该方法利用参考真实分组进行评估，并弥补了现有方法在最坏情况下的误差。

    

    没有一个最好的聚类算法，我们仍然希望能够区分出在某些任务类型上表现良好和系统性表现不佳的方法。传统上，聚类算法使用内部或外部有效度量进行评估。内部度量量化所得分区的不同方面，例如，簇紧密度的平均程度或点的可分离性。然而，它们的有效性是有问题的，因为它们促使的聚类有时可能是无意义的。另一方面，外部度量将算法的输出与由专家提供的参考真实分组进行比较。在本文中，我们认为常用的经典分区相似性评分，例如规范化互信息、Fowlkes-Mallows或调整兰德指数，缺少一些可取的属性，例如，它们不能正确识别最坏情况，也不易解释。

    There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
    
[^90]: 通过蒸馏预测分布的不确定性进行联邦学习

    Federated Learning with Uncertainty via Distilled Predictive Distributions. (arXiv:2206.07562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07562](http://arxiv.org/abs/2206.07562)

    本论文提出了一种联邦学习的不确定性框架，每个客户端在每轮中推断其参数的后验分布和后验预测分布，并将其蒸馏为单一的深度神经网络发送给服务器。这种方法可以解决现有联邦学习方法无法估计模型不确定性的问题，并在有限数据环境下取得更准确的预测。

    

    大多数现有的联邦学习方法无法估计模型/预测的不确定性，因为客户端模型使用标准损失函数最小化方法进行训练，忽略了这种不确定性。然而，在许多情况下，特别是在有限数据环境中，考虑每个客户端模型参数的不确定性是有益的，因为它可以导致更准确的预测，并且可靠的不确定性估计可以用于诸如分布外（OOD）检测和序贯决策任务（如主动学习）等任务。我们提出了一个带有不确定性的联邦学习框架，在每一轮中，每个客户端推断其参数的后验分布和后验预测分布（PPD），将PPD蒸馏成一个单一的深度神经网络，并将该网络发送到服务器。与最近一些贝叶斯方法不同，我们的方法不要求发送所有原始数据至服务器，保护了客户隐私。

    Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require send
    
[^91]: GNN在推广带限函数方面的优越性比NN更加明显

    Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05904](http://arxiv.org/abs/2206.05904)

    本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。

    

    图神经网络（GNN）以其整合图形信息的能力被广泛用于数据分析。然而，GNN的表达能力仅针对图级任务进行了研究，而不是针对节点级任务，例如节点分类，其中试图从观察到的节点标签中插值出缺失的标签信息。本文研究了GNN在所述分类任务中的表达能力，它实质上是一个函数插值问题。具体而言，我们导出了GNN插值$\mathbb{R}^d$中带限函数所需的权重和层数。我们的结果显示，使用GNN架构以$\epsilon$-近似离散带限信号仅需要$O((\log \epsilon^{-1})^{d})$个权重，这比使用完全连接的神经网络（NN）得到的最佳结果的所需权重少得多 - 特别地，使用使用$O((\log \epsilon^{-1})^{d})$个样本来训练GNN以$\epsilon$-逼近带限函数。

    Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
    
[^92]: 非同态图的解耦自监督学习

    Decoupled Self-supervised Learning for Non-Homophilous Graphs. (arXiv:2206.03601v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03601](http://arxiv.org/abs/2206.03601)

    本文提出了一种用于非同态图的解耦自监督学习（DSSL）框架。通过模拟节点和链接的生成过程，将不同邻域之间的不同潜在语义解耦到自监督学习过程中。该框架对编码器不敏感，并且不需要预制的增强，对不同的图具有灵活性。

    

    本文研究了在图上进行节点表示学习的自监督学习问题。大部分现有的自监督学习方法都假设图是同质的，即连接的节点通常属于同一类或具有相似的特征。然而，在现实世界的图中，这种同质性的假设并不总是成立。为了解决这个问题，我们提出了一种用于图神经网络的解耦自监督学习（DSSL）框架。DSSL通过从语义结构的潜变量建模中模拟节点和链接的生成过程，将不同邻域之间的不同潜在语义解耦到自监督学习过程中。我们的DSSL框架对编码器不敏感，并且不需要预制的增强，因此适用于不同的图。为了有效优化该框架，我们推导了自监督目标的证据下界，并开发了一个具有变分特性的可扩展训练算法。

    This paper studies the problem of conducting self-supervised learning for node representation learning on graphs. Most existing self-supervised learning methods assume the graph is homophilous, where linked nodes often belong to the same class or have similar features. However, such assumptions of homophily do not always hold in real-world graphs. We address this problem by developing a decoupled self-supervised learning (DSSL) framework for graph neural networks. DSSL imitates a generative process of nodes and links from latent variable modeling of the semantic structure, which decouples different underlying semantics between different neighborhoods into the self-supervised learning process. Our DSSL framework is agnostic to the encoders and does not need prefabricated augmentations, thus is flexible to different graphs. To effectively optimize the framework, we derive the evidence lower bound of the self-supervised objective and develop a scalable training algorithm with variational 
    
[^93]: 最小化后验熵产生了最优摘要统计量

    Minimising the Expected Posterior Entropy Yields Optimal Summary Statistics. (arXiv:2206.02340v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.02340](http://arxiv.org/abs/2206.02340)

    该论文介绍了从大型数据集中提取低维摘要统计量的重要性，提出了通过最小化后验熵来获取最优摘要统计量的方法，并提供了实践建议和示例验证。

    

    从大型数据集中提取低维摘要统计量对于高效（无似然）推断非常重要。我们对不同类别的摘要进行了表征，并证明它们对于正确分析降维算法至关重要。我们建议通过在模型的先验预测分布下最小化期望后验熵（EPE）来获取摘要。许多现有方法等效于或是最小化EPE的特殊或极限情况。我们开发了一种方法来获取最小化EPE的高保真摘要；我们将其应用于基准和真实世界的示例。我们既提供了获取有效摘要的统一视角，又为实践者提供了具体建议。

    Extracting low-dimensional summary statistics from large datasets is essential for efficient (likelihood-free) inference. We characterise different classes of summaries and demonstrate their importance for correctly analysing dimensionality reduction algorithms. We propose obtaining summaries by minimising the expected posterior entropy (EPE) under the prior predictive distribution of the model. Many existing methods are equivalent to or are special or limiting cases of minimising the EPE. We develop a method to obtain high-fidelity summaries that minimise the EPE; we apply it to benchmark and real-world examples. We both offer a unifying perspective for obtaining informative summaries and provide concrete recommendations for practitioners.
    
[^94]: 稀疏神经网络的PAC-Bayes预测界

    A PAC-Bayes oracle inequality for sparse neural networks. (arXiv:2204.12392v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2204.12392](http://arxiv.org/abs/2204.12392)

    这篇论文研究了在非参数回归设置中利用稀疏深度神经网络的Gibbs后验分布，通过Metropolis-adjusted Langevin算法可以访问后验分布。通过对网络权重的稀疏集合进行统一先验的混合，证明了该方法能够适应未知的正则性和层次结构的回归函数，并达到了极小化最优收敛速率（除了对数因子）。

    

    我们研究了非参数回归设置中稀疏深度神经网络的Gibbs后验分布。通过Metropolis-adjusted Langevin算法可以访问后验分布。通过对网络权重的稀疏集合进行统一先验的混合，我们证明了一个预测界，该界表明该方法能够适应未知的正则性和层次结构的回归函数。该估计器达到了极小化最优收敛速率（除了对数因子）。

    We study the Gibbs posterior distribution for sparse deep neural nets in a nonparametric regression setting. The posterior can be accessed via Metropolis-adjusted Langevin algorithms. Using a mixture over uniform priors on sparse sets of network weights, we prove an oracle inequality which shows that the method adapts to the unknown regularity and hierarchical structure of the regression function. The estimator achieves the minimax-optimal rate of convergence (up to a logarithmic factor).
    
[^95]: 大规模医疗数据记录中的多层次随机优化填补方法

    Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records. (arXiv:2110.09680v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.09680](http://arxiv.org/abs/2110.09680)

    本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。

    

    探索和分析大规模数据集最近在研究和发展社区中引起了越来越多的关注。长期以来，人们一直认识到许多数据集中包含大量缺失的数值数据。我们引入了一种基于Kriging理论的数学原则随机优化填补方法，该方法被证明是一种强大的填补方法。然而，其计算成本和潜在的数值不稳定性会导致昂贵和/或不可靠的预测，可能限制其在大规模数据集上的使用。在本文中，我们将最近开发的多层次随机优化方法应用于大规模医疗记录中的填补问题。该方法基于计算应用数学技术，并具有高精度。特别地，对于最佳线性无偏预测器（BLUP），该多层次形式化是精确的，而且计算速度更快，数值稳定性更高。

    Exploration and analysis of massive datasets has recently generated increasing interest in the research and development communities. It has long been a recognized problem that many datasets contain significant levels of missing numerical data. We introduce a mathematically principled stochastic optimization imputation method based on the theory of Kriging. This is shown to be a powerful method for imputation. However, its computational effort and potential numerical instabilities produce costly and/or unreliable predictions, potentially limiting its use on large scale datasets. In this paper, we apply a recently developed multi-level stochastic optimization approach to the problem of imputation in massive medical records. The approach is based on computational applied mathematics techniques and is highly accurate. In particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level formulation is exact, and is also significantly faster and more numerically stable. This permits
    
[^96]: 稀疏加低秩矩阵分解: 一种离散优化方法

    Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. (arXiv:2109.12701v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.12701](http://arxiv.org/abs/2109.12701)

    本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。

    

    本文研究稀疏加低秩分解问题(SLR)，即将损坏的数据矩阵分解为包含基本真值的低秩矩阵和包含扰动的稀疏矩阵。 SLR是运筹学和机器学习领域的基础问题，在数据压缩、潜在语义索引、协同过滤和医学成像等各种应用中出现。我们提出了一种新的离散模型，并设计了交替最小化启发式算法以及新的半定松弛算法来解决这个问题。此外，我们还开发了一个自定义分支定界算法，利用我们的启发式算法和凸松弛来解决小规模的SLR问题。我们的启发式算法可以解决 $n=10000$ 的问题规模。

    We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in m
    
[^97]: 用循环神经网络预测通货膨胀

    Predicting Inflation with Recurrent Neural Networks. (arXiv:2104.03757v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2104.03757](http://arxiv.org/abs/2104.03757)

    本文应用LSTM循环神经网络模型，研究了预测通货膨胀的能力，并发现在长期预测和宏观经济不确定性加剧期间表现良好。有趣的是，LSTM所涉及的因素与商业周期指标高度相关，这说明这些信号作为通胀预测因子的实用性。

    

    本文应用了循环神经网络（LSTM）来预测通货膨胀。作为时间序列的一种吸引人的模型，LSTM按照时间步骤顺序处理数据，并明确学习动态依赖关系。本文还探讨了该模型的降维能力，以揭示可以解释通货膨胀过程的经济意义因素。对美国数据进行的实证结果表明，估计的神经网络在常见基准模型（包括其他机器学习模型）中表现具有竞争力但并不突出。特别是LSTM在长期预测和宏观经济不确定性加剧期间表现良好。有趣的是，LSTM所涉及的因素与商业周期指标高度相关，这说明这些信号作为通胀预测因子的实用性。本文还揭示了网络初始化和架构对预测性能的影响。

    This paper applies a recurrent neural network, the LSTM, to forecast inflation. This is an appealing model for time series as it processes each time step sequentially and explicitly learns dynamic dependencies. The paper also explores the dimension reduction capability of the model to uncover economically-meaningful factors that can explain the inflation process. Results from an exercise with US data indicate that the estimated neural nets present competitive, but not outstanding, performance against common benchmarks (including other machine learning models). The LSTM in particular is found to perform well at long horizons and during periods of heightened macroeconomic uncertainty. Interestingly, LSTM-implied factors present high correlation with business cycle indicators, informing on the usefulness of such signals as inflation predictors. The paper also sheds light on the impact of network initialization and architecture on forecast performance.
    
[^98]: 重审超参数模型中的最小描述长度复杂度

    Revisiting minimum description length complexity in overparameterized models. (arXiv:2006.10189v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10189](http://arxiv.org/abs/2006.10189)

    本文重审了超参数模型中的最小描述长度复杂度。通过定义一个新的基于MDL的复杂度度量，我们发现复杂度不仅取决于参数数量，还与设计矩阵或核矩阵的奇异值和信噪比有关。

    

    复杂度是统计学习理论中的一个基本概念，旨在提供有关泛化性能的信息。在低维度情况下，参数数量在一定程度上是成功的，但在超参数模型中，当参数数量超过训练样本数量时，其合理性不足。我们重新审视了基于Rissanen最小描述长度（MDL）原理的复杂度度量，并定义了一种新的适用于超参数模型的基于MDL的复杂度（MDL-COMP）。MDL-COMP通过对一个良好的Ridge估计类所引起的编码而定义出来的最优性准则。我们对线性模型和核方法的MDL-COMP进行了广泛的理论刻画，并表明它不仅是参数数量的函数，而是设计或核矩阵的奇异值和信噪比的函数。对于具有n个观测值，d个参数和独立同分布的高斯预测因子的线性模型，MDL-COMP的尺度是线性的。

    Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors, MDL-COMP scales li
    
[^99]: 理解Transformer训练的困难

    Understanding the Difficulty of Training Transformers. (arXiv:2004.08249v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.08249](http://arxiv.org/abs/2004.08249)

    该论文研究了Transformer训练的困难。他们发现不平衡的梯度不是训练不稳定的根本原因，而是每一层的放大效应导致训练不稳定。他们观察到轻量级的依赖限制了模型潜力，导致表现较差的训练模型。

    

    Transformer在许多自然语言处理任务中被证明是有效的。然而，它们的训练需要设计先进的优化器和学习率调度器的非平凡工作（例如，传统的SGD无法有效训练Transformer）。我们的目标是从经验和理论的角度理解$\textit{什么使得Transformer的训练变得困难}$。我们的分析表明，不平衡的梯度并不是训练不稳定的根本原因。相反，我们确定了一种影响训练的放大效应--对于多层Transformer模型中的每一层，它对其残差分支的依赖程度较高，导致训练不稳定，因为它放大了小的参数扰动（例如参数更新），并导致模型输出中的显著扰动。然而，我们观察到轻量级的依赖限制了模型的潜力，并导致表现较差的训练模型。在我们的分析启发下，我们提出了Admin（$\textbf{Ad}$aptive 重述部分

    Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand $\textit{what complicates Transformer training}$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin ($\textbf{Ad}$aptive 
    
[^100]: 基于分布式多元回归建模的选择受数据保护约束的生物标志物

    Distributed Multivariate Regression Modeling For Selecting Biomarkers Under Data Protection Constraints. (arXiv:1803.00422v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1803.00422](http://arxiv.org/abs/1803.00422)

    该论文提出了一种基于分布式多元回归建模的选择生物标志物的方法，解决了数据保护约束的问题，并克服了传统方法在处理大量生物标志物时丢失信息的局限性。

    

    临床生物标志物的发现需要大量的患者队列，并且通过跨机构间的汇总数据方法进行辅助。在许多国家，特别是在临床环境中，数据保护约束禁止不同研究机构之间交换个体级数据，从而阻碍了联合分析的进行。为了规避这个问题，只能交换非披露式的汇总数据，这通常是通过手动进行，并且在转移之前需要明确的许可，也就是数据调用的数量和数据量应该受到限制。这不允许进行更复杂的任务，例如变量选择，因为通常只传输简单的汇总统计数据。其他方法提出了需要更复杂的汇总数据或使用输入数据扰动的方法，但是这些方法要么不能处理大量的生物标志物，要么会丢失信息。在这里，我们提出了一种多变量回归方法来识别生物标志物。

    The discovery of clinical biomarkers requires large patient cohorts and is aided by a pooled data approach across institutions. In many countries, data protection constraints, especially in the clinical environment, forbid the exchange of individual-level data between different research institutes, impeding the conduct of a joint analyses. To circumvent this problem, only non-disclosive aggregated data is exchanged, which is often done manually and requires explicit permission before transfer, i.e., the number of data calls and the amount of data should be limited. This does not allow for more complex tasks such as variable selection, as only simple aggregated summary statistics are typically transferred. Other methods have been proposed that require more complex aggregated data or use input data perturbation, but these methods can either not deal with a high number of biomarkers or lose information. Here, we propose a multivariable regression approach for identifying biomarkers by aut
    

