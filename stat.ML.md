# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery.](http://arxiv.org/abs/2401.02930) | Dagma-DCE是一种可解释的、非参数的可微因果发现方案，使用可解释的因果强度度量定义加权邻接矩阵，并在模拟数据集中达到最先进的性能水平。 |
| [^2] | [Class-wise Generalization Error: an Information-Theoretic Analysis.](http://arxiv.org/abs/2401.02904) | 本文通过信息论分析研究了类别普适误差问题，并提出了使用KL散度的信息论界限和使用条件互信息(CMI)的更紧界限，能够准确捕捉复杂的类别普适误差。 |
| [^3] | [Nonlinear functional regression by functional deep neural network with kernel embedding.](http://arxiv.org/abs/2401.02890) | 本文提出了一种函数深度神经网络用于非线性函数回归的方法，通过平滑核积分变换和数据相关的维度缩减方法，取得了良好的预测效果。 |
| [^4] | [Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors.](http://arxiv.org/abs/2401.02739) | 本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。 |
| [^5] | [On the numerical reliability of nonsmooth autodiff: a MaxPool case study.](http://arxiv.org/abs/2401.02736) | 本文研究了涉及非平滑MaxPool操作的神经网络自动微分的数值可靠性，并发现最近的研究表明AD几乎在每个地方都与导数相符，即使在存在非平滑操作的情况下也是如此。但在实践中，AD使用的是浮点数，需要探索可能导致AD数值不正确的情况。通过研究不同选择的非平滑MaxPool雅可比矩阵对训练过程的影响，我们找到了分歧区和补偿区两个可能导致AD数值不正确的子集。 |
| [^6] | [Shared active subspace for multivariate vector-valued functions.](http://arxiv.org/abs/2401.02735) | 本文提出了一种共享主动子空间的方法来处理多元向量值函数，可以通过操纵梯度或计算对称正定矩阵实现。实验结果表明，SPD级别的方法比梯度级别的方法更好，并且在正态分布情况下表现接近向量值方法。 |
| [^7] | [TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis.](http://arxiv.org/abs/2401.02708) | 本文提出了一种适应时间的三元组坐标损失函数TripleSurv，通过引入样本对之间的生存时间差异来鼓励模型量化排名相对风险，从而提高生存分析的准确性。 |
| [^8] | [Improving sample efficiency of high dimensional Bayesian optimization with MCMC.](http://arxiv.org/abs/2401.02650) | 本文提出了一种基于马尔科夫链蒙特卡罗的方法，用于改进高维贝叶斯优化的样本效率。实验结果表明，该方法在高维顺序优化和强化学习任务中表现优于现有方法。 |
| [^9] | [Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery.](http://arxiv.org/abs/2401.02592) | 本研究提出了一种保证非凸分解方法用于张量列车恢复的新方法。该方法通过优化左正交TT格式来实现正交结构，并使用黎曼梯度下降算法来优化因子。我们证明了该方法具有局部线性收敛性，并且在满足受限等谱性质的条件下能够以线性速率收敛到真实张量。 |
| [^10] | [Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel.](http://arxiv.org/abs/2401.02520) | 本论文提出了在任意元素间依赖下进行结构化矩阵估计的通用框架，并证明了提出的最小二乘估计器在各种噪声分布下的紧致性。此外，论文还提出了一个新颖的结果，论述了无关低秩矩阵的结构特点。最后，论文还展示了该框架在结构化马尔可夫转移核估计问题中的应用。 |
| [^11] | [PAC-Bayes-Chernoff bounds for unbounded losses.](http://arxiv.org/abs/2401.01148) | 这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。 |
| [^12] | [LITE: A Stable Framework for Lattice-Integrated Embedding of Topological Descriptors.](http://arxiv.org/abs/2312.17093) | 本文介绍了一种稳定的格子集成拓扑描述符框架，将持久图转化为有限维向量空间的元素，通过基于离散测度的功能，证明了部分成员在度量上的稳定性，并展示了与现有方法相竞争甚至超越的性能。 |
| [^13] | [Sensitivity Analysis in the Presence of Intrinsic Stochasticity for Discrete Fracture Network Simulations.](http://arxiv.org/abs/2312.04722) | 本研究提出了一种在离散裂缝网络模拟中考虑内在随机性的敏感性分析方法，用于估计颗粒达到系统边缘的突破时间。这一方法能够解决参数选择的不确定性和随机性对QoI的影响。 |
| [^14] | [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance.](http://arxiv.org/abs/2311.14212) | 该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。 |
| [^15] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^16] | [Efficient Estimation for Longitudinal Network via Adaptive Merging.](http://arxiv.org/abs/2211.07866) | 本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。 |
| [^17] | [Causal Inference with Invalid Instruments: Exploring Nonlinear Treatment Models with Machine Learning.](http://arxiv.org/abs/2203.12808) | 提出一种名为TSCI的新方法，使用机器学习探索非线性治疗模型，并调整不同形式的其他工具变量假设违背，用于无效工具变量下的因果推断问题。 |

# 详细

[^1]: Dagma-DCE：可解释的、非参数的可微因果发现

    Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery. (arXiv:2401.02930v1 [cs.LG])

    [http://arxiv.org/abs/2401.02930](http://arxiv.org/abs/2401.02930)

    Dagma-DCE是一种可解释的、非参数的可微因果发现方案，使用可解释的因果强度度量定义加权邻接矩阵，并在模拟数据集中达到最先进的性能水平。

    

    我们介绍了Dagma-DCE，这是一种可解释的、与模型无关的可微因果发现方案。当前的非参数或超参数方法在可微因果发现中使用不透明的``独立性''代理来证明是否包含或排除因果关系。我们理论上和实证上展示了这些代理可能与实际的因果强度任意不同。与现有的可微因果发现算法相比，Dagma-DCE使用可解释的因果强度度量来定义加权邻接矩阵。在一些模拟数据集中，我们展示了我们的方法达到了最先进的性能水平。我们还展示了Dagma-DCE允许领域专家进行有原则的阈值设定和稀疏惩罚。我们的方法的代码在https://github.com/DanWaxman/DAGMA-DCE上开源，并且可以很容易地适应任意的可微模型。

    We introduce Dagma-DCE, an interpretable and model-agnostic scheme for differentiable causal discovery. Current non- or over-parametric methods in differentiable causal discovery use opaque proxies of ``independence'' to justify the inclusion or exclusion of a causal relationship. We show theoretically and empirically that these proxies may be arbitrarily different than the actual causal strength. Juxtaposed to existing differentiable causal discovery algorithms, \textsc{Dagma-DCE} uses an interpretable measure of causal strength to define weighted adjacency matrices. In a number of simulated datasets, we show our method achieves state-of-the-art level performance. We additionally show that \textsc{Dagma-DCE} allows for principled thresholding and sparsity penalties by domain-experts. The code for our method is available open-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be adapted to arbitrary differentiable models.
    
[^2]: 类别普适误差：信息论分析

    Class-wise Generalization Error: an Information-Theoretic Analysis. (arXiv:2401.02904v1 [cs.LG])

    [http://arxiv.org/abs/2401.02904](http://arxiv.org/abs/2401.02904)

    本文通过信息论分析研究了类别普适误差问题，并提出了使用KL散度的信息论界限和使用条件互信息(CMI)的更紧界限，能够准确捕捉复杂的类别普适误差。

    

    现有的监督学习普适性理论通常采用整体方法，并提供了整个数据分布上预期普适化的界限，这隐含地假定模型对所有类别都具有类似的普适性。然而在实践中，不同类别的普适性性能存在显著差异，这无法被现有的普适性界限所捕捉。本文通过理论上研究类别普适误差来解决这个问题，该误差量化了每个个体类别的普适性能力。我们利用KL散度推导了一种新的信息论界限来衡量类别普适误差，并进一步使用条件互信息(CMI)得到了几个更紧的界限，在实践中更容易估计。我们在不同的神经网络上进行了实验证实，验证了我们提出的界限准确地捕捉了复杂的类别普适误差。

    Existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes similarly for all the classes. In practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. In this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. We derive a novel information-theoretic bound for class-generalization error using the KL divergence, and we further obtain several tighter bounds using the conditional mutual information (CMI), which are significantly easier to estimate in practice. We empirically validate our proposed bounds in different neural networks and show that they accurately capture the complex class-generalization err
    
[^3]: 函数深度神经网络在非线性函数回归中的应用

    Nonlinear functional regression by functional deep neural network with kernel embedding. (arXiv:2401.02890v1 [stat.ML])

    [http://arxiv.org/abs/2401.02890](http://arxiv.org/abs/2401.02890)

    本文提出了一种函数深度神经网络用于非线性函数回归的方法，通过平滑核积分变换和数据相关的维度缩减方法，取得了良好的预测效果。

    

    随着深度学习在语音识别、图像分类和自然语言处理等领域的迅速发展，它也被广泛应用于函数数据分析中。然而，由于无限维的输入，我们需要一个强大的维度缩减方法来处理非线性函数回归任务。在本文中，基于平滑核积分变换的思想，我们提出了一种具有高效且完全数据依赖的维度缩减方法的函数深度神经网络。我们的函数网络由以下步骤组成：核嵌入步骤：利用数据相关的平滑核进行积分变换；投影步骤：通过基于嵌入核的特征函数基底进行维度缩减；最后是一个表达丰富的深度ReLU神经网络进行预测。

    With the rapid development of deep learning in various fields of science and technology, such as speech recognition, image classification, and natural language processing, recently it is also widely applied in the functional data analysis (FDA) with some empirical success. However, due to the infinite dimensional input, we need a powerful dimension reduction method for functional learning tasks, especially for the nonlinear functional regression. In this paper, based on the idea of smooth kernel integral transformation, we propose a functional deep neural network with an efficient and fully data-dependent dimension reduction method. The architecture of our functional net consists of a kernel embedding step: an integral transformation with a data-dependent smooth kernel; a projection step: a dimension reduction by projection with eigenfunction basis based on the embedding kernel; and finally an expressive deep ReLU neural network for the prediction. The utilization of smooth kernel embe
    
[^4]: 扩散变分推断：扩散模型作为表达性变分后验

    Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])

    [http://arxiv.org/abs/2401.02739](http://arxiv.org/abs/2401.02739)

    本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。

    

    我们提出了去噪扩散变分推断（DDVI），一种用扩散模型作为表达性变分后验的潜变量模型的近似推断算法。我们的方法通过辅助潜变量增加了变分后验，从而得到一个表达性的模型类，通过反转用户指定的加噪过程在潜空间中进行扩散。我们通过优化一个受到觉醒-睡眠算法启发的边际似然新下界来拟合这些模型。我们的方法易于实现（它适配了正则化的ELBO扩展），与黑盒变分推断兼容，并且表现优于基于归一化流或对抗网络的替代近似后验类别。将我们的方法应用于深度潜变量模型时，我们的方法得到了去噪扩散变分自动编码器（DD-VAE）算法。我们将该算法应用于生物学中的一个激励任务 -- 从人类基因组中推断潜在血统 -- 超过了强基线模型。

    We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
    
[^5]: 关于非平滑自动微分的数值可靠性：MaxPool案例研究

    On the numerical reliability of nonsmooth autodiff: a MaxPool case study. (arXiv:2401.02736v1 [cs.LG])

    [http://arxiv.org/abs/2401.02736](http://arxiv.org/abs/2401.02736)

    本文研究了涉及非平滑MaxPool操作的神经网络自动微分的数值可靠性，并发现最近的研究表明AD几乎在每个地方都与导数相符，即使在存在非平滑操作的情况下也是如此。但在实践中，AD使用的是浮点数，需要探索可能导致AD数值不正确的情况。通过研究不同选择的非平滑MaxPool雅可比矩阵对训练过程的影响，我们找到了分歧区和补偿区两个可能导致AD数值不正确的子集。

    

    本文考虑了涉及非平滑MaxPool操作的神经网络自动微分（AD）的可靠性问题。我们研究了在不同精度级别（16位、32位、64位）和卷积架构（LeNet、VGG和ResNet）以及不同数据集（MNIST、CIFAR10、SVHN和ImageNet）上的AD行为。尽管AD可能是错误的，但最近的研究表明，它在几乎每个地方都与导数相符，即使在存在非平滑操作（如MaxPool和ReLU）的情况下也是如此。另一方面，在实践中，AD使用的是浮点数（而不是实数），因此需要探索AD可能在数值上不正确的子集。这些子集包括分歧区（AD在实数上不正确）和补偿区（AD在浮点数上不正确但在实数上正确）。我们使用SGD进行训练过程，并研究了MaxPool非平滑雅可比矩阵的不同选择对训练过程的影响。

    This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool
    
[^6]: 共享主动子空间用于多元向量值函数

    Shared active subspace for multivariate vector-valued functions. (arXiv:2401.02735v1 [stat.ME])

    [http://arxiv.org/abs/2401.02735](http://arxiv.org/abs/2401.02735)

    本文提出了一种共享主动子空间的方法来处理多元向量值函数，可以通过操纵梯度或计算对称正定矩阵实现。实验结果表明，SPD级别的方法比梯度级别的方法更好，并且在正态分布情况下表现接近向量值方法。

    

    本文提出了几种方法作为基线来计算多元向量值函数的共享主动子空间。目标是最小化原始空间上的函数评估与重构空间上的函数评估之间的偏差。这些方法通过操纵梯度或从每个分量函数的梯度计算的对称正定（半正定）矩阵来获得所有分量函数的共同结构。与现有的只适用于正态分布的向量值方法不同，这些方法可以应用于任何数据无论其潜在分布。我们在五个优化问题上测试了这些方法的有效性。实验结果表明，总体而言， SPD级别的方法优于梯度级别的方法，并且在正态分布情况下接近向量值方法。有趣的是，在大多数情况下，只需取SPD矩阵之和即可。

    This paper proposes several approaches as baselines to compute a shared active subspace for multivariate vector-valued functions. The goal is to minimize the deviation between the function evaluations on the original space and those on the reconstructed one. This is done either by manipulating the gradients or the symmetric positive (semi-)definite (SPD) matrices computed from the gradients of each component function so as to get a single structure common to all component functions. These approaches can be applied to any data irrespective of the underlying distribution unlike the existing vector-valued approach that is constrained to a normal distribution. We test the effectiveness of these methods on five optimization problems. The experiments show that, in general, the SPD-level methods are superior to the gradient-level ones, and are close to the vector-valued approach in the case of a normal distribution. Interestingly, in most cases it suffices to take the sum of the SPD matrices 
    
[^7]: TripleSurv：适应时间三元组坐标损失用于生存分析

    TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])

    [http://arxiv.org/abs/2401.02708](http://arxiv.org/abs/2401.02708)

    本文提出了一种适应时间的三元组坐标损失函数TripleSurv，通过引入样本对之间的生存时间差异来鼓励模型量化排名相对风险，从而提高生存分析的准确性。

    

    生存分析中的一个核心挑战是对被截尾的事件时间数据进行建模，其中感兴趣的事件可能是死亡、失败或特定事件的发生。过去的研究表明，排序损失和最大似然估计（MLE）损失函数被广泛应用于生存分析。然而，排序损失仅关注生存时间排名，不考虑样本对于确切生存时间值的潜在影响。此外，MLE是无界的且容易受到异常值（例如，被截尾数据）的影响，这可能导致建模性能较差。为了处理学习过程的复杂性并利用有价值的生存时间值，我们提出了一种适应时间三元组坐标损失函数TripleSurv，通过将样本对之间的生存时间差异引入排序中，以鼓励模型量化排名相对风险，最终提高预测准确性。

    A core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation (MLE)loss functions are widely-used for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. Furthermore, the MLE is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predi
    
[^8]: 通过MCMC改进高维贝叶斯优化的样本效率

    Improving sample efficiency of high dimensional Bayesian optimization with MCMC. (arXiv:2401.02650v1 [cs.LG])

    [http://arxiv.org/abs/2401.02650](http://arxiv.org/abs/2401.02650)

    本文提出了一种基于马尔科夫链蒙特卡罗的方法，用于改进高维贝叶斯优化的样本效率。实验结果表明，该方法在高维顺序优化和强化学习任务中表现优于现有方法。

    

    高维空间中的顺序优化方法经常面临维度诅咒。目前的高斯过程方法在跟踪高斯过程后验的计算复杂度上仍存在负担，并且需要将优化问题划分为小区域以确保探索或假设一个潜在的低维结构。我们提出了一种基于马尔科夫链蒙特卡罗的新方法，通过将候选点转移到更有希望的位置来有效地从近似后验中采样。我们在高斯过程汤普森采样设置下提供了其收敛的理论保证。我们还通过实验表明，我们算法的Metropolis-Hastings版本和Langevin Dynamics版本在高维顺序优化和强化学习基准测试中均优于现有方法。

    Sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. Current approaches under the Gaussian process framework are still burdened by the computational complexity of tracking Gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. With the idea of transiting the candidate points towards more promising positions, we propose a new method based on Markov Chain Monte Carlo to efficiently sample from an approximated posterior. We provide theoretical guarantees of its convergence in the Gaussian process Thompson sampling setting. We also show experimentally that both the Metropolis-Hastings and the Langevin Dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.
    
[^9]: 保证非凸分解方法用于张量列车恢复的研究

    Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. (arXiv:2401.02592v1 [stat.ML])

    [http://arxiv.org/abs/2401.02592](http://arxiv.org/abs/2401.02592)

    本研究提出了一种保证非凸分解方法用于张量列车恢复的新方法。该方法通过优化左正交TT格式来实现正交结构，并使用黎曼梯度下降算法来优化因子。我们证明了该方法具有局部线性收敛性，并且在满足受限等谱性质的条件下能够以线性速率收敛到真实张量。

    

    在本文中，我们首次提供了对于分解方法的收敛性保证。具体而言，为了避免尺度歧义并便于理论分析，我们优化所谓的左正交TT格式，强制使大部分因子彼此正交。为了确保正交结构，我们利用黎曼梯度下降（RGD）来优化Stiefel流形上的这些因子。我们首先深入研究TT分解问题，并建立了RGD的局部线性收敛性。值得注意的是，随着张量阶数的增加，收敛速率仅经历线性下降。然后，我们研究了感知问题，即从线性测量中恢复TT格式张量。假设感知算子满足受限等谱性质（RIP），我们证明在适当的初始化下，通过谱初始化获得，RGD也会以线性速率收敛到真实张量。此外，我们扩展了我们的研究。

    In this paper, we provide the first convergence guarantee for the factorization approach. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal TT format which enforces orthonormality among most of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factorization problem and establish the local linear convergence of RGD. Notably, the rate of convergence only experiences a linear decline as the tensor order increases. We then study the sensing problem that aims to recover a TT format tensor from linear measurements. Assuming the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper initialization, which could be obtained through spectral initialization, RGD also converges to the ground-truth tensor at a linear rate. Furthermore, we expand our 
    
[^10]: 在任意元素间依赖下的结构化矩阵学习与马尔可夫转移核估计

    Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel. (arXiv:2401.02520v1 [stat.ML])

    [http://arxiv.org/abs/2401.02520](http://arxiv.org/abs/2401.02520)

    本论文提出了在任意元素间依赖下进行结构化矩阵估计的通用框架，并证明了提出的最小二乘估计器在各种噪声分布下的紧致性。此外，论文还提出了一个新颖的结果，论述了无关低秩矩阵的结构特点。最后，论文还展示了该框架在结构化马尔可夫转移核估计问题中的应用。

    

    结构化矩阵估计问题通常在强噪声依赖假设下进行研究。本文考虑噪声低秩加稀疏矩阵恢复的一般框架，其中噪声矩阵可以来自任意具有元素间任意依赖的联合分布。我们提出了一个无关相位约束的最小二乘估计器，并且证明了它在各种噪声分布下都是紧致的，既满足确定性下界又匹配最小化风险。为了实现这一点，我们建立了一个新颖的结果，断言两个任意的低秩无关矩阵之间的差异必须在其元素上扩散能量，换句话说不能太稀疏，这揭示了无关低秩矩阵的结构，可能引起独立兴趣。然后，我们展示了我们框架在几个重要的统计机器学习问题中的应用。在估计结构化马尔可夫转移核的问题中，采用了这种方法。

    The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method
    
[^11]: 无界损失的PAC-Bayes-Chernoff界限

    PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])

    [http://arxiv.org/abs/2401.01148](http://arxiv.org/abs/2401.01148)

    这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。

    

    我们提出了一种新的用于无界损失的高概率PAC-Bayes参考界限。这个结果可以理解为Chernoff界限的PAC-Bayes版本。证明技巧依赖于通过Cramér变换对损失进行统一边界的尾部随机变量。我们强调了我们主要结果的两个应用。首先，我们证明了我们的界限解决了许多PAC-Bayes界限上的自由参数优化的开放问题。最后，我们证明了我们的方法允许在损失函数上进行灵活的假设，从而产生了广义了之前的界限，并且可以通过最小化来获得类似Gibbs的后验概率。

    We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
    
[^12]: LITE：稳定的格子集成拓扑描述符框架

    LITE: A Stable Framework for Lattice-Integrated Embedding of Topological Descriptors. (arXiv:2312.17093v2 [math.AT] UPDATED)

    [http://arxiv.org/abs/2312.17093](http://arxiv.org/abs/2312.17093)

    本文介绍了一种稳定的格子集成拓扑描述符框架，将持久图转化为有限维向量空间的元素，通过基于离散测度的功能，证明了部分成员在度量上的稳定性，并展示了与现有方法相竞争甚至超越的性能。

    

    在本文中，我们引入了一种新的持久图描述符家族。我们的方法将这些图转化为有限维向量空间中的元素，使用基于它们所引起的离散测度的泛函。虽然我们的焦点主要是在基于身份和频率的转换上，但我们并不局限于这些类型的技术。我们将这个转换家族称为LITE （Lattice Integrated Topological Embedding），并且对该家族的一些成员在1-康托洛维奇-鲁宾斯坦度量下证明了稳定性，确保对细微的数据变化具有反应性。广泛的对比分析表明，我们的描述符在拓扑数据分析文献中与当前最先进的方法相竞争，并且经常超越现有的方法。这项研究不仅为数据科学家引入了创新的观点，而且批评了目前关于向量化方法学的文献的轨迹。

    In this paper, we introduce a new family of descriptors for persistence diagrams. Our approach transforms these diagrams into elements of a finite-dimensional vector space using functionals based on the discrete measures they induce. While our focus is primarily on identity and frequency-based transformations, we do not restrict our approach exclusively to this types of techniques. We term this family of transformations as LITE (Lattice Integrated Topological Embedding) and prove stability for some members of this family against the 1-$Kantorovitch$-$Rubinstein$ metric, ensuring its responsiveness to subtle data variations. Extensive comparative analysis reveals that our descriptor performs competitively with the current state-of-art from the topological data analysis literature, and often surpasses, the existing methods. This research not only introduces an innovative perspective for data scientists but also critiques the current trajectory of literature on methodologies for vectorizi
    
[^13]: 在离散裂缝网络模拟中考虑内在随机性的敏感性分析

    Sensitivity Analysis in the Presence of Intrinsic Stochasticity for Discrete Fracture Network Simulations. (arXiv:2312.04722v2 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2312.04722](http://arxiv.org/abs/2312.04722)

    本研究提出了一种在离散裂缝网络模拟中考虑内在随机性的敏感性分析方法，用于估计颗粒达到系统边缘的突破时间。这一方法能够解决参数选择的不确定性和随机性对QoI的影响。

    

    由于地下裂缝网络的直接观测往往不可行，大规模的离散裂缝网络（DFN）模拟器常用于研究颗粒的地下输运。尽管这些模拟器在多个工程应用中取得了很多成功，但是涉及到感兴趣量（QoI）的估计，如颗粒达到系统边缘的突破时间，会受到两种不同的不确定性影响。DFN模拟器的运行需要设置多个参数值来决定裂缝的位置和大小、裂缝的密度以及整体渗透性; 对参数选择的不确定性将导致QoI中存在一定程度的不确定性，称为认知不确定性。此外，由于DFN模拟器依赖于随机过程来放置裂缝和控制流动，理解这种随机性如何影响QoI需要进行多次模拟器的运行。

    Large-scale discrete fracture network (DFN) simulators are standard fare for studies involving the sub-surface transport of particles since direct observation of real world underground fracture networks is generally infeasible. While these simulators have seen numerous successes over several engineering applications, estimations on quantities of interest (QoI) - such as breakthrough time of particles reaching the edge of the system - suffer from a two distinct types of uncertainty. A run of a DFN simulator requires several parameter values to be set that dictate the placement and size of fractures, the density of fractures, and the overall permeability of the system; uncertainty on the proper parameter choices will lead to some amount of uncertainty in the QoI, called epistemic uncertainty. Furthermore, since DFN simulators rely on stochastic processes to place fractures and govern flow, understanding how this randomness affects the QoI requires several runs of the simulator at distinc
    
[^14]: 注释敏感性：训练数据收集方法影响模型性能

    Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.14212](http://arxiv.org/abs/2311.14212)

    该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。

    

    当训练数据由人工注释者收集时，注释工具的设计、给予注释者的指示、注释者的特征以及他们之间的互动都可能对训练数据产生影响。这项研究证明了创建注释工具时的设计选择也会影响基于得到的注释训练的模型。我们引入了"注释敏感性"这个术语，用来指代注释数据收集方法对注释本身以及下游模型性能和预测的影响。我们在五种实验条件下对仇恨言论和冒犯性语言进行注释收集，随机将注释者分配到不同条件下。然后，在每个得到的五个数据集上对BERT模型进行微调，并在每个条件的保留部分上评估模型性能。我们发现在以下方面条件之间存在明显差异：1）仇恨言论/冒犯性语言注释的比例，2）模型性能。

    When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
    
[^15]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^16]: 自适应合并下的纵向网络有效估计

    Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07866](http://arxiv.org/abs/2211.07866)

    本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。

    

    纵向网络由多个节点之间的时间边序列组成，其中时间边在实时中被观察到。随着在线社交平台和电子商务的兴起，它已经变得普遍，但在文献中往往被忽略。本文提出了一个有效的纵向网络估计框架，利用自适应网络合并、张量分解和点过程的优势。它合并相邻的稀疏网络，以扩大观测边的数量并减少估计方差，同时通过利用本地时间结构进行自适应网络邻域控制引入的估计偏差。提出了一个投影梯度下降算法来促进估计，其中每次迭代的估计错误上界被建立。进行了彻底的分析，以量化所提出方法的渐近行为，结果表明它可以显着减少估计偏差。

    Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
    
[^17]: 无效工具变量下的因果推断: 探索基于机器学习的非线性治疗模型

    Causal Inference with Invalid Instruments: Exploring Nonlinear Treatment Models with Machine Learning. (arXiv:2203.12808v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2203.12808](http://arxiv.org/abs/2203.12808)

    提出一种名为TSCI的新方法，使用机器学习探索非线性治疗模型，并调整不同形式的其他工具变量假设违背，用于无效工具变量下的因果推断问题。

    

    我们讨论了在可能存在无效工具变量的观测研究中的因果推断问题。我们提出了一种名为“两阶段曲率识别”(TSCI)的新方法，它使用机器学习探索非线性治疗模型，并调整不同形式的其他工具变量假设违背。TSCI的成功需要工具变量对治疗的影响与其违背形式不同。我们实现了一步新颖的偏差校正来消除可能高复杂度机器学习所造成的偏差。我们提出的TSCI估计器即使机器学习算法不能一致地估计治疗模型，也被证明是渐进无偏和正态的。我们设计了一个数据依赖方法来选择几个候选违背形式中的最佳形式。我们应用TSCI研究了教育对收入的影响。

    We discuss causal inference for observational studies with possibly invalid instrumental variables. We propose a novel methodology called two-stage curvature identification (TSCI), which explores the nonlinear treatment model with machine learning and adjusts for different forms of violating the instrumental variable assumptions. The success of TSCI requires the instrumental variable's effect on treatment to differ from its violation form. A novel bias correction step is implemented to remove bias resulting from potentially high complexity of machine learning. Our proposed TSCI estimator is shown to be asymptotically unbiased and normal even if the machine learning algorithm does not consistently estimate the treatment model. We design a data-dependent method to choose the best among several candidate violation forms. We apply TSCI to study the effect of education on earnings.
    

