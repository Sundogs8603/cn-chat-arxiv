# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Self-Distillation for Gaussian Process Regression and Classification.](http://arxiv.org/abs/2304.02641) | 本文针对高斯过程回归和分类提出了数据中心方法和分布中心方法，分析后发现其与内核岭回归自我蒸馏和普通GPR对应。其中GPC的分布中心方法近似对应于数据复制和协方差的特定缩放。 |
| [^2] | [Query lower bounds for log-concave sampling.](http://arxiv.org/abs/2304.02599) | 该论文研究了对数凹采样的查询下界，在强对数凹和对数光滑分布中采样需要 $\Omega(\log \kappa)$ 查询，在采样高斯分布中需要 $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ 查询。 |
| [^3] | [Bayesian neural networks via MCMC: a Python-based tutorial.](http://arxiv.org/abs/2304.02595) | 本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。 |
| [^4] | [Self-Supervised Siamese Autoencoders.](http://arxiv.org/abs/2304.02549) | 本论文提出了一种新的自监督方法，名为SidAE，它结合了孪生架构和去噪自编码器的优点，可以更好地提取输入数据的特征，以在多个下游任务中获得更好的性能。 |
| [^5] | [Optimal Sketching Bounds for Sparse Linear Regression.](http://arxiv.org/abs/2304.02261) | 本文研究稀疏线性回归的最优草图界限，表明稀疏恢复比稀疏回归更容易草图，对于稀疏l_p回归，其上界包括l_2的额外添加项。 |
| [^6] | [Mixed Regression via Approximate Message Passing.](http://arxiv.org/abs/2304.02229) | 本文提出了一种新的近似消息传递算法来解决在广义线性模型中的回归问题，该算法适用于混合线性回归、最大仿射回归和专家混合模型等问题。 |
| [^7] | [On the universal approximation property of radial basis function neural networks.](http://arxiv.org/abs/2304.02220) | 本论文研究了一种新的径向基函数神经网络类别，证明这些网络能够逼近任何连续多元函数，还讨论了有限个固定中心的RBF网络的逼近条件。 |
| [^8] | [Structure Learning with Continuous Optimization: A Sober Look and Beyond.](http://arxiv.org/abs/2304.02146) | 本文探讨了连续优化在有向无环图结构学习中的优点和缺点，分析了不相等噪声方差公式中的非凸性问题，并建议未来研究将更多地考虑先验知识和已知结构，以实现更健壮的优化方法。 |
| [^9] | [Sequential Linearithmic Time Optimal Unimodal Fitting When Minimizing Univariate Linear Losses.](http://arxiv.org/abs/2304.02141) | 本文提出了一种线性对数时间算法，用于解决单变量学习模型在线性损失函数下得分输出的最优单峰转换问题。 |
| [^10] | [Algorithm-Dependent Bounds for Representation Learning of Multi-Source Domain Adaptation.](http://arxiv.org/abs/2304.02064) | 通过信息论工具的使用，该论文研究了多源域自适应表示学习中的联合分布对齐，提出了算法相关的泛化界限，并提出了一种新颖的深度学习算法，解决了目标偏移问题。 |
| [^11] | [Effective Theory of Transformers at Initialization.](http://arxiv.org/abs/2304.02034) | 本研究分析了宽且深的Transformer中的前向和后向信号传播，提出了特定的初始化和训练超参数宽度缩放建议，并在实际设置中验证了这些建议。 |
| [^12] | [Bounding probabilities of causation through the causal marginal problem.](http://arxiv.org/abs/2304.02023) | 本文介绍了一种通过因果边际问题，限定因果概率的方法，可以大大缩小现有的因果概率范围。 |
| [^13] | [Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases.](http://arxiv.org/abs/2303.15739) | 本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。 |
| [^14] | [A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser.](http://arxiv.org/abs/2301.13731) | 本文提出了一种新的收敛插入播放算法，使用一个松弛的近端去噪器和一个松弛的PGD算法，能够收敛于更广泛的正则化参数范围内。 |
| [^15] | [Learning Data Representations with Joint Diffusion Models.](http://arxiv.org/abs/2301.13622) | 本文提出了一种扩展香草扩散模型的方法，使用分类器进行联合端到端训练，以提高分类和生成数据的质量。 |
| [^16] | [A Characterization of Multioutput Learnability.](http://arxiv.org/abs/2301.02729) | 该论文研究了多输出函数类的学习问题，提出了多输出函数类学习的特征化判别标准，即每个单输出子类可学习时多输出函数类才可学习，在多标记分类和多输出回归领域取得了重要进展。 |
| [^17] | [Generalisation under gradient descent via deterministic PAC-Bayes.](http://arxiv.org/abs/2209.02525) | 本文介绍了一种新的PAC-Bayesian泛化界限，适用于使用梯度下降方法或连续梯度流训练模型的优化算法，且无需随机化。 |
| [^18] | [Rethinking Initialization of the Sinkhorn Algorithm.](http://arxiv.org/abs/2206.07630) | 本文认为Sinkhorn算法的初始化备受忽视，但数据相关的初始化可以提高算法性能，并适用于端到端学习。 |
| [^19] | [Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling.](http://arxiv.org/abs/2106.01357) | 本文提出了一种名为Diffusion SB (DSB) 的逼近迭代比例拟合程序，用于解决Schrödinger Bridge问题，该问题可以产生从数据分布中生成样本的扩散过程，而有限时间内完成。 |
| [^20] | [Generative Adversarial Networks (GANs Survey): Challenges, Solutions, and Future Directions.](http://arxiv.org/abs/2005.00065) | 生成对抗网络（GANs）是一种学习复杂高维概率分布的新型深度生成模型，但其训练存在着诸多挑战，如模式崩溃、不收敛及不稳定性。最近，针对这些挑战，提出了多种GANs的设计和优化方案来解决这些问题。 |

# 详细

[^1]: 高斯过程回归和分类的自我蒸馏

    Self-Distillation for Gaussian Process Regression and Classification. (arXiv:2304.02641v1 [stat.ML])

    [http://arxiv.org/abs/2304.02641](http://arxiv.org/abs/2304.02641)

    本文针对高斯过程回归和分类提出了数据中心方法和分布中心方法，分析后发现其与内核岭回归自我蒸馏和普通GPR对应。其中GPC的分布中心方法近似对应于数据复制和协方差的特定缩放。

    

    我们提出了两种方法来将知识蒸馏的概念扩展到高斯过程回归（GPR）和高斯过程分类（GPC）中；数据中心方法和分布中心方法。数据中心方法最像目前机器学习的大多数蒸馏技术，它在教师的确定性预测上重新适合一个模型，而分布中心方法则重新使用完整概率后验进行下一次迭代。通过分析这些方法的特性，我们表明GPR的数据中心方法与已知的内核岭回归自我蒸馏结果密切相关，而GPR的分布中心方法与具有特定超参数选择的普通GPR相对应。此外，我们演示了GPC的分布中心方法近似对应于数据复制和协方差的特定缩放，而GPC的数据中心方法需要重新定义模型。

    We propose two approaches to extend the notion of knowledge distillation to Gaussian Process Regression (GPR) and Gaussian Process Classification (GPC); data-centric and distribution-centric. The data-centric approach resembles most current distillation techniques for machine learning, and refits a model on deterministic predictions from the teacher, while the distribution-centric approach, re-uses the full probabilistic posterior for the next iteration. By analyzing the properties of these approaches, we show that the data-centric approach for GPR closely relates to known results for self-distillation of kernel ridge regression and that the distribution-centric approach for GPR corresponds to ordinary GPR with a very particular choice of hyperparameters. Furthermore, we demonstrate that the distribution-centric approach for GPC approximately corresponds to data duplication and a particular scaling of the covariance and that the data-centric approach for GPC requires redefining the mod
    
[^2]: 对数凹采样的查询下界

    Query lower bounds for log-concave sampling. (arXiv:2304.02599v1 [math.ST])

    [http://arxiv.org/abs/2304.02599](http://arxiv.org/abs/2304.02599)

    该论文研究了对数凹采样的查询下界，在强对数凹和对数光滑分布中采样需要 $\Omega(\log \kappa)$ 查询，在采样高斯分布中需要 $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ 查询。

    

    最近几年，对数凹采样在算法方面取得了显著的进展，但相应的证明此任务的下界的问题仍然很难，以前只知道在一维中存在较小的下界。在这项工作中，我们建立了以下查询下界：（1）在维度 $d\ge 2$中从强对数凹和对数光滑分布中采样需要 $\Omega(\log \kappa)$ 查询，这在任何固定维度上都是最优的，（2）从高斯分布中采样需要 $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ 查询（因此也适用于在维数 $d$ 中采样一般的对数凹和光滑分布），这对于高斯类几乎是最优的。这里 $\kappa$ 是目标分布的条件数。我们的证明依赖于（1）一种多尺度构造，受到了关于谐振分析中的Kakeya猜想的工作的启发，以及（2）一种新颖的约简，证明了块Krylov算法在此问题中是最佳的。

    Log-concave sampling has witnessed remarkable algorithmic advances in recent years, but the corresponding problem of proving lower bounds for this task has remained elusive, with lower bounds previously known only in dimension one. In this work, we establish the following query lower bounds: (1) sampling from strongly log-concave and log-smooth distributions in dimension $d\ge 2$ requires $\Omega(\log \kappa)$ queries, which is sharp in any constant dimension, and (2) sampling from Gaussians in dimension $d$ (hence also from general log-concave and log-smooth distributions in dimension $d$) requires $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ queries, which is nearly sharp for the class of Gaussians. Here $\kappa$ denotes the condition number of the target distribution. Our proofs rely upon (1) a multiscale construction inspired by work on the Kakeya conjecture in harmonic analysis, and (2) a novel reduction that demonstrates that block Krylov algorithms are optimal for this probl
    
[^3]: 基于MCMC的贝叶斯神经网络：基于Python的教程

    Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])

    [http://arxiv.org/abs/2304.02595](http://arxiv.org/abs/2304.02595)

    本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。

    

    贝叶斯推断为机器学习和深度学习提供了参数估计和不确定性量化的方法。变分推断和马尔科夫链蒙特卡罗（MCMC）采样技术用于实现贝叶斯推断。在过去三十年中，MCMC方法在适应更大的模型（如深度学习）和大数据问题方面面临了许多挑战。包括梯度的高级提议（例如Langevin提议分布）提供了一种解决MCMC采样中的一些限制的方法，此外，MCMC方法通常被限制在统计学家的使用范围内，并且仍不是深度学习研究人员的主流方法。我们提供了一个MCMC方法的教程，涵盖了简单的贝叶斯线性和逻辑模型，以及贝叶斯神经网络。这个教程的目的是通过编码来弥合理论和实现之间的差距，鉴于当前MCMC方法的普及程度仍然较低。

    Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
    
[^4]: 自监督的孪生自编码器

    Self-Supervised Siamese Autoencoders. (arXiv:2304.02549v1 [cs.LG])

    [http://arxiv.org/abs/2304.02549](http://arxiv.org/abs/2304.02549)

    本论文提出了一种新的自监督方法，名为SidAE，它结合了孪生架构和去噪自编码器的优点，可以更好地提取输入数据的特征，以在多个下游任务中获得更好的性能。

    

    完全监督的模型通常需要大量的标记训练数据，这往往是昂贵且难以获得的。相反，自监督表示学习减少了实现相同或更高下游性能所需的标记数据量。目标是在自监督任务上预先训练深度神经网络，以便网络能够从原始输入数据中提取有意义的特征。然后，将这些特征用作下游任务（如图像分类）中的输入。在先前的研究中，自编码器和孪生网络（如SimSiam）已成功应用于这些任务中。然而，仍然存在一些挑战，例如将特征的特性（例如，细节级别）与给定的任务和数据集匹配。在本文中，我们提出了一种结合了孪生架构和去噪自编码器优势的新自监督方法。我们展示了我们的模型，名为SidAE（孪生去噪自编码器），在多个下游任务上胜过了两个自监督最新基准。

    Fully supervised models often require large amounts of labeled training data, which tends to be costly and hard to acquire. In contrast, self-supervised representation learning reduces the amount of labeled data needed for achieving the same or even higher downstream performance. The goal is to pre-train deep neural networks on a self-supervised task such that afterwards the networks are able to extract meaningful features from raw input data. These features are then used as inputs in downstream tasks, such as image classification. Previously, autoencoders and Siamese networks such as SimSiam have been successfully employed in those tasks. Yet, challenges remain, such as matching characteristics of the features (e.g., level of detail) to the given task and data set. In this paper, we present a new self-supervised method that combines the benefits of Siamese architectures and denoising autoencoders. We show that our model, called SidAE (Siamese denoising autoencoder), outperforms two se
    
[^5]: 稀疏线性回归的最优草图界限

    Optimal Sketching Bounds for Sparse Linear Regression. (arXiv:2304.02261v1 [cs.DS])

    [http://arxiv.org/abs/2304.02261](http://arxiv.org/abs/2304.02261)

    本文研究稀疏线性回归的最优草图界限，表明稀疏恢复比稀疏回归更容易草图，对于稀疏l_p回归，其上界包括l_2的额外添加项。

    

    本文研究了各种损失函数下k-稀疏线性回归的遗忘草图，如l_p范数或广泛的hinge-like损失函数类，其中包括logistic和ReLU损失。我们表明，对于稀疏l_2范数回归，存在一个遗忘草图分布，具有Θ(klog(d/k)/ε^2)排，这是紧的，直到一个常数因子。这扩展到l_p损失，上界还有一个附加的O(klog(k/ε)/ε^2)项。这建立了与相关的稀疏恢复问题的出人意料的分离，这是稀疏回归的一个重要特例。对于这个问题，在l_2范数下，我们观察到一个O(klog(d)/ε+klog(k/ε)/ε^2)行的上界，表明稀疏恢复比稀疏回归更容易草图。对于包括稀疏logistic和稀疏ReLU回归在内的hinge-like损失函数下的稀疏回归，我们给出了与之对应的最优草图界。

    We study oblivious sketching for $k$-sparse linear regression under various loss functions such as an $\ell_p$ norm, or from a broad class of hinge-like loss functions, which includes the logistic and ReLU losses. We show that for sparse $\ell_2$ norm regression, there is a distribution over oblivious sketches with $\Theta(k\log(d/k)/\varepsilon^2)$ rows, which is tight up to a constant factor. This extends to $\ell_p$ loss with an additional additive $O(k\log(k/\varepsilon)/\varepsilon^2)$ term in the upper bound. This establishes a surprising separation from the related sparse recovery problem, which is an important special case of sparse regression. For this problem, under the $\ell_2$ norm, we observe an upper bound of $O(k \log (d)/\varepsilon + k\log(k/\varepsilon)/\varepsilon^2)$ rows, showing that sparse recovery is strictly easier to sketch than sparse regression. For sparse regression under hinge-like loss functions including sparse logistic and sparse ReLU regression, we giv
    
[^6]: 通过近似消息传递的混合回归（Mixed Regression via Approximate Message Passing）

    Mixed Regression via Approximate Message Passing. (arXiv:2304.02229v1 [stat.ML])

    [http://arxiv.org/abs/2304.02229](http://arxiv.org/abs/2304.02229)

    本文提出了一种新的近似消息传递算法来解决在广义线性模型中的回归问题，该算法适用于混合线性回归、最大仿射回归和专家混合模型等问题。

    

    本文研究了广义线性模型（GLM）中具有多个信号和潜变量的回归问题。该模型被称为矩阵GLM，涵盖了许多在统计学习中广泛研究的问题，包括混合线性回归、最大仿射回归和专家混合模型等。我们提出了一种新的近似消息传递（AMP）算法来估计矩阵GLM中的信号和潜变量，并在高维极限中对其性能进行了严格的表征。该表征是通过状态演化递归来计算的，从而可以精确计算渐近性能度量，例如信噪比下降阈值（threshold）。

    We study the problem of regression in a generalized linear model (GLM) with multiple signals and latent variables. This model, which we call a matrix GLM, covers many widely studied problems in statistical learning, including mixed linear regression, max-affine regression, and mixture-of-experts. In mixed linear regression, each observation comes from one of $L$ signal vectors (regressors), but we do not know which one; in max-affine regression, each observation comes from the maximum of $L$ affine functions, each defined via a different signal vector. The goal in all these problems is to estimate the signals, and possibly some of the latent variables, from the observations. We propose a novel approximate message passing (AMP) algorithm for estimation in a matrix GLM and rigorously characterize its performance in the high-dimensional limit. This characterization is in terms of a state evolution recursion, which allows us to precisely compute performance measures such as the asymptotic 
    
[^7]: 关于径向基函数神经网络普适逼近性质的研究

    On the universal approximation property of radial basis function neural networks. (arXiv:2304.02220v1 [cs.LG])

    [http://arxiv.org/abs/2304.02220](http://arxiv.org/abs/2304.02220)

    本论文研究了一种新的径向基函数神经网络类别，证明这些网络能够逼近任何连续多元函数，还讨论了有限个固定中心的RBF网络的逼近条件。

    

    本文研究了一种新的径向基函数神经网络类别，其中平滑因子被替换为位移。我们在激活函数的一定条件下证明了这些网络能够逼近欧几里得空间d维紧致子集上的任何连续多元函数。对于有限个固定中心的RBF网络，我们描述了保证任意精度逼近的条件。

    In this paper we consider a new class of RBF (Radial Basis Function) neural networks, in which smoothing factors are replaced with shifts. We prove under certain conditions on the activation function that these networks are capable of approximating any continuous multivariate function on any compact subset of the $d$-dimensional Euclidean space. For RBF networks with finitely many fixed centroids we describe conditions guaranteeing approximation with arbitrary precision.
    
[^8]: 带连续优化的结构学习：审慎观察及其发展

    Structure Learning with Continuous Optimization: A Sober Look and Beyond. (arXiv:2304.02146v1 [cs.LG])

    [http://arxiv.org/abs/2304.02146](http://arxiv.org/abs/2304.02146)

    本文探讨了连续优化在有向无环图结构学习中的优点和缺点，分析了不相等噪声方差公式中的非凸性问题，并建议未来研究将更多地考虑先验知识和已知结构，以实现更健壮的优化方法。

    

    本文研究连续优化在有向无环图（DAG）结构学习中的表现好坏及其原因，并提出了可能使搜索过程更可靠的方向。我们分析了连续方法在假设噪声方差相等和不相等的情况下的现象，并通过提供反例、理论证明和可能的替代解释来表明这种陈述在任一情况下都可能不成立。我们进一步证明了对于非相等噪声方差公式，非凸性可能是主要问题，而连续结构学习方面的最新进展则无法在学习速度和实现得分方面优于贪心搜索，并建议融合先验知识或已知结构的更健壮的优化方法是未来研究的一个有希望的方向。

    This paper investigates in which cases continuous optimization for directed acyclic graph (DAG) structure learning can and cannot perform well and why this happens, and suggests possible directions to make the search procedure more reliable. Reisach et al. (2021) suggested that the remarkable performance of several continuous structure learning approaches is primarily driven by a high agreement between the order of increasing marginal variances and the topological order, and demonstrated that these approaches do not perform well after data standardization. We analyze this phenomenon for continuous approaches assuming equal and non-equal noise variances, and show that the statement may not hold in either case by providing counterexamples, justifications, and possible alternative explanations. We further demonstrate that nonconvexity may be a main concern especially for the non-equal noise variances formulation, while recent advances in continuous structure learning fail to achieve impro
    
[^9]: 线性损失下的最优单峰拟合问题的线性对数时间算法

    Sequential Linearithmic Time Optimal Unimodal Fitting When Minimizing Univariate Linear Losses. (arXiv:2304.02141v1 [cs.LG])

    [http://arxiv.org/abs/2304.02141](http://arxiv.org/abs/2304.02141)

    本文提出了一种线性对数时间算法，用于解决单变量学习模型在线性损失函数下得分输出的最优单峰转换问题。

    

    本文关注于单变量学习模型在线性损失函数下得分输出的最优单峰转换问题。我们证明了分数面值和目标区域之间的最佳映射是一个矩形函数。为了获得观测样本的最优矩形拟合，我们提出了一种顺序方法，可以在每个新样本到来时进行估计。我们的方法每次迭代具有对数时间复杂度，并且具有最优的效率。

    This paper focuses on optimal unimodal transformation of the score outputs of a univariate learning model under linear loss functions. We demonstrate that the optimal mapping between score values and the target region is a rectangular function. To produce this optimal rectangular fit for the observed samples, we propose a sequential approach that can its estimation with each incoming new sample. Our approach has logarithmic time complexity per iteration and is optimally efficient.
    
[^10]: 算法相关的界限对多源域自适应表示学习的影响

    Algorithm-Dependent Bounds for Representation Learning of Multi-Source Domain Adaptation. (arXiv:2304.02064v1 [cs.LG])

    [http://arxiv.org/abs/2304.02064](http://arxiv.org/abs/2304.02064)

    通过信息论工具的使用，该论文研究了多源域自适应表示学习中的联合分布对齐，提出了算法相关的泛化界限，并提出了一种新颖的深度学习算法，解决了目标偏移问题。

    

    我们使用信息论工具从表示学习的角度对多源域自适应进行了新颖的分析，具体地，我们研究了有少量目标标签的监督 MDA 和带有伪标签的无监督 MDA 的联合分布对齐。我们进一步针对这两种情况提供了算法相关的泛化界限，其中泛化由参数与数据之间的互信息来描述。然后，我们提出了一种新颖的深度 MDA 算法，通过联合对齐隐含地解决了目标偏移问题。最后，这些互信息界限扩展到此算法，提供了一个非平庸的梯度范数估计。该算法在改进内存效率的同时，在目标偏移 MDA 基准测试中具有可比较的性能。

    We use information-theoretic tools to derive a novel analysis of Multi-source Domain Adaptation (MDA) from the representation learning perspective. Concretely, we study joint distribution alignment for supervised MDA with few target labels and unsupervised MDA with pseudo labels, where the latter is relatively hard and less commonly studied. We further provide algorithm-dependent generalization bounds for these two settings, where the generalization is characterized by the mutual information between the parameters and the data. Then we propose a novel deep MDA algorithm, implicitly addressing the target shift through joint alignment. Finally, the mutual information bounds are extended to this algorithm providing a non-vacuous gradient-norm estimation. The proposed algorithm has comparable performance to the state-of-the-art on target-shifted MDA benchmark with improved memory efficiency.
    
[^11]: 初始化时Transformer的有效理论分析

    Effective Theory of Transformers at Initialization. (arXiv:2304.02034v1 [cs.LG])

    [http://arxiv.org/abs/2304.02034](http://arxiv.org/abs/2304.02034)

    本研究分析了宽且深的Transformer中的前向和后向信号传播，提出了特定的初始化和训练超参数宽度缩放建议，并在实际设置中验证了这些建议。

    

    我们对宽且深的Transformer（即使用多头自注意块和多层感知机块的残差神经网络）中的前向和后向信号传播进行了有效理论分析。该分析建议这些模型的初始化和训练超参数采用特定的宽度缩放。我们随后采用这些建议，在实际设置中对视觉和语言Transformer进行训练。

    We perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, i.e., residual neural networks with multi-head self-attention blocks and multilayer perceptron blocks. This analysis suggests particular width scalings of initialization and training hyperparameters for these models. We then take up such suggestions, training Vision and Language Transformers in practical setups.
    
[^12]: 通过因果边际问题限定因果概率的范围

    Bounding probabilities of causation through the causal marginal problem. (arXiv:2304.02023v1 [stat.ML])

    [http://arxiv.org/abs/2304.02023](http://arxiv.org/abs/2304.02023)

    本文介绍了一种通过因果边际问题，限定因果概率的方法，可以大大缩小现有的因果概率范围。

    

    因果概率在法律、医疗和公共政策中对决策起着基础性的作用。然而，其准确定位具有挑战性，需要一些强假设，比如单调性。在没有这样的假设的情况下，现有的工作需要包含相同处理和结果变量的数据集的多个观测，以便对这些概率进行边界约束。然而，在许多临床试验和公共政策评估中，存在独立的数据集，这些数据集检查了不同治疗方式对同一结果变量的影响。在这里，我们描述了如何通过强制独立数据集中构造的SCM（“因果边际问题”）的反事实一致性，从而大大缩小现有的因果概率范围。接下来，我们描述了一种新的信息理论方法，用于通过使用条件互信息量化反事实影响来证明因果概率的虚假性。

    Probabilities of Causation play a fundamental role in decision making in law, health care and public policy. Nevertheless, their point identification is challenging, requiring strong assumptions such as monotonicity. In the absence of such assumptions, existing work requires multiple observations of datasets that contain the same treatment and outcome variables, in order to establish bounds on these probabilities. However, in many clinical trials and public policy evaluation cases, there exist independent datasets that examine the effect of a different treatment each on the same outcome variable. Here, we outline how to significantly tighten existing bounds on the probabilities of causation, by imposing counterfactual consistency between SCMs constructed from such independent datasets ('causal marginal problem'). Next, we describe a new information theoretic approach on falsification of counterfactual probabilities, using conditional mutual information to quantify counterfactual influe
    
[^13]: 深度ReLU神经网络在过参数化情况下的贝叶斯自由能

    Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])

    [http://arxiv.org/abs/2303.15739](http://arxiv.org/abs/2303.15739)

    本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。

    

    在人工智能的许多研究领域中，深度神经网络已被证明可用于估计高维输入空间中的未知函数。然而，它们的泛化性能尚未从理论角度完全澄清，因为它们是不可识别的和奇异的学习机器。此外，ReLU函数不可微，奇异学习理论中的代数或解析方法无法应用于它。本文研究了一种过参数化情况下的深度ReLU神经网络，并证明了Bayesian自由能是有界的，即使层数比估计未知数据生成函数所必需的层数更多。由于Bayesian广义误差等于样本大小的自由能增加，因此我们的结果也表明，Bayesian广义误差不会增加。

    In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
    
[^14]: 一种松弛的近端梯度下降算法用于带有近端去噪器的收敛插入-播放算法

    A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser. (arXiv:2301.13731v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13731](http://arxiv.org/abs/2301.13731)

    本文提出了一种新的收敛插入播放算法，使用一个松弛的近端去噪器和一个松弛的PGD算法，能够收敛于更广泛的正则化参数范围内。

    

    本文提出了一种新的收敛插入播放算法。插入播放方法是一种有效的迭代算法，用于解决被表述为数据适应项和正则化项之和的图像反问题的最小化问题。插入播放方法通过在近端算法（如近端梯度下降）中插入一个预先训练好的去噪器来执行正则化。为确保PnP方案的收敛，许多工作研究深度去噪器的特定参数化。然而，现有结果要么需要无法验证的假设或次优假设，要么在逆问题的参数上假设限制性条件。观察到这些限制可能是由于使用的近端算法，因此，我们研究了一种松弛版本的PGD算法（用于最小化凸函数和弱凸函数之和）。当与一个松弛的近端去噪器插入时，我们展示了所提出的PnP-$\alpha$PGD算法能够收敛于更广泛的正则化参数范围内。

    This paper presents a new convergent Plug-and-Play (PnP) algorithm. PnP methods are efficient iterative algorithms for solving image inverse problems formulated as the minimization of the sum of a data-fidelity term and a regularization term. PnP methods perform regularization by plugging a pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent (PGD). To ensure convergence of PnP schemes, many works study specific parametrizations of deep denoisers. However, existing results require either unverifiable or suboptimal hypotheses on the denoiser, or assume restrictive conditions on the parameters of the inverse problem. Observing that these limitations can be due to the proximal algorithm in use, we study a relaxed version of the PGD algorithm for minimizing the sum of a convex function and a weakly convex one. When plugged with a relaxed proximal denoiser, we show that the proposed PnP-$\alpha$PGD algorithm converges for a wider range of regularization parameters
    
[^15]: 使用联合扩散模型进行数据表示学习

    Learning Data Representations with Joint Diffusion Models. (arXiv:2301.13622v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13622](http://arxiv.org/abs/2301.13622)

    本文提出了一种扩展香草扩散模型的方法，使用分类器进行联合端到端训练，以提高分类和生成数据的质量。

    

    联合机器学习模型通常允许合成和分类数据，但常常在这些任务之间表现不平衡，或者不稳定。本文提出了扩展香草扩散模型以使用分类器进行稳定的联合端到端训练的方法。该模型在所有评估基准测试中在分类和生成质量方面均优于最新的混合方法。在我们的联合训练方法上，本文还介绍了如何通过引入一种方法进行直接益处共享生成和鉴别表示，以提供视觉反事实解释。

    Joint machine learning models that allow synthesizing and classifying data often offer uneven performance between those tasks or are unstable to train. In this work, we depart from a set of empirical observations that indicate the usefulness of internal representations built by contemporary deep diffusion-based generative models not only for generating but also predicting. We then propose to extend the vanilla diffusion model with a classifier that allows for stable joint end-to-end training with shared parameterization between those objectives. The resulting joint diffusion model outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality on all evaluated benchmarks. On top of our joint training approach, we present how we can directly benefit from shared generative and discriminative representations by introducing a method for visual counterfactual explanations.
    
[^16]: 多输出可学习性的特征化研究

    A Characterization of Multioutput Learnability. (arXiv:2301.02729v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02729](http://arxiv.org/abs/2301.02729)

    该论文研究了多输出函数类的学习问题，提出了多输出函数类学习的特征化判别标准，即每个单输出子类可学习时多输出函数类才可学习，在多标记分类和多输出回归领域取得了重要进展。

    

    本文考虑了批处理和在线学习中的多输出函数类学习问题。我们证明了，当且仅当函数类的每个单输出子类都可学习时，多输出函数类才是可学习的。这提供了多标记分类和多输出回归在批处理和在线学习中可学习性的完整特征化。作为扩展，我们还考虑了在赌博反馈环境下的多标记学习，并展示了与完全反馈环境下类似的特征化。

    We consider the problem of learning multioutput function classes in batch and online settings. In both settings, we show that a multioutput function class is learnable if and only if each single-output restriction of the function class is learnable. This provides a complete characterization of the learnability of multilabel classification and multioutput regression in both batch and online settings. As an extension, we also consider multilabel learnability in the bandit feedback setting and show a similar characterization as in the full-feedback setting.
    
[^17]: 基于确定性PAC-Bayes的梯度下降下的泛化

    Generalisation under gradient descent via deterministic PAC-Bayes. (arXiv:2209.02525v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.02525](http://arxiv.org/abs/2209.02525)

    本文介绍了一种新的PAC-Bayesian泛化界限，适用于使用梯度下降方法或连续梯度流训练模型的优化算法，且无需随机化。

    

    我们为使用梯度下降方法或连续梯度流训练模型建立了细分的PAC-Bayesian泛化界限。与PAC-Bayes设定中的标准做法相反，我们的结果适用于确定性的优化算法，而不需要任何去随机化的步骤。我们的界限是完全可计算的，取决于初始分布的密度和轨迹上训练目标的海森矩阵。我们展示了我们的框架可以应用于各种迭代优化算法，包括随机梯度下降（SGD）、动量算法和阻尼哈密顿动力学。

    We establish disintegrated PAC-Bayesian generalisation bounds for models trained with gradient descent methods or continuous gradient flows. Contrary to standard practice in the PAC-Bayesian setting, our result applies to optimisation algorithms that are deterministic, without requiring any de-randomisation step. Our bounds are fully computable, depending on the density of the initial distribution and the Hessian of the training objective over the trajectory. We show that our framework can be applied to a variety of iterative optimisation algorithms, including stochastic gradient descent (SGD), momentum-based schemes, and damped Hamiltonian dynamics.
    
[^18]: 重新思考Sinkhorn算法的初始化

    Rethinking Initialization of the Sinkhorn Algorithm. (arXiv:2206.07630v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.07630](http://arxiv.org/abs/2206.07630)

    本文认为Sinkhorn算法的初始化备受忽视，但数据相关的初始化可以提高算法性能，并适用于端到端学习。

    

    尽管最优输运问题最初被制定为线性规划，但添加熵正则化已被证明在许多应用中具有优越的计算和统计效果。Sinkhorn算法是解决这个正则化问题的最常用方法，并且已经有多次尝试通过使用如正则化参数退火、动量或加速度来减少其运行时间。本文的前提是Sinkhorn算法的初始化相对较少受到关注，可能是由于两种先入为主的观念:由于正则化的OT问题是凸问题，因此可能不值得设计良好的初始化，因为任何初始化都是可行的；其次，因为Sinkhorn算法的输出通常在端到端管道中展开，所以数据相关的初始化会对雅各比计算造成偏差。我们挑战这种传统智慧，并展示了数据相关的初始化可以提高Sinkhorn算法的性能，并适用于端到端学习。

    While the optimal transport (OT) problem was originally formulated as a linear program, the addition of entropic regularization has proven beneficial both computationally and statistically, for many applications. The Sinkhorn fixed-point algorithm is the most popular approach to solve this regularized problem, and, as a result, multiple attempts have been made to reduce its runtime using, e.g., annealing in the regularization parameter, momentum or acceleration. The premise of this work is that initialization of the Sinkhorn algorithm has received comparatively little attention, possibly due to two preconceptions: since the regularized OT problem is convex, it may not be worth crafting a good initialization, since any is guaranteed to work; secondly, because the outputs of the Sinkhorn algorithm are often unrolled in end-to-end pipelines, a data-dependent initialization would bias Jacobian computations. We challenge this conventional wisdom, and show that data-dependent initializers re
    
[^19]: 应用于基于得分模型的扩散Schrödinger桥

    Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling. (arXiv:2106.01357v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.01357](http://arxiv.org/abs/2106.01357)

    本文提出了一种名为Diffusion SB (DSB) 的逼近迭代比例拟合程序，用于解决Schrödinger Bridge问题，该问题可以产生从数据分布中生成样本的扩散过程，而有限时间内完成。

    

    逐步应用高斯噪声可以将复杂的数据分布转化为近似高斯分布。反向定义了一种生成模型。当正向噪声过程由随机微分方程（SDE）给出时，Song等人（2021）展示了如何使用得分匹配估计相应时间不均匀漂移的反向时间SDE。这种方法的局限性是需要运行足够长时间的正向时间SDE，才能使最终分布近似为高斯分布。相比之下，解决Schrödinger桥问题(SB)，即在路径空间上的熵正则化最优输运问题，可以产生从数据分布中生成样本的扩散过程，而有限时间内完成。我们提出了扩散SB（DSB），一个解决SB问题的原始近似迭代比例拟合（IPF）程序，并提供理论分析和生成建模实验。

    Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schr\"odinger Bridge problem (SB), i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the 
    
[^20]: 生成对抗网络（GANs）综述：挑战，解决方案和未来方向。

    Generative Adversarial Networks (GANs Survey): Challenges, Solutions, and Future Directions. (arXiv:2005.00065v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.00065](http://arxiv.org/abs/2005.00065)

    生成对抗网络（GANs）是一种学习复杂高维概率分布的新型深度生成模型，但其训练存在着诸多挑战，如模式崩溃、不收敛及不稳定性。最近，针对这些挑战，提出了多种GANs的设计和优化方案来解决这些问题。

    

    生成对抗网络（GANs）是一种新型的深度生成模型，最近引起了相当多的关注。GANs可以隐式地学习图像，音频和数据中的复杂高维分布。然而，由于网络结构的不当设计，目标函数的使用和优化算法的选择，GANs的训练存在着主要的挑战，如模式崩溃，不收敛和不稳定性。最近，为了解决这些挑战，基于重新设计的网络架构，新的目标函数和替代优化算法的技术，提出了几种更好的GANs设计和优化的解决方案。据我们所知，目前还没有一篇专门关注这些解决方案的广泛和系统的综述研究。在本研究中，我们对GANs设计和优化解决方案的发展进行了全面的调查。

    Generative Adversarial Networks (GANs) is a novel class of deep generative models which has recently gained significant attention. GANs learns complex and high-dimensional distributions implicitly over images, audio, and data. However, there exists major challenges in training of GANs, i.e., mode collapse, non-convergence and instability, due to inappropriate design of network architecture, use of objective function and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges.
    

