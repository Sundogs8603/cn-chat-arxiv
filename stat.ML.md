# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](https://arxiv.org/abs/2403.03950) | 通过使用分类代替回归训练值函数，本文提出了一种简单方法来改善深度强化学习的性能和可扩展性 |
| [^2] | [Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications](https://arxiv.org/abs/2403.03905) | 我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限 |
| [^3] | [Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage](https://arxiv.org/abs/2403.03868) | 该论文提出了一种构建具有有限样本精确覆盖的预测集的通用框架，可以解决在数据驱动情境中由于选择偏差导致的边缘有效预测区间误导问题。 |
| [^4] | [On the Origins of Linear Representations in Large Language Models](https://arxiv.org/abs/2403.03867) | 本文研究了大型语言模型中线性表示的起源，通过引入简单的潜变量模型，并展示了梯度下降的隐性偏差与下一个标记预测目标共同促进了概念的线性表示。 |
| [^5] | [Public-data Assisted Private Stochastic Optimization: Power and Limitations](https://arxiv.org/abs/2403.03856) | 该研究研究了公共数据辅助的私有差分隐私算法在随机凸优化问题中的限制和能力，展示出简单策略在这一领域的最优性。 |
| [^6] | [Accelerating Convergence of Score-Based Diffusion Models, Provably](https://arxiv.org/abs/2403.03852) | 设计了新颖的无需训练的算法，以加速流行的确定性和随机采样器，改进了确定性采样器的收敛速率至$O(1/{T}^2)$，提升了随机采样器的收敛速率至$O(1/T)$。 |
| [^7] | [Conformal prediction for multi-dimensional time series by ellipsoidal sets](https://arxiv.org/abs/2403.03850) | 开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于在多元时间序列中构建预测区域，具有更小的预测区域和有效的覆盖。 |
| [^8] | [Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters](https://arxiv.org/abs/2403.03816) | 该论文提出了一种针对具有噪声参数的黑盒模拟器的鲁棒贝叶斯优化方法，在优化过程中联合考虑控制参数和不确定性参数，以有效减少方差并充分利用控制与噪声的交互作用。 |
| [^9] | [Incentivized Learning in Principal-Agent Bandit Games](https://arxiv.org/abs/2403.03811) | 本文考虑了一个重复的委托-代理赌博游戏，在其中委托方通过提供激励以影响代理方的决策，目标是迭代学习激励策略最大化效用，提出了关于委托方后悔的几乎最优学习算法，支撑理论保证通过数值实验。 |
| [^10] | [AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs](https://arxiv.org/abs/2403.03772) | 通过有效地并行化现有的因果发现方法，本研究实现了对数千个维度的扩展，特别是将LiNGAM方法并行化，获得了多达32倍的加速。 |
| [^11] | [Online model error correction with neural networks: application to the Integrated Forecasting System](https://arxiv.org/abs/2403.03702) | 使用神经网络为欧洲中程气象中心的集成预测系统开发模型误差校正，以解决机器学习天气预报模型在表示动力平衡和适用于数据同化实验方面的挑战。 |
| [^12] | [Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models](https://arxiv.org/abs/2403.03695) | 该论文提出了一种针对不均匀尖峰维格纳模型的最优光谱方法，通过对转化矩阵的深入严格分析，在最佳阈值处实现了异常值和正重叠的相位转换。 |
| [^13] | [Spectral Algorithms on Manifolds through Diffusion](https://arxiv.org/abs/2403.03669) | 本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。 |
| [^14] | [Exact objectives of random linear programs and mean widths of random polyhedrons](https://arxiv.org/abs/2403.03637) | 该论文利用随机对偶理论，对随机线性规划的确切目标进行了研究，特别关注线性目标与随机多面体的平均宽度的联系。 |
| [^15] | [Reducing the dimensionality and granularity in hierarchical categorical variables](https://arxiv.org/abs/2403.03613) | 提出了一种减少层次分类变量维度和粒度的方法，通过实体嵌入和自上而下聚类算法来降低层内维度和整体粒度。 |
| [^16] | [Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application](https://arxiv.org/abs/2403.03602) | 通过基于数据的方法模拟了气缸压力和循环变化，以解决无法捕捉循环变化的问题，对于燃烧控制设计非常重要 |
| [^17] | [Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices](https://arxiv.org/abs/2403.03589) | 该研究提出了一种更有效地估计处理效应的活跃自适应实验设计方法，通过优化协变量密度和倾向得分来降低渐近方差。 |
| [^18] | [Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond](https://arxiv.org/abs/2403.03562) | 该研究提出了一种高效算法，用于解决群分布鲁棒优化问题，通过两级有限和凹凸最小最大优化结构和随机方差减小镜像Prox算法，实现对所有组的方差减少，并支持非恒定学习率。 |
| [^19] | [Uncertainty quantification for deeponets with ensemble kalman inversion](https://arxiv.org/abs/2403.03444) | 使用集合卡尔曼反演方法针对DeepONets提出了一种高效的不确定性量化推断方法。 |
| [^20] | [CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver](https://arxiv.org/abs/2403.03391) | CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。 |
| [^21] | [Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems](https://arxiv.org/abs/2403.03361) | 本文研究了线性赌博问题中Thompson-Sampling算法变体的贝叶斯遗憾，通过使用链接论证建立了具有度量动作空间的新界限，为$d$维线性赌博问题提供了$O(d\sqrt{T})$的严格率。 |
| [^22] | [Hypothesis Spaces for Deep Learning](https://arxiv.org/abs/2403.03353) | 本文介绍了一种应用深度神经网络的深度学习假设空间，并构建了一个再生核Banach空间，研究了正则化学习和最小插值问题，证明了学习模型的解可以表示为线性组合。 |
| [^23] | [Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects](https://arxiv.org/abs/2403.03240) | 研究提出了一种三重/去偏Lasso方法，用于统计推断条件平均处理效应，不要求直接假设稀疏性，有效估计了线性模型之间的差异。 |
| [^24] | [From Displacements to Distributions: A Machine-Learning Enabled Framework for Quantifying Uncertainties in Parameters of Computational Models](https://arxiv.org/abs/2403.03233) | 该论文提出了一种机器学习框架，可以量化计算模型参数不确定性的两种来源，通过结合数据一致性和学习不确定量框架，可以有效处理测量误差和兴趣数量映射问题 |
| [^25] | [Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks](https://arxiv.org/abs/2403.01636) | 通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。 |
| [^26] | [Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards](https://arxiv.org/abs/2402.18571) | 提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。 |
| [^27] | [Scalable Robust Sparse Principal Component Analysis](https://arxiv.org/abs/2402.16712) | 本文提出了一个优化框架，可在稀疏稳健的情况下估计一维子空间，通过引入线性松弛方法和新颖的拟合程序，实现了全局最优的稳健稀疏子空间，具有多项式时间效率且可扩展性强。 |
| [^28] | [Ensemble sampling for linear bandits: small ensembles suffice](https://arxiv.org/abs/2311.08376) | 该论文对随机线性赌臂环境中的集成抽样进行了首次实用和严格的分析，展示了在标准假设下，采用规模为$d \log T$的集成抽样可以获得接近$\sqrt{T}$阶的后悔，而不需要集成大小与$T$线性扩展。 |
| [^29] | [Exploration via linearly perturbed loss minimisation](https://arxiv.org/abs/2311.07565) | 提出了一种名为EVILL的随机探索方法，通过解决线性扰动的负对数似然函数的极小化问题来工作，提供了关于随机奖励扰动产生良好赌博算法的简洁解释，并在实践中展示了与汤普森抽样风格参数扰动方法性能相匹配的能力 |
| [^30] | [From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach](https://arxiv.org/abs/2311.03260) | 提出了一种新型的连续深度图神经网络KuramotoGNN，通过采用Kuramoto模型来减轻GNN中的过度平滑现象，实现节点特征的差异化，取得了优于基线GNN和现有方法的实验效果。 |
| [^31] | [Two-step interpretable modeling of Intensive Care Acquired Infections](https://arxiv.org/abs/2301.11146) | 提出了一种整合高分辨率纵向数据与生存模型动态预测能力的新方法，通过结合低分辨率数据和卷积神经网络提取的预测特征，提高了模型的预测能力，同时保持了模型可解释性。 |
| [^32] | [Differentially Private Generalized Linear Models Revisited](https://arxiv.org/abs/2205.03014) | 研究了具有凸损失的线性预测器的$(\epsilon,\delta)$-差分隐私学习问题，为两个损失函数子类提供了结果，并在所有参数上展示了基本紧密的上界和下界。 |
| [^33] | [Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias.](http://arxiv.org/abs/2310.14814) | 本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。 |
| [^34] | [Distribution-Free Statistical Dispersion Control for Societal Applications.](http://arxiv.org/abs/2309.13786) | 提出了一个简单而灵活的框架，用于处理具有社会意义的无分布统计离散度控制，可以应用于高风险应用。 |
| [^35] | [AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies.](http://arxiv.org/abs/2308.05027) | AbDiffuser是一个物理性扩散模型，用于联合生成抗体的三维结构和序列。该方法利用领域知识和基于物理的约束改善蛋白质扩散，处理序列长度变化，并能够生成与参考集合的序列和结构特性密切匹配的抗体。实验结果表明，AbDiffuser能够生成高水平表达的抗体，其中57.1%的设计选择是紧密结合剂。 |
| [^36] | [Linear Convergence Bounds for Diffusion Models via Stochastic Localization.](http://arxiv.org/abs/2308.03686) | 通过随机定位方法，我们提供了一种解决扩散模型中线性收敛界限问题的方法，并证明了这种方法可以在有限二阶矩条件下达到可接受的精度。 |
| [^37] | [A polynomial-time iterative algorithm for random graph matching with non-vanishing correlation.](http://arxiv.org/abs/2306.00266) | 本论文提出了一种具有非零相关性的随机图匹配的多项式迭代算法，并且在边缘相关性非零时成功恢复潜在匹配。 |
| [^38] | [High-Fidelity Image Compression with Score-based Generative Models.](http://arxiv.org/abs/2305.18231) | 本文提出了一种基于分数的生成模型的两阶段方法，该方法在图像压缩领域取得了显著的表现，实验证明该方法在一定比特率下能够提高图像的感知质量。 |

# 详细

[^1]: 停止回归：通过分类训练值函数实现可扩展的深度强化学习

    Stop Regressing: Training Value Functions via Classification for Scalable Deep RL

    [https://arxiv.org/abs/2403.03950](https://arxiv.org/abs/2403.03950)

    通过使用分类代替回归训练值函数，本文提出了一种简单方法来改善深度强化学习的性能和可扩展性

    

    值函数是深度强化学习（RL）的核心组件。这些通过神经网络参数化的函数，使用均方误差回归目标进行训练，以匹配自举目标值。然而，将使用回归的价值型RL方法扩展到大型网络，如高容量的Transformers，已被证明是具有挑战性的。本文观察到了这一差异，探讨了通过使用分类而不是回归来训练值函数是否也可以简单地提高深度RL的可扩展性。我们证明，使用分类交叉熵训练的值函数在各种领域中显著提高了性能和可扩展性。这些领域包括：在Atari 2600游戏上使用SoftMoEs进行单一任务RL。

    arXiv:2403.03950v1 Announce Type: cross  Abstract: Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs
    
[^2]: 黑盒$k$-to-$1$-PCA降维：理论与应用

    Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications

    [https://arxiv.org/abs/2403.03905](https://arxiv.org/abs/2403.03905)

    我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限

    

    $k$-主成分分析（$k$-PCA）问题是一种基本的算法原语，在数据分析和降维应用中被广泛使用。在统计环境中，$k$-PCA的目标是识别一个分布的协方差矩阵的顶部特征空间，我们只能通过样本隐式访问这个矩阵。受这些隐式设置的启发，我们分析黑盒缩减方法作为设计$k$-PCA算法的框架，其中我们通过黑盒$1$-PCA预言模拟对未知目标矩阵的访问，该预言返回一个近似的顶部特征向量，根据两个流行的近似概念。尽管这种黑盒方法可能是设计$k$-PCA算法中最自然的基于降维的方法，这种方法，即通过递归调用$1$-PCA预言调用了$k$次，以前很难理解。

    arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
    
[^3]: 焦点置信: 带有选择条件覆盖的整体预测

    Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage

    [https://arxiv.org/abs/2403.03868](https://arxiv.org/abs/2403.03868)

    该论文提出了一种构建具有有限样本精确覆盖的预测集的通用框架，可以解决在数据驱动情境中由于选择偏差导致的边缘有效预测区间误导问题。

    

    整体预测建立在边缘有效的预测区间上，该区间以某种规定的概率覆盖了随机抽取的新测试点的未知结果。在实践中，常见情况是，在看到测试单元后，从业者以数据驱动的方式决定关注哪些测试单元，并希望量化焦点单元的不确定性。在这种情况下，对于这些焦点单元的边缘有效预测区间可能会因选择偏差而具有误导性。本文提出了一个构建具有有限样本精确覆盖的预测集的通用框架，该覆盖是有条件于所选单元的。其一般形式适用于任意选择规则，并将Mondrian整体预测推广到多个测试单元和非等变分类器。然后，我们为多个现实的选择规则计算了适用于我们框架的计算效率实现，包括top-K选择、优化等。

    arXiv:2403.03868v1 Announce Type: cross  Abstract: Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability. In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s). In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected. Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization
    
[^4]: 论大型语言模型中线性表示的起源

    On the Origins of Linear Representations in Large Language Models

    [https://arxiv.org/abs/2403.03867](https://arxiv.org/abs/2403.03867)

    本文研究了大型语言模型中线性表示的起源，通过引入简单的潜变量模型，并展示了梯度下降的隐性偏差与下一个标记预测目标共同促进了概念的线性表示。

    

    近期研究表明，大型语言模型的表示空间中编码了高层语义概念是"线性"的。本文研究了这种线性表示的起源。为此，我们引入了一个简单的潜变量模型来抽象和形式化下一个标记预测的概念动态。我们利用这种形式化展示了下一个标记预测目标（具有交叉熵的softmax）和梯度下降的隐性偏差共同促进了概念的线性表示。实验表明，当学习与潜变量模型匹配的数据时，线性表示会出现，从而确认这种简单结构已足以产生线性表示。我们还使用 LLaMA-2 大型语言模型验证了这一理论的部分预测，证明了简化模型提供了可推广的见解。

    arXiv:2403.03867v1 Announce Type: new  Abstract: Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.
    
[^5]: 公共数据辅助下的私有随机优化：动力和限制

    Public-data Assisted Private Stochastic Optimization: Power and Limitations

    [https://arxiv.org/abs/2403.03856](https://arxiv.org/abs/2403.03856)

    该研究研究了公共数据辅助的私有差分隐私算法在随机凸优化问题中的限制和能力，展示出简单策略在这一领域的最优性。

    

    我们研究了公共数据辅助的差分隐私（PA-DP）算法的限制和能力。具体来说，我们关注具有标记或未标记公共数据的随机凸优化（SCO）问题。对于完整/标记的公共数据，我们表明任何$(\epsilon,\delta)$-PA-DP都具有超出风险$\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big\} \big)$，其中$d$是维数，${n_{\text{pub}}}$是公共样本数量，${n_{\text{priv}}}$是私有样本数量，$n={n_{\text{pub}}}+{n_{\text{priv}}}$. 这些下界是通过我们对PA-DP均值估计的新下界建立的，其形式相似。在常数因素的影响下，这些下界表明将所有数据视为私有或丢弃私有数据的简单策略是最优的。我们还研究了具有未标记公共数据的PA-DP监督学习。

    arXiv:2403.03856v1 Announce Type: new  Abstract: We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has excess risk $\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big\} \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of public samples, ${n_{\text{priv}}}$ is the number of private samples, and $n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \textit{unlabe
    
[^6]: 加速基于分数的扩散模型的收敛性，有保证

    Accelerating Convergence of Score-Based Diffusion Models, Provably

    [https://arxiv.org/abs/2403.03852](https://arxiv.org/abs/2403.03852)

    设计了新颖的无需训练的算法，以加速流行的确定性和随机采样器，改进了确定性采样器的收敛速率至$O(1/{T}^2)$，提升了随机采样器的收敛速率至$O(1/T)$。

    

    基于分数的扩散模型在实践中取得了显著的经验性能，但通常由于在采样阶段需要进行大量函数评估而导致采样速度较慢。尽管近年来一系列工作致力于加速扩散生成建模，但加速技术的理论基础仍然严重有限。在本文中，我们设计了新颖的无需训练的算法来加速流行的确定性（即DDIM）和随机（即DDPM）采样器。我们的加速确定性采样器以$O(1/{T}^2)$的速率收敛，其中$T$为步数，改进了DDIM采样器的$O(1/T)$速率；而我们的加速随机采样器以$O(1/T)$的速率收敛，优于DDPM采样器的$O(1/\sqrt{T})$速率。我们算法的设计利用了更高阶逼近的见解，并具有类似于流行的高阶ODE求解器的直觉。

    arXiv:2403.03852v1 Announce Type: cross  Abstract: Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like 
    
[^7]: 利用椭球集进行多维时间序列的合规预测

    Conformal prediction for multi-dimensional time series by ellipsoidal sets

    [https://arxiv.org/abs/2403.03850](https://arxiv.org/abs/2403.03850)

    开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于在多元时间序列中构建预测区域，具有更小的预测区域和有效的覆盖。

    

    合规预测（CP）因其无需假设分布、不受模型限制且在理论上可靠而成为一种流行的不确定性量化方法。对于监督学习中的预测问题，大多数CP方法专注于为单变量响应构建预测区间。在本文中，我们开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于为多元响应构建预测区域，特别是在不可交换的多元时间序列环境中。在理论上，我们估计了条件覆盖间隙的有限样本高概率界限。在实证方面，我们证明了$\texttt{MultiDimSPCI}$在各种多元时间序列上保持有效覆盖，同时产生比CP和非CP基线更小的预测区域。

    arXiv:2403.03850v1 Announce Type: cross  Abstract: Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.
    
[^8]: 针对方差减少：具有噪声参数的黑盒模拟器的鲁棒贝叶斯优化

    Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters

    [https://arxiv.org/abs/2403.03816](https://arxiv.org/abs/2403.03816)

    该论文提出了一种针对具有噪声参数的黑盒模拟器的鲁棒贝叶斯优化方法，在优化过程中联合考虑控制参数和不确定性参数，以有效减少方差并充分利用控制与噪声的交互作用。

    

    在许多科学应用中，需要优化控制参数$\mathbf{x}$的黑盒模拟器。在这些应用中，模拟器通常采用形式$f(\mathbf{x},\boldsymbol{\theta})$，其中$\boldsymbol{\theta}$是实践中不确定的参数。鲁棒优化的目标是优化期望$\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$，其中$\boldsymbol{\Theta} \sim \mathcal{P}$是模拟$\boldsymbol{\theta}$不确定性的随机变量。为此，现有的黑盒方法通常采用两阶段方法来选择下一个点$(\mathbf{x},\boldsymbol{\theta})$，其中$\mathbf{x}$和$\boldsymbol{\theta}$通过不同的收获函数分别优化。因此，这些方法未对$(\mathbf{x},\boldsymbol{\theta})$进行联合获取，可能无法充分利用控制与噪声相互作用实现有效的鲁棒优化。

    arXiv:2403.03816v1 Announce Type: cross  Abstract: The optimization of a black-box simulator over control parameters $\mathbf{x}$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form $f(\mathbf{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective $\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$, where $\boldsymbol{\Theta} \sim \mathcal{P}$ is a random variable that models uncertainty on $\boldsymbol{\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\mathbf{x},\boldsymbol{\theta})$, where $\mathbf{x}$ and $\boldsymbol{\theta}$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over $(\mathbf{x},\boldsymbol{\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust opti
    
[^9]: 委托-代理赌博游戏中的激励学习

    Incentivized Learning in Principal-Agent Bandit Games

    [https://arxiv.org/abs/2403.03811](https://arxiv.org/abs/2403.03811)

    本文考虑了一个重复的委托-代理赌博游戏，在其中委托方通过提供激励以影响代理方的决策，目标是迭代学习激励策略最大化效用，提出了关于委托方后悔的几乎最优学习算法，支撑理论保证通过数值实验。

    

    本文考虑了一个重复的委托-代理赌博游戏，委托方只能通过代理与环境互动。委托方和代理方的目标不一致，选择行动的权利仅归代理方所有。然而，委托方可以通过提供激励来影响代理方的决策，激励将计入其奖励之中。委托方的目标是迭代学习一种激励策略，以最大化其总效用。这一框架扩展了传统的赌博问题，并受到了几个实际应用的启发，如医疗保健或生态税收，传统的机制设计理论往往忽视了问题的学习方面。我们在多臂和线性背景设置下提出了关于委托方后悔的几乎最优（相对于一个时域 T 的）学习算法。最后，我们通过数值实验支持我们的理论保证。

    arXiv:2403.03811v1 Announce Type: cross  Abstract: This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.
    
[^10]: 加速LiNGAM: 以GPU速度学习因果DAGs

    AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs

    [https://arxiv.org/abs/2403.03772](https://arxiv.org/abs/2403.03772)

    通过有效地并行化现有的因果发现方法，本研究实现了对数千个维度的扩展，特别是将LiNGAM方法并行化，获得了多达32倍的加速。

    

    现有基于组合优化或搜索的因果发现方法速度较慢，限制了它们在大规模数据集上的应用。最近的一些方法尝试通过连续优化的结构学习来解决这一限制，但迄今为止这些方法并未提供统计保证。本文通过有效地并行化现有的因果发现方法，展示我们实际上可以将它们扩展到数千个维度，使其在规模更大的问题上变得实用。具体而言，我们对LiNGAM方法进行了并行化，这种方法与变量数量成二次关系，与现有的顺序实现相比，我们在基准数据集上获得了多达32倍的加速。特别是，我们专注于DirectLiNGAM中的因果排序子过程，并实现了GPU核心以加速它。这使我们能够将DirectLiNGAM应用于因果推断。

    arXiv:2403.03772v1 Announce Type: new  Abstract: Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inferen
    
[^11]: 在线模型误差校正与神经网络: 应用于集成预测系统

    Online model error correction with neural networks: application to the Integrated Forecasting System

    [https://arxiv.org/abs/2403.03702](https://arxiv.org/abs/2403.03702)

    使用神经网络为欧洲中程气象中心的集成预测系统开发模型误差校正，以解决机器学习天气预报模型在表示动力平衡和适用于数据同化实验方面的挑战。

    

    最近几年，在全球数值天气预报模型的完全数据驱动开发方面取得了显著进展。这些机器学习天气预报模型具有其优势，尤其是准确性和较低的计算需求，但也存在其弱点：它们难以表示基本动力平衡，并且远未适用于资料同化实验。混合建模出现为解决这些限制的一种有希望的方法。混合模型将基于物理的核心组件与统计组件（通常是神经网络）集成在一起，以增强预测能力。在本文中，我们提出使用神经网络为欧洲中程气象中心的运行集成预测系统（IFS）开发模型误差校正。神经网络最初会离线进行预训练，使用大量运行分析数据集

    arXiv:2403.03702v1 Announce Type: cross  Abstract: In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models. These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments. Hybrid modelling emerges as a promising approach to address these limitations. Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities. In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network. The neural network is initially pre-trained offline using a large dataset of operational analyses and a
    
[^12]: 具有块结构尖峰模型中的光谱相位转换和最优PCA

    Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models

    [https://arxiv.org/abs/2403.03695](https://arxiv.org/abs/2403.03695)

    该论文提出了一种针对不均匀尖峰维格纳模型的最优光谱方法，通过对转化矩阵的深入严格分析，在最佳阈值处实现了异常值和正重叠的相位转换。

    

    我们讨论了不均匀尖峰维格纳模型，这是最近引入的一个理论框架，用于研究各种学习场景中的结构噪声，通过随机矩阵理论的棱镜，特别关注其光谱特性。我们的主要目标是找到一种最优的光谱方法，并将在不均匀，块结构的维格纳模型中有名的\cite{BBP}（BBP）相位转换准则扩展到我们的情况。我们对一个转换矩阵进行了彻底的严格分析，并展示了出现1）在极限光谱分布的群外的异常值和2）相关特征向量与信号之间的正重叠的转变正好发生在最佳阈值处，使得所提出的光谱方法在不均匀维格纳问题的迭代方法类中是最优的。

    arXiv:2403.03695v1 Announce Type: cross  Abstract: We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties. Our primary objective is to find an optimal spectral method and to extend the celebrated \cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model. We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem.
    
[^13]: 通过扩散在流形上的谱算法

    Spectral Algorithms on Manifolds through Diffusion

    [https://arxiv.org/abs/2403.03669](https://arxiv.org/abs/2403.03669)

    本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。

    

    在重现核希尔伯特空间（RKHS）中应用的谱算法的现有研究主要集中在一般核函数上，经常忽略输入特征空间的固有结构。我们的论文引入了一个新的视角，主张输入数据位于一个嵌入到更高维欧几里得空间中的低维流形内。我们研究了RKHS中谱算法的收敛性能，特别是那些由热核生成的，被称为扩散空间的空间。通过结合输入的流形结构，我们采用积分算子技术推导了关于广义范数的紧收敛上界，这表明估计器在强意义下收敛到目标函数，意味着函数本身及其导数同时收敛。这些界提供了两个重要优势：首先，它们是完全连续的。

    arXiv:2403.03669v1 Announce Type: cross  Abstract: The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contin
    
[^14]: 随机线性规划的确切目标和随机多面体的平均宽度

    Exact objectives of random linear programs and mean widths of random polyhedrons

    [https://arxiv.org/abs/2403.03637](https://arxiv.org/abs/2403.03637)

    该论文利用随机对偶理论，对随机线性规划的确切目标进行了研究，特别关注线性目标与随机多面体的平均宽度的联系。

    

    我们将\emph{随机线性规划}（rlps）作为\emph{随机优化问题}（rops）的一个子类，并研究它们的典型行为。我们特别关注与随机多面体/多面体的平均宽度相关的适当线性目标。利用\emph{随机对偶理论}（RDT）\cite{StojnicRegRndDlt10}强大的工具，我们在大维度情境下获得了程序目标的确切特征。特别是，对于任意$\alpha=\lim_{n\rightarrow\infty}\frac{m}{n}\in(0,\infty)$，任意单位向量$\mathbf{c}\in{\mathbb R}^n$，任意固定的$\mathbf{a}\in{\mathbb R}^n$，和具有iid标准正态项的$A\in {\mathbb R}^{m\times n}$，我们有

    arXiv:2403.03637v1 Announce Type: cross  Abstract: We consider \emph{random linear programs} (rlps) as a subclass of \emph{random optimization problems} (rops) and study their typical behavior. Our particular focus is on appropriate linear objectives which connect the rlps to the mean widths of random polyhedrons/polytopes. Utilizing the powerful machinery of \emph{random duality theory} (RDT) \cite{StojnicRegRndDlt10}, we obtain, in a large dimensional context, the exact characterizations of the program's objectives. In particular, for any $\alpha=\lim_{n\rightarrow\infty}\frac{m}{n}\in(0,\infty)$, any unit vector $\mathbf{c}\in{\mathbb R}^n$, any fixed $\mathbf{a}\in{\mathbb R}^n$, and $A\in {\mathbb R}^{m\times n}$ with iid standard normal entries, we have   \begin{eqnarray*}   \lim_{n\rightarrow\infty}{\mathbb P}_{A} \left ( (1-\epsilon) \xi_{opt}(\alpha;\mathbf{a})   \leq \min_{A\mathbf{x}\leq \mathbf{a}}\mathbf{c}^T\mathbf{x} \leq (1+\epsilon) \xi_{opt}(\alpha;\mathbf{a}) \right 
    
[^15]: 减少层次分类变量的维度和粒度

    Reducing the dimensionality and granularity in hierarchical categorical variables

    [https://arxiv.org/abs/2403.03613](https://arxiv.org/abs/2403.03613)

    提出了一种减少层次分类变量维度和粒度的方法，通过实体嵌入和自上而下聚类算法来降低层内维度和整体粒度。

    

    层次分类变量往往具有许多级别（高粒度）和每个级别内许多类别（高维度）。将这些协变量包含在预测模型中可能导致过度拟合和估计问题。在当前文献中，层次协变量通常通过嵌套随机效应来纳入。然而，这并不有助于假设类别对响应变量具有相同的影响。本文提出了一种获得层次分类变量简化表示的方法。我们展示了如何在层次设置中应用实体嵌入。随后，我们提出了一种自上而下的聚类算法，利用嵌入中编码的信息来减少层内维度以及层次分类变量的整体粒度。在模拟实验中，我们展示了我们的方法可以有效地应用。

    arXiv:2403.03613v1 Announce Type: cross  Abstract: Hierarchical categorical variables often exhibit many levels (high granularity) and many classes within each level (high dimensionality). This may cause overfitting and estimation issues when including such covariates in a predictive model. In current literature, a hierarchical covariate is often incorporated via nested random effects. However, this does not facilitate the assumption of classes having the same effect on the response variable. In this paper, we propose a methodology to obtain a reduced representation of a hierarchical categorical variable. We show how entity embedding can be applied in a hierarchical setting. Subsequently, we propose a top-down clustering algorithm which leverages the information encoded in the embeddings to reduce both the within-level dimensionality as well as the overall granularity of the hierarchical categorical variable. In simulation experiments, we show that our methodology can effectively appro
    
[^16]: 基于数据的具有循环变化的气缸压力模型用于燃烧控制：RCCI发动机应用

    Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application

    [https://arxiv.org/abs/2403.03602](https://arxiv.org/abs/2403.03602)

    通过基于数据的方法模拟了气缸压力和循环变化，以解决无法捕捉循环变化的问题，对于燃烧控制设计非常重要

    

    气缸压力基础控制是先进的预混合燃烧概念的关键推动因素，除了确保稳健且安全的操作外，还可以实现气缸压力和热释放的塑造。这需要快速的面向控制的燃烧模型。多年来，已经提出了平均值模型，可以预测燃烧指标（例如，总的指示平均有效压力，或释放总热量的曲轴旋转角度），或者预测完整的气缸压力。然而，这些模型无法捕捉循环变化，这对于燃烧概念的控制设计至关重要，例如，反应性控制压缩点火，这可能会遭受大幅度的循环变化。在本研究中，使用基于数据的方法对气缸压力和循环变化进行了建模。该模型结合了主成分分解和高斯过程回归。进行了详细研究。

    arXiv:2403.03602v1 Announce Type: cross  Abstract: Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts. Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping. This requires fast control-oriented combustion models. Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure. However, these models are not able to capture cyclic variations. This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations. In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach. The model combines Principle Component Decomposition and Gaussian Process Regression. A detailed study is performed
    
[^17]: 用于处理因变量选择的活跃自适应实验设计的处理效应估计

    Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices

    [https://arxiv.org/abs/2403.03589](https://arxiv.org/abs/2403.03589)

    该研究提出了一种更有效地估计处理效应的活跃自适应实验设计方法，通过优化协变量密度和倾向得分来降低渐近方差。

    

    这项研究设计了一个自适应实验，用于高效地估计平均处理效应（ATEs）。我们考虑了一个自适应实验，其中实验者按顺序从由实验者决定的协变量密度中抽样一个实验单元，并分配一种处理。在分配处理后，实验者立即观察相应的结果。在实验结束时，实验者利用收集的样本估算出一个ATE。实验者的目标是通过较小的渐近方差估计ATE。现有研究已经设计了一些能够自适应优化倾向得分（处理分配概率）的实验。作为这种方法的一个概括，我们提出了一个框架，该框架下实验者优化协变量密度以及倾向得分，并发现优化协变量密度和倾向得分比仅优化倾向得分可以减少渐近方差更多的情况。

    arXiv:2403.03589v1 Announce Type: cross  Abstract: This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than o
    
[^18]: 高效算法用于经验群分布鲁棒优化及更多

    Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond

    [https://arxiv.org/abs/2403.03562](https://arxiv.org/abs/2403.03562)

    该研究提出了一种高效算法，用于解决群分布鲁棒优化问题，通过两级有限和凹凸最小最大优化结构和随机方差减小镜像Prox算法，实现对所有组的方差减少，并支持非恒定学习率。

    

    我们研究了群分布鲁棒优化（GDRO）的经验对应问题，旨在最小化$m$个不同组中的最大经验风险。我们将经验GDRO表述为$\textit{两级}$有限和凹凸最小最大优化问题，并开发了一种随机方差减小镜像Prox算法。与现有方法不同，我们通过逐组抽样技术构建了随机梯度，并为所有组执行方差减少，充分利用了经验GDRO的$\textit{两级}$有限和结构。此外，我们通过一个一索引偏移加权平均来计算快照和镜像快照点，这使我们与朴素的遍历平均方法有所不同。我们的算法还支持非恒定学习率，这与现有文献不同。我们建立了期望和高概率收敛保证，展示出$\m

    arXiv:2403.03562v1 Announce Type: new  Abstract: We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm. Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\textit{two-level}$ finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average. Our algorithm also supports non-constant learning rates, which is different from existing literature. We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\m
    
[^19]: 使用集合卡尔曼反演进行DeepONets的不确定性量化

    Uncertainty quantification for deeponets with ensemble kalman inversion

    [https://arxiv.org/abs/2403.03444](https://arxiv.org/abs/2403.03444)

    使用集合卡尔曼反演方法针对DeepONets提出了一种高效的不确定性量化推断方法。

    

    近年来，操作员学习，特别是DeepONet，因其高效地学习跨不同领域的输入和输出函数之间的复杂映射而受到广泛关注。在有限且带噪声数据的实际场景中，访问DeepONet预测中的不确定性变得至关重要，特别是在使命关键或安全关键应用中。现有方法要么计算密集，要么产生令人不满意的不确定性量化，为DeepONets量身定制高效且信息丰富的不确定性量化（UQ）技术留下了空间。在这项工作中，我们提出了一种利用集合卡尔曼反演（EKI）方法的新型推断方法，用于操作员学习的高效UQ。EKI以其无导数、噪声抗干扰和高度可并行化的特性而闻名，已经证明了在面向物理的神经网络的UQ中的优势。

    arXiv:2403.03444v1 Announce Type: cross  Abstract: In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our inn
    
[^20]: CoRMF: 临界有序循环均场伊辛求解器

    CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver

    [https://arxiv.org/abs/2403.03391](https://arxiv.org/abs/2403.03391)

    CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。

    

    我们提出了一种基于RNN的高效伊辛模型求解器，称为Criticality-ordered Recurrent Mean Field (CoRMF)，用于前向伊辛问题。在其核心部分，通过贪婪算法对N个自旋的伊辛模型进行了关键有序自旋序列的引入，从而可以利用自回归均场因子分解，并通过循环神经网络(RNNs)进行优化。我们的方法具有两个显著特点：(i)通过利用底层伊辛图的近似树结构，新获得的关键性顺序使变分均场和RNN之间得以统一，从而允许有效地利用概率推断来探究通常难以处理的伊辛模型;(ii)它具有良好的模块化、独立于模型而又足够表达能力，因此可以完全适用于任何前向伊辛推理问题，而且工作量极小。计算上，通过使用一种方差减少

    arXiv:2403.03391v1 Announce Type: cross  Abstract: We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-redu
    
[^21]: 链式信息论界限和线性赌博问题的严格遗憾率

    Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems

    [https://arxiv.org/abs/2403.03361](https://arxiv.org/abs/2403.03361)

    本文研究了线性赌博问题中Thompson-Sampling算法变体的贝叶斯遗憾，通过使用链接论证建立了具有度量动作空间的新界限，为$d$维线性赌博问题提供了$O(d\sqrt{T})$的严格率。

    

    本文研究了一种Thompson-Sampling算法变体在赌博问题中的贝叶斯遗憾。它基于[Russo and Van Roy，2015]的信息论框架，更具体地，基于[Dong and Van Roy，2020]的率-失真分析，在那里他们证明了$d$维线性赌博设置的遗憾率为$O(d\sqrt{T \log(T)})$的界限。我们专注于具有度量动作空间的赌博问题，并使用链接论证建立了依赖于动作空间度量熵的新界限，针对Thompson-Sampling的一个变体。在奖励的适当连续性假设下，我们的界限为$d$维线性赌博问题提供了$O(d\sqrt{T})$的严格率。

    arXiv:2403.03361v1 Announce Type: cross  Abstract: This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for bandit problems. It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\sqrt{T \log(T)})$ for the $d$-dimensional linear bandit setting. We focus on bandit problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling.   Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\sqrt{T})$ for $d$-dimensional linear bandit problems.
    
[^22]: 深度学习的假设空间

    Hypothesis Spaces for Deep Learning

    [https://arxiv.org/abs/2403.03353](https://arxiv.org/abs/2403.03353)

    本文介绍了一种应用深度神经网络的深度学习假设空间，并构建了一个再生核Banach空间，研究了正则化学习和最小插值问题，证明了学习模型的解可以表示为线性组合。

    

    本文介绍了一种应用深度神经网络（DNNs）的深度学习假设空间。通过将DNN视为两个变量的函数，即物理变量和参数变量，我们考虑了DNNs的原始集合，参数变量位于由DNNs的权重矩阵和偏置决定的一组深度和宽度中。然后在弱*拓扑中完成原始DNN集合的线性跨度，以构建一个物理变量函数的Banach空间。我们证明所构造的Banach空间是一个再生核Banach空间（RKBS），并构造其再生核。通过为学习模型的解建立表达定理，我们研究了两个学习模型，正则化学习和最小插值问题在结果RKBS中。表达定理揭示了这些学习模型的解可以表示为线性组合

    arXiv:2403.03353v1 Announce Type: cross  Abstract: This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of
    
[^23]: 用于统计推断的三重/去偏Lasso：条件平均处理效应

    Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects

    [https://arxiv.org/abs/2403.03240](https://arxiv.org/abs/2403.03240)

    研究提出了一种三重/去偏Lasso方法，用于统计推断条件平均处理效应，不要求直接假设稀疏性，有效估计了线性模型之间的差异。

    

    本研究调查了关于条件平均处理效应（CATEs）的估计和统计推断，CATEs作为表示个性化因果效应的一个指标已经引起了关注。在我们的数据生成过程中，我们假设与二值处理相关联的结果采用线性模型，并将CATE定义为这些线性模型的预期结果之间的差异。本研究允许线性模型是高维的，我们的兴趣在于对CATE进行一致估计和统计推断。在高维线性回归中，一种典型的方法是假设稀疏性。但是，在我们的研究中，我们不直接假设稀疏性。相反，我们仅在线性模型的差异中考虑稀疏性。我们首先使用双重稳健估计器来近似这种差异，然后用Lasso正则化将差异回归到协变量上。尽管这种回归估计量是

    arXiv:2403.03240v1 Announce Type: cross  Abstract: This study investigates the estimation and the statistical inference about Conditional Average Treatment Effects (CATEs), which have garnered attention as a metric representing individualized causal effects. In our data-generating process, we assume linear models for the outcomes associated with binary treatments and define the CATE as a difference between the expected outcomes of these linear models. This study allows the linear models to be high-dimensional, and our interest lies in consistent estimation and statistical inference for the CATE. In high-dimensional linear regression, one typical approach is to assume sparsity. However, in our study, we do not assume sparsity directly. Instead, we consider sparsity only in the difference of the linear models. We first use a doubly robust estimator to approximate this difference and then regress the difference on covariates with Lasso regularization. Although this regression estimator is
    
[^24]: 从位移到分布：一种用于量化计算模型参数不确定性的机器学习框架

    From Displacements to Distributions: A Machine-Learning Enabled Framework for Quantifying Uncertainties in Parameters of Computational Models

    [https://arxiv.org/abs/2403.03233](https://arxiv.org/abs/2403.03233)

    该论文提出了一种机器学习框架，可以量化计算模型参数不确定性的两种来源，通过结合数据一致性和学习不确定量框架，可以有效处理测量误差和兴趣数量映射问题

    

    这项工作提出了结合两种框架的新扩展，用于量化工程系统建模中的混合不确定性源，即aleatoric（即不可减少的）和epistemic（即可减少的）。数据一致性（DC）框架提出了一个逆问题和解决方案，以量化以给定量化兴趣图定的回拉和推进测度的aleatoric不确定性。不幸的是，预先指定的兴趣数量映射并不总是在与系统输出相关的数据收集之前是可用的。数据本身经常受到测量误差（即epistemic不确定性）的污染，这使得指定一个有用的兴趣数量映射的过程变得复杂。学习不确定量（LUQ）框架定义了一个正式的三步机器学习启用过程，用于将嘈杂数据集转化为学习到的兴趣的映射样本，以启用基于DC的反演。我们在LUQ中开发了一个强大的过滤步骤。

    arXiv:2403.03233v1 Announce Type: cross  Abstract: This work presents novel extensions for combining two frameworks for quantifying both aleatoric (i.e., irreducible) and epistemic (i.e., reducible) sources of uncertainties in the modeling of engineered systems. The data-consistent (DC) framework poses an inverse problem and solution for quantifying aleatoric uncertainties in terms of pullback and push-forward measures for a given Quantity of Interest (QoI) map. Unfortunately, a pre-specified QoI map is not always available a priori to the collection of data associated with system outputs. The data themselves are often polluted with measurement errors (i.e., epistemic uncertainties), which complicates the process of specifying a useful QoI. The Learning Uncertain Quantities (LUQ) framework defines a formal three-step machine-learning enabled process for transforming noisy datasets into samples of a learned QoI map to enable DC-based inversion. We develop a robust filtering step in LUQ 
    
[^25]: 通过多任务强化学习实现高效的短视探索

    Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks

    [https://arxiv.org/abs/2403.01636](https://arxiv.org/abs/2403.01636)

    通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。

    

    多任务强化学习（MTRL）方法在许多重要的强化学习（RL）任务中应用广泛，但近期MTRL理论的进展主要集中在通过假设任务间共享结构来提高统计效率，对于RL中至关重要的探索这一关键方面却大多被忽视。本文通过展示，当代理在足够多样化的任务集上训练时，具有短视探索设计（如$\epsilon$-贪心）的通用策略共享算法可以在MTRL中具有高样本效率，从我们所知，这是对“探索收益”在MTRL中的首次理论证明，也有助于解释短视探索在实践中应用广泛的成功。为了验证多样性的作用，我们在合成机器人控制任务上进行了实验证明。

    arXiv:2403.01636v1 Announce Type: cross  Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control
    
[^26]: 用于满足多样用户偏好的算术控制LLMs：具有多目标奖励的方向偏好对齐

    Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards

    [https://arxiv.org/abs/2402.18571](https://arxiv.org/abs/2402.18571)

    提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。

    

    针对大型语言模型（LLMs）的精细控制仍然是一个重要挑战，阻碍了它们适应各种用户需求。本文提出了方向偏好对齐（DPA）框架，通过多目标奖励建模来表示多样化的偏好配置，将用户偏好建模为奖励空间中的方向（即单位向量）以实现用户相关的偏好控制。

    arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
    
[^27]: 可扩展的稳健稀疏主成分分析

    Scalable Robust Sparse Principal Component Analysis

    [https://arxiv.org/abs/2402.16712](https://arxiv.org/abs/2402.16712)

    本文提出了一个优化框架，可在稀疏稳健的情况下估计一维子空间，通过引入线性松弛方法和新颖的拟合程序，实现了全局最优的稳健稀疏子空间，具有多项式时间效率且可扩展性强。

    

    在这项工作中，我们提出了一个优化框架来估计稀疏稳健的一维子空间。我们的目标是最小化表示误差和l1范数准则下的惩罚。鉴于问题是NP难的，我们引入了一种基于线性松弛的方法。此外，我们提出了一种利用简单比例和排序技术的新型拟合程序。所提出的算法展示了$O(n^2 m \log n)$的最坏时间复杂度，并且在某些情况下，实现了稀疏稳健子空间的全局最优，从而展示了多项式时间效率。与现有方法相比，所提出的算法找到具有最低不一致性的子空间，提供了在稀疏性和拟合之间更平滑的权衡。其架构具有可扩展性，对于2000x2000的矩阵，计算速度相较CPU版本提升了16倍。此外，这种方法...

    arXiv:2402.16712v1 Announce Type: new  Abstract: In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is
    
[^28]: 线性赌臂的集成抽样：小集成足矣

    Ensemble sampling for linear bandits: small ensembles suffice

    [https://arxiv.org/abs/2311.08376](https://arxiv.org/abs/2311.08376)

    该论文对随机线性赌臂环境中的集成抽样进行了首次实用和严格的分析，展示了在标准假设下，采用规模为$d \log T$的集成抽样可以获得接近$\sqrt{T}$阶的后悔，而不需要集成大小与$T$线性扩展。

    

    我们首次对随机线性赌臂设定下的集成抽样进行了有用且严谨的分析。特别地，我们展示了在标准假设下，对于一个具有交互作用时间跨度$T$的$d$维随机线性赌臂，采用集成大小为$\smash{d \log T}$的集成抽样，遭受的后悔最多为$\smash{(d \log T)^{5/2} \sqrt{T}}$阶。我们的结果是在任何结构化环境中第一个不要求集成大小与$T$线性扩展的结果，这使得集成抽样失去意义，同时获得了接近$\smash{\sqrt{T}}$阶的后悔。我们的结果也是第一个允许无限动作集的结果。

    arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
    
[^29]: 通过线性扰动的损失最小化来进行探索

    Exploration via linearly perturbed loss minimisation

    [https://arxiv.org/abs/2311.07565](https://arxiv.org/abs/2311.07565)

    提出了一种名为EVILL的随机探索方法，通过解决线性扰动的负对数似然函数的极小化问题来工作，提供了关于随机奖励扰动产生良好赌博算法的简洁解释，并在实践中展示了与汤普森抽样风格参数扰动方法性能相匹配的能力

    

    我们引入了一种称为通过线性损失扰动进行探索（EVILL）的随机探索方法，用于结构化随机赌博问题，该方法通过解决线性扰动正则化负对数似然函数的极小化问题来工作。我们展示，对于广义线性赌博问题，EVILL可以简化为扰动历史探索（PHE），一种通过在随机扰动奖励上进行训练来进行探索的方法。通过这样做，我们对随机奖励扰动何时以及为何产生良好的赌博算法提供了简单干净的解释。我们提出了先前PHE类型方法中不含的数据相关扰动，使EVILL能够在理论和实践中与汤普森抽样风格参数扰动方法的性能相匹配。此外，我们展示了一个超出广义线性赌博的例子，其中PHE导致不一致的估计，从而导致线性后悔，而EVILL则保持表现。

    arXiv:2311.07565v2 Announce Type: replace  Abstract: We introduce exploration via linear loss perturbations (EVILL), a randomised exploration method for structured stochastic bandit problems that works by solving for the minimiser of a linearly perturbed regularised negative log-likelihood function. We show that, for the case of generalised linear bandits, EVILL reduces to perturbed history exploration (PHE), a method where exploration is done by training on randomly perturbed rewards. In doing so, we provide a simple and clean explanation of when and why random reward perturbations give rise to good bandit algorithms. We propose data-dependent perturbations not present in previous PHE-type methods that allow EVILL to match the performance of Thompson-sampling-style parameter-perturbation methods, both in theory and in practice. Moreover, we show an example outside generalised linear bandits where PHE leads to inconsistent estimates, and thus linear regret, while EVILL remains performa
    
[^30]: 从耦合振荡器到图神经网络：基于Kuramoto模型的方法减轻过度平滑现象

    From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach

    [https://arxiv.org/abs/2311.03260](https://arxiv.org/abs/2311.03260)

    提出了一种新型的连续深度图神经网络KuramotoGNN，通过采用Kuramoto模型来减轻GNN中的过度平滑现象，实现节点特征的差异化，取得了优于基线GNN和现有方法的实验效果。

    

    我们提出了Kuramoto图神经网络（KuramotoGNN），一种新颖的连续深度图神经网络（GNN），它采用Kuramoto模型来缓解过度平滑现象，即随着层数增加，GNN中节点特征变得难以区分的问题。Kuramoto模型捕捉了非线性耦合振荡器的同步行为。从耦合振荡器的视角，我们首先展示了Kuramoto模型与基本GNN之间的联系，然后说明了GNN中的过度平滑现象可以被解释为Kuramoto模型中的相位同步。KuramotoGNN用频率同步取代了这种相位同步，以防止节点特征收敛到一起，同时使系统达到稳定的同步状态。我们在各种实验中验证了KuramotoGNN在减少过度平滑方面相对于基线GNN和现有方法的优势。

    arXiv:2311.03260v2 Announce Type: replace-cross  Abstract: We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various 
    
[^31]: 两步可解释建模：重症监护获得性感染

    Two-step interpretable modeling of Intensive Care Acquired Infections

    [https://arxiv.org/abs/2301.11146](https://arxiv.org/abs/2301.11146)

    提出了一种整合高分辨率纵向数据与生存模型动态预测能力的新方法，通过结合低分辨率数据和卷积神经网络提取的预测特征，提高了模型的预测能力，同时保持了模型可解释性。

    

    我们提出了一种新颖的方法，将高分辨率的纵向数据与生存模型的动态预测能力相结合。其目的是双重的：提高预测能力同时保持模型的可解释性。为了超越人工神经网络的黑匣子范式，我们提出了一种简约且稳健的半参数方法（即基于里程碑竞争风险模型），将常规收集的低分辨率数据与从卷积神经网络中提取的预测特征相结合，该神经网络是在高分辨率的时间相关信息上训练的。然后，我们使用显著性地图来分析和解释该模型的额外预测能力。为了说明我们的方法，我们重点关注被送往重症监护病房的患者中与医疗相关的感染。

    arXiv:2301.11146v2 Announce Type: replace-cross  Abstract: We present a novel methodology for integrating high resolution longitudinal data with the dynamic prediction capabilities of survival models. The aim is two-fold: to improve the predictive power while maintaining interpretability of the models. To go beyond the black box paradigm of artificial neural networks, we propose a parsimonious and robust semi-parametric approach (i.e., a landmarking competing risks model) that combines routinely collected low-resolution data with predictive features extracted from a convolutional neural network, that was trained on high resolution time-dependent information. We then use saliency maps to analyze and explain the extra predictive power of this model. To illustrate our methodology, we focus on healthcare-associated infections in patients admitted to an intensive care unit.
    
[^32]: 重新审视差分隐私广义线性模型

    Differentially Private Generalized Linear Models Revisited

    [https://arxiv.org/abs/2205.03014](https://arxiv.org/abs/2205.03014)

    研究了具有凸损失的线性预测器的$(\epsilon,\delta)$-差分隐私学习问题，为两个损失函数子类提供了结果，并在所有参数上展示了基本紧密的上界和下界。

    

    我们研究了具有凸损失的线性预测器的$(\epsilon,\delta)$-差分隐私学习问题。我们针对两个损失函数子类提供了结果。第一种情况是当损失是光滑且非负但不一定利普希兹时（如平方损失）。对于这种情况，我们建立了关于过量总体风险的上界，为$\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(n\epsilon)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{n\epsilon}\right\}\right)$，其中$n$是样本数，$d$是问题的维度，$w^*$是总体风险的最小化者。除了对$\Vert w^\ast\Vert$的依赖之外，我们的界限在所有参数上基本上是紧密的。特别地，我们展示了一个$\tilde{\Omega}\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(n\epsilon)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{n\epsilon}\right\}}$的下界。

    arXiv:2205.03014v2 Announce Type: replace  Abstract: We study the problem of $(\epsilon,\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an upper bound on the excess population risk of $\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(n\epsilon)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{n\epsilon}\right\}\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\Vert w^\ast\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\tilde{\Omega}\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(n\epsilon)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{n\epsilon}\right\}}
    
[^33]: 在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练

    Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14814](http://arxiv.org/abs/2310.14814)

    本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。

    

    自训练是半监督学习中一种众所周知的方法。它包括对模型自信度高的未标记数据进行伪标签分配，并将其视为标记样本进行处理。对于神经网络，通常使用softmax预测概率作为自信度度量，尽管已知它们对错误预测也过于自信。当数据标注受到某种约束时，这种现象尤为明显，即样本选择偏差存在。为了解决这个问题，我们提出了一种新的自信度度量方法，称为$\mathcal{T}$-相似度，它基于线性分类器的集成预测多样性。我们通过研究稳定点并描述单个成员的多样性与其性能之间的关系来提供我们方法的理论分析。我们通过对三种不同伪标签策略的实验验证了我们自信度度量的好处。

    Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
    
[^34]: 社会应用的无分布统计离散度控制

    Distribution-Free Statistical Dispersion Control for Societal Applications. (arXiv:2309.13786v1 [cs.LG])

    [http://arxiv.org/abs/2309.13786](http://arxiv.org/abs/2309.13786)

    提出了一个简单而灵活的框架，用于处理具有社会意义的无分布统计离散度控制，可以应用于高风险应用。

    

    在负责任的机器学习中，对模型性能的显式有限样本统计保证是一个重要因素。之前的研究主要关注于界定预测器的期望损失或者个体预测将承受的损失值在一个指定范围内的概率。然而，对于许多高风险应用而言，理解和控制损失分布的离散度，或者说人群中不同个体对算法决策的影响程度是至关重要的。我们开始研究具有社会意义的无分布统计离散度控制，并提出了一个简单但灵活的框架，可以处理比以前的工作更丰富的统计功能类。我们通过在有毒评论检测、医学影像和电影推荐等实验中验证了我们的方法。

    Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning. Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range. However, for many high-stakes applications, it is crucial to understand and control the dispersion of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation.
    
[^35]: AbDiffuser：体外功能抗体的全原子生成

    AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies. (arXiv:2308.05027v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.05027](http://arxiv.org/abs/2308.05027)

    AbDiffuser是一个物理性扩散模型，用于联合生成抗体的三维结构和序列。该方法利用领域知识和基于物理的约束改善蛋白质扩散，处理序列长度变化，并能够生成与参考集合的序列和结构特性密切匹配的抗体。实验结果表明，AbDiffuser能够生成高水平表达的抗体，其中57.1%的设计选择是紧密结合剂。

    

    我们介绍了一个名为AbDiffuser的等变物理性扩散模型，用于联合生成抗体的三维结构和序列。AbDiffuser建立在一种新的蛋白质结构表示上，依赖于一种针对齐位蛋白的新型架构，并利用强扩散先验改善去噪过程。我们的方法通过利用领域知识和基于物理的约束改善了蛋白质扩散；处理序列长度变化；并将内存复杂性降低一个数量级，实现了骨架和侧链的生成。我们在体内和体外验证了AbDiffuser。数值实验展示了AbDiffuser生成与参考集合的序列和结构特性密切匹配的抗体的能力。实验室实验证实，发现的16种HER2抗体均以高水平表达，并且57.1%的设计选择是紧密结合剂。

    We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
    
[^36]: 通过随机定位方法获得扩散模型的线性收敛界限

    Linear Convergence Bounds for Diffusion Models via Stochastic Localization. (arXiv:2308.03686v1 [stat.ML])

    [http://arxiv.org/abs/2308.03686](http://arxiv.org/abs/2308.03686)

    通过随机定位方法，我们提供了一种解决扩散模型中线性收敛界限问题的方法，并证明了这种方法可以在有限二阶矩条件下达到可接受的精度。

    

    扩散模型是从高维数据分布中生成近似样本的有效方法。最近的一些研究结果提供了关于这种模型的收敛速度的多项式界限，假设$L^2$准确的得分估计器。然而，到目前为止，已知的最佳界限要么对数据维度是超线性的，要么需要强平滑性假设。我们提供了第一个假设只需要数据分布有有限二阶矩的收敛界限，这些界限对于数据维度是线性的（乘以对数因子）。我们证明了扩散模型最多需要$\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$步，就可以将带有方差为$\delta$的高斯噪声损坏的任意数据分布在Kullback--Leibler散度下近似到$\varepsilon^2$。我们的证明依赖于前人的Girsanov方法。我们引入了对于反向SD离散化误差的精细处理。

    Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SD
    
[^37]: 一种具有非零相关性的随机图匹配的多项式迭代算法

    A polynomial-time iterative algorithm for random graph matching with non-vanishing correlation. (arXiv:2306.00266v1 [cs.DS])

    [http://arxiv.org/abs/2306.00266](http://arxiv.org/abs/2306.00266)

    本论文提出了一种具有非零相关性的随机图匹配的多项式迭代算法，并且在边缘相关性非零时成功恢复潜在匹配。

    

    我们提出了一种用于匹配两个相关的Erdős-Rényi图表的有效算法，它们具有 $n$ 个顶点，其边缘通过潜在的顶点对应关系相互关联。当边缘密度 $q=n^{-\alpha+o(1)}$，对于一个常数 $\alpha \in [0,1)$ 时，我们展示了我们的算法具有多项式运行时间，并且只要边缘相关性非零，就能成功恢复潜在匹配。这与我们先前关于匹配两个具有非零相关性的高斯Wigner矩阵的多项式时间算法的工作密切相关，并且在边缘相关性低于Otter常数的平方根（约为0.338）时，提供了第一个多项式时间随机图匹配算法（无论 $q$ 的范围如何）。

    We propose an efficient algorithm for matching two correlated Erd\H{o}s--R\'enyi graphs with $n$ vertices whose edges are correlated through a latent vertex correspondence. When the edge density $q= n^{- \alpha+o(1)}$ for a constant $\alpha \in [0,1)$, we show that our algorithm has polynomial running time and succeeds to recover the latent matching as long as the edge correlation is non-vanishing. This is closely related to our previous work on a polynomial-time algorithm that matches two Gaussian Wigner matrices with non-vanishing correlation, and provides the first polynomial-time random graph matching algorithm (regardless of the regime of $q$) when the edge correlation is below the square root of the Otter's constant (which is $\approx 0.338$).
    
[^38]: 基于分数的生成模型的高保真图像压缩

    High-Fidelity Image Compression with Score-based Generative Models. (arXiv:2305.18231v1 [eess.IV])

    [http://arxiv.org/abs/2305.18231](http://arxiv.org/abs/2305.18231)

    本文提出了一种基于分数的生成模型的两阶段方法，该方法在图像压缩领域取得了显著的表现，实验证明该方法在一定比特率下能够提高图像的感知质量。

    

    尽管扩散生成模型在文本到图像生成中取得了巨大的成功，但在图像压缩领域复制这个成功却很困难。在本文中，我们展示了扩散模型可以显著提高在给定比特率下的感知质量，通过 FID 分数评估，表现超越了 PO-ELIC 和 HiFiC 的现有方法。我们通过一个简单但在理论上有动机的两阶段方法实现了这一点，该方法结合了以 MSE 为目标的自动编码器和一个进一步基于分数的解码器。然而，正如我们将展示的那样，实现细节很重要，最佳设计决策可能与典型的文本到图像模型有很大不同。

    Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.
    

