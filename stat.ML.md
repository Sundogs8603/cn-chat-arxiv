# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mixture of multilayer stochastic block models for multiview clustering.](http://arxiv.org/abs/2401.04682) | 本文提出了一种混合多层次随机块模型的方法，用于聚合来自不同信息源的多个聚类，并将观测分区到不同的聚类中，考虑到它们在组件内的特定性。该方法在合成数据和全球食品贸易网络中得到了有趣的结果。 |
| [^2] | [Linear Recursive Feature Machines provably recover low-rank matrices.](http://arxiv.org/abs/2401.04553) | 该论文介绍了递归特征机器（RFMs）算法，它通过交替重新加权特征向量和在转换空间中学习预测函数来执行显式的特征学习。研究分析了RFM在稀疏线性回归和低秩矩阵恢复问题中的维数减少性能。 |
| [^3] | [Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection and Beyond.](http://arxiv.org/abs/2401.04535) | 我们提出了一种半监督深度Sobolev回归器，利用深度神经网络进行梯度范数正则化，可以同时估计回归函数和其梯度，即使存在显著领域变化。这在半监督学习中利用无标签数据方面具有可证优势。 |
| [^4] | [Stable generative modeling using diffusion maps.](http://arxiv.org/abs/2401.04372) | 本文提出了一种稳定的生成建模方法，通过将扩散映射与朗之万动力学相结合，在仅有有限数量的训练样本的情况下生成新样本，并解决了时间步长僵硬随机微分方程中的稳定性问题。 |
| [^5] | [Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes.](http://arxiv.org/abs/2401.04286) | 本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。 |
| [^6] | [Predicting the structure of dynamic graphs.](http://arxiv.org/abs/2401.04280) | 本文提出了一种预测动态图结构的方法，利用时间序列方法预测未来时间点的节点度，并结合通量平衡分析方法获得未来图的结构，评估了该方法在合成和真实数据集上的实用性和适用性。 |
| [^7] | [On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond.](http://arxiv.org/abs/2401.03301) | 本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。 |
| [^8] | [Execution time budget assignment for mixed criticality systems.](http://arxiv.org/abs/2401.02431) | 本文提出了一种启发式方法，根据执行时间变异性为每个低关键实时任务分配执行时间预算，降低了超出预算的概率。 |
| [^9] | [Isolated pulsar population synthesis with simulation-based inference.](http://arxiv.org/abs/2312.14848) | 本论文使用模拟推断方法结合脉冲星种群合成，来限制孤立银河射电脉冲星的磁旋转特性。 |
| [^10] | [Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications.](http://arxiv.org/abs/2312.02828) | 本论文研究了带有无界方差的有偏噪声对随机逼近算法的收敛速度的影响，并介绍了该算法在各个领域的应用。 |
| [^11] | [Risk Assessment and Statistical Significance in the Age of Foundation Models.](http://arxiv.org/abs/2310.07132) | 本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。 |
| [^12] | [Attention to Entropic Communication.](http://arxiv.org/abs/2307.11423) | 该论文研究了在通信理论中结合注意力和相对熵的概念。研究发现，在通信中使用注意力导向的加权相对熵是不适当的，而适当的注意力通信可通过发送者仅需要了解接收者的效用函数来实现最佳通知。 |
| [^13] | [Online Laplace Model Selection Revisited.](http://arxiv.org/abs/2307.06093) | 本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。 |
| [^14] | [Learning Likelihood Ratios with Neural Network Classifiers.](http://arxiv.org/abs/2305.10500) | 该研究介绍了一种使用神经网络分类器参数化计算似然比的技巧，并详细比较了不同设置的性能。这对于许多数据或基于模拟的科学应用非常有用。 |
| [^15] | [Auditing and Generating Synthetic Data with Controllable Trust Trade-offs.](http://arxiv.org/abs/2304.10819) | 本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。 |
| [^16] | [Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression.](http://arxiv.org/abs/2304.01561) | 本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。 |
| [^17] | [Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator.](http://arxiv.org/abs/2302.09580) | 该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。 |
| [^18] | [General-Purpose In-Context Learning by Meta-Learning Transformers.](http://arxiv.org/abs/2212.04458) | 本文展示了Transformer和其他黑盒模型可以通过元学习训练成为通用的上下文学习器，该模型可以在各种问题上进行测试集预测，无需定义推理模型、训练损失或优化算法。 |
| [^19] | [Distribution Free Prediction Sets for Node Classification.](http://arxiv.org/abs/2211.14555) | 本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类，证明了提供了比简单的符合预测应用更加紧致和良好校准的预测集。 |

# 详细

[^1]: 混合多层次随机块模型用于多视角聚类

    Mixture of multilayer stochastic block models for multiview clustering. (arXiv:2401.04682v1 [cs.LG])

    [http://arxiv.org/abs/2401.04682](http://arxiv.org/abs/2401.04682)

    本文提出了一种混合多层次随机块模型的方法，用于聚合来自不同信息源的多个聚类，并将观测分区到不同的聚类中，考虑到它们在组件内的特定性。该方法在合成数据和全球食品贸易网络中得到了有趣的结果。

    

    在这项工作中，我们提出了一种用于聚合来自不同信息源的多个聚类的原始方法。每个分区由观测之间的共属性矩阵进行编码。我们的方法使用了混合多层次随机块模型（SBM）将具有相似信息的共属性矩阵分组为组件，并将观测分区到不同的聚类中，考虑到它们在组件内的特定性。模型参数的可识别性被建立，并提出了一个变分贝叶斯EM算法来估计这些参数。贝叶斯框架允许选择最优的聚类和组件数量。利用合成数据将提出的方法与共识聚类和基于张量的算法进行了比较，用于大规模复杂网络中的社区检测。最后，该方法被用于分析全球食品贸易网络，得到了有趣的结构。

    In this work, we propose an original method for aggregating multiple clustering coming from different sources of information. Each partition is encoded by a co-membership matrix between observations. Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters. The Bayesian framework allows for selecting an optimal number of clusters and components. The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. Finally, the method is utilized to analyze global food trading networks, leading to structures of interest.
    
[^2]: 线性递归特征机器可以可靠地恢复低秩矩阵

    Linear Recursive Feature Machines provably recover low-rank matrices. (arXiv:2401.04553v1 [stat.ML])

    [http://arxiv.org/abs/2401.04553](http://arxiv.org/abs/2401.04553)

    该论文介绍了递归特征机器（RFMs）算法，它通过交替重新加权特征向量和在转换空间中学习预测函数来执行显式的特征学习。研究分析了RFM在稀疏线性回归和低秩矩阵恢复问题中的维数减少性能。

    

    机器学习中一个基本的问题是要理解神经网络如何准确预测，同时似乎避免了维数诅咒。一个可能的解释是神经网络的常见训练算法隐含地进行维数减少 - 一个被称为特征学习的过程。最近的工作假设特征学习的效果可以从一个称为平均梯度外积（AGOP）的经典统计估计器中推测出来。作者提出了递归特征机器（RFMs）作为一种算法，通过在转换空间中交替进行（1）通过AGOP对特征向量重新加权和（2）学习预测函数，显式地执行特征学习。在这项工作中，我们通过关注稀疏线性回归和低秩矩阵恢复中出现的过参数化问题的类别，开发了关于RFM如何进行维数减少的第一个理论保证。具体地，我们展示了RFM在限制条件下的性能。

    A fundamental problem in machine learning is to understand how neural networks make accurate predictions, while seemingly bypassing the curse of dimensionality. A possible explanation is that common training algorithms for neural networks implicitly perform dimensionality reduction - a process called feature learning. Recent work posited that the effects of feature learning can be elicited from a classical statistical estimator called the average gradient outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as an algorithm that explicitly performs feature learning by alternating between (1) reweighting the feature vectors by the AGOP and (2) learning the prediction function in the transformed space. In this work, we develop the first theoretical guarantees for how RFM performs dimensionality reduction by focusing on the class of overparametrized problems arising in sparse linear regression and low-rank matrix recovery. Specifically, we show that RFM restricted t
    
[^3]: 半监督深度Sobolev回归: 估计、变量选择及其他

    Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection and Beyond. (arXiv:2401.04535v1 [stat.ML])

    [http://arxiv.org/abs/2401.04535](http://arxiv.org/abs/2401.04535)

    我们提出了一种半监督深度Sobolev回归器，利用深度神经网络进行梯度范数正则化，可以同时估计回归函数和其梯度，即使存在显著领域变化。这在半监督学习中利用无标签数据方面具有可证优势。

    

    我们提出了SDORE，一种半监督深度Sobolev回归器，用于非参数估计潜在的回归函数及其梯度。SDORE使用深度神经网络来最小化经验风险，并采用梯度范数正则化，允许对无标签数据计算梯度范数。我们对SDORE的收敛速度进行了全面分析，并建立了回归函数的最小化最优速率。重要的是，在存在显著领域变化的情况下，我们还推导出了关联的插值梯度估计器的收敛速度。这些理论结果为选择正则化参数和确定神经网络的大小提供了有价值的先验指导，并展示了在半监督学习中利用无标签数据的可证优势。据我们所知，SDORE是第一个同时估计回归函数及其梯度的可证神经网络方法，具有多样化的应用。

    We propose SDORE, a semi-supervised deep Sobolev regressor, for the nonparametric estimation of the underlying regression function and its gradient. SDORE employs deep neural networks to minimize empirical risk with gradient norm regularization, allowing computation of the gradient norm on unlabeled data. We conduct a comprehensive analysis of the convergence rates of SDORE and establish a minimax optimal rate for the regression function. Crucially, we also derive a convergence rate for the associated plug-in gradient estimator, even in the presence of significant domain shift. These theoretical findings offer valuable prior guidance for selecting regularization parameters and determining the size of the neural network, while showcasing the provable advantage of leveraging unlabeled data in semi-supervised learning. To the best of our knowledge, SDORE is the first provable neural network-based approach that simultaneously estimates the regression function and its gradient, with diverse
    
[^4]: 使用扩散映射进行稳定的生成建模

    Stable generative modeling using diffusion maps. (arXiv:2401.04372v1 [stat.ML])

    [http://arxiv.org/abs/2401.04372](http://arxiv.org/abs/2401.04372)

    本文提出了一种稳定的生成建模方法，通过将扩散映射与朗之万动力学相结合，在仅有有限数量的训练样本的情况下生成新样本，并解决了时间步长僵硬随机微分方程中的稳定性问题。

    

    我们考虑从仅有足够数量的训练样本可得到的未知分布中抽样的问题。在生成建模的背景下，这样的设置最近引起了相当大的关注。本文中，我们提出了一种将扩散映射和朗之万动力学相结合的生成模型。扩散映射用于从可用的训练样本中近似得到漂移项，然后在离散时间的朗之万采样器中实现生成新样本。通过将核带宽设置为与未调整的朗之万算法中使用的时间步长匹配，我们的方法可以有效地避免通常与时间步长僵硬随机微分方程相关的稳定性问题。更准确地说，我们引入了一种新颖的分裂步骤方案，确保生成的样本保持在训练样本的凸包内。我们的框架可以自然地扩展为生成条件样本。我们展示了性能。

    We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance
    
[^5]: 宽而深的ReLU神经网络的普适一致性以及Kolmogorov-Donoho最优函数类的极小极限收敛速率

    Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])

    [http://arxiv.org/abs/2401.04286](http://arxiv.org/abs/2401.04286)

    本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。

    

    本文首先扩展了FL93的结果，并证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则的普适一致性。与FL93中分解估计和经验误差的方法不同，我们根据一个广泛的神经网络能够插值任意数量的点的观察，直接分析分类风险。其次，我们给出了一类概率测度的充分条件，在这些条件下，基于神经网络的分类器实现了极小极限收敛速率。我们的结果源于实践者观察到神经网络通常被训练成达到0训练误差的事实，这也是我们提出的神经网络分类器的情况。我们的证明依赖于最近在经验风险最小化和深ReLU神经网络的逼近速率方面的发展，适用于不同的感兴趣函数类的应用。

    In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas
    
[^6]: 预测动态图的结构

    Predicting the structure of dynamic graphs. (arXiv:2401.04280v1 [cs.LG])

    [http://arxiv.org/abs/2401.04280](http://arxiv.org/abs/2401.04280)

    本文提出了一种预测动态图结构的方法，利用时间序列方法预测未来时间点的节点度，并结合通量平衡分析方法获得未来图的结构，评估了该方法在合成和真实数据集上的实用性和适用性。

    

    动态图嵌入、归纳和增量学习有助于预测任务，如节点分类和链接预测。然而，从图的时间序列中预测未来时间步的图结构，允许有新节点，并没有受到太多关注。在本文中，我们提出了一种这样的方法。我们使用时间序列方法预测未来时间点的节点度，并将其与通量平衡分析（一种在生物化学中使用的线性规划方法）结合起来，以获得未来图的结构。此外，我们探索了不同参数值的预测图分布。我们使用合成和真实数据集评估了该方法，并展示了其实用性和适用性。

    Dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. However, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. In this paper, we present such an approach. We use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. Furthermore, we explore the predictive graph distribution for different parameter values. We evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.
    
[^7]: 在样本高效的离线强化学习中：数据多样性、后验采样，以及更多

    On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])

    [http://arxiv.org/abs/2401.03301](http://arxiv.org/abs/2401.03301)

    本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。

    

    我们试图理解什么促进了对于序贝叶斯决策的历史数据集进行样本高效学习，这个问题通常被称为离线强化学习（RL）。此外，我们对于在利用（值）函数逼近的同时享受样本效率的算法感兴趣。在本文中，我们通过提出一个包括离线RL中覆盖度量的先前概念的数据多样性概念来解决这些基本问题，并且利用这个概念将基于版本空间（VS）、正则化优化（RO）和后验采样（PS）的三个不同类别的离线RL算法进行统一。我们在标准假设下证明，基于VS、基于RO和基于PS的算法达到了\emph{可比}的样本效率，这恢复了在有限和线性模型类别下的最优性的标准假设的边界。这个结果令人惊讶，因为之前的研究表明这些算法不具有有利性的样本效率。

    We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
    
[^8]: 执行时间预算分配在混合关键系统中

    Execution time budget assignment for mixed criticality systems. (arXiv:2401.02431v2 [cs.PF] CROSS LISTED)

    [http://arxiv.org/abs/2401.02431](http://arxiv.org/abs/2401.02431)

    本文提出了一种启发式方法，根据执行时间变异性为每个低关键实时任务分配执行时间预算，降低了超出预算的概率。

    

    本文提出使用统计离散参数来量化程序的执行时间变异性，并展示了如何在混合关键实时系统中利用执行时间变异性。我们提出了一种启发式方法，根据执行时间变异性为每个低关键实时任务分配执行时间预算。通过实验证明，与不考虑执行时间变异性参数的算法相比，所提出的启发式方法降低了超出分配预算的概率。

    In this paper we propose to quantify execution time variability of programs using statistical dispersion parameters. We show how the execution time variability can be exploited in mixed criticality real-time systems. We propose a heuristic to compute the execution time budget to be allocated to each low criticality real-time task according to its execution time variability. We show using experiments and simulations that the proposed heuristic reduces the probability of exceeding the allocated budget compared to algorithms which do not take into account the execution time variability parameter.
    
[^9]: 用基于模拟推断的孤立脉冲星种群合成

    Isolated pulsar population synthesis with simulation-based inference. (arXiv:2312.14848v1 [astro-ph.HE] CROSS LISTED)

    [http://arxiv.org/abs/2312.14848](http://arxiv.org/abs/2312.14848)

    本论文使用模拟推断方法结合脉冲星种群合成，来限制孤立银河射电脉冲星的磁旋转特性。

    

    我们将脉冲星种群合成与基于模拟推断相结合，以限制孤立银河射电脉冲星的磁旋转特性。我们首先构建了一个灵活的框架来模拟中子星的诞生特性和演化，重点是它们的动力学、旋转和磁性特征。特别是，我们从对数正态分布中采样初始磁场强度B和自转周期P，并用幂律来捕捉后期磁场的衰减。每个对数正态分布由均值μlogB，μlogP和标准差σlogB，σlogP描述，而幂律由指数a_late描述，共计五个自由参数。然后我们模拟了星体的射电发射和观测偏差，以模拟三个射电调查中的探测，并通过改变输入参数产生了一个大型的合成P-Ṗ图数据库。接着我们采用基于模拟推断的方法进行推断

    We combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated Galactic radio pulsars. We first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. In particular, we sample initial magnetic-field strengths, $B$, and spin periods, $P$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. Each log-normal is described by a mean, $\mu_{\log B}, \mu_{\log P}$, and standard deviation, $\sigma_{\log B}, \sigma_{\log P}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. We subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $P$-$\dot{P}$ diagrams by varying our input parameters. We then follow a simulation-based inference approach 
    
[^10]: 随机逼近的收敛速度：带有无界方差的有偏噪声和应用

    Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.02828](http://arxiv.org/abs/2312.02828)

    本论文研究了带有无界方差的有偏噪声对随机逼近算法的收敛速度的影响，并介绍了该算法在各个领域的应用。

    

    1951年罗宾斯和莫洛引入的随机逼近（SA）算法已经成为解方程$\mathbf{f}({\boldsymbol{\theta}}) = \mathbf{0}$的标准方法，当只有$\mathbf{f}(\cdot)$的带噪声测量可用时。如果对于某个函数$J(\cdot)$，$\mathbf{f}({\boldsymbol{\theta}}) = \nabla J({\boldsymbol{\theta}})$，那么SA也可以用来寻找$J(\cdot)$的一个稳定点。在每个时间$t$，当前的猜测${\boldsymbol{\theta}}_t$通过形式为$\mathbf{f}({\boldsymbol{\theta}}_t) + {\boldsymbol{\xi}}_{t+1}$的带噪声测量更新为${\boldsymbol{\theta}}_{t+1}$。在许多文献中，假设误差项${\boldsymbol{\xi}}_{t+1}$的条件均值为零，和/或者它的条件方差随$t$（而不是${\boldsymbol{\theta}}_t$）被限制。多年来，SA已经应用于各种领域，本文重点研究其中一个领域。

    The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) = \nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. At each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. In much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). Over the years, SA has been applied to a variety of areas, out of which the focus in this paper i
    
[^11]: 在基础模型时代的风险评估和统计显著性

    Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])

    [http://arxiv.org/abs/2310.07132](http://arxiv.org/abs/2310.07132)

    本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。

    

    我们提出了一个分布框架，用于评估具有统计显著性的基础模型的社会技术风险。我们的方法依赖于一种基于实际随机变量的一阶和二阶随机优势的新的统计相对测试。我们表明，这个测试中的二阶统计与在计量经济学和数学金融中常用的平均风险模型相联系，用于在选择方案时平衡风险和效用。利用这个框架，我们正式开发了一种基于风险意识的基础模型选择方法，给定由指定度量量化的防护栏。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，作为聚合一系列度量的手段，并根据这些组合的随机优势进行模型选择。我们的测试的统计显著性在理论上由通过中心极限的渐近分析支持。

    We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
    
[^12]: 注意力对熵通信的影响

    Attention to Entropic Communication. (arXiv:2307.11423v1 [cs.IT])

    [http://arxiv.org/abs/2307.11423](http://arxiv.org/abs/2307.11423)

    该论文研究了在通信理论中结合注意力和相对熵的概念。研究发现，在通信中使用注意力导向的加权相对熵是不适当的，而适当的注意力通信可通过发送者仅需要了解接收者的效用函数来实现最佳通知。

    

    注意力的概念是指在人工智能中强调特定数据重要性的数值权重，在通信理论中相对熵（RE，也称为库尔巴克-勒布勒散度）发挥着核心作用。在这里，我们结合了这些概念，即注意力和RE。RE引导带宽有限通信中的最佳编码以及通过最大熵原理（MEP）进行最佳消息解码。在编码场景中，RE可以从四个要求中推导出来，即分析性、局部性、适当性和校准性。而用于通信中注意力导向的加权RE实际上是不适当的。为了看到适当的注意力通信是如何出现的，我们分析了一个场景，即消息发送者希望确保接收者能够执行知情的操作。如果接收者使用MEP解码消息，则发送者只需要知道接收者的效用函数来进行最佳通知，不需要知道接收者的策略。

    The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receive
    
[^13]: 在线 Laplace 模型选择的再探讨

    Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])

    [http://arxiv.org/abs/2307.06093](http://arxiv.org/abs/2307.06093)

    本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。

    

    Laplace 近似为神经网络提供了一个封闭形式的模型选择目标。在贝叶斯深度学习领域，将神经网络参数与超参数（如权重衰减强度）一起进行优化的在线变体方法再次引起了人们的关注。然而，这些方法违反了 Laplace 方法的一个关键假设，即近似是围绕损失的模态进行的，这就对它们的合理性提出了质疑。本研究重新推导了在线 Laplace 方法，展示了它们针对 Laplace 证据的一个修正模态的变分上界，从而避免了对平稳性的假设。在线 Laplace 方法及其修正模态的对应点满足两个条件：1. 神经网络参数是一个最大后验概率，满足 Laplace 方法的假设；2. 超参数最大化 Laplace 证据，从而促使在线方法的应用。我们通过使用全批量梯度的在线算法演示了这些最优点在实践中的近似程度。

    The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien
    
[^14]: 使用神经网络分类器学习似然比

    Learning Likelihood Ratios with Neural Network Classifiers. (arXiv:2305.10500v1 [hep-ph])

    [http://arxiv.org/abs/2305.10500](http://arxiv.org/abs/2305.10500)

    该研究介绍了一种使用神经网络分类器参数化计算似然比的技巧，并详细比较了不同设置的性能。这对于许多数据或基于模拟的科学应用非常有用。

    

    在科学中，似然比是统计推断的关键性量，它使得假设检验、置信区间构建、分布加权等成为可能。然而，许多现代科学应用使用基于数据或基于模拟的模型，而计算似然比可能非常困难甚至不可能。通过应用所谓的“似然比技巧”，可以使用聪明的神经网络分类器参数化来计算似然比的近似值。可以定义许多不同的神经网络设置来满足此过程，每个设置在使用有限训练数据时近似似然比的性能各异。我们提出了一系列经验研究，详细介绍了几种常见损失函数和分类器输出参数化在近似两个一元和多元高斯分布的似然比方面的表现以及模拟高能物理的信号和背景事件。

    The likelihood ratio is a crucial quantity for statistical inference in science that enables hypothesis testing, construction of confidence intervals, reweighting of distributions, and more. Many modern scientific applications, however, make use of data- or simulation-driven models for which computing the likelihood ratio can be very difficult or even impossible. By applying the so-called ``likelihood ratio trick,'' approximations of the likelihood ratio may be computed using clever parametrizations of neural network-based classifiers. A number of different neural network setups can be defined to satisfy this procedure, each with varying performance in approximating the likelihood ratio when using finite training data. We present a series of empirical studies detailing the performance of several common loss functionals and parametrizations of the classifier output in approximating the likelihood ratio of two univariate and multivariate Gaussian distributions as well as simulated high-e
    
[^15]: 可控的信任权衡下的合成数据审计与生成

    Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])

    [http://arxiv.org/abs/2304.10819](http://arxiv.org/abs/2304.10819)

    本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。

    

    现实中收集的数据往往存在偏差、不平衡，并且有泄露敏感和隐私信息的风险。这一事实引发了创建合成数据集的想法，以减轻真实数据中固有的风险、偏见、伤害和隐私问题。这个概念依赖于生成AI模型，以产生不偏执、保护隐私的合成数据，同时忠实于真实数据。在这种新范式中，我们如何知道这种方法是否兑现了其承诺？我们提出了一个审计框架，提供了对合成数据集和基于它们训练的AI模型的全面评估，围绕偏见和歧视的预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。我们通过审计多个生成模型在不同用例中展示了我们的框架，包括教育、医疗保健、银行、人力资源，以及从表格，时间序列到自然语言的不同模态。我们的用例展示了在合成数据生成中平衡信任和效用的权衡的重要性。

    Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
    
[^16]: Shallow ReLU$^k$神经网络的逼近速率及其在非参数回归中的应用

    Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])

    [http://arxiv.org/abs/2304.01561](http://arxiv.org/abs/2304.01561)

    本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。

    

    本研究探讨与Shallow ReLU$^k$神经网络相关的变异空间的逼近容量。结果表明，在有限变异范数下，容纳了足够平滑的函数。对于较少平滑的函数，根据变异范数建立了逼近速率。运用这些结果，我们可以证明Shallow ReLU$^k$神经网络的最优逼近速率。同时阐明了这些结果如何用于推导深层神经网络和卷积神经网络(CNNs)的逼近界限。为应用研究，我们使用了三种ReLU神经网络模型：浅层神经网络，超参数神经网络和CNN进行非参数回归收敛速率研究。特别地，我们展示了浅层神经网络可以实现学习H\"older函数的最优渐进速率，这补充了深层神经网络的最近结果。

    We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
    
[^17]: 基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究

    Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09580](http://arxiv.org/abs/2302.09580)

    该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。

    

    高斯过程提供了一个灵活的非参数框架，用于近似高维空间中的函数。协方差核是高斯过程的主要引擎，包含了预测分布的相关性。对于具有时空数据集的应用，合适的核应该建模联合的时空依赖关系。可分离的时空协方差核提供了简单和计算效率较高的方案。然而，非可分离核包含了更好地捕捉观测到的相关性的时空交互作用。大多数具有显式表达式的非可分离核是基于数学考虑（可允许条件）而非基于第一原理导出的。我们提出了一种基于物理论证的混合谱方法来生成协方差核。我们使用这种方法推导了一类新型的物理动机的非可分离协方差核，它们的根源来自随机线性...

    Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
    
[^18]: 通过元学习Transformer实现通用上下文学习

    General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04458](http://arxiv.org/abs/2212.04458)

    本文展示了Transformer和其他黑盒模型可以通过元学习训练成为通用的上下文学习器，该模型可以在各种问题上进行测试集预测，无需定义推理模型、训练损失或优化算法。

    

    现代机器学习要求系统设计者指定学习流程的方方面面，例如损失函数、架构和优化器。而元学习，或者学会学习，目标是学习这些方面，并承诺以更少的手动工作开启更大的能力。元学习的一个特别雄心勃勃的目标是从头开始训练通用的上下文学习算法，仅使用带有最小归纳偏见的黑盒模型。这样的模型接收训练数据，并在各种问题上产生测试集预测，而无需定义推理模型、训练损失或优化算法。在本文中，我们展示了Transformer和其他黑盒模型可以被元训练成为通用的上下文学习器。我们通过模型大小、任务数量和元优化引起的算法之间的转换进行了表征，这些算法可以实现泛化，也可以实现记忆，还有一些算法根本无法进行元训练。

    Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-opti
    
[^19]: 无分布假设的节点分类预测集

    Distribution Free Prediction Sets for Node Classification. (arXiv:2211.14555v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14555](http://arxiv.org/abs/2211.14555)

    本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类，证明了提供了比简单的符合预测应用更加紧致和良好校准的预测集。

    

    图神经网络通常可以在许多重要的真实数据集上达到高精度分类的效果，但其无法提供严格的预测不确定性定义。由于图结构引起的数据点依赖性，量化GNN模型的置信度很困难。本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类。我们对现有的换位分类方法进行了改进，通过适当加权符合分数来反映网络结构。通过在常用标准基准数据集上使用流行的GNN模型进行实验，我们证明了我们的方法提供了比简单的符合预测应用更加紧致和良好校准的预测集。

    Graph Neural Networks (GNNs) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. Quantifying the confidence of GNN models is difficult due to the dependence between datapoints induced by the graph structure.  We leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. We do this by taking an existing approach for conformal classification that relies on \textit{exchangeable} data and modifying it by appropriately weighting the conformal scores to reflect the network structure. We show through experiments on standard benchmark datasets using popular GNN models that our approach provides tighter and better calibrated prediction sets than a naive application of conformal prediction.
    

