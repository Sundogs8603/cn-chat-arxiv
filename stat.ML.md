# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Computational Complexity of Learning Gaussian Single-Index Models](https://arxiv.org/abs/2403.05529) | 该论文研究了学习高斯单指数模型的计算复杂性，在高维回归问题中展示了计算有效算法所需的样本复杂度，并表明这种复杂度是充分的。 |
| [^2] | [Poly-View Contrastive Learning](https://arxiv.org/abs/2403.05490) | 本研究提出了多视图对比学习方法，通过新的表示学习目标优化匹配多个相关视图，在ImageNet1k数据集上的实验结果显示，相比于SimCLR模型，多视图对比模型在更少的训练轮数和更小的批大小下表现更优。 |
| [^3] | [An Improved Algorithm for Learning Drifting Discrete Distributions](https://arxiv.org/abs/2403.05446) | 提出了一种可以在分布漂移情况下学习离散分布的自适应算法，能够解决过去样本数量选择的权衡问题，并利用数据相关的界来表征统计误差。 |
| [^4] | [An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization](https://arxiv.org/abs/2403.05425) | 提出一种两步优化框架，通过最小平均方差估计方法识别目标函数的有效降维子空间，然后在该子空间内构建高斯过程模型并进行优化，能够平衡探索与利用的权衡，提高在高维环境中的收敛速度 |
| [^5] | [Variational Inference of Parameters in Opinion Dynamics Models](https://arxiv.org/abs/2403.05358) | 通过将估计问题转化为可直接解决的优化任务，本研究提出了一种使用变分推断来估计意见动态ABM参数的方法。 |
| [^6] | [Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks](https://arxiv.org/abs/2403.05293) | 研究了动量对梯度下降优化轨迹的影响，通过连续时间方法找到固有量 $\lambda$，对于恢复稀疏解具有帮助。 |
| [^7] | [An Efficient Quasi-Random Sampling for Copulas](https://arxiv.org/abs/2403.05281) | 使用生成对抗网络（GANs）为任何Copula生成准随机样本的高效方法 |
| [^8] | [Continual Learning and Catastrophic Forgetting](https://arxiv.org/abs/2403.05175) | 人工神经网络在持续学习过程中容易出现灾难性遗忘，这一问题是深度学习中持续学习领域的关键挑战。 |
| [^9] | [Greedy feature selection: Classifier-dependent feature selection via greedy methods](https://arxiv.org/abs/2403.05138) | 提出了一种新的基于分类器的特征选择方法，通过贪婪方式在每一步识别最重要的特征，从而在理论和实际应用中得出有效性。 |
| [^10] | [Follow-the-Perturbed-Leader with Fr\'{e}chet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds](https://arxiv.org/abs/2403.05134) | 研究指出在对抗性和随机赌博机中，基于随机扰动的Follow-the-Perturbed-Leader策略具有弗歇泰尔分布尾部最优性，实现了最佳选择的能力。 |
| [^11] | [Provable Multi-Party Reinforcement Learning with Diverse Human Feedback](https://arxiv.org/abs/2403.05006) | 该研究首次提出了多方协作强化学习的理论研究，通过整合多个个体不同偏好的元学习与不同社会福利函数的采用，克服了传统RLHF方法无法捕捉并平衡多个个体偏好的局限性。 |
| [^12] | [Stacking as Accelerated Gradient Descent](https://arxiv.org/abs/2403.04978) | Stacking提出了一种理论解释，即实现了Nesterov的加速梯度下降形式，并证明对于某些深度线性残差网络，提供了加速训练。 |
| [^13] | [Deep Backward and Galerkin Methods for the Finite State Master Equation](https://arxiv.org/abs/2403.04975) | 该论文提出了两种神经网络方法来解决有限状态均场博弈的主方程，通过数值实验验证了这两种方法的有效性。 |
| [^14] | [Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy](https://arxiv.org/abs/2403.04867) | 该论文提出了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。 |
| [^15] | [Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge](https://arxiv.org/abs/2403.04805) | 使用领域特定结构信息来引导修剪的方法 DASH 在学习动态基因调控网络模型时表现出色，提供了更有意义的生物学见解 |
| [^16] | [Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration](https://arxiv.org/abs/2403.04629) | 提出了ShapleyBO框架，用Shapley值解释贝叶斯优化提议，量化每个参数对于优化过程的贡献，并能够区分不同类型的不确定性探索贡献。 |
| [^17] | [What makes an image realistic?](https://arxiv.org/abs/2403.04493) | 论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。 |
| [^18] | [Dendrogram of mixing measures: Learning latent hierarchy and model selection for finite mixture models](https://arxiv.org/abs/2403.01684) | 通过混合模型的潜在混合度量的树状图，我们提出一种新的方式来总结和选择混合模型，能够在模型参数仅具有较弱可识别性时一致地选择真实混合组分的数量，并从树中获得参数估计的逐点最优收敛速率。 |
| [^19] | [Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach](https://arxiv.org/abs/2402.17987) | 提出了一种完全贝叶斯雷达自动目标识别的框架，采用最优贝叶斯融合来有效地汇总多个雷达的分类概率向量，以改进无人机雷达截面识别效果。 |
| [^20] | [A Variational Autoencoder for Neural Temporal Point Processes with Dynamic Latent Graphs](https://arxiv.org/abs/2312.16083) | 提出了一种用于捕捉混合时间动态的新颖变分自编码器模型，使用顺序潜变量模型在子间隔内学习事件间的依赖图，提高了在预测事件间隔时间方面的准确性。 |
| [^21] | [Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations](https://arxiv.org/abs/2311.02490) | 窗口式安德森加速在对称不动点迭代中具有改进的根线性收敛率，模拟和实验结果证实其超越标准不动点方法。 |
| [^22] | [Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model](https://arxiv.org/abs/2210.03859) | SRLDA方法在波纹模型假设下具有线性分类全局最优解，并在实验中表现出比RLDA和ILDA更好的性能。 |
| [^23] | [Settling the Sample Complexity of Model-Based Offline Reinforcement Learning](https://arxiv.org/abs/2204.05275) | 该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。 |
| [^24] | [Testing Stationarity and Change Point Detection in Reinforcement Learning](https://arxiv.org/abs/2203.01707) | 开发了一种能够在非平稳环境中进行策略优化的强化学习方法，通过测试最优Q函数的非平稳性并开发序贯变点检测方法来实现。 |
| [^25] | [Intriguing Properties of Input-dependent Randomized Smoothing](https://arxiv.org/abs/2110.05365) | 输入相关平滑方法虽然被用来获取可靠鲁棒分类器，但缺乏形式保证，其证书并不合理，因受到维度诅咒影响；提出了一个理论和实践框架，使得即使在维度诅咒存在的情况下，也能在严格的限制条件下使用输入相关平滑。 |
| [^26] | [Group selection and shrinkage: Structured sparsity for semiparametric additive models](https://arxiv.org/abs/2105.12081) | 本文引入了结构化稀疏估计器，结合组子集选择与收缩，适用于稀疏半参数加性建模，提出了相应的优化框架和有限样本误差边界。 |
| [^27] | [A unified framework for hard and soft clustering with regularized optimal transport](https://arxiv.org/abs/1711.04366) | 这个方法提出了一个统一的框架，将离散数据推断有限混合模型的问题建模为带有正则化最优输运的问题，同时在聚类中融合了硬聚类和软聚类，并且实验证明当参数$\lambda>1$时可以提高推断性能，当$\lambda\to 0$时适用于分类。 |
| [^28] | [Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection.](http://arxiv.org/abs/2401.12924) | 本文对于使用图像数据集进行森林火灾检测的支持向量机（SVM）进行了性能分析，并研究了关键因素如数据预处理、特征提取和模型训练。这项研究有助于开发高效的森林火灾检测系统。 |
| [^29] | [Scalable neural network models and terascale datasets for particle-flow reconstruction.](http://arxiv.org/abs/2309.06782) | 本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。 |
| [^30] | [Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift.](http://arxiv.org/abs/2306.16406) | 本文研究了在通用的数据集转移条件下，利用半参数效率理论，高效估计目标总体风险的问题。 |
| [^31] | [High-Fidelity Image Compression with Score-based Generative Models.](http://arxiv.org/abs/2305.18231) | 本文提出了一种基于分数的生成模型的两阶段方法，该方法在图像压缩领域取得了显著的表现，实验证明该方法在一定比特率下能够提高图像的感知质量。 |
| [^32] | [Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations.](http://arxiv.org/abs/2212.14411) | 本文研究了非参数顺序检验和置信区间，在一般非参数数据生成过程下提供了类型I错误和期望拒绝时间保证，提高了其灵活性和性能。 |
| [^33] | [Dual control variate for faster black-box variational inference.](http://arxiv.org/abs/2210.07290) | 本论文提出了双控制变量方法，能够同时减少数据子抽样和蒙特卡罗抽样带来的梯度估计方差，提高黑盒变分推断的准确性和效率。 |

# 详细

[^1]: 学习高斯单指数模型的计算复杂性

    The Computational Complexity of Learning Gaussian Single-Index Models

    [https://arxiv.org/abs/2403.05529](https://arxiv.org/abs/2403.05529)

    该论文研究了学习高斯单指数模型的计算复杂性，在高维回归问题中展示了计算有效算法所需的样本复杂度，并表明这种复杂度是充分的。

    

    单指数模型是具有植入结构的高维回归问题，其中标签依赖于通过通用、非线性和潜在非确定性转换的输入的未知一维投影。因此，它们涵盖了广泛的统计推断任务类别，并提供了一个丰富的模板，用于研究高维情况下的统计和计算折衷。尽管恢复隐藏方向的信息论样本复杂度与维度$d$是线性的，但我们表明，在统计查询（SQ）框架和低阶多项式（LDP）框架内，计算高效的算法必须需要$\Omega(d^{k^\star/2})$个样本，其中$k^\star$是我们明确表征的与模型相关的“生成”指数。此外，我们通过建立使用部分迹的匹配上界来证明这个样本复杂度也是充分的。

    arXiv:2403.05529v1 Announce Type: new  Abstract: Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace
    
[^2]: 多视图对比学习

    Poly-View Contrastive Learning

    [https://arxiv.org/abs/2403.05490](https://arxiv.org/abs/2403.05490)

    本研究提出了多视图对比学习方法，通过新的表示学习目标优化匹配多个相关视图，在ImageNet1k数据集上的实验结果显示，相比于SimCLR模型，多视图对比模型在更少的训练轮数和更小的批大小下表现更优。

    

    对比学习通常会匹配一组不相关的负视图中相关视图的配对。视图可以是生成的（例如通过增强）或被观察到的。本文研究了当存在多于两个相关视图时的匹配，我们称之为多视图任务，并利用信息最大化和充分统计导出了新的表示学习目标。我们表明，在计算资源无限时，应最大化相关视图的数量；而在固定计算预算的情况下，减少独特样本的数量同时增加这些样本的视图数量是有益的。特别地，以256的批大小训练128轮的多视图对比模型在ImageNet1k上表现优于在批大小为4096且进行1024轮训练的SimCLR模型，挑战了对比模型需要大批大小和多次训练轮数的信念。

    arXiv:2403.05490v1 Announce Type: cross  Abstract: Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.
    
[^3]: 一种用于学习漂移离散分布的改进算法

    An Improved Algorithm for Learning Drifting Discrete Distributions

    [https://arxiv.org/abs/2403.05446](https://arxiv.org/abs/2403.05446)

    提出了一种可以在分布漂移情况下学习离散分布的自适应算法，能够解决过去样本数量选择的权衡问题，并利用数据相关的界来表征统计误差。

    

    我们提出了一种新的自适应算法，用于在分布漂移情况下学习离散分布。在这种情况下，我们观察来自一个随时间变化的离散分布的独立样本序列，目标是估计当前分布。由于我们每个时间步只能访问一个样本，一个良好的估计需要谨慎选择要使用的过去样本的数量。为了使用更多样本，我们必须诉诸更早的样本，这会因为分布变化引入的偏差而产生漂移误差。另一方面，如果我们使用较少的过去样本，估计会有较高的方差，导致较大的统计误差。我们提出了一种新颖的自适应算法，可以解决这种权衡，而无需任何漂移的先验知识。与以前的自适应结果不同，我们的算法使用基于数据的界来表征统计误差。

    arXiv:2403.05446v1 Announce Type: new  Abstract: We present a new adaptive algorithm for learning discrete distributions under distribution drift. In this setting, we observe a sequence of independent samples from a discrete distribution that is changing over time, and the goal is to estimate the current distribution. Since we have access to only a single sample for each time step, a good estimation requires a careful choice of the number of past samples to use. To use more samples, we must resort to samples further in the past, and we incur a drift error due to the bias introduced by the change in distribution. On the other hand, if we use a small number of past samples, we incur a large statistical error as the estimation has a high variance. We present a novel adaptive algorithm that can solve this trade-off without any prior knowledge of the drift. Unlike previous adaptive results, our algorithm characterizes the statistical error using data-dependent bounds. This technicality enab
    
[^4]: 一种用于高维贝叶斯优化的自适应降维估计方法

    An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization

    [https://arxiv.org/abs/2403.05425](https://arxiv.org/abs/2403.05425)

    提出一种两步优化框架，通过最小平均方差估计方法识别目标函数的有效降维子空间，然后在该子空间内构建高斯过程模型并进行优化，能够平衡探索与利用的权衡，提高在高维环境中的收敛速度

    

    贝叶斯优化在低到中维欧几里得空间的各种应用中展现出了令人印象深刻的结果。然而，将贝叶斯优化扩展到高维设置仍然是一个重大挑战。我们通过提出一个两步优化框架来解决这一挑战。首先，我们使用最小平均方差估计（MAVE）方法识别目标函数的有效降维（EDR）子空间。随后，我们在该EDR子空间内构建一个高斯过程模型，并使用预期改进准则进行优化。我们的算法能够灵活地同时或顺序地执行这些步骤。在顺序方法中，我们通过在子空间估计和函数优化之间分配采样预算来仔细平衡探索利用的权衡，我们的算法在高维环境中的收敛速度已经得到改善。

    arXiv:2403.05425v1 Announce Type: new  Abstract: Bayesian optimization (BO) has shown impressive results in a variety of applications within low-to-moderate dimensional Euclidean spaces. However, extending BO to high-dimensional settings remains a significant challenge. We address this challenge by proposing a two-step optimization framework. Initially, we identify the effective dimension reduction (EDR) subspace for the objective function using the minimum average variance estimation (MAVE) method. Subsequently, we construct a Gaussian process model within this EDR subspace and optimize it using the expected improvement criterion. Our algorithm offers the flexibility to operate these steps either concurrently or in sequence. In the sequential approach, we meticulously balance the exploration-exploitation trade-off by distributing the sampling budget between subspace estimation and function optimization, and the convergence rate of our algorithm in high-dimensional contexts has been es
    
[^5]: 意见动态模型中参数的变分推断

    Variational Inference of Parameters in Opinion Dynamics Models

    [https://arxiv.org/abs/2403.05358](https://arxiv.org/abs/2403.05358)

    通过将估计问题转化为可直接解决的优化任务，本研究提出了一种使用变分推断来估计意见动态ABM参数的方法。

    

    尽管基于代理人的模型（ABMs）在研究社会现象中被频繁使用，但参数估计仍然是一个挑战，通常依赖于昂贵的基于模拟的启发式方法。本研究利用变分推断来估计意见动态ABM的参数，通过将估计问题转化为可直接解决的优化任务。

    arXiv:2403.05358v1 Announce Type: cross  Abstract: Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.   Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.   We validate our method on a
    
[^6]: 利用连续时间理解对角线性网络训练中的动量

    Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks

    [https://arxiv.org/abs/2403.05293](https://arxiv.org/abs/2403.05293)

    研究了动量对梯度下降优化轨迹的影响，通过连续时间方法找到固有量 $\lambda$，对于恢复稀疏解具有帮助。

    

    在这项工作中，我们研究了动量对梯度下降优化轨迹的影响。我们利用连续时间方法分析带有步长 $\gamma$ 和动量参数 $\beta$ 的动量梯度下降，从而找到一个固有量 $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$，这一量唯一定义了优化路径并提供了一个简单的加速规则。在超参数化回归设置中训练 $2$ 层对角线性网络时，通过一个隐式正则化问题来表征恢复的解。然后证明小的 $\lambda$ 值有助于恢复稀疏解。最后，我们给出了随机动量梯度下降的类似但较弱结果。我们提供支持我们论断的数值实验。

    arXiv:2403.05293v1 Announce Type: new  Abstract: In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\gamma$ and momentum parameter $\beta$ that allows us to identify an intrinsic quantity $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.
    
[^7]: 一种高效的用于Copulas的准随机抽样方法

    An Efficient Quasi-Random Sampling for Copulas

    [https://arxiv.org/abs/2403.05281](https://arxiv.org/abs/2403.05281)

    使用生成对抗网络（GANs）为任何Copula生成准随机样本的高效方法

    

    这篇论文研究了一种在蒙特卡罗计算中用于Copulas的高效准随机抽样方法。传统方法如条件分布法（CDM）在处理高维或隐式Copulas时存在局限性，指的是那些无法通过现有参数Copulas准确表示的Copulas。相反，本文提出使用生成模型，例如生成对抗网络（GANs），为任何Copula生成准随机样本。GANs是一种用于学习复杂数据分布的隐式生成模型，有助于简化抽样过程。在我们的研究中，GANs被用来学习从均匀分布到Copulas的映射。一旦学习了这种映射，从Copula获取准随机样本只需输入来自均匀分布的准随机样本。这种方法为任何Copula提供了更灵活的方式。此外，我们提供了t

    arXiv:2403.05281v1 Announce Type: new  Abstract: This paper examines an efficient method for quasi-random sampling of copulas in Monte Carlo computations. Traditional methods, like conditional distribution methods (CDM), have limitations when dealing with high-dimensional or implicit copulas, which refer to those that cannot be accurately represented by existing parametric copulas. Instead, this paper proposes the use of generative models, such as Generative Adversarial Networks (GANs), to generate quasi-random samples for any copula. GANs are a type of implicit generative models used to learn the distribution of complex data, thus facilitating easy sampling. In our study, GANs are employed to learn the mapping from a uniform distribution to copulas. Once this mapping is learned, obtaining quasi-random samples from the copula only requires inputting quasi-random samples from the uniform distribution. This approach offers a more flexible method for any copula. Additionally, we provide t
    
[^8]: 持续学习与灾难性遗忘

    Continual Learning and Catastrophic Forgetting

    [https://arxiv.org/abs/2403.05175](https://arxiv.org/abs/2403.05175)

    人工神经网络在持续学习过程中容易出现灾难性遗忘，这一问题是深度学习中持续学习领域的关键挑战。

    

    本书章节探讨了持续学习的动态过程，即从非静态数据流中逐步学习的过程。尽管持续学习是人脑的一种自然技能，但对于人工神经网络来说却是非常具有挑战性的。一个重要原因是在学习新知识时，这些网络往往会迅速而彻底地忘记以前所学的内容，这一现象被称为灾难性遗忘。在过去的十年中，持续学习已成为深度学习中一个被广泛研究的课题。本书章节回顾了这一领域产生的见解。

    arXiv:2403.05175v1 Announce Type: cross  Abstract: This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data. Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, continual learning has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.
    
[^9]: 基于贪婪方法的分类器相关特征选择: 贪婪特征选择方法介绍

    Greedy feature selection: Classifier-dependent feature selection via greedy methods

    [https://arxiv.org/abs/2403.05138](https://arxiv.org/abs/2403.05138)

    提出了一种新的基于分类器的特征选择方法，通过贪婪方式在每一步识别最重要的特征，从而在理论和实际应用中得出有效性。

    

    这项研究旨在介绍一种新的特征排名方法，即贪婪特征选择，用于分类任务。在统计学习中，通常通过与应用于利用减少数量特征进行预测的分类器无关的方法来实现特征选择。相反，贪婪特征选择根据选定的分类器在每一步识别最重要的特征。 在本文中，从理论上探讨了这种方案的优点，如Vapnik-Chervonenkis（VC）维度或核对齐等模型能力指标，并通过考虑将其应用于预测太阳活动的地质效应表现问题的方法进行了数值测试。

    arXiv:2403.05138v1 Announce Type: cross  Abstract: The purpose of this study is to introduce a new approach to feature ranking for classification tasks, called in what follows greedy feature selection. In statistical learning, feature selection is usually realized by means of methods that are independent of the classifier applied to perform the prediction using that reduced number of features. Instead, greedy feature selection identifies the most important feature at each step and according to the selected classifier. In the paper, the benefits of such scheme are investigated theoretically in terms of model capacity indicators, such as the Vapnik-Chervonenkis (VC) dimension or the kernel alignment, and tested numerically by considering its application to the problem of predicting geo-effective manifestations of the active Sun.
    
[^10]: 带有弗歇泰尔分布的扰动领导者追踪：在对抗性赌博机和最佳选择中的最优性

    Follow-the-Perturbed-Leader with Fr\'{e}chet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds

    [https://arxiv.org/abs/2403.05134](https://arxiv.org/abs/2403.05134)

    研究指出在对抗性和随机赌博机中，基于随机扰动的Follow-the-Perturbed-Leader策略具有弗歇泰尔分布尾部最优性，实现了最佳选择的能力。

    

    本文研究了在对抗性和随机$K$臂老虎机中Follow-the-Perturbed-Leader（FTPL）策略的最优性。尽管Follow-the-Regularized-Leader（FTRL）框架在各种正则化选择下被广泛使用，但依赖于随机扰动的FTPL框架却鲜有关注，尽管其固有的简单性。在对抗性赌博机中，FTPL若扰动遵循具有弗歇泰尔尾部的分布，有人猜想可能可以实现$\mathcal{O}(\sqrt{KT})$的后悔。本文通过对形状$\alpha=2$的Fr\'{e}chet分布进行了研究，证明了FTPL确实达到了这一界限，并且在随机赌博机中具有显著的对数后悔，意味着FTPL具备了最佳选择（BOBW）的能力。然而，这一结果只在一定程度上解决了上述猜想，因为他们的分析严重依赖于Fr\'{e

    arXiv:2403.05134v1 Announce Type: cross  Abstract: This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL) policy in both adversarial and stochastic $K$-armed bandits. Despite the widespread use of the Follow-the-Regularized-Leader (FTRL) framework with various choices of regularization, the FTPL framework, which relies on random perturbations, has not received much attention, despite its inherent simplicity. In adversarial bandits, there has been conjecture that FTPL could potentially achieve $\mathcal{O}(\sqrt{KT})$ regrets if perturbations follow a distribution with a Fr\'{e}chet-type tail. Recent work by Honda et al. (2023) showed that FTPL with Fr\'{e}chet distribution with shape $\alpha=2$ indeed attains this bound and, notably logarithmic regret in stochastic bandits, meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result only partly resolves the above conjecture because their analysis heavily relies on the specific form of the Fr\'{e
    
[^11]: 具有多元人类反馈的可证明多方协作强化学习

    Provable Multi-Party Reinforcement Learning with Diverse Human Feedback

    [https://arxiv.org/abs/2403.05006](https://arxiv.org/abs/2403.05006)

    该研究首次提出了多方协作强化学习的理论研究，通过整合多个个体不同偏好的元学习与不同社会福利函数的采用，克服了传统RLHF方法无法捕捉并平衡多个个体偏好的局限性。

    

    用人类反馈进行强化学习（RLHF）是一种新兴范式，旨在将模型与人类偏好进行匹配。我们的工作探索了明确建模多个个体不同偏好的多方RLHF的理论研究。我们展示了传统RLHF方法如何失败，因为学习单一奖励函数无法捕捉和平衡多个个体的偏好。为了克服这些局限性，我们结合元学习来学习多个偏好，并采用不同的社会福利函数来整合多方的偏好。我们关注离线学习设置，并为优化不同社会福利函数（如Nash、Utilitarian和Leximin福利）建立样本复杂度界限，同时提供效率和公平性保证。

    arXiv:2403.05006v1 Announce Type: cross  Abstract: Reinforcement learning with human feedback (RLHF) is an emerging paradigm to align models with human preferences. Typically, RLHF aggregates preferences from multiple individuals who have diverse viewpoints that may conflict with each other. Our work \textit{initiates} the theoretical study of multi-party RLHF that explicitly models the diverse preferences of multiple individuals. We show how traditional RLHF approaches can fail since learning a single reward function cannot capture and balance the preferences of multiple individuals. To overcome such limitations, we incorporate meta-learning to learn multiple preferences and adopt different social welfare functions to aggregate the preferences across multiple parties. We focus on the offline learning setting and establish sample complexity bounds, along with efficiency and fairness guarantees, for optimizing diverse social welfare functions such as Nash, Utilitarian, and Leximin welfa
    
[^12]: Stacking作为加速梯度下降算法

    Stacking as Accelerated Gradient Descent

    [https://arxiv.org/abs/2403.04978](https://arxiv.org/abs/2403.04978)

    Stacking提出了一种理论解释，即实现了Nesterov的加速梯度下降形式，并证明对于某些深度线性残差网络，提供了加速训练。

    

    Stacking是一种启发式技术，通过逐渐增加层数并通过从旧层复制参数来初始化新层，用于训练深度残差网络，已经被证明在提高深度神经网络训练效率方面非常成功。本文提出了对于Stacking有效性的理论解释：即，Stacking实现了Nesterov的加速梯度下降的一种形式。该理论还涵盖了诸如提升方法中构建的加法集成等更简单的模型，并为每一轮提升过程中初始化新分类器的类似广泛使用的实用启发式提供了解释。我们还证明了对于某些深度线性残差网络，通过对Nesterov的加速梯度方法的一个新的潜能函数分析，Stacking确实提供了加速训练，从而允许更新中的误差。我们进行了概念验证实验来验证我们的理论。

    arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our
    
[^13]: 深度反向和Galerkin方法用于有限状态主方程

    Deep Backward and Galerkin Methods for the Finite State Master Equation

    [https://arxiv.org/abs/2403.04975](https://arxiv.org/abs/2403.04975)

    该论文提出了两种神经网络方法来解决有限状态均场博弈的主方程，通过数值实验验证了这两种方法的有效性。

    

    本论文提出并分析了两种神经网络方法，用于解决有限状态均场博弈的主方程。解决MFGs为具有有限但大量代理人群的随机、微分博弈提供近似纳什均衡。主方程是一个偏微分方程（PDE），其解表征任何可能的初始分布下的MFG均衡。我们提出的第一种方法依赖于在时间分量上的反向归纳，而第二种方法直接解决了PDE，而无需离散化时间。对于两种方法，我们证明了两种类型的结果：存在神经网络，可以使算法的损失函数任意小，并且反之，如果损失很小，则神经网络是主方程解的良好近似。我们通过对文献中的基准问题进行了维度为15的数值实验，并做出对比。

    arXiv:2403.04975v1 Announce Type: cross  Abstract: This paper proposes and analyzes two neural network methods to solve the master equation for finite-state mean field games (MFGs). Solving MFGs provides approximate Nash equilibria for stochastic, differential games with finite but large populations of agents. The master equation is a partial differential equation (PDE) whose solution characterizes MFG equilibria for any possible initial distribution. The first method we propose relies on backward induction in a time component while the second method directly tackles the PDE without discretizing time. For both approaches, we prove two types of results: there exist neural networks that make the algorithms' loss functions arbitrarily small, and conversely, if the losses are small, then the neural networks are good approximations of the master equation's solution. We conclude the paper with numerical experiments on benchmark problems from the literature up to dimension 15, and a compariso
    
[^14]: 组隐私放大和子抽样的Rényi差分隐私统一放大

    Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy

    [https://arxiv.org/abs/2403.04867](https://arxiv.org/abs/2403.04867)

    该论文提出了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。

    

    差分隐私(DP)具有多种理想属性，如对后处理的鲁棒性、组隐私和通过子抽样放大，这些属性可以相互独立推导。我们的目标是确定是否通过联合考虑这些属性中的多个可以获得更强的隐私保证。为此，我们专注于组隐私和通过子抽样放大的组合。为了提供适合机器学习算法的保证，我们在Rényi-DP框架中进行了分析，这比$(\epsilon,\delta)$-DP具有更有利的组合属性。作为这个分析的一部分，我们开发了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。我们发现，它不仅让我们改进和泛化现有的放大结果。

    arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
    
[^15]: 不是所有的票据都是平等的，而我们知道：用领域特定知识来引导修剪

    Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge

    [https://arxiv.org/abs/2403.04805](https://arxiv.org/abs/2403.04805)

    使用领域特定结构信息来引导修剪的方法 DASH 在学习动态基因调控网络模型时表现出色，提供了更有意义的生物学见解

    

    神经结构学习对于科学发现和可解释性至关重要。然而，当代侧重于计算资源效率的修剪算法在选择符合领域专业知识的有意义模型方面面临算法障碍。为了减轻这一挑战，我们提出了DASH，利用可用的领域特定结构信息来引导修剪。在学习动态基因调控网络模型的背景下，我们展示了DASH与现有一般知识相结合，提供了与生物学一致的数据特定见解。对于这一任务，我们展示了在具有地面真实信息的合成数据和两个真实世界应用中，DASH的有效性，其优于竞争方法很大，并提供了更有意义的生物学见解。我们的工作表明，领域特定的结构信息具有提高模型衍生科学洞见的潜力。

    arXiv:2403.04805v1 Announce Type: new  Abstract: Neural structure learning is of paramount importance for scientific discovery and interpretability. Yet, contemporary pruning algorithms that focus on computational resource efficiency face algorithmic barriers to select a meaningful model that aligns with domain expertise. To mitigate this challenge, we propose DASH, which guides pruning by available domain-specific structural information. In the context of learning dynamic gene regulatory network models, we show that DASH combined with existing general knowledge on interaction partners provides data-specific insights aligned with biology. For this task, we show on synthetic data with ground truth information and two real world applications the effectiveness of DASH, which outperforms competing methods by a large margin and provides more meaningful biological insights. Our work shows that domain specific structural information bears the potential to improve model-derived scientific insi
    
[^16]: 用Shapley值解释贝叶斯优化促进人工智能与人类协作

    Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration

    [https://arxiv.org/abs/2403.04629](https://arxiv.org/abs/2403.04629)

    提出了ShapleyBO框架，用Shapley值解释贝叶斯优化提议，量化每个参数对于优化过程的贡献，并能够区分不同类型的不确定性探索贡献。

    

    贝叶斯优化（BO）与高斯过程（GP）已成为解决黑匣子优化问题的不可或缺的算法。然而，BO本身也常常被认为是一个黑匣子，缺乏提供为何提议评估某些参数的理由的方法。我们通过提出ShapleyBO来解决这个问题，这是一个用博弈论Shapley值解释BO提议的框架。它量化了每个参数对BO的收获函数的贡献。利用Shapley值的线性性，我们能够进一步确定每个参数对于像置信边界这样的加法收获函数推动BO的探索和开发的强度。我们还展示了ShapleyBO能够解决探索对于勘探aleatoric和认识epistemic不确定性的贡献。

    arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
    
[^17]: 使图像真实的因素是什么？

    What makes an image realistic?

    [https://arxiv.org/abs/2403.04493](https://arxiv.org/abs/2403.04493)

    论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。

    

    在过去的十年里，我们在生成看起来真实的数据方面取得了巨大进展，无论是图像、文本、音频还是视频。在这里，我们讨论了与之密切相关的问题，即量化现实主义，即设计能够可靠地区分真实数据和不真实数据的函数。从算法信息理论的观点出发，我们讨论了为什么这个问题很具挑战性，为什么一个好的生成模型单独不能解决它，以及一个好的解决方案应该是什么样的。特别是，我们引入了通用评论者的概念，不像对抗性评论者那样需要对抗性训练。尽管通用评论者并不立即实用，但它们既可以作为引导实际实现的北极星，也可以作为一个工具。

    arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
    
[^18]: 混合度量的树状图：学习潜在层次结构和有限混合模型的模型选择

    Dendrogram of mixing measures: Learning latent hierarchy and model selection for finite mixture models

    [https://arxiv.org/abs/2403.01684](https://arxiv.org/abs/2403.01684)

    通过混合模型的潜在混合度量的树状图，我们提出一种新的方式来总结和选择混合模型，能够在模型参数仅具有较弱可识别性时一致地选择真实混合组分的数量，并从树中获得参数估计的逐点最优收敛速率。

    

    我们提出了一种新的方式，通过过度拟合的潜在混合度量的层次聚类树（树状图）来汇总和选择混合模型。我们提出的方法连接了凝聚式层次聚类和混合建模。树状图的构建源自混合度量的收敛理论，因此，我们既可以一致地选择真实混合组分的数量，也可以从树中获得参数估计的逐点最优收敛速率，即使模型参数仅具有较弱可识别性。在理论上，它阐述了在层次聚类中选择最佳群集数的选择。在实践中，与传统的混合模型汇总方式相比，树状图揭示了有关亚群层次的更多信息。我们进行了几项模拟研究来支持我们的理论。我们还通过应用程序展示了该方法。

    arXiv:2403.01684v1 Announce Type: cross  Abstract: We present a new way to summarize and select mixture models via the hierarchical clustering tree (dendrogram) of an overfitted latent mixing measure. Our proposed method bridges agglomerative hierarchical clustering and mixture modeling. The dendrogram's construction is derived from the theory of convergence of the mixing measures, and as a result, we can both consistently select the true number of mixing components and obtain the pointwise optimal convergence rate for parameter estimation from the tree, even when the model parameters are only weakly identifiable. In theory, it explicates the choice of the optimal number of clusters in hierarchical clustering. In practice, the dendrogram reveals more information on the hierarchy of subpopulations compared to traditional ways of summarizing mixture models. Several simulation studies are carried out to support our theory. We also illustrate the methodology with an application to single-c
    
[^19]: 多态雷达对空中飞行器雷达截面识别：一种贝叶斯融合方法

    Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach

    [https://arxiv.org/abs/2402.17987](https://arxiv.org/abs/2402.17987)

    提出了一种完全贝叶斯雷达自动目标识别的框架，采用最优贝叶斯融合来有效地汇总多个雷达的分类概率向量，以改进无人机雷达截面识别效果。

    

    arXiv:2402.17987v1 公告类型：跨领域 摘要：无人机的雷达自动目标识别（RATR）涉及发射电磁波并对接收到的雷达回波执行目标类型识别，对国防和航空航天应用至关重要。先前的研究突出了多态雷达配置在RATR中优于单态雷达的优势。然而，多态雷达配置中的融合方法通常以概率方式次优地组合来自各个雷达的分类向量。为了解决这个问题，我们提出了一个完全贝叶斯RATR框架，采用最优贝叶斯融合（OBF）来聚合来自多个雷达的分类概率向量。OBF基于期望0-1损失，根据多个时间步骤的历史观测更新目标无人机类型的递归贝叶斯分类（RBC）后验分布。我们使用模拟的随机行走轨迹评估了这种方法，共涉及七种机动目标。

    arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
    
[^20]: 用于动态潜在图的神经时序点过程的变分自编码器

    A Variational Autoencoder for Neural Temporal Point Processes with Dynamic Latent Graphs

    [https://arxiv.org/abs/2312.16083](https://arxiv.org/abs/2312.16083)

    提出了一种用于捕捉混合时间动态的新颖变分自编码器模型，使用顺序潜变量模型在子间隔内学习事件间的依赖图，提高了在预测事件间隔时间方面的准确性。

    

    连续观察到的事件发生往往表现出自激和互激效应，可以很好地用时序点过程模型化。除此之外，这些事件动态也可能随时间变化，具有某种周期性趋势。我们提出了一种新颖的变分自编码器来捕捉这种混合的时间动态。具体而言，输入序列的整个时间间隔被划分为一组子间隔。假设每个子间隔内的事件动态是稳定的，但在这些子间隔之间可能会发生变化。特别地，我们使用一个顺序潜变量模型来学习每个子间隔中观察维度之间的依赖图。该模型通过使用学习到的依赖图来消除过去事件的非贡献影响，预测未来的事件发生时间。通过这样做，提出的模型在预测事件间隔时间上展示出更高的准确度。

    arXiv:2312.16083v2 Announce Type: replace  Abstract: Continuously-observed event occurrences, often exhibit self- and mutually-exciting effects, which can be well modeled using temporal point processes. Beyond that, these event dynamics may also change over time, with certain periodic trends. We propose a novel variational auto-encoder to capture such a mixture of temporal dynamics. More specifically, the whole time interval of the input sequence is partitioned into a set of sub-intervals. The event dynamics are assumed to be stationary within each sub-interval, but could be changing across those sub-intervals. In particular, we use a sequential latent variable model to learn a dependency graph between the observed dimensions, for each sub-interval. The model predicts the future event times, by using the learned dependency graph to remove the noncontributing influences of past events. By doing so, the proposed model demonstrates its higher accuracy in predicting inter-event times and e
    
[^21]: 对称不动点迭代的窗口式安德森加速收敛率的改进

    Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations

    [https://arxiv.org/abs/2311.02490](https://arxiv.org/abs/2311.02490)

    窗口式安德森加速在对称不动点迭代中具有改进的根线性收敛率，模拟和实验结果证实其超越标准不动点方法。

    

    本文研究了常用的窗口式安德森加速（AA）算法用于不动点方法，$x^{(k+1)}=q(x^{(k)})$。它首次证明了当算子$q$是线性且对称时，使用先前迭代的滑动窗口的窗口式AA算法能够改进根线性收敛因子，超过不动点迭代。当$q$是非线性的，但在固定点处具有对称雅可比矩阵时，经过略微修改的AA算法被证明对比不动点迭代具有类似的根线性收敛因子改进。模拟验证了我们的观察。此外，使用不同数据模型进行的实验表明，在Tyler的M估计中，AA明显优于标准的不动点方法。

    arXiv:2311.02490v2 Announce Type: replace-cross  Abstract: This paper studies the commonly utilized windowed Anderson acceleration (AA) algorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides the first proof that when the operator $q$ is linear and symmetric the windowed AA, which uses a sliding window of prior iterates, improves the root-linear convergence factor over the fixed-point iterations. When $q$ is nonlinear, yet has a symmetric Jacobian at a fixed point, a slightly modified AA algorithm is proved to have an analogous root-linear convergence factor improvement over fixed-point iterations. Simulations verify our observations. Furthermore, experiments with different data models demonstrate AA is significantly superior to the standard fixed-point methods for Tyler's M-estimation.
    
[^22]: 具有波纹协方差模型的谱校正和正则化线性判别分析

    Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model

    [https://arxiv.org/abs/2210.03859](https://arxiv.org/abs/2210.03859)

    SRLDA方法在波纹模型假设下具有线性分类全局最优解，并在实验中表现出比RLDA和ILDA更好的性能。

    

    本文提出了一种改进的线性判别分析方法，称为谱校正和正则化LDA（SRLDA）。该方法整合了样本谱校正协方差矩阵和正则化判别分析的设计思想。在大维随机矩阵分析框架的支持下，证明了SRLDA在波纹模型假设下具有线性分类全局最优解。通过仿真数据分析，证明SRLDA分类器优于RLDA和ILDA，并接近理论分类器。对不同数据集的实验表明，SRLDA算法在分类和降维方面优于当前使用的工具。

    arXiv:2210.03859v3 Announce Type: replace-cross  Abstract: This paper proposes an improved linear discriminant analysis called spectrally-corrected and regularized LDA (SRLDA). This method integrates the design ideas of the sample spectrally-corrected covariance matrix and the regularized discriminant analysis. With the support of a large-dimensional random matrix analysis framework, it is proved that SRLDA has a linear classification global optimal solution under the spiked model assumption. According to simulation data analysis, the SRLDA classifier performs better than RLDA and ILDA and is closer to the theoretical classifier. Experiments on different data sets show that the SRLDA algorithm performs better in classification and dimensionality reduction than currently used tools.
    
[^23]: 解决模型为基础的离线强化学习的样本复杂性问题

    Settling the Sample Complexity of Model-Based Offline Reinforcement Learning

    [https://arxiv.org/abs/2204.05275](https://arxiv.org/abs/2204.05275)

    该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。

    

    本文关注离线强化学习（RL），它利用预先收集的数据进行学习，无需进一步探索。有效的离线RL应能适应分布转移和有限的数据覆盖。然而，先前的算法或分析要么受到次优样本复杂性的困扰，要么产生高昂的烧录成本以达到样本最优性，从而对样本匮乏应用中的高效离线RL构成障碍。

    arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We p
    
[^24]: 在强化学习中测试平稳性和变点检测

    Testing Stationarity and Change Point Detection in Reinforcement Learning

    [https://arxiv.org/abs/2203.01707](https://arxiv.org/abs/2203.01707)

    开发了一种能够在非平稳环境中进行策略优化的强化学习方法，通过测试最优Q函数的非平稳性并开发序贯变点检测方法来实现。

    

    我们考虑可能非平稳环境下的离线强化学习（RL）方法。许多文献中现有的RL算法依赖于需要系统转换和奖励函数随时间保持恒定的平稳性假设。然而，实践中平稳性假设是有限制的，并且在许多应用中很可能被违反，包括交通信号控制、机器人技术和移动健康。在本文中，我们开发了一种一致的程序，基于预先收集的历史数据测试最优Q函数的非平稳性，无需额外的在线数据收集。基于所提出的检验，我们进一步开发了一种顺序变点检测方法，可以自然地与现有最先进的RL方法相结合，在非平稳环境中进行策略优化。我们的方法的有效性通过理论结果、仿真研究和实践中的案例得到了展示。

    arXiv:2203.01707v3 Announce Type: replace-cross  Abstract: We consider offline reinforcement learning (RL) methods in possibly nonstationary environments. Many existing RL algorithms in the literature rely on the stationarity assumption that requires the system transition and the reward function to be constant over time. However, the stationarity assumption is restrictive in practice and is likely to be violated in a number of applications, including traffic signal control, robotics and mobile health. In this paper, we develop a consistent procedure to test the nonstationarity of the optimal Q-function based on pre-collected historical data, without additional online data collection. Based on the proposed test, we further develop a sequential change point detection method that can be naturally coupled with existing state-of-the-art RL methods for policy optimization in nonstationary environments. The usefulness of our method is illustrated by theoretical results, simulation studies, an
    
[^25]: 输入相关随机平滑的有趣特性

    Intriguing Properties of Input-dependent Randomized Smoothing

    [https://arxiv.org/abs/2110.05365](https://arxiv.org/abs/2110.05365)

    输入相关平滑方法虽然被用来获取可靠鲁棒分类器，但缺乏形式保证，其证书并不合理，因受到维度诅咒影响；提出了一个理论和实践框架，使得即使在维度诅咒存在的情况下，也能在严格的限制条件下使用输入相关平滑。

    

    随机平滑目前被认为是获得可靠鲁棒分类器的最先进方法。尽管其性能显著，但该方法存在诸如“认证准确性瀑布”、认证与准确性之间的权衡，甚至公平性问题等严重问题。为了克服这些缺陷，已经提出了输入相关的平滑方法。然而，我们证明了这些方法缺乏形式保证，因此得到的证书并不合理。我们表明，在一般情况下，输入相关平滑受到维度诅咒的影响，导致方差函数具有较低的半弹性。另一方面，我们提出了一个理论和实践框架，即使在维度诅咒存在的情况下，也能在严格的限制条件下使用输入相关平滑。我们展示了一个具体的平滑方差设计。

    arXiv:2110.05365v3 Announce Type: replace-cross  Abstract: Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as "certified accuracy waterfalls", certification vs.\ accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance 
    
[^26]: 组选择和收缩：半参数加性模型的结构稀疏性

    Group selection and shrinkage: Structured sparsity for semiparametric additive models

    [https://arxiv.org/abs/2105.12081](https://arxiv.org/abs/2105.12081)

    本文引入了结构化稀疏估计器，结合组子集选择与收缩，适用于稀疏半参数加性建模，提出了相应的优化框架和有限样本误差边界。

    

    稀疏回归和分类估计器尊重组结构，适用于各种统计学和机器学习问题，从多任务学习到稀疏加性建模再到分层选择。本文引入了结构化稀疏估计器，将组子集选择与收缩结合起来。为了适应复杂的结构，我们的估计器允许组之间存在任意重叠。我们开发了一个优化框架来拟合非凸正则化曲面，并提出了回归函数估计的有限样本误差界限。作为需要结构的应用，我们研究了稀疏半参数加性建模，这是一种允许每个预测变量效应为零、线性或非线性的过程。对于这个任务，与替代方案相比，新的估计器在合成数据上在几个度量上有所改进。最后，我们展示了它们在模型建立中的有效性。

    arXiv:2105.12081v3 Announce Type: replace-cross  Abstract: Sparse regression and classification estimators that respect group structures have application to an assortment of statistical and machine learning problems, from multitask learning to sparse additive modeling to hierarchical selection. This work introduces structured sparse estimators that combine group subset selection with shrinkage. To accommodate sophisticated structures, our estimators allow for arbitrary overlap between groups. We develop an optimization framework for fitting the nonconvex regularization surface and present finite-sample error bounds for estimation of the regression function. As an application requiring structure, we study sparse semiparametric additive modeling, a procedure that allows the effect of each predictor to be zero, linear, or nonlinear. For this task, the new estimators improve across several metrics on synthetic data compared to alternatives. Finally, we demonstrate their efficacy in modelin
    
[^27]: 一个带有正则化最优输运的硬聚类和软聚类统一框架

    A unified framework for hard and soft clustering with regularized optimal transport

    [https://arxiv.org/abs/1711.04366](https://arxiv.org/abs/1711.04366)

    这个方法提出了一个统一的框架，将离散数据推断有限混合模型的问题建模为带有正则化最优输运的问题，同时在聚类中融合了硬聚类和软聚类，并且实验证明当参数$\lambda>1$时可以提高推断性能，当$\lambda\to 0$时适用于分类。

    

    在这篇论文中，我们将从离散数据推断有限混合模型的问题阐述为一个带有参数$\lambda\geq 0$的熵正则化的最优输运问题。我们的方法统一了硬聚类和软聚类，当$\lambda=1$时，期望最大化（EM）算法被完全恢复。我们提出的聚类算法族依赖于使用交替最小化来解决非凸问题。我们研究了我们的广义$\lambda$-EM算法的收敛性质，并展示了在推断指数族有限混合模型时，最小化过程中的每一步都有一个封闭形式的解。实验突出了采用参数$\lambda>1$来提高推断性能以及$\lambda\to 0$用于分类的好处。

    arXiv:1711.04366v2 Announce Type: replace  Abstract: In this paper, we formulate the problem of inferring a Finite Mixture Model from discrete data as an optimal transport problem with entropic regularization of parameter $\lambda\geq 0$. Our method unifies hard and soft clustering, the Expectation-Maximization (EM) algorithm being exactly recovered for $\lambda=1$. The family of clustering algorithm we propose rely on the resolution of nonconvex problems using alternating minimization. We study the convergence property of our generalized $\lambda-$EM algorithms and show that each step in the minimization process has a closed form solution when inferring finite mixture models of exponential families. Experiments highlight the benefits of taking a parameter $\lambda>1$ to improve the inference performance and $\lambda\to 0$ for classification.
    
[^28]: 对于森林火灾检测中具有挑战性数据集的支持向量机（SVM）的性能分析

    Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection. (arXiv:2401.12924v1 [stat.ML])

    [http://arxiv.org/abs/2401.12924](http://arxiv.org/abs/2401.12924)

    本文对于使用图像数据集进行森林火灾检测的支持向量机（SVM）进行了性能分析，并研究了关键因素如数据预处理、特征提取和模型训练。这项研究有助于开发高效的森林火灾检测系统。

    

    本文深入分析了使用图像数据集进行森林火灾检测的支持向量机（SVM）的性能和利用情况。随着森林火灾对生态系统和人类定居点的威胁日益增加，迅速准确的检测系统的需求至关重要。SVM以其强大的分类能力而闻名，在图像中识别与火灾相关的模式方面表现出熟练度。通过在标记数据上进行训练，SVM获得了识别与火灾相关的独特属性的能力，如火焰、烟雾或森林区域视觉特征的变化。本文全面研究了使用SVM的各个要素，包括数据预处理、特征提取和模型训练。严格评估了准确性、效率和实际适用性等参数。从这项研究中获得的知识有助于开发高效的森林火灾检测系统。

    This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets. With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance. SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images. By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area. The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training. It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability. The knowledge gained from this study aids in the development of efficient forest fire detection sy
    
[^29]: 可扩展的神经网络模型和千兆级数据集用于粒子流重建

    Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])

    [http://arxiv.org/abs/2309.06782](http://arxiv.org/abs/2309.06782)

    本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。

    

    本研究针对高能电子-正电子碰撞中基于高度粒度探测器模拟的完整事件重建，研究了可扩展的机器学习模型。粒子流（PF）重建可通过跟踪和量能器团簇或击中来构建监督学习任务。我们比较了图神经网络和基于内核的变换器，并证明两者都避免了二次内存分配和计算成本，同时实现了真实的粒子流重建。我们展示了在超级计算机上进行的超参数调优显著提高了模型的物理性能。我们还展示了所得模型在硬件处理器上具有高度可移植性，支持NVIDIA, AMD和英特尔 Habana卡。最后，我们证明了模型可以在由跟踪和量能器击中组成的高粒度输入上进行训练，从而获得与基准相竞争的物理性能。有关复现研究的数据集和软件已发布。

    We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
    
[^30]: 通用形式下的高效且多重稳健的风险估计方法在数据转移中

    Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift. (arXiv:2306.16406v1 [stat.ME])

    [http://arxiv.org/abs/2306.16406](http://arxiv.org/abs/2306.16406)

    本文研究了在通用的数据集转移条件下，利用半参数效率理论，高效估计目标总体风险的问题。

    

    统计机器学习方法经常面临来自感兴趣总体的有限数据的挑战。一种解决方法是利用来自辅助源总体的数据，这些数据与目标领域的某些条件分布相同或以其他方式相连。利用这种"数据转移"条件的技术被称为"领域适应"或"迁移学习"。尽管有大量关于数据转移的文献，但很少有研究探讨如何有效利用辅助总体来提高目标总体上机器学习任务风险评估的准确性。在本文中，我们利用半参数效率理论研究了在不同的数据集转移条件下高效估计目标总体风险的一般问题。我们考虑了一类通用的数据集转移条件，其中包括三种流行条件——协变量、标签和概念转移——作为特例。我们允许部分非重叠。

    Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \emph{dataset shift} conditions are known as \emph{domain adaptation} or \emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population.  In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions -- covariate, label and concept shift -- as special cases. We allow for partially non-overlappi
    
[^31]: 基于分数的生成模型的高保真图像压缩

    High-Fidelity Image Compression with Score-based Generative Models. (arXiv:2305.18231v1 [eess.IV])

    [http://arxiv.org/abs/2305.18231](http://arxiv.org/abs/2305.18231)

    本文提出了一种基于分数的生成模型的两阶段方法，该方法在图像压缩领域取得了显著的表现，实验证明该方法在一定比特率下能够提高图像的感知质量。

    

    尽管扩散生成模型在文本到图像生成中取得了巨大的成功，但在图像压缩领域复制这个成功却很困难。在本文中，我们展示了扩散模型可以显著提高在给定比特率下的感知质量，通过 FID 分数评估，表现超越了 PO-ELIC 和 HiFiC 的现有方法。我们通过一个简单但在理论上有动机的两阶段方法实现了这一点，该方法结合了以 MSE 为目标的自动编码器和一个进一步基于分数的解码器。然而，正如我们将展示的那样，实现细节很重要，最佳设计决策可能与典型的文本到图像模型有很大不同。

    Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.
    
[^32]: 近似最优的非参数顺序检验和具有可能相关观测的置信区间

    Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations. (arXiv:2212.14411v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2212.14411](http://arxiv.org/abs/2212.14411)

    本文研究了非参数顺序检验和置信区间，在一般非参数数据生成过程下提供了类型I错误和期望拒绝时间保证，提高了其灵活性和性能。

    

    顺序检验和其隐含的置信区间在任意停止时间下都能提供灵活的统计推断和即时决策。然而，强有力的保证仅适用于在实践中低估或浓度界限为基础的顺序序列，而这些序列具有次优的拒绝时间。在本文中，我们考虑罗宾斯（Robbins）1970年的延迟启动正态混合顺序概率比检验，并在一般非参数数据生成过程下提供了首个渐近类型I错误和期望拒绝时间保证，其中渐近性质由测试的烧入时间确定。类型I错误的结果主要依赖于鞅强不变原理，并证明这些检验（及其隐含的置信区间）具有接近所需α水平的类型I错误率。期望拒绝时间的结果主要利用了一种受伊藤引理启发的恒等式。

    Sequential tests and their implied confidence sequences, which are valid at arbitrary stopping times, promise flexible statistical inference and on-the-fly decision making. However, strong guarantees are limited to parametric sequential tests that under-cover in practice or concentration-bound-based sequences that over-cover and have suboptimal rejection times. In this work, we consider \cite{robbins1970boundary}'s delayed-start normal-mixture sequential probability ratio tests, and we provide the first asymptotic type-I-error and expected-rejection-time guarantees under general non-parametric data generating processes, where the asymptotics are indexed by the test's burn-in time. The type-I-error results primarily leverage a martingale strong invariance principle and establish that these tests (and their implied confidence sequences) have type-I error rates approaching a desired $\alpha$-level. The expected-rejection-time results primarily leverage an identity inspired by It\^o's lemm
    
[^33]: 双控制变量加速黑盒变分推断

    Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07290](http://arxiv.org/abs/2210.07290)

    本论文提出了双控制变量方法，能够同时减少数据子抽样和蒙特卡罗抽样带来的梯度估计方差，提高黑盒变分推断的准确性和效率。

    

    黑盒变分推断是一种广泛使用的贝叶斯后验推断框架，但在某些情况下，梯度估计中的高方差会损害准确性和效率。这种方差来自两个随机源：数据子抽样和蒙特卡罗抽样。现有的控制变量仅解决蒙特卡罗噪声，而增量梯度方法通常仅解决数据子抽样，我们提出了一种新的“双”控制变量，能够同时减少两种噪声源的方差。我们确认这导致了减少方差和在多个现实世界应用中提高优化效果。

    Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
    

