# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [New Equivalences Between Interpolation and SVMs: Kernels and Structured Features.](http://arxiv.org/abs/2305.02304) | 本文提出了一种新的、灵活的分析框架，用于证明在任意再生核希尔伯特空间中特征的SVP条件 |
| [^2] | [Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally.](http://arxiv.org/abs/2305.02247) | 本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。 |
| [^3] | [Shotgun crystal structure prediction using machine-learned formation energies.](http://arxiv.org/abs/2305.02158) | 本研究使用机器学习方法在多个结构预测标准测试中精确识别含有100个以上原子的许多材料的全局最小结构，并以单次能量评估为基础，取代了重复的第一原理能量计算过程。 |
| [^4] | [Low-complexity subspace-descent over symmetric positive definite manifold.](http://arxiv.org/abs/2305.02041) | 本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。 |
| [^5] | [Commentary on explainable artificial intelligence methods: SHAP and LIME.](http://arxiv.org/abs/2305.02012) | 这篇评论对可解释人工智能方法 SHAP 和 LIME 进行了评述和比较，提出了一个框架且突出了它们的优缺点。 |
| [^6] | [fairml: A Statistician's Take on Fair Machine Learning Modelling.](http://arxiv.org/abs/2305.02009) | 这篇论文介绍了一个基于统计方法的公平机器学习建模包fairml，该包实现了分类、回归和核估计等方法，可以在保障公平和问责方面提供帮助。 |
| [^7] | [Experimental Design for Any $p$-Norm.](http://arxiv.org/abs/2305.01942) | 该论文提出了一种通用的$p$-范数目标实验设计问题的解决算法，可适用于各种特殊情况，并且是已知最好的界的插值。 |
| [^8] | [Inferential Moments of Uncertain Multivariable Systems.](http://arxiv.org/abs/2305.01841) | 本文提出了一种新的分析不确定多变量系统行为的方法，使用推断矩描述分布预计如何响应新信息，特别关注推断偏差，以改善情境感知能力。 |
| [^9] | [Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features.](http://arxiv.org/abs/2305.01807) | 本研究首次从理论上研究了基于协方差神经网络的可转移性，证明了当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。 |
| [^10] | [Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems.](http://arxiv.org/abs/2305.01773) | 本文提出了一种利用图神经网络建模大规模动态系统的深度状态空间模型，在保持多峰预测分布的准确性的同时，通过确定性矩匹配规则实现了无样本推断，提高了预测的效率和稳定性。 |
| [^11] | [DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness.](http://arxiv.org/abs/2305.01770) | DeCom是一种基于深度耦合因子分解机的RSV预测算法，该算法结合了正常的季节性RSV传播模式和COVID-19符合NPI措施下RSV传播的不稳定性，预测效果更加准确。 |
| [^12] | [Adversarial Generative NMF for Single Channel Source Separation.](http://arxiv.org/abs/2305.01758) | 本文提出了一种基于对抗生成的非负矩阵分解单声道源分离方法，通过对抗训练NMF基，实现了在没有强监督数据可用的情况下提高重构信号质量的效果。 |
| [^13] | [Expressive Mortality Models through Gaussian Process Kernels.](http://arxiv.org/abs/2305.01728) | 本研究基于高斯过程框架，利用核函数的加法和乘法结构设计了一个遗传编程算法，能够学习特定人群的年龄和年份特定的死亡率曲面，为不同人群中队列效应的存在性带来了新的见解，并提供了相对平滑程度的分析工作。 |
| [^14] | [Slow Kill for Big Data Learning.](http://arxiv.org/abs/2305.01726) | 本文提出了一种称为“慢杀”的技术，它利用非凸约束优化、自适应$\ell_2$收缩和逐步增加的学习率，可以在大规模数据上实现高效变量筛选和统计精度。 |
| [^15] | [Auditing and Generating Synthetic Data with Controllable Trust Trade-offs.](http://arxiv.org/abs/2304.10819) | 本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。 |
| [^16] | [$(\alpha_D,\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives.](http://arxiv.org/abs/2302.14320) | 本文提出了一个双重目标GAN，通过使用可调的$\alpha$-loss来建模每个目标。在足够大的样本数和容量下，这类GAN的非零和游戏简化为最小化$f$-散度。最后，调整$(\alpha_D,\alpha_G)$可以缓解训练不稳定性。 |
| [^17] | [A survey on online active learning.](http://arxiv.org/abs/2302.08893) | 在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。本文综述了在线主动学习的最新进展、基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范式中不同的评估指标。 |
| [^18] | [Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs.](http://arxiv.org/abs/2302.02865) | 本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。 |
| [^19] | [Identifiability of latent-variable and structural-equation models: from linear to nonlinear.](http://arxiv.org/abs/2302.02672) | 本文研究了潜变量和结构方程模型的可辨识性问题，展示了如何推广现有的模型，使其可以用于非线性问题，并且仍然保持可辨识性。 |
| [^20] | [Convergence for score-based generative modeling with polynomial complexity.](http://arxiv.org/abs/2206.06227) | 本文证明了对于得分模型而言，从一个概率分布中抽样的核心机制，在$L^2(p)$准确估计$\nabla \ln p$后可以多项式收敛。同时提供了对基于得分的生成模型的理论分析，为使用退火程序生成样本提供了理论基础。 |
| [^21] | [Streaming Algorithms for High-Dimensional Robust Statistics.](http://arxiv.org/abs/2204.12399) | 本研究提出了首个高维健壮统计学流算法，具有几乎最优的存储需求，特别是在Huber污染模型下的高维健壮均值估计任务中，提供了一个高效的单遍流算法。 |
| [^22] | [HARFE: Hard-Ridge Random Feature Expansion.](http://arxiv.org/abs/2202.02877) | 论文提出了一种适用于高维稀疏可加函数的硬岭随机特征扩展方法（HARFE）模型，它可以通过应用基于硬阈值追踪的算法来进行近似计算，同时利用稀疏岭回归（SRR）表达式来取得稀疏模型选择和岭回归平滑之间的平衡，相比其他算法，HARFE方法在合成数据和真实数据集上具有更低的误差。 |

# 详细

[^1]: 揭示插值和支持向量机之间的新等价性：核函数和结构化特征

    New Equivalences Between Interpolation and SVMs: Kernels and Structured Features. (arXiv:2305.02304v1 [stat.ML])

    [http://arxiv.org/abs/2305.02304](http://arxiv.org/abs/2305.02304)

    本文提出了一种新的、灵活的分析框架，用于证明在任意再生核希尔伯特空间中特征的SVP条件

    

    支持向量机（SVM）是一种监督学习算法，它通过核技巧将数据映射到高维特征空间，找到最大间隔线性分类器。最近的研究表明，在某些过度参数化的情况下，SVM的决策函数与最小范数标签插值完全重合。这种支持向量增殖（SVP）现象特别有趣，因为它使我们能够通过利用线性和核模型中无害插值的最近分析来理解SVM的性能。然而，先前关于SVP的工作对数据/特征分布和频谱做出了限制性假设。在本文中，我们提出了一种新的、灵活的分析框架，用于在任意再生核希尔伯特空间中，对标签的生成模型的一类灵活性特征进行SVP证明。我们提出了局限于一般有界正交系统族（例如Fourier函数族）特征的SVP条件

    The support vector machine (SVM) is a supervised learning algorithm that finds a maximum-margin linear classifier, often after mapping the data to a high-dimensional feature space via the kernel trick. Recent work has demonstrated that in certain sufficiently overparameterized settings, the SVM decision function coincides exactly with the minimum-norm label interpolant. This phenomenon of support vector proliferation (SVP) is especially interesting because it allows us to understand SVM performance by leveraging recent analyses of harmless interpolation in linear and kernel models. However, previous work on SVP has made restrictive assumptions on the data/feature distribution and spectrum. In this paper, we present a new and flexible analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert space with a flexible class of generative models for the labels. We present conditions for SVP for features in the families of general bounded orthonormal systems (e.g. Fourier f
    
[^2]: 毫不畏惧地选择：几乎所有的小批量训练方案都能够优化。

    Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])

    [http://arxiv.org/abs/2305.02247](http://arxiv.org/abs/2305.02247)

    本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。

    

    我们证明了带有确定性或随机性、数据独立的小批量梯度下降训练的匹配上下一般化误差界限，但批量选择规则是任意的。我们考虑光滑的Lipschitz-凸性/非凸性/强凸性损失函数，并证明了随机梯度下降的经典上限界限也适用于这样任意的非自适应批量调度，包括所有确定性的调度方案。进一步地，对于凸和强凸的损失函数，我们直接证明了在上述批量调度类上一致的一般化误差下的匹配下限界限，表明所有这样的批量调度都能达到最优的一般化。最后，对于光滑的（非Lipschitz）非凸性损失函数，我们证明了在所考虑的类别内，包括所有随机批处理方案，全批量（确定性）梯度下降是最优的。

    We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.
    
[^3]: 使用机器学习的形成能量预测方法进行猎枪晶体结构预测

    Shotgun crystal structure prediction using machine-learned formation energies. (arXiv:2305.02158v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.02158](http://arxiv.org/abs/2305.02158)

    本研究使用机器学习方法在多个结构预测标准测试中精确识别含有100个以上原子的许多材料的全局最小结构，并以单次能量评估为基础，取代了重复的第一原理能量计算过程。

    

    可以通过找到原子构型能量曲面的全局或局部极小值来预测组装原子的稳定或亚稳定晶体结构。通常，这需要重复的第一原理能量计算，这在包含30个以上原子的大型系统中是不实际的。本研究使用简单但功能强大的机器学习工作流，使用机器学习辅助第一原理能量计算，对大量虚拟创建的晶体结构进行非迭代式单次筛选，从而在解决晶体结构预测问题方面取得了重大进展。

    Stable or metastable crystal structures of assembled atoms can be predicted by finding the global or local minima of the energy surface with respect to the atomic configurations. Generally, this requires repeated first-principles energy calculations that are impractical for large systems, such as those containing more than 30 atoms in the unit cell. Here, we have made significant progress in solving the crystal structure prediction problem with a simple but powerful machine-learning workflow; using a machine-learning surrogate for first-principles energy calculations, we performed non-iterative, single-shot screening using a large library of virtually created crystal structures. The present method relies on two key technical components: transfer learning, which enables a highly accurate energy prediction of pre-relaxed crystalline states given only a small set of training samples from first-principles calculations, and generative models to create promising and diverse crystal structure
    
[^4]: 对称正定流形上低复杂度的子空间下降算法

    Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])

    [http://arxiv.org/abs/2305.02041](http://arxiv.org/abs/2305.02041)

    本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。

    

    本文提出了一种低复杂度的黎曼子空间下降算法，用于在对称正定（SPD）流形上对函数进行最小化。与现有的黎曼梯度下降变体不同的是，所提出的方法利用 carefully chosen 的子空间，使得更新可以写成迭代的 Cholesky 因子和一个稀疏矩阵的乘积形式。由此产生的更新避免了昂贵的矩阵操作，如矩阵指数和密集矩阵乘法，这些操作通常在几乎所有其他 Riemannian 优化算法中都是必需的。

    This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
    
[^5]: 可解释人工智能方法评述：SHAP 和 LIME

    Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])

    [http://arxiv.org/abs/2305.02012](http://arxiv.org/abs/2305.02012)

    这篇评论对可解释人工智能方法 SHAP 和 LIME 进行了评述和比较，提出了一个框架且突出了它们的优缺点。

    

    可解释人工智能（XAI）方法已经发展出来，将机器学习模型的黑匣子转化为更易理解的形式。这些方法有助于传达模型的工作原理，旨在使机器学习模型更透明，并增加最终用户对其输出的信任。 SHapley Additive exPlanations（SHAP）和Local Interpretable Model Agnostic Explanation（LIME）是两种在表格数据中广泛使用的XAI方法。在这篇评论中，我们讨论了两种方法的可解释性度量是如何生成的，并提出了一个解释它们输出的框架，突出了它们的优缺点。

    eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
    
[^6]: 公平机器学习建模的统计学观点

    fairml: A Statistician's Take on Fair Machine Learning Modelling. (arXiv:2305.02009v1 [stat.ML])

    [http://arxiv.org/abs/2305.02009](http://arxiv.org/abs/2305.02009)

    这篇论文介绍了一个基于统计方法的公平机器学习建模包fairml，该包实现了分类、回归和核估计等方法，可以在保障公平和问责方面提供帮助。

    

    机器学习在关键保障公平和问责的应用中的采用，导致文献中出现了大量模型建议，主要以约束条件的优化问题形式减少或消除敏感属性对响应的影响。虽然这种方法从理论上来看非常灵活，但所得的模型有些黑盒性质：很少有关于其统计属性的表述，关于其应用最佳实践的讨论，以及如何将其扩展到其他问题而不是其最初设计用途的研究。此外，估计每个模型需要特定的实现，涉及适当的求解器，从软件工程角度来看是不太理想的。在本文中，我们介绍了fairml R软件包，该软件包实现了我们之前的工作（Scutari，Panero和Proissl 2022）以及文献中的相关模型。fairml是围绕分类、回归和核估计的统计方法设计的。

    The adoption of machine learning in applications where it is crucial to ensure fairness and accountability has led to a large number of model proposals in the literature, largely formulated as optimisation problems with constraints reducing or eliminating the effect of sensitive attributes on the response. While this approach is very flexible from a theoretical perspective, the resulting models are somewhat black-box in nature: very little can be said about their statistical properties, what are the best practices in their applied use, and how they can be extended to problems other than those they were originally designed for. Furthermore, the estimation of each model requires a bespoke implementation involving an appropriate solver which is less than desirable from a software engineering perspective.  In this paper, we describe the fairml R package which implements our previous work (Scutari, Panero, and Proissl 2022) and related models in the literature. fairml is designed around cla
    
[^7]: 任意$p$-范数的实验设计

    Experimental Design for Any $p$-Norm. (arXiv:2305.01942v1 [cs.DS])

    [http://arxiv.org/abs/2305.01942](http://arxiv.org/abs/2305.01942)

    该论文提出了一种通用的$p$-范数目标实验设计问题的解决算法，可适用于各种特殊情况，并且是已知最好的界的插值。

    

    我们考虑了一个通用的$p$-范数目标，用于实验设计问题，并将一些经典目标（D/A/E-设计）作为特殊情况进行了概括。我们证明了一种随机局部搜索方法提供了一个统一的算法来解决所有的$p$-范数问题。这提供了第一个适用于普遍$p$-范数目标的近似算法，并且是特殊情况下已知最优界的一个很好的插值。

    We consider a general $p$-norm objective for experimental design problems that captures some well-studied objectives (D/A/E-design) as special cases. We prove that a randomized local search approach provides a unified algorithm to solve this problem for all $p$. This provides the first approximation algorithm for the general $p$-norm objective, and a nice interpolation of the best known bounds of the special cases.
    
[^8]: 不确定多变量系统的推断矩

    Inferential Moments of Uncertain Multivariable Systems. (arXiv:2305.01841v1 [physics.data-an])

    [http://arxiv.org/abs/2305.01841](http://arxiv.org/abs/2305.01841)

    本文提出了一种新的分析不确定多变量系统行为的方法，使用推断矩描述分布预计如何响应新信息，特别关注推断偏差，以改善情境感知能力。

    

    本文提出了一种使用称为“推断矩”的一组量来分析不确定多变量系统行为的新范式。边缘化是一种不确定性量化过程，它通过平均条件概率来量化所关注概率的期望值。推断矩是描述分布预计如何响应新信息的高阶条件概率矩。本文研究了推断偏差，它是期望的概率波动，随着推断更新另一个变量而发生变化。我们以推断矩的形式找到了互信息的幂级数展开式，这意味着推断矩逻辑可能对通常使用信息论工具执行的任务有用。我们在两个应用中探讨了贝叶斯网络的推断偏差，以改善情境感知能力。

    This article offers a new paradigm for analyzing the behavior of uncertain multivariable systems using a set of quantities we call \emph{inferential moments}. Marginalization is an uncertainty quantification process that averages conditional probabilities to quantify the \emph{expected value} of a probability of interest. Inferential moments are higher order conditional probability moments that describe how a distribution is expected to respond to new information. Of particular interest in this article is the \emph{inferential deviation}, which is the expected fluctuation of the probability of one variable in response to an inferential update of another. We find a power series expansion of the Mutual Information in terms of inferential moments, which implies that inferential moment logic may be useful for tasks typically performed with information theoretic tools. We explore this in two applications that analyze the inferential deviations of a Bayesian Network to improve situational aw
    
[^9]: 基于协方差神经网络的可转移学习和应用于解释性脑龄预测

    Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features. (arXiv:2305.01807v1 [cs.LG])

    [http://arxiv.org/abs/2305.01807](http://arxiv.org/abs/2305.01807)

    本研究首次从理论上研究了基于协方差神经网络的可转移性，证明了当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。

    

    图卷积网络（GCN）利用基于拓扑图的卷积操作来组合图上的信息进行推理任务。我们最近的工作中，通过使用协方差矩阵作为图来设计了一种类似于传统PCA数据分析方法的协方差神经网络（VNN），并具有显著的优势。本文首先从理论上研究了VNN的可转移性。可转移性的概念是从学习模型可以在“兼容”的数据集上泛化的直观期望中产生的。我们展示了VNN从GCN继承的无标度数据处理架构，并证明当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。

    Graph convolutional networks (GCN) leverage topology-driven graph convolutional operations to combine information across the graph for inference tasks. In our recent work, we have studied GCNs with covariance matrices as graphs in the form of coVariance neural networks (VNNs) that draw similarities with traditional PCA-driven data analysis approaches while offering significant advantages over them. In this paper, we first focus on theoretically characterizing the transferability of VNNs. The notion of transferability is motivated from the intuitive expectation that learning models could generalize to "compatible" datasets (possibly of different dimensionalities) with minimal effort. VNNs inherit the scale-free data processing architecture from GCNs and here, we show that VNNs exhibit transferability of performance over datasets whose covariance matrices converge to a limit object. Multi-scale neuroimaging datasets enable the study of the brain at multiple scales and hence, can validate
    
[^10]: 大规模动态系统的深度状态空间模型的廉价和确定性推断

    Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems. (arXiv:2305.01773v1 [cs.LG])

    [http://arxiv.org/abs/2305.01773](http://arxiv.org/abs/2305.01773)

    本文提出了一种利用图神经网络建模大规模动态系统的深度状态空间模型，在保持多峰预测分布的准确性的同时，通过确定性矩匹配规则实现了无样本推断，提高了预测的效率和稳定性。

    

    图神经网络通常被用于建模相互作用的动态系统，因为它们优雅地适应于具有变化和大量代理的系统。虽然在确定性相互作用系统方面取得了很多进展，但对于有兴趣获得未来轨迹的预测分布的随机系统，模型更具挑战性。现有方法要么计算速度慢，因为它们依赖于蒙特卡罗抽样，要么做出简化假设，使得预测分布是单峰的。在本文中，我们提出了一个深度状态空间模型，它采用图神经网络来建模底层的相互作用动态系统。预测分布是多峰的，并且具有高斯混合模型的形式，其中高斯分量的矩可以通过确定性矩匹配规则计算。我们的矩匹配方案可以用于无样本推断，从而实现更有效和稳定的预测。实验结果表明，我们的方法能够有效地建模和预测随机系统的轨迹，即使存在巨大的不确定性。

    Graph neural networks are often used to model interacting dynamical systems since they gracefully scale to systems with a varying and high number of agents. While there has been much progress made for deterministic interacting systems, modeling is much more challenging for stochastic systems in which one is interested in obtaining a predictive distribution over future trajectories. Existing methods are either computationally slow since they rely on Monte Carlo sampling or make simplifying assumptions such that the predictive distribution is unimodal. In this work, we present a deep state-space model which employs graph neural networks in order to model the underlying interacting dynamical system. The predictive distribution is multimodal and has the form of a Gaussian mixture model, where the moments of the Gaussian components can be computed via deterministic moment matching rules. Our moment matching scheme can be exploited for sample-free inference, leading to more efficient and sta
    
[^11]: DeCom：基于深度耦合因子分解机的非药物干预感知的COVID-19后RSV预测

    DeCom: Deep Coupled-Factorization Machine for Post COVID-19 Respiratory Syncytial Virus Prediction with Nonpharmaceutical Interventions Awareness. (arXiv:2305.01770v1 [cs.LG])

    [http://arxiv.org/abs/2305.01770](http://arxiv.org/abs/2305.01770)

    DeCom是一种基于深度耦合因子分解机的RSV预测算法，该算法结合了正常的季节性RSV传播模式和COVID-19符合NPI措施下RSV传播的不稳定性，预测效果更加准确。

    

    呼吸道合胞病毒（RSV）是婴幼儿最危险的呼吸系统疾病之一。由于COVID-19的非药物干预（NPI）措施，RSV的季节性传播模式在2020年已经中断，并在2021年北半球提前数月出现转变。因此，理解COVID-19如何影响RSV并构建预测算法以预测COVID-19后RSV再出现的时间和强度至关重要。本文提出了一种名为DeCom的深度耦合张量分解机，用于COVID-19后RSV预测。DeCom利用张量分解和残差建模，能可靠地学习受COVID-19影响下的RSV传播，同时考虑正常季节性RSV传播模式和NPI。在真实的RSV数据集上的实验结果显示，DeCom比现有RSV预测算法更准确，并可最大程度地提高预测效果。

    Respiratory syncytial virus (RSV) is one of the most dangerous respiratory diseases for infants and young children. Due to the nonpharmaceutical intervention (NPI) imposed in the COVID-19 outbreak, the seasonal transmission pattern of RSV has been discontinued in 2020 and then shifted months ahead in 2021 in the northern hemisphere. It is critical to understand how COVID-19 impacts RSV and build predictive algorithms to forecast the timing and intensity of RSV reemergence in post-COVID-19 seasons. In this paper, we propose a deep coupled tensor factorization machine, dubbed as DeCom, for post COVID-19 RSV prediction. DeCom leverages tensor factorization and residual modeling. It enables us to learn the disrupted RSV transmission reliably under COVID-19 by taking both the regular seasonal RSV transmission pattern and the NPI into consideration. Experimental results on a real RSV dataset show that DeCom is more accurate than the state-of-the-art RSV prediction algorithms and achieves up 
    
[^12]: 基于对抗生成的非负矩阵分解单声道源分离方法

    Adversarial Generative NMF for Single Channel Source Separation. (arXiv:2305.01758v1 [eess.AS])

    [http://arxiv.org/abs/2305.01758](http://arxiv.org/abs/2305.01758)

    本文提出了一种基于对抗生成的非负矩阵分解单声道源分离方法，通过对抗训练NMF基，实现了在没有强监督数据可用的情况下提高重构信号质量的效果。

    

    对抗学习正则化函数的思想最近被引入了更广泛的反问题背景下。该方法的灵感在于意识到不仅需要学习组成所需信号的基本特征，而且或者更重要的是，需要学习避免表示中的哪些特征。在本文中，我们将应用这种方法来解决通过非负矩阵分解进行源分离的问题，并提出一种新的对抗训练NMF基的方法。我们通过数值实验表明，无论是图像还是音频分离，在没有强监督数据可用的情况下，都会明显提高重构信号的质量。

    The idea of adversarial learning of regularization functionals has recently been introduced in the wider context of inverse problems. The intuition behind this method is the realization that it is not only necessary to learn the basic features that make up a class of signals one wants to represent, but also, or even more so, which features to avoid in the representation. In this paper, we will apply this approach to the problem of source separation by means of non-negative matrix factorization (NMF) and present a new method for the adversarial training of NMF bases. We show in numerical experiments, both for image and audio separation, that this leads to a clear improvement of the reconstructed signals, in particular in the case where little or no strong supervision data is available.
    
[^13]: 通过高斯过程核表达寿命模型的灵活性

    Expressive Mortality Models through Gaussian Process Kernels. (arXiv:2305.01728v1 [stat.ML])

    [http://arxiv.org/abs/2305.01728](http://arxiv.org/abs/2305.01728)

    本研究基于高斯过程框架，利用核函数的加法和乘法结构设计了一个遗传编程算法，能够学习特定人群的年龄和年份特定的死亡率曲面，为不同人群中队列效应的存在性带来了新的见解，并提供了相对平滑程度的分析工作。

    

    我们开发了一个灵活的高斯过程（GP）框架，用于学习年龄和年份特定的死亡率曲面的协方差结构。利用GP核的加法和乘法结构，我们设计一个遗传编程算法来搜索针对给定人群的最具表现力的核。我们的组合搜索基于年龄-期间-队列（APC）范例，以构建最能匹配死亡率数据集的时空动态的协方差先验。我们在合成案例研究中应用得到的遗传算法（GA）来验证GA恢复APC结构的能力，并在人类死亡数据库的实际国家级数据集上进行分析。我们的机器学习分析提供了有关不同人群中队列效应存在或不存在的新见解，以及沿年龄和年份维度的死亡率曲面的相对平滑程度。我们的建模工作是在Python的PyTorch库中完成的，提供了深入的分析。

    We develop a flexible Gaussian Process (GP) framework for learning the covariance structure of Age- and Year-specific mortality surfaces. Utilizing the additive and multiplicative structure of GP kernels, we design a genetic programming algorithm to search for the most expressive kernel for a given population. Our compositional search builds off the Age-Period-Cohort (APC) paradigm to construct a covariance prior best matching the spatio-temporal dynamics of a mortality dataset. We apply the resulting genetic algorithm (GA) on synthetic case studies to validate the ability of the GA to recover APC structure, and on real-life national-level datasets from the Human Mortality Database. Our machine-learning based analysis provides novel insight into the presence/absence of Cohort effects in different populations, and into the relative smoothness of mortality surfaces along the Age and Year dimensions. Our modelling work is done with the PyTorch libraries in Python and provides an in-depth 
    
[^14]: 大数据学习的慢杀技巧

    Slow Kill for Big Data Learning. (arXiv:2305.01726v1 [stat.ML])

    [http://arxiv.org/abs/2305.01726](http://arxiv.org/abs/2305.01726)

    本文提出了一种称为“慢杀”的技术，它利用非凸约束优化、自适应$\ell_2$收缩和逐步增加的学习率，可以在大规模数据上实现高效变量筛选和统计精度。

    

    大数据应用通常涉及大量的观察和特征，这为变量选择和参数估计带来了新的挑战。本文介绍了一种称为“慢杀”的新技术，它利用非凸约束优化、自适应$\ell_2$收缩和逐步增加的学习率。在慢杀迭代过程中，问题规模可以减小，这使其特别适用于大规模变量筛选。统计和优化之间的相互作用提供了有关控制分位数、步长和收缩参数以放松所需的正则性条件以实现所需统计精度的有价值的见解。实验结果表明慢杀在各种情况下优于现有算法，并且在大规模数据上具有高效的计算能力。

    Big-data applications often involve a vast number of observations and features, creating new challenges for variable selection and parameter estimation. This paper presents a novel technique called ``slow kill,'' which utilizes nonconvex constrained optimization, adaptive $\ell_2$-shrinkage, and increasing learning rates. The fact that the problem size can decrease during the slow kill iterations makes it particularly effective for large-scale variable screening. The interaction between statistics and optimization provides valuable insights into controlling quantiles, stepsize, and shrinkage parameters in order to relax the regularity conditions required to achieve the desired level of statistical accuracy. Experimental results on real and synthetic data show that slow kill outperforms state-of-the-art algorithms in various situations while being computationally efficient for large-scale data.
    
[^15]: 可控的信任权衡下的合成数据审计与生成

    Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])

    [http://arxiv.org/abs/2304.10819](http://arxiv.org/abs/2304.10819)

    本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。

    

    现实中收集的数据往往存在偏差、不平衡，并且有泄露敏感和隐私信息的风险。这一事实引发了创建合成数据集的想法，以减轻真实数据中固有的风险、偏见、伤害和隐私问题。这个概念依赖于生成AI模型，以产生不偏执、保护隐私的合成数据，同时忠实于真实数据。在这种新范式中，我们如何知道这种方法是否兑现了其承诺？我们提出了一个审计框架，提供了对合成数据集和基于它们训练的AI模型的全面评估，围绕偏见和歧视的预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。我们通过审计多个生成模型在不同用例中展示了我们的框架，包括教育、医疗保健、银行、人力资源，以及从表格，时间序列到自然语言的不同模态。我们的用例展示了在合成数据生成中平衡信任和效用的权衡的重要性。

    Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
    
[^16]: $(\alpha_D,\alpha_G)$-GANs：通过双重目标来解决GAN训练不稳定性

    $(\alpha_D,\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives. (arXiv:2302.14320v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14320](http://arxiv.org/abs/2302.14320)

    本文提出了一个双重目标GAN，通过使用可调的$\alpha$-loss来建模每个目标。在足够大的样本数和容量下，这类GAN的非零和游戏简化为最小化$f$-散度。最后，调整$(\alpha_D,\alpha_G)$可以缓解训练不稳定性。

    

    为了解决GAN的训练不稳定性，我们引入了一类具有不同价值函数（目标）的双重目标GAN，特别地，我们使用可调的分类损失——$\alpha$-loss来建模每个目标，以得到由$(\alpha_D,\alpha_G)\in(0,\infty]^2$参数化的$(\alpha_D,\alpha_G)$-GAN。对于足够大的样本数和G、D的容量，我们展示了在$(\alpha_D,\alpha_G)$的适当条件下，导致的非零和游戏简化为最小化$f$-散度。在有限的样本数和容量的情况下，我们定义估计误差，以量化相对于无限样本下的最优设定而言生成器性能的差距，并得到了这个误差的上界，证明了在某些条件下它的阶有效。最后，我们强调了调整$(\alpha_D,\alpha_G)$在缓解合成2D高斯混合问题的训练不稳定性方面的价值。

    In an effort to address the training instabilities of GANs, we introduce a class of dual-objective GANs with different value functions (objectives) for the generator (G) and discriminator (D). In particular, we model each objective using $\alpha$-loss, a tunable classification loss, to obtain $(\alpha_D,\alpha_G)$-GANs, parameterized by $(\alpha_D,\alpha_G)\in (0,\infty]^2$. For sufficiently large number of samples and capacities for G and D, we show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\alpha_D,\alpha_G)$. In the finite sample and capacity setting, we define estimation error to quantify the gap in the generator's performance relative to the optimal setting with infinite samples and obtain upper bounds on this error, showing it to be order optimal under certain conditions. Finally, we highlight the value of tuning $(\alpha_D,\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture
    
[^17]: 在线主动学习综述

    A survey on online active learning. (arXiv:2302.08893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08893](http://arxiv.org/abs/2302.08893)

    在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。本文综述了在线主动学习的最新进展、基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范式中不同的评估指标。

    

    在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。近年来，随着数据仅以未标记形式可用的实际应用日益增多，最小化与收集标记观测相关的成本问题引起了广泛关注。标注每个观测可以耗费大量的时间和成本，使得获取大量标记数据变得困难。为了克服这个问题，许多主动学习策略已经提出，旨在选择最具信息量的观测进行标记，以提高机器学习模型的性能。这些方法可以广泛地分为两类：静态基于池的和基于流的主动学习。基于池的主动学习涉及从封闭的未标记数据池中选择一部分观测，已成为许多调查和文献综述的重点。然而，随着在线数据流的不断增加，基于流的主动学习策略变得更加吸引人，因为它们允许模型适应新进数据。在本综述中，我们综述了在线主动学习的最新进展，讨论了基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范例中不同的评估指标。

    Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. Howev
    
[^18]: 概率对比学习恢复了不确定性输入的正确估计

    Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02865](http://arxiv.org/abs/2302.02865)

    本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。

    

    最近，对比学习编码器被证明可以翻转数据生成过程：它们可以将每个输入（如图像）编码成生成该图像的真实潜变量（Zimmermann等人，2021）。然而，现实世界的观察结果通常存在内在的模糊性。例如，图像可能模糊或只显示3D物体的2D视图，因此可能有多个潜变量生成它们。这使得潜变量的真实后验概率具有异方差不确定性。在这种设置下，我们扩展了常见的InfoNCE目标和编码器，以预测潜变量分布而不是点。我们证明这些分布恢复了数据生成过程的正确后验分布，包括其不确定性水平的估计，该估计存在潜变量空间的旋转。除了提供校准的不确定性估计之外，这些后验分布还允许在图像检索中计算可信区间。它们包括具有与给定查询相同的潜变量的图像。

    Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
    
[^19]: 潜变量和结构方程模型的可辨识性：从线性到非线性

    Identifiability of latent-variable and structural-equation models: from linear to nonlinear. (arXiv:2302.02672v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02672](http://arxiv.org/abs/2302.02672)

    本文研究了潜变量和结构方程模型的可辨识性问题，展示了如何推广现有的模型，使其可以用于非线性问题，并且仍然保持可辨识性。

    

    在多变量统计学中，线性高斯模型通常是不可辨识的。在本文中，我们展示了如何推广现有的模型，使其可以用于非线性问题，并且仍然保持可辨识性。通过引入时间序列或观测到的辅助变量，我们可以克服非高斯性是否足够的限制，来实现这样的可辨识性。本文回顾了线性和非线性可辨识性理论。

    An old problem in multivariate statistics is that linear Gaussian models are often unidentifiable, i.e. some parameters cannot be uniquely estimated. In factor (component) analysis, an orthogonal rotation of the factors is unidentifiable, while in linear regression, the direction of effect cannot be identified. For such linear models, non-Gaussianity of the (latent) variables has been shown to provide identifiability. In the case of factor analysis, this leads to independent component analysis, while in the case of the direction of effect, non-Gaussian versions of structural equation modelling solve the problem. More recently, we have shown how even general nonparametric nonlinear versions of such models can be estimated. Non-Gaussianity is not enough in this case, but assuming we have time series, or that the distributions are suitably modulated by some observed auxiliary variables, the models are identifiable. This paper reviews the identifiability theory for the linear and nonlinear
    
[^20]: 得分模型的多项式复杂度的重要性证明

    Convergence for score-based generative modeling with polynomial complexity. (arXiv:2206.06227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06227](http://arxiv.org/abs/2206.06227)

    本文证明了对于得分模型而言，从一个概率分布中抽样的核心机制，在$L^2(p)$准确估计$\nabla \ln p$后可以多项式收敛。同时提供了对基于得分的生成模型的理论分析，为使用退火程序生成样本提供了理论基础。

    

    得分模型（SGM）是一种学习概率分布并生成更多样本的高效方法。本文证明了SGM背后的核心机制即在$L^2(p)$准确估计$\nabla \ln p$后从概率密度$p$中抽样的多项式收敛保证。与之前的工作相比，我们不会产生随时间指数增长或遭受维数灾难的误差。我们的保证适用于任何平滑分布，且与其对数Sobolev常数多项式相关。使用我们的保证，我们对基于得分的生成模型进行了理论分析，将白噪声输入转换为从不同噪声尺度给定得分估计的学习数据分布的样本。我们的分析为使用退火程序生成好的样本提供了理论基础，因为我们的证明基本上是依赖于使用。

    Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $\nabla \ln p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using
    
[^21]: 面向高维健壮统计学的流算法

    Streaming Algorithms for High-Dimensional Robust Statistics. (arXiv:2204.12399v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2204.12399](http://arxiv.org/abs/2204.12399)

    本研究提出了首个高维健壮统计学流算法，具有几乎最优的存储需求，特别是在Huber污染模型下的高维健壮均值估计任务中，提供了一个高效的单遍流算法。

    

    本文研究流模型下的高维健壮统计学问题。最近的一些工作针对一系列高维健壮估计任务提出了计算有效率的算法。不幸的是，所有之前的算法都需要存储整个数据集，使得内存至少呈二次方与维度同阶。本研究提出了首个高维健壮统计学流算法，其具有几乎最优的存储需求（以对数因子表示）。本文主要结果是在Huber污染模型下的高维健壮均值估计任务，我们给出了一个高效的单遍流算法，具有几乎最优的误差保证和空间复杂度几乎线性于维度。作为推论，我们得到了几个更复杂的任务的几乎最优空间复杂度的流算法，包括健壮协方差估计，健壮回归，更普遍的健壮随机优化等。

    We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimiz
    
[^22]: HARFE: 硬岭随机特征扩展方法

    HARFE: Hard-Ridge Random Feature Expansion. (arXiv:2202.02877v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.02877](http://arxiv.org/abs/2202.02877)

    论文提出了一种适用于高维稀疏可加函数的硬岭随机特征扩展方法（HARFE）模型，它可以通过应用基于硬阈值追踪的算法来进行近似计算，同时利用稀疏岭回归（SRR）表达式来取得稀疏模型选择和岭回归平滑之间的平衡，相比其他算法，HARFE方法在合成数据和真实数据集上具有更低的误差。

    

    本论文针对高维稀疏可加函数，提出一种随机特征模型——硬岭随机特征扩展方法（HARFE）。该方法利用基于硬阈值追踪的算法，应用于稀疏岭回归（SRR）问题，来近似计算相对于随机特征矩阵的系数。该SRR表达式在稀疏模型选择和岭回归平滑之间取得平衡，从而有利于处理噪声和异常值。此外，为了匹配加性函数假设，我们在随机特征矩阵中采用了随机稀疏连接模式。我们证明了HARFE方法会收敛至给定误差界限，具体取决于噪声和稀疏岭回归模型参数。基于合成数据和真实数据集的数值结果表明，HARFE方法的误差低于（或与）其他最先进算法相当。

    We propose a random feature model for approximating high-dimensional sparse additive functions called the hard-ridge random feature expansion method (HARFE). This method utilizes a hard-thresholding pursuit-based algorithm applied to the sparse ridge regression (SRR) problem to approximate the coefficients with respect to the random feature matrix. The SRR formulation balances between obtaining sparse models that use fewer terms in their representation and ridge-based smoothing that tend to be robust to noise and outliers. In addition, we use a random sparse connectivity pattern in the random feature matrix to match the additive function assumption. We prove that the HARFE method is guaranteed to converge with a given error bound depending on the noise and the parameters of the sparse ridge regression model. Based on numerical results on synthetic data as well as on real datasets, the HARFE approach obtains lower (or comparable) error than other state-of-the-art algorithms.
    

