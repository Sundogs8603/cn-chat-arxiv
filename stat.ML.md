# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Incentive-Theoretic Bayesian Inference for Collaborative Science.](http://arxiv.org/abs/2307.03748) | 本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。 |
| [^2] | [GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies.](http://arxiv.org/abs/2307.03675) | GeoPhy是一种创新的、完全可微的系统发育推断方法，通过在连续几何空间中表示拓扑分布，实现了可扩展的变分推断，克服了不限制拓扑结构的挑战。 |
| [^3] | [Online Network Source Optimization with Graph-Kernel MAB.](http://arxiv.org/abs/2307.03641) | 提出了Grab-UCB算法用于在线学习大规模网络中最优的源放置位置，通过使用自适应的图词典模型来描述网络过程，并利用稀疏谱表示实现了数据高效的学习框架。 |
| [^4] | [BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits.](http://arxiv.org/abs/2307.03587) | BOF-UCB是一种用于非平稳环境下的背景线性赌博机的贝叶斯优化频率算法，其结合了贝叶斯和频率学派原则，提高了在动态环境中的性能。它利用贝叶斯更新推断后验分布，并使用频率学派方法计算上界信心界以平衡探索和开发。实验证明，BOF-UCB优于现有方法，是非平稳环境中顺序决策的有前途的解决方案。 |
| [^5] | [Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization.](http://arxiv.org/abs/2307.03571) | 本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。 |
| [^6] | [MALIBO: Meta-learning for Likelihood-free Bayesian Optimization.](http://arxiv.org/abs/2307.03565) | MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。 |
| [^7] | [Learning Theory of Distribution Regression with Neural Networks.](http://arxiv.org/abs/2307.03487) | 本文提出了一个全连接神经网络框架用于分布回归问题，解决了传统神经网络无法直接处理概率分布输入的困难，并建立了逼近理论和学习理论。 |
| [^8] | [On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers.](http://arxiv.org/abs/2307.03460) | 本文研究了动态实现的Hamiltonian Monte Carlo (HMC)算法和No U-Turn Sampler (NUTS) 的收敛性，证明了NUTS作为动态HMC的特例，并且在一定条件下具有遍历性和几何遍历性。同时改进了HMC的收敛性结果，证明了在目标分布为高斯分布的微扰情况下，无需任何有界条件，HMC也是遍历的。 |
| [^9] | [Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data.](http://arxiv.org/abs/2307.03410) | 这篇论文提出了一个适用于特征分布式数据的可扩展高维多变量线性回归算法，具有通信复杂度不依赖于特征维度和快速收敛性的优势，可应用于大规模数据集和具有多变量响应变量的场景。 |
| [^10] | [Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms.](http://arxiv.org/abs/2307.03357) | 本文通过统计学习理论的算法稳定性，分析了随机组合梯度下降算法的稳定性和泛化性，引入了组合一致稳定性概念并与SCO问题的泛化性建立了定量关系。 |
| [^11] | [Quantification of Uncertainty with Adversarial Models.](http://arxiv.org/abs/2307.03217) | 该论文提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。与先前的方法相比，QUAM对认知不确定性的近似误差更小。 |
| [^12] | [Degree Heterogeneity in Higher-Order Networks: Inference in the Hypergraph $\boldsymbol{\beta}$-Model.](http://arxiv.org/abs/2307.02818) | 本文对具有多层的超图β模型进行了研究，推导了最大似然估计的收敛速率和极限分布，并构建了模型参数的置信区间。同时，我们还建立了超图β模型中似然比检验的渐近正态性。 |
| [^13] | [Adaptive Strategies in Non-convex Optimization.](http://arxiv.org/abs/2306.10278) | 本文介绍了应用于随机优化和深度神经网络训练中的自适应算法。算法可以自动适应不同的噪声水平和梯度比例范围，从而达到（近似）最优速度。 |
| [^14] | [A Characterization of Online Multiclass Learnability.](http://arxiv.org/abs/2303.17716) | 在线多类学习问题中，使用Multiclass Littlestone维度可以刻画标签数目为无界情况下的可学习性。 |
| [^15] | [Continuous-Time Functional Diffusion Processes.](http://arxiv.org/abs/2303.00800) | 连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。 |
| [^16] | [Black-Box Batch Active Learning for Regression.](http://arxiv.org/abs/2302.08981) | 本文提出了一种黑盒批量主动学习方法，该方法仅利用模型预测进行评估，适用于各种机器学习模型，并扩展了一系列白盒批量主动学习方法。 |
| [^17] | [Federated Variational Inference Methods for Structured Latent Variable Models.](http://arxiv.org/abs/2302.03314) | 本论文提出了一种基于结构化变分推断的联合学习方法，该方法适用于各种结构化概率模型。此外，还提供了一种通信高效的变体算法，证明了所提出算法的有效性和性能。 |
| [^18] | [Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model.](http://arxiv.org/abs/2205.13085) | 本研究使用异方差噪声模型，开发了一种名为广义根因因果推断（GRCI）的算法，能够更准确地识别疾病的患者特定根本原因。 |
| [^19] | [Beyond Cuts in Small Signal Scenarios -- Enhanced Sneutrino Detectability Using Machine Learning.](http://arxiv.org/abs/2108.03125) | 本研究提出一种利用机器学习的方式，在信号与背景高度重叠的情况下增强LHC新物理探测的灵敏度。使用XGBoost和深度神经网络模型来利用可观测量之间的相关性比传统的削减-计数方法更有效，并采用模板拟合方法分析模型输出。 |
| [^20] | [F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2004.11145) | 本文提出了一个灵活完全分散化的演员-评论家多智能体强化学习框架，在大规模合作多智能体环境中，通过设计一个基于原始-对偶混合梯度下降的算法框架，可以分别学习每个智能体，并实现策略改进和价值评价。 |
| [^21] | [Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning.](http://arxiv.org/abs/2001.04413) | 提出了一种新的原则 "向后特征修正"，通过训练过程中自动修正较低层次特征的错误，使得深度学习能够进行深度（分层）学习。 |

# 详细

[^1]: 激励理论的贝叶斯推理在协作科学中的应用

    Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])

    [http://arxiv.org/abs/2307.03748](http://arxiv.org/abs/2307.03748)

    本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。

    

    当代科学研究是一项分布式的、协作的工作，由研究团队、监管机构、资助机构、商业合作伙伴和科学机构组成，彼此互动并面对不同的激励。为了保持科学严谨性，统计方法应该认识到这种情况。为此，我们研究了假设检验的情况，其中有一个代理人（例如研究人员或制药公司）对未知参数拥有私人先验知识，还有一个委托人（如政策制定者或监管机构）希望根据参数值做出决策。代理人根据他们的私人先验选择是否进行统计试验，然后试验的结果由委托人用来做出决策。我们展示了委托人如何进行统计推断，利用代理人的战略行为所透露的信息，也就是他们选择是否进行试验。具体来说，我们展示了如何计算p值，从而综合利用代理人的行为和试验的结果进行推理。

    Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the p
    
[^2]: GeoPhy: 利用几何梯度实现可微分的系统发育推断

    GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies. (arXiv:2307.03675v1 [cs.LG])

    [http://arxiv.org/abs/2307.03675](http://arxiv.org/abs/2307.03675)

    GeoPhy是一种创新的、完全可微的系统发育推断方法，通过在连续几何空间中表示拓扑分布，实现了可扩展的变分推断，克服了不限制拓扑结构的挑战。

    

    系统发育推断是在分子进化模型基础上进行的，它对于理解生物数据中的进化关系至关重要。考虑到进化树变量的不确定性，包括树拓扑结构和分支上的进化距离，对于准确地从分子数据中推断物种关系以及需要进行变量边缘化的任务来说至关重要。变分贝叶斯方法是开发可扩展、实用模型的关键，然而，在不限制可能的树拓扑结构的组合数的情况下进行系统发育推断仍然具有挑战性。在本研究中，我们引入了一种新颖的、完全可微的系统发育推断公式，利用连续几何空间中的拓扑分布来表示。通过对设计空间和渐近矩的实际考虑，我们的方法GeoPhy可以实现变分推断而不限制拓扑结构的多样性。

    Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topolo
    
[^3]: 在线网络源优化与图内核MAB

    Online Network Source Optimization with Graph-Kernel MAB. (arXiv:2307.03641v1 [cs.LG])

    [http://arxiv.org/abs/2307.03641](http://arxiv.org/abs/2307.03641)

    提出了Grab-UCB算法用于在线学习大规模网络中最优的源放置位置，通过使用自适应的图词典模型来描述网络过程，并利用稀疏谱表示实现了数据高效的学习框架。

    

    我们提出了Grab-UCB，一种基于图内核的多臂赌博算法，用于在线学习在大规模网络中最优的源放置位置，以最大化先验未知网络过程所获得的奖励。由于不确定性，需要在线学习，然而这在维度灾难中受到了影响。为了实现样本效率，我们使用自适应的图词典模型来描述网络过程，这通常导致稀疏的谱表示。这使得数据高效的学习框架成为可能，其学习率与谱表示模型的维度相关，而不是网络的维度。然后，我们提出了Grab-UCB，一种在线顺序决策策略，它在优化行动策略的同时学习谱表示的参数。我们推导出了与网络参数相关的性能保证，这进一步影响顺序决策策略的学习曲线。

    We propose Grab-UCB, a graph-kernel multi-arms bandit algorithm to learn online the optimal source placement in large scale networks, such that the reward obtained from a priori unknown network processes is maximized. The uncertainty calls for online learning, which suffers however from the curse of dimensionality. To achieve sample efficiency, we describe the network processes with an adaptive graph dictionary model, which typically leads to sparse spectral representations. This enables a data-efficient learning framework, whose learning rate scales with the dimension of the spectral representation model instead of the one of the network. We then propose Grab-UCB, an online sequential decision strategy that learns the parameters of the spectral representation while optimizing the action strategy. We derive the performance guarantees that depend on network parameters, which further influence the learning curve of the sequential decision strategy We introduce a computationally simplifie
    
[^4]: BOF-UCB: 一种用于非平稳环境下的上下界信心算法的贝叶斯优化频率算法

    BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v1 [cs.LG])

    [http://arxiv.org/abs/2307.03587](http://arxiv.org/abs/2307.03587)

    BOF-UCB是一种用于非平稳环境下的背景线性赌博机的贝叶斯优化频率算法，其结合了贝叶斯和频率学派原则，提高了在动态环境中的性能。它利用贝叶斯更新推断后验分布，并使用频率学派方法计算上界信心界以平衡探索和开发。实验证明，BOF-UCB优于现有方法，是非平稳环境中顺序决策的有前途的解决方案。

    

    我们提出了一种新颖的贝叶斯优化频率上下界信心算法（BOF-UCB），用于非平稳环境下的随机背景线性赌博机。贝叶斯和频率学派原则的独特结合增强了算法在动态环境中的适应性和性能。BOF-UCB算法利用顺序贝叶斯更新推断未知回归参数的后验分布，并随后采用频率学派方法通过最大化后验分布上的期望收益来计算上界信心界（UCB）。我们提供了BOF-UCB性能的理论保证，并在合成数据集和强化学习环境中的经典控制任务中展示了其有效性。我们的结果表明，BOF-UCB优于现有的方法，在非平稳环境中进行顺序决策是一个有前途的解决方案。

    We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound (BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary environments. This unique combination of Bayesian and frequentist principles enhances adaptability and performance in dynamic settings. The BOF-UCB algorithm utilizes sequential Bayesian updates to infer the posterior distribution of the unknown regression parameter, and subsequently employs a frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing the expected reward over the posterior distribution. We provide theoretical guarantees of BOF-UCB's performance and demonstrate its effectiveness in balancing exploration and exploitation on synthetic datasets and classical control tasks in a reinforcement learning setting. Our results show that BOF-UCB outperforms existing methods, making it a promising solution for sequential decision-making in non-stationary environments.
    
[^5]: 平滑边缘：利用Hadamard超参数化在稀疏正则化的平滑优化中的一般框架

    Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])

    [http://arxiv.org/abs/2307.03571](http://arxiv.org/abs/2307.03571)

    本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。

    

    本文介绍了一种用于（结构化）稀疏正则化问题中的$\ell_q$和$\ell_{p,q}$正则化的平滑方法。这些非平滑且可能非凸的问题的优化通常依赖于专门的过程。相比之下，我们的一般框架与主流的一阶优化方法（如随机梯度下降和加速变体）兼容，无需任何修改。这是通过平滑优化转移实现的，其中选定模型参数的超参数化使用Hadamard乘积和惩罚的改变。在超参数问题中，通过用替代参数进行平滑和凸性的$\ell_2$正则化，能够在原始参数化中引入非平滑和非凸性的$\ell_q$或$\ell_{p,q}$正则化。我们证明了我们的方法不仅能够得到匹配的全局最小值，还能得到等价的局部最小值。这在非凸稀疏正则化中尤其有用，因为在这种情况下找到全局最小值非常困难。

    This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
    
[^6]: MALIBO: 元学习应用于无似然贝叶斯优化

    MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])

    [http://arxiv.org/abs/2307.03565](http://arxiv.org/abs/2307.03565)

    MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。

    

    贝叶斯优化是一种优化昂贵黑盒函数的流行方法。传统的贝叶斯优化会从头开始优化每个新的目标任务，而元学习则是利用相关任务的知识来更快地优化新任务的一种方式。然而，现有的元学习贝叶斯优化方法依赖于标准模型，这些模型存在可伸缩性问题，并且对不同任务之间观察数据的尺度和噪声类型非常敏感。此外，它们常常忽视与任务相似性相关的不确定性，这导致在仅有有限观察数据或新任务与相关任务差异显著时，任务适应性不可靠。为了解决这些限制，我们提出了一种新颖的元学习贝叶斯优化方法，旨在绕开标准模型，直接学习跨任务的查询效用。我们的方法明确建模任务的不确定性，并引入了一个辅助模型，使其能够对新任务进行稳健适应。大量实验证明了我们方法的有效性。

    Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that ou
    
[^7]: 分布回归的神经网络学习理论

    Learning Theory of Distribution Regression with Neural Networks. (arXiv:2307.03487v1 [stat.ML])

    [http://arxiv.org/abs/2307.03487](http://arxiv.org/abs/2307.03487)

    本文提出了一个全连接神经网络框架用于分布回归问题，解决了传统神经网络无法直接处理概率分布输入的困难，并建立了逼近理论和学习理论。

    

    本文旨在通过全连接神经网络（FNN）建立分布回归的逼近理论和学习理论。与传统回归方法不同的是，分布回归的输入变量是概率测度。然后我们常常需要进行二阶段抽样过程来近似分布的实际信息。另一方面，传统的神经网络结构要求输入变量为向量。当输入样本是概率分布时，传统的深度神经网络方法无法直接使用，分布回归问题变得困难。因此，一个对于分布输入进行明确定义的神经网络结构是非常需求的。关于分布回归的神经网络实现没有数学模型和理论分析。为了克服技术难题并解决这个问题，我们建立了一个新颖的全连接神经网络框架。

    In this paper, we aim at establishing an approximation theory and a learning theory of distribution regression via a fully connected neural network (FNN). In contrast to the classical regression methods, the input variables of distribution regression are probability measures. Then we often need to perform a second-stage sampling process to approximate the actual information of the distribution. On the other hand, the classical neural network structure requires the input variable to be a vector. When the input samples are probability distributions, the traditional deep neural network method cannot be directly used and the difficulty arises for distribution regression. A well-defined neural network structure for distribution inputs is intensively desirable. There is no mathematical model and theoretical analysis on neural network realization of distribution regression. To overcome technical difficulties and address this issue, we establish a novel fully connected neural network framework
    
[^8]: 动态实现的Hamiltonian Monte Carlo和No U-Turn Samplers的收敛性

    On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers. (arXiv:2307.03460v1 [stat.CO])

    [http://arxiv.org/abs/2307.03460](http://arxiv.org/abs/2307.03460)

    本文研究了动态实现的Hamiltonian Monte Carlo (HMC)算法和No U-Turn Sampler (NUTS) 的收敛性，证明了NUTS作为动态HMC的特例，并且在一定条件下具有遍历性和几何遍历性。同时改进了HMC的收敛性结果，证明了在目标分布为高斯分布的微扰情况下，无需任何有界条件，HMC也是遍历的。

    

    针对动态实现的Hamiltonian Monte Carlo (HMC)算法，例如No U-Turn Sampler (NUTS)，在许多具有挑战性的推理问题中具有成功的经验证据，但关于它们行为的理论结果还不足。本文旨在填补这一空白。具体而言，我们考虑了一个称为动态HMC的通用MCMC算法类。我们证明了这个通用框架涵盖了NUTS作为一个特例，并且作为一个附带结果，证明了目标分布的不变性。其次，我们建立了使NUTS不可约和非周期的条件，并作为推论而证明了遍历性。在类似于HMC的条件下，我们还证明了NUTS具有几何遍历性。最后，我们改进了现有的HMC收敛性结果，证明了这个方法在目标分布是高斯分布的微扰的情况下，无需对步长和leapfrog步数进行任何有界条件，也是遍历的。

    There is substantial empirical evidence about the success of dynamic implementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler (NUTS), in many challenging inference problems but theoretical results about their behavior are scarce. The aim of this paper is to fill this gap. More precisely, we consider a general class of MCMC algorithms we call dynamic HMC. We show that this general framework encompasses NUTS as a particular case, implying the invariance of the target distribution as a by-product. Second, we establish conditions under which NUTS is irreducible and aperiodic and as a corrolary ergodic. Under conditions similar to the ones existing for HMC, we also show that NUTS is geometrically ergodic. Finally, we improve existing convergence results for HMC showing that this method is ergodic without any boundedness condition on the stepsize and the number of leapfrog steps, in the case where the target is a perturbation of a Gaussian distribution.
    
[^9]: 可扩展高维多变量线性回归用于特征分布式数据翻译标题

    Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data. (arXiv:2307.03410v1 [stat.ML])

    [http://arxiv.org/abs/2307.03410](http://arxiv.org/abs/2307.03410)

    这篇论文提出了一个适用于特征分布式数据的可扩展高维多变量线性回归算法，具有通信复杂度不依赖于特征维度和快速收敛性的优势，可应用于大规模数据集和具有多变量响应变量的场景。

    

    特征分布式数据是指根据特征划分并存储在多个计算节点上的数据，在具有大量特征的应用中越来越常见。本文提出了一个适用于这种数据的两阶段放松贪婪算法 (TSRGA)，用于应用多变量线性回归。TSRGA 的主要优势在于其通信复杂度不依赖于特征维度，使其能够高度扩展到非常大的数据集。此外，对于多变量响应变量，TSRGA 可用于产生低秩系数估计。通过模拟实验证明了TSRGA 的快速收敛性。最后，我们将提出的TSRGA 应用于一种金融应用中，利用来自 10-K 报告的非结构化数据，证明了其在具有许多密集大维矩阵的应用中的实用性。

    Feature-distributed data, referred to data partitioned by features and stored across multiple computing nodes, are increasingly common in applications with a large number of features. This paper proposes a two-stage relaxed greedy algorithm (TSRGA) for applying multivariate linear regression to such data. The main advantage of TSRGA is that its communication complexity does not depend on the feature dimension, making it highly scalable to very large data sets. In addition, for multivariate response variables, TSRGA can be used to yield low-rank coefficient estimates. The fast convergence of TSRGA is validated by simulation experiments. Finally, we apply the proposed TSRGA in a financial application that leverages unstructured data from the 10-K reports, demonstrating its usefulness in applications with many dense large-dimensional matrices.
    
[^10]: 随机组合梯度下降算法的稳定性和泛化

    Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v1 [cs.LG])

    [http://arxiv.org/abs/2307.03357](http://arxiv.org/abs/2307.03357)

    本文通过统计学习理论的算法稳定性，分析了随机组合梯度下降算法的稳定性和泛化性，引入了组合一致稳定性概念并与SCO问题的泛化性建立了定量关系。

    

    许多机器学习任务可以被形式化为随机组合优化（SCO）问题，例如强化学习、AUC最大化和元学习，其中目标函数涉及与期望相关的嵌套组合。虽然已经有大量研究致力于研究SCO算法的收敛行为，但对于它们的泛化性能如何，即从训练示例构建的学习算法在未来的测试示例上的行为如何，却很少有研究。在本文中，我们通过统计学习理论框架下的算法稳定性，提供了随机组合梯度下降算法的稳定性和泛化性分析。首先，我们引入了一种稳定性概念，称为组合一致稳定性，并建立了它与SCO问题的泛化性之间的定量关系。然后，我们为两种流行的随机组合优化问题建立了组合一致稳定性结果。

    Many machine learning tasks can be formulated as a stochastic compositional optimization (SCO) problem such as reinforcement learning, AUC maximization, and meta-learning, where the objective function involves a nested composition associated with an expectation. While a significant amount of studies has been devoted to studying the convergence behavior of SCO algorithms, there is little work on understanding their generalization, i.e., how these learning algorithms built from training examples would behave on future test examples. In this paper, we provide the stability and generalization analysis of stochastic compositional gradient descent algorithms through the lens of algorithmic stability in the framework of statistical learning theory. Firstly, we introduce a stability concept called compositional uniform stability and establish its quantitative relation with generalization for SCO problems. Then, we establish the compositional uniform stability results for two popular stochastic
    
[^11]: 使用对抗模型量化不确定性

    Quantification of Uncertainty with Adversarial Models. (arXiv:2307.03217v1 [cs.LG])

    [http://arxiv.org/abs/2307.03217](http://arxiv.org/abs/2307.03217)

    该论文提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。与先前的方法相比，QUAM对认知不确定性的近似误差更小。

    

    在实际应用中，量化不确定性对于可操作的预测非常重要。预测不确定性的关键在于估计认知不确定性，它被定义为一个散度函数和后验的乘积的积分。当前的方法如Deep Ensembles或MC dropout在估计认知不确定性方面表现不佳，因为它们主要考虑后验在采样模型时。我们提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。因此，与先前的方法相比，QUAM对认知不确定性的近似误差更小。乘积较大的模型对应于对抗模型（不是对抗性示例！）。对抗模型既有较高的后验，也有其预测与其他模型之间的较高差异。

    Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of
    
[^12]: 高阶网络中的度异质性：超图β模型的推断

    Degree Heterogeneity in Higher-Order Networks: Inference in the Hypergraph $\boldsymbol{\beta}$-Model. (arXiv:2307.02818v1 [math.ST])

    [http://arxiv.org/abs/2307.02818](http://arxiv.org/abs/2307.02818)

    本文对具有多层的超图β模型进行了研究，推导了最大似然估计的收敛速率和极限分布，并构建了模型参数的置信区间。同时，我们还建立了超图β模型中似然比检验的渐近正态性。

    

    随机图中的β模型通常用于表示具有度异质性的网络中的配对交互。超图β模型超越了配对交互，Stasi等人于2014年引入了超图β模型，用于捕捉具有高阶（多向）交互的网络中的度异质性。本文首次对具有多层的超图β模型进行了严格研究，它允许在不同层次中存在不同大小的超边。首先，我们推导了最大似然（ML）估计的收敛速率，并确定了它们的最小极小速率。我们还推导了ML估计的极限分布，并构建了模型参数的渐近有效置信区间。接下来，我们考虑了超图β模型中的拟合优度问题。具体而言，我们在零假设下建立了似然比（LR）检验的渐近正态性。

    The $\boldsymbol{\beta}$-model for random graphs is commonly used for representing pairwise interactions in a network with degree heterogeneity. Going beyond pairwise interactions, Stasi et al. (2014) introduced the hypergraph $\boldsymbol{\beta}$-model for capturing degree heterogeneity in networks with higher-order (multi-way) interactions. In this paper we initiate the rigorous study of the hypergraph $\boldsymbol{\beta}$-model with multiple layers, which allows for hyperedges of different sizes across the layers. To begin with, we derive the rates of convergence of the maximum likelihood (ML) estimate and establish their minimax rate optimality. We also derive the limiting distribution of the ML estimate and construct asymptotically valid confidence intervals for the model parameters. Next, we consider the goodness-of-fit problem in the hypergraph $\boldsymbol{\beta}$-model. Specifically, we establish the asymptotic normality of the likelihood ratio (LR) test under the null hypothe
    
[^13]: 非凸优化中的自适应策略

    Adaptive Strategies in Non-convex Optimization. (arXiv:2306.10278v1 [cs.LG])

    [http://arxiv.org/abs/2306.10278](http://arxiv.org/abs/2306.10278)

    本文介绍了应用于随机优化和深度神经网络训练中的自适应算法。算法可以自动适应不同的噪声水平和梯度比例范围，从而达到（近似）最优速度。

    

    如果算法不需要先验知识就可以表现得与知道这个问题特定参数的算法相竞争，那么就说算法对某个参数是自适应的。本论文介绍了我们在以下场景中开发自适应算法的工作：1. 在随机优化环境中，我们只收到随机梯度，并且评估这些梯度的噪声水平极大地影响了收敛速度。通常需要调整噪声水平才能实现最优速率，而我们开发了自适应算法，可以在不知道噪声范围的情况下自动保证（近似）最优速率。2. 在训练深度神经网络时，每个坐标轴上的梯度大小比例可以散布在非常广的范围内，除非采用像BatchNorm这样的归一化技术。在这种情况下，不考虑梯度比例问题的算法可能表现非常差。

    An algorithm is said to be adaptive to a certain parameter (of the problem) if it does not need a priori knowledge of such a parameter but performs competitively to those that know it. This dissertation presents our work on adaptive algorithms in following scenarios: 1. In the stochastic optimization setting, we only receive stochastic gradients and the level of noise in evaluating them greatly affects the convergence rate. Tuning is typically required when without prior knowledge of the noise scale in order to achieve the optimal rate. Considering this, we designed and analyzed noise-adaptive algorithms that can automatically ensure (near)-optimal rates under different noise scales without knowing it. 2. In training deep neural networks, the scales of gradient magnitudes in each coordinate can scatter across a very wide range unless normalization techniques, like BatchNorm, are employed. In such situations, algorithms not addressing this problem of gradient scales can behave very poor
    
[^14]: 在线多类可学习性的刻画。

    A Characterization of Online Multiclass Learnability. (arXiv:2303.17716v1 [cs.LG])

    [http://arxiv.org/abs/2303.17716](http://arxiv.org/abs/2303.17716)

    在线多类学习问题中，使用Multiclass Littlestone维度可以刻画标签数目为无界情况下的可学习性。

    

    我们考虑当标签数目是无界的时候在线多类学习问题。我们展示了Multiclass Littlestone维度，这个概念首次出现在\cite{DanielyERMprinciple}中，继续刻画了该场景下的在线可学习性。我们的结果补充了最近的工作，\cite{Brukhimetal2022}给出了当标签空间是无界的情况下批处理多类可学习性的刻画。

    We consider the problem of online multiclass learning when the number of labels is unbounded. We show that the Multiclass Littlestone dimension, first introduced in \cite{DanielyERMprinciple}, continues to characterize online learnability in this setting. Our result complements the recent work by \cite{Brukhimetal2022} who give a characterization of batch multiclass learnability when the label space is unbounded.
    
[^15]: 连续时间功能扩散过程

    Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00800](http://arxiv.org/abs/2303.00800)

    连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。

    

    我们引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。 FDPs需要一种新的数学框架来描述前向和反向动力学，并进行多个扩展以得出实际的训练目标。这些扩展包括Girsanov定理的无限维版本，以便能够计算ELBO，以及采样定理的无限维版本，以确保可数个点上的函数评估等价于无限维函数。我们使用FDPs在函数空间中构建了一种新型生成模型，不需要专门的网络架构，并且可以处理任何类型的连续数据。我们在真实数据上的结果显示，使用简单的多层感知机（MLP）结构，FDPs实现了高质量的图像生成，所需的参数数量比现有的扩散模型低几个数量级。

    We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.
    
[^16]: 黑盒批量主动学习回归

    Black-Box Batch Active Learning for Regression. (arXiv:2302.08981v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08981](http://arxiv.org/abs/2302.08981)

    本文提出了一种黑盒批量主动学习方法，该方法仅利用模型预测进行评估，适用于各种机器学习模型，并扩展了一系列白盒批量主动学习方法。

    

    批量主动学习是一种常用的方法，可以通过重复获取数据点的标签来高效地训练机器学习模型，尤其是对于大规模的初始未标记数据集。然而，许多最近的批量主动学习方法都是白盒方法，并且通常仅限于可微分的参数模型：它们使用基于模型嵌入或一阶和二阶导数的获取函数对未标记的数据点进行评分。在本文中，我们提出了一种黑盒批量主动学习回归任务的方法，作为白盒方法的扩展。关键的是，我们的方法仅依赖于模型的预测。这种方法适用于各种机器学习模型，包括常规的和贝叶斯深度学习模型以及非可微分模型，如随机森林。它基于贝叶斯原则，并利用最近的基于核的方法。这使得我们能够扩展一系列现有的最先进的白盒批量主动学习方法（BADGE）。

    Batch active learning is a popular approach for efficiently training machine learning models on large, initially unlabelled datasets by repeatedly acquiring labels for batches of data points. However, many recent batch active learning methods are white-box approaches and are often limited to differentiable parametric models: they score unlabeled points using acquisition functions based on model embeddings or first- and second-order derivatives. In this paper, we propose black-box batch active learning for regression tasks as an extension of white-box approaches. Crucially, our method only relies on model predictions. This approach is compatible with a wide range of machine learning models, including regular and Bayesian deep learning models and non-differentiable models such as random forests. It is rooted in Bayesian principles and utilizes recent kernel-based approaches. This allows us to extend a wide range of existing state-of-the-art white-box batch active learning methods (BADGE,
    
[^17]: 分布式潜变量模型的联合变分推断方法

    Federated Variational Inference Methods for Structured Latent Variable Models. (arXiv:2302.03314v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03314](http://arxiv.org/abs/2302.03314)

    本论文提出了一种基于结构化变分推断的联合学习方法，该方法适用于各种结构化概率模型。此外，还提供了一种通信高效的变体算法，证明了所提出算法的有效性和性能。

    

    联邦学习方法使得在数据分布的情况下进行模型训练成为可能，而无需数据离开其原始位置，并且在各个领域引起了越来越多的关注。然而，现有方法存在局限性，无法应用于许多结构化概率模型。我们提出了一种基于结构化变分推断的通用而优雅的解决方案，并针对联邦学习环境进行了适应。此外，我们提供了一种类似于经典FedAvg算法的通信高效型变体。通过与分层贝叶斯神经网络和主题模型进行比较，我们证明了所提出算法的有效性和性能。

    Federated learning methods enable model training across distributed data sources without data leaving their original locations and have gained increasing interest in various fields. However, existing approaches are limited, excluding many structured probabilistic models. We present a general and elegant solution based on structured variational inference, widely used in Bayesian machine learning, adapted for the federated setting. Additionally, we provide a communication-efficient variant analogous to the canonical FedAvg algorithm. The proposed algorithms' effectiveness is demonstrated, and their performance is compared with hierarchical Bayesian neural networks and topic models.
    
[^18]: 使用异方差噪声模型识别特定患者的根本原因

    Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model. (arXiv:2205.13085v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.13085](http://arxiv.org/abs/2205.13085)

    本研究使用异方差噪声模型，开发了一种名为广义根因因果推断（GRCI）的算法，能够更准确地识别疾病的患者特定根本原因。

    

    复杂疾病由许多因素引起，即使在相同的诊断类别中，这些因素可能在患者之间也会有所不同。然而，一些潜在的根本原因仍然可能在每个患者中引发疾病的发展。因此，我们专注于识别疾病的患者特定根本原因，即在结构方程模型中外生误差项的样本特定预测能力。我们从线性设置推广到异方差噪声模型，其中$ Y = m（X）+ \varepsilon\sigma（X）$，其中$m（X）$和$\sigma（X）$分别代表条件均值和均值绝对偏差的非线性函数。该模型保持可识别性，但引入了一些非平凡的挑战，需要一种名为广义根因因果推断（GRCI）的定制算法来正确提取误差项。与现有的替代方法相比，GRCI更准确地恢复了特定患者的根本原因。

    Complex diseases are caused by a multitude of factors that may differ between patients even within the same diagnostic category. A few underlying root causes may nevertheless initiate the development of disease within each patient. We therefore focus on identifying patient-specific root causes of disease, which we equate to the sample-specific predictivity of the exogenous error terms in a structural equation model. We generalize from the linear setting to the heteroscedastic noise model where $Y = m(X) + \varepsilon\sigma(X)$ with non-linear functions $m(X)$ and $\sigma(X)$ representing the conditional mean and mean absolute deviation, respectively. This model preserves identifiability but introduces non-trivial challenges that require a customized algorithm called Generalized Root Causal Inference (GRCI) to extract the error terms correctly. GRCI recovers patient-specific root causes more accurately than existing alternatives.
    
[^19]: 利用机器学习提高Sneutrino探测效率——在小信号场景中超越削减方法

    Beyond Cuts in Small Signal Scenarios -- Enhanced Sneutrino Detectability Using Machine Learning. (arXiv:2108.03125v3 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2108.03125](http://arxiv.org/abs/2108.03125)

    本研究提出一种利用机器学习的方式，在信号与背景高度重叠的情况下增强LHC新物理探测的灵敏度。使用XGBoost和深度神经网络模型来利用可观测量之间的相关性比传统的削减-计数方法更有效，并采用模板拟合方法分析模型输出。

    

    本文提出了一种利用机器学习增强LHC新物理搜索灵敏度的方法，特别是在背景支配和信号与背景在可观测量上存在高度重叠的情况下。我们使用了两种不同的模型，XGBoost和深度神经网络，利用可观测量之间的相关性，并将这种方法与传统的削减-计数方法进行了比较。我们考虑了不同的方法来分析模型的输出，并发现模板拟合通常比简单的削减方法表现更好。通过Shapley分解，我们获得了对事件运动学和机器学习模型输出之间关系的额外洞察。我们以具体的亚稳Sneutrino超对称情景为例，但该方法可以应用于更广泛的模型类别中。

    We investigate enhancing the sensitivity of new physics searches at the LHC by machine learning in the case of background dominance and a high degree of overlap between the observables for signal and background. We use two different models, XGBoost and a deep neural network, to exploit correlations between observables and compare this approach to the traditional cut-and-count method. We consider different methods to analyze the models' output, finding that a template fit generally performs better than a simple cut. By means of a Shapley decomposition, we gain additional insight into the relationship between event kinematics and the machine learning model output. We consider a supersymmetric scenario with a metastable sneutrino as a concrete example, but the methodology can be applied to a much wider class of models.
    
[^20]: F2A2: 灵活完全去中心化的合作多智能体强化学习的近似演员-评论家算法

    F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. (arXiv:2004.11145v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.11145](http://arxiv.org/abs/2004.11145)

    本文提出了一个灵活完全分散化的演员-评论家多智能体强化学习框架，在大规模合作多智能体环境中，通过设计一个基于原始-对偶混合梯度下降的算法框架，可以分别学习每个智能体，并实现策略改进和价值评价。

    

    传统的中央集权的多智能体强化学习算法在复杂应用中有时不实用，因为智能体之间缺乏互动，存在维度灾难和计算复杂性。因此，出现了一些分散化的多智能体强化学习算法。然而，现有的分散化方法只能处理完全合作的设置，在训练过程中需要传输大量信息。传统的块坐标梯度下降方法可以简化计算，但会引起严重的偏差。本文提出了一个灵活的完全分散化的演员-评论家多智能体强化学习框架，可以组合大多数演员-评论家方法，并处理大规模一般合作多智能体环境。设计了一种基于原始-对偶混合梯度下降的算法框架，分别学习每个智能体来实现分散化。从每个智能体的角度来看，实现了策略改进和价值评价。

    Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluatio
    
[^21]: 《向后特征修正：深度学习如何进行深度（分层）学习》

    Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. (arXiv:2001.04413v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.04413](http://arxiv.org/abs/2001.04413)

    提出了一种新的原则 "向后特征修正"，通过训练过程中自动修正较低层次特征的错误，使得深度学习能够进行深度（分层）学习。

    

    深度学习也被称为分层学习，学习者通过将复杂的目标函数分解为一系列更简单的函数来降低样本和时间复杂度。本文通过对训练目标进行随机梯度下降，从理论上分析了多层神经网络如何有效和自动地进行这种分层学习。在概念上，我们提出了一种理论特征，即某些类型的深度（即超常层）神经网络在某些分层任务上仍然可以以高效率的样本和时间进行训练，而现有的算法（包括逐层训练、核方法等）均无法高效。我们建立了一种名为“向后特征修正”的新原则，在训练过程中，较低层次特征的错误可以自动修正。我们认为这是深度学习如何进行深度（分层）学习的关键所在。

    Deep learning is also known as hierarchical learning, where the learner _learns_ to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning _efficiently_ and _automatically_ by SGD on the training objective.  On the conceptual side, we present a theoretical characterizations of how certain types of deep (i.e. super-constant layer) neural networks can still be sample and time efficiently trained on some hierarchical tasks, when no existing algorithm (including layerwise training, kernel method, etc) is known to be efficient. We establish a new principle called "backward feature correction", where the errors in the lower-level features can be automatically corrected when training together with the higher-level layers. We believe this is a key behind how deep learning is performing deep (hierarchical) learning, as 
    

