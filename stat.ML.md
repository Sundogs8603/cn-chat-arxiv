# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles.](http://arxiv.org/abs/2307.03176) | 通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。 |
| [^2] | [Multiplicative Updates for Online Convex Optimization over Symmetric Cones.](http://arxiv.org/abs/2307.03136) | 本文研究了在线凸优化问题，使用了对称锥乘法权重更新算法(SCMWU)，该算法在任意对称锥的迹为一处进行在线优化，并且在实验证明了其是无悔算法。 |
| [^3] | [Beyond Intuition, a Framework for Applying GPs to Real-World Data.](http://arxiv.org/abs/2307.03093) | 提出了一个框架，用于确定高斯过程在实际问题中的适用性，并建立一个稳健且明确的模型。通过对核函数设计和计算可扩展性选项的指导，该框架在冰川高程变化的案例研究中实现了更准确的结果。 |
| [^4] | [PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models.](http://arxiv.org/abs/2307.03034) | 本文研究了一种一般观测模型下的不安定多臂赌博机问题，提出了PCL-可索引性和Whittle索引的分析方法，并通过近似过程将问题转化为有限状态问题。数值实验表明算法表现优秀。 |
| [^5] | [Computable Stability for Persistence Rank Function Machine Learning.](http://arxiv.org/abs/2307.02904) | 本文讨论了持续性同调秩函数在拓扑数据分析中的应用，并指出秩函数相对于条码的优势在于更易于计算，并且可以直接应用函数数据分析的方法。然而，秩函数的稳定性问题尚待解决。 |
| [^6] | [Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight.](http://arxiv.org/abs/2307.02884) | 本文研究了在部分可观察的马尔可夫决策过程（POMDPs）中学习的样本高效性，提出了一个增强的反馈模型，利用事后多观察数据实现了对两种新的POMDP子类的样本高效学习。 |
| [^7] | [Degree Heterogeneity in Higher-Order Networks: Inference in the Hypergraph $\boldsymbol{\beta}$-Model.](http://arxiv.org/abs/2307.02818) | 本文对具有多层的超图β模型进行了研究，推导了最大似然估计的收敛速率和极限分布，并构建了模型参数的置信区间。同时，我们还建立了超图β模型中似然比检验的渐近正态性。 |
| [^8] | [When Does Confidence-Based Cascade Deferral Suffice?.](http://arxiv.org/abs/2307.02764) | 本研究旨在探讨何时基于置信度的级联延迟可能失败，以及何时备选的延迟策略可能表现更好。通过理论分析和实验证明事后延迟机制能够显著提高性能。 |
| [^9] | [ALPCAH: Sample-wise Heteroscedastic PCA with Tail Singular Value Regularization.](http://arxiv.org/abs/2307.02745) | 本文提出了一种新的PCA方法，可以估计样本的噪声方差，从而改进与数据的低秩结构相关的子空间基础的估计值。 |
| [^10] | [Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?.](http://arxiv.org/abs/2307.02732) | 本文首次对任务级别的评估进行了研究，发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。 |
| [^11] | [Understanding Uncertainty Sampling.](http://arxiv.org/abs/2307.02719) | 本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。 |
| [^12] | [Kernels, Data & Physics.](http://arxiv.org/abs/2307.02693) | 该论文主要介绍了NTK方法在机器学习问题中的应用，通过找到可处理的内核表达形式来解决一般无法解决的问题，重点讨论了数据精炼和对抗鲁棒性等实际应用。 |
| [^13] | [Panel Data Nowcasting: The Case of Price-Earnings Ratios.](http://arxiv.org/abs/2307.02673) | 本文使用结构化机器学习回归方法对面板数据进行实时预测，针对混合频率的时间序列数据结构提出了稀疏组 LASSO 正则化方法，并且在实证结果中显示出优于其他方法的表现。 |
| [^14] | [A Complete Characterisation of Structured Missingness.](http://arxiv.org/abs/2307.02650) | 提出了一种用于描述结构缺失的分类体系，其中每个缺失指示向量可以依赖于除自身之外的所有缺失指示向量和数据矩阵。将这个新框架嵌入到已有的MCAR、MAR和MNAR机制分解中。 |
| [^15] | [Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation.](http://arxiv.org/abs/2307.02598) | 这篇论文解决了表示学习中的潜变量识别和"支持外"图像生成问题，展示了加法解码器能够对潜变量进行识别，并提供了理论依据支持这种方法的有效性。 |
| [^16] | [Conditional independence testing under model misspecification.](http://arxiv.org/abs/2307.02520) | 该论文研究了模型错误下的条件独立性检验，在这种情况下提出了新的近似或上界来衡量基于回归的测试的测试误差，并引入了一种新颖的基于回归的CI检验方法RBPT，对模型错误具有鲁棒性。 |
| [^17] | [Generalization Guarantees via Algorithm-dependent Rademacher Complexity.](http://arxiv.org/abs/2307.02501) | 提出了一种通过算法和数据相关的假设类的经验Rademacher复杂度来控制泛化错误的方法，基于有限分形维度获得了新的界限，并简化了对随机梯度下降的无维度泛化界限的证明。 |
| [^18] | [Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision.](http://arxiv.org/abs/2306.16564) | 本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。 |
| [^19] | [Active Policy Improvement from Multiple Black-box Oracles.](http://arxiv.org/abs/2306.10259) | 本研究提出了MAPS和MAPS-SE两个算法，可在多黑盒预言情况下，采用模仿学习并主动选择和改进最优预言，显著提升了性能。 |
| [^20] | [Fairness in Multi-Task Learning via Wasserstein Barycenters.](http://arxiv.org/abs/2306.10155) | 本文提出了一种方法，通过多元Wasserstein barycenters扩展`Strong Demographic Parity`的定义，实现多任务学习中的公平性，包括回归和二元分类任务。在实验中表现出良好的效果。 |
| [^21] | [On the Noise Sensitivity of the Randomized SVD.](http://arxiv.org/abs/2305.17435) | 通过对R-SVD在低秩信号加噪声测量模型下的分析，证明了当信噪比(SNR)超过某个依赖于降维因子的可检测门限时，R-SVD产生的最大奇异值是一个离群值；在门限以下，没有离群值从奇异值块中产生 |
| [^22] | [Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression.](http://arxiv.org/abs/2303.02118) | 本研究研究了混合稀疏线性回归问题，在实际应用中发现了统计和计算之间的权衡关系，并确定了样本复杂度和运行时间之间的平滑信息-计算权衡关系。 |
| [^23] | [A Finite-Particle Convergence Rate for Stein Variational Gradient Descent.](http://arxiv.org/abs/2211.09721) | 本文提供了Stein变分梯度下降算法的有限粒子收敛速度，证明了当目标分布为次高斯且具有Lipschitz积分核时，使用适当的步长序列和粒子数量，可以以1/√(log log n)的速度将核Stein差异逼近零。 |
| [^24] | [Modeling Content Creator Incentives on Algorithm-Curated Platforms.](http://arxiv.org/abs/2206.13102) | 该论文讨论了在线平台上内容创作者激励机制的建模，通过分析算法选择对曝光游戏（包括现代分解和两塔架构）中（纳什）均衡的影响，提出了使用曝光游戏模型进行预部署审计的方法，以识别期望和激励内容之间的不匹配。 |
| [^25] | [Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations.](http://arxiv.org/abs/2206.04779) | 该论文研究了离线强化学习从视觉观察中的挑战和机遇，针对这一复杂领域建立了视觉领域中连续控制的简单基准，并设计了一系列基准任务，以更好地表示现实世界离线RL问题中的数据分布，并通过对两种基于视觉的在线强化学习算法的简单修改进行评估。 |
| [^26] | [Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach.](http://arxiv.org/abs/2203.00126) | 本文提出了一种用于从高维噪声数据中学习低维非线性结构的算法，该算法使用自适应带宽选择过程，并获得了理论上的收敛性证明。算法的低维嵌入结果可用于数据可视化、聚类和预测等任务。 |
| [^27] | [Descriptive vs. inferential community detection in networks: pitfalls, myths, and half-truths.](http://arxiv.org/abs/2112.00183) | 本文将现有的社区检测方法根据目标分为描述性和推理性。描述性方法基于上下文相关的社区结构发现网络模式，而推理性方法通过生成模型拟合数据，揭示网络形成机制并分离结构和随机性。 |
| [^28] | [The computational asymptotics of Gaussian variational inference and the Laplace approximation.](http://arxiv.org/abs/2104.05886) | 本论文分析了高斯变分推断和拉普拉斯近似的渐近凸性特性，并提出了两个算法（CLA和CSVI）来利用这些特性。 |

# 详细

[^1]: 异构特征子采样的Ridge Ensemble的学习曲线

    Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])

    [http://arxiv.org/abs/2307.03176](http://arxiv.org/abs/2307.03176)

    通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。

    

    特征包装是一种旨在通过在随机子样本或特征投影上训练估计器来减少预测方差的成熟集成方法。通常，集成选择是同质的，即估计器可用的特征维数在整个集成中是均匀的。在这里，我们介绍了异构特征集成方法，其中的估计器基于变动的特征维数，并研究其在线性回归设置中的性能。我们研究了一个线性预测器的集成，每个预测器使用部分可用特征进行岭回归拟合。我们允许这些子集中包含的特征数量有所变化。利用统计物理中的复制技巧，我们推导了具有确定性线性掩模的岭回归集成的学习曲线。对于具有各向同性特征噪声的等相相关数据，我们得到了学习曲线的显式表达式。利用这些推导表达式，我们研究了集成在不同特征维数下的性能。

    Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
    
[^2]: 在对称锥上进行的在线凸优化的乘法更新

    Multiplicative Updates for Online Convex Optimization over Symmetric Cones. (arXiv:2307.03136v1 [math.OC])

    [http://arxiv.org/abs/2307.03136](http://arxiv.org/abs/2307.03136)

    本文研究了在线凸优化问题，使用了对称锥乘法权重更新算法(SCMWU)，该算法在任意对称锥的迹为一处进行在线优化，并且在实验证明了其是无悔算法。

    

    我们研究了在线凸优化问题，其中可能的操作是对称锥中的迹为一的元素，这扩展了广泛研究的专家设置及其量子对应物。对称锥为一些最重要的优化模型提供了统一的框架，包括线性、二阶锥和半定优化。使用欧几里德约旦代数领域的工具，我们引入了对称锥乘法权重更新(SCMWU)，这是一个在任意对称锥的迹为一处进行在线优化的无投影算法。我们证明了SCMWU等价于Follow-the-Regularized-Leader和Online Mirror Descent，其正则化器为对称锥负熵。通过这个结构结果，我们证明了SCMWU是无悔算法，并通过大量实验验证了我们的理论结果。

    We study online convex optimization where the possible actions are trace-one elements in a symmetric cone, generalizing the extensively-studied experts setup and its quantum counterpart. Symmetric cones provide a unifying framework for some of the most important optimization models, including linear, second-order cone, and semidefinite optimization. Using tools from the field of Euclidean Jordan Algebras, we introduce the Symmetric-Cone Multiplicative Weights Update (SCMWU), a projection-free algorithm for online optimization over the trace-one slice of an arbitrary symmetric cone. We show that SCMWU is equivalent to Follow-the-Regularized-Leader and Online Mirror Descent with symmetric-cone negative entropy as regularizer. Using this structural result we show that SCMWU is a no-regret algorithm, and verify our theoretical results with extensive experiments. Our results unify and generalize the analysis for the Multiplicative Weights Update method over the probability simplex and the M
    
[^3]: 超越直觉，将高斯过程应用于实际数据的框架

    Beyond Intuition, a Framework for Applying GPs to Real-World Data. (arXiv:2307.03093v1 [cs.LG])

    [http://arxiv.org/abs/2307.03093](http://arxiv.org/abs/2307.03093)

    提出了一个框架，用于确定高斯过程在实际问题中的适用性，并建立一个稳健且明确的模型。通过对核函数设计和计算可扩展性选项的指导，该框架在冰川高程变化的案例研究中实现了更准确的结果。

    

    高斯过程（GPs）提供了一种用于小型、结构化和相关数据集的回归的吸引人的方法。然而，它们的应用受到计算成本的限制，并且对于如何将GPs应用于复杂的高维数据集的指导有限。我们提出了一个框架，用于确定GPs在给定问题中的适用性以及如何建立一个强大且明确的GP模型。指导方针形式化了经验丰富的GP实践者的决策，特别强调了核函数设计和计算可扩展性选项。然后，我们将该框架应用于冰川高程变化的案例研究中，在测试时产生了更准确的结果。

    Gaussian Processes (GPs) offer an attractive method for regression over small, structured and correlated datasets. However, their deployment is hindered by computational costs and limited guidelines on how to apply GPs beyond simple low-dimensional datasets. We propose a framework to identify the suitability of GPs to a given problem and how to set up a robust and well-specified GP model. The guidelines formalise the decisions of experienced GP practitioners, with an emphasis on kernel design and options for computational scalability. The framework is then applied to a case study of glacier elevation change yielding more accurate results at test time.
    
[^4]: 带有一般观测模型的不安定赌博机问题的PCL-可索引性和Whittle索引

    PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models. (arXiv:2307.03034v1 [stat.ML])

    [http://arxiv.org/abs/2307.03034](http://arxiv.org/abs/2307.03034)

    本文研究了一种一般观测模型下的不安定多臂赌博机问题，提出了PCL-可索引性和Whittle索引的分析方法，并通过近似过程将问题转化为有限状态问题。数值实验表明算法表现优秀。

    

    本文考虑了一种一般观测模型，用于不安定多臂赌博机问题。由于资源约束或环境或固有噪声，玩家操作需要基于某种有误差的反馈机制。通过建立反馈/观测动力学的一般概率模型，我们将问题表述为一个从任意初始信念（先验信息）开始的具有可数信念状态空间的不安定赌博机问题。我们利用具有部分守恒定律（PCL）的可实现区域方法，分析了无限状态问题的可索引性和优先级索引（Whittle索引）。最后，我们提出了一个近似过程，将问题转化为可以应用Niño-Mora和Bertsimas针对有限状态问题的AG算法的问题。数值实验表明，我们的算法具有出色的性能。

    In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
    
[^5]: 可计算的稳定性对于持续性秩函数机器学习的影响

    Computable Stability for Persistence Rank Function Machine Learning. (arXiv:2307.02904v1 [math.AT])

    [http://arxiv.org/abs/2307.02904](http://arxiv.org/abs/2307.02904)

    本文讨论了持续性同调秩函数在拓扑数据分析中的应用，并指出秩函数相对于条码的优势在于更易于计算，并且可以直接应用函数数据分析的方法。然而，秩函数的稳定性问题尚待解决。

    

    持续性同调条码和图是拓扑数据分析的基础。它们在许多真实数据环境中得到广泛应用，将拓扑信息的变化（通过细胞同调测量）与数据的变化相关联，然而，由于其复杂的几何结构，它们在统计环境中的使用具有挑战性。在本文中，我们重新审视了持续性同调秩函数——一种衡量“形状”的不变量，它在条码和持续性图之前被引入，并以更适合数据和计算的形式捕捉相同的信息。尤其是，由于它们是函数，当持续性同调以秩函数形式表示时，可以直接应用函数数据分析领域的技术，这是一种适用于函数的统计学领域。然而，与条码相比，秩函数的受欢迎程度较低，因为它们面临着稳定性的挑战——这是验证它们在数据分析中使用的关键性质，而这种稳定性很难保证。

    Persistent homology barcodes and diagrams are a cornerstone of topological data analysis. Widely used in many real data settings, they relate variation in topological information (as measured by cellular homology) with variation in data, however, they are challenging to use in statistical settings due to their complex geometric structure. In this paper, we revisit the persistent homology rank function -- an invariant measure of ``shape" that was introduced before barcodes and persistence diagrams and captures the same information in a form that is more amenable to data and computation. In particular, since they are functions, techniques from functional data analysis -- a domain of statistics adapted for functions -- apply directly to persistent homology when represented by rank functions. Rank functions, however, have been less popular than barcodes because they face the challenge that stability -- a property that is crucial to validate their use in data analysis -- is difficult to gua
    
[^6]: 使用事后多观察数据的POMDP样本高效学习

    Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight. (arXiv:2307.02884v1 [cs.LG])

    [http://arxiv.org/abs/2307.02884](http://arxiv.org/abs/2307.02884)

    本文研究了在部分可观察的马尔可夫决策过程（POMDPs）中学习的样本高效性，提出了一个增强的反馈模型，利用事后多观察数据实现了对两种新的POMDP子类的样本高效学习。

    

    本文研究了在部分可观察的马尔可夫决策过程（POMDPs）中学习的样本高效性，这是强化学习中一个在最坏情况下被证明是指数级困难的问题。受到现实世界中游戏中的加载等情景的启发，我们提出了一个增强的反馈模型，称为“事后多观察数据”，其中在与POMDP进行交互的每个周期之后，学习者可以收集到从遇到的潜在状态发出的多个附加观测数据，但不能直接观测到潜在状态本身。我们证明了在这个反馈模型下，对于两种新的POMDP子类（多观测展示POMDP和可区分POMDP），可以实现样本高效的学习。这两个子类相对于广泛研究的展示POMDP子类来说更加普遍和放松，而在标准轨迹反馈下可以实现样本高效学习。值得注意的是，可区分POMDP只需使用最少的观测数据和反馈进行学习。

    This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called ``multiple observations in hindsight'', where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: \emph{multi-observation revealing POMDPs} and \emph{distinguishable POMDPs}. Both subclasses generalize and substantially relax \emph{revealing POMDPs} -- a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require th
    
[^7]: 高阶网络中的度异质性：超图β模型的推断

    Degree Heterogeneity in Higher-Order Networks: Inference in the Hypergraph $\boldsymbol{\beta}$-Model. (arXiv:2307.02818v1 [math.ST])

    [http://arxiv.org/abs/2307.02818](http://arxiv.org/abs/2307.02818)

    本文对具有多层的超图β模型进行了研究，推导了最大似然估计的收敛速率和极限分布，并构建了模型参数的置信区间。同时，我们还建立了超图β模型中似然比检验的渐近正态性。

    

    随机图中的β模型通常用于表示具有度异质性的网络中的配对交互。超图β模型超越了配对交互，Stasi等人于2014年引入了超图β模型，用于捕捉具有高阶（多向）交互的网络中的度异质性。本文首次对具有多层的超图β模型进行了严格研究，它允许在不同层次中存在不同大小的超边。首先，我们推导了最大似然（ML）估计的收敛速率，并确定了它们的最小极小速率。我们还推导了ML估计的极限分布，并构建了模型参数的渐近有效置信区间。接下来，我们考虑了超图β模型中的拟合优度问题。具体而言，我们在零假设下建立了似然比（LR）检验的渐近正态性。

    The $\boldsymbol{\beta}$-model for random graphs is commonly used for representing pairwise interactions in a network with degree heterogeneity. Going beyond pairwise interactions, Stasi et al. (2014) introduced the hypergraph $\boldsymbol{\beta}$-model for capturing degree heterogeneity in networks with higher-order (multi-way) interactions. In this paper we initiate the rigorous study of the hypergraph $\boldsymbol{\beta}$-model with multiple layers, which allows for hyperedges of different sizes across the layers. To begin with, we derive the rates of convergence of the maximum likelihood (ML) estimate and establish their minimax rate optimality. We also derive the limiting distribution of the ML estimate and construct asymptotically valid confidence intervals for the model parameters. Next, we consider the goodness-of-fit problem in the hypergraph $\boldsymbol{\beta}$-model. Specifically, we establish the asymptotic normality of the likelihood ratio (LR) test under the null hypothe
    
[^8]: 何时使用基于置信度的级联延迟足够？

    When Does Confidence-Based Cascade Deferral Suffice?. (arXiv:2307.02764v1 [cs.LG])

    [http://arxiv.org/abs/2307.02764](http://arxiv.org/abs/2307.02764)

    本研究旨在探讨何时基于置信度的级联延迟可能失败，以及何时备选的延迟策略可能表现更好。通过理论分析和实验证明事后延迟机制能够显著提高性能。

    

    级联是一种经典的策略，可以实现适应性地在样本之间变化的推理成本，其中按顺序调用一系列分类器。延迟规则确定是否调用序列中的下一个分类器，或者终止预测。一种简单的延迟规则利用当前分类器的置信度，例如基于最大预测的softmax概率。尽管对级联结构不敏感——例如不建模下游模型的错误——但这种基于置信度的延迟经常在实践中表现出色。在本文中，我们旨在更好地理解基于置信度的延迟可能失败的条件，以及何时备选的延迟策略可能更好。我们首先对最优延迟规则进行了理论表征，精确地描述了基于置信度的延迟可能受到影响的设置。然后我们研究了事后延迟机制，并验证它们可以显著提高性能。

    Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improv
    
[^9]: ALPCAH：具有尾部奇异值正则化的样本异方差PCA

    ALPCAH: Sample-wise Heteroscedastic PCA with Tail Singular Value Regularization. (arXiv:2307.02745v1 [stat.ML])

    [http://arxiv.org/abs/2307.02745](http://arxiv.org/abs/2307.02745)

    本文提出了一种新的PCA方法，可以估计样本的噪声方差，从而改进与数据的低秩结构相关的子空间基础的估计值。

    

    主成分分析（PCA）是数据降维领域中的关键工具，对于各种数据科学问题都非常有用。然而，许多应用涉及到具有不同数据源的噪声特性导致质量不均匀的异质数据。处理这种混合数据集的方法被称为异方差方法。当前的方法如HePPCAT假设基础系数为高斯分布，但在实践中可能不成立。其他方法如加权PCA（WPCA）假设噪声方差已知，但在实践中很难确定。本文开发了一种PCA方法，可以估计样本的噪声方差，并将这些信息用于模型中，以改进与数据的低秩结构相关的子空间基础的估计值。这样做不需要对低秩成分进行分布假设，也不需要假设噪声方差已知。模拟实验显示了其有效性。

    Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction that is useful for various data science problems. However, many applications involve heterogeneous data that varies in quality due to noise characteristics associated with different sources of the data. Methods that deal with this mixed dataset are known as heteroscedastic methods. Current methods like HePPCAT make Gaussian assumptions of the basis coefficients that may not hold in practice. Other methods such as Weighted PCA (WPCA) assume the noise variances are known, which may be difficult to know in practice. This paper develops a PCA method that can estimate the sample-wise noise variances and use this information in the model to improve the estimate of the subspace basis associated with the low-rank structure of the data. This is done without distributional assumptions of the low-rank component and without assuming the noise variances are known. Simulations show the effectiveness of acc
    
[^10]: 评估评估器：当前的少样本学习基准适用吗？

    Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?. (arXiv:2307.02732v1 [cs.LG])

    [http://arxiv.org/abs/2307.02732](http://arxiv.org/abs/2307.02732)

    本文首次对任务级别的评估进行了研究，发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。

    

    在过去的十年中，提出了许多少样本学习的基准。然而，所有这些基准都集中在对许多任务平均性能的评估上，但在这种情况下如何可靠地评估和调整针对个别任务训练的模型的问题尚未解决。本文首次对任务级别的评估进行了研究，这在部署模型时是一个基本步骤。我们测量了少样本场景中性能估计器的准确性，考虑了模型选择的策略，并检查了通常被认为是鲁棒的评估器失败的原因。我们得出结论，用较少的折叠进行交叉验证是直接估计模型性能的最佳选择，而用自助法或大量折叠进行交叉验证更适合于模型选择的目的。总的来说，我们发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。

    Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. However all of these benchmarks focus on performance averaged over many tasks, and the question of how to reliably evaluate and tune models trained for individual tasks in this regime has not been addressed. This paper presents the first investigation into task-level evaluation -- a fundamental step when deploying a model. We measure the accuracy of performance estimators in the few-shot setting, consider strategies for model selection, and examine the reasons for the failure of evaluators usually thought of as being robust. We conclude that cross-validation with a low number of folds is the best choice for directly estimating the performance of a model, whereas using bootstrapping or cross validation with a large number of folds is better for model selection purposes. Overall, we find that existing benchmarks for few-shot learning are not designed in such a way that one can get a reliable picture of how e
    
[^11]: 理解不确定性采样

    Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])

    [http://arxiv.org/abs/2307.02719](http://arxiv.org/abs/2307.02719)

    本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。

    

    不确定性采样是一种常见的主动学习算法，它顺序地查询当前预测模型对数据样本的不确定性。然而，不确定性采样的使用往往是启发式的：（i）关于在特定任务和特定损失函数下对“不确定性”的准确定义没有共识；（ii）没有理论保证能够给出一个标准协议来实施该算法，例如，在随机梯度下降等优化算法框架下如何处理顺序到达的注释数据。在本研究中，我们系统地研究了流式和池式主动学习下的不确定性采样算法。我们提出了一个等效损失的概念，该概念取决于使用的不确定性度量和原始损失函数，并确立了不确定性采样算法本质上是针对这种等效损失进行优化。这一观点验证了算法的适当性。

    Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
    
[^12]: 内核，数据和物理。

    Kernels, Data & Physics. (arXiv:2307.02693v1 [cs.LG])

    [http://arxiv.org/abs/2307.02693](http://arxiv.org/abs/2307.02693)

    该论文主要介绍了NTK方法在机器学习问题中的应用，通过找到可处理的内核表达形式来解决一般无法解决的问题，重点讨论了数据精炼和对抗鲁棒性等实际应用。

    

    这是Julia Kempe教授在Les Houches举办的夏季学校“机器学习的统计物理”中所讲授的课程笔记。笔记讨论了所谓的NTK方法，该方法通过找到可处理的内核表达形式来理解一般无法解决的问题。笔记主要关注实际应用，如数据精炼和对抗鲁棒性，也讨论了归纳偏差的示例。

    Lecture notes from the course given by Professor Julia Kempe at the summer school "Statistical physics of Machine Learning" in Les Houches. The notes discuss the so-called NTK approach to problems in machine learning, which consists of gaining an understanding of generally unsolvable problems by finding a tractable kernel formulation. The notes are mainly focused on practical applications such as data distillation and adversarial robustness, examples of inductive bias are also discussed.
    
[^13]: 面板数据实时预测：以市盈率为例。

    Panel Data Nowcasting: The Case of Price-Earnings Ratios. (arXiv:2307.02673v1 [econ.EM])

    [http://arxiv.org/abs/2307.02673](http://arxiv.org/abs/2307.02673)

    本文使用结构化机器学习回归方法对面板数据进行实时预测，针对混合频率的时间序列数据结构提出了稀疏组 LASSO 正则化方法，并且在实证结果中显示出优于其他方法的表现。

    

    本文使用结构化机器学习回归方法，对由不同频率采样的系列数据进行面板数据实时预测。受到预测大量公司收益的问题的启发，考虑到宏观经济、金融和新闻时间序列以不同频率采样的混合面板数据结构，我们重点研究了稀疏组 LASSO 正则化方法。实证结果表明，相比分析师的预测、预测组合、公司特定时间序列回归模型和标准的机器学习方法，我们的机器学习面板数据回归模型表现出更好的性能。

    The paper uses structured machine learning regressions for nowcasting with panel data consisting of series sampled at different frequencies. Motivated by the problem of predicting corporate earnings for a large cross-section of firms with macroeconomic, financial, and news time series sampled at different frequencies, we focus on the sparse-group LASSO regularization which can take advantage of the mixed frequency time series panel data structures. Our empirical results show the superior performance of our machine learning panel data regression models over analysts' predictions, forecast combinations, firm-specific time series regression models, and standard machine learning methods.
    
[^14]: 结构缺失的完整特征化

    A Complete Characterisation of Structured Missingness. (arXiv:2307.02650v1 [stat.ME])

    [http://arxiv.org/abs/2307.02650](http://arxiv.org/abs/2307.02650)

    提出了一种用于描述结构缺失的分类体系，其中每个缺失指示向量可以依赖于除自身之外的所有缺失指示向量和数据矩阵。将这个新框架嵌入到已有的MCAR、MAR和MNAR机制分解中。

    

    我们处理大型复杂数据源的能力不断增加，为我们提供了新的重要应用研究问题，例如如何处理大规模数据库中的缺失值。Mitra等人（2023年）注意到了结构缺失（SM）的现象，即缺失具有潜在的结构。现有的缺失机制定义分类通常假设变量的缺失指示向量$M_1$，$M_2$，...，$M_p$在给定相关数据矩阵$\mathbf{X}$的条件下是独立的。鉴于这在多变量设置中通常不适用于描述SM，我们引入了一种SM的分类体系，其中每个${M}_j$除了$\mathbf{X}$之外，还可以依赖于$\mathbf{M}_{-j}$（即除了${M}_j$之外的所有缺失指示向量）。我们将这个新框架嵌入到将机制分解为MCAR、MAR和MNAR（Rubin, 1976）的成熟分解中，从而允许我们将机制重新组合到一个更广泛的设置中。

    Our capacity to process large complex data sources is ever-increasing, providing us with new, important applied research questions to address, such as how to handle missing values in large-scale databases. Mitra et al. (2023) noted the phenomenon of Structured Missingness (SM), which is where missingness has an underlying structure. Existing taxonomies for defining missingness mechanisms typically assume that variables' missingness indicator vectors $M_1$, $M_2$, ..., $M_p$ are independent after conditioning on the relevant portion of the data matrix $\mathbf{X}$. As this is often unsuitable for characterising SM in multivariate settings, we introduce a taxonomy for SM, where each ${M}_j$ can depend on $\mathbf{M}_{-j}$ (i.e., all missingness indicator vectors except ${M}_j$), in addition to $\mathbf{X}$. We embed this new framework within the well-established decomposition of mechanisms into MCAR, MAR, and MNAR (Rubin, 1976), allowing us to recast mechanisms into a broader setting, wh
    
[^15]: 添加解码器用于潜变量识别和笛卡尔积推算

    Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. (arXiv:2307.02598v1 [cs.LG])

    [http://arxiv.org/abs/2307.02598](http://arxiv.org/abs/2307.02598)

    这篇论文解决了表示学习中的潜变量识别和"支持外"图像生成问题，展示了加法解码器能够对潜变量进行识别，并提供了理论依据支持这种方法的有效性。

    

    我们解决了表示学习中的潜变量识别和“支持外”图像生成问题。我们展示了在一类我们称为“加法”的解码器中，这两者是可能的，这些解码器类似于用于面向对象表示学习（OCRL）的解码器，并且非常适用于可以分解为多个特定对象图像的图像。我们提供了在使用加法解码器完全解决重构问题时，对潜变量块进行了置换和块状逆变换的识别的条件。这个保证仅基于关于潜因子分布的非常弱的假设，潜因子可能存在统计依赖并且具有几乎任意形状的支持。我们的结果提供了非线性独立成分分析（ICA）可能性的新设置，并且增加了我们对OCRL方法的理论理解。我们还从理论上证明了加法解码器可以

    We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can 
    
[^16]: 模型错误下的条件独立性检验

    Conditional independence testing under model misspecification. (arXiv:2307.02520v1 [stat.ML])

    [http://arxiv.org/abs/2307.02520](http://arxiv.org/abs/2307.02520)

    该论文研究了模型错误下的条件独立性检验，在这种情况下提出了新的近似或上界来衡量基于回归的测试的测试误差，并引入了一种新颖的基于回归的CI检验方法RBPT，对模型错误具有鲁棒性。

    

    条件独立性（CI）检验是现代统计学和机器学习中基础且具有挑战性的问题。许多现代的CI检验方法依赖于强大的监督学习方法来学习回归函数或贝叶斯预测器作为中间步骤。尽管这些方法在监督学习方法准确估计回归函数或贝叶斯预测器时保证了控制第一类错误，但它们在模型错误导致失败时的行为尚不清楚。从更广泛的意义上讲，即使使用了通用逼近器（例如深度神经网络），模型错误也可能出现。因此，我们研究了在模型错误下的基于回归的CI检验的性能。具体地，我们提出了新的近似或上界来衡量依赖于错误的三个基于回归的测试的测试误差。此外，我们引入了Rao-Blackwellized Predictor Test（RBPT），这是一种新颖的基于回归的CI检验，对模型错误具有鲁棒性。

    Conditional independence (CI) testing is fundamental and challenging in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step. Although the methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors, their behavior is less understood when they fail due to model misspecification. In a broader sense, model misspecification can arise even when universal approximators (e.g., deep neural nets) are employed. Then, we study the performance of regression-based CI tests under model misspecification. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a novel regression-based CI test robust against model mis
    
[^17]: 通过算法相关的Rademacher复杂度实现泛化保证

    Generalization Guarantees via Algorithm-dependent Rademacher Complexity. (arXiv:2307.02501v1 [stat.ML])

    [http://arxiv.org/abs/2307.02501](http://arxiv.org/abs/2307.02501)

    提出了一种通过算法和数据相关的假设类的经验Rademacher复杂度来控制泛化错误的方法，基于有限分形维度获得了新的界限，并简化了对随机梯度下降的无维度泛化界限的证明。

    

    现代机器学习算法的泛化行为需要算法和数据相关的泛化界限来解释。在这个背景下，存在着涉及(各种形式的)互信息的信息论泛化界限，以及基于假设集稳定性的界限。我们提出了一个在技术上不同但在概念上相关的复杂度度量，来控制泛化错误，即算法和数据相关的假设类的经验Rademacher复杂度。结合Rademacher复杂度的标准属性和该类的便捷结构，我们能够：(i)基于有限分形维度获得新的界限，这些界限将前人工作中的分形维度界限从连续的假设类推广到有限假设类，并且避免了之前工作中需要的互信息项；(ii)大大简化了最近提出的针对随机梯度下降的无维度泛化界限的证明。

    Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient 
    
[^18]: 通过Pareto Optimal自监督实现大型语言模型的自动校准和错误修正

    Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])

    [http://arxiv.org/abs/2306.16564](http://arxiv.org/abs/2306.16564)

    本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。

    

    大型语言模型(LLM)已经展现了出色的能力，适用于广泛的应用领域，但是准确性仍然是一个重要的增长领域，特别是在生物医学等关键领域。一种有效的方法，用于校准LLM响应的置信水平，对于自动检测错误并促进人机协作验证至关重要。一个重要的校准信号来源是专家指定的编程监督，通常具有较低的成本，但也有其自身的局限性，如噪声和覆盖范围。在本文中，我们引入了一种Pareto Optimal自监督框架，可以利用可用的编程监督来系统地校准LLM响应，通过为每个响应生成风险评分，而不需要任何额外的手动工作。这通过学习一个调和模型来实现，将LLM输出与其他可用的监督来源相协调，将更不确定的响应分配更高的风险评分。

    Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
    
[^19]: 多黑盒预言下的主动策略改进

    Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v1 [cs.LG])

    [http://arxiv.org/abs/2306.10259](http://arxiv.org/abs/2306.10259)

    本研究提出了MAPS和MAPS-SE两个算法，可在多黑盒预言情况下，采用模仿学习并主动选择和改进最优预言，显著提升了性能。

    

    强化学习在各种复杂领域中取得了重大进展，但是通过强化学习确定有效策略往往需要进行广泛的探索，而模仿学习旨在通过使用专家演示来指导探索，缓解这个问题。在真实世界情境下，人们通常只能接触到多个次优的黑盒预言，而不是单个最优的预言，这些预言不能在所有状态下普遍优于彼此，这给主动决定在哪种状态下使用哪种预言以及如何改进各自估计值函数提出了挑战。本文介绍了一个可行的解决方案，即MAPS和MAPS-SE算法。

    Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAP
    
[^20]: 通过Wasserstein Barycenters实现多任务学习中的公平性

    Fairness in Multi-Task Learning via Wasserstein Barycenters. (arXiv:2306.10155v1 [stat.ML])

    [http://arxiv.org/abs/2306.10155](http://arxiv.org/abs/2306.10155)

    本文提出了一种方法，通过多元Wasserstein barycenters扩展`Strong Demographic Parity`的定义，实现多任务学习中的公平性，包括回归和二元分类任务。在实验中表现出良好的效果。

    

    算法公平性是机器学习中的一个已经成熟的领域，旨在减少数据中的偏差。最近的进展提出了各种方法来确保单变量环境下的公平性，即目标是去除单个任务的偏差。然而，将公平性扩展到多任务环境，其中使用共享表示来优化多个目标，仍未得到充分开发。为了填补这一差距，我们利用多元Wasserstein barycenters将\textit{Strong Demographic Parity}的定义扩展到多任务学习中。我们的方法为最优的公平多任务预测器提供了封闭式解，包括回归和二元分类任务。我们开发了一种数据驱动的估计过程，以寻找解决方案，并在合成和实际数据集上运行数字实验。经验结果突显了我们的后处理方法在促进公平决策方面的实际价值。

    Algorithmic Fairness is an established field in machine learning that aims to reduce biases in data. Recent advances have proposed various methods to ensure fairness in a univariate environment, where the goal is to de-bias a single task. However, extending fairness to a multi-task setting, where more than one objective is optimised using a shared representation, remains underexplored. To bridge this gap, we develop a method that extends the definition of \textit{Strong Demographic Parity} to multi-task learning using multi-marginal Wasserstein barycenters. Our approach provides a closed form solution for the optimal fair multi-task predictor including both regression and binary classification tasks. We develop a data-driven estimation procedure for the solution and run numerical experiments on both synthetic and real datasets. The empirical results highlight the practical value of our post-processing methodology in promoting fair decision-making.
    
[^21]: 关于随机SVD的噪声敏感性

    On the Noise Sensitivity of the Randomized SVD. (arXiv:2305.17435v1 [cs.IT])

    [http://arxiv.org/abs/2305.17435](http://arxiv.org/abs/2305.17435)

    通过对R-SVD在低秩信号加噪声测量模型下的分析，证明了当信噪比(SNR)超过某个依赖于降维因子的可检测门限时，R-SVD产生的最大奇异值是一个离群值；在门限以下，没有离群值从奇异值块中产生

    

    随机奇异值分解(R-SVD)是一种流行的基于草图的算法，用于有效计算大矩阵的部分奇异值分解。当矩阵是低秩时，R-SVD可以精确地产生其部分奇异值分解；但当秩较大时，它只能产生近似值。受数据科学和主成分分析(PCA)应用的驱动，我们在低秩信号加噪声测量模型下分析了R-SVD；具体来说，当其输入为尖峰型随机矩阵时。证明了R-SVD产生的奇异值表现出类似BBP的相变：当信噪比(SNR)超过某个依赖于降维因子的可检测门限时，最大奇异值是一个离群值；在门限以下，没有离群值从奇异值块中产生。我们进一步计算了地面真值信号奇异向量与R-SVD产生的近似值之间的重叠的渐近公式。降维具有负面的影响。

    The randomized singular value decomposition (R-SVD) is a popular sketching-based algorithm for efficiently computing the partial SVD of a large matrix. When the matrix is low-rank, the R-SVD produces its partial SVD exactly; but when the rank is large, it only yields an approximation.  Motivated by applications in data science and principal component analysis (PCA), we analyze the R-SVD under a low-rank signal plus noise measurement model; specifically, when its input is a spiked random matrix. The singular values produced by the R-SVD are shown to exhibit a BBP-like phase transition: when the SNR exceeds a certain detectability threshold, that depends on the dimension reduction factor, the largest singular value is an outlier; below the threshold, no outlier emerges from the bulk of singular values. We further compute asymptotic formulas for the overlap between the ground truth signal singular vectors and the approximations produced by the R-SVD.  Dimensionality reduction has the adve
    
[^22]: 混合稀疏线性回归中的统计与计算权衡

    Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression. (arXiv:2303.02118v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02118](http://arxiv.org/abs/2303.02118)

    本研究研究了混合稀疏线性回归问题，在实际应用中发现了统计和计算之间的权衡关系，并确定了样本复杂度和运行时间之间的平滑信息-计算权衡关系。

    

    我们考虑具有两个部分的混合稀疏线性回归问题，其中需要从n个无标签的噪声线性测量中恢复两个实数k稀疏信号β1、β2。稀疏度允许在维度上是亚线性的，且假设添加的噪声是独立的高斯噪声，方差为σ²。之前的研究表明，该问题存在一个k/SNR²到k²/SNR²的统计到计算间隙，类似于其他具有计算挑战性的高维推断问题，如稀疏主成分分析和鲁棒稀疏均值估计；这里的SNR是信噪比。通过低次多项式方法，我们证明了这个问题存在更广泛的计算障碍，但只在非常狭窄的对称参数范围内才表现出计算困难。我们确定了样本复杂度n和运行时间之间的平滑信息-计算权衡关系，对于任何随机化算法。

    We consider the problem of mixed sparse linear regression with two components, where two real $k$-sparse signals $\beta_1, \beta_2$ are to be recovered from $n$ unlabelled noisy linear measurements. The sparsity is allowed to be sublinear in the dimension, and additive noise is assumed to be independent Gaussian with variance $\sigma^2$. Prior work has shown that the problem suffers from a $\frac{k}{SNR^2}$-to-$\frac{k^2}{SNR^2}$ statistical-to-computational gap, resembling other computationally challenging high-dimensional inference problems such as Sparse PCA and Robust Sparse Mean Estimation; here $SNR$ is the signal-to-noise ratio. We establish the existence of a more extensive computational barrier for this problem through the method of low-degree polynomials, but show that the problem is computationally hard only in a very narrow symmetric parameter regime. We identify a smooth information-computation tradeoff between the sample complexity $n$ and runtime for any randomized algor
    
[^23]: 《Stein变分梯度下降的有限粒子收敛速度》

    A Finite-Particle Convergence Rate for Stein Variational Gradient Descent. (arXiv:2211.09721v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09721](http://arxiv.org/abs/2211.09721)

    本文提供了Stein变分梯度下降算法的有限粒子收敛速度，证明了当目标分布为次高斯且具有Lipschitz积分核时，使用适当的步长序列和粒子数量，可以以1/√(log log n)的速度将核Stein差异逼近零。

    

    我们首次提供了Stein变分梯度下降（SVGD）的有限粒子收敛速度，这是一种用一组粒子逼近概率分布的流行算法。具体来说，只要目标分布是次高斯的，并且具有Lipschitz积分核，使用n个粒子和适当的步长序列进行SVGD，核Stein差异将以1/√(log log n)的速度趋于零。我们怀疑n的依赖性可以改进，希望我们的明确的非渐近证明策略能为未来的改进提供模板。

    We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with n particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order 1/sqrt(log log n) rate. We suspect that the dependence on n can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.
    
[^24]: 在算法策划平台上建模内容创作者的激励机制

    Modeling Content Creator Incentives on Algorithm-Curated Platforms. (arXiv:2206.13102v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2206.13102](http://arxiv.org/abs/2206.13102)

    该论文讨论了在线平台上内容创作者激励机制的建模，通过分析算法选择对曝光游戏（包括现代分解和两塔架构）中（纳什）均衡的影响，提出了使用曝光游戏模型进行预部署审计的方法，以识别期望和激励内容之间的不匹配。

    

    内容创作者在争夺用户注意力。他们的影响力在很大程度上取决于在线平台开发者所做的算法选择。为了最大限度地提高曝光率，许多创作者采取战略性的调整，如搜索引擎优化行业的例子所证明。这导致了对有限用户注意力池的竞争。我们在所谓的曝光游戏中形式化了这些动态，这是一种由算法引起的激励模型，其中包括现代分解和（深层）两塔架构。我们证明了看似无害的算法选择，例如非负与无约束分解，在曝光游戏中显著影响（纳什）均衡的存在和特性。我们提出使用创作者行为模型，如曝光游戏，进行（ex-ante）预部署审计。这样的审计可以识别期望和激励内容之间的不匹配，并在内容过滤和管理等事后措施上进行补充。为此，我们提出了一些工具。

    Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by algorithms, including modern factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices, e.g., non-negative vs. unconstrained factorization, significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models, like exposure games, for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools 
    
[^25]: 离线强化学习从视觉观察中的挑战和机遇

    Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04779](http://arxiv.org/abs/2206.04779)

    该论文研究了离线强化学习从视觉观察中的挑战和机遇，针对这一复杂领域建立了视觉领域中连续控制的简单基准，并设计了一系列基准任务，以更好地表示现实世界离线RL问题中的数据分布，并通过对两种基于视觉的在线强化学习算法的简单修改进行评估。

    

    离线强化学习已经展现了在利用大规模预先收集的数据集进行策略学习方面的巨大潜力，使得Agent可以避免通常费时昂贵的在线数据收集。然而，在连续动作空间中，基于视觉观察的离线强化学习仍然未被充分探索，在这个复杂的领域中对关键挑战的理解有限。在本文中，我们为视觉领域中的连续控制建立简单的基准，并引入了一系列针对离线强化学习的基准任务，这些任务旨在更好地表示现实世界离线RL问题中存在的数据分布，并受离线RL从视觉观察中的一组期望所指导，包括对视觉干扰的稳健性和动力学中可视化变化的识别能力。通过使用这套基准任务，我们展示了对两种广泛使用的基于视觉的在线强化学习算法DreamerV2和DrQ-v2进行简单修改的可行性。

    Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suf
    
[^26]: 从高维噪声数据中学习低维非线性结构：一种积分算子方法的应用

    Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach. (arXiv:2203.00126v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.00126](http://arxiv.org/abs/2203.00126)

    本文提出了一种用于从高维噪声数据中学习低维非线性结构的算法，该算法使用自适应带宽选择过程，并获得了理论上的收敛性证明。算法的低维嵌入结果可用于数据可视化、聚类和预测等任务。

    

    我们提出了一种核谱嵌入算法，用于从高维噪声观测中学习低维非线性结构，其中假设数据集从本质上是一个低维流形，并受到高维噪声的污染。该算法采用了一种自适应带宽选择过程，不依赖于对底层流形的先验知识。所获得的低维嵌入还可以进一步用于数据可视化、聚类和预测等下游任务。我们的方法在理论上得到了证明，并且具有实际可解释性。具体而言，我们在样本的维度和大小相对较大时，建立了最终嵌入到无噪声对应的收敛性，并刻画了信噪比对收敛速度和相变的影响。我们还证明了嵌入到由核定义的积分算子的特征函数的收敛性。

    We propose a kernel-spectral embedding algorithm for learning low-dimensional nonlinear structures from high-dimensional and noisy observations, where the datasets are assumed to be sampled from an intrinsically low-dimensional manifold and corrupted by high-dimensional noise. The algorithm employs an adaptive bandwidth selection procedure which does not rely on prior knowledge of the underlying manifold. The obtained low-dimensional embeddings can be further utilized for downstream purposes such as data visualization, clustering and prediction. Our method is theoretically justified and practically interpretable. Specifically, we establish the convergence of the final embeddings to their noiseless counterparts when the dimension and size of the samples are comparably large, and characterize the effect of the signal-to-noise ratio on the rate of convergence and phase transition. We also prove convergence of the embeddings to the eigenfunctions of an integral operator defined by the kern
    
[^27]: 在网络中的描述性与推理性社区检测：陷阱、误解和半真相

    Descriptive vs. inferential community detection in networks: pitfalls, myths, and half-truths. (arXiv:2112.00183v7 [physics.soc-ph] UPDATED)

    [http://arxiv.org/abs/2112.00183](http://arxiv.org/abs/2112.00183)

    本文将现有的社区检测方法根据目标分为描述性和推理性。描述性方法基于上下文相关的社区结构发现网络模式，而推理性方法通过生成模型拟合数据，揭示网络形成机制并分离结构和随机性。

    

    社区检测是网络科学中最重要的方法学领域之一，过去几十年来引起了相当大的关注。该领域涉及将网络自动划分为基本构建块，以提供其大规模结构的摘要。尽管其重要性和广泛应用，但在实际应用中，目前的方法与被认为是最先进的方法之间存在明显差距。本文试图通过将现有方法分为“描述性”和“推理性”目标来解决这一差距。描述性方法根据依赖于上下文的社区结构概念在网络中发现模式，而推理性方法构建生成模型，并尝试将其拟合到数据中。通过这种方式，它们能够揭示网络形成的机制，并将结构与随机性分离开来。

    Community detection is one of the most important methodological fields of network science, and one which has attracted a significant amount of attention over the past decades. This area deals with the automated division of a network into fundamental building blocks, with the objective of providing a summary of its large-scale structure. Despite its importance and widespread adoption, there is a noticeable gap between what is arguably the state-of-the-art and the methods that are actually used in practice in a variety of fields. Here we attempt to address this discrepancy by dividing existing methods according to whether they have a "descriptive" or an "inferential" goal. While descriptive methods find patterns in networks based on context-dependent notions of community structure, inferential methods articulate generative models, and attempt to fit them to data. In this way, they are able to provide insights into the mechanisms of network formation, and separate structure from randomnes
    
[^28]: 高斯变分推断和拉普拉斯近似的计算渐近特性

    The computational asymptotics of Gaussian variational inference and the Laplace approximation. (arXiv:2104.05886v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2104.05886](http://arxiv.org/abs/2104.05886)

    本论文分析了高斯变分推断和拉普拉斯近似的渐近凸性特性，并提出了两个算法（CLA和CSVI）来利用这些特性。

    

    高斯变分推断和拉普拉斯近似是用简单且可扩展的随机优化算法将贝叶斯后验推断表述为优化问题的流行替代方法，然而，这两种方法的关键限制是优化问题的解通常是无法计算的；即使在简单的情况下，问题也是非凸的。因此，最近发展的统计保证 -- 所有都涉及到全局最优值的（数据）渐近性质 -- 在实践中并不可靠。在这项工作中，我们提供了两个重要的贡献：对高斯族变分推断和拉普拉斯近似所需的最大后验概率 (MAP) 问题的渐近凸性性质进行了理论分析；并提出了两个算法 -- 一致拉普拉斯近似 (CLA) 和一致随机变分推断 (CSVI) -- 利用这些性质。

    Gaussian variational inference and the Laplace approximation are popular alternatives to Markov chain Monte Carlo that formulate Bayesian posterior inference as an optimization problem, enabling the use of simple and scalable stochastic optimization algorithms. However, a key limitation of both methods is that the solution to the optimization problem is typically not tractable to compute; even in simple settings the problem is nonconvex. Thus, recently developed statistical guarantees -- which all involve the (data) asymptotic properties of the global optimum -- are not reliably obtained in practice. In this work, we provide two major contributions: a theoretical analysis of the asymptotic convexity properties of variational inference with a Gaussian family and the maximum a posteriori (MAP) problem required by the Laplace approximation; and two algorithms -- consistent Laplace approximation (CLA) and consistent stochastic variational inference (CSVI) -- that exploit these properties t
    

