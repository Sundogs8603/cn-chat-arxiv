# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ChemCrow: Augmenting large-language models with chemistry tools.](http://arxiv.org/abs/2304.05376) | 本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。 |
| [^2] | [The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning.](http://arxiv.org/abs/2304.05366) | 本论文阐述了无免费午餐定理的监督学习中的限制，证明了归纳偏差可以提高学习算法的效果，并且展示了神经网络模型的偏好与现实世界的数据分布相关。 |
| [^3] | [Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling.](http://arxiv.org/abs/2304.05365) | 本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。 |
| [^4] | [Diffusion Models for Constrained Domains.](http://arxiv.org/abs/2304.05364) | 本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。 |
| [^5] | [Generative Modeling via Hierarchical Tensor Sketching.](http://arxiv.org/abs/2304.05305) | 本文提出了一种利用分层张量草图来近似高维概率密度的方法，通过随机奇异值分解技术解决线性方程达到此目的，其算法复杂度在高维密度维度上呈线性规模。 |
| [^6] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^7] | [Inhomogeneous graph trend filtering via a l2,0 cardinality penalty.](http://arxiv.org/abs/2304.05223) | 本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。 |
| [^8] | [Automatic Gradient Descent: Deep Learning without Hyperparameters.](http://arxiv.org/abs/2304.05187) | 本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。 |
| [^9] | [Generative modeling for time series via Schr{\"o}dinger bridge.](http://arxiv.org/abs/2304.05093) | 本文提出了一种基于Schr{\"o}dinger bridge的时序生成模型，可以通过熵插值最优传输来处理参考路径空间上的参考概率分布和目标测度之间的关系，模型的解由有限时间区间内的随机微分方程和由数据估计的路径依赖漂移函数组成，通过模拟SB扩散过程产生新的合成数据样本，实验证明该方法在玩具自回归模型、GARCH模型和分形布朗运动方面具有很好的性能。 |
| [^10] | [Actually Sparse Variational Gaussian Processes.](http://arxiv.org/abs/2304.05091) | 提出了一种新的跨领域变量高斯过程类，利用紧支撑B样条基函数，可以使用稀疏线性代数来显著加快矩阵运算， 实现在大规模数据集下快速高效地建模快速变化的空间现象。 |
| [^11] | [A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models.](http://arxiv.org/abs/2304.04916) | 本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。 |
| [^12] | [Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning.](http://arxiv.org/abs/2304.04824) | 本论文提出了一个基于梯度的不确定性归因方法来确定导致预测的不确定性的最具问题性的输入区域，从而实现可解释和可操作的贝叶斯深度学习。 |
| [^13] | [Reinforcement Learning from Passive Data via Latent Intentions.](http://arxiv.org/abs/2304.04782) | 本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。 |
| [^14] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^15] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^16] | [Asynchronous Online Federated Learning with Reduced Communication Requirements.](http://arxiv.org/abs/2303.15226) | 提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，能够处理异构和延迟设备，实现参与学习任务的可访问性和效率。 |
| [^17] | [How many dimensions are required to find an adversarial example?.](http://arxiv.org/abs/2303.14173) | 本文研究了对抗性漏洞如何取决于受限于高维输入空间中的子空间维数，同时针对标准PGD攻击的对抗性成功率提出了单调递增函数表达式。 |
| [^18] | [Neural Laplace Control for Continuous-time Delayed Systems.](http://arxiv.org/abs/2302.12604) | 本文介绍了一种神经拉普拉斯控制方法，用于解决具有不规则状态观测和未知延迟的连续时间环境下的离线强化学习问题。 |
| [^19] | [Revised Conditional t-SNE: Looking Beyond the Nearest Neighbors.](http://arxiv.org/abs/2302.03493) | 修正的条件t-SNE算法通过对高维相似度进行条件限制，解决了在数据经过标签良好聚类时，条件t-SNE算法的不足。并通过对相似度矩阵存储的改变提高了可扩展性。 |
| [^20] | [On a continuous time model of gradient descent dynamics and instability in deep learning.](http://arxiv.org/abs/2302.01952) | 本文提出了一个连续时间流模型——主要流（PF），通过对Hessian矩阵特征分解的依赖，捕捉到了梯度下降中的发散和振荡行为、逃逸局部极小值和鞍点的连续性流，并解释了深度学习中的稳定边缘现象。通过对不稳定性的新理解，提出了一种学习率适应方法，可以控制训练稳定性和测试集评估性能之间的权衡。 |
| [^21] | [The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima.](http://arxiv.org/abs/2210.01513) | 本文研究了一种梯度优化方法SAM，发现在凸二次目标中它会在最小值两侧来回振荡，但在非二次情况中从光谱范数的角度执行梯度下降，更新方式被认为是Hessian矩阵在领先特征向量方向上的导数，鼓励漂移向更宽的极小值。 |
| [^22] | [Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability.](http://arxiv.org/abs/2209.15594) | 传统梯度下降分析不适用于现代神经网络用全批次或大批次梯度下降训练，Cohen等人(2021)发现的梯度下降边缘稳定性现象表明，当锐度达到不稳定性截止值$2/\eta$时，迭代具有自稳定性，并表现出隐式偏向稳定边缘解的偏差，这种现象通过捕获二阶和三阶导数的比例系数得到了解释。 |
| [^23] | [DIET: Conditional independence testing with marginal dependence measures of residual information.](http://arxiv.org/abs/2208.08579) | 本文提出了一种名为解耦的独立性检验（DIET）算法，通过利用边际独立统计量测试条件独立关系，避免了需要大量预测模型的拟合和相互作用的启发式方法所导致的损失功率问题。 |
| [^24] | [Delayed Feedback in Generalised Linear Bandits Revisited.](http://arxiv.org/abs/2207.10786) | 研究了广义线性赌博机中的延迟奖励现象，提出了一种自然的乐观算法，可实现一个独立于时间的惩罚函数，降低了现有工作中随着时间增长而增加的惩罚函数的界限。 |
| [^25] | [Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient.](http://arxiv.org/abs/2206.01209) | 本文开发了针对具有局部Lipschitz连续梯度的凸优化问题的加速一阶方法，分别提出了无约束凸优化的加速近端梯度算法(APG)以及约束凸优化的一阶近端增广拉格朗日方法。通过这些方法，可以快速地寻找出解决方案。 |
| [^26] | [Multi-model Ensemble Analysis with Neural Network Gaussian Processes.](http://arxiv.org/abs/2202.04152) | 本文提出了一种基于神经网络高斯过程的统计方法，名为NN-GPR，该方法可以高效地进行多个气候模型的集成分析，并在表面温度和降水预测中表现出色，能够保留多个尺度的地理空间信号和捕捉年际变化，特别在高变异区域具有改进的准确性和不确定性量化技能。 |
| [^27] | [Entropy Regularized Reinforcement Learning Using Large Deviation Theory.](http://arxiv.org/abs/2106.03931) | 本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。 |
| [^28] | [On Feature Relevance Uncertainty: A Monte Carlo Dropout Sampling Approach.](http://arxiv.org/abs/2008.01468) | 本文提出了一种名为MCRP的方法，采用蒙特卡罗估计计算特征相关性分布，以评估特征相关性的不确定性，以更好地理解神经网络感知和推理。 |
| [^29] | [Convolutional Neural Networks for Epileptic Seizure Prediction.](http://arxiv.org/abs/1811.00915) | 本文提出了一种使用卷积神经网络拓扑结构进行iEEG的癫痫发作预测的新方法，避免提取手工特征，并在多个数据集上进行了评估。研究结果表明该方法具有普遍适用性。 |
| [^30] | [Pairwise Covariates-adjusted Block Model for Community Detection.](http://arxiv.org/abs/1807.03469) | 基于SBM模型，双重协变量调整的PCABM模型添加了关于节点间关系的附加信息。SCWA算法对PCABM模型进行了高效求解。模拟实验和实际数据分析表明PCABM模型具有优异的性能和预测能力。 |

# 详细

[^1]: ChemCrow:用化学工具增强大型语言模型

    ChemCrow: Augmenting large-language models with chemistry tools. (arXiv:2304.05376v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.05376](http://arxiv.org/abs/2304.05376)

    本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。

    

    近期大型语言模型(LLMs)在跨领域的任务表现出一定的优势，但在化学相关问题上却表现不佳。此外，这些模型缺乏访问外部知识源，限制了它们在科学应用中的有用性。在本研究中，我们介绍了ChemCrow，一种LLM化学代理，旨在完成有机合成、药物发现和材料设计等任务。通过整合13个专家设计的工具，ChemCrow提高了LLM在化学中的性能，并产生了新的能力。我们的评估，包括LLM和人类专家评估，证明了ChemCrow在自动化各种化学任务方面的有效性。令人惊讶的是，我们发现GPT-4作为评估器无法区分明显错误的GPT-4完成和GPT-4 + ChemCrow性能。这种工具的滥用有很大的风险，我们讨论了它们的潜在危害。在负责任的情况下，ChemCrow不仅可以帮助专业化学家并降低成本。

    Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers ba
    
[^2]: 《无免费午餐定理、科尔莫戈洛夫复杂性及归纳偏差在机器学习中的作用》

    The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning. (arXiv:2304.05366v1 [cs.LG])

    [http://arxiv.org/abs/2304.05366](http://arxiv.org/abs/2304.05366)

    本论文阐述了无免费午餐定理的监督学习中的限制，证明了归纳偏差可以提高学习算法的效果，并且展示了神经网络模型的偏好与现实世界的数据分布相关。

    

    监督学习的无免费午餐定理指出，没有一个学习算法可以解决所有问题，或者所有学习算法在均匀分布的学习问题上平均精度达到完全相同。因此，这些定理经常被引用来支持个别问题需要特别定制的归纳偏差的概念。我们认为，尽管几乎所有均匀采样的数据集具有高复杂性，但现实世界中的问题不成比例地产生低复杂度的数据，并且我们认为神经网络模型也具有同样的偏好，这种偏好使用科尔莫戈洛夫复杂度进行了形式化。值得注意的是，我们展示了为特定领域设计的体系结构，例如计算机视觉，可以压缩各种看似不相关的领域的数据集。我们的实验表明，预先训练和即使是随机初始化的语言模型都更喜欢生成低复杂度的序列。尽管无免费午餐定理似乎表明各个问题需要专门的学习算法，但我们解释说，学习算法通常可以通过编码关于真实世界数据分布的先前知识的归纳偏差来改进。

    No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we exp
    
[^3]: 我们实现了个性化治疗吗？使用重复采样的在线强化学习算法进行个性化评估

    Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])

    [http://arxiv.org/abs/2304.05365](http://arxiv.org/abs/2304.05365)

    本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。

    

    在数字健康中，使用强化学习（RL）个性化治疗序列以支持用户采取更健康的行为越来越受到关注。这种连续决策问题涉及到基于用户的上下文（例如，先前的活动水平、位置等）在何时治疗以及如何治疗的决定。在线RL算法是这个问题的一个有前途的数据驱动方法，因为它基于每个用户的历史反馈进行学习，并利用这些知识个性化这些决策。然而，要决定是否应在实际部署的“优化”干预中包含RL算法，我们必须评估数据证据，表明RL算法实际上正在将治疗个性化适应其用户。由于RL算法中的随机性，人们可能会对其在某些状态下的学习并使用此学习来提供特定治疗的能力产生误解。我们使用工作定义的个性化，并介绍了一种重复采样政策评估方法来评估在线RL算法实现的个性化水平。我们使用模拟评估了我们提出的方法，并展示了我们的方法可以准确地识别个性化的策略。我们提出的方法在优化数字健康的个性化干预方面具有潜在应用。

    There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
    
[^4]: 约束域的扩散模型

    Diffusion Models for Constrained Domains. (arXiv:2304.05364v1 [cs.LG])

    [http://arxiv.org/abs/2304.05364](http://arxiv.org/abs/2304.05364)

    本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。

    

    降噪扩散模型是新近涌现的一种生成模型，它在无条件图像生成和语音生成等众多领域实现了最先进的成果。它们由破坏数据的加噪过程和定义为加噪扩散的时间反演的后向阶段组成。以这些成功为基础，扩散模型最近扩展到了黎曼流形设置。然而，这些黎曼扩散模型要求在所有时间上定义测地线。虽然该设置包括许多重要应用，但不包括由不等式约束集定义的流形，这在许多科学领域，如机器人和蛋白设计中是普遍存在的。在本文中，我们介绍了两种方法来弥合这个差距。首先，我们设计了一个基于不等式约束诱导的对数障碍度量的加噪过程。其次，我们介绍了一种基于反射布朗运动的加噪过程。由于现有的扩散模型不能直接应用于约束域，因此本文提出了两种方法来创建约束域的降噪扩散模型。

    Denoising diffusion models are a recent class of generative models which achieve state-of-the-art results in many domains such as unconditional image generation and text-to-speech tasks. They consist of a noising process destroying the data and a backward stage defined as the time-reversal of the noising diffusion. Building on their success, diffusion models have recently been extended to the Riemannian manifold setting. Yet, these Riemannian diffusion models require geodesics to be defined for all times. While this setting encompasses many important applications, it does not include manifolds defined via a set of inequality constraints, which are ubiquitous in many scientific domains such as robotics and protein design. In this work, we introduce two methods to bridge this gap. First, we design a noising process based on the logarithmic barrier metric induced by the inequality constraints. Second, we introduce a noising process based on the reflected Brownian motion. As existing diffu
    
[^5]: 基于分层张量草图的生成建模

    Generative Modeling via Hierarchical Tensor Sketching. (arXiv:2304.05305v1 [math.NA])

    [http://arxiv.org/abs/2304.05305](http://arxiv.org/abs/2304.05305)

    本文提出了一种利用分层张量草图来近似高维概率密度的方法，通过随机奇异值分解技术解决线性方程达到此目的，其算法复杂度在高维密度维度上呈线性规模。

    

    我们提出了一种使用经验分布来近似高维概率密度的分层张量网络方法。该方法利用随机奇异值分解（SVD）技术，并涉及在该张量网络中解线性方程以获得张量核心。该算法的复杂性在高维密度的维度上呈线性规模。通过几个数值实验，对估计误差进行了分析，证明了此方法的有效性。

    We propose a hierarchical tensor-network approach for approximating high-dimensional probability density via empirical distribution. This leverages randomized singular value decomposition (SVD) techniques and involves solving linear equations for tensor cores in this tensor network. The complexity of the resulting algorithm scales linearly in the dimension of the high-dimensional density. An analysis of estimation error demonstrates the effectiveness of this method through several numerical experiments.
    
[^6]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^7]: 基于L2，0基数惩罚的不均匀图趋势过滤。

    Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])

    [http://arxiv.org/abs/2304.05223](http://arxiv.org/abs/2304.05223)

    本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。

    

    我们研究了在图上估计分段平滑信号的方法，并提出了一种$\ell_{2,0}$-范数惩罚图趋势过滤（GTF）模型，以估计在节点之间具有不均匀平滑水平的分段平滑图信号。我们证明了所提出的GTF模型同时是基于节点上的信号的k-means聚类和基于图的最小割，其中聚类和割共享相同的分配矩阵。我们提出了两种方法来解决所提出的GTF模型：一种是基于谱分解的方法，另一种是基于模拟退火的方法。在合成和现实数据集的实验中，我们展示了所提出的GTF模型在降噪、支持恢复和半监督分类任务上表现更好，且比现有方法更高效地解决了大型数据集的问题。

    We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
    
[^8]: 自动梯度下降：无超参数的深度学习

    Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])

    [http://arxiv.org/abs/2304.05187](http://arxiv.org/abs/2304.05187)

    本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。

    

    本文提出了一种新的方法来派生特定于神经网络结构的优化算法，实现了无超参数的一阶优化器，称为“自动梯度下降”。该方法利用神经体系结构显式地定义网络结构参数来优化深度全连接网络和卷积网络，证明了在标准基准测试数据集上与手动调整优化器效果相当。该算法扩展了镜像下降方法以处理非凸性复合目标函数。

    The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im
    
[^9]: 基于Schr{\"o}dinger bridge的时序生成建模

    Generative modeling for time series via Schr{\"o}dinger bridge. (arXiv:2304.05093v1 [math.OC])

    [http://arxiv.org/abs/2304.05093](http://arxiv.org/abs/2304.05093)

    本文提出了一种基于Schr{\"o}dinger bridge的时序生成模型，可以通过熵插值最优传输来处理参考路径空间上的参考概率分布和目标测度之间的关系，模型的解由有限时间区间内的随机微分方程和由数据估计的路径依赖漂移函数组成，通过模拟SB扩散过程产生新的合成数据样本，实验证明该方法在玩具自回归模型、GARCH模型和分形布朗运动方面具有很好的性能。

    

    本文提出了一种新的基于Schr{\"o}dinger bridge (SB)方法的时序生成模型。该方法通过熵插值最优传输来处理参考路径空间上的参考概率分布和与时间序列的联合数据分布一致的目标测度之间的关系，模型的解由有限时间区间内的随机微分方程和路径依赖漂移函数组成，因此能很好地保持时间序列分布的时域特性。我们可以通过核回归方法或LSTM神经网络从数据样本中估计漂移函数，并通过模拟SB扩散过程产生新的合成数据样本。文章通过一系列数值实验评估了我们的生成模型的性能。首先，我们用玩具自回归模型、GARCH模型和分形布朗运动来测试，用边际和时间依赖度量我们算法的准确性。

    We propose a novel generative model for time series based on Schr{\"o}dinger bridge (SB) approach. This consists in the entropic interpolation via optimal transport between a reference probability measure on path space and a target measure consistent with the joint data distribution of the time series. The solution is characterized by a stochastic differential equation on finite horizon with a path-dependent drift function, hence respecting the temporal dynamics of the time series distribution. We can estimate the drift function from data samples either by kernel regression methods or with LSTM neural networks, and the simulation of the SB diffusion yields new synthetic data samples of the time series. The performance of our generative model is evaluated through a series of numerical experiments. First, we test with a toy autoregressive model, a GARCH Model, and the example of fractional Brownian motion, and measure the accuracy of our algorithm with marginal and temporal dependencies 
    
[^10]: 实际稀疏变分高斯过程

    Actually Sparse Variational Gaussian Processes. (arXiv:2304.05091v1 [stat.ML])

    [http://arxiv.org/abs/2304.05091](http://arxiv.org/abs/2304.05091)

    提出了一种新的跨领域变量高斯过程类，利用紧支撑B样条基函数，可以使用稀疏线性代数来显著加快矩阵运算， 实现在大规模数据集下快速高效地建模快速变化的空间现象。

    

    高斯过程（GP）通常因计算和内存需求的不利扩展而受到批评。对于大量数据集，稀疏 GP 通过在少量感应变量的条件下对数据进行总结来减少这些需求。然而，在需要许多感应变量的大型数据集（例如低长度尺度空间数据）中，即使是稀疏 GP 也可能变得计算昂贵，受使用感应变量的数量的限制。在这项工作中，我们提出了一种新的跨领域变量高斯过程类，通过将 GP 投影到一组紧支撑 B 样条基函数上来构建。我们方法的关键优势在于，B 样条基函数的紧支撑允许使用稀疏线性代数来显著加快矩阵运算，并大大减少存储占用。这使我们能够使用数以万计的感应变量非常有效地建模快速变化的空间现象，而先前的方法由于感应变量的使用限制而无法实现。

    Gaussian processes (GPs) are typically criticised for their unfavourable scaling in both computational and memory requirements. For large datasets, sparse GPs reduce these demands by conditioning on a small set of inducing variables designed to summarise the data. In practice however, for large datasets requiring many inducing variables, such as low-lengthscale spatial data, even sparse GPs can become computationally expensive, limited by the number of inducing variables one can use. In this work, we propose a new class of inter-domain variational GP, constructed by projecting a GP onto a set of compactly supported B-spline basis functions. The key benefit of our approach is that the compact support of the B-spline basis functions admits the use of sparse linear algebra to significantly speed up matrix operations and drastically reduce the memory footprint. This allows us to very efficiently model fast-varying spatial phenomena with tens of thousands of inducing variables, where previo
    
[^11]: 一种基于数据驱动的状态聚合方法用于动态离散选择模型

    A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])

    [http://arxiv.org/abs/2304.04916](http://arxiv.org/abs/2304.04916)

    本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。

    

    我们研究了动态离散选择模型，其中一个常见的问题是使用代理行为数据估计代理奖励函数（也称为“结构参数”）的参数。这种模型的最大似然估计需要动态规划，这受到维度灾难的限制。在本文中，我们提出了一种新颖的算法，提供了一种数据驱动的方法来选择和聚合状态，降低了估计的计算和样本复杂度。我们的方法分两个阶段。在第一阶段中，我们使用灵活的反向强化学习方法来估计代理Q函数。我们使用这些估计的Q函数，以及一个聚类算法，选择了一些最为重要的状态，这些状态对于驱动Q函数的变化最为关键。在第二阶段，利用这些被选择的“聚合”状态，我们使用常用的嵌套固定点算法进行最大似然估计。所提出的二阶段方法实现了...

    We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
    
[^12]: 基于梯度的不确定性归因于可解释的贝叶斯深度学习

    Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning. (arXiv:2304.04824v1 [cs.LG])

    [http://arxiv.org/abs/2304.04824](http://arxiv.org/abs/2304.04824)

    本论文提出了一个基于梯度的不确定性归因方法来确定导致预测的不确定性的最具问题性的输入区域，从而实现可解释和可操作的贝叶斯深度学习。

    

    深度学习模型所作出的预测容易受到数据扰动、对抗攻击和超出分布范围的输入的影响。为了构建一个可信赖的AI系统，准确量化预测的不确定性至关重要。虽然当前的工作重点是提高不确定性量化的准确性和效率，但有必要确定不确定性源并采取措施减轻对预测的影响。因此，我们提出了开发可解释和可操作的贝叶斯深度学习方法来不仅进行准确的不确定性量化，而且解释不确定性、确定其来源并提出减轻不确定性影响的策略。具体而言，我们引入了一种基于梯度的不确定性归因方法来确定最具问题性的输入区域，从而导致预测的不确定性。与现有方法相比，所提出的UA-Backprop方法具有竞争性的准确性、松弛的假设和高效性。

    Predictions made by deep learning models are prone to data perturbations, adversarial attacks, and out-of-distribution inputs. To build a trusted AI system, it is therefore critical to accurately quantify the prediction uncertainties. While current efforts focus on improving uncertainty quantification accuracy and efficiency, there is a need to identify uncertainty sources and take actions to mitigate their effects on predictions. Therefore, we propose to develop explainable and actionable Bayesian deep learning methods to not only perform accurate uncertainty quantification but also explain the uncertainties, identify their sources, and propose strategies to mitigate the uncertainty impacts. Specifically, we introduce a gradient-based uncertainty attribution method to identify the most problematic regions of the input that contribute to the prediction uncertainty. Compared to existing methods, the proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high efficiency.
    
[^13]: 通过潜在意图从被动数据中进行强化学习

    Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])

    [http://arxiv.org/abs/2304.04782](http://arxiv.org/abs/2304.04782)

    本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。

    

    被动观察数据丰富而富有信息，然而当前强化学习方法很少能够利用该数据。本文提出了一种通过建模意图从被动数据中进行学习的方法，该方法通过衡量当智能体为实现特定任务而采取行动时未来结果的可能性如何变化来学习意图。我们提出了一个时差学习目标来学习意图，得到了一个类似于传统强化学习的算法，但是完全是从被动数据中学习得到的。通过优化该目标，我们的智能体可以同时从原始的观察数据中学习出状态、策略和环境下的可能结果。从理论和实验上看，该方法学习出的特征可用于下游任务的价值预测。

    Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m
    
[^14]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^15]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^16]: 带有降低通信要求的异步在线联邦学习

    Asynchronous Online Federated Learning with Reduced Communication Requirements. (arXiv:2303.15226v1 [cs.LG])

    [http://arxiv.org/abs/2303.15226](http://arxiv.org/abs/2303.15226)

    提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，能够处理异构和延迟设备，实现参与学习任务的可访问性和效率。

    

    在线联邦学习（FL）使得地理分布的设备可以从本地的流数据中学习到全局共享模型。大多数关于在线FL的文献都考虑了最佳情况下的参与客户端和通信渠道。然而，这些假设在实际应用中通常无法满足。异步设置可以反映出更现实的环境，例如由于可用的计算能力和电池限制而发生的异构客户端参与，以及由通信渠道或落后设备引起的延迟。此外，在大多数应用中，必须考虑能源效率。我们提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，通过减少参与者的通信开销，提高了参与学习任务的可访问性和效率。此外，该方法能够处理异构和延迟设备，使其更适用于实际应用。

    Online federated learning (FL) enables geographically distributed devices to learn a global shared model from locally available streaming data. Most online FL literature considers a best-case scenario regarding the participating clients and the communication channels. However, these assumptions are often not met in real-world applications. Asynchronous settings can reflect a more realistic environment, such as heterogeneous client participation due to available computational power and battery constraints, as well as delays caused by communication channels or straggler devices. Further, in most applications, energy efficiency must be taken into consideration. Using the principles of partial-sharing-based communications, we propose a communication-efficient asynchronous online federated learning (PAO-Fed) strategy. By reducing the communication overhead of the participants, the proposed method renders participation in the learning task more accessible and efficient. In addition, the prop
    
[^17]: 找到对抗样本需要多少维度？

    How many dimensions are required to find an adversarial example?. (arXiv:2303.14173v1 [cs.LG])

    [http://arxiv.org/abs/2303.14173](http://arxiv.org/abs/2303.14173)

    本文研究了对抗性漏洞如何取决于受限于高维输入空间中的子空间维数，同时针对标准PGD攻击的对抗性成功率提出了单调递增函数表达式。

    

    过去探索对抗性漏洞的研究都着眼于对手可以扰动模型输入的所有维度的情况。另一方面，许多最近的研究考虑以下情况：（i）对手可以扰动有限数量的输入参数或（ii）多模态问题中的模态子集。在这两种情况下，对抗性样本有效地受限于高维输入空间中的子空间$V$。出于这个动机，我们在本文中研究了对抗性漏洞如何取决于$V$的维数。特别地，我们展示了标准PGD攻击的对抗性成功率如何表现为$\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$的单调递增函数，其中$\epsilon$是扰动预算，$\frac{1}{p}+\frac{q}{q}=1$，只要$p>1$（当$p=1$时会出现额外的细微差别，我们对此进行了详细的分析）。这个函数形式可以很容易地推导。

    Past work exploring adversarial vulnerability have focused on situations where an adversary can perturb all dimensions of model input. On the other hand, a range of recent works consider the case where either (i) an adversary can perturb a limited number of input parameters or (ii) a subset of modalities in a multimodal problem. In both of these cases, adversarial examples are effectively constrained to a subspace $V$ in the ambient input space $\mathcal{X}$. Motivated by this, in this work we investigate how adversarial vulnerability depends on $\dim(V)$. In particular, we show that the adversarial success of standard PGD attacks with $\ell^p$ norm constraints behaves like a monotonically increasing function of $\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$ where $\epsilon$ is the perturbation budget and $\frac{1}{p} + \frac{1}{q} =1$, provided $p > 1$ (the case $p=1$ presents additional subtleties which we analyze in some detail). This functional form can be easily deriv
    
[^18]: 连续时间延迟系统的神经拉普拉斯控制

    Neural Laplace Control for Continuous-time Delayed Systems. (arXiv:2302.12604v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12604](http://arxiv.org/abs/2302.12604)

    本文介绍了一种神经拉普拉斯控制方法，用于解决具有不规则状态观测和未知延迟的连续时间环境下的离线强化学习问题。

    

    许多实际的离线强化学习问题包括具有延迟的连续时间环境。这些环境具有两个显著特点：首先，观察到的状态x(t)在不规则的时间间隔内进行观察；其次，当前行动a(t)仅在未知延迟g > 0 的情况下影响未来状态x(t+g)。这样的环境的一个典型例子是卫星控制，其中地球和卫星之间的通信链路会造成观测不规则和延迟。现有的离线强化学习算法在具有时间不规则观测或已知延迟的环境中取得了成功。然而，涉及时间不规则观测和未知延迟的环境仍然是一个开放且具有挑战性的问题。因此，我们提出了神经拉普拉斯控制，一种连续时间基于模型的离线强化学习方法，将神经拉普拉斯动力学模型与模型预测控制（MPC）规划器相结合，并能够从离线数据集中进行学习。

    Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g > 0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sam
    
[^19]: 修正的条件t-SNE算法：超越最近邻近的限制

    Revised Conditional t-SNE: Looking Beyond the Nearest Neighbors. (arXiv:2302.03493v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03493](http://arxiv.org/abs/2302.03493)

    修正的条件t-SNE算法通过对高维相似度进行条件限制，解决了在数据经过标签良好聚类时，条件t-SNE算法的不足。并通过对相似度矩阵存储的改变提高了可扩展性。

    

    条件t-SNE（ct-SNE）是t-SNE的最新扩展，它允许从嵌入中删除已知的聚类信息，以获得揭示标签信息之外的结构的可视化结果。这在需要消除一组类之间不想要的差异时非常有用。我们发现在许多现实情况下，ct-SNE的表现并不理想，尤其是当数据在原始的高维空间中按标签进行良好的聚类时。我们提出了一种改进的方法，通过对高维相似度进行条件限制，并将基于标签的最近邻和跨标签的最近邻存储在不同的矩阵中。这还使得我们能够使用最近提出的t-SNE加速技术，提高算法的可扩展性。从合成数据的实验中，我们发现我们提出的方法解决了考虑的问题，并改善了嵌入质量。但是在包含批次效应的实际数据中，预期的改进并不总是存在。我们认为经过修订的ct-SNE较原始的算法更优。

    Conditional t-SNE (ct-SNE) is a recent extension to t-SNE that allows removal of known cluster information from the embedding, to obtain a visualization revealing structure beyond label information. This is useful, for example, when one wants to factor out unwanted differences between a set of classes. We show that ct-SNE fails in many realistic settings, namely if the data is well clustered over the labels in the original high-dimensional space. We introduce a revised method by conditioning the high-dimensional similarities instead of the low-dimensional similarities and storing within- and across-label nearest neighbors separately. This also enables the use of recently proposed speedups for t-SNE, improving the scalability. From experiments on synthetic data, we find that our proposed method resolves the considered problems and improves the embedding quality. On real data containing batch effects, the expected improvement is not always there. We argue revised ct-SNE is preferable ove
    
[^20]: 关于梯度下降动态和深度学习不稳定性的连续时间模型研究

    On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01952](http://arxiv.org/abs/2302.01952)

    本文提出了一个连续时间流模型——主要流（PF），通过对Hessian矩阵特征分解的依赖，捕捉到了梯度下降中的发散和振荡行为、逃逸局部极小值和鞍点的连续性流，并解释了深度学习中的稳定边缘现象。通过对不稳定性的新理解，提出了一种学习率适应方法，可以控制训练稳定性和测试集评估性能之间的权衡。

    

    深度学习成功的秘诀在于神经网络和基于梯度的优化的结合。然而，理解梯度下降的行为，特别是其不稳定性，落后于其经验成功。为了增加研究梯度下降的理论工具，我们提出了主要流（PF），一种近似梯度下降动态的连续时间流。据我们所知，PF是唯一捕捉到梯度下降的发散和振荡行为，包括逃逸局部极小值和鞍点的连续性流。通过其对于Hessian特征分解的依赖，PF解释了深度学习中最近观察到的稳定边缘现象。通过对不稳定性的新理解，我们提出了一种学习率适应方法，使我们能够控制训练稳定性和测试集评估性能之间的权衡。

    The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.
    
[^21]: 锐度感知优化的动态：从峡谷反弹到漂向宽极小值

    The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima. (arXiv:2210.01513v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01513](http://arxiv.org/abs/2210.01513)

    本文研究了一种梯度优化方法SAM，发现在凸二次目标中它会在最小值两侧来回振荡，但在非二次情况中从光谱范数的角度执行梯度下降，更新方式被认为是Hessian矩阵在领先特征向量方向上的导数，鼓励漂移向更宽的极小值。

    

    本文研究了一种名为锐度感知优化（SAM）的梯度优化方法，该方法在图像和语言预测问题上表现出较好的性能。我们表明，当使用SAM应用于凸二次目标时，针对大多数随机初始化，它会收敛于在沿着主曲率最大方向的最小值两侧来回振荡的循环，并给出了收敛率的界限。在非二次情况下，我们表明这种振荡实质上是在Hessian矩阵的光谱范数上以更小的步长执行梯度下降，SAM的更新可被认为是一个三阶导数 - 即Hessian矩阵在领先的特征向量方向上的导数，它鼓励朝着更宽的极小值漂移。

    We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence.  In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM's update may be regarded as a third derivative -the derivative of the Hessian in the leading eigenvector direction -- that encourages drift toward wider minima.
    
[^22]: 自稳定性：梯度下降在稳定边缘的隐式偏差

    Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. (arXiv:2209.15594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15594](http://arxiv.org/abs/2209.15594)

    传统梯度下降分析不适用于现代神经网络用全批次或大批次梯度下降训练，Cohen等人(2021)发现的梯度下降边缘稳定性现象表明，当锐度达到不稳定性截止值$2/\eta$时，迭代具有自稳定性，并表现出隐式偏向稳定边缘解的偏差，这种现象通过捕获二阶和三阶导数的比例系数得到了解释。

    

    传统的梯度下降分析表明，当Hessian矩阵的最大特征值，也称为锐度$S(\theta)$，被$2/\eta$限制时，训练是“稳定的”，训练损失单调下降。然而，最近的研究发现，当用全批量或大批量梯度下降训练现代神经网络时，这种假设不成立。最近，Cohen等人(2021)观察到了两个重要现象。第一个现象被称为渐进锐化，在训练期间锐度稳步增加，直到达到不稳定性截止值$2/\eta$。第二个现象被称为稳定边缘，在剩余的训练过程中，锐度停留在$2/\eta$，而损失则持续下降，尽管不是单调的。我们证明了，在稳定边缘处，梯度下降的动态可以由一个三次泰勒展开式捕获:当迭代在Hessian矩阵的最大特征向量方向上发散时，锐度呈二次增长，比例系数由损失的二阶和三阶导数决定。当锐度精确为$2/\eta$时，我们表明，迭代的自稳定性可以永久地保持在稳定边缘。此外，我们通过实验证明，这种自稳定现象使梯度下降具有隐式偏向稳定边缘解的偏差，而稳定边缘是损失明显低于稳定区域的区域。

    Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\theta)$, is bounded by $2/\eta$, training is "stable" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessi
    
[^23]: DIET: 利用剩余信息的边际相关度量进行条件独立性检验

    DIET: Conditional independence testing with marginal dependence measures of residual information. (arXiv:2208.08579v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.08579](http://arxiv.org/abs/2208.08579)

    本文提出了一种名为解耦的独立性检验（DIET）算法，通过利用边际独立统计量测试条件独立关系，避免了需要大量预测模型的拟合和相互作用的启发式方法所导致的损失功率问题。

    

    条件随机化检验（CRT）用于评估变量$x$在已知协变量$z$的情况下对另一个变量$y$的预测能力。CRT通常需要大量预测模型的拟合，这通常是计算上不可行的。现有的解决方案通常将数据集分成训练和测试部分，或依靠相互作用的启发式方法，这两种方法都会导致功率损失。本文提出了解耦的独立性检验（DIET）算法，通过利用边际独立统计量测试条件独立关系，避免了这两种问题。DIET测试两个随机变量的边际独立性：$F(x \mid z)$和$F(y \mid z)$其中$F(\cdot \mid z)$是条件累积分布函数（CDF），这些变量被称为“信息残差”。我们给出了DIET实现有限样本的类型1错误控制和功率大于类型1错误率的充分条件。然后，我们证明当DIET应用时，数据的分布不需要满足任何特定要求。

    Conditional randomization tests (CRTs) assess whether a variable $x$ is predictive of another variable $y$, having observed covariates $z$. CRTs require fitting a large number of predictive models, which is often computationally intractable. Existing solutions to reduce the cost of CRTs typically split the dataset into a train and test portion, or rely on heuristics for interactions, both of which lead to a loss in power. We propose the decoupled independence test (DIET), an algorithm that avoids both of these issues by leveraging marginal independence statistics to test conditional independence relationships. DIET tests the marginal independence of two random variables: $F(x \mid z)$ and $F(y \mid z)$ where $F(\cdot \mid z)$ is a conditional cumulative distribution function (CDF). These variables are termed "information residuals." We give sufficient conditions for DIET to achieve finite sample type-1 error control and power greater than the type-1 error rate. We then prove that when 
    
[^24]: 延迟反馈在广义线性赌博机中的研究再访

    Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10786](http://arxiv.org/abs/2207.10786)

    研究了广义线性赌博机中的延迟奖励现象，提出了一种自然的乐观算法，可实现一个独立于时间的惩罚函数，降低了现有工作中随着时间增长而增加的惩罚函数的界限。

    

    随着许多真实世界的应用中奖励几乎总是被延迟，导致要求即时奖励的模型难以应用。本文将研究在广义线性赌博机中延迟奖励的现象。我们证明了一种自然的乐观算法适应延迟反馈领域能够有一个与时间无关的惩罚函数。这比现有的工作显著的提高了，因为最佳的已知的惩罚函数的界限随着时间的推移而增加。

    The stochastic generalised linear bandit is a well-understood model for sequential decision-making problems, with many algorithms achieving near-optimal regret guarantees under immediate feedback. However, the stringent requirement for immediate rewards is unmet in many real-world applications where the reward is almost always delayed. We study the phenomenon of delayed rewards in generalised linear bandits in a theoretical manner. We show that a natural adaptation of an optimistic algorithm to the delayed feedback achieves a regret bound where the penalty for the delays is independent of the horizon. This result significantly improves upon existing work, where the best known regret bound has the delay penalty increasing with the horizon. We verify our theoretical results through experiments on simulated data.
    
[^25]: 具有局部Lipschitz连续梯度的凸优化的加速一阶方法。

    Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient. (arXiv:2206.01209v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.01209](http://arxiv.org/abs/2206.01209)

    本文开发了针对具有局部Lipschitz连续梯度的凸优化问题的加速一阶方法，分别提出了无约束凸优化的加速近端梯度算法(APG)以及约束凸优化的一阶近端增广拉格朗日方法。通过这些方法，可以快速地寻找出解决方案。

    

    本文针对具有局部Lipschitz连续梯度的凸优化问题，开发了加速的一阶方法。特别地，我们首先考虑了具有LLCG的无约束凸优化，并提出了加速近端梯度算法(APG)来解决它。所提出的APG方法具有可验证的终止准则，并且在寻找无约束凸优化和强凸优化问题的ε-残差解时，其操作复杂度分别为${\cal O}(\varepsilon^{-1/2}\log \varepsilon^{-1})$和${\cal O}(\log \varepsilon^{-1})$。然后，我们考虑了具有LLCG的约束凸优化，并提出了一种一阶近端增广拉格朗日方法来解决它，通过将我们提出的APG方法应用于近似求解一系列近端增广拉格朗日子问题。由此得出的方法具有可验证的终止准则，并且具有快速的收敛速度。

    In this paper we develop accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient (LLCG), which is beyond the well-studied class of convex optimization with Lipschitz continuous gradient. In particular, we first consider unconstrained convex optimization with LLCG and propose accelerated proximal gradient (APG) methods for solving it. The proposed APG methods are equipped with a verifiable termination criterion and enjoy an operation complexity of ${\cal O}(\varepsilon^{-1/2}\log \varepsilon^{-1})$ and ${\cal O}(\log \varepsilon^{-1})$ for finding an $\varepsilon$-residual solution of an unconstrained convex and strongly convex optimization problem, respectively. We then consider constrained convex optimization with LLCG and propose an first-order proximal augmented Lagrangian method for solving it by applying one of our proposed APG methods to approximately solve a sequence of proximal augmented Lagrangian subproblems. The resulting method is 
    
[^26]: 采用神经网络高斯过程的多模型集成分析

    Multi-model Ensemble Analysis with Neural Network Gaussian Processes. (arXiv:2202.04152v4 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2202.04152](http://arxiv.org/abs/2202.04152)

    本文提出了一种基于神经网络高斯过程的统计方法，名为NN-GPR，该方法可以高效地进行多个气候模型的集成分析，并在表面温度和降水预测中表现出色，能够保留多个尺度的地理空间信号和捕捉年际变化，特别在高变异区域具有改进的准确性和不确定性量化技能。

    

    多模型集成分析将多个气候模型的信息集成到一个统一的预测中。然而，现有的基于模型平均的集成方法可能会稀释细粒度的空间信息，并在重新缩放低分辨率气候模型时引入偏差。我们提出了一种统计方法，称为NN-GPR，使用具有无限宽的深度神经网络协方差函数的高斯过程回归（GPR）。NN-GPR对模型之间的关系不需要任何假设，不需要通过插值到公共网格来统一，不需要考虑平稳性假设，并且其预测算法自动进行降尺度处理。模型实验表明，NN-GPR在保留多个尺度的地理空间信号和捕捉年际变化方面在表面温度和降水预测方面可以非常熟练。我们的预测特别显示在高变异区域具有改进的准确性和不确定性量化技能，这使我们能够便宜地评估气候变化对生态系统的影响。

    Multi-model ensemble analysis integrates information from multiple climate models into a unified projection. However, existing integration approaches based on model averaging can dilute fine-scale spatial information and incur bias from rescaling low-resolution climate models. We propose a statistical approach, called NN-GPR, using Gaussian process regression (GPR) with an infinitely wide deep neural network based covariance function. NN-GPR requires no assumptions about the relationships between models, no interpolation to a common grid, no stationarity assumptions, and automatically downscales as part of its prediction algorithm. Model experiments show that NN-GPR can be highly skillful at surface temperature and precipitation forecasting by preserving geospatial signals at multiple scales and capturing inter-annual variability. Our projections particularly show improved accuracy and uncertainty quantification skill in regions of high variability, which allows us to cheaply assess ta
    
[^27]: 利用大偏差理论的熵正则化强化学习

    Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03931](http://arxiv.org/abs/2106.03931)

    本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。

    

    强化学习是机器学习中一个重要的研究领域，越来越多地被应用于物理学中的复杂优化问题。同时，物理学中的概念也为强化学习带来了重大进展，如熵正则化强化学习。然而，针对熵正则化强化学习中优化的解析解目前是一个未解之谜。在本文中，我们建立了熵正则化强化学习与非平衡统计力学的联系，重点关注在罕见事件条件下的马尔可夫过程。在长时间极限下，我们应用大偏差理论的方法，推导出马尔可夫决策过程（MDP）模型中最优策略和最优动态的精确解析结果，从而得到了一个新的熵正则化强化学习的分析和计算框架，经过模拟验证。

    Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The
    
[^28]: 特征相关性不确定性的蒙特卡罗Dropout采样方法研究

    On Feature Relevance Uncertainty: A Monte Carlo Dropout Sampling Approach. (arXiv:2008.01468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.01468](http://arxiv.org/abs/2008.01468)

    本文提出了一种名为MCRP的方法，采用蒙特卡罗估计计算特征相关性分布，以评估特征相关性的不确定性，以更好地理解神经网络感知和推理。

    

    理解神经网络决策是实现智能系统在现实应用中的关键，然而这些系统不透明的决策过程在需要解释性的情况下是不利的。为了更好地理解神经网络的决策，机器学习领域近年来引入了许多特征解释技术，并成为验证其推理能力的重要组成部分。然而，现有方法并不允许关于特征相关性的不确定性进行陈述。本文提出了一种基于蒙特卡罗估计的特征相关性分布的计算方法，即Monte Carlo Relevance Propagation (MCRP)用于特征相关性的不确定性估计，以计算特征相关性不确定性分数，从而深入理解神经网络的感知和推理。

    Understanding decisions made by neural networks is key for the deployment of intelligent systems in real world applications. However, the opaque decision making process of these systems is a disadvantage where interpretability is essential. Many feature-based explanation techniques have been introduced over the last few years in the field of machine learning to better understand decisions made by neural networks and have become an important component to verify their reasoning capabilities. However, existing methods do not allow statements to be made about the uncertainty regarding a feature's relevance for the prediction. In this paper, we introduce Monte Carlo Relevance Propagation (MCRP) for feature relevance uncertainty estimation. A simple but powerful method based on Monte Carlo estimation of the feature relevance distribution to compute feature relevance uncertainty scores that allow a deeper understanding of a neural network's perception and reasoning.
    
[^29]: 用于癫痫发作预测的卷积神经网络

    Convolutional Neural Networks for Epileptic Seizure Prediction. (arXiv:1811.00915v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1811.00915](http://arxiv.org/abs/1811.00915)

    本文提出了一种使用卷积神经网络拓扑结构进行iEEG的癫痫发作预测的新方法，避免提取手工特征，并在多个数据集上进行了评估。研究结果表明该方法具有普遍适用性。

    

    癫痫是最常见的神经系统疾病，准确预测癫痫发作将有助于克服患者的不确定和无助感。本研究提出了一种针对颅内脑电图（iEEG）进行癫痫发作预测的新方法，并使用卷积神经网络（CNN）拓扑结构进行信号特征的确定和预ictal和interictal段的二元分类。我们对来自四只狗和三名患者的长期记录进行了三个不同模型的评估。总的来说，我们的研究结果表明了方法的普遍适用性。本研究还讨论了我们方法的优点和局限性。

    Epilepsy is the most common neurological disorder and an accurate forecast of seizures would help to overcome the patient's uncertainty and helplessness. In this contribution, we present and discuss a novel methodology for the classification of intracranial electroencephalography (iEEG) for seizure prediction. Contrary to previous approaches, we categorically refrain from an extraction of hand-crafted features and use a convolutional neural network (CNN) topology instead for both the determination of suitable signal characteristics and the binary classification of preictal and interictal segments. Three different models have been evaluated on public datasets with long-term recordings from four dogs and three patients. Overall, our findings demonstrate the general applicability. In this work we discuss the strengths and limitations of our methodology.
    
[^30]: 基于双重协变量调整的块模型用于社区检测

    Pairwise Covariates-adjusted Block Model for Community Detection. (arXiv:1807.03469v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/1807.03469](http://arxiv.org/abs/1807.03469)

    基于SBM模型，双重协变量调整的PCABM模型添加了关于节点间关系的附加信息。SCWA算法对PCABM模型进行了高效求解。模拟实验和实际数据分析表明PCABM模型具有优异的性能和预测能力。

    

    社区检测是网络研究中最基本的问题之一。随机块模型(SBM)是一种广泛应用的模型，已开发出各种估计方法并揭示了它们的社区检测一致性结果。但是，SBM受到一种假设的限制，即同一社区中的所有节点都是随机等价的，这可能不适用于实际应用。我们引入了一种基于双重协变量调整的随机块模型(PCABM)，即将双重协变量信息合并到SBM中。我们研究了协变量系数和社区分配的极大似然估计值。证明了在适当的稀疏条件下，协变量系数估计和社区分配均一致。介绍了一种带有调整的谱聚类（SCWA），以高效地解决PCABM问题。我们推导了SCWA检测社区的误差界限，证明了算法能够实现精确的社区恢复。数值模拟和实际数据分析表明PCABM优于现有方法。

    One of the most fundamental problems in network study is community detection. The stochastic block model (SBM) is a widely used model, for which various estimation methods have been developed with their community detection consistency results unveiled. However, the SBM is restricted by the strong assumption that all nodes in the same community are stochastically equivalent, which may not be suitable for practical applications. We introduce a pairwise covariates-adjusted stochastic block model (PCABM), a generalization of SBM that incorporates pairwise covariate information. We study the maximum likelihood estimates of the coefficients for the covariates as well as the community assignments. It is shown that both the coefficient estimates of the covariates and the community assignments are consistent under suitable sparsity conditions. Spectral clustering with adjustment (SCWA) is introduced to efficiently solve PCABM. Under certain conditions, we derive the error bound of community det
    

