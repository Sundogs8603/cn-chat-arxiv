# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Emerging Statistical Machine Learning Techniques for Extreme Temperature Forecasting in U.S. Cities.](http://arxiv.org/abs/2307.14285) | 本文使用新兴的统计机器学习技术对美国城市的极端气温进行了全面分析，发现多层感知器是最有效的方法，并使用该方法对未来极端气温进行了预测和假设测试。 |
| [^2] | [Learning Disentangled Discrete Representations.](http://arxiv.org/abs/2307.14151) | 通过替换标准的高斯变分自动编码器，使用定制的分类变分自动编码器，我们发现离散潜在空间的底层网格结构可以有效缓解解离表示中的旋转不变性问题，并提供了优化解离表示的无监督模型选择策略。 |
| [^3] | [Toward Design of Synthetic Active Inference Agents by Mere Mortals.](http://arxiv.org/abs/2307.14145) | 本文讨论了如何设计一个能够支持非专家工程师开发主动推理代理的软件工具箱，以实现在边缘设备上运行的有效代理。旨在加速主动推理代理的民主化进程。 |
| [^4] | [Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards.](http://arxiv.org/abs/2307.14138) | 本研究研究了具有因果关系奖励的分段稳定组合半强盗问题，并提出了上界置信度算法以应对非平稳环境中的挑战。此外，引入了组重启的概念作为结构化环境中的备份策略。 |
| [^5] | [Cliqueful graphs as a means of calculating the maximal number of maximum cliques of simple graphs.](http://arxiv.org/abs/2307.14120) | 本论文研究了充满团图的概念，并且发现在简单图中，充满团图的最大数量取决于饱和复合充满团图。通过具体计算，我们得到了在n个顶点上具有最多最大团数量的图形式表达式。 |
| [^6] | [Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks.](http://arxiv.org/abs/2307.14085) | 本文研究了强化学习中的量化斯坦克伯格均衡问题，提出了省样本量的在线和离线算法，并通过推断追随者的行动来学习量化响应模型。 |
| [^7] | [Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification.](http://arxiv.org/abs/2307.14025) | 本论文提出一种基于拓扑正则化的多实例学习方法，用于罕见贫血疾病的红细胞分类。通过从单个红细胞图像中提取多尺度的拓扑特征来进行模型正则化，以保持数据的特征拓扑属性。实验结果表明，该方法是有效的。 |
| [^8] | [MCMC-Correction of Score-Based Diffusion Models for Model Composition.](http://arxiv.org/abs/2307.14012) | 本文提出了一种修正基于得分的扩散模型的方法，使其能够与各种MCMC方法结合，从而实现模型组合和进行更好的采样。 |
| [^9] | [Simulation-based Inference for Cardiovascular Models.](http://arxiv.org/abs/2307.13918) | 本研究将心血管模型的逆问题作为统计推理进行解决，在体外进行了五个生物标记物的不确定性分析，展示了模拟推理的能力。 |
| [^10] | [Online learning in bandits with predicted context.](http://arxiv.org/abs/2307.13916) | 本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。 |
| [^11] | [Corruption-Robust Lipschitz Contextual Search.](http://arxiv.org/abs/2307.13903) | 该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。 |
| [^12] | [Learning sources of variability from high-dimensional observational studies.](http://arxiv.org/abs/2307.13868) | 本研究提出了一种针对高维观测研究的方法，将因果估计泛化到任意维度或可测空间的结果，并提出了一种用于名义变量的因果偏差测试。实验证明该方法相比现有策略在有限样本有效性和功率方面有改进。 |
| [^13] | [How to Scale Your EMA.](http://arxiv.org/abs/2307.13813) | 本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。 |
| [^14] | [Source Condition Double Robust Inference on Functionals of Inverse Problems.](http://arxiv.org/abs/2307.13793) | 本文提出了一种源条件双稳健推断方法，用于估计线性逆问题解的线性函数参数，无需知道哪个逆问题更良好，该方法能确保对感兴趣的参数的渐近正态性，并提供了对迭代Tikhonov正则化对抗估计器的新保证。 |
| [^15] | [Implicitly Normalized Explicitly Regularized Density Estimation.](http://arxiv.org/abs/2307.13763) | 我们提出了一种新的非参数密度估计方法，基于正则化密度的 Sobolev 范数，通过适当的初始化和使用自然梯度，可以得到性能良好的解，该方法在 Anomaly Detection benchmark 中排名第二。 |
| [^16] | [Manifold Filter-Combine Networks.](http://arxiv.org/abs/2307.04056) | 这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。 |
| [^17] | [Policy Gradient Algorithms Implicitly Optimize by Continuation.](http://arxiv.org/abs/2305.06851) | 本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。 |
| [^18] | [Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards.](http://arxiv.org/abs/2304.14989) | 本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。 |
| [^19] | [Quantizing Heavy-tailed Data in Statistical Estimation: (Near) Minimax Rates, Covariate Quantization, and Uniform Recovery.](http://arxiv.org/abs/2212.14562) | 本文研究了在统计估计问题中对重尾数据的量化方法，提出了一种截断和适当抖动的方案，并证明了该方案可以实现（近乎）最小化的估计误差速率。具体应用包括协方差估计、压缩感知和矩阵补全，结果表明量化对乘法因子的影响较小。在同时对协变量和响应进行量化的压缩感知问题中，虽然恢复程序是非凸的，但所有局部极小值都有近乎最优的误差界。 |
| [^20] | [An optimal control perspective on diffusion-based generative modeling.](http://arxiv.org/abs/2211.01364) | 本文建立了随机最优控制与基于扩散的生成模型之间的联系，推导了用于控制潜在SDE边际密度演化的汉密尔顿-雅可比-贝尔曼方程，并将生成建模表述为对合适度量之间Kullback-Leibler散度的最小化。此外，作者还开发了一种新型扩散方法用于采样非归一化密度。 |
| [^21] | [Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes.](http://arxiv.org/abs/2210.07612) | 本研究研究了机器学习模型中不确定性估计的问题，证明了通过调整超参数可以提高边际似然，但交叉验证度量表现出双下降现象。 |
| [^22] | [Estimating large causal polytrees from small samples.](http://arxiv.org/abs/2209.07028) | 本文介绍了一种算法，可以在变量数量远大于样本大小的情况下，准确地估计大规模因果多树结构，而几乎不需要任何分布或建模的假设。 |
| [^23] | [Beyond the Edge of Stability via Two-step Gradient Updates.](http://arxiv.org/abs/2206.04172) | 本文研究了梯度下降对于非Lipschitz梯度的损失函数的收敛性，发现在过参数化模型中，尽管存在局部不稳定性和振荡行为，梯度下降仍然能够收敛。 |
| [^24] | [Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice.](http://arxiv.org/abs/2202.12780) | 这篇用户指南介绍了机器学习和精算实践中一致性评分函数的比较和校准评估技术，并强调了事先确定目标预测函数和选择相应评分函数的重要性。 |
| [^25] | [Optimal Simple Regret in Bayesian Best Arm Identification.](http://arxiv.org/abs/2111.09885) | 该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。 |
| [^26] | [Compressible Spectral Mixture Kernels with Sparse Dependency Structures for Gaussian Processes.](http://arxiv.org/abs/1808.00560) | 本文提出了一种带有稀疏依赖结构的可压缩频谱混合核用于高斯过程，通过改进原始核的泛化性能。具体方法包括通过交叉协方差和交叉卷积泛化依赖结构，以及通过参数化时间和相位延迟提高依赖结构的表达能力。 |

# 详细

[^1]: 新兴的统计机器学习技术在美国城市的极端气温预测中的应用

    Emerging Statistical Machine Learning Techniques for Extreme Temperature Forecasting in U.S. Cities. (arXiv:2307.14285v1 [stat.AP])

    [http://arxiv.org/abs/2307.14285](http://arxiv.org/abs/2307.14285)

    本文使用新兴的统计机器学习技术对美国城市的极端气温进行了全面分析，发现多层感知器是最有效的方法，并使用该方法对未来极端气温进行了预测和假设测试。

    

    本文使用新兴的统计机器学习技术对极端气温模式进行了全面分析。我们的研究重点是探索和比较各种统计模型在气候时间序列预测中的有效性。所考虑的模型包括自回归综合移动平均、指数平滑、多层感知器和高斯过程。我们应用这些方法于美国五个人口最多的城市的气候时间序列数据，利用Python和Julia来展示统计计算在理解气候变化及其影响中的作用。我们的研究结果突出了统计方法之间的差异，并确定了多层感知器作为最有效的方法。此外，我们使用这个最佳方法对未来的极端气温进行了预测，考察温度变化是否大于零，从而测试了一个假设。

    In this paper, we present a comprehensive analysis of extreme temperature patterns using emerging statistical machine learning techniques. Our research focuses on exploring and comparing the effectiveness of various statistical models for climate time series forecasting. The models considered include Auto-Regressive Integrated Moving Average, Exponential Smoothing, Multilayer Perceptrons, and Gaussian Processes. We apply these methods to climate time series data from five most populated U.S. cities, utilizing Python and Julia to demonstrate the role of statistical computing in understanding climate change and its impacts. Our findings highlight the differences between the statistical methods and identify Multilayer Perceptrons as the most effective approach. Additionally, we project extreme temperatures using this best-performing method, up to 2030, and examine whether the temperature changes are greater than zero, thereby testing a hypothesis.
    
[^2]: 学习解离的离散表示

    Learning Disentangled Discrete Representations. (arXiv:2307.14151v1 [cs.LG])

    [http://arxiv.org/abs/2307.14151](http://arxiv.org/abs/2307.14151)

    通过替换标准的高斯变分自动编码器，使用定制的分类变分自动编码器，我们发现离散潜在空间的底层网格结构可以有效缓解解离表示中的旋转不变性问题，并提供了优化解离表示的无监督模型选择策略。

    

    最近在图像生成、基于模型的增强学习和文本到图像生成方面取得了成功，这些都证明了离散潜在表示的经验优势，尽管其背后的原因尚不清楚。我们通过将标准的高斯变分自动编码器（VAE）替换为定制的分类变分自动编码器，探索了离散潜在空间和解离表示之间的关系。我们显示分类分布的底层网格结构减轻了与多变量高斯分布相关的旋转不变性问题，作为解离表示的高效归纳先验。我们提供了分析和实证结果，证明了离散VAE在学习解离表示方面的优势。此外，我们引入了第一个支持解离表示的无监督模型选择策略。

    Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.
    
[^3]: 通过凡人设计合成主动推理代理

    Toward Design of Synthetic Active Inference Agents by Mere Mortals. (arXiv:2307.14145v1 [stat.ML])

    [http://arxiv.org/abs/2307.14145](http://arxiv.org/abs/2307.14145)

    本文讨论了如何设计一个能够支持非专家工程师开发主动推理代理的软件工具箱，以实现在边缘设备上运行的有效代理。旨在加速主动推理代理的民主化进程。

    

    主动推理代理的理论特性是令人印象深刻的，但是我们如何在边缘设备上实现有效的硬件和软件代理呢？这是一个有趣的问题，因为策略探索的计算负荷呈指数级增长，而边缘设备的计算资源非常有限。在本文中，我们讨论了一个支持非专家工程师开发有效主动推理代理的软件工具箱所必需的特性。我们介绍了一个正在开发中的工具箱，旨在加速主动推理代理的民主化进程，就像TensorFlow推动了深度学习技术的应用一样。

    The theoretical properties of active inference agents are impressive, but how do we realize effective agents in working hardware and software on edge devices? This is an interesting problem because the computational load for policy exploration explodes exponentially, while the computational resources are very limited for edge devices. In this paper, we discuss the necessary features for a software toolbox that supports a competent non-expert engineer to develop working active inference agents. We introduce a toolbox-in-progress that aims to accelerate the democratization of active inference agents in a similar way as TensorFlow propelled applications of deep learning technology.
    
[^4]: 分段稳定组合半强盗问题及因果关系奖励研究

    Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.14138v1 [cs.LG])

    [http://arxiv.org/abs/2307.14138](http://arxiv.org/abs/2307.14138)

    本研究研究了具有因果关系奖励的分段稳定组合半强盗问题，并提出了上界置信度算法以应对非平稳环境中的挑战。此外，引入了组重启的概念作为结构化环境中的备份策略。

    

    本文研究了具有因果关系奖励的分段稳定组合半强盗问题。在非平稳环境中，基本臂的分布变化、奖励之间的因果关系，或者两者同时改变奖励生成过程。在这样的环境中，最优的决策者必须跟随这两个变化源，并相应地进行适应。在组合半强盗设置中，问题变得更加严重，因为决策者只观察到所选臂组合的结果。我们提出的策略核心是上界置信度（Upper Confidence Bound, UCB）算法。我们假设代理依靠自适应的方法来应对这一挑战。具体来说，它使用基于广义似然比检验的变点检测器。此外，我们引入了组重启的概念作为结构化环境中决策过程中的新型备份策略。最后，我们的算法整合了一个跟踪机制以追踪

    We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the 
    
[^5]: 作为计算简单图的最大团的最大数量的手段的充满团图

    Cliqueful graphs as a means of calculating the maximal number of maximum cliques of simple graphs. (arXiv:2307.14120v1 [math.CO])

    [http://arxiv.org/abs/2307.14120](http://arxiv.org/abs/2307.14120)

    本论文研究了充满团图的概念，并且发现在简单图中，充满团图的最大数量取决于饱和复合充满团图。通过具体计算，我们得到了在n个顶点上具有最多最大团数量的图形式表达式。

    

    一个简单图在n个顶点上可能包含许多最大团。但它可能包含多少个呢？我们将展示最大团的最大数量取决于所谓的充满团图，具体地说，如果n≥15，我们将展示它取决于饱和复合充满团图。利用这一点，我们将展示包含3^{⌊n/3⌋}c个最大团的图在n个顶点上具有最多的最大团数量，其中c∈{1,4/3,2}，取决于n模3的值。

    A simple graph on $n$ vertices may contain a lot of maximum cliques. But how many can it potentially contain? We will show that the maximum number of maximum cliques is taken over so-called cliqueful graphs, more specifically, later we will show that it is taken over saturated composite cliqueful graphs, if $n \ge 15$. Using this we will show that the graph that contains $3^{\lfloor n/3 \rfloor}c$ maxcliques has the most number of maxcliques on $n$ vertices, where $c\in\{1,\frac{4}{3},2\}$, depending on $n \text{ mod } 3$.
    
[^6]: 行动胜于言辞：证明了从策略反馈中省样本量的量化斯坦克伯格均衡的强化学习

    Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks. (arXiv:2307.14085v1 [cs.LG])

    [http://arxiv.org/abs/2307.14085](http://arxiv.org/abs/2307.14085)

    本文研究了强化学习中的量化斯坦克伯格均衡问题，提出了省样本量的在线和离线算法，并通过推断追随者的行动来学习量化响应模型。

    

    本文研究具有领导者-追随者结构的情境马尔科夫博弈中学习量化斯坦克伯格均衡（QSE）的强化学习（RL）。在游戏开始时，领导者宣布她的策略并承诺执行。追随者观察领导者的策略，然后采取量化响应策略，通过解决由领导者策略引发的熵正则化策略优化问题来确定。领导者的目标是通过与追随者的交互并从数据中学习，找到自己的最优策略，从而获得最优的预期总回报。这个问题的一个关键挑战是领导者无法观察到追随者的奖励，并且需要从追随者对抗领导者策略的行动中推断出追随者的量化响应模型。我们在函数逼近的背景下提出了适用于在线和离线设置的样本效率算法。我们的算法基于（i）通过最大似然学习量化响应模型

    We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood 
    
[^7]: 基于拓扑正则化的多实例学习用于红细胞疾病分类

    Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification. (arXiv:2307.14025v1 [cs.LG])

    [http://arxiv.org/abs/2307.14025](http://arxiv.org/abs/2307.14025)

    本论文提出一种基于拓扑正则化的多实例学习方法，用于罕见贫血疾病的红细胞分类。通过从单个红细胞图像中提取多尺度的拓扑特征来进行模型正则化，以保持数据的特征拓扑属性。实验结果表明，该方法是有效的。

    

    使用显微图像诊断罕见的贫血疾病对于熟练的专家和机器学习方法来说都具有挑战性。由于在单个血样中有数千个与疾病相关的细胞，这构成了一个复杂的多实例学习（MIL）问题。虽然红细胞的空间邻域本身并不重要，但整个血样的拓扑结构，即数据的几何性质，包含了有益的特征，以解决典型的MIL问题，如梯度消失和在有限数据上训练时的过拟合。因此，我们开发了一种基于拓扑的方法，从单个红细胞图像的包中提取多尺度的拓扑特征。这些拓扑特征被用来对模型进行正则化，强制保持数据的特征拓扑属性。在包含71个罕见贫血疾病患者的数据集上，包括521张红细胞显微图像，我们的实验表明拓扑正则化是一个有效的方法。

    Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effe
    
[^8]: MCMC-修正基于得分的扩散模型用于模型组合

    MCMC-Correction of Score-Based Diffusion Models for Model Composition. (arXiv:2307.14012v1 [stat.ML])

    [http://arxiv.org/abs/2307.14012](http://arxiv.org/abs/2307.14012)

    本文提出了一种修正基于得分的扩散模型的方法，使其能够与各种MCMC方法结合，从而实现模型组合和进行更好的采样。

    

    扩散模型可以用得分或能量函数来参数化。能量参数化具有更好的理论特性，主要是它可以通过在提议样本中总能量的变化基于Metropolis-Hastings修正步骤来进行扩展采样过程。然而，它似乎产生了稍微较差的性能，更重要的是，由于基于得分的扩散模型的普遍流行，现有的预训练能量参数化模型的可用性受到限制。这种限制削弱了模型组合的目的，即将预训练模型组合起来从新分布中进行采样。然而，我们的提议建议保留得分参数化，而是通过对得分函数进行线积分来计算基于能量的接受概率。这使我们能够重用现有的扩散模型，并将反向过程与各种马尔可夫链蒙特卡罗（MCMC）方法组合起来。

    Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our 
    
[^9]: 基于模拟的推理用于心血管模型

    Simulation-based Inference for Cardiovascular Models. (arXiv:2307.13918v1 [stat.ML])

    [http://arxiv.org/abs/2307.13918](http://arxiv.org/abs/2307.13918)

    本研究将心血管模型的逆问题作为统计推理进行解决，在体外进行了五个生物标记物的不确定性分析，展示了模拟推理的能力。

    

    在过去的几十年中，血流动力学模拟器不断发展，已成为研究体外心血管系统的首选工具。虽然这样的工具通常用于从生理参数模拟全身血流动力学，但解决将波形映射回合理的生理参数的逆问题仍然有很大的潜力和挑战。受模拟推理（SBI）的进展的启发，我们将这个逆问题作为统计推理来处理。与其他方法不同，SBI为感兴趣的参数提供了后验分布，提供了关于个体测量的不确定性的多维表示。我们通过对比几种测量模态来展示这种能力，进行了五个临床感兴趣的生物标志物的体外不确定性分析。除了确认已知事实，比如估计心率的可行性，我们的研究还突出了…

    Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlight
    
[^10]: 在预测上下文中的在线学习问题

    Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])

    [http://arxiv.org/abs/2307.13916](http://arxiv.org/abs/2307.13916)

    本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。

    

    我们考虑在每个时刻，代理只能访问到上下文的一个带噪声的版本以及误差方差（或者这个方差的一个估计）。这一设置受到了许多应用的启发，在这些应用中，用于决策的真实上下文是不可观测的，而只有一个由可能复杂的机器学习算法预测出的上下文。当上下文误差是非衰减的时候，经典的bandit算法无法达到次线性的后悔。我们提出了在这一设置下，第一个具有次线性后悔的在线算法，并与适当的基准进行了比较。关键的思想是将经典统计学中的测量误差模型推广到在线决策设置中，这是非平凡的，因为策略依赖于有噪声的上下文观察。

    We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
    
[^11]: 腐败鲁棒的Lipschitz上下文搜索

    Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])

    [http://arxiv.org/abs/2307.13903](http://arxiv.org/abs/2307.13903)

    该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。

    

    我研究了学习具有被篡改的二进制信号的Lipschitz函数的问题。学习者试图学习一个由对手选择的Lipschitz函数$f$。在每一轮中，对手在输入空间中选择一个上下文向量$x_t$，学习者对真实函数值$f(x_t)$进行猜测，并接收一个指示猜测是高还是低的二进制信号。在总共$C$轮中，信号可能被篡改，但学习者不知道$C$的值。学习者的目标是造成小的累积损失。我提出了一个自然而强大的技术验证，对设计腐败鲁棒算法非常有用。我设计了一些算法（将Lipschitz参数$L$视为常数）：对于对称损失，学习者在$d=1$时达到后悔$O(C\log T)$，在$d>1$时达到后悔$O_d(C\log T + T^{(d-1)/d})$；对于计价损失，学习者在$d/(d+1)$时达到后悔$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。

    I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
    
[^12]: 从高维观测研究中学习变异源

    Learning sources of variability from high-dimensional observational studies. (arXiv:2307.13868v1 [stat.ME])

    [http://arxiv.org/abs/2307.13868](http://arxiv.org/abs/2307.13868)

    本研究提出了一种针对高维观测研究的方法，将因果估计泛化到任意维度或可测空间的结果，并提出了一种用于名义变量的因果偏差测试。实验证明该方法相比现有策略在有限样本有效性和功率方面有改进。

    

    因果推断研究是否存在一个变量影响观测结果。通过诸如“平均治疗效果”等量化指标，这一范式在许多生物领域中被采用，从疫苗和药物开发到政策干预。不幸的是，大多数方法通常仅限于单变量结果。我们的工作将因果估计泛化到任意维度或可测空间的结果，并将传统的因果估计形式化为名义变量的因果偏差测试。我们提出了一种简单的技术来调整一致性条件独立性测试，并证明了这些测试是一致性因果偏差测试。数值实验表明，与现有策略相比，我们的方法Causal CDcorr在有限样本有效性和功率方面均有改进。我们的方法都是开源的，可在github.com/ebridge2/cdcorr上获得。

    Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
    
[^13]: 如何扩展您的EMA（arXiv:2307.13813v1 [stat.ML]）

    How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])

    [http://arxiv.org/abs/2307.13813](http://arxiv.org/abs/2307.13813)

    本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。

    

    在实际机器学习中，保持训练动态在批量大小之间的一致性是一种重要工具，它能够在批量大小和墙钟时间之间进行权衡。这种权衡通常通过一个缩放规则来实现，例如，在随机梯度下降中，应该将学习率与批量大小呈线性关系。另一个实际机器学习的重要工具是模型指数移动平均（EMA），它是一个不接收梯度信息的模型副本，而是以一定的动量跟随其目标模型。这个模型EMA可以提高监督学习的稳健性和泛化性能，稳定伪标记，为自监督学习提供学习信号。之前的研究将模型EMA与优化分开处理，导致批量大小之间存在不同的训练动态和较低的模型性能。在这项工作中，我们提供了在存在模型EMA的情况下进行优化的缩放规则，并展示了其效果。

    Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
    
[^14]: 逆问题函数的源条件双稳健推断

    Source Condition Double Robust Inference on Functionals of Inverse Problems. (arXiv:2307.13793v1 [stat.ME])

    [http://arxiv.org/abs/2307.13793](http://arxiv.org/abs/2307.13793)

    本文提出了一种源条件双稳健推断方法，用于估计线性逆问题解的线性函数参数，无需知道哪个逆问题更良好，该方法能确保对感兴趣的参数的渐近正态性，并提供了对迭代Tikhonov正则化对抗估计器的新保证。

    

    本文考虑了线性逆问题解的线性函数参数的估计。任何这样的参数都有一个双稳健表示，该表示依赖于对偶线性逆问题的解，其中对偶解可以被视为逆倾向函数的推广。我们提供了第一个源条件双稳健推断方法，只要原始或对偶逆问题足够良好，无需知道哪个逆问题更良好，就能确保对感兴趣的参数的渐近正态性。我们的结果是通过对线性逆问题的迭代Tikhonov正则化对抗估计器在一般假设空间上的新的保证而实现的，这是一个独立发展的利益。

    We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov regularized adversarial estimators for linear inverse problems, over general hypothesis spaces, which are developments of independent interest.
    
[^15]: 隐式归一化显式正则化密度估计

    Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])

    [http://arxiv.org/abs/2307.13763](http://arxiv.org/abs/2307.13763)

    我们提出了一种新的非参数密度估计方法，基于正则化密度的 Sobolev 范数，通过适当的初始化和使用自然梯度，可以得到性能良好的解，该方法在 Anomaly Detection benchmark 中排名第二。

    

    我们提出了一种新的非参数密度估计方法，该方法是基于正则化密度的 Sobolev 范数。这种方法与核密度估计有明显差异，可以清晰解释模型的偏差。虽然我们无法得到相关核函数的闭合解析形式，但我们证明可以通过采样进行近似。决定密度的优化问题是非凸的，标准的梯度方法效果不好。然而，我们证明在适当的初始化和使用自然梯度的情况下，可以得到性能良好的解。最后，虽然该方法提供的是非归一化的密度，无法使用对数似然进行交叉验证，但我们证明可以采用基于 Fisher 散度的分数匹配方法来解决这个问题。我们在最近的异常检测基准套件 ADBench 上评估了得到的方法，并发现它在超过15个算法中排名第二。

    We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
    
[^16]: 流形滤波-组合网络

    Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])

    [http://arxiv.org/abs/2307.04056](http://arxiv.org/abs/2307.04056)

    这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。

    

    我们介绍了一类大型流形神经网络(MNNs)，我们称之为流形滤波-组合网络。这个类别包括了Wang、Ruiz和Ribeiro之前的研究中考虑的MNNs，流形散射变换(一种基于小波的神经网络模型)，以及其他有趣的之前在文献中未考虑的示例，如Kipf和Welling的图卷积网络的流形等效。然后，我们考虑了一种基于构建数据驱动图的方法，用于在没有对流形有全局知识的情况下实现这样的网络，而只能访问有限数量的样本点。我们提供了网络在样本点数趋于无穷大时能够保证收敛到其连续极限的充分条件。与之前的工作(主要关注特定的MNN结构和图构建)不同，我们的收敛速度并不依赖于使用的滤波器数量。而且，它表现出线性的收敛速度。

    We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
    
[^17]: 通过连续方式隐式优化的政策梯度算法

    Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])

    [http://arxiv.org/abs/2305.06851](http://arxiv.org/abs/2305.06851)

    本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。

    

    强化学习中的直接策略优化通常通过政策梯度算法解决，该算法通过随机梯度上升优化策略参数。本文提供了一种新的理论解释和证明这些算法的方法。首先，我们将直接策略优化问题建立在优化连续框架下。后者是一种用于优化非凸函数的框架，其中以连续的替代目标函数序列为基础。其次，我们证明了优化仿射高斯策略并执行熵正则化可以解释为通过连续隐式地优化确定性策略。基于这些理论结果，我们认为政策梯度算法中的探索包括计算当前的策略收益的连续函数，策略的方差应该是历史依赖性函数，以避免局部最值而不是仅仅最大化政策的收益。

    Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
    
[^18]: Kullback-Leibler Maillard采样在有界奖励的多臂赌博机问题中的应用

    Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])

    [http://arxiv.org/abs/2304.14989](http://arxiv.org/abs/2304.14989)

    本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。

    

    本文研究了奖励分布集中在区间$[0,1]$内的$K$臂数臂赌博机问题。本文提出了一种名为Kullback-Leibler Maillard Sampling (KL-MS)的新算法，它是Maillard采样在KL空间的自然扩展。实验表明，KL-MS在Bernoulli奖励时具有渐近最优性能，其最坏情况遗憾度上界为$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$，其中$\mu^*$是最优臂的期望奖励，$T$是时段长度。

    We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
    
[^19]: 对统计估计中重尾数据的量化：（近乎）最小化速率，协变量量化和一致恢复

    Quantizing Heavy-tailed Data in Statistical Estimation: (Near) Minimax Rates, Covariate Quantization, and Uniform Recovery. (arXiv:2212.14562v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.14562](http://arxiv.org/abs/2212.14562)

    本文研究了在统计估计问题中对重尾数据的量化方法，提出了一种截断和适当抖动的方案，并证明了该方案可以实现（近乎）最小化的估计误差速率。具体应用包括协方差估计、压缩感知和矩阵补全，结果表明量化对乘法因子的影响较小。在同时对协变量和响应进行量化的压缩感知问题中，虽然恢复程序是非凸的，但所有局部极小值都有近乎最优的误差界。

    

    本文研究了在一些基本统计估计问题中对重尾数据的量化，其中底层分布具有一定阶数的有界矩。我们提出在均匀量化之前对数据进行截断和适当抖动。我们的主要观点是，通过所提出的方案产生的量化数据仅需实现（近乎）最小化的估计误差速率。特别地，对协方差估计、压缩感知和矩阵补全进行了具体的结果推导，所有结果都表明量化仅使乘法因子稍微恶化。此外，我们研究了同时对协变量（即感知向量）和响应进行量化的压缩感知问题。在协变量量化下，尽管我们的恢复程序是非凸的，因为协方差矩阵估计值缺乏正半定性，但所有局部极小值都被证明具有近乎最优的误差界。此外，通过乘积过程的集中不等式

    This paper studies the quantization of heavy-tailed data in some fundamental statistical estimation problems, where the underlying distributions have bounded moments of some order. We propose to truncate and properly dither the data prior to a uniform quantization. Our major standpoint is that (near) minimax rates of estimation error are achievable merely from the quantized data produced by the proposed scheme. In particular, concrete results are worked out for covariance estimation, compressed sensing, and matrix completion, all agreeing that the quantization only slightly worsens the multiplicative factor. Besides, we study compressed sensing where both covariate (i.e., sensing vector) and response are quantized. Under covariate quantization, although our recovery program is non-convex because the covariance matrix estimator lacks positive semi-definiteness, all local minimizers are proved to enjoy near optimal error bound. Moreover, by the concentration inequality of product process
    
[^20]: 对基于扩散的生成模型的最优控制视角

    An optimal control perspective on diffusion-based generative modeling. (arXiv:2211.01364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01364](http://arxiv.org/abs/2211.01364)

    本文建立了随机最优控制与基于扩散的生成模型之间的联系，推导了用于控制潜在SDE边际密度演化的汉密尔顿-雅可比-贝尔曼方程，并将生成建模表述为对合适度量之间Kullback-Leibler散度的最小化。此外，作者还开发了一种新型扩散方法用于采样非归一化密度。

    

    我们建立了随机最优控制与基于随机微分方程（SDE）的生成模型之间的联系，例如最近发展起来的扩散概率模型。特别地，我们推导出一个汉密尔顿-雅可比-贝尔曼方程，用于控制潜在的SDE边际密度的演化。这个视角允许将最优控制理论的方法应用于生成建模中。首先，我们展示了证据下界是控制理论中广为人知的验证定理的直接结果。此外，我们可以将基于扩散的生成建模表述为路径空间中合适度量之间的Kullback-Leibler散度的最小化。最后，我们开发了一种从非归一化密度中进行采样的新型扩散方法，这在统计学和计算科学中经常出现的问题。我们证明了我们的时序反向扩散采样器（DIS）可以胜过其他基于扩散的采样方法。

    We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approache
    
[^21]: 不确定性估计中的单调性和双下降现象在高斯过程中的应用

    Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes. (arXiv:2210.07612v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07612](http://arxiv.org/abs/2210.07612)

    本研究研究了机器学习模型中不确定性估计的问题，证明了通过调整超参数可以提高边际似然，但交叉验证度量表现出双下降现象。

    

    尽管评估预测的可靠性非常重要，但对于机器学习模型的不确定性量化（UQ）指标直到最近才开始得到严格的表征。一个显著问题是维度诅咒：普遍认为边缘似然应该与交叉验证度量类似，并且两者在输入维度较大时都会恶化。我们证明通过调整超参数以最大化边际似然（经验贝叶斯过程），性能（以边际似然测量）随着输入维度的增加单调改善。另一方面，我们证明交叉验证度量表现出不同的行为特征，即双下降现象。最近因在某些情况下性能提高而受到关注的冷态后验似乎加剧了这些现象。我们经验证实，我们的结果在真实数据上成立，超出我们考虑的假设范围。

    Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically} with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assump
    
[^22]: 从小样本中估计大的因果多树

    Estimating large causal polytrees from small samples. (arXiv:2209.07028v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2209.07028](http://arxiv.org/abs/2209.07028)

    本文介绍了一种算法，可以在变量数量远大于样本大小的情况下，准确地估计大规模因果多树结构，而几乎不需要任何分布或建模的假设。

    

    我们考虑从相对较小的独立同分布样本中估计大的因果多树的问题。这是在变量数量与样本大小相比非常大的情况下确定因果结构的问题，例如基因调控网络。我们提出了一种算法，在这种情况下以高准确度恢复树形结构。该算法除了一些温和的非退化条件外，基本不需要分布或建模的假设。

    We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.
    
[^23]: 通过两步梯度更新超越稳定边界

    Beyond the Edge of Stability via Two-step Gradient Updates. (arXiv:2206.04172v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04172](http://arxiv.org/abs/2206.04172)

    本文研究了梯度下降对于非Lipschitz梯度的损失函数的收敛性，发现在过参数化模型中，尽管存在局部不稳定性和振荡行为，梯度下降仍然能够收敛。

    

    梯度下降（GD）是现代机器学习中的强大工具，因其在高维空间中的可扩展性和效率而闻名。对于具有Lipschitz梯度的损失函数，GD只能找到局部极小值点，可以看作是潜在梯度流的“真实”离散化方法。然而，许多涉及过参数化模型的机器学习设置并不属于这个问题类别，这促使研究超越所谓的“稳定边界”（Edge of Stability，EoS），其中步长越过与Lipschitz常数成反比的可允许阈值。令人惊讶的是，经验证明尽管存在局部不稳定性和振荡行为，GD仍然收敛。对这一现象的初步理论分析主要集中在过参数化的范围内，在此范围内选择较大的学习率可能与在最小化器流形内隐含的“尖度最小化”正则化相关，请详见论文了解更多细节。

    Gradient Descent (GD) is a powerful workhorse of modern machine learning thanks to its scalability and efficiency in high-dimensional spaces. Its ability to find local minimisers is only guaranteed for losses with Lipschitz gradients, where it can be seen as a `bona-fide' discretisation of an underlying gradient flow. Yet, many ML setups involving overparametrised models do not fall into this problem class, which has motivated research beyond the so-called ``Edge of Stability'' (EoS), where the step-size crosses the admissibility threshold inversely proportional to the Lipschitz constant above. Perhaps surprisingly, GD has been empirically observed to still converge regardless of local instability and oscillatory behavior.  The incipient theoretical analysis of this phenomena has mainly focused in the overparametrised regime, where the effect of choosing a large learning rate may be associated to a `Sharpness-Minimisation' implicit regularisation within the manifold of minimisers, unde
    
[^24]: 模型比较和校准评估：机器学习和精算实践中一致性评分函数的用户指南

    Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice. (arXiv:2202.12780v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.12780](http://arxiv.org/abs/2202.12780)

    这篇用户指南介绍了机器学习和精算实践中一致性评分函数的比较和校准评估技术，并强调了事先确定目标预测函数和选择相应评分函数的重要性。

    

    精算师和数据科学家主要的任务之一是构建适用于保险领域中诸如索赔金额或索赔数量等现象的有效预测模型。这些模型理想情况下利用给定的特征信息来提高预测的准确性。本用户指南重新审视和明确了用于评估模型校准性或适当性的统计技术，以及比较和排名不同模型的技术。在这样做时，强调了事先明确指定目标预测函数（如平均值或分位数）以及选择与此目标函数一致的评分函数在模型比较中的重要性。提供了实际选择评分函数的指导。致力于填补科学与实际应用中的差距，主要专注于现有结果和最佳实践的教学展示，并通过两个真实数据案例进行了说明和解释。

    One of the main tasks of actuaries and data scientists is to build good predictive models for certain phenomena such as the claim size or the number of claims in insurance. These models ideally exploit given feature information to enhance the accuracy of prediction. This user guide revisits and clarifies statistical techniques to assess the calibration or adequacy of a model on the one hand, and to compare and rank different models on the other hand. In doing so, it emphasises the importance of specifying the prediction target functional at hand a priori (e.g. the mean or a quantile) and of choosing the scoring function in model comparison in line with this target functional. Guidance for the practical choice of the scoring function is provided. Striving to bridge the gap between science and daily practice in application, it focuses mainly on the pedagogical presentation of existing results and of best practice. The results are accompanied and illustrated by two real data case studies 
    
[^25]: 贝叶斯最优臂识别中的最优简单遗憾

    Optimal Simple Regret in Bayesian Best Arm Identification. (arXiv:2111.09885v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09885](http://arxiv.org/abs/2111.09885)

    该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。

    

    我们考虑多臂赌博机问题中的最优臂识别。在先验条件具有一定的连续性的情况下，我们表征了贝叶斯简单遗憾的速率。与贝叶斯遗憾最小化不同，贝叶斯简单遗憾的主导项来源于最优臂和次优臂之间间隙小于$\sqrt{\frac{\log T}{T}}$的区域。我们提出了一种简单易行的计算算法，其主导项匹配了下界，只差一个常数因子；模拟结果支持了我们的理论发现。

    We consider best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading term in the Bayesian simple regret derives from the region where the gap between optimal and suboptimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings.
    
[^26]: 带有稀疏依赖结构的可压缩频谱混合核用于高斯过程

    Compressible Spectral Mixture Kernels with Sparse Dependency Structures for Gaussian Processes. (arXiv:1808.00560v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1808.00560](http://arxiv.org/abs/1808.00560)

    本文提出了一种带有稀疏依赖结构的可压缩频谱混合核用于高斯过程，通过改进原始核的泛化性能。具体方法包括通过交叉协方差和交叉卷积泛化依赖结构，以及通过参数化时间和相位延迟提高依赖结构的表达能力。

    

    频谱混合（SM）核是一种描述复杂模式的通用内核类用于高斯过程（GPs）。本文通过模型压缩和时间相位（TP）调制依赖结构，改进了原始的（SM）核使其具有更好的泛化性能。具体而言，通过采用Bienaymés恒等式，我们通过SM组件之间的交叉协方差来泛化依赖结构。然后，我们提出了一种新的带有依赖结构（SMD）的SM核，通过SM组件之间的交叉卷积。此外，我们通过参数化时间和相位延迟来改善依赖结构的表达能力。该依赖结构在频谱密度、协方差行为和采样路径方面具有清晰的解释。为了丰富SMD的有效超参数初始化、可压缩的SM核组件和稀疏的依赖结构，我们引入了一种新的结构适应（SA）算法。

    Spectral mixture (SM) kernels comprise a powerful class of generalized kernels for Gaussian processes (GPs) to describe complex patterns. This paper introduces model compression and time- and phase (TP) modulated dependency structures to the original (SM) kernel for improved generalization of GPs. Specifically, by adopting Bienaym\'es identity, we generalize the dependency structure through cross-covariance between the SM components. Then, we propose a novel SM kernel with a dependency structure (SMD) by using cross-convolution between the SM components. Furthermore, we ameliorate the expressiveness of the dependency structure by parameterizing it with time and phase delays. The dependency structure has clear interpretations in terms of spectral density, covariance behavior, and sampling path. To enrich the SMD with effective hyperparameter initialization, compressible SM kernel components, and sparse dependency structures, we introduce a novel structure adaptation (SA) algorithm in th
    

