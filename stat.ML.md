# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum.](http://arxiv.org/abs/2401.06738) | 本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。 |
| [^2] | [Valid causal inference with unobserved confounding in high-dimensional settings.](http://arxiv.org/abs/2401.06564) | 本研究通过提出不确定性区间，实现了在存在未观测到的混淆因素和高维度干扰模型的情况下的有效半参数推断。 |
| [^3] | [Boosting Causal Additive Models.](http://arxiv.org/abs/2401.06523) | 我们提出了一种基于提升方法的学习加法结构方程模型的方法，通过引入一类评分函数和逐分量梯度下降的策略来确定变量间的因果顺序，实验证明其在高维数据集中具有竞争力和鲁棒性。 |
| [^4] | [A comprehensive framework for multi-fidelity surrogate modeling with noisy data: a gray-box perspective.](http://arxiv.org/abs/2401.06447) | 该论文介绍了一个综合的多保真度代理建模框架，能够将黑盒模型和白盒模型的信息结合起来，并能够处理噪声污染的数据，并估计出无噪声的高保真度函数。 |
| [^5] | [Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo.](http://arxiv.org/abs/2401.06325) | 本文提出了一种更高效的基于扩散的蒙特卡罗采样算法RS-DMC，通过改进评分估计方法来解决原始DMC算法的梯度复杂性问题。该算法将整个扩散过程划分为多个段落，并使用递归评分估计实现更快的采样速度。 |
| [^6] | [A tree-based varying coefficient model.](http://arxiv.org/abs/2401.05982) | 本论文介绍了一种基于树的可变系数模型，使用循环梯度提升机进行建模，实现了逐维早停和特征重要性评分，该模型能够产生与基于神经网络的VCM相当的结果。 |
| [^7] | [Factor Importance Ranking and Selection using Total Indices.](http://arxiv.org/abs/2401.00800) | 该论文提出了一种使用总指数进行因子重要性排名和选择的新方法，该方法不依赖于特定的预测算法，可以直接从有噪声的数据中估计因子的预测潜力。 |
| [^8] | [On the Generalization Properties of Diffusion Models.](http://arxiv.org/abs/2311.01797) | 本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。 |
| [^9] | [Pure Exploration under Mediators' Feedback.](http://arxiv.org/abs/2308.15552) | 本研究提出了一种严格推广的传统最优臂识别问题，即中介反馈下的最优臂识别（BAI-MF），通过引入中介者来模拟一些实际决策问题，如离线学习、部分可控环境和人类反馈。 |
| [^10] | [Nonlinear Meta-Learning Can Guarantee Faster Rates.](http://arxiv.org/abs/2307.10870) | 非线性元学习可以保证更快的收敛速度。 |
| [^11] | [Bridging RL Theory and Practice with the Effective Horizon.](http://arxiv.org/abs/2304.09853) | 本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。 |
| [^12] | [OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems.](http://arxiv.org/abs/2304.06686) | 本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。 |
| [^13] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^14] | [Product Jacobi-Theta Boltzmann machines with score matching.](http://arxiv.org/abs/2303.05910) | 本文介绍了一种使用Score matching的乘积Jacobi-Theta Boltzmann机器（pJTBM），它比原始的RTBM更高效地拟合概率密度。 |
| [^15] | [Controlling Moments with Kernel Stein Discrepancies.](http://arxiv.org/abs/2211.05408) | 本研究分析了核斯坦离差（KSD）控制性质，发现标准KSD无法控制矩的收敛，提出了可控制矩和弱收敛的下游扩散KSD，并且发展了可以准确描述$q$-Wasserstein收敛的KSD。 |
| [^16] | [EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search.](http://arxiv.org/abs/2210.06015) | 提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。 |
| [^17] | [A finite sample analysis of the benign overfitting phenomenon for ridge function estimation.](http://arxiv.org/abs/2007.12882) | 本论文研究了岭函数估计中的良性过拟合现象，并在有限维度情况下对线性模型进行了分析。 |

# 详细

[^1]: 噪声自适应（加速）随机重力球动量

    Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum. (arXiv:2401.06738v1 [math.OC])

    [http://arxiv.org/abs/2401.06738](http://arxiv.org/abs/2401.06738)

    本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。

    

    我们分析了在光滑，强凸环境中随机重力球动量（SHB）的收敛性。Kidambi等人（2018）表明，对于二次函数，SHB（带有小批量）无法达到加速的收敛速度，并猜想SHB的实际收益是小批量的副产品。我们通过展示当批量大小大于一定阈值时，SHB可以获得加速的收敛速度来证实这一观点。特别地，对于条件数为$\kappa$的强凸二次函数，我们证明了使用标准步长和动量参数的SHB具有$O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$的收敛速度，其中$T$为迭代次数，$\sigma^2$为随机梯度的方差。为确保收敛到极小值，我们提出了一种多阶段方法，结果是噪声自适应的$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$速度。对于一般的强凸函数，我们在实验中展示了所提方法的有效性。

    We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$ rate. For general strongly-
    
[^2]: 在高维度设置中处理未观测到的混淆因素的有效因果推断

    Valid causal inference with unobserved confounding in high-dimensional settings. (arXiv:2401.06564v1 [stat.ME])

    [http://arxiv.org/abs/2401.06564](http://arxiv.org/abs/2401.06564)

    本研究通过提出不确定性区间，实现了在存在未观测到的混淆因素和高维度干扰模型的情况下的有效半参数推断。

    

    近年来，已提出了各种方法来估计因果效应，并通过后模型选择或机器学习估计器对高维度干扰模型进行估计，从而获得统一有效的置信区间。这些方法通常要求观测到所有混淆因素以确保效果的识别。我们的贡献在于展示了如何在存在未观测到的混淆因素和高维度干扰模型的情况下获得有效的半参数推断。我们提出了不确定性区间，允许存在未观测到的混淆，并且当未观测到的混淆相对于样本量很小时，结果推断是有效的；后者用收敛速率来形式化。通过仿真实验，我们展示了所提出的区间的有限样本性质，并研究了一种改进区间的经验覆盖率的替代程序，当未观测到的混淆的数量较大时，可以提高区间的覆盖率。

    Various methods have recently been proposed to estimate causal effects with confidence intervals that are uniformly valid over a set of data generating processes when high-dimensional nuisance models are estimated by post-model-selection or machine learning estimators. These methods typically require that all the confounders are observed to ensure identification of the effects. We contribute by showing how valid semiparametric inference can be obtained in the presence of unobserved confounders and high-dimensional nuisance models. We propose uncertainty intervals which allow for unobserved confounding, and show that the resulting inference is valid when the amount of unobserved confounding is small relative to the sample size; the latter is formalized in terms of convergence rates. Simulation experiments illustrate the finite sample properties of the proposed intervals and investigate an alternative procedure that improves the empirical coverage of the intervals when the amount of unob
    
[^3]: 提升因果加法模型

    Boosting Causal Additive Models. (arXiv:2401.06523v1 [stat.ML])

    [http://arxiv.org/abs/2401.06523](http://arxiv.org/abs/2401.06523)

    我们提出了一种基于提升方法的学习加法结构方程模型的方法，通过引入一类评分函数和逐分量梯度下降的策略来确定变量间的因果顺序，实验证明其在高维数据集中具有竞争力和鲁棒性。

    

    我们提出了一种基于提升方法的学习加法结构方程模型(SEMs)的方法，重点关注确定变量间因果顺序的理论方面。我们引入了一类基于任意回归技术的评分函数，为其建立了一致地支持真实因果顺序的必要条件。我们的分析表明，提升方法与提前停止符合这些条件，从而为因果顺序提供了一致的评分函数。为了应对高维数据集带来的挑战，我们通过在加法结构方程模型空间中进行逐分量梯度下降来改进我们的方法。我们的模拟研究强调了我们的理论结果在较低维度下的有效性，并证明了我们的高维适应方法与最先进的方法相当。此外，我们的方法对于超参数的选择具有鲁棒性，使得程序易于调整。

    We present a boosting-based method to learn additive Structural Equation Models (SEMs) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. We introduce a family of score functions based on arbitrary regression techniques, for which we establish necessary conditions to consistently favor the true causal ordering. Our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. To address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive SEMs. Our simulation study underlines our theoretical results for lower dimensions and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. In addition, it exhibits robustness with respect to the choice of the hyperparameters making the procedure easy to tune.
    
[^4]: 一个综合的多保真度代理建模框架，带有噪声数据：从灰盒的角度来看

    A comprehensive framework for multi-fidelity surrogate modeling with noisy data: a gray-box perspective. (arXiv:2401.06447v1 [stat.ME])

    [http://arxiv.org/abs/2401.06447](http://arxiv.org/abs/2401.06447)

    该论文介绍了一个综合的多保真度代理建模框架，能够将黑盒模型和白盒模型的信息结合起来，并能够处理噪声污染的数据，并估计出无噪声的高保真度函数。

    

    计算机模拟（即白盒模型）在模拟复杂工程系统方面比以往任何时候都更加必不可少。然而，仅凭计算模型往往无法完全捕捉现实的复杂性。当物理实验可行时，增强计算模型提供的不完整信息变得非常重要。灰盒建模涉及到将数据驱动模型（即黑盒模型）和白盒模型（即基于物理的模型）的信息融合的问题。在本文中，我们提出使用多保真度代理模型（MFSMs）来执行这个任务。MFSM将不同计算保真度的模型的信息集成到一个新的代理模型中。我们提出的多保真度代理建模框架能够处理被噪声污染的数据，并能够估计底层无噪声的高保真度函数。我们的方法强调以置信度的形式提供其预测中不确定性的精确估计。

    Computer simulations (a.k.a. white-box models) are more indispensable than ever to model intricate engineering systems. However, computational models alone often fail to fully capture the complexities of reality. When physical experiments are accessible though, it is of interest to enhance the incomplete information offered by computational models. Gray-box modeling is concerned with the problem of merging information from data-driven (a.k.a. black-box) models and white-box (i.e., physics-based) models. In this paper, we propose to perform this task by using multi-fidelity surrogate models (MFSMs). A MFSM integrates information from models with varying computational fidelity into a new surrogate model. The multi-fidelity surrogate modeling framework we propose handles noise-contaminated data and is able to estimate the underlying noise-free high-fidelity function. Our methodology emphasizes on delivering precise estimates of the uncertainty in its predictions in the form of confidence 
    
[^5]: 通过基于扩散的蒙特卡罗方法实现更快的采样而无需等温性  (arXiv:2401.06325v1 [stat.ML])

    Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo. (arXiv:2401.06325v1 [stat.ML])

    [http://arxiv.org/abs/2401.06325](http://arxiv.org/abs/2401.06325)

    本文提出了一种更高效的基于扩散的蒙特卡罗采样算法RS-DMC，通过改进评分估计方法来解决原始DMC算法的梯度复杂性问题。该算法将整个扩散过程划分为多个段落，并使用递归评分估计实现更快的采样速度。

    

    为了从一般的目标分布$p_*\propto e^{-f_*}$中进行采样，超越等温条件，Huang等人（2023）提出了通过反向扩散进行采样，从而产生了基于扩散的蒙特卡罗（DMC）方法。具体而言，DMC遵循扩散过程的反向SDE，将目标分布转化为标准高斯分布，并利用非参数评分估计。然而，原始的DMC算法遇到了高梯度复杂性的问题，导致对获得的样本的误差容差$\epsilon$的依赖成指数增长。在本文中，我们证明了DMC的高复杂性源于其冗余的评分估计设计，并提出了一种更高效的算法，称为RS-DMC，基于一种新颖的递归评分估计方法。特别地，我们首先将整个扩散过程划分为多个段落，然后将评分估计步骤（在任何时间步骤）形式化为一系列相互连接的均值估计步骤。

    To sample from a general target distribution $p_*\propto e^{-f_*}$ beyond the isoperimetric condition, Huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of DMC originates from its redundant design of score estimation, and proposed a more efficient algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation an
    
[^6]: 基于树的可变系数模型介绍

    A tree-based varying coefficient model. (arXiv:2401.05982v1 [stat.ML])

    [http://arxiv.org/abs/2401.05982](http://arxiv.org/abs/2401.05982)

    本论文介绍了一种基于树的可变系数模型，使用循环梯度提升机进行建模，实现了逐维早停和特征重要性评分，该模型能够产生与基于神经网络的VCM相当的结果。

    

    本论文介绍了一种基于树的可变系数模型(VCM)，其中可变系数使用Delong等人(2023)的循环梯度提升机(CGBM)进行建模。使用CGBM对系数函数进行建模，可以进行逐维早停和特征重要性评分。逐维早停不仅可以减少维度特定的过拟合风险，还可以揭示维度之间模型复杂性的差异。使用特征重要性评分可以进行简单的特征选择和易于解释的模型解释。该模型在Richman和Wüthrich（2023）使用的相同的模拟和真实数据示例上进行评估，结果表明，它在样本外损失方面产生了与他们的基于神经网络的VCM LocalGLMnet相当的结果。

    The paper introduces a tree-based varying coefficient model (VCM) where the varying coefficients are modelled using the cyclic gradient boosting machine (CGBM) from Delong et al. (2023). Modelling the coefficient functions using a CGBM allows for dimension-wise early stopping and feature importance scores. The dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. The use of feature importance scores allows for simple feature selection and easy model interpretation. The model is evaluated on the same simulated and real data examples as those used in Richman and W\"uthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based VCM called LocalGLMnet.
    
[^7]: 使用总指数进行因子重要性排名和选择

    Factor Importance Ranking and Selection using Total Indices. (arXiv:2401.00800v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2401.00800](http://arxiv.org/abs/2401.00800)

    该论文提出了一种使用总指数进行因子重要性排名和选择的新方法，该方法不依赖于特定的预测算法，可以直接从有噪声的数据中估计因子的预测潜力。

    

    因子重要性衡量了每个特征对输出预测准确性的影响。许多现有的研究关注基于模型的重要性，但一个学习算法中的重要特征在另一个模型中可能没有多大意义。因此，一个因子重要性度量应该在不依赖于特定预测算法的情况下表征特征的预测潜力。该算法无关的重要性被称为内在重要性。为了绕过建模过程，我们介绍了预测潜力与全局敏感性分析中的总Sobol'指数之间的等价性，并引入了一种新的一致估计器，可以直接从有噪声的数据中估计。结合正向选择和反向消除，我们提出了FIRST，即使用总（Sobol'）指数进行因子重要性排名和选择。我们提供了大量的仿真实验来证明FIRST的有效性。

    Factor importance measures the impact of each feature on output prediction accuracy. Many existing works focus on the model-based importance, but an important feature in one learning algorithm may hold little significance in another model. Hence, a factor importance measure ought to characterize the feature's predictive potential without relying on a specific prediction algorithm. Such algorithm-agnostic importance is termed as intrinsic importance in Williamson et al. (2023), but their estimator again requires model fitting. To bypass the modeling step, we present the equivalence between predictiveness potential and total Sobol' indices from global sensitivity analysis, and introduce a novel consistent estimator that can be directly estimated from noisy data. Integrating with forward selection and backward elimination gives rise to FIRST, Factor Importance Ranking and Selection using Total (Sobol') indices. Extensive simulations are provided to demonstrate the effectiveness of FIRST o
    
[^8]: 关于扩散模型的泛化属性

    On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])

    [http://arxiv.org/abs/2311.01797](http://arxiv.org/abs/2311.01797)

    本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。

    

    扩散模型是一类生成模型，用于建立一个随机传输映射，将经验观测到的但未知的目标分布与已知的先验分布联系起来。尽管在实际应用中取得了显著的成功，但对其泛化能力的理论理解仍未充分发展。本文对扩散模型的泛化属性进行了全面的理论研究。我们建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，表明在样本大小$n$和模型容量$m$上都存在多项式小的泛化误差($O(n^{-2/5}+m^{-4/5})$)，在停止训练时可以避免维度诅咒（即数据维度不呈指数级增长）。此外，我们将定量分析扩展到了一个数据依赖的情景，其中目标分布被描绘为一系列的概率密度函数。

    Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
    
[^9]: 纯探索下的中介反馈

    Pure Exploration under Mediators' Feedback. (arXiv:2308.15552v1 [cs.LG])

    [http://arxiv.org/abs/2308.15552](http://arxiv.org/abs/2308.15552)

    本研究提出了一种严格推广的传统最优臂识别问题，即中介反馈下的最优臂识别（BAI-MF），通过引入中介者来模拟一些实际决策问题，如离线学习、部分可控环境和人类反馈。

    

    随机多臂赌博机是一种顺序决策框架，每一步交互中学习者选择一个臂并观察一个随机回报。在最优臂识别（BAI）问题的背景下，学习者的目标是尽可能准确和高效地找到最优臂，即具有最高期望回报的臂。然而，传统BAI问题的顺序交互协议，即学习者在每一轮中对选择的臂具有完全控制权，无法有效地模拟一些值得关注的决策问题（例如，离线学习，部分可控环境和人类反馈）。因此，在这项工作中，我们提出了一种新的严格推广的传统BAI问题，称之为中介反馈下的最优臂识别（BAI-MF）。更具体地说，我们考虑了学习者可以访问一组中介者的情况，每个中介者都选择要拉动的臂。

    Stochastic multi-armed bandits are a sequential-decision-making framework, where, at each interaction step, the learner selects an arm and observes a stochastic reward. Within the context of best-arm identification (BAI) problems, the goal of the agent lies in finding the optimal arm, i.e., the one with highest expected reward, as accurately and efficiently as possible. Nevertheless, the sequential interaction protocol of classical BAI problems, where the agent has complete control over the arm being pulled at each round, does not effectively model several decision-making problems of interest (e.g., off-policy learning, partially controllable environments, and human feedback). For this reason, in this work, we propose a novel strict generalization of the classical BAI problem that we refer to as best-arm identification under mediators' feedback (BAI-MF). More specifically, we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on 
    
[^10]: 非线性元学习可以保证更快的收敛速度

    Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])

    [http://arxiv.org/abs/2307.10870](http://arxiv.org/abs/2307.10870)

    非线性元学习可以保证更快的收敛速度。

    

    最近许多关于元学习的理论研究旨在利用相关任务中的相似表示结构来简化目标任务，并实现收敛速率的保证。然而，在实践中，表示往往是高度非线性的，引入了每个任务中不可简单平均的非平凡偏差。本研究通过非线性表示推导出元学习的理论保证。

    Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
    
[^11]: 用有效的视野连接强化学习理论和实践

    Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])

    [http://arxiv.org/abs/2304.09853](http://arxiv.org/abs/2304.09853)

    本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。

    

    深度强化学习在某些环境中表现出色，但在其他环境中却失败得非常严重。理想情况下，强化学习理论应该能够解释这种现象，提供预测实际性能的界限。不幸的是，当前的理论还没有这种能力。本文通过引入包含155个MDP的新数据集BRIDGE，将标准的深度强化学习算法与之前的样本复杂度先前界进行比较，并发现了一个意想不到的性质：当最高Q值的动作在随机策略下的Q值也是最高的时，深度强化学习往往会成功；反之，失败的可能性较高。基于这一性质，我们将其概括为一个新的MDP复杂度度量，称为有效的视野。

    Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
    
[^12]: OKRidge: 用于学习动态系统的可扩展 k 稀疏岭回归

    OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])

    [http://arxiv.org/abs/2304.06686](http://arxiv.org/abs/2304.06686)

    本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。

    

    本文解决了科学发现中的一个重要问题，即，确定非线性动态系统的稀疏控制方程，通过求解稀疏岭回归问题可以证明最优性，以确定驱动基础动态的项。我们提出了一种称为 OKRidge 的快速算法，用一种新颖的下界计算方法，涉及鞍点公式，然后使用线性系统或基于 ADMM 的方法来解决，其中可以通过解决另一个线性系统和单调回归问题来有效地计算近端算子。我们还提出了一种启动我们求解器的方法，利用了波束搜索。在实验中，我们的方法达到可证明的最优性，并且运行时间比商业求解器 Gurobi 解决的现有 MIP公式运行时间快几个数量级。

    We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
    
[^13]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^14]: 使用Score matching的乘积Jacobi-Theta Boltzmann机器

    Product Jacobi-Theta Boltzmann machines with score matching. (arXiv:2303.05910v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05910](http://arxiv.org/abs/2303.05910)

    本文介绍了一种使用Score matching的乘积Jacobi-Theta Boltzmann机器（pJTBM），它比原始的RTBM更高效地拟合概率密度。

    

    估计概率密度函数是一个不容易的任务，在过去几年中，人们已经开始使用机器学习技术来解决这个问题。借鉴Boltzmann机器的架构，可以得到成功的应用。本文介绍了一种称为乘积Jacobi-Theta Boltzmann机器（pJTBM）的模型，它是Riemann-Theta Boltzmann机器（RTBM）的受限版本，具有对角隐藏部分连接矩阵。我们展示了基于Fisher散度的Score matching可以用来比原始的RTBM更高效地拟合pJTBM的概率密度。

    The estimation of probability density functions is a non trivial task that over the last years has been tackled with machine learning techniques. Successful applications can be obtained using models inspired by the Boltzmann machine (BM) architecture. In this manuscript, the product Jacobi-Theta Boltzmann machine (pJTBM) is introduced as a restricted version of the Riemann-Theta Boltzmann machine (RTBM) with diagonal hidden sector connection matrix. We show that score matching, based on the Fisher divergence, can be used to fit probability densities with the pJTBM more efficiently than with the original RTBM.
    
[^15]: 用核斯坦离差控制矩

    Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.05408](http://arxiv.org/abs/2211.05408)

    本研究分析了核斯坦离差（KSD）控制性质，发现标准KSD无法控制矩的收敛，提出了可控制矩和弱收敛的下游扩散KSD，并且发展了可以准确描述$q$-Wasserstein收敛的KSD。

    

    核斯坦离差（KSD）用于衡量分布逼近的质量，并且可以在目标密度具有不可计算的归一化常数时计算。显著的应用包括诊断近似MCMC采样器和非归一化统计模型的适配度检验。本文分析了KSD的收敛控制性质。我们首先证明了用于弱收敛控制的标准KSD无法控制矩的收敛。为了解决这个限制，我们提供了一组充分条件，下游扩散KSD可以同时控制矩和弱收敛。作为一个直接的结果，我们发展了对于每个$q>0$，第一组已知可以准确描述$q$-Wasserstein收敛的KSD。

    Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q > 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
    
[^16]: EC-NAS: 面向神经架构搜索的能耗感知表格基准

    EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06015](http://arxiv.org/abs/2210.06015)

    提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。

    

    近年来，选择、训练和部署深度学习模型所需的能量消耗不断增加。本文旨在支持设计能效高、训练资源消耗较低、适用于实际边缘/移动计算环境并具有环境可持续性的深度学习模型。我们提出将能效作为神经架构搜索 (NAS) 的一项额外性能指标，并通过添加不同架构的能耗和碳足迹信息，提供更新的表格基准 EC-NAS 以在较低计算成本下评估 NAS 策略。EC-NAS 还包括用于预测能耗的代理模型，并有助于降低总能耗。

    Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
    
[^17]: 岭函数估计中良性过拟合现象的有限样本分析

    A finite sample analysis of the benign overfitting phenomenon for ridge function estimation. (arXiv:2007.12882v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2007.12882](http://arxiv.org/abs/2007.12882)

    本论文研究了岭函数估计中的良性过拟合现象，并在有限维度情况下对线性模型进行了分析。

    

    最近在大规模机器学习中进行的广泛数值实验揭示了一个相当反直觉的相变现象，即样本大小与模型参数数量之比的函数关系。当参数数量$p$接近样本大小$n$时，泛化误差增加，但令人惊讶的是，当$p>n$时它再次开始减小。这一现象在\cite{belkin2019reconciling}中引起了理论界的关注，最近已经进行了深入研究，特别是针对比深度神经网络更简单的模型，例如线性模型中参数取最小范数解的情况。首先在当$p$和$n$趋于无穷大的渐近情况下进行研究（参见\cite{hastie2019surprises}），然后在有限维情况下更具体地针对线性模型进行研究（参见\cite{bartlett2020benign}，\cite{tsigler2020benign}，\cite{lecue2022geometrical}）。

    Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error increases, but surprisingly, it starts decreasing again past the threshold $p=n$. This phenomenon, brought to the theoretical community attention in \cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-squares problem, firstly in the asymptotic regime when $p$ and $n$ tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in the finite dimensional regime and more specifically for linear models \cite{bartlett2020benign}, \cite{tsigler2020benign}, \cite{lecue2022geometrical}. In the p
    

