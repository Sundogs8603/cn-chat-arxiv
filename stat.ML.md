# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Can we Agree? On the Rash\=omon Effect and the Reliability of Post-Hoc Explainable AI.](http://arxiv.org/abs/2308.07247) | 本研究通过在罗织集中使用SHAP将样本大小与模型的解释进行了比较，发现随着样本大小的增加，解释逐渐趋于一致。然而，少于128个样本的解释变异性高，限制了可靠知识的提取。我们的研究结果为信任解释所需的充足数据提供了指导。 |
| [^2] | [A Time-aware tensor decomposition for tracking evolving patterns.](http://arxiv.org/abs/2308.07126) | 提出了一种适用于跟踪演变模式的时空张量分解方法tPARAFAC2，通过时间正则化器从时间数据中提取逐渐演变的模式。 |
| [^3] | [iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN.](http://arxiv.org/abs/2308.07117) | iSTFTNet2是一个改进的基于iSTFT的声码器，使用1D-2D CNN来模拟时间和谱图结构，从而在保持语音质量的同时提高了速度和轻量级性能。 |
| [^4] | [No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning.](http://arxiv.org/abs/2308.07047) | 本文提出了一种无需正则化的高效且有效的模型，用于解决不完整标签分布学习问题。通过正确处理观察到的小度量并逐渐增加缺失度量的权重，可以学习到准确的标签分布。 |
| [^5] | [Greedy online change point detection.](http://arxiv.org/abs/2308.07012) | GOCPD是一种贪心算法，在线变点检测方法，通过最大化数据来自两个独立模型的概率来找到变点，具有较高的准确性，并且可以通过三分搜索进行加速。 |
| [^6] | [Shape-Graph Matching Network (SGM-net): Registration for Statistical Shape Analysis.](http://arxiv.org/abs/2308.06869) | 本文通过使用新颖的神经网络架构和无监督损失函数，提出了一种解决统计形状分析中形状图匹配的方法，该方法在匹配性能和计算成本方面实现了显著的改进。 |
| [^7] | [Fr\'echet Statistics Based Change Point Detection in Multivariate Hawkes Process.](http://arxiv.org/abs/2308.06769) | 本文提出了一种基于Frechet统计的方法，用于在多变量Hawkes过程中检测变点。通过将点过程分成窗口，并利用核矩阵来重构有符号的拉普拉斯矩阵，我们的方法能够准确地检测和描述多变量Hawkes过程因果结构中的变化，具有潜在的金融和神经科学等领域应用价值。 |
| [^8] | [Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection.](http://arxiv.org/abs/2308.06740) | 我们提出了一种加权稀疏偏最小二乘（wsPLS）方法，用于联合样本和特征选择。该方法通过引入$\ell_\infty/\ell_0$-范数约束来选择特定子集的样本，并证明了约束具有全局收敛性质。此外，我们还扩展了方法以适用于多视角数据。 |
| [^9] | [Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards.](http://arxiv.org/abs/2308.06717) | 本文研究了一个自私学习代理和学习委托人之间的重复对自选游戏，探索如何估计和激励具有隐藏奖励的不完全知识代理。 |
| [^10] | [Law of Balance and Stationary Distribution of Stochastic Gradient Descent.](http://arxiv.org/abs/2308.06671) | 本文证明了随机梯度下降算法中的小批量噪音会使解决方案向平衡解靠近，只要损失函数包含重新缩放对称性。利用这个结果，我们导出了对角线性网络的随机梯度流稳态分布，该分布展示了复杂的非线性现象。这些发现揭示了动态梯度下降法在训练神经网络中的工作原理。 |
| [^11] | [Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation.](http://arxiv.org/abs/2308.06422) | 本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。 |
| [^12] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^13] | [Diffusion Model in Causal Inference with Unmeasured Confounders.](http://arxiv.org/abs/2308.03669) | 本研究扩展了扩散模型的使用，提出了一种新的模型BDCM，可以在存在无法测量的混淆因素的情况下更准确地回答因果问题。 |
| [^14] | [Spectral Ranking Inferences based on General Multiway Comparisons.](http://arxiv.org/abs/2308.02918) | 本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。 |
| [^15] | [A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation.](http://arxiv.org/abs/2308.02293) | 通过引入高阶总变差正则化的随机优化算法，可以高效地训练非线性神经网络，避免过拟合问题。 |
| [^16] | [Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions.](http://arxiv.org/abs/2307.10644) | 本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。 |
| [^17] | [Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB.](http://arxiv.org/abs/2307.07539) | 本文提出了对GP-UCB算法进行改进，使其具有几乎最优的次线性遗憾，并解决了关于遗憾分析的开放问题。 |
| [^18] | [Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges.](http://arxiv.org/abs/2307.01050) | 本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。 |
| [^19] | [Decentralized SGD and Average-direction SAM are Asymptotically Equivalent.](http://arxiv.org/abs/2306.02913) | 分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力 |
| [^20] | [How many samples are needed to leverage smoothness?.](http://arxiv.org/abs/2305.16014) | 本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。 |
| [^21] | [Pre-processing training data improves accuracy and generalisability of convolutional neural network based landscape semantic segmentation.](http://arxiv.org/abs/2304.14625) | 本文研究了卷积神经网络（CNN）训练和景观语义分割中的数据准备方法，并发现分层随机抽样方法和较少的大补丁能改善模型准确性，而数据增强和缩放对创建一般化模型非常重要。 |
| [^22] | [Augmented balancing weights as linear regression.](http://arxiv.org/abs/2304.14545) | 本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。 |
| [^23] | [Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification.](http://arxiv.org/abs/2304.02849) | 该论文介绍了一种新的分类方法，用于提高鲁棒性，减少标签噪声的影响，其基于正态分布，并可通过最小化负对数似然来学习参数。 |
| [^24] | [Model-Free Reinforcement Learning with the Decision-Estimation Coefficient.](http://arxiv.org/abs/2211.14250) | 本文介绍了决策估计系数和Estimation-to-Decisions元算法，在无模型强化学习中结合乐观估计，提供了更宽松的估计误差概念的保证。 |
| [^25] | [Overparameterized random feature regression with nearly orthogonal data.](http://arxiv.org/abs/2211.06077) | 本文研究了过度参数化的随机特征回归(RFRR)在几乎正交数据上的行为，证明了在第一层宽度大于样本大小的情况下，RFRR的训练误差、交叉验证和泛化误差可以高概率集中在核岭回归(KRR)的相应值周围，同时给出了用多项式核近似KRR性能的方法。 |
| [^26] | [Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation.](http://arxiv.org/abs/2203.05400) | 该论文研究了高斯过程插值中光滑参数估计的渐近界限。结果表明，光滑参数的最大似然估计不能在渐近意义下欠平滑真值，并且最大似然估计能恢复一类分段支持自相似函数的真实光滑度。 |
| [^27] | [Matrix Reordering for Noisy Disordered Matrices: Optimality and Computationally Efficient Algorithms.](http://arxiv.org/abs/2201.06438) | 本文研究了基于有噪声的无序矩阵模型的矩阵重排序问题，建立了最优解的统计极限，并提出了一种计算效率较高的算法。实验结果表明，该算法在处理单细胞RNA测序数据时具有优越性能。 |
| [^28] | [Inverse Extended Kalman Filter -- Part I: Fundamentals.](http://arxiv.org/abs/2201.01539) | 该论文介绍了逆扩展卡尔曼滤波器(I-EKF)在非线性过程动力学和未知输入方面的关键挑战，并提供了具有重要理论稳定性保证的解决方案。 |
| [^29] | [Localization in 1D non-parametric latent space models from pairwise affinities.](http://arxiv.org/abs/2108.03098) | 本文研究了从成对关联中的非参数化潜在空间模型中进行一维定位的问题，提出了一种估计过程，能够以高概率的方式将所有潜在位置定位到最大误差为$\sqrt{\log(n)/n}$的范围内，该速率被证明是最小化的。 |
| [^30] | [Locally differentially private estimation of nonlinear functionals of discrete distributions.](http://arxiv.org/abs/2107.03940) | 本文研究了在局部差分隐私背景下估计离散分布的非线性函数的问题，并展示了互动和非互动的隐私机制。通过对幂和函数的二次风险行为的研究，在非互动情况下提出了两种插补类型的估计方法。 |
| [^31] | [Contrastive Attraction and Contrastive Repulsion for Representation Learning.](http://arxiv.org/abs/2105.03746) | 本论文提出了一种双重对比学习策略，通过分别比较正样本和负样本在各自群组内的关系，并对正负群组之间进行对比，以进一步提高对比学习方法的性能和鲁棒性。 |
| [^32] | [Leveraged Matrix Completion with Noise.](http://arxiv.org/abs/2011.05885) | 本文对带噪声的低秩矩阵补全问题进行了研究，通过使用杠杆得分来量化每个元素的重要性，并设计了一种非均匀采样过程来更准确地完成补全。 |
| [^33] | [Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning.](http://arxiv.org/abs/1905.12204) | 本文探索了使用学习算法近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。研究结果展示了提出方法在解决机器人/机器调度问题上的近乎最优性。 |
| [^34] | [Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks.](http://arxiv.org/abs/1903.10047) | 在更可信的情况下，我们展示了ResNet类型的CNN可以在一些重要的函数类中实现极小值最优误差率，并且可以通过复制全连接神经网络的学习能力来实现。 |

# 详细

[^1]: 能否达成一致？罗织效应和事后可解释人工智能的可靠性

    Can we Agree? On the Rash\=omon Effect and the Reliability of Post-Hoc Explainable AI. (arXiv:2308.07247v1 [cs.LG])

    [http://arxiv.org/abs/2308.07247](http://arxiv.org/abs/2308.07247)

    本研究通过在罗织集中使用SHAP将样本大小与模型的解释进行了比较，发现随着样本大小的增加，解释逐渐趋于一致。然而，少于128个样本的解释变异性高，限制了可靠知识的提取。我们的研究结果为信任解释所需的充足数据提供了指导。

    

    罗织效应给从机器学习模型中获得可靠知识带来了挑战。本研究使用SHAP在一个罗织集中考察了样本大小对模型解释的影响。在5个公共数据集上的实验证明，随着样本大小的增加，解释逐渐趋于一致。少于128个样本的解释变异性高，限制了可靠知识的提取。然而，随着更多数据的增加，模型之间的一致性也得到了改善，允许形成共识。集成模型常常具有更高的一致性。这些结果为信任解释所需的充足数据提供了指导。样本数量较少时的变异性表明，没有经过验证的结论可能是不可靠的。还需要进一步研究更多的模型类型、数据领域和解释方法。在神经网络和特定模型解释方法中测试收敛性将是有影响力的。这里探索的方法为从模糊的情况中获取知识提供了有原则的技术。

    The Rash\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous 
    
[^2]: 一种适用于跟踪演变模式的时空张量分解方法

    A Time-aware tensor decomposition for tracking evolving patterns. (arXiv:2308.07126v1 [cs.LG])

    [http://arxiv.org/abs/2308.07126](http://arxiv.org/abs/2308.07126)

    提出了一种适用于跟踪演变模式的时空张量分解方法tPARAFAC2，通过时间正则化器从时间数据中提取逐渐演变的模式。

    

    时间演变的数据集通常可以组织成一个高阶张量，其中的一个模式是时间模式。虽然张量分解已经成功地用于捕捉这类高阶数据集中的潜在模式，但往往忽略了时间的因素，允许时间点的重新排序。在最近的研究中，引入了时间正则化器来解决这个问题。然而，现有方法仍然不允许潜在模式在时间上发生变化（例如，大脑中的空间变化，主题中的上下文变化）。本文中，我们提出了一种基于PARAFAC2的时空张量分解方法tPARAFAC2，通过时间正则化器从时间数据中提取逐渐演变的模式。通过对合成数据的大量实验，我们证明了tPARAFAC2能够准确地捕捉到演变中的潜在模式，表现优于PARAFAC2和带有时间平滑正则化的耦合矩阵分解方法。

    Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regulariza
    
[^3]: iSTFTNet2：使用1D-2D CNN更快、更轻量级的基于iSTFT的神经声码器

    iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN. (arXiv:2308.07117v1 [cs.SD])

    [http://arxiv.org/abs/2308.07117](http://arxiv.org/abs/2308.07117)

    iSTFTNet2是一个改进的基于iSTFT的声码器，使用1D-2D CNN来模拟时间和谱图结构，从而在保持语音质量的同时提高了速度和轻量级性能。

    

    逆短时傅里叶变换网络（iSTFTNet）通过使用快速、轻量级的1D CNN作为主干，并将一些神经过程替换为iSTFT，以获得快速、轻量级和高保真度的语音合成。由于1D CNN很难模拟高维谱图，频率维度通过时间上采样进行降维。然而，这种策略会损害增强速度的潜力。因此，我们提出了iSTFTNet2，它是iSTFTNet的改进版本，采用1D-2D CNN，使用1D和2D CNN分别建模时间和谱图结构。我们设计了一个2D CNN，在转换到少量频率空间后执行频率上采样。这种设计使得能够模拟高维谱图而不损害速度。结果表明，iSTFTNet2使得iSTFTNet更快、更轻量级，同时保持可比较的语音质量。

    The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis. It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT. Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling. However, this strategy compromises the potential to enhance the speed. Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively. We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space. This design facilitates the modeling of high-dimensional spectrograms without compromising the speed. The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality. Audio samples are availa
    
[^4]: 无需正则化：一种用于不完整标签分布学习的高效且有效模型

    No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning. (arXiv:2308.07047v1 [cs.LG])

    [http://arxiv.org/abs/2308.07047](http://arxiv.org/abs/2308.07047)

    本文提出了一种无需正则化的高效且有效的模型，用于解决不完整标签分布学习问题。通过正确处理观察到的小度量并逐渐增加缺失度量的权重，可以学习到准确的标签分布。

    

    标签分布学习（LDL）为样本分配软标签，即度量。现实中，获得完整度量通常是费力的，从而引发了不完整LDL（InLDL）。然而，InLDL经常遭受性能退化。为了解决这个问题，现有方法需要一个或多个显式正则化，导致繁琐的参数调整和额外的计算。我们认为，标签分布本身可能提供有用的先验信息，当适当使用时，可以在不需要任何显式正则化的情况下解决InLDL问题。在本文中，我们提供了一种合理的替代方法来利用这样的先验信息。我们的观点是大度量很可能更受关注，小度量很容易被忽视，而缺失的度量在InLDL中完全被忽略。为了学习准确的标签分布，重要的是不忽视观察到的小度量，而是给它们适当的大权重，同时逐渐增加缺失度量的权重。

    Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we 
    
[^5]: 贪心在线变点检测

    Greedy online change point detection. (arXiv:2308.07012v1 [eess.SP])

    [http://arxiv.org/abs/2308.07012](http://arxiv.org/abs/2308.07012)

    GOCPD是一种贪心算法，在线变点检测方法，通过最大化数据来自两个独立模型的概率来找到变点，具有较高的准确性，并且可以通过三分搜索进行加速。

    

    标准的在线变点检测方法对异常值敏感，容易产生较高的误报率。为了克服这个缺点，我们提出了一种计算上具有吸引力的方法——贪心在线变点检测（GOCPD）。该方法通过最大化数据来自两个独立模型（时间上的）级联的概率来找到变点。我们证明，在具有单个变点的时间序列中，这个目标是单峰的，因此变点检测可以通过具有对数复杂度的三分搜索进行加速。我们在合成数据上验证了GOCPD的有效性，并在实际的单变量和多变量设置中验证了我们的发现。

    Standard online change point detection (CPD) methods tend to have large false discovery rates as their detections are sensitive to outliers. To overcome this drawback, we propose Greedy Online Change Point Detection (GOCPD), a computationally appealing method which finds change points by maximizing the probability of the data coming from the (temporal) concatenation of two independent models. We show that, for time series with a single change point, this objective is unimodal and thus CPD can be accelerated via ternary search with logarithmic complexity. We demonstrate the effectiveness of GOCPD on synthetic data and validate our findings on real-world univariate and multivariate settings.
    
[^6]: 形状-图匹配网络（SGM-net）：统计形状分析的注册

    Shape-Graph Matching Network (SGM-net): Registration for Statistical Shape Analysis. (arXiv:2308.06869v1 [cs.CV])

    [http://arxiv.org/abs/2308.06869](http://arxiv.org/abs/2308.06869)

    本文通过使用新颖的神经网络架构和无监督损失函数，提出了一种解决统计形状分析中形状图匹配的方法，该方法在匹配性能和计算成本方面实现了显著的改进。

    

    本文主要研究数据对象形状图的统计分析，即由连接着任意形状的曲线的节点集合。这里的关键需求是跨对象进行点（节点到节点，边到边）的受限制注册。这又需要在排列群上进行优化，给定节点（数量、位置）和边（形状、放置、大小）在对象之间存在差异的挑战。本文使用一种新颖的神经网络架构解决了这个注册问题，并采用弹性形状度量曲线开发了无监督损失函数。该架构在匹配性能方面效果显著，而且相对于基准方法，在计算成本上降低了一个数量级。我们使用模拟数据和真实的2D和3D形状图验证了所提出的方法的有效性。代码和数据将公开提供。

    This paper focuses on the statistical analysis of shapes of data objects called shape graphs, a set of nodes connected by articulated curves with arbitrary shapes. A critical need here is a constrained registration of points (nodes to nodes, edges to edges) across objects. This, in turn, requires optimization over the permutation group, made challenging by differences in nodes (in terms of numbers, locations) and edges (in terms of shapes, placements, and sizes) across objects. This paper tackles this registration problem using a novel neural-network architecture and involves an unsupervised loss function developed using the elastic shape metric for curves. This architecture results in (1) state-of-the-art matching performance and (2) an order of magnitude reduction in the computational cost relative to baseline approaches. We demonstrate the effectiveness of the proposed approach using both simulated data and real-world 2D and 3D shape graphs. Code and data will be made publicly avail
    
[^7]: 基于Frechet统计的多变量Hawkes过程中的变点检测

    Fr\'echet Statistics Based Change Point Detection in Multivariate Hawkes Process. (arXiv:2308.06769v1 [stat.ML])

    [http://arxiv.org/abs/2308.06769](http://arxiv.org/abs/2308.06769)

    本文提出了一种基于Frechet统计的方法，用于在多变量Hawkes过程中检测变点。通过将点过程分成窗口，并利用核矩阵来重构有符号的拉普拉斯矩阵，我们的方法能够准确地检测和描述多变量Hawkes过程因果结构中的变化，具有潜在的金融和神经科学等领域应用价值。

    

    本文提出了一种使用Frechet统计方法对多变量Hawkes过程中的因果网络进行变点检测的新方法。我们的方法将点过程分成重叠的窗口，在每个窗口中估计核矩阵，并通过将核矩阵视为因果网络的邻接矩阵来重构有符号的拉普拉斯矩阵。通过在模拟和真实加密货币数据集上进行实验，我们证明了我们的方法的有效性。我们的结果显示，我们的方法能够准确地检测和描述多变量Hawkes过程因果结构中的变化，并在金融和神经科学等领域具有潜在的应用价值。所提出的方法是对点过程设置中Frechet统计之前工作的扩展，并对多变量点过程的变点检测领域做出了重要贡献。

    This paper proposes a new approach for change point detection in causal networks of multivariate Hawkes processes using Frechet statistics. Our method splits the point process into overlapping windows, estimates kernel matrices in each window, and reconstructs the signed Laplacians by treating the kernel matrices as the adjacency matrices of the causal network. We demonstrate the effectiveness of our method through experiments on both simulated and real-world cryptocurrency datasets. Our results show that our method is capable of accurately detecting and characterizing changes in the causal structure of multivariate Hawkes processes, and may have potential applications in fields such as finance and neuroscience. The proposed method is an extension of previous work on Frechet statistics in point process settings and represents an important contribution to the field of change point detection in multivariate point processes.
    
[^8]: 加权稀疏偏最小二乘用于联合样本和特征选择

    Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection. (arXiv:2308.06740v1 [cs.LG])

    [http://arxiv.org/abs/2308.06740](http://arxiv.org/abs/2308.06740)

    我们提出了一种加权稀疏偏最小二乘（wsPLS）方法，用于联合样本和特征选择。该方法通过引入$\ell_\infty/\ell_0$-范数约束来选择特定子集的样本，并证明了约束具有全局收敛性质。此外，我们还扩展了方法以适用于多视角数据。

    

    稀疏偏最小二乘（sPLS）是一种常见的数据融合降维技术，通过寻找具有最大方差的少数变量的线性组合来将数据样本从两个视图投影出来。然而，sPLS提取的是两个数据集之间的所有数据样本之间的组合，因此无法检测样本的潜在子集。为了通过识别特定的样本子集并去除异常值来扩展sPLS的应用，我们提出了一种$\ell_\infty/\ell_0$-范数约束加权稀疏PLS（$\ell_\infty/\ell_0$-wsPLS）方法，用于联合样本和特征选择，其中$\ell_\infty/\ell_0$-范数约束用于选择样本子集。我们证明了$\ell_\infty/\ell_0$-范数约束具有Kurdyka-\L{ojasiewicz}性质，从而开发了一种全局收敛算法来解决它。此外，在各种实际问题中，多视角数据可以具有相同的样本集。为此，我们扩展了$\ell_\infty$-norm-wsPLS方法以适用于多视角数据。

    Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection, where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\ell_\inft
    
[^9]: 估计和激励具有隐藏奖励的不完全知识代理

    Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards. (arXiv:2308.06717v1 [cs.LG])

    [http://arxiv.org/abs/2308.06717](http://arxiv.org/abs/2308.06717)

    本文研究了一个自私学习代理和学习委托人之间的重复对自选游戏，探索如何估计和激励具有隐藏奖励的不完全知识代理。

    

    在实践中，激励提供者（即委托人）通常无法观察受到激励的代理的奖励实现情况，这与许多先前研究过的委托人-代理模型形成了对比。这种信息不对称性使委托人仅通过观察代理的决策就要始终估计代理的未知奖励变得更加困难，当代理需要学习自己的奖励时，这个挑战变得更加困难。这种复杂情况在各种现实场景中被观察到，从可再生能源储存合同到个性化医疗保健激励。因此，它不仅提供了有趣的理论问题，而且具有广泛的实际相关性。本文探讨了一个自私学习代理和学习委托人之间的重复对自选游戏。代理解决多臂老虎机（MAB）问题，以最大化他们预期的奖励和激励。除代理的学习外，委托人还训练了一个并行算法，并面临一个折中情况

    In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-of
    
[^10]: 动态梯度下降法的平衡法则与稳态分布

    Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])

    [http://arxiv.org/abs/2308.06671](http://arxiv.org/abs/2308.06671)

    本文证明了随机梯度下降算法中的小批量噪音会使解决方案向平衡解靠近，只要损失函数包含重新缩放对称性。利用这个结果，我们导出了对角线性网络的随机梯度流稳态分布，该分布展示了复杂的非线性现象。这些发现揭示了动态梯度下降法在训练神经网络中的工作原理。

    

    随机梯度下降（SGD）算法是我们用于训练神经网络的算法。然而，我们很难理解SGD如何在神经网络的非线性和退化的损失曲面中进行导航。在这项工作中，我们证明了SGD的小批量噪音可以使解决方案向平衡解靠近，只要损失函数包含一个重新缩放对称性。由于简单扩散过程和SGD动力学的差异在对称性存在时最重要，我们的理论表明，损失函数的对称性是了解SGD工作方式的重要线索。然后，我们将这个结果应用于导出具有任意深度和宽度的对角线性网络的随机梯度流的稳态分布。稳态分布展现了复杂的非线性现象，如相变、破坏的遍历性和波动反转。这些现象仅在深层网络中存在，表明了一种基本的新的加深训练理论。

    The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundam
    
[^11]: 使用基于聚类的树状Parzen估计的敏感性感知混合精度量化和宽度优化的深度神经网络

    Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])

    [http://arxiv.org/abs/2308.06422](http://arxiv.org/abs/2308.06422)

    本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。

    

    随着深度学习模型的复杂性和计算需求的提高，对神经网络设计的有效优化方法的需求变得至关重要。本文引入了一种创新的搜索机制，用于自动选择单个神经网络层的最佳位宽和层宽。这导致深度神经网络效率的明显提高。通过利用基于Hessian的剪枝策略，有选择地减少搜索域，确保移除非关键参数。随后，我们通过使用基于聚类的树状Parzen估计器开发有利和不利结果的替代模型。这种策略允许对架构可能性进行简化的探索，并迅速确定表现最好的设计。通过对知名数据集进行严格测试，我们的方法证明了与现有方法相比的明显优势。与领先的压缩策略相比，我们的方法取得了令人瞩目的成果。

    As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
    
[^12]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^13]: 无法测量混淆因素下因果推断中的扩散模型

    Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])

    [http://arxiv.org/abs/2308.03669](http://arxiv.org/abs/2308.03669)

    本研究扩展了扩散模型的使用，提出了一种新的模型BDCM，可以在存在无法测量的混淆因素的情况下更准确地回答因果问题。

    

    我们研究了如何在无法测量的混淆因素存在的情况下，扩展扩散模型的使用，以从观测数据中回答因果问题。在Pearl的使用有向无环图（DAG）捕捉因果干预的框架中，提出了一种基于扩散模型的因果模型（DCM），可以更准确地回答因果问题，假设所有混淆因素都是可以观察到的。然而，实际中存在无法测量的混淆因素，这使得DCM无法应用。为了缓解DCM的这一局限性，我们提出了一个扩展模型，称为基于反门准则的DCM（BDCM），其思想根植于在DAG中找到要包括在扩散模型解码过程中的变量的反门准则，这样我们可以将DCM扩展到存在无法测量的混淆因素的情况。合成数据实验表明，我们提出的模型在无法测量混淆因素的情况下更精确地捕捉到了反事实分布。

    We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
    
[^14]: 基于广义多维比较的光谱排名推断

    Spectral Ranking Inferences based on General Multiway Comparisons. (arXiv:2308.02918v1 [stat.ME])

    [http://arxiv.org/abs/2308.02918](http://arxiv.org/abs/2308.02918)

    本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。

    

    本文研究了在一个非常普遍和更加真实的情景中，使用光谱方法对未观察到的比较实体的偏好分数进行估计和不确定性量化的性能。在这种情况下，比较图由可能具有异构大小的超边组成，对于给定的超边，比较数量可能仅为1。这种设置在实际应用中普遍存在，避免了需要指定图的随机性以及在常用的Bradley-Terry-Luce (BTL)或Plackett-Luce (PL)模型中施加的限制性均匀采样假设。此外，在适用BTL或PL模型的情况下，我们揭示了光谱估计量与最大似然估计量（MLE）之间的关系。我们发现，通过应用从等权重传统光谱方法估计得到的最佳加权，可以实现与MLE相同的渐近效率的双步光谱方法。考虑到渐近情况，

    This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptot
    
[^15]: 用正则化高阶总变差的随机优化方法训练非线性神经网络

    A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])

    [http://arxiv.org/abs/2308.02293](http://arxiv.org/abs/2308.02293)

    通过引入高阶总变差正则化的随机优化算法，可以高效地训练非线性神经网络，避免过拟合问题。

    

    尽管包括深度神经网络在内的高度表达的参数模型可以更好地建模复杂概念，但训练这种高度非线性模型已知会导致严重的过拟合风险。针对这个问题，本研究考虑了一种k阶总变差（k-TV）正则化，它被定义为要训练的参数模型的k阶导数的平方积分，通过惩罚k-TV来产生一个更平滑的函数，从而避免过拟合。尽管将k-TV项应用于一般的参数模型由于积分而导致计算复杂，本研究提供了一种随机优化算法，可以高效地训练带有k-TV正则化的一般模型，而无需进行显式的数值积分。这种方法可以应用于结构任意的深度神经网络的训练，因为它只需要进行简单的随机梯度优化即可实现。

    While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
    
[^16]: Fisher-Rao距离和逆推到SPD锥距离在多元正态分布之间的应用

    Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])

    [http://arxiv.org/abs/2307.10644](http://arxiv.org/abs/2307.10644)

    本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。

    

    许多科学领域，如扩散张量成像、结构张量计算机视觉、雷达信号处理和机器学习等，都存在着多元正态分布的数据集。为了处理这些正态数据集以进行过滤、分类或聚类等下游任务，需要定义合适的正态和它们之间的路径之间的差异度量。Fisher-Rao距离，作为Fisher信息度量引起的Riemann几何距离，是一种合理的度量距离，但除了一些特殊情况外，并没有闭式求解。本文首先报告了一种快速且鲁棒的方法，可以精确地近似计算多元正态分布之间的Fisher-Rao距离。其次，我们介绍了一类基于正态流形到高维对称正定锥的子流形的微分同胚嵌入的距离。

    Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
    
[^17]: 在希尔伯特空间中改进自标准化浓度：对GP-UCB算法的次线性遗憾

    Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB. (arXiv:2307.07539v1 [cs.LG])

    [http://arxiv.org/abs/2307.07539](http://arxiv.org/abs/2307.07539)

    本文提出了对GP-UCB算法进行改进，使其具有几乎最优的次线性遗憾，并解决了关于遗憾分析的开放问题。

    

    在核化赌博机问题中，学习器旨在通过仅在顺序选择的点处进行噪声评估，顺序计算位于再生核希尔伯特空间中的函数的最优解。特别地，学习器旨在最小化遗憾，遗憾是所做选择的次优性度量。可以说最受欢迎的算法是高斯过程上界置信区间（GP-UCB）算法，它涉及根据未知函数的简单线性估计器进行行动。尽管它很受欢迎，但现有的GP-UCB遗憾分析给出了次优遗憾率，对于许多常用的内核（如Matérn内核）而言，遗憾率并不次线性。这引发了一个长期存在的问题：现有的GP-UCB遗憾分析是否紧密，或者是否可以通过使用更复杂的分析技术改进界限？在这项工作中，我们解决了这个开放问题，并证明了GP-UCB具有几乎最优的遗憾。特别地，我们的结果直接暗示了次线性遗憾率。

    In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made. Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function. Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Mat\'ern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques? In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results directly imply sublinear regret rate
    
[^18]: 运输、变分推断和扩散：应用于回火流和薛定谔桥的论文研究

    Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])

    [http://arxiv.org/abs/2307.01050](http://arxiv.org/abs/2307.01050)

    本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。

    

    本文探讨了最优运输与变分推断之间的联系，重点研究了正向和反向随机微分方程以及Girsanov变换。我们提出了一个基于路径空间散度的采样和生成建模的原则性和系统性框架。我们的工作最终发展出一个新颖的基于得分的回火流技术（与统计物理中的Jarzynski和Crooks恒等式有关）和一个正则化的迭代比例拟合（IPF）型目标，不同于标准IPF的顺序性。通过一系列的生成建模示例和基于双井的稀有事件任务，我们展示了所提方法的潜力。

    This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
    
[^19]: 分散化SGD和平均方向SAM在渐近意义下是等价的

    Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])

    [http://arxiv.org/abs/2306.02913](http://arxiv.org/abs/2306.02913)

    分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力

    

    分散随机梯度下降（D-SGD）允许在没有中央服务器的控制下，大量设备同时进行协作学习。然而，现有理论认为，分散化不可避免地削弱了泛化能力。本文挑战传统信念，提出了完全新的角度来理解分散学习。我们证明了在一般非凸非-$\beta$-平滑设置下，D-SGD隐式地最小化了平均方向锐度感知最小化（SAM）算法的损失函数。这种惊人的渐近等价揭示了内在的正则化-优化权衡以及分散化的三个优点：（1）D-SGD中存在一个自由的不确定性评估机制，可以提高后验估计；（2）D-SGD表现出梯度平滑效应；（3）D-SGD的锐度正则化效应不会随着总批处理大小的增加而减少，这证明了潜在的泛化能力

    Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
    
[^20]: 当前机器学习需要多少样本才能利用平滑性？

    How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])

    [http://arxiv.org/abs/2305.16014](http://arxiv.org/abs/2305.16014)

    本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。

    

    统计学习的核心原则之一是，目标函数的平滑性可以打破维度灾难。然而，通过泰勒展开学习平滑函数需要足够接近一起的样本来获得高阶导数的有意义估计，这在数据量相对较小的机器学习问题中似乎很困难。本文通过推导广义泛化误差的新的下界，研究了常数和瞬态区域在实践中通常被忽略却发挥了主导作用的问题。

    A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
    
[^21]: 预处理训练数据改善了基于卷积神经网络的景观语义分割的准确性和可泛化性

    Pre-processing training data improves accuracy and generalisability of convolutional neural network based landscape semantic segmentation. (arXiv:2304.14625v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2304.14625](http://arxiv.org/abs/2304.14625)

    本文研究了卷积神经网络（CNN）训练和景观语义分割中的数据准备方法，并发现分层随机抽样方法和较少的大补丁能改善模型准确性，而数据增强和缩放对创建一般化模型非常重要。

    

    本文在澳大利亚昆士兰州湿热带和阿瑟顿高原地区的航空摄影中，试验了不同的数据准备方法，用于卷积神经网络（CNN）训练和土地利用土地覆盖（LULC）特征的语义分割。通过试验和排名不同的训练补丁选择采样策略，补丁和批次大小以及数据增强和缩放，我们比较了模型的准确性。通过生成LULC分类，我们还比较了使用一个网格补丁的单次传递和多次传递以及每个补丁的三个旋转版本的模型准确性。我们的结果表明：对于产生训练补丁，分层随机抽样方法改善了面积较小的类别的准确性，对较大类别的影响很小；相较于较多的小补丁，较少的大补丁能提高模型准确性；应用数据增强和缩放对创建一般化模型至关重要。

    In this paper, we trialled different methods of data preparation for Convolutional Neural Network (CNN) training and semantic segmentation of land use land cover (LULC) features within aerial photography over the Wet Tropics and Atherton Tablelands, Queensland, Australia. This was conducted through trialling and ranking various training patch selection sampling strategies, patch and batch sizes and data augmentations and scaling. We also compared model accuracy through producing the LULC classification using a single pass of a grid of patches and averaging multiple grid passes and three rotated version of each patch. Our results showed: a stratified random sampling approach for producing training patches improved the accuracy of classes with a smaller area while having minimal effect on larger classes; a smaller number of larger patches compared to a larger number of smaller patches improves model accuracy; applying data augmentations and scaling are imperative in creating a generalise
    
[^22]: 自动去偏重重配作为线性回归

    Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])

    [http://arxiv.org/abs/2304.14545](http://arxiv.org/abs/2304.14545)

    本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。

    

    我们提供了对于自动去偏重重配(AutoDML)的新颖特征描述。这些估算器将结果建模与重配相结合，直接估计反向倾向积分权重。当结果与权重模型都是某些（可能是无限的）基础中的线性时，我们表明增强的估算器等同于具有将原始结果模型系数和OLS相结合的系数的单个线性模型；在许多设置中，增强估算器合并为仅使用OLS. 然后，我们将这些结果扩展到特定的结果和重配模型选择上。我们首先表明，使用(内核)岭回归作为结果和重配模型的联合估算器等同于单个、欠平滑(内核)岭回归；当考虑到渐近速率时，这一结果也成立。当代替权重模型为套索回归时，我们给出了特殊情况的解析表达式并且演示了…

    We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
    
[^23]: 分类中异方差标签噪声的逻辑正态似然

    Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification. (arXiv:2304.02849v1 [cs.LG])

    [http://arxiv.org/abs/2304.02849](http://arxiv.org/abs/2304.02849)

    该论文介绍了一种新的分类方法，用于提高鲁棒性，减少标签噪声的影响，其基于正态分布，并可通过最小化负对数似然来学习参数。

    

    在回归中估计异方差标签噪声的一种自然方法是将观测到的（可能带有噪声的）目标建模为一个正态分布的样本，其参数可以通过最小化负对数似然来学习。该损失具有期望的损失衰减特性，因为它可以降低高误差示例的贡献。直观地说，这种行为可以通过减少过拟合来提高对标签噪声的鲁棒性。我们提出了这种简单且概率化方法在分类中的扩展，具有相同的期望损失衰减特性。我们通过测量其对分类中标签噪声的鲁棒性来评估该方法的有效性。我们进行了启发性的实验，探索了该方法的内部工作原理，包括对超参数的敏感性，消融研究等。

    A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This loss has desirable loss attenuation properties, as it can reduce the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and more.
    
[^24]: 无模型强化学习与决策估计系数

    Model-Free Reinforcement Learning with the Decision-Estimation Coefficient. (arXiv:2211.14250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14250](http://arxiv.org/abs/2211.14250)

    本文介绍了决策估计系数和Estimation-to-Decisions元算法，在无模型强化学习中结合乐观估计，提供了更宽松的估计误差概念的保证。

    

    本文考虑交互式决策问题，涵盖结构化赌博机和具有通用函数逼近的强化学习。最近，Foster等人（2021）引入了决策估计系数，这是一种统计复杂度的度量，其为交互式决策制定了一个最优遗憾的下界，并引入了Estimation-to-Decisions元算法，该算法以相同数量为上界。Estimation-to-Decisions是一种约简，将（监督）在线估计的算法提升为决策制定的算法。在本文中，我们展示了通过将Estimation-to-Decisions与Zhang（2022）引入的一种专门的乐观估计形式相结合，可以得到比Foster等人（2021）更为宽松的估计误差概念的保证。我们利用这种方法导出了与值函数逼近相结合的无模型强化学习的遗憾界，并给出了策略。

    We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al. (2021) introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this paper, we show that by combining Estimation-to-Decisions with a specialized form of optimistic estimation introduced by Zhang (2022), it is possible to obtain guarantees that improve upon those of Foster et al. (2021) by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation, and give str
    
[^25]: 过度参数化的随机特征回归与几乎正交数据

    Overparameterized random feature regression with nearly orthogonal data. (arXiv:2211.06077v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.06077](http://arxiv.org/abs/2211.06077)

    本文研究了过度参数化的随机特征回归(RFRR)在几乎正交数据上的行为，证明了在第一层宽度大于样本大小的情况下，RFRR的训练误差、交叉验证和泛化误差可以高概率集中在核岭回归(KRR)的相应值周围，同时给出了用多项式核近似KRR性能的方法。

    

    本文研究了随机特征岭回归 (RFRR)，其中包括一个具有随机高斯初始化的两层神经网络。我们研究了在过度参数化情况下，第一层的宽度远大于样本大小的条件下，采用几乎正交确定性单位长度输入数据向量进行 RFRR 的非渐近行为。我们的分析显示了 RFRR 训练误差、交叉验证和泛化误差的非渐近集中结果，在核岭回归 (KRR) 的相应值周围有高概率出现。该 KRR 是由非线性随机特征映射生成的期望核导出的。然后，我们通过激活函数的 Hermite 多项式展开获得的多项式核矩阵来近似 KRR 的性能，其次数仅取决于不同数据点之间的正交性。这个多项式核确定了 RFRR 和 KRR 的渐近行为。

    We investigate the properties of random feature ridge regression (RFRR) given by a two-layer neural network with random Gaussian initialization. We study the non-asymptotic behaviors of the RFRR with nearly orthogonal deterministic unit-length input data vectors in the overparameterized regime, where the width of the first layer is much larger than the sample size. Our analysis shows high-probability non-asymptotic concentration results for the training errors, cross-validations, and generalization errors of RFRR centered around their respective values for a kernel ridge regression (KRR). This KRR is derived from an expected kernel generated by a nonlinear random feature map. We then approximate the performance of the KRR by a polynomial kernel matrix obtained from the Hermite polynomial expansion of the activation function, whose degree only depends on the orthogonality among different data points. This polynomial kernel determines the asymptotic behavior of the RFRR and the KRR. Our 
    
[^26]: 高斯过程插值中光滑参数估计的渐近界限

    Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation. (arXiv:2203.05400v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2203.05400](http://arxiv.org/abs/2203.05400)

    该论文研究了高斯过程插值中光滑参数估计的渐近界限。结果表明，光滑参数的最大似然估计不能在渐近意义下欠平滑真值，并且最大似然估计能恢复一类分段支持自相似函数的真实光滑度。

    

    常见的方法是用Matern协方差核将确定性响应函数（如计算机实验的输出）建模为高斯过程。Matern核的光滑参数决定了模型在大数据极限下的许多重要属性，包括条件均值收敛到响应函数的速率。我们证明，当数据在固定有界子集$\mathbb{R}^d$上获得时，光滑参数的最大似然估计不能在渐近意义下欠平滑真值。换句话说，如果数据生成的响应函数具有Sobolev光滑度$\nu_0 > d/2$，那么光滑参数估计不能在渐近意义下小于$\nu_0$。这一下界是精准的。此外，我们还展示了最大似然估计在一类分段支持自相似函数中能恢复真实的光滑度。对于交叉验证，我们证明了一个渐近下界$\nu_0-d/2$，但这很不可能成立。

    It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Mat\'ern covariance kernel. The smoothness parameter of a Mat\'ern kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of $\mathbb{R}^d$. That is, if the data-generating response function has Sobolev smoothness $\nu_0 > d/2$, then the smoothness parameter estimate cannot be asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be s
    
[^27]: 基于有噪声的无序矩阵的矩阵重排序：最优性和计算效率算法

    Matrix Reordering for Noisy Disordered Matrices: Optimality and Computationally Efficient Algorithms. (arXiv:2201.06438v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2201.06438](http://arxiv.org/abs/2201.06438)

    本文研究了基于有噪声的无序矩阵模型的矩阵重排序问题，建立了最优解的统计极限，并提出了一种计算效率较高的算法。实验结果表明，该算法在处理单细胞RNA测序数据时具有优越性能。

    

    受单细胞生物学和宏基因组学应用的启发，我们研究了基于有噪声、无序单调Toeplitz矩阵模型的矩阵重排序问题。我们在一个决策理论框架下建立了这个问题的基本统计极限，并证明了一个约束最小二乘估计量能够达到最优速率。然而，由于计算复杂性的原因，我们分析了一种流行的多项式时间算法——光谱排序，并表明它是次优的。为了解决这个问题，我们提出了一种新颖的多项式时间自适应排序算法，并保证了性能的提升。对两个真实单细胞RNA测序数据集的模拟和分析结果表明，我们的算法优于现有方法。

    Motivated by applications in single-cell biology and metagenomics, we investigate the problem of matrix reordering based on a noisy disordered monotone Toeplitz matrix model. We establish the fundamental statistical limit for this problem in a decision-theoretic framework and demonstrate that a constrained least squares estimator achieves the optimal rate. However, due to its computational complexity, we analyze a popular polynomial-time algorithm, spectral seriation, and show that it is suboptimal. To address this, we propose a novel polynomial-time adaptive sorting algorithm with guaranteed performance improvement. Simulations and analyses of two real single-cell RNA sequencing datasets demonstrate the superiority of our algorithm over existing methods.
    
[^28]: 逆扩展卡尔曼滤波器--第一部分: 基础

    Inverse Extended Kalman Filter -- Part I: Fundamentals. (arXiv:2201.01539v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2201.01539](http://arxiv.org/abs/2201.01539)

    该论文介绍了逆扩展卡尔曼滤波器(I-EKF)在非线性过程动力学和未知输入方面的关键挑战，并提供了具有重要理论稳定性保证的解决方案。

    

    最近反对抗系统的进展引起了对贝叶斯逆滤波的研究的重视。例如，对于以预测对手未来步骤为目的，估计对手的卡尔曼滤波跟踪估计，引发了逆卡尔曼滤波(I-KF)的最近形式。在逆滤波的背景下，我们通过提出逆扩展卡尔曼滤波器(I-EKF)来解决非线性过程动力学和前向滤波器的不明输入的关键挑战。本文及其附属文(Part II)的目的是详细阐述I-EKF的理论。在本文中，我们假设具备完美的系统模型信息，并推导了前向和逆向状态空间模型均为非线性时的I-EKF,同时获得了带未知输入的I-KF。我们还利用有界非线性性和未知矩阵方法提供了理论稳定性保证。

    Recent advances in counter-adversarial systems have garnered significant research attention to inverse filtering from a Bayesian perspective. For example, interest in estimating the adversary's Kalman filter tracked estimate with the purpose of predicting the adversary's future steps has led to recent formulations of inverse Kalman filter (I-KF). In this context of inverse filtering, we address the key challenges of non-linear process dynamics and unknown input to the forward filter by proposing an inverse extended Kalman filter (I-EKF). The purpose of this paper and the companion paper (Part II) is to develop the theory of I-EKF in detail. In this paper, we assume perfect system model information and derive I-EKF with and without an unknown input when both forward and inverse state-space models are non-linear. In the process, I-KF-with-unknown-input is also obtained. We then provide theoretical stability guarantees using both bounded non-linearity and unknown matrix approaches and pro
    
[^29]: 从成对关联中的非参数化潜在空间模型中进行1D定位

    Localization in 1D non-parametric latent space models from pairwise affinities. (arXiv:2108.03098v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2108.03098](http://arxiv.org/abs/2108.03098)

    本文研究了从成对关联中的非参数化潜在空间模型中进行一维定位的问题，提出了一种估计过程，能够以高概率的方式将所有潜在位置定位到最大误差为$\sqrt{\log(n)/n}$的范围内，该速率被证明是最小化的。

    

    本文考虑从成对关联中估计一维环面上的潜在位置的问题。对于一对项之间的关联观测被建模为对潜在位置$x^*_{i},x^*_{j}$的函数$f(x^*_{i},x^*_{j})$的噪声观测。关联函数$f$是未知的，仅假设其满足一些形状约束，确保当$x$和$y$之间的距离较小时，$f(x,y)$较大，反之亦然。这种非参数化建模提供了适应数据的良好灵活性。我们引入了一种估计过程，能够以高概率的方式将所有潜在位置定位到最大误差为$\sqrt{\log(n)/n}$的范围内。这个速率被证明是最小化的。我们还对一种计算效率更高的变体进行了分析，该变体在一些更严格的假设下成立。我们的总体结果可以应用于统计排序问题，从而得到最大误差的新界限。

    We consider the problem of estimating latent positions in a one-dimensional torus from pairwise affinities. The observed affinity between a pair of items is modeled as a noisy observation of a function $f(x^*_{i},x^*_{j})$ of the latent positions $x^*_{i},x^*_{j}$ of the two items on the torus. The affinity function $f$ is unknown, and it is only assumed to fulfill some shape constraints ensuring that $f(x,y)$ is large when the distance between $x$ and $y$ is small, and vice-versa. This non-parametric modeling offers a good flexibility to fit data. We introduce an estimation procedure that provably localizes all the latent positions with a maximum error of the order of $\sqrt{\log(n)/n}$, with high-probability. This rate is proven to be minimax optimal. A computationally efficient variant of the procedure is also analyzed under some more restrictive assumptions. Our general results can be instantiated to the problem of statistical seriation, leading to new bounds for the maximum error 
    
[^30]: 非线性离散分布的局部差分隐私估计

    Locally differentially private estimation of nonlinear functionals of discrete distributions. (arXiv:2107.03940v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2107.03940](http://arxiv.org/abs/2107.03940)

    本文研究了在局部差分隐私背景下估计离散分布的非线性函数的问题，并展示了互动和非互动的隐私机制。通过对幂和函数的二次风险行为的研究，在非互动情况下提出了两种插补类型的估计方法。

    

    我们研究了在局部差分隐私的背景下估计离散分布的非线性函数的问题。初始数据$x_1,\ldots,x_n \in [K]$被假设为i.i.d.并根据未知的离散分布$p = (p_1,\ldots,p_K)$进行分布。只有$\alpha$-局部差分隐私(LDP)样本$z_1,...,z_n$是公开可用的，其中术语“局部”表示每个$z_i$都是使用一个个人属性$x_i$生成的。我们展示了可以使用互动的隐私机制（PM）或非互动的隐私机制来实现这一目标。我们描述了作为$K, \, n$和$\alpha$的函数的幂和函数$F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$的二次风险的行为。在非互动情况下，我们研究了两种插补类型的估计$F_{\gamma}$的方法，适用于所有$\gamma >0$，其类似于Jiao等(2017)在多项式模型中分析的MLE。

    We study the problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data $x_1,\ldots,x_n \in [K]$ are supposed i.i.d. and distributed according to an unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally differentially private (LDP) samples $z_1,...,z_n$ are publicly available, where the term 'local' means that each $z_i$ is produced using one individual attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma >0$ as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study two plug-in type estimators of $F_{\gamma}$, for all $\gamma >0$, that are similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model. However, due to 
    
[^31]: 对比吸引和对比排斥在表示学习中的应用

    Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03746](http://arxiv.org/abs/2105.03746)

    本论文提出了一种双重对比学习策略，通过分别比较正样本和负样本在各自群组内的关系，并对正负群组之间进行对比，以进一步提高对比学习方法的性能和鲁棒性。

    

    对比学习方法以自我监督方式有效地学习数据表示，其中编码器通过一对多的softmax交叉熵损失将每个正样本与多个负样本进行对比。最近的对比学习方法通过利用大量未标记的图像数据，在预训练模型上取得了有希望的结果，如ImageNet。然而，大多数方法认为来自同一实例的增强视图是正样本对，而来自其他实例的视图则是负样本对。这种二分法不充分地考虑样本之间的关系，并且在应用到野外图像时往往会产生较差的性能。为了进一步提高对比学习的性能，并增强其在各种数据集上的鲁棒性，我们提出了一种双重对比学习策略，分别在各自的群组内比较正样本和负样本，然后进行正负群组之间的对比。

    Contrastive learning (CL) methods effectively learn data representations in a self-supervision manner, where the encoder contrasts each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image data, recent CL methods have achieved promising results when pretrained on large-scale datasets, such as ImageNet. However, most of them consider the augmented views from the same instance are positive pairs, while views from other instances are negative ones. Such binary partition insufficiently considers the relation between samples and tends to yield worse performance when generalized on images in the wild. In this paper, to further improve the performance of CL and enhance its robustness on various datasets, {we propose a doubly CL strategy that separately compares positive and negative samples within their own groups, and then proceeds with a contrast between positive and negative groups}. We realize this stra
    
[^32]: 带噪声的杠杆矩阵补全

    Leveraged Matrix Completion with Noise. (arXiv:2011.05885v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.05885](http://arxiv.org/abs/2011.05885)

    本文对带噪声的低秩矩阵补全问题进行了研究，通过使用杠杆得分来量化每个元素的重要性，并设计了一种非均匀采样过程来更准确地完成补全。

    

    在过去十年中，从子采样测量中完成低秩矩阵引起了很多关注。现有的研究表明，为了在一定的概率下理论上确保完成一个$n \times n$的秩为$r$的带噪声矩阵，需要$\mathcal{O}(nr\log^2(n))$个数据，但是需要一些相当苛刻的假设：(1)所含的矩阵必须是不相关的；(2)观测值服从均匀分布。这种限制性部分是由于忽略了杠杆得分和每个元素的预测信息的作用。在本文中，我们使用杠杆得分来描述每个元素的重要性，并显著放宽了假设：(1)对底层低秩矩阵不施加任何其他结构假设；(2)被观测到的元素通过杠杆得分适当地依赖于它们的重要性。在这些假设下，我们设计了一种非均匀/偏倚的采样过程，可以揭示“重要”的元素。

    Completing low-rank matrices from subsampled measurements has received much attention in the past decade. Existing works indicate that $\mathcal{O}(nr\log^2(n))$ datums are required to theoretically secure the completion of an $n \times n$ noisy matrix of rank $r$ with high probability, under some quite restrictive assumptions: (1) the underlying matrix must be incoherent; (2) observations follow the uniform distribution. The restrictiveness is partially due to ignoring the roles of the leverage score and the oracle information of each element. In this paper, we employ the leverage scores to characterize the importance of each element and significantly relax assumptions to: (1) not any other structure assumptions are imposed on the underlying low-rank matrix; (2) elements being observed are appropriately dependent on their importance via the leverage score. Under these assumptions, instead of uniform sampling, we devise an ununiform/biased sampling procedure that can reveal the ``impor
    
[^33]: 使用GNN学习NP-Hard多智能体分配规划：在随机图上的推理和可证明的拍卖适配Q学习

    Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning. (arXiv:1905.12204v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1905.12204](http://arxiv.org/abs/1905.12204)

    本文探索了使用学习算法近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。研究结果展示了提出方法在解决机器人/机器调度问题上的近乎最优性。

    

    本文探讨了使用基于学习的算法来近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。特别地，我们考虑了一类称为多机器人奖励收集问题（MRRC）的机器人/机器调度问题。这些MRRC问题很好地模拟了共乘、取送和其他相关问题。在将MRRC问题表示为顺序决策问题时，我们观察到每个状态可以被表示为概率图模型（PGM）的扩展，我们将其称为随机PGMs。然后，我们为随机PGMs开发了一种均场推理方法。接下来，我们提出了（1）一个可进行顺序转移的Q函数估计器和（2）一个支持顺序转移的拍卖方法，以在多项式时间内选择联合分配。这些方法导致了一个具有至少$1-1/e$最优性的强化学习框架。在解决MRRC问题的实验结果中突出了近乎最优性。

    This paper explores the possibility of near-optimally solving multi-agent, multi-task NP-hard planning problems with time-dependent rewards using a learning-based algorithm. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial time. These result in a reinforcement learning framework with at least $1-1/e$ optimality. Experimental results on solving MRRC problems highlight the near-optimality 
    
[^34]: 近似和非参数估计的ResNet类型卷积神经网络

    Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks. (arXiv:1903.10047v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1903.10047](http://arxiv.org/abs/1903.10047)

    在更可信的情况下，我们展示了ResNet类型的CNN可以在一些重要的函数类中实现极小值最优误差率，并且可以通过复制全连接神经网络的学习能力来实现。

    

    卷积神经网络(CNNs)已经在多种函数类中显示出了在近似和估计误差率方面的最优性(在极小值最大化意义上)。然而，以前分析的最优CNN是不现实的宽度，并且很难通过优化获得，因为在重要的函数类中具有稀疏约束，包括Holder类。我们展示了ResNet类型的CNN可以在更可信的情况下实现这些类的极小值最优误差率--它可以是密集的，并且其宽度、通道大小和滤波器大小与样本大小无关。关键思想是，我们可以通过定制的CNN复制全连接神经网络(FNNs)的学习能力，只要FNNs具有块稀疏结构。我们的理论在某种意义上是通用的，我们可以自动将块稀疏FNNs实现的任何近似率转化为CNNs实现的近似率。作为一个应用，我们推导了前述类型CNN的近似和估计误差率。

    Convolutional neural networks (CNNs) have been shown to achieve optimal approximation and estimation error rates (in minimax sense) in several function classes. However, previous analyzed optimal CNNs are unrealistically wide and difficult to obtain via optimization due to sparse constraints in important function classes, including the H\"older class. We show a ResNet-type CNN can attain the minimax optimal error rates in these classes in more plausible situations -- it can be dense, and its width, channel size, and filter size are constant with respect to sample size. The key idea is that we can replicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have \textit{block-sparse} structures. Our theory is general in a sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. As an application, we derive approximation and estimation error rates of the aformentioned type of CNNs f
    

