# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [RED CoMETS: An ensemble classifier for symbolically represented multivariate time series.](http://arxiv.org/abs/2307.13679) | 本文介绍了一种名为RED CoMETS的集成分类器，用于处理符号化表示的多变量时间序列数据。它在多变量设置中展现出竞争力的准确性，并在'HandMovementDirection'数据集上实现了最高的报告准确性。 |
| [^2] | [AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling.](http://arxiv.org/abs/2307.13616) | 该论文提出了一个新的解决方案，通过考虑变量之间的潜在相互作用，减少风险建模中的拟议差别，以实现更公平的保险定价和风险选择。 |
| [^3] | [Do algorithms and barriers for sparse principal component analysis extend to other structured settings?.](http://arxiv.org/abs/2307.13535) | 该论文研究了在尖峰Wishart模型下，通过一类子空间并集模型捕捉信号结构的主成分分析问题。通过统计和计算的视角，我们建立了基本限制，并展示了自然的投影功率方法在解决方案的统计近似最优邻域中的局部收敛性。我们还通过具体案例的分析展示了计算难度。结果表明，对于基本稀疏PCA观察到的现象在其结构化对应物中也同样存在。 |
| [^4] | [Continuous Time Evidential Distributions for Irregular Time Series.](http://arxiv.org/abs/2307.13503) | 该论文提出了一种在连续时间中学习不规则时间序列的证据分布的策略，能够在任何感兴趣的时间上对部分观测到的特征进行良好校准和灵活的推断，并且在稀疏、不规则观测的时间上扩展不确定性。该方法在时间序列分类任务上表现出竞争性的性能，并能够在遇到噪音数据时实现基于不确定性的推断。 |
| [^5] | [Finding Money Launderers Using Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2307.13499) | 本文介绍了一种使用异构图神经网络来寻找洗钱者的方法，该方法在真实的银行交易和商业角色数据构建的大型异构网络中识别洗钱活动。为了解决洗钱活动中犯罪分子的合作问题，我们扩展了同质图神经网络方法，提出了一种新颖的消息聚合方法。 |
| [^6] | [Fundamental causal bounds of quantum random access memories.](http://arxiv.org/abs/2307.13460) | 本研究通过采用相对论量子场论和量子多体系统中的Lieb-Robinson界限，批判性地探讨了基于因果性的快速量子存储器的内在界限。研究表明在混合量子声学系统中，QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。 |
| [^7] | [Scaff-PD: Communication Efficient Fair and Robust Federated Learning.](http://arxiv.org/abs/2307.13381) | Scaff-PD是一个高效通信、公平及鲁棒的分布式学习算法。它通过优化一系列针对异构客户端的分布鲁棒目标来提高公平性，利用特殊结构和加速的原始-对偶算法，在通信效率和收敛速度方面取得显著的提升。在多个基准数据集上的评估结果显示，Scaff-PD在提高公平性和鲁棒性方面有效，并同时保持竞争性的准确性。这使得Scaff-PD成为资源受限和异构环境下分布式学习的一种有前景的方法。 |
| [^8] | [Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation.](http://arxiv.org/abs/2307.13371) | 提出了一种名为BALLET的框架，用于在高维和非平稳场景下的贝叶斯优化。它使用两个概率模型，一个粗糙的高斯过程用于识别感兴趣的区域，一个局部高斯过程用于优化该区域。BALLET能够有效地缩小搜索空间，并且比标准的无感兴趣区域过滤的贝叶斯优化具有更紧的遗憾界限。 |
| [^9] | [Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations.](http://arxiv.org/abs/2307.13370) | 本文提出了一种用于计算双规则化Wasserstein重心的算法，并通过阻尼Sinkhorn迭代和精确的最大化/最小化步骤保证了收敛性。此算法的非精确变体使用近似的蒙特卡罗采样实现，在自由支撑/网格自由设置中提供了第一个非渐近收敛保证。 |
| [^10] | [The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation.](http://arxiv.org/abs/2307.13332) | 本文研究了在线性离策略值函数估计中的逼近因子，并在多种设置下建立了最优的渐近逼近因子，这些因子决定了离策略评估的困难程度。 |
| [^11] | [Modify Training Directions in Function Space to Reduce Generalization Error.](http://arxiv.org/abs/2307.13290) | 本文提出了在函数空间中修改训练方向的方法，通过在神经网络函数空间中进行特征分解和统计理论的理论分析，我们证明了这种方法可以降低总的泛化误差。 |
| [^12] | [Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework.](http://arxiv.org/abs/2307.13147) | 该论文研究了将路径相关的NJ-ODE方法扩展到具有噪声观测和相关观测框架的问题。研究提出了两种扩展方法，并提供了理论保证和实证示例。 |
| [^13] | [A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning.](http://arxiv.org/abs/2307.13127) | 本文提出了一种差分隐私加权经验风险最小化算法，可以在使用敏感数据的情况下保护隐私。这是第一个在权重ERM中应用差分隐私的算法，并且在一定的条件下提供了严格的DP保证。 |
| [^14] | [Conformal prediction for frequency-severity modeling.](http://arxiv.org/abs/2307.13124) | 这个论文提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，扩展了split conformal prediction技术到两阶段频率-严重性建模领域，并通过使用随机森林作为严重性模型，利用了袋外机制消除了校准集的需要，并实现了具有自适应宽度的预测区间的生成。 |
| [^15] | [Detection of Common Subtrees with Identical Label Distribution.](http://arxiv.org/abs/2307.13068) | 本文介绍了一种用于树状数据的新型模式——具有相同标签分布的常见子树的检测方法，并开发了一个复杂的搜索算法来解决同构问题，提出了一种新的树形无损压缩方案进行模式枚举，从理论和实践方面验证了该方法的优势。 |
| [^16] | [Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials.](http://arxiv.org/abs/2307.12840) | 通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。 |
| [^17] | [A Flexible Framework for Incorporating Patient Preferences Into Q-Learning.](http://arxiv.org/abs/2307.12022) | 这个论文提出了一种称为潜在效用Q学习的方法，能够将患者偏好纳入复合结果的动态治疗方案中，解决了传统方法对时间点和结果数量的限制，能够实现强大的性能。 |
| [^18] | [Long-Tail Theory under Gaussian Mixtures.](http://arxiv.org/abs/2307.10736) | 该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。 |
| [^19] | [Sharp Convergence Rates for Matching Pursuit.](http://arxiv.org/abs/2307.07679) | 本文通过提升现有的下界来匹配最佳上界，对匹配追踪的性能进行了精确描述，并构造了一个最坏情况的字典来证明现有上界的无法改进。 |
| [^20] | [A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models.](http://arxiv.org/abs/2307.05946) | 本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。 |
| [^21] | [Differentially Private Distributed Estimation and Learning.](http://arxiv.org/abs/2306.15865) | 本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。 |
| [^22] | [Can we falsify the justification of the validity of Wald confidence intervals of doubly robust functionals, without assumptions?.](http://arxiv.org/abs/2306.10590) | 本文提出无假设检验方法，可否定分析师对基于双重机器学习估计的Wald置信区间在广泛的双重稳健函数类中的有效性的证明。 |
| [^23] | [Dictionary Learning under Symmetries via Group Representations.](http://arxiv.org/abs/2305.19557) | 本文研究在预定变换群下学习不变的字典问题。利用非阿贝尔傅里叶分析，提供了算法，建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。 |
| [^24] | [Trajectory-oriented optimization of stochastic epidemiological models.](http://arxiv.org/abs/2305.03926) | 本论文提出了一个基于轨迹的优化方法来处理随机流行病学模型，可以找到与实际观测值接近的实际轨迹，而不是仅使平均模拟结果与实测数据相符。 |
| [^25] | [Stabilizing Transformer Training by Preventing Attention Entropy Collapse.](http://arxiv.org/abs/2303.06296) | 本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。 |
| [^26] | [Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence.](http://arxiv.org/abs/2302.08580) | 本文提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法，并采用混合近端外梯度法结构和在线学习框架来更新Hessian逼近矩阵。 |
| [^27] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^28] | [Multi-Armed Bandits and Quantum Channel Oracles.](http://arxiv.org/abs/2301.08544) | 本论文研究了量子算法在多臂赌博机问题中的应用，发现在可以查询奖励的随机性以及臂的叠加态时可以实现二次加速，但在只能有限地访问奖励的随机性时，查询复杂度与经典算法相同。 |
| [^29] | [Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing.](http://arxiv.org/abs/2207.11159) | 本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。 |
| [^30] | [Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump ODEs.](http://arxiv.org/abs/2206.14284) | 本文研究了使用路径相关的神经跳跃ODE对通用动力学进行最优估计的问题，并通过实证研究支持了这些理论结果，展示了其在非马尔可夫数据和限价订单簿数据方面的优势。 |
| [^31] | [Bayesian Non-stationary Linear Bandits for Large-Scale Recommender Systems.](http://arxiv.org/abs/2202.03167) | 本文提出了一种基于贝叶斯方法的非平稳线性赌臂算法，用于处理大规模推荐系统中的高维上下文信息。通过使用随机投影和指数增长的权重，该算法能够适应非平稳环境中的动态变化，并取得了较好的性能。 |
| [^32] | [Optimal Simple Regret in Bayesian Best Arm Identification.](http://arxiv.org/abs/2111.09885) | 该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。 |
| [^33] | [Stochastic Subgradient Descent Escapes Active Strict Saddles on Weakly Convex Functions.](http://arxiv.org/abs/2108.02072) | 本论文研究了在非光滑随机优化中，随机子梯度下降法（SGD）对活动严格鞍点的非收敛性。通过引入Verdier分层条件和角度条件，我们表明在弱凸函数类中，SGD通常收敛于局部极小值点。 |
| [^34] | [Constrained Classification and Policy Learning.](http://arxiv.org/abs/2106.12886) | 研究了受限分类和策略学习中替代损失程序的一致性和适用性。 |
| [^35] | [Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes.](http://arxiv.org/abs/2105.08232) | 本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。 |
| [^36] | [Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors.](http://arxiv.org/abs/2009.13370) | 本文使用复制方法估计了具有马尔科夫或隐马尔科夫信号先验的线性模型的自由能、平均互信息和最小均方误差（MMSE）。研究发现，在后验均值估计器下，线性模型可以分解为具有状态信息的单输入AWGN信道，而状态分布遵循马尔科夫链的随机矩阵的左Perron-Frobenius特征向量。数值结果证明，通过复制方法得到的结果与Metropolis-Hastings算法或其他近似传递算法的结果非常接近。 |
| [^37] | [Non-linear Neurons with Human-like Apical Dendrite Activations.](http://arxiv.org/abs/2003.03229) | 本论文提出了一种新的人工神经元模型和激活函数，通过使用单个神经元学习非线性决策边界，并在多个基准数据集上取得了优于传统方法的结果。 |
| [^38] | [Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds.](http://arxiv.org/abs/1905.10448) | 本论文在紧致黎曼流形上定义了一种几何散射变换，该变换类似于欧几里得散射变换，具有局部同构的不变性和某些类型的微分同胚的稳定性，实证结果证明了其在几何学习任务中的实用性。 |

# 详细

[^1]: RED CoMETS: 一种用于符号化表示的多变量时间序列的集成分类器

    RED CoMETS: An ensemble classifier for symbolically represented multivariate time series. (arXiv:2307.13679v1 [cs.LG])

    [http://arxiv.org/abs/2307.13679](http://arxiv.org/abs/2307.13679)

    本文介绍了一种名为RED CoMETS的集成分类器，用于处理符号化表示的多变量时间序列数据。它在多变量设置中展现出竞争力的准确性，并在'HandMovementDirection'数据集上实现了最高的报告准确性。

    

    多变量时间序列分类是一个快速发展的研究领域，可在金融、医疗、工程等实际应用中使用。多变量时间序列数据的分类复杂性来自于其高维度、时间依赖性和长度不一致性。本文介绍了一种名为RED CoMETS（Random Enhanced Co-eye for Multivariate Time Series）的新型集成分类器，它解决了这些挑战。RED CoMETS基于Co-eye的成功，并将其能力扩展到处理多变量数据。使用UCR档案中的基准数据集对RED CoMETS的性能进行评估，在多变量设置中与最先进的技术相比，它显示出竞争力的准确性。值得注意的是，它在文献中对于'HandMovementDirection'数据集实现了最高的报告准确性。此外，该方法显著地改进了传统的Co-eye方法。

    Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method signific
    
[^2]: AI与保险伦理：在风险建模中减少拟议差别的新解决方案

    AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling. (arXiv:2307.13616v1 [stat.ML])

    [http://arxiv.org/abs/2307.13616](http://arxiv.org/abs/2307.13616)

    该论文提出了一个新的解决方案，通过考虑变量之间的潜在相互作用，减少风险建模中的拟议差别，以实现更公平的保险定价和风险选择。

    

    机器学习的发展引起了广大公众的关注，在最近几年中，有许多新闻文章质疑其客观性：种族主义，性别歧视等。受监管机构对保险中数据道德使用的关注日益增长，保险精算师社区必须重新思考定价和风险选择实践，以实现更公平的保险。公平是一个哲学概念，在每个司法管辖区都有很多不同的定义，这些定义相互影响，目前尚未达成一致意见。在欧洲，基本权利宪章规定了有关歧视和算法中使用敏感个人数据的指导方针。如果简单地删除受保护变量可以防止任何所谓的“直接”歧视，那么模型仍然可以通过变量之间潜在的相互作用“间接”歧视个人，从而带来更好的性能（因此更好地量化风险，进行细分）。

    The development of Machine Learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots Driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. Equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. In Europe, the Charter of Fundamental Rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. If the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation 
    
[^3]: 算法和稀疏主成分分析的障碍是否适用于其他结构设置？

    Do algorithms and barriers for sparse principal component analysis extend to other structured settings?. (arXiv:2307.13535v1 [stat.ML])

    [http://arxiv.org/abs/2307.13535](http://arxiv.org/abs/2307.13535)

    该论文研究了在尖峰Wishart模型下，通过一类子空间并集模型捕捉信号结构的主成分分析问题。通过统计和计算的视角，我们建立了基本限制，并展示了自然的投影功率方法在解决方案的统计近似最优邻域中的局部收敛性。我们还通过具体案例的分析展示了计算难度。结果表明，对于基本稀疏PCA观察到的现象在其结构化对应物中也同样存在。

    

    在尖峰Wishart模型下，我们研究了一种主成分分析问题，其中信号中的结构通过一类子空间并集模型来捕捉。这个通用类别包括基本稀疏PCA以及带有图稀疏性的变体。为了在统计和计算的统一视角下研究这些问题，我们建立了与问题实例的几何有关的基本限制，并展示了自然的投影功率方法在解决方案的统计近似最优邻域中的局部收敛性。我们通过对普适基础中路径稀疏性和树稀疏性的两种重要特殊情况进行端到端分析，补充了这些结果，展示了初始化方法和相匹配的计算难度证据。总的来说，我们的结果表明，对于基本稀疏PCA观察到的几个现象自然地扩展到其结构化对应物中。

    We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
    
[^4]: 不规则时间序列的连续时间证据分布

    Continuous Time Evidential Distributions for Irregular Time Series. (arXiv:2307.13503v1 [cs.LG])

    [http://arxiv.org/abs/2307.13503](http://arxiv.org/abs/2307.13503)

    该论文提出了一种在连续时间中学习不规则时间序列的证据分布的策略，能够在任何感兴趣的时间上对部分观测到的特征进行良好校准和灵活的推断，并且在稀疏、不规则观测的时间上扩展不确定性。该方法在时间序列分类任务上表现出竞争性的性能，并能够在遇到噪音数据时实现基于不确定性的推断。

    

    在许多现实世界中的场景中，如医疗保健领域，不规则时间序列很难进行预测。当观测不连续时，在任何给定时间推断特征的值是困难的，因为它可能取决于最后一次观察的时间而具有一系列的值。为了描述这种不确定性，我们提出了EDICT，一种在连续时间中学习不规则时间序列的证据分布的策略。这个分布可以在任何感兴趣的时间上对部分观测到的特征进行良好校准和灵活的推断，同时在稀疏、不规则观测的时间上扩展不确定性。我们演示了EDICT在具有挑战性的时间序列分类任务上取得了竞争性的性能，并在遇到噪音数据时实现了基于不确定性的推断。

    Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
    
[^5]: 使用异构图神经网络寻找洗钱者

    Finding Money Launderers Using Heterogeneous Graph Neural Networks. (arXiv:2307.13499v1 [cs.LG])

    [http://arxiv.org/abs/2307.13499](http://arxiv.org/abs/2307.13499)

    本文介绍了一种使用异构图神经网络来寻找洗钱者的方法，该方法在真实的银行交易和商业角色数据构建的大型异构网络中识别洗钱活动。为了解决洗钱活动中犯罪分子的合作问题，我们扩展了同质图神经网络方法，提出了一种新颖的消息聚合方法。

    

    当前的反洗钱系统主要基于规则，存在明显的问题，难以高效和准确地检测洗钱活动。因此，近年来出现了对利用机器学习的替代方法的探索热潮。由于犯罪分子通常在洗钱活动中合作，因此考虑到不同类型的客户关系和链接变得至关重要。本文介绍了一种基于图神经网络（GNN）的方法，从挪威最大的银行DNB的真实银行交易和商业角色数据构建的大型异构网络中识别洗钱活动。具体而言，我们扩展了一种称为消息传递神经网络（MPNN）的同质GNN方法，以在异构图上有效运行。作为该方法的一部分，我们提出了一种新颖的方法，用于在图的不同边之间聚合消息。

    Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our finding
    
[^6]: 量子随机访问内存的基本因果界限

    Fundamental causal bounds of quantum random access memories. (arXiv:2307.13460v1 [quant-ph])

    [http://arxiv.org/abs/2307.13460](http://arxiv.org/abs/2307.13460)

    本研究通过采用相对论量子场论和量子多体系统中的Lieb-Robinson界限，批判性地探讨了基于因果性的快速量子存储器的内在界限。研究表明在混合量子声学系统中，QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。

    

    量子设备应遵守量子物理原则。量子随机访问内存（QRAM）是许多重要量子算法（如线性代数、数据搜索和机器学习）的基本组件，通常被认为在给定N个量子比特时，可以以O(log N)的电路深度处理O(N)的数据量。然而，当处理大量量子比特的相互作用局部的量子材料时，这一主张似乎违反了相对论原理。在我们的研究中，我们批判性地探讨了基于因果性的快速量子存储器的内在界限，利用相对论量子场论和Lieb-Robinson界限在量子多体系统中。在本文中，我们考虑了一个在混合量子声学系统中高效的硬件设计的QRAM。假设时钟周期约为10^{-3}秒，格子间距约为1微米，我们展示了QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。

    Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimens
    
[^7]: Scaff-PD:高效率通信、公平及鲁棒的分布式学习算法

    Scaff-PD: Communication Efficient Fair and Robust Federated Learning. (arXiv:2307.13381v1 [cs.LG])

    [http://arxiv.org/abs/2307.13381](http://arxiv.org/abs/2307.13381)

    Scaff-PD是一个高效通信、公平及鲁棒的分布式学习算法。它通过优化一系列针对异构客户端的分布鲁棒目标来提高公平性，利用特殊结构和加速的原始-对偶算法，在通信效率和收敛速度方面取得显著的提升。在多个基准数据集上的评估结果显示，Scaff-PD在提高公平性和鲁棒性方面有效，并同时保持竞争性的准确性。这使得Scaff-PD成为资源受限和异构环境下分布式学习的一种有前景的方法。

    

    我们提出了一种名为Scaff-PD的快速和高效通信的分布式学习算法。我们的方法通过优化一系列针对异构客户端的分布鲁棒目标来改善公平性。我们利用这些目标的特殊结构，并设计了一个加速的原始-对偶（APD）算法，该算法使用修正偏差的局部步骤（如Scaffold）以在通信效率和收敛速度方面取得显著的提升。我们在多个基准数据集上评估了Scaff-PD，并展示了其在提高公平性和鲁棒性方面的有效性，同时保持竞争性的准确性。我们的结果表明，Scaff-PD是在资源受限和异构环境中进行分布式学习的一种有前景的方法。

    We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
    
[^8]: 学习适应性水平集估计的贝叶斯优化感兴趣区域

    Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation. (arXiv:2307.13371v1 [cs.LG])

    [http://arxiv.org/abs/2307.13371](http://arxiv.org/abs/2307.13371)

    提出了一种名为BALLET的框架，用于在高维和非平稳场景下的贝叶斯优化。它使用两个概率模型，一个粗糙的高斯过程用于识别感兴趣的区域，一个局部高斯过程用于优化该区域。BALLET能够有效地缩小搜索空间，并且比标准的无感兴趣区域过滤的贝叶斯优化具有更紧的遗憾界限。

    

    我们研究了在高维和非平稳场景下的贝叶斯优化（BO）。现有的算法通常需要大量的超参数调整，限制了它们的实际有效性。我们提出了一个名为BALLET的框架，该框架以非参数概率模型（如高斯过程）的超级级集的高置信度感兴趣区域（ROI）为自适应过滤器。我们的方法易于调整，并且能够专注于可以通过现有的BO方法解决的优化空间的局部区域。关键思想是使用两个概率模型：一个粗糙的高斯过程用于识别ROI，一个局部高斯过程用于ROI内的优化。我们理论上证明了BALLET可以高效地缩小搜索空间，并且比标准的无ROI过滤的BO具有更紧的遗憾界限。我们在合成和真实世界的优化任务中经验性地证明了BALLET的有效性。

    We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
    
[^9]: 通过阻尼Sinkhorn迭代实现双熵Wasserstein重心的计算保证

    Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations. (arXiv:2307.13370v1 [math.OC])

    [http://arxiv.org/abs/2307.13370](http://arxiv.org/abs/2307.13370)

    本文提出了一种用于计算双规则化Wasserstein重心的算法，并通过阻尼Sinkhorn迭代和精确的最大化/最小化步骤保证了收敛性。此算法的非精确变体使用近似的蒙特卡罗采样实现，在自由支撑/网格自由设置中提供了第一个非渐近收敛保证。

    

    我们研究了双规则化Wasserstein重心的计算，这是一种最近引入的由内部和外部规则化强度控制的熵重心族。先前的研究表明，各种规则化参数选择统一了几个熵惩罚重心的概念，同时揭示了新的概念，包括偏差重心的特殊情况。在本文中，我们提出并分析了一种计算双规则化Wasserstein重心的算法。我们的过程基于阻尼Sinkhorn迭代，然后是精确的最大化/最小化步骤，对任何规则化参数的选择都保证收敛。我们的算法的非精确变体，可以使用近似的蒙特卡罗采样来实现，在自由支撑/网格自由设置中为近似Wasserstein重心之间的离散点云提供了第一个非渐近收敛保证。

    We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
    
[^10]: 在错误指定的离策略值函数估计中的最佳逼近因子

    The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])

    [http://arxiv.org/abs/2307.13332](http://arxiv.org/abs/2307.13332)

    本文研究了在线性离策略值函数估计中的逼近因子，并在多种设置下建立了最优的渐近逼近因子，这些因子决定了离策略评估的困难程度。

    

    已经知道，在强化学习中的理论保证在函数逼近的错误指定中会出现乘法放大因子。然而，这些\emph{逼近因子}的性质，特别是在给定的学习问题中的最佳形式，仍然不为人所了解。在本文中，我们研究了这个问题在线性离策略值函数估计中的广泛设置中的逼近因子，其中仍有许多开放问题。我们研究了在多种设置下的逼近因子，例如加权$L_2$范数（其中加权是离线状态分布），$L_\infty$范数，状态别名的存在与否以及对状态空间的全面与部分覆盖。对于所有这些设置，我们建立了最优的渐近逼近因子（至多常数）。特别地，我们的界限确定了$L_2(\mu)$范数的两个依赖于实例的因子和$L_\infty$范数的一个因子，它们被证明决定了离策略评估的困难程度。

    Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
    
[^11]: 在函数空间中修改训练方向以降低泛化误差

    Modify Training Directions in Function Space to Reduce Generalization Error. (arXiv:2307.13290v1 [stat.ML])

    [http://arxiv.org/abs/2307.13290](http://arxiv.org/abs/2307.13290)

    本文提出了在函数空间中修改训练方向的方法，通过在神经网络函数空间中进行特征分解和统计理论的理论分析，我们证明了这种方法可以降低总的泛化误差。

    

    我们提出了在神经网络函数空间中基于神经切换核和Fisher信息矩阵的特征分解的修改自然梯度下降法的理论分析。我们首先给出了在高斯分布和无限宽度极限的假设下，通过理论方法从特征分解和统计理论中显式推导出该修改自然梯度所学习的函数的表达式。因此，我们通过将总的泛化误差分解为函数空间中不同特征空间的误差，提出了一个平衡训练集误差和训练集与真实数据之间分布差异的准则。通过这种方法，我们建立了在函数空间中修改神经网络的训练方向会导致总的泛化误差的减少。

    We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demo
    
[^12]: 将路径相关的NJ-ODE扩展到有噪声的观测和相关观测框架

    Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework. (arXiv:2307.13147v1 [stat.ML])

    [http://arxiv.org/abs/2307.13147](http://arxiv.org/abs/2307.13147)

    该论文研究了将路径相关的NJ-ODE方法扩展到具有噪声观测和相关观测框架的问题。研究提出了两种扩展方法，并提供了理论保证和实证示例。

    

    路径相关的神经跳跃ODE (PD-NJ-ODE) 是一种用于预测具有不规则和不完整观测的连续时间随机过程的模型。具体而言，该方法通过学习给定不规则采样的不完整过去观测的最优预测。迄今为止，假设过程本身和坐标分别观测时间是独立的，并且假设观测是无噪声的。在这项工作中，我们讨论了两种扩展来解除这些限制，并提供了理论保证以及它们的实证示例。

    The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
    
[^13]: 一个差分隐私加权经验风险最小化算法及其在结果加权学习中的应用

    A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning. (arXiv:2307.13127v1 [stat.ML])

    [http://arxiv.org/abs/2307.13127](http://arxiv.org/abs/2307.13127)

    本文提出了一种差分隐私加权经验风险最小化算法，可以在使用敏感数据的情况下保护隐私。这是第一个在权重ERM中应用差分隐私的算法，并且在一定的条件下提供了严格的DP保证。

    

    在经验风险最小化(ERM)框架中，使用包含个人信息的数据来构建预测模型是常见的做法。尽管这些模型在预测上可以非常准确，但使用敏感数据得到的结果可能容易受到隐私攻击。差分隐私(DP)是一种有吸引力的框架，可以通过提供数学上可证明的隐私损失界限来解决这些数据隐私问题。先前的工作主要集中在将DP应用于无权重的ERM中。我们考虑到了权重ERM(wERM)的重要推广。在wERM中，可以为每个个体的目标函数贡献分配不同的权重。在这个背景下，我们提出了第一个有差分隐私保障的wERM算法，并在一定的正则条件下提供了严格的理论证明。将现有的DP-ERM程序扩展到wERM为结果加权学习铺平了道路。

    It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to derivin
    
[^14]: 频率-严重性建模的符合性预测

    Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])

    [http://arxiv.org/abs/2307.13124](http://arxiv.org/abs/2307.13124)

    这个论文提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，扩展了split conformal prediction技术到两阶段频率-严重性建模领域，并通过使用随机森林作为严重性模型，利用了袋外机制消除了校准集的需要，并实现了具有自适应宽度的预测区间的生成。

    

    我们提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，将分割符合性预测技术扩展到两阶段频率-严重性建模领域。通过模拟和真实数据集展示了该框架的有效性。当基础严重性模型是随机森林时，我们扩展了两阶段分割符合性预测过程，展示了如何利用袋外机制消除校准集的需要，并实现具有自适应宽度的预测区间的生成。

    We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
    
[^15]: 检测具有相同标签分布的常见子树

    Detection of Common Subtrees with Identical Label Distribution. (arXiv:2307.13068v1 [cs.DS])

    [http://arxiv.org/abs/2307.13068](http://arxiv.org/abs/2307.13068)

    本文介绍了一种用于树状数据的新型模式——具有相同标签分布的常见子树的检测方法，并开发了一个复杂的搜索算法来解决同构问题，提出了一种新的树形无损压缩方案进行模式枚举，从理论和实践方面验证了该方法的优势。

    

    频繁模式挖掘是一种分析结构化数据的相关方法，如序列，树或图。它的目的是识别数据集的特征子结构。本文针对树状数据的一种新类型的模式进行了探讨：具有相同标签分布的常见子树。由于基础同构问题是图同构完全问题，因此其检测远非明显。我们开发了一种精心设计的搜索算法，并从理论和数值角度进行了分析。基于此，通过一种名为DAG-RW的新的树形无损压缩方案来执行模式的枚举，其复杂性也得到了研究。该方法在计算时间和对文献中的实际数据集的分析方面都表现出非常好的性能。与其他子结构（如拓扑子树和标记子树）相比，其同构问题是线性的，所发现的模式提供了数据的更节约表示。

    Frequent pattern mining is a relevant method to analyse structured data, like sequences, trees or graphs. It consists in identifying characteristic substructures of a dataset. This paper deals with a new type of patterns for tree data: common subtrees with identical label distribution. Their detection is far from obvious since the underlying isomorphism problem is graph isomorphism complete. An elaborated search algorithm is developed and analysed from both theoretical and numerical perspectives. Based on this, the enumeration of patterns is performed through a new lossless compression scheme for trees, called DAG-RW, whose complexity is investigated as well. The method shows very good properties, both in terms of computation times and analysis of real datasets from the literature. Compared to other substructures like topological subtrees and labelled subtrees for which the isomorphism problem is linear, the patterns found provide a more parsimonious representation of the data.
    
[^16]: 通过舒尔多项式高效学习具有一个隐藏层的ReLU网络

    Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials. (arXiv:2307.12840v1 [cs.LG])

    [http://arxiv.org/abs/2307.12840](http://arxiv.org/abs/2307.12840)

    通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。

    

    我们研究了在标准高斯分布下，关于平方损失的PAC学习$k$个ReLU激活的线性组合的问题。我们的主要结果是针对这个学习任务的一种高效算法，其样本和计算复杂性为$(dk/\epsilon)^{O(k)}$，其中$\epsilon>0$是目标精度。之前的工作给出了一个复杂性为$(dk/\epsilon)^{h(k)}$的算法，其中函数$h(k)$在$k$上的规模是超多项式的。有趣的是，我们的算法在相关统计查询算法类中接近最优。总体而言，我们的算法使用张量分解来识别一个子空间，使得所有$O(k)$阶矩在正交方向上都很小。其分析基于舒尔多项式理论，以显示较低阶误差张量的情况下，更高阶的误差张量也很小。

    We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
    
[^17]: 将患者偏好纳入Q学习的灵活框架

    A Flexible Framework for Incorporating Patient Preferences Into Q-Learning. (arXiv:2307.12022v1 [cs.LG])

    [http://arxiv.org/abs/2307.12022](http://arxiv.org/abs/2307.12022)

    这个论文提出了一种称为潜在效用Q学习的方法，能够将患者偏好纳入复合结果的动态治疗方案中，解决了传统方法对时间点和结果数量的限制，能够实现强大的性能。

    

    在现实世界的医疗问题中，通常存在多个竞争性的关注点，如治疗疗效和副作用严重程度。然而，用于估计动态治疗方案 (DTRs) 的统计方法通常假设只有一个关注点，而处理复合结果的方法很少，存在重要限制，包括对单个时间点和两个结果的限制、无法纳入患者的自述偏好以及有限的理论保证。为此，我们提出了一个新的方法来解决这些限制，我们称之为潜在效用Q学习(LUQ-Learning)。LUQ-Learning采用潜在模型方法，自然地将Q学习扩展到复合结果设置，并为每个患者选择理想的结果权衡。与之前的方法不同，我们的框架允许任意数量的时间点和结果，纳入陈述的偏好，并实现强大的渐近性能。

    In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with rea
    
[^18]: 高斯混合下的长尾理论

    Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])

    [http://arxiv.org/abs/2307.10736](http://arxiv.org/abs/2307.10736)

    该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。

    

    我们提出了一个简单的高斯混合模型来生成遵循Feldman的长尾理论（2020）的数据。我们证明，在提出的模型中，线性分类器无法将泛化误差降低到一定水平以下，而具有记忆能力的非线性分类器可以。这证实了对于长尾分布，必须考虑罕见的训练样本以实现对新数据的最佳泛化。最后，我们通过在合成和真实数据上的实验证明，当子群体频率分布的尾部变短时，线性模型和非线性模型之间的性能差距可以减小。

    We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
    
[^19]: 匹配追踪的快速收敛速度

    Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])

    [http://arxiv.org/abs/2307.07679](http://arxiv.org/abs/2307.07679)

    本文通过提升现有的下界来匹配最佳上界，对匹配追踪的性能进行了精确描述，并构造了一个最坏情况的字典来证明现有上界的无法改进。

    

    本文研究了匹配追踪的基本限制，即通过字典中的元素的稀疏线性组合来近似目标函数的纯贪婪算法。当目标函数包含在对应于字典的变化空间中时，许多令人印象深刻的研究在过去几十年中获得了匹配追踪的收敛速度的上界和下界，但它们并不匹配。本文的主要贡献是填补这一差距，并获得匹配追踪性能的精确描述。我们通过改进现有的下界以匹配最佳上界来实现这一目标。具体来说，我们构造了一个最坏情况的字典，证明了现有的上界不能改进。事实证明，与其他贪婪算法变体不同，收敛速度是次优的，并且由解某个非线性方程的解决方案决定。这使我们得出结论，任意程度的收缩都会改善匹配追踪效果。

    We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
    
[^20]: 一种贝叶斯方法用于量化交通预测模型中的不确定性和改善泛化能力

    A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])

    [http://arxiv.org/abs/2307.05946](http://arxiv.org/abs/2307.05946)

    本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。

    

    交通数据预测的深度学习模型可以通过多层架构对复杂函数进行优化建模，但这些方法的一个主要缺点是大多数方法不提供带有不确定性估计的预测结果，而这对于交通运营和控制是必需的。本研究提出了一种贝叶斯循环神经网络框架，通过引入谱归一化到其隐藏层，实现交通预测中的不确定性量化和更高的泛化能力。我们的论文表明，归一化通过控制模型的复杂性并减少对训练数据的过度拟合风险，改善了深度神经网络的泛化性能。

    Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
    
[^21]: 差分隐私分布式估计和学习

    Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])

    [http://arxiv.org/abs/2306.15865](http://arxiv.org/abs/2306.15865)

    本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。

    

    我们研究了在网络环境中的分布式估计和学习问题，其中代理通过交换信息来估计从其私下观察的样本中未知的统计属性。通过交换私有观测信息，代理可以集体估计未知数量，但他们也面临隐私风险。我们的聚合方案的目标是在时间和网络中高效地组合观测数据，同时满足代理的隐私需求，而不需要任何超越他们本地附近的协调。我们的算法使参与的代理能够从离线或随时间在线获取的私有信号中估计完整的充分统计量，并保护其信号和网络附近的隐私。这是通过线性聚合方案和调整的随机化方案实现的，将噪声添加到交换的估计数据中以满足差分隐私（DP）。

    We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
    
[^22]: 我们能否在不做任何假设的情况下，证伪Wald置信区间在双重稳健函数下的有效性？

    Can we falsify the justification of the validity of Wald confidence intervals of doubly robust functionals, without assumptions?. (arXiv:2306.10590v1 [stat.ME])

    [http://arxiv.org/abs/2306.10590](http://arxiv.org/abs/2306.10590)

    本文提出无假设检验方法，可否定分析师对基于双重机器学习估计的Wald置信区间在广泛的双重稳健函数类中的有效性的证明。

    

    本文提出了一种可行的版本的无假设检验方法，可否定分析师对报道的以双重机器学习(DML)估计量为中心的名义$(1-\alpha)$Wald置信区间的有效性的证明，对Rotnitzky等人所研究的双重稳健(DR)函数类的任何成员进行检验。DR函数类在经济学和生物统计学中具有广泛和核心的重要性。它严格包括两个类别，即(i)可以被写成条件期望的仿射函数期望的均方连续函数的类别，这是由Chernozhukov等人研究的，以及Robins等人所研究的类别。目前DR函数的最先进的估计值是DML估计值。$\hat{\psi}_{1}$的偏差取决于两个辅助函数$b$和$p$的估计率的乘积。最常见的是，分析师证明了

    In this article we develop a feasible version of the assumption-lean tests in Liu et al. 20 that can falsify an analyst's justification for the validity of a reported nominal $(1 - \alpha)$ Wald confidence interval (CI) centered at a double machine learning (DML) estimator for any member of the class of doubly robust (DR) functionals studied by Rotnitzky et al. 21. The class of DR functionals is broad and of central importance in economics and biostatistics. It strictly includes both (i) the class of mean-square continuous functionals that can be written as an expectation of an affine functional of a conditional expectation studied by Chernozhukov et al. 22 and the class of functionals studied by Robins et al. 08. The present state-of-the-art estimators for DR functionals $\psi$ are DML estimators $\hat{\psi}_{1}$. The bias of $\hat{\psi}_{1}$ depends on the product of the rates at which two nuisance functions $b$ and $p$ are estimated. Most commonly an analyst justifies the validity o
    
[^23]: 通过群表示学习对称下的字典学习

    Dictionary Learning under Symmetries via Group Representations. (arXiv:2305.19557v1 [math.OC])

    [http://arxiv.org/abs/2305.19557](http://arxiv.org/abs/2305.19557)

    本文研究在预定变换群下学习不变的字典问题。利用非阿贝尔傅里叶分析，提供了算法，建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。

    

    字典学习问题可以被看作是一个数据驱动的过程，旨在学习一个合适的变换，以便通过示例数据直接表示数据的稀疏性。本文研究了在预定的变换群下学习不变的字典问题。自然的应用领域包括冷冻电镜、多目标跟踪、同步和姿态估计等。我们特别从数学表示理论的角度研究了这个问题。通过利用非阿贝尔傅里叶分析，我们为符合这些不变性的字典学习提供了算法。我们将自然界中的字典学习问题，其自然被建模为无限维度的问题，与相关的计算问题，这必然是有限维度的问题，联系起来。我们建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。

    The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix o
    
[^24]: 基于轨迹的随机流行病学模型优化

    Trajectory-oriented optimization of stochastic epidemiological models. (arXiv:2305.03926v1 [stat.AP])

    [http://arxiv.org/abs/2305.03926](http://arxiv.org/abs/2305.03926)

    本论文提出了一个基于轨迹的优化方法来处理随机流行病学模型，可以找到与实际观测值接近的实际轨迹，而不是仅使平均模拟结果与实测数据相符。

    

    针对随机模型，为了进行预测和运行模拟，需要进行地面实测标定。由于输出结果通常是通过集成或分布来描述，因此需要对每个成员进行标定。本文提出了一种基于高斯过程代理和Thompson采样的优化策略来寻找与事实相一致的输入参数设置和随机数种子，该Trajectory Oriented Optimization（TOO）方法可以产生与实际观测值接近的实际轨迹，而不是仅虽然模拟的平均行为与事实相符。

    Epidemiological models must be calibrated to ground truth for downstream tasks such as producing forward projections or running what-if scenarios. The meaning of calibration changes in case of a stochastic model since output from such a model is generally described via an ensemble or a distribution. Each member of the ensemble is usually mapped to a random number seed (explicitly or implicitly). With the goal of finding not only the input parameter settings but also the random seeds that are consistent with the ground truth, we propose a class of Gaussian process (GP) surrogates along with an optimization strategy based on Thompson sampling. This Trajectory Oriented Optimization (TOO) approach produces actual trajectories close to the empirical observations instead of a set of parameter settings where only the mean simulation behavior matches with the ground truth.
    
[^25]: 防止注意力熵崩溃的Transformer训练稳定性研究

    Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])

    [http://arxiv.org/abs/2303.06296](http://arxiv.org/abs/2303.06296)

    本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。

    This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.

    训练稳定性对于Transformer至关重要。本文通过研究注意力层的演变来探究Transformer的训练动态。特别地，我们在训练过程中跟踪每个注意力头的注意力熵，这是模型锐度的代理。我们发现，在不同的架构和任务中存在一种常见模式，即低注意力熵伴随着高训练不稳定性，这可能采取振荡损失或发散的形式。我们将病态低注意力熵，对应高度集中的注意力分数，称为$\textit{熵崩溃}$。作为一种解决方案，我们提出了$\sigma$Reparam，一种简单而有效的解决方案，其中我们使用谱归一化和额外的学习标量重新参数化所有线性层。我们证明了所提出的重新参数化成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。此外，我们

    Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
    
[^26]: 在线学习引导的曲率逼近：具有全局非渐近超线性收敛的拟牛顿法。

    Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence. (arXiv:2302.08580v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.08580](http://arxiv.org/abs/2302.08580)

    本文提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法，并采用混合近端外梯度法结构和在线学习框架来更新Hessian逼近矩阵。

    

    拟牛顿算法是解决无约束最小化问题中最受欢迎的迭代方法之一，这主要归功于其良好的超线性收敛性质。然而，现有的算法结果存在限制，要么提供了具有渐近超线性收敛速度的全局收敛保证，要么仅在初始点和初始Hessian逼近选择适当的情况下提供了局部非渐近超线性速率。特别地，目前没有拟牛顿方法的分析保证了具有明确超线性收敛速率的全局收敛性。在本文中，我们填补了这一空白，并提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法。与传统的拟牛顿方法不同，我们基于混合近端外梯度法构建了我们的算法，并提出了一种新颖的在线学习框架来更新Hessian逼近矩阵。

    Quasi-Newton algorithms are among the most popular iterative methods for solving unconstrained minimization problems, largely due to their favorable superlinear convergence property. However, existing results for these algorithms are limited as they provide either (i) a global convergence guarantee with an asymptotic superlinear convergence rate, or (ii) a local non-asymptotic superlinear rate for the case that the initial point and the initial Hessian approximation are chosen properly. In particular, no current analysis for quasi-Newton methods guarantees global convergence with an explicit superlinear convergence rate. In this paper, we close this gap and present the first globally convergent quasi-Newton method with an explicit non-asymptotic superlinear convergence rate. Unlike classical quasi-Newton methods, we build our algorithm upon the hybrid proximal extragradient method and propose a novel online learning framework for updating the Hessian approximation matrices. Specificall
    
[^27]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^28]: 多臂赌博机和量子通道预测

    Multi-Armed Bandits and Quantum Channel Oracles. (arXiv:2301.08544v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.08544](http://arxiv.org/abs/2301.08544)

    本论文研究了量子算法在多臂赌博机问题中的应用，发现在可以查询奖励的随机性以及臂的叠加态时可以实现二次加速，但在只能有限地访问奖励的随机性时，查询复杂度与经典算法相同。

    

    多臂赌博机是强化学习理论的重要支柱之一。最近，人们开始研究用于多臂赌博机问题的量子算法，并发现当可以在叠加态中查询臂和奖励随机性时，可以实现二次加速（在查询复杂度上）。在这里，我们引入了进一步的赌博机模型，其中我们只能有限地访问奖励的随机性，但我们仍然可以在叠加态中查询臂。我们证明当如此时查询复杂度与经典算法相同。这推广了先前的结果，即当预测器具有正的失效概率时，对于未结构化搜索无法实现加速。

    Multi-armed bandits are one of the theoretical pillars of reinforcement learning. Recently, the investigation of quantum algorithms for multi-armed bandit problems was started, and it was found that a quadratic speed-up (in query complexity) is possible when the arms and the randomness of the rewards of the arms can be queried in superposition. Here we introduce further bandit models where we only have limited access to the randomness of the rewards, but we can still query the arms in superposition. We show that then the query complexity is the same as for classical algorithms. This generalizes the prior result that no speed-up is possible for unstructured search when the oracle has positive failure probability.
    
[^29]: 基于需求学习和公平资源消耗平衡的网络收益管理

    Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing. (arXiv:2207.11159v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11159](http://arxiv.org/abs/2207.11159)

    本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。

    

    除了最大化总收益外，很多行业的决策者还希望确保不同资源之间消耗的平衡。例如，在零售行业中，确保来自不同供应商的资源平衡消耗有助于提高公平性并维持良好的渠道关系；在云计算行业中，资源消耗的平衡有助于提高客户满意度并降低运营成本。针对这些实际需求，本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。我们引入了规范化收益的概念，即通过平衡正则化将公平的资源消耗平衡纳入到收益最大化的目标中。我们提出了一种基于上置信界限（UCB）需求学习方法的原始对偶在线策略来最大化规范化收益。我们采用了几个创新方法来应对需求学习和资源消耗平衡的挑战。

    In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a good channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innov
    
[^30]: 使用路径相关的神经跳跃ODE对通用动力学进行最优估计

    Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump ODEs. (arXiv:2206.14284v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.14284](http://arxiv.org/abs/2206.14284)

    本文研究了使用路径相关的神经跳跃ODE对通用动力学进行最优估计的问题，并通过实证研究支持了这些理论结果，展示了其在非马尔可夫数据和限价订单簿数据方面的优势。

    

    本文研究了使用神经跳跃ODE（NJ-ODE）框架的路径相关扩展来预测一般随机过程的问题。虽然NJ-ODE是第一个建立起针对不规则观测时间序列预测的收敛性保证的框架，但这些结果仅适用于来自具有完整观测的It\^o扩散的数据，特别是所有坐标同时观测到的马尔可夫过程。在本研究中，我们通过利用签名变换的重构性质将这些结果推广到通用的、可能是非马尔可夫或不连续的随机过程，并通过实证研究支持了这些理论结果，在非马尔可夫数据的情况下，路径相关的NJ-ODE优于原始NJ-ODE框架。此外，我们还展示了PD-NJ-ODE可以成功应用于经典的随机滤波问题和限价订单簿（LOB）数据。

    This paper studies the problem of forecasting general stochastic processes using a path-dependent extension of the Neural Jump ODE (NJ-ODE) framework. While NJ-ODE was the first framework to establish convergence guarantees for the prediction of irregularly observed time series, these results were limited to data stemming from It\^o-diffusions with complete observations, in particular Markov processes where all coordinates are observed simultaneously. In this work, we generalise these results to generic, possibly non-Markovian or discontinuous, stochastic processes with incomplete observations, by utilising the reconstruction properties of the signature transform. These theoretical results are supported by empirical studies, where it is shown that the path-dependent NJ-ODE outperforms the original NJ-ODE framework in the case of non-Markovian data. Moreover, we show that PD-NJ-ODE can be applied successfully to classical stochastic filtering problems and to limit order book (LOB) data.
    
[^31]: 大规模推荐系统中的贝叶斯非平稳线性赌臂

    Bayesian Non-stationary Linear Bandits for Large-Scale Recommender Systems. (arXiv:2202.03167v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03167](http://arxiv.org/abs/2202.03167)

    本文提出了一种基于贝叶斯方法的非平稳线性赌臂算法，用于处理大规模推荐系统中的高维上下文信息。通过使用随机投影和指数增长的权重，该算法能够适应非平稳环境中的动态变化，并取得了较好的性能。

    

    充分利用上下文信息可能会提高推荐系统的性能。在大数据时代，这种附加信息通常具有多个维度。因此，开发能够实时处理这种高维上下文的决策算法非常重要。当决策者需要推荐多种物品时，这具有特殊的挑战性。此外，物品的流行度或用户的偏好变化可能会由于环境中分布变化的鲁棒性不足而影响已部署推荐系统的性能。在本文中，我们在线性上下文多臂赌博机框架的基础上解决了这个问题。我们针对高维特征向量、大量臂和非平稳生成奖励的问题，开发了一种基于汤普森抽样的决策策略。我们的策略通过随机投影减少了特征向量的维度，并使用了指数增长的权重来适应非平稳性。

    Taking advantage of contextual information can potentially boost the performance of recommender systems. In the era of big data, such side information often has several dimensions. Thus, developing decision-making algorithms to cope with such a high-dimensional context in real time is essential. That is specifically challenging when the decision-maker has a variety of items to recommend. In addition, changes in items' popularity or users' preferences can hinder the performance of the deployed recommender system due to a lack of robustness to distribution shifts in the environment. In this paper, we build upon the linear contextual multi-armed bandit framework to address this problem. We develop a decision-making policy for a linear bandit problem with high-dimensional feature vectors, a large set of arms, and non-stationary reward-generating processes. Our Thompson sampling-based policy reduces the dimension of feature vectors using random projection and uses exponentially increasing w
    
[^32]: 贝叶斯最优臂识别中的最优简单遗憾

    Optimal Simple Regret in Bayesian Best Arm Identification. (arXiv:2111.09885v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09885](http://arxiv.org/abs/2111.09885)

    该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。

    

    我们考虑多臂赌博机问题中的最优臂识别。在先验条件具有一定的连续性的情况下，我们表征了贝叶斯简单遗憾的速率。与贝叶斯遗憾最小化不同，贝叶斯简单遗憾的主导项来源于最优臂和次优臂之间间隙小于$\sqrt{\frac{\log T}{T}}$的区域。我们提出了一种简单易行的计算算法，其主导项匹配了下界，只差一个常数因子；模拟结果支持了我们的理论发现。

    We consider best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading term in the Bayesian simple regret derives from the region where the gap between optimal and suboptimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings.
    
[^33]: 随机子梯度下降逃离弱凸函数的活动严格鞍点

    Stochastic Subgradient Descent Escapes Active Strict Saddles on Weakly Convex Functions. (arXiv:2108.02072v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2108.02072](http://arxiv.org/abs/2108.02072)

    本论文研究了在非光滑随机优化中，随机子梯度下降法（SGD）对活动严格鞍点的非收敛性。通过引入Verdier分层条件和角度条件，我们表明在弱凸函数类中，SGD通常收敛于局部极小值点。

    

    在非光滑随机优化中，我们证明了随机子梯度下降法（SGD）对最近由Davis和Drusvyatskiy称为活动严格鞍点的收敛性失败。这些点位于函数$f$具有二阶负曲率方向的流形$M$上。在这个流形之外，$f$的Clarke亚微分的范数有下界。我们对$f$有两个条件。第一个假设是一个Verdier分层条件，它是Whitney分层的一种细化。它使我们能够建立Bolte等人关于Whitney分层函数的投影公式的增强版本，这对独立的兴趣。第二个假设，称为角度条件，可以控制迭代点到$M$的距离。当$f$是弱凸的时候，我们的假设是泛化的。因此，在可定义的弱凸函数类中，SGD通常收敛于局部极小值点。

    In non-smooth stochastic optimization, we establish the non-convergence of the stochastic subgradient descent (SGD) to the critical points recently called active strict saddles by Davis and Drusvyatskiy. Such points lie on a manifold $M$ where the function $f$ has a direction of second-order negative curvature. Off this manifold, the norm of the Clarke subdifferential of $f$ is lower-bounded. We require two conditions on $f$. The first assumption is a Verdier stratification condition, which is a refinement of the popular Whitney stratification. It allows us to establish a reinforced version of the projection formula of Bolte \emph{et.al.} for Whitney stratifiable functions, and which is of independent interest. The second assumption, termed the angle condition, allows to control the distance of the iterates to $M$. When $f$ is weakly convex, our assumptions are generic. Consequently, generically in the class of definable weakly convex functions, the SGD converges to a local minimizer.
    
[^34]: 受限分类和策略学习

    Constrained Classification and Policy Learning. (arXiv:2106.12886v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2106.12886](http://arxiv.org/abs/2106.12886)

    研究了受限分类和策略学习中替代损失程序的一致性和适用性。

    

    现代机器学习方法对于分类问题使用了一些替代损失技术，如AdaBoost、支持向量机和深度神经网络，以绕过最小化经验分类风险的计算复杂性。这些技术在因果策略学习问题中也很有用，因为个性化治疗规则的估计可以被视为一种加权（成本敏感）分类问题。Zhang（2004年）和Bartlett等人（2006年）研究的替代损失方法的一致性关键依赖于正确规范的假设，即指定的分类器集合足够丰富，包含一个最佳分类器。然而，当分类器集合受到可解释性或公平性的限制时，这个假设较不可靠，这导致在这种次佳情景下替代损失方法的适用性未知。本文研究了在受限类集合条件下的替代损失程序的一致性。

    Modern machine learning approaches to classification, including AdaBoost, support vector machines, and deep neural networks, utilize surrogate loss techniques to circumvent the computational complexity of minimizing empirical classification risk. These techniques are also useful for causal policy learning problems, since estimation of individualized treatment rules can be cast as a weighted (cost-sensitive) classification problem. Consistency of the surrogate loss approaches studied in Zhang (2004) and Bartlett et al. (2006) crucially relies on the assumption of correct specification, meaning that the specified set of classifiers is rich enough to contain a first-best classifier. This assumption is, however, less credible when the set of classifiers is constrained by interpretability or fairness, leaving the applicability of surrogate loss based algorithms unknown in such second-best scenarios. This paper studies consistency of surrogate loss procedures under a constrained set of class
    
[^35]: 噪声低秩矩阵恢复的几何分析在精确参数化和过度参数化区间中的应用

    Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes. (arXiv:2105.08232v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.08232](http://arxiv.org/abs/2105.08232)

    本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。

    

    矩阵感知问题是一种重要的低秩优化问题，在矩阵补全、相位同步/恢复、稳健PCA和电力系统状态估计等领域都有广泛应用。本文研究了通过线性测量损坏的噪声低秩矩阵感知问题。我们考虑了搜索秩r等于未知真实秩r*的情况（精确参数化情况），以及r大于r*的情况（过度参数化情况）。我们量化了约束等异性性质（restricted isometry property，RIP）在塑造非凸分解公式的整体景观和帮助局部搜索算法成功方面的作用。首先，我们在RIP常数小于 1/(1+sqrt(r*/r))的假设下，对非凸问题的任意局部极小值和真实值之间的最大距离进行了全局保证。然后，我们提出了一种新颖的方法，称为鲁棒错误定位几何分析（Robust Error-Locating Geometric Analysis，RELGA）算法，用于实现在存在噪声的情况下的精确低秩矩阵恢复。RELGA算法通过组合错误定位机制和几何分析，提供了理论保证，即使在噪声水平相对较大的情况下，也可以实现精确的矩阵恢复。对于过度参数化情况，我们提出了一种局部搜索算法，称为连续子空间优化（Successive Subspace Optimization，SSO）算法，在噪声水平和RIP常数的一定条件下，可以收敛到真实解。我们的分析揭示了SSO的成功取决于初始化、非退化性和几何条件的组合。

    The matrix sensing problem is an important low-rank optimization problem that has found a wide range of applications, such as matrix completion, phase synchornization/retrieval, robust PCA, and power system state estimation. In this work, we focus on the general matrix sensing problem with linear measurements that are corrupted by random noise. We investigate the scenario where the search rank $r$ is equal to the true rank $r^*$ of the unknown ground truth (the exact parametrized case), as well as the scenario where $r$ is greater than $r^*$ (the overparametrized case). We quantify the role of the restricted isometry property (RIP) in shaping the landscape of the non-convex factorized formulation and assisting with the success of local search algorithms. First, we develop a global guarantee on the maximum distance between an arbitrary local minimizer of the non-convex problem and the ground truth under the assumption that the RIP constant is smaller than $1/(1+\sqrt{r^*/r})$. We then p
    
[^36]: 具有马尔科夫或隐马尔科夫信号先验的线性模型的复制分析

    Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v5 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2009.13370](http://arxiv.org/abs/2009.13370)

    本文使用复制方法估计了具有马尔科夫或隐马尔科夫信号先验的线性模型的自由能、平均互信息和最小均方误差（MMSE）。研究发现，在后验均值估计器下，线性模型可以分解为具有状态信息的单输入AWGN信道，而状态分布遵循马尔科夫链的随机矩阵的左Perron-Frobenius特征向量。数值结果证明，通过复制方法得到的结果与Metropolis-Hastings算法或其他近似传递算法的结果非常接近。

    

    本文基于统计物理中的复制方法，估计了在线性模型下的自由能、平均互信息和最小均方误差（MMSE），并假设了两个条件：（1）源由马尔科夫链生成，（2）源通过隐藏马尔科夫模型生成。我们的估计是基于后验均值估计器，表明具有马尔科夫源或隐藏马尔科夫源的线性模型可以分解为具有状态信息的单输入AWGN信道，其中状态分布遵循马尔科夫链的随机矩阵的左Perron-Frobenius特征向量，其曼哈顿范数为1。数值结果表明，通过复制方法获得的自由能和均方误差与研究文献中Metropolis-Hastings算法或一些众所周知的近似传递算法的对应结果非常接近。

    This paper estimates free energy, average mutual information, and minimum mean square error (MMSE) of a linear model under two assumptions: (1) the source is generated by a Markov chain, (2) the source is generated via a hidden Markov model. Our estimates are based on the replica method in statistical physics. We show that under the posterior mean estimator, the linear model with Markov sources or hidden Markov sources is decoupled into single-input AWGN channels with state information available at both encoder and decoder where the state distribution follows the left Perron-Frobenius eigenvector with unit Manhattan norm of the stochastic matrix of Markov chains. Numerical results show that the free energies and MSEs obtained via the replica method are closely approximate to their counterparts achieved by the Metropolis-Hastings algorithm or some well-known approximate message passing algorithms in the research literature.
    
[^37]: 具有类人类树突激活的非线性神经元

    Non-linear Neurons with Human-like Apical Dendrite Activations. (arXiv:2003.03229v4 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2003.03229](http://arxiv.org/abs/2003.03229)

    本论文提出了一种新的人工神经元模型和激活函数，通过使用单个神经元学习非线性决策边界，并在多个基准数据集上取得了优于传统方法的结果。

    

    为了对线性不可分的数据进行分类，通常将神经元组织成至少包含一个隐藏层的多层神经网络。受神经科学的一些最新发现的启发，我们提出了一种新的人工神经元模型和一种新颖的激活函数，可使用单个神经元学习非线性决策边界。我们展示了一个标准神经元接上我们的新型树突激活函数（ADA）可以以100%的准确率学习XOR逻辑函数。此外，我们对计算机视觉、信号处理和自然语言处理领域的六个基准数据集进行了实验，即MOROCO、UTKFace、CREMA-D、Fashion-MNIST、Tiny ImageNet和ImageNet，结果显示ADA和漏电ADA函数在各种神经网络结构（如一层或两层隐藏层的多层感知机和卷积神经网络）上优于修正线性单元（ReLU）、漏电ReLU、径向基函数（RBF）和Swish。

    In order to classify linearly non-separable data, neurons are typically organized into multi-layer neural networks that are equipped with at least one hidden layer. Inspired by some recent discoveries in neuroscience, we propose a new model of artificial neuron along with a novel activation function enabling the learning of nonlinear decision boundaries using a single neuron. We show that a standard neuron followed by our novel apical dendrite activation (ADA) can learn the XOR logical function with 100% accuracy. Furthermore, we conduct experiments on six benchmark data sets from computer vision, signal processing and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST, Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and Swish, for various neural network architectures, e.g. one-hidden-layer or two-hidden-layer multi-layer perceptrons (MLPs) and convolutional ne
    
[^38]: 在紧致黎曼流形上的几何小波散射网络

    Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds. (arXiv:1905.10448v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1905.10448](http://arxiv.org/abs/1905.10448)

    本论文在紧致黎曼流形上定义了一种几何散射变换，该变换类似于欧几里得散射变换，具有局部同构的不变性和某些类型的微分同胚的稳定性，实证结果证明了其在几何学习任务中的实用性。

    

    近十年前，欧几里得散射变换被引入以改善对卷积神经网络的数学理解。受到最近对几何深度学习的兴趣的启发，该学科旨在将卷积神经网络推广到流形和图结构域，我们在流形上定义了一种几何散射变换。类似于欧几里得散射变换，几何散射变换基于一系列小波滤波器和逐点非线性。它对局部同构具有不变性，对某些类型的微分同胚具有稳定性。实证结果证明了它在几何学习任务中的实用性。我们的结果推广了欧几里得散射的变形稳定性和局部平移不变性，并展示了将使用的滤波器结构与数据的基础几何联系起来的重要性。

    The Euclidean scattering transform was introduced nearly a decade ago to improve the mathematical understanding of convolutional neural networks. Inspired by recent interest in geometric deep learning, which aims to generalize convolutional neural networks to manifold and graph-structured domains, we define a geometric scattering transform on manifolds. Similar to the Euclidean scattering transform, the geometric scattering transform is based on a cascade of wavelet filters and pointwise nonlinearities. It is invariant to local isometries and stable to certain types of diffeomorphisms. Empirical results demonstrate its utility on several geometric learning tasks. Our results generalize the deformation stability and local translation invariance of Euclidean scattering, and demonstrate the importance of linking the used filter structures to the underlying geometry of the data.
    

