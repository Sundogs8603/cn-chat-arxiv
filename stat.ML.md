# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks.](http://arxiv.org/abs/2304.14994) | 本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。 |
| [^2] | [Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards.](http://arxiv.org/abs/2304.14989) | 本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。 |
| [^3] | [PAM: Plaid Atoms Model for Bayesian Nonparametric Analysis of Grouped Data.](http://arxiv.org/abs/2304.14954) | PAM模型是基于贝叶斯非参数的分组数据分析的新方法，它通过使用零增强贝塔（ZAB）分布实现了多组间的原子共享和独特性，并且在实际应用中表现出良好的性能。 |
| [^4] | [On the 1-Wasserstein Distance between Location-Scale Distributions and the Effect of Differential Privacy.](http://arxiv.org/abs/2304.14869) | 研究了独立的位置-比例分布的1-Wasserstein距离的计算公式，提供了一个新的线性上界和高斯情况下的渐近界限并研究了差分隐私的影响。 |
| [^5] | [Hyperparameter Optimization through Neural Network Partitioning.](http://arxiv.org/abs/2304.14766) | 本文提出了一种将训练数据和神经网络模型分区的方法，将每个分区与特定的数据片段关联并进行优化，通过优化这些分区的子网络的“训练之外的样本”损失，实现了在单次训练运行中降低超参数优化代价的效果。 |
| [^6] | [Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy.](http://arxiv.org/abs/2304.14762) | 本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。 |
| [^7] | [Recognizable Information Bottleneck.](http://arxiv.org/abs/2304.14618) | 本文提出了一种可识别信息瓶颈（RIB），其通过可识别性评论家来规范表示的可识别性，从而实现了一种有效的数据推广方式，相比现有的信息瓶颈能够更好地保证在实际应用中的有效性。 |
| [^8] | [Counterfactual Explanation with Missing Values.](http://arxiv.org/abs/2304.14606) | 本论文提出了一种新的反事实解释方法（CEPIA），可以处理数据中的缺失值，使得用户可以获得有效的行动建议并了解缺失值对建议的影响。 |
| [^9] | [Augmented balancing weights as linear regression.](http://arxiv.org/abs/2304.14545) | 本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。 |
| [^10] | [A Chain Rule for the Expected Suprema of Bernoulli Processes.](http://arxiv.org/abs/2304.14474) | 该论文得出了一个关于伯努利过程期望最大值的上限界限，该界限由指数集在均匀利普希茨函数类下的属性和函数类得出。 |
| [^11] | [One-Step Distributional Reinforcement Learning.](http://arxiv.org/abs/2304.14421) | 本文提出一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性，提供了统一的理论，具有更好的表现。 |
| [^12] | [Network Cascade Vulnerability using Constrained Bayesian Optimization.](http://arxiv.org/abs/2304.14420) | 本研究基于约束贝叶斯优化，以修改输电线路保护设置为敌对攻击的候选方案，探讨了最大化级联网络退化的保护设置规律，发现将所有电网线路的保护设置最大失配并不会导致最多的级联。 |
| [^13] | [Some of the variables, some of the parameters, some of the times, with some physics known: Identification with partial information.](http://arxiv.org/abs/2304.14214) | 本文介绍了一种利用神经网络识别动力系统的新方法，在不对数据进行修改的情况下，通过数值积分和部分已知物理学，能够从任意时间点的采样数据中学习。 |
| [^14] | [Long-term Forecasting with TiDE: Time-series Dense Encoder.](http://arxiv.org/abs/2304.08424) | TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。 |
| [^15] | [Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value.](http://arxiv.org/abs/2304.07718) | Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。 |
| [^16] | [Theoretical Guarantees for Sparse Principal Component Analysis based on the Elastic Net.](http://arxiv.org/abs/2212.14194) | 本文填补了流行的SPCA算法的理论保证的空白，证明了在稀疏峰值协方差模型下，该算法一致地恢复了主子空间。 |
| [^17] | [A Generic Approach for Reproducible Model Distillation.](http://arxiv.org/abs/2211.12631) | 本文提出了一种用中心极限定理为基础的通用稳定模型蒸馏方法，能够从候选学生模型中搜索与老师模型相一致的合理模型，使用一个多重检验框架来选择一组足够大的伪数据集，以选出一致的学生模型。 |
| [^18] | [Training Neural Networks for Sequential Change-point Detection.](http://arxiv.org/abs/2210.17312) | 本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。 |
| [^19] | [LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks.](http://arxiv.org/abs/2206.09333) | LogGENE采用分位数回归框架预测基因表达水平的完整条件分位数，从而为高通量基因组学提供了一种能提供解释和报告不确定性估计、鲁棒性强的推断方法。 |
| [^20] | [Beyond Cuts in Small Signal Scenarios -- Enhanced Sneutrino Detectability Using Machine Learning.](http://arxiv.org/abs/2108.03125) | 本研究提出一种利用机器学习的方式，在信号与背景高度重叠的情况下增强LHC新物理探测的灵敏度。使用XGBoost和深度神经网络模型来利用可观测量之间的相关性比传统的削减-计数方法更有效，并采用模板拟合方法分析模型输出。 |
| [^21] | [Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization.](http://arxiv.org/abs/1903.02140) | 本文介绍了利用规范空间证明学习大规模神经网络的目标函数在收敛到全局最小值时是凸优化问题。使用点线性转换的方法建立原始NN模型空间和规范空间之间的关系，证明了使用梯度下降方法，只要差异矩阵保持完整秩，就一定能收敛到零损失的全局最小值。大规模NN具有奇异的差异矩阵的概率非常小。 |

# 详细

[^1]: 用神经网络求解初值偏微分方程的稳定可扩展方法

    A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])

    [http://arxiv.org/abs/2304.14994](http://arxiv.org/abs/2304.14994)

    本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。

    

    与传统的网格和基于网格的方法不同，神经网络有可能打破维数灾难，在使用经典求解器困难或不可能的问题中提供近似解。全局最小化神经网络参数中的 PDE 残差对于边界值问题效果良好，但是灾难性忘却损害了这种方法对于初值问题的适用性。在替代的局部时间方法中，可以将优化问题转化为网络参数上的常微分方程（ODE），并将解向前传播。然而，我们证明了目前基于这种方法的方法存在两个关键问题。首先，遵循 ODE 会导致问题条件增长无法控制，最终导致不可接受的大数值误差。其次，随着 ODE 方法随着 m 的数量呈立方级别扩展。

    Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m
    
[^2]: Kullback-Leibler Maillard采样在有界奖励的多臂赌博机问题中的应用

    Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])

    [http://arxiv.org/abs/2304.14989](http://arxiv.org/abs/2304.14989)

    本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。

    

    本文研究了奖励分布集中在区间$[0,1]$内的$K$臂数臂赌博机问题。本文提出了一种名为Kullback-Leibler Maillard Sampling (KL-MS)的新算法，它是Maillard采样在KL空间的自然扩展。实验表明，KL-MS在Bernoulli奖励时具有渐近最优性能，其最坏情况遗憾度上界为$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$，其中$\mu^*$是最优臂的期望奖励，$T$是时段长度。

    We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
    
[^3]: PAM：基于贝叶斯非参数的分组数据分析的格子原子模型

    PAM: Plaid Atoms Model for Bayesian Nonparametric Analysis of Grouped Data. (arXiv:2304.14954v1 [stat.ME])

    [http://arxiv.org/abs/2304.14954](http://arxiv.org/abs/2304.14954)

    PAM模型是基于贝叶斯非参数的分组数据分析的新方法，它通过使用零增强贝塔（ZAB）分布实现了多组间的原子共享和独特性，并且在实际应用中表现出良好的性能。

    

    我们考虑对观测数据进行相关聚类的问题。所提出的模型称为格子原子模型（PAM），对于每个组估计一组聚类，并允许一些聚类与其他组共享或仅为该组所拥有。PAM基于对众所周知的粘性破裂过程的扩展，通过将零添加为聚类权重的可能值之一，从而在模型中产生零增强贝塔（ZAB）分布。因此，ZAB允许在多个组中一些聚类的权重恰好为零，从而实现组间共享和独特的原子。我们探讨了PAM的理论属性，并展示了其与已知贝叶斯非参数模型的联系。我们提出了一种有效的切片取样器用于后验推断。我们还针对多元或计数数据提出了所提出模型的轻微扩展。通过使用真实世界数据集进行的模拟研究和应用说明了该模型的良好性能。

    We consider dependent clustering of observations in groups. The proposed model, called the plaid atoms model (PAM), estimates a set of clusters for each group and allows some clusters to be either shared with other groups or uniquely possessed by the group. PAM is based on an extension to the well-known stick-breaking process by adding zero as a possible value for the cluster weights, resulting in a zero-augmented beta (ZAB) distribution in the model. As a result, ZAB allows some cluster weights to be exactly zero in multiple groups, thereby enabling shared and unique atoms across groups. We explore theoretical properties of PAM and show its connection to known Bayesian nonparametric models. We propose an efficient slice sampler for posterior inference. Minor extensions of the proposed model for multivariate or count data are presented. Simulation studies and applications using real-world datasets illustrate the model's desirable performance.
    
[^4]: 关于位置-比例分布的1-Wasserstein距离及差分隐私的影响

    On the 1-Wasserstein Distance between Location-Scale Distributions and the Effect of Differential Privacy. (arXiv:2304.14869v1 [math.PR])

    [http://arxiv.org/abs/2304.14869](http://arxiv.org/abs/2304.14869)

    研究了独立的位置-比例分布的1-Wasserstein距离的计算公式，提供了一个新的线性上界和高斯情况下的渐近界限并研究了差分隐私的影响。

    

    我们提供了独立于位置-比例分布的1-Wasserstein距离的准确计算公式。 这些公式是用位置和比例参数以及特殊函数(如标准高斯分布或伽马函数)表示的。 特别是，我们发现独立于位置-比例分布的一维1-矩量距离等于同一族中折叠分布的均值，其基础位置和比例等于原始分布的位置和比例之差。 我们提供了一个新的线性上界和高斯情况下的渐近界限。 我们利用这些闭合的公式和界限来研究拉普拉斯和高斯机制对1-Wasserstein距离的影响。

    We provide an exact expressions for the 1-Wasserstein distance between independent location-scale distributions. The expressions are represented using location and scale parameters and special functions such as the standard Gaussian CDF or the Gamma function. Specifically, we find that the 1-Wasserstein distance between independent univariate location-scale distributions is equivalent to the mean of a folded distribution within the same family whose underlying location and scale are equal to the difference of the locations and scales of the original distributions. A new linear upper bound on the 1-Wasserstein distance is presented and the asymptotic bounds of the 1-Wasserstein distance are detailed in the Gaussian case. The effect of differential privacy using the Laplace and Gaussian mechanisms on the 1-Wasserstein distance is studied using the closed-form expressions and bounds.
    
[^5]: 通过神经网络分割进行超参数优化

    Hyperparameter Optimization through Neural Network Partitioning. (arXiv:2304.14766v1 [cs.LG])

    [http://arxiv.org/abs/2304.14766](http://arxiv.org/abs/2304.14766)

    本文提出了一种将训练数据和神经网络模型分区的方法，将每个分区与特定的数据片段关联并进行优化，通过优化这些分区的子网络的“训练之外的样本”损失，实现了在单次训练运行中降低超参数优化代价的效果。

    

    调整恰当的超参对于获得神经网络中良好的泛化行为至关重要。它们可以强制适当的归纳偏差，正则化模型并提高性能，特别是在有限的数据情况下。在本文中，我们提出了一种简单而有效的方法来优化超参数，该方法受边缘似然的启发，这是一种不需要验证数据的优化目标。我们的方法将训练数据和神经网络模型分为 K 个数据分片和参数分区。每个分区仅与特定的数据片段关联并进行优化。将这些分区组合成子网络使我们能够将子网络的“训练之外的样本”损失定义为超参数优化的目标，即在子网络看不到的数据片段上计算损失。我们证明，我们可以将这个目标应用到单次训练运行中，同时显着降低超参数优化的代价。

    Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance -- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly c
    
[^6]: 利用扰动来改善基于核化斯坦距的拟合优度检验

    Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])

    [http://arxiv.org/abs/2304.14762](http://arxiv.org/abs/2304.14762)

    本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。

    

    核化斯坦距（KSD）是一种广泛用于拟合优度检验的基于得分的差异度量。即使目标分布具有未知的标准化因子，例如在贝叶斯分析中，也可以应用它。我们理论上和实验证明，当目标分布和替代分布具有相同且相距较远的模式但在混合比例上有所不同时，KSD检验可能会出现低功率问题。我们提出通过马尔科夫转移核对观测样本进行扰动，使其相对于目标分布不变。这使我们可以在扰动样本上使用KSD检验。我们提供的数值证据表明，使用适当选择的核时，所提出的方法可以比KSD检验具有更高的功率。

    Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
    
[^7]: 可识别信息瓶颈

    Recognizable Information Bottleneck. (arXiv:2304.14618v1 [cs.LG])

    [http://arxiv.org/abs/2304.14618](http://arxiv.org/abs/2304.14618)

    本文提出了一种可识别信息瓶颈（RIB），其通过可识别性评论家来规范表示的可识别性，从而实现了一种有效的数据推广方式，相比现有的信息瓶颈能够更好地保证在实际应用中的有效性。

    

    信息瓶颈（IB）通过信息压缩学习表示，从而推广到未见过的数据。然而，由于虚无的推广边界，现有的IB在实际场景中无法保证推广性。最近的PAC-Bayes IB使用信息复杂度而不是信息压缩来建立与相互信息推广界限的联系。然而，它需要计算昂贵的二阶曲率，阻碍了其实际应用。本文建立了表示的可识别性与近期的函数条件互信息（f-CMI）推广边界之间的联系，该边界的估计要简单得多。在此基础上，我们提出了一种可识别信息瓶颈（RIB），通过基于Bregman散度下的密度比匹配优化的可识别性评论家来规范表示的可识别性。在多个标准数据集上进行了广泛实验验证。

    Information Bottlenecks (IBs) learn representations that generalize to unseen data by information compression. However, existing IBs are practically unable to guarantee generalization in real-world scenarios due to the vacuous generalization bound. The recent PAC-Bayes IB uses information complexity instead of information compression to establish a connection with the mutual information generalization bound. However, it requires the computation of expensive second-order curvature, which hinders its practical application. In this paper, we establish the connection between the recognizability of representations and the recent functional conditional mutual information (f-CMI) generalization bound, which is significantly easier to estimate. On this basis we propose a Recognizable Information Bottleneck (RIB) which regularizes the recognizability of representations through a recognizability critic optimized by density ratio matching under the Bregman divergence. Extensive experiments on sev
    
[^8]: 缺失值下的反事实解释方法

    Counterfactual Explanation with Missing Values. (arXiv:2304.14606v1 [cs.LG])

    [http://arxiv.org/abs/2304.14606](http://arxiv.org/abs/2304.14606)

    本论文提出了一种新的反事实解释方法（CEPIA），可以处理数据中的缺失值，使得用户可以获得有效的行动建议并了解缺失值对建议的影响。

    

    反事实解释是一种提供扰动以改变分类器预测结果的事后解释方法。现有方法需要完整信息的输入，但实际情况中往往会有缺失值。本文提出一种新的CE框架（称为CEPIA），使用户可以在有缺失值的情况下获得有效的操作，并阐明缺失值对操作的影响。

    Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an "action" to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation ca
    
[^9]: 自动去偏重重配作为线性回归

    Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])

    [http://arxiv.org/abs/2304.14545](http://arxiv.org/abs/2304.14545)

    本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。

    

    我们提供了对于自动去偏重重配(AutoDML)的新颖特征描述。这些估算器将结果建模与重配相结合，直接估计反向倾向积分权重。当结果与权重模型都是某些（可能是无限的）基础中的线性时，我们表明增强的估算器等同于具有将原始结果模型系数和OLS相结合的系数的单个线性模型；在许多设置中，增强估算器合并为仅使用OLS. 然后，我们将这些结果扩展到特定的结果和重配模型选择上。我们首先表明，使用(内核)岭回归作为结果和重配模型的联合估算器等同于单个、欠平滑(内核)岭回归；当考虑到渐近速率时，这一结果也成立。当代替权重模型为套索回归时，我们给出了特殊情况的解析表达式并且演示了…

    We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
    
[^10]: 一项针对伯努利过程期望最大值的链式规则

    A Chain Rule for the Expected Suprema of Bernoulli Processes. (arXiv:2304.14474v1 [math.PR])

    [http://arxiv.org/abs/2304.14474](http://arxiv.org/abs/2304.14474)

    该论文得出了一个关于伯努利过程期望最大值的上限界限，该界限由指数集在均匀利普希茨函数类下的属性和函数类得出。

    

    我们得出了一个关于伯努利过程期望最大值的上限界限，该界限由指数集在均匀利普希茨函数类下的属性和函数类得出，拓展了Maurer针对高斯过程的早期结果。证明必须基于Bednorz和Latala针对伯努利过程有界性的最近结果。

    We obtain an upper bound on the expected supremum of a Bernoulli process indexed by the image of an index set under a uniformly Lipschitz function class in terms of properties of the index set and the function class, extending an earlier result of Maurer for Gaussian processes. The proof makes essential use of recent results of Bednorz and Latala on the boundedness of Bernoulli processes.
    
[^11]: 一步分布式强化学习

    One-Step Distributional Reinforcement Learning. (arXiv:2304.14421v1 [cs.LG])

    [http://arxiv.org/abs/2304.14421](http://arxiv.org/abs/2304.14421)

    本文提出一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性，提供了统一的理论，具有更好的表现。

    

    强化学习（Reinforcement Learning，RL）允许一个能代理与环境进行连续交互的系统最大化预期收益。在分布式RL（DistrRL）范式下，代理不仅局限于期望值，而是捕捉跨越所有时间步骤的回报概率分布。DistrRL算法的集合提高了经验性能，但DistrRL的理论仍未完全理解，尤其在控制案例中。本文提出了简单的一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性。与DistrRL相反，我们证明了我们的方法针对策略评估和控制都具有统一的理论。我们提出了两种OS-DistrRL算法，并提供了几乎确定的收敛分析。所提出的方法在多种环境中比分类DistrRL好。

    Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.
    
[^12]: 使用约束贝叶斯优化的网络级联漏洞研究

    Network Cascade Vulnerability using Constrained Bayesian Optimization. (arXiv:2304.14420v1 [cs.SI])

    [http://arxiv.org/abs/2304.14420](http://arxiv.org/abs/2304.14420)

    本研究基于约束贝叶斯优化，以修改输电线路保护设置为敌对攻击的候选方案，探讨了最大化级联网络退化的保护设置规律，发现将所有电网线路的保护设置最大失配并不会导致最多的级联。

    

    评估电网的脆弱性常常是通过敌手能够对网络造成的损害量来衡量的。然而，这样攻击的级联影响通常被忽视，尽管级联是大规模停电的主要原因之一。本文探讨了将输电线路保护设置修改为敌对攻击的候选方案，只要网络平衡状态不改变，攻击就可以保持不被检测到。这构成了贝叶斯优化过程中的一个黑盒子函数基础，其目标是找到最大化级联网络退化的保护设置。广泛的实验表明，与常识相反，将所有网络线路的保护设置最大失配并不会导致最多的级联。更令人惊讶的是，即使在资源受限的情况下，仍然可以找到能够产生与实例相当严重的级联的设置。

    Measures of power grid vulnerability are often assessed by the amount of damage an adversary can exact on the network. However, the cascading impact of such attacks is often overlooked, even though cascades are one of the primary causes of large-scale blackouts. This paper explores modifications of transmission line protection settings as candidates for adversarial attacks, which can remain undetectable as long as the network equilibrium state remains unaltered. This forms the basis of a black-box function in a Bayesian optimization procedure, where the objective is to find protection settings that maximize network degradation due to cascading. Extensive experiments reveal that, against conventional wisdom, maximally misconfiguring the protection settings of all network lines does not cause the most cascading. More surprisingly, even when the degree of misconfiguration is resource constrained, it is still possible to find settings that produce cascades comparable in severity to instanc
    
[^13]: 论文标题：某些变量，某些参数，某些时间，某些已知物理学：带有部分信息的识别

    Some of the variables, some of the parameters, some of the times, with some physics known: Identification with partial information. (arXiv:2304.14214v1 [cs.LG])

    [http://arxiv.org/abs/2304.14214](http://arxiv.org/abs/2304.14214)

    本文介绍了一种利用神经网络识别动力系统的新方法，在不对数据进行修改的情况下，通过数值积分和部分已知物理学，能够从任意时间点的采样数据中学习。

    

    实验数据通常由独立测量的变量组成，在不同的采样率(连续测量之间的非均匀${\Delta}$t)下，仅在特定时间点才对所有变量的子集进行采样。从这样的数据中识别动力系统的方法通常使用插值、插值或子采样来重新组织或修改训练数据$ \textit {prior}$ 学习。部分物理知识也可能在$\textit {a priori}$（精确或近似）中可用，并且数据驱动技术可以补充此知识。在这里，我们利用基于数值积分方法和$\textit {a priori}$物理知识的神经网络架构来识别基本控制微分方程的右手边。这种神经网络模型的迭代允许从在任意时间点采样的数据中学习$\textit {without}$数据修改。重要的是，我们将网络与可用的部分物理集成

    Experimental data is often comprised of variables measured independently, at different sampling rates (non-uniform ${\Delta}$t between successive measurements); and at a specific time point only a subset of all variables may be sampled. Approaches to identifying dynamical systems from such data typically use interpolation, imputation or subsampling to reorganize or modify the training data $\textit{prior}$ to learning. Partial physical knowledge may also be available $\textit{a priori}$ (accurately or approximately), and data-driven techniques can complement this knowledge. Here we exploit neural network architectures based on numerical integration methods and $\textit{a priori}$ physical knowledge to identify the right-hand side of the underlying governing differential equations. Iterates of such neural-network models allow for learning from data sampled at arbitrary time points $\textit{without}$ data modification. Importantly, we integrate the network with available partial physical
    
[^14]: 用TiDE进行长期预测：时间序列稠密编码器

    Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])

    [http://arxiv.org/abs/2304.08424](http://arxiv.org/abs/2304.08424)

    TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。

    

    最近的研究表明，相比于基于Transformer的方法，简单的线性模型在长期时间序列预测中表现更好。鉴于此，我们提出了一种基于多层感知机(MLP)的编码器-解码器模型，即时间序列稠密编码器(TiDE)，用于长期时间序列预测。它既享有线性模型的简单性和速度，又能处理协变量和非线性依赖。从理论上讲，我们证明了我们模型的最简线性类比在一些假设下可以达到线性动态系统(LDS)的近乎最优误差率。实证上，我们表明，我们的方法可以在流行的长期时间序列预测基准测试中匹配或胜过以前的方法，同时比最佳的基于Transformer的模型快5-10倍。

    Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
    
[^15]: Data-OOB:以无需额外计算的Out-of-bag估计为准的数据价值估计方法

    Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])

    [http://arxiv.org/abs/2304.07718](http://arxiv.org/abs/2304.07718)

    Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。

    

    数据评估是一个强大的框架，可以为模型训练提供统计洞察力，以区分哪些数据对于模型训练是有益的，哪些是有害的。纵观各种下游任务，许多以Shapley为基础的数据价值评估方法均显示出了很有前途的结果。然而，由于这需要训练大量的模型，因此众所周知，这是具有挑战性的。因此，将此应用于大型数据集是不可行的。为了解决这个问题，我们提出了Data-OOB，这是一种新的数据价值估计方法，针对bagging模型，它利用了out-of-bag估计。所提出的方法在计算上是高效的，可以通过重复使用训练好的弱学习器来扩展到数百万个数据。具体而言，当评估100个输入维度且存在$10^6$个样本时，Data-OOB仅需要在单个CPU处理器上执行不到2.25个小时。此外，Data-OOB在理论上有坚实的解释，当两个离差值函数相同时，其识别具有相同重要性的数据点。

    Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
    
[^16]: 基于弹性网的稀疏主成分分析的理论保证

    Theoretical Guarantees for Sparse Principal Component Analysis based on the Elastic Net. (arXiv:2212.14194v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.14194](http://arxiv.org/abs/2212.14194)

    本文填补了流行的SPCA算法的理论保证的空白，证明了在稀疏峰值协方差模型下，该算法一致地恢复了主子空间。

    

    稀疏主成分分析（SPCA）在高维数据分析中广泛用于降维和特征提取。尽管在过去的二十年中出现了许多方法论和理论发展，但流行的SPCA算法（由Zou、Hastie和Tibshirani于2006年提出）的理论保证仍然不明确。本文旨在填补这一关键差距。我们首先重新审视了Zou等人（2006年）的SPCA算法并呈现了我们的实现。同时，我们还研究了一种在Zou等人（2006年）中可以被认为是SPCA的极限情况的计算更有效的变体。我们为两种算法提供了到达稳定点的收敛性保证，并证明了，在稀疏峰值协方差模型下，在温和的正则条件下，这两种算法都可以一致地恢复主子空间。我们还表明，它们的估计误差边界与现有工作的最佳边界或最小极限速率匹配，直到某些对数。

    Sparse principal component analysis (SPCA) is widely used for dimensionality reduction and feature extraction in high-dimensional data analysis. Despite many methodological and theoretical developments in the past two decades, the theoretical guarantees of the popular SPCA algorithm proposed by Zou, Hastie & Tibshirani (2006) are still unknown. This paper aims to address this critical gap. We first revisit the SPCA algorithm of Zou et al. (2006) and present our implementation. We also study a computationally more efficient variant of the SPCA algorithm in Zou et al. (2006) that can be considered as the limiting case of SPCA. We provide the guarantees of convergence to a stationary point for both algorithms and prove that, under a sparse spiked covariance model, both algorithms can recover the principal subspace consistently under mild regularity conditions. We show that their estimation error bounds match the best available bounds of existing works or the minimax rates up to some logar
    
[^17]: 可复现模型蒸馏的通用方法

    A Generic Approach for Reproducible Model Distillation. (arXiv:2211.12631v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.12631](http://arxiv.org/abs/2211.12631)

    本文提出了一种用中心极限定理为基础的通用稳定模型蒸馏方法，能够从候选学生模型中搜索与老师模型相一致的合理模型，使用一个多重检验框架来选择一组足够大的伪数据集，以选出一致的学生模型。

    

    模型蒸馏是一种流行的生成可解释机器学习模型的方法。它使用一个可解释的“学生”模型去模仿黑盒“老师”模型产生的预测结果。然而，当学生模型对于用于训练的数据集的变异性敏感时，关于学生模型的解释就不可靠了。现有的稳定模型蒸馏策略通过检查是否生成了足够大的伪数据来稳定模型蒸馏，但目前为止，这些方法只适用于特定的学生模型。在本文中，我们提出了一种基于中心极限定理的通用稳定模型蒸馏方法。我们首先从候选的学生模型集合开始，搜索与老师模型相一致的合理候选模型。然后，我们构建一个多重检验框架，选择一组可以使得一致的学生模型被选中的伪数据集。

    Model distillation has been a popular method for producing interpretable machine learning. It uses an interpretable "student" model to mimic the predictions made by the black box "teacher" model. However, when the student model is sensitive to the variability of the data sets used for training even when keeping the teacher fixed, the corresponded interpretation is not reliable. Existing strategies stabilize model distillation by checking whether a large enough corpus of pseudo-data is generated to reliably reproduce student models, but methods to do so have so far been developed for a specific student model. In this paper, we develop a generic approach for stable model distillation based on central limit theorem for the average loss. We start with a collection of candidate student models and search for candidates that reasonably agree with the teacher. Then we construct a multiple testing framework to select a corpus size such that the consistent student model would be selected under d
    
[^18]: 训练神经网络用于时序变点检测

    Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17312](http://arxiv.org/abs/2210.17312)

    本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。

    

    检测数据流中的突变分布转换，即所谓的变点检测，是统计学和机器学习中的一个基本问题。我们引入了一种新颖的方法，使用神经网络进行在线变点检测。具体而言，我们的方法是训练神经网络来逐步计算检测统计量的累积和，当发生变点时，该量会显著变化。我们使用合成和真实世界数据证明了所提出的方法在检测变点方面的优越性和潜力。

    Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
    
[^19]: LogGENE: 一种用于深度医疗推理任务的平滑检查损失替代方法

    LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks. (arXiv:2206.09333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09333](http://arxiv.org/abs/2206.09333)

    LogGENE采用分位数回归框架预测基因表达水平的完整条件分位数，从而为高通量基因组学提供了一种能提供解释和报告不确定性估计、鲁棒性强的推断方法。

    

    在可靠的深度学习中，挖掘大型数据集并从中获得校准的预测具有即时相关性和实用性。本研究开发了基于深度神经网络的推断方法，适用于基因表达等大型数据集。然而，与典型的深度学习方法不同的是，我们的推断技术在准确性方面实现了最先进的性能，同时还能提供解释和报告不确定性估计。我们采用分位数回归框架来预测给定一组基因表达的完整条件分位数。条件分位数除了有助于提供预测的丰富解释外，还能够抵抗测量噪声。我们的技术在高通量基因组学中特别重要，这是一个正在引领个性化医疗、靶向药物设计和传递的新时代。然而，用于驱动估计过程的检查损失，在分位数回归中并无不同之处。

    Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not diffe
    
[^20]: 利用机器学习提高Sneutrino探测效率——在小信号场景中超越削减方法

    Beyond Cuts in Small Signal Scenarios -- Enhanced Sneutrino Detectability Using Machine Learning. (arXiv:2108.03125v3 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2108.03125](http://arxiv.org/abs/2108.03125)

    本研究提出一种利用机器学习的方式，在信号与背景高度重叠的情况下增强LHC新物理探测的灵敏度。使用XGBoost和深度神经网络模型来利用可观测量之间的相关性比传统的削减-计数方法更有效，并采用模板拟合方法分析模型输出。

    

    本文提出了一种利用机器学习增强LHC新物理搜索灵敏度的方法，特别是在背景支配和信号与背景在可观测量上存在高度重叠的情况下。我们使用了两种不同的模型，XGBoost和深度神经网络，利用可观测量之间的相关性，并将这种方法与传统的削减-计数方法进行了比较。我们考虑了不同的方法来分析模型的输出，并发现模板拟合通常比简单的削减方法表现更好。通过Shapley分解，我们获得了对事件运动学和机器学习模型输出之间关系的额外洞察。我们以具体的亚稳Sneutrino超对称情景为例，但该方法可以应用于更广泛的模型类别中。

    We investigate enhancing the sensitivity of new physics searches at the LHC by machine learning in the case of background dominance and a high degree of overlap between the observables for signal and background. We use two different models, XGBoost and a deep neural network, to exploit correlations between observables and compare this approach to the traditional cut-and-count method. We consider different methods to analyze the models' output, finding that a template fit generally performs better than a simple cut. By means of a Shapley decomposition, we gain additional insight into the relationship between event kinematics and the machine learning model output. We consider a supersymmetric scenario with a metastable sneutrino as a concrete example, but the methodology can be applied to a much wider class of models.
    
[^21]: 大规模神经网络学习为什么行为类似于凸优化

    Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization. (arXiv:1903.02140v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.02140](http://arxiv.org/abs/1903.02140)

    本文介绍了利用规范空间证明学习大规模神经网络的目标函数在收敛到全局最小值时是凸优化问题。使用点线性转换的方法建立原始NN模型空间和规范空间之间的关系，证明了使用梯度下降方法，只要差异矩阵保持完整秩，就一定能收敛到零损失的全局最小值。大规模NN具有奇异的差异矩阵的概率非常小。

    

    本文提出了一些理论工作，以解释为什么简单的梯度下降方法在解决学习大规模神经网络的非凸优化问题中如此成功。在介绍了一种称为规范空间的数学工具之后，我们证明了规范模型空间中的学习NN目标函数是凸的。我们进一步阐明了原始NN模型空间和规范空间之间的梯度之间的关系是通过所谓的差异矩阵表示的逐点线性变换相关的。此外，我们已经证明，如果差异矩阵保持完整秩，梯度下降方法一定会收敛到零损失的全局最小值。如果这个完整秩条件成立，在NN的学习中的行为与正常的凸优化相同。最后，我们证明，大规模NN具有奇异的差异矩阵的概率非常小。特别是，当超参数化的NN是...

    In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are 
    

