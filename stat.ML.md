# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mapping the Multiverse of Latent Representations](https://rss.arxiv.org/abs/2402.01514) | 提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。 |
| [^2] | [Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape](https://rss.arxiv.org/abs/2402.01258) | 本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。 |
| [^3] | [Functional Bilevel Optimization for Machine Learning](https://arxiv.org/abs/2403.20233) | 介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。 |
| [^4] | [A tutorial on learning from preferences and choices with Gaussian Processes](https://arxiv.org/abs/2403.11782) | 提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。 |
| [^5] | [Multistep Consistency Models](https://arxiv.org/abs/2403.06807) | 本文提出了多步一致性模型，通过在一致性模型和扩散模型之间插值，实现了采样速度和采样质量的平衡。 |
| [^6] | [Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket](https://arxiv.org/abs/2402.14029) | 提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。 |
| [^7] | [Overcoming Saturation in Density Ratio Estimation by Iterated Regularization](https://arxiv.org/abs/2402.13891) | 引入迭代正则化方法解决了密度比估计中的饱和问题，实现了快速收敛，在密度比估计基准测试和大规模深度无监督领域自适应模型的重要性加权集成中表现优异。 |
| [^8] | [Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance](https://arxiv.org/abs/2402.11742) | 该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。 |
| [^9] | [Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach](https://arxiv.org/abs/2402.11338) | 该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。 |
| [^10] | [Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention](https://arxiv.org/abs/2402.10198) | 本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。 |
| [^11] | [Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States](https://arxiv.org/abs/2402.07875) | 本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。 |
| [^12] | [Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation](https://arxiv.org/abs/2402.07723) | 本论文通过分数阻尼库仑方程证明了重尾SDE的高概率泛化界限，并且相对于参数维度，界限的依赖性要好于p。 |
| [^13] | [Rethinking Scaling Laws for Learning in Strategic Environments](https://arxiv.org/abs/2402.07588) | 本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。 |
| [^14] | [The Implicit Bias of Gradient Noise: A Symmetry Perspective](https://arxiv.org/abs/2402.07193) | 本研究通过对对称性的存在进行分析，揭示了梯度噪声在随机梯度下降中的隐性偏见。我们发现不同类型的对称性会导致不同的学习动态，其中一类对称性可以自然收敛，而另一类对称性几乎总是发散。此外，我们的研究结果适用于没有对称性的损失函数，对于理解训练动态和解释相关实际问题具有普适性。 |
| [^15] | [Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF](https://arxiv.org/abs/2402.06886) | 本文提出了一种基于惩罚的方法来解决Bilevel强化学习和RLHF问题，这是首个有原则的算法框架。通过理论分析和实验证明了算法的有效性。 |
| [^16] | [SMC Is All You Need: Parallel Strong Scaling](https://arxiv.org/abs/2402.06173) | SMC并行扩展方法pSMC具有理论收敛速度，具有有界的时间复杂性和内存要求，适用于贝叶斯推断的问题。 |
| [^17] | [Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams](https://arxiv.org/abs/2402.06122) | 本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。 |
| [^18] | [Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL](https://arxiv.org/abs/2402.05724) | 本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。 |
| [^19] | [Stein Boltzmann Sampling: A Variational Approach for Global Optimization](https://arxiv.org/abs/2402.04689) | Stein Boltzmann抽样（SBS）是一种全局优化的变分方法，通过从Boltzmann分布中采样，由Stein Variational Gradient Descent算法实现，具有渐近收敛性，并在各种基准函数上比较了几种最先进的全局优化算法，尤其适合作为高效全局优化方法的延续，能够产生更好的解决方案并有效利用预算。 |
| [^20] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^21] | [A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs](https://arxiv.org/abs/2402.04493) | 这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。 |
| [^22] | [Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies](https://arxiv.org/abs/2402.03819) | SMOTE是一种处理不平衡数据集的常用重新平衡策略，它通过复制原始少数样本来重新生成原始分布。本研究证明了SMOTE的密度在少数样本分布的边界附近逐渐减小，从而验证了BorderLine SMOTE策略的合理性。此外，研究还提出了两种新的SMOTE相关策略，并与其他重新平衡方法进行了比较。最终发现，在数据集极度不平衡的情况下，SMOTE、提出的方法或欠采样程序是最佳的策略。 |
| [^23] | [Statistical Test for Anomaly Detections by Variational Auto-Encoders](https://arxiv.org/abs/2402.03724) | 本研究提出了一种利用变分自动编码器进行异常检测的统计测试方法（VAE-AD测试），通过量化异常区域的可靠性，可以控制误检的概率到所期望的水平。 |
| [^24] | [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287) | 图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。 |
| [^25] | [Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI](https://arxiv.org/abs/2402.00809) | 《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。 |
| [^26] | [Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge](https://arxiv.org/abs/2312.12558) | 研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。 |
| [^27] | [Provably Stable Feature Rankings with SHAP and LIME.](http://arxiv.org/abs/2401.15800) | 这项研究提出了一种通过利用多重假设检验的思想，来设计可靠地排名机器学习模型中最重要特征的特征归因方法，旨在解决SHAP和LIME等常用方法由于随机采样导致的高度不稳定性问题。实验证明了该方法的有效性和计算效率。 |
| [^28] | [Provably Scalable Black-Box Variational Inference with Structured Variational Families.](http://arxiv.org/abs/2401.10989) | 本文研究了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族，并通过理论证明结构化变分族可以在迭代复杂性上表现更好，缩放效果更好。 |
| [^29] | [Robust and Conjugate Gaussian Process Regression.](http://arxiv.org/abs/2311.00463) | 本文提出了一种健壮和共轭的高斯过程（RCGP）回归方法，通过泛化贝叶斯推断实现了可靠的闭式更新，适用于各种实际应用场景。 |
| [^30] | [Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs.](http://arxiv.org/abs/2310.17816) | 在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。 |
| [^31] | [Adam through a Second-Order Lens.](http://arxiv.org/abs/2310.14963) | 该论文提出了AdamQLR，它是一个通过将K-FAC中的技术与Adam的更新方法相结合的优化器，通过考虑二阶数据上的Adam行为而得到启发。在回归和分类任务上进行了评估，结果显示AdamQLR在运行时间和推广性能方面表现出良好的竞争力。 |
| [^32] | [Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal.](http://arxiv.org/abs/2310.08672) | 通过对学生进行因果与预测性定位的比较，研究探讨了在学生助学金续签领域实验中鼓励对象选择的价值。这项大规模实地实验揭示了定位干预对于不同学生的效果，为干预策略的优化提供了参考。 |
| [^33] | [Neural Diffusion Models.](http://arxiv.org/abs/2310.08337) | 本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。 |
| [^34] | [Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes.](http://arxiv.org/abs/2310.07216) | 通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。 |
| [^35] | [Learning to Scale Logits for Temperature-Conditional GFlowNets.](http://arxiv.org/abs/2310.02823) | 这项研究提出了一种名为LSL-GFN的新型架构设计，可以大大加速温度条件下GFlowNets的训练，从而提高GFlowNets的探索和利用能力。 |
| [^36] | [Symmetry Leads to Structured Constraint of Learning.](http://arxiv.org/abs/2309.16932) | 本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。 |
| [^37] | [PAGER: A Framework for Failure Analysis of Deep Regression Models.](http://arxiv.org/abs/2309.10977) | PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。 |
| [^38] | [Kernel Debiased Plug-in Estimation.](http://arxiv.org/abs/2306.08598) | 本文提出了一种高效、不需要实现影响函数且可计算的去偏插值估计方法。 |
| [^39] | [Interpreting and Improving Diffusion Models Using the Euclidean Distance Function.](http://arxiv.org/abs/2306.04848) | 本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。 |
| [^40] | [On Riemannian Projection-free Online Learning.](http://arxiv.org/abs/2305.19349) | 本文提出了一种针对非凸约束集情况下的曲线空间在线测地凸优化的无投影算法，获得了次线性遗憾保证。 |
| [^41] | [Fast global convergence of gradient descent for low-rank matrix approximation.](http://arxiv.org/abs/2305.19206) | 本文证明了对于低秩矩阵逼近问题，使用小随机初始化的梯度下降方法具有非常快的全局收敛性，尤其是当最大特征值相同时。退化自由算法比Riemannian梯度下降方法更有效，在运行时间上可以减少29％。 |
| [^42] | [Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line.](http://arxiv.org/abs/2305.14689) | 本文研究了线性数据最小二乘岭正则化的去噪问题，证明了在欠参数化情况下会出现双峰谷现象。 |
| [^43] | [Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations.](http://arxiv.org/abs/2305.11308) | 本文介绍了一种多目标设计反事实(MCD)方法，可帮助设计师识别设计修改，提高功能性能。MCD通过支持多目标查询和解耦反事实搜索和采样过程来提高效率并改进现有的反事实搜索方法，证明其在自行车设计案例中的有效性。 |
| [^44] | [NUBO: A Transparent Python Package for Bayesian Optimisation.](http://arxiv.org/abs/2305.06709) | NUBO是一个透明的Python包，用于优化昂贵的黑盒函数，它利用高斯过程做代理模型以及获取函数来指导选择候选点，专注于透明度和用户体验。 |
| [^45] | [Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under ManhattanWorld AssumptionWithout Ambiguity.](http://arxiv.org/abs/2303.17166) | 本文提出一种基于学习的标定方法，使用热度图回归来消除曼哈顿世界假设下鱼眼图片中横向角度歧义，同时恢复旋转和消除鱼眼失真。该方法使用优化的对角线点缓解图像中缺乏消失点的情况，并在实验证明其性能优于现有技术。 |
| [^46] | [On the Expressivity of Persistent Homology in Graph Learning.](http://arxiv.org/abs/2302.09826) | 本文通过在图学习任务中的理论讨论和实证分析，证明了持续同调技术在捕捉具有显著拓扑结构的数据集中的长程图性质方面表现出的高表达性。 |
| [^47] | [Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning.](http://arxiv.org/abs/2212.01168) | 通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。 |
| [^48] | [Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes.](http://arxiv.org/abs/2211.02763) | 本文介绍了一种新的方法，使用GFlowNets和变分贝叶斯联合学习因果模型的结构和机制，不仅能够处理非线性和非高斯数据，在模拟数据上也能与几个基线方法相竞争。 |
| [^49] | [Unpacking the Black Box: Regulating Algorithmic Decisions.](http://arxiv.org/abs/2110.03443) | 本文研究如何在代理使用复杂的“黑盒”预测函数进行决策的情况下，对算法决策进行最优调控。研究发现，限制代理使用透明度足够高的预测函数是低效的，而针对激励偏差源头的目标化工具可以提供次优解决方案，从而改善福利。 |

# 详细

[^1]: 映射潜在表示的多元宇宙

    Mapping the Multiverse of Latent Representations

    [https://rss.arxiv.org/abs/2402.01514](https://rss.arxiv.org/abs/2402.01514)

    提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。

    

    响应最近对通过多元宇宙分析来应对机器学习中的可靠性和稳健性问题的呼吁，我们提出了PRESTO，一种系统的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。尽管这些模型得到了广泛的应用，但对它们嵌入的变异性仍然不被充分理解，导致了不必要的复杂性和不可靠的表示。我们的框架使用持续同调来表征不同组合的多样化机器学习方法、(超)参数配置和数据集所产生的潜在空间，从而使我们能够测量它们之间的成对(非)相似性并对其分布进行统计推理。正如我们在理论上和实证上所证明的那样，我们的流程保持了潜在表示集合的理想特性，可以用于进行敏感性分析、检测异常嵌入或高效有效地导航超参。

    Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperpa
    
[^2]: Transformers在上下文中学习非线性特征：关于注意力场景中的非凸均场动态研究

    Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape

    [https://rss.arxiv.org/abs/2402.01258](https://rss.arxiv.org/abs/2402.01258)

    本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。

    

    基于Transformer架构的大型语言模型展示了在上下文中学习的令人印象深刻的能力。然而，关于这一现象产生的现有理论研究仅限于对线性回归任务上训练的单层注意力的动态。在本文中，我们研究了一个由全连接层和线性注意力层组成的Transformer的优化。MLP充当了一个常见的非线性表示或特征映射，极大地增强了上下文学习的能力。我们在均场和两个时间尺度的极限情况下证明了参数分布的无限维损失景观，虽然高度非凸，但变得相当温和。我们还分析了均场动态的二阶稳定性，并表明Wasserstein梯度流几乎总是避开鞍点。此外，我们建立了获得远离临界点和接近临界点的具体改进速率的新方法。

    Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the 
    
[^3]: 机器学习中的函数双层优化

    Functional Bilevel Optimization for Machine Learning

    [https://arxiv.org/abs/2403.20233](https://arxiv.org/abs/2403.20233)

    介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。

    

    在本文中，我们介绍了针对机器学习中的双层优化问题的一种新的函数视角，其中内部目标在函数空间上被最小化。这些类型的问题通常通过在参数设置下开发的方法来解决，其中内部目标对于预测函数的参数强凸。函数视角不依赖于此假设，特别允许使用超参数化的神经网络作为内部预测函数。我们提出了可扩展和高效的算法来解决函数双层优化问题，并展示了我们方法在适合自然函数双层结构的仪表回归和强化学习任务上的优势。

    arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
    
[^4]: 使用高斯过程从偏好和选择中学习的教程

    A tutorial on learning from preferences and choices with Gaussian Processes

    [https://arxiv.org/abs/2403.11782](https://arxiv.org/abs/2403.11782)

    提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。

    

    偏好建模位于经济学、决策理论、机器学习和统计学的交叉点。通过理解个体的偏好及其选择方式，我们可以构建更接近他们期望的产品，为跨领域的更高效、个性化应用铺平道路。此教程的目标是提供一个连贯、全面的偏好学习框架，使用高斯过程演示如何将理性原则（来自经济学和决策理论）无缝地纳入学习过程中。通过合适地定制似然函数，这一框架使得能够构建涵盖随机效用模型、辨识限制和对象和标签偏好的多重冲突效用情景的偏好学习模型。

    arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
    
[^5]: 多步一致性模型

    Multistep Consistency Models

    [https://arxiv.org/abs/2403.06807](https://arxiv.org/abs/2403.06807)

    本文提出了多步一致性模型，通过在一致性模型和扩散模型之间插值，实现了采样速度和采样质量的平衡。

    

    扩散模型相对容易训练，但生成样本需要许多步骤。一致性模型更难训练，但可以在一个步骤中生成样本。本文提出了多步一致性模型：通过一致性模型和TRACT的统一，可以在一致性模型和扩散模型之间进行插值：在采样速度和采样质量之间取得平衡。具体来说，1步一致性模型是传统的一致性模型，而我们展示了$\infty$步一致性模型是扩散模型。多步一致性模型在实践中表现良好。将样本预算从单步增加到2-8步，我们可以更轻松地训练模型，生成更高质量的样本，同时保留大部分采样速度优势。在Imagenet 64上8步达到1.4的FID，在Imagenet128上8步达到2.1的FID。

    arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
    
[^6]: 冻结网络中的部分搜索足以找到强大的彩票票证

    Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket

    [https://arxiv.org/abs/2402.14029](https://arxiv.org/abs/2402.14029)

    提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。

    

    arXiv:2402.14029v1 公告类型：跨越 摘要：随机初始化的稠密网络包含可以在不进行权重学习的情况下实现高准确度的子网络--强大的彩票票证（SLTs）。最近，Gadhikar等人（2023年）在理论和实验证明，SLTs也可以在随机修剪的源网络中找到，从而减少SLT的搜索空间。然而，这限制了对甚至比源网络更稀疏的SLTs的搜索，导致由于意外的高稀疏性而准确度较差。本文提出了一种通过独立于所需SLT稀疏性的任意比率减少SLT搜索空间的方法。通过冻结一部分初始权重的随机子集，将其排除在搜索空间之外--即，通过永久修剪它们或将它们锁定为SLT的固定部分。事实上，通过我们与随机冻结变量的子集和逼近，在这种减少的搜索空间中，SLT的存在在理论上是得到保证的。除此之外，还可以减少...

    arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
    
[^7]: 克服迭代正则化中密度比估计的饱和问题

    Overcoming Saturation in Density Ratio Estimation by Iterated Regularization

    [https://arxiv.org/abs/2402.13891](https://arxiv.org/abs/2402.13891)

    引入迭代正则化方法解决了密度比估计中的饱和问题，实现了快速收敛，在密度比估计基准测试和大规模深度无监督领域自适应模型的重要性加权集成中表现优异。

    

    从有限样本中估计两个概率密度的比率，是机器学习和统计学中的一个核心任务。在这项工作中，我们发现一大类密度比估计的核方法存在错误饱和问题，这阻碍了算法在高度规则学习问题上实现快速错误收敛率。为了解决饱和问题，我们引入了迭代正则化方法在密度比估计中以实现快速错误率。我们的方法在密度比估计基准测试以及大规模评估深度无监督领域自适应模型的重要性加权集成方面表现优异。

    arXiv:2402.13891v1 Announce Type: new  Abstract: Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.
    
[^8]: 平衡数据，不平衡光谱：揭示具有光谱不平衡的类别差异

    Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance

    [https://arxiv.org/abs/2402.11742](https://arxiv.org/abs/2402.11742)

    该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。

    

    分类模型被期望在不同类别上表现同样良好，但在实践中，它们的表现往往存在较大差距。这个类别偏见问题在样本不平衡的数据集中得到了广泛研究，但在平衡数据集中相对被忽视。在这项工作中，我们提出了特征中的光谱不平衡概念作为类别差异的潜在来源，并研究光谱不平衡与类别偏见在理论和实践中的联系。为了建立光谱不平衡与类别差距之间的关系，我们开发了一个用于研究类别差异的理论框架，并推导了高维混合模型设定中每类错误的精确表达式。然后我们在11个不同的最先进的预训练编码器中研究了这一现象，并展示了我们提出的框架如何用于比较编码器的质量，以及评估和组合数据增强策略。

    arXiv:2402.11742v1 Announce Type: new  Abstract: Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation stra
    
[^9]: 具有部分反馈的公平分类：一种基于探索的数据收集方法

    Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach

    [https://arxiv.org/abs/2402.11338](https://arxiv.org/abs/2402.11338)

    该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。

    

    在许多预测场景（例如信贷放款）中，只有过去被积极分类的样本才会观察到真实结果。这些过去的观察结果形成了用于训练分类器以进行未来预测的训练数据集。然而，这样的训练数据集缺乏关于过去（错误地）被负面分类的样本结果的信息，可能导致错误的分类器。我们提出了一种方法，利用可用数据训练分类器，并提供一系列探索策略来收集关于否则会被忽略的子群体的结果数据。对于任何探索策略，该方法都具有以下保证：（1）所有子群体都得到了探索，（2）假阳性的比例受到了限制，（3）训练的分类器收敛到一个“期望”的分类器。正确的探索策略取决于上下文；它可以选择以改善学习保证

    arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
    
[^10]: 使用锐度感知最小化和通道注意力解锁Transformer在时间序列预测中的潜力

    Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention

    [https://arxiv.org/abs/2402.10198](https://arxiv.org/abs/2402.10198)

    本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。

    

    Transformer架构在自然语言处理和计算机视觉中取得了突破性的性能，但在多元长期预测方面，它们仍然不如更简单的线性基线。为了更好地理解这一现象，我们首先研究了一个玩具线性预测问题，展示了尽管Transformer具有高表达能力，但它们无法收敛到真正的解决方案。我们进一步确定Transformer的注意力是造成其低泛化能力的原因。基于这一认识，我们提出了一个浅层轻量级的Transformer模型，在锐度感知优化的情况下成功避免了坏的局部最小值。我们通过实验证明，这个结果适用于所有常用的实际多元时间序列数据集。特别是，相比当前最先进的模型TSMixer，SAMformer的平均性能提高了14.33%，并且参数数量减少了约4倍。

    arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
    
[^11]: 线性二次控制中策略梯度的隐性偏差：对未见初始状态的外推

    Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States

    [https://arxiv.org/abs/2402.07875](https://arxiv.org/abs/2402.07875)

    本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。

    

    在现代机器学习中，模型可以以多种方式拟合训练数据，其中一些在未见（测试）数据上表现良好，而其他一些则不然。有趣的是，在这种情况下，梯度下降经常展现出一种隐性偏差，导致在未见数据上表现出色。这种隐性偏差在监督学习中已经得到了广泛研究，但在最优控制（强化学习）中却了解得较少。在那里，通过梯度下降学习应用于系统的控制器被称为策略梯度，并且一个非常重要的问题是学习的控制器在对未见初始状态的外推程度。本文在理论上研究了策略梯度在对未见初始状态的外推方面的隐性偏差。我们以基本的线性二次调节器（LQR）问题为重点，确立了外推程度取决于训练中系统在初始状态下引起的探索程度。

    In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
    
[^12]: 通过分数阻尼库仑方程证明重尾SDEs的泛化界限

    Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation

    [https://arxiv.org/abs/2402.07723](https://arxiv.org/abs/2402.07723)

    本论文通过分数阻尼库仑方程证明了重尾SDE的高概率泛化界限，并且相对于参数维度，界限的依赖性要好于p。

    

    过去几年来，理解重尾随机优化算法的泛化性能引起了越来越多的关注。在利用重尾随机微分方程作为代理来阐明随机优化器的有趣方面时，先前的工作要么提供预期的泛化界限，要么引入了不可计算的信息论术语。为了解决这些缺点，在本文中，我们证明了重尾SDE的高概率泛化界限，这些界限不含任何非平凡的信息论术语。为了实现这个目标，我们基于估计与所谓的分数阻尼库仑方程相关联的熵流，开发了新的证明技术（这是一种控制相应重尾SDE分布演化的偏微分方程）。除了获得高概率界限之外，我们还展示了我们的界限相对于参数维度的依赖性要好于p。

    Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to p
    
[^13]: 重新思考战略环境中学习的比例定律

    Rethinking Scaling Laws for Learning in Strategic Environments

    [https://arxiv.org/abs/2402.07588](https://arxiv.org/abs/2402.07588)

    本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。

    

    越来越大的机器学习模型的部署反映出一个共识：模型越有表达能力，越拥有大量数据，就能改善性能。随着模型在各种真实场景中的部署，它们不可避免地面临着战略环境。本文考虑了模型与战略互动对比例定律的相互作用对性能的影响这个自然问题。我们发现战略互动可以打破传统的比例定律观点，即性能并不一定随着模型的扩大和/或表达能力的增强（即使有无限数据）而单调提高。我们通过战略回归、战略分类和多智能体强化学习的例子展示了这一现象的影响，这些例子展示了战略环境中的限制模型或策略类的表达能力即可。

    The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
    
[^14]: 梯度噪声的隐性偏见：从对称性角度来看

    The Implicit Bias of Gradient Noise: A Symmetry Perspective

    [https://arxiv.org/abs/2402.07193](https://arxiv.org/abs/2402.07193)

    本研究通过对对称性的存在进行分析，揭示了梯度噪声在随机梯度下降中的隐性偏见。我们发现不同类型的对称性会导致不同的学习动态，其中一类对称性可以自然收敛，而另一类对称性几乎总是发散。此外，我们的研究结果适用于没有对称性的损失函数，对于理解训练动态和解释相关实际问题具有普适性。

    

    我们对随机梯度下降（SGD）在损失函数存在连续对称性时的学习动态进行了表征，说明了SGD和梯度下降之间的分歧是多么巨大。我们展示了根据对称性对学习动态的影响方式，我们可以将一族对称性分为两类。对于一类对称性，SGD自然地收敛到具有平衡和对齐梯度噪声的解。对于另一类对称性，SGD几乎总是发散的。然后，我们展示了即使损失函数中没有对称性，我们的结果依然适用并可以帮助我们理解训练动态。我们的主要结果是普遍的，它只依赖于对称性的存在，而与损失函数的细节无关。我们证明了所提出的理论对于逐步变形和平坦化提供了解释，并可以应用于常见的实际问题，如表示正则化。

    We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
    
[^15]: Bilevel强化学习和RLHF的有原则的基于惩罚的方法

    Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF

    [https://arxiv.org/abs/2402.06886](https://arxiv.org/abs/2402.06886)

    本文提出了一种基于惩罚的方法来解决Bilevel强化学习和RLHF问题，这是首个有原则的算法框架。通过理论分析和实验证明了算法的有效性。

    

    最近，Bilevel优化已被应用于许多机器学习任务中。然而，它们的应用仅限于监督学习设置，其中考虑了具有良性结构的静态目标函数。但是，激励设计、反向强化学习(RL)和来自人类反馈的RLHF等Bilevel问题通常被建模为超越简单静态目标结构的动态目标函数，这给使用现有Bilevel解决方案带来了重大挑战。为了解决这一新的Bilevel问题类别，我们通过惩罚形式引入了解决Bilevel RL问题的第一个原则性算法框架。我们通过理论研究问题的景观及其基于惩罚的（策略）梯度算法进行了验证。我们通过在Stackelberg马尔可夫博弈、来自人类反馈的RL和激励设计中进行模拟来证明我们算法的有效性。

    Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.
    
[^16]: SMC就是你需要的：并行强扩展

    SMC Is All You Need: Parallel Strong Scaling

    [https://arxiv.org/abs/2402.06173](https://arxiv.org/abs/2402.06173)

    SMC并行扩展方法pSMC具有理论收敛速度，具有有界的时间复杂性和内存要求，适用于贝叶斯推断的问题。

    

    在贝叶斯推断的一般框架中，目标分布只能按比例常数进行评估。传统的一致Bayesian方法，如序贯蒙特卡洛(SMC)和马尔科夫链蒙特卡洛(MCMC)，具有无界的时间复杂性要求。我们开发了一种完全并行的序贯蒙特卡洛(pSMC)方法，可以证明它具有并行强扩展性，即如果允许异步进程数量增长，时间复杂性(和每个节点的内存)仍然保持有界。更具体地说，pSMC具有MSE$=O(1/NR)$的理论收敛速度，其中$N$表示每个处理器中的通信样本数量，$R$表示处理器数量。特别地，对于适当大的问题相关$N$，当$R\rightarrow \infty$时，该方法以固定有限的时间复杂性Cost$=O(1)$收敛到无穷小精度MSE$=O(\varepsilon^2)$，没有效率泄漏，即计算复杂性Cost$=O(\varepsilon)$。

    In the general framework of Bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes the number of communicating samples in each processor and $R$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $R \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon
    
[^17]: 使用PEAK进行窥探：多个数据流均值的顺序、非参数复合假设检验

    Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams

    [https://arxiv.org/abs/2402.06122](https://arxiv.org/abs/2402.06122)

    本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。

    

    我们提出了一种新颖的非参数顺序复合假设检验方法，用于多个数据流的均值。我们的方法名为PEAK（基于期望平均资产的窥探），基于测试即博弈的框架，提供了一个在任何停止时间上的非渐进α水平测试。PEAK在计算上可行，并且能够有效拒绝在满足我们的非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。我们在强化学习中的最佳臂识别和阈值识别任务中对我们的理论结果进行了数值验证，并展示了我们的方法在计算效率上优于现有的测试方法。

    We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
    
[^18]: 基于模型的强化学习在平均场博弈中并不比单个智能体强化学习更加困难

    Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL

    [https://arxiv.org/abs/2402.05724](https://arxiv.org/abs/2402.05724)

    本研究研究了在平均场博弈中基于模型的强化学习的样本复杂度，提出了部分基于模型的Eluder维度（P-MBED）概念来衡量模型类复杂度，并且证明在基本假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。

    

    我们研究了在平均场博弈中基于模型的函数逼近下强化学习样本复杂度，该方法需要策略性探索以找到纳什均衡策略。我们引入了部分基于模型的Eluder维度（P-MBED），这是一种更有效的概念来描述模型类复杂度。值得注意的是，P-MBED可以衡量从给定的平均场模型类转换而来的单个智能体模型类的复杂度，并且潜在上可能比\citet{huang2023statistical}提出的MBED指数级低。我们提出了一种模型消除算法，具有新颖的探索策略，并建立了与P-MBED相关的样本复杂度结果，这些结果表明，在基本可实现性和Lipschitz连续性假设下，学习平均场博弈的纳什均衡并不比解决对数个单个智能体强化学习问题更具统计挑战性。我们进一步将我们的结果推广到多类型平均场博弈。

    We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
    
[^19]: Stein Boltzmann抽样：一种全局优化的变分方法

    Stein Boltzmann Sampling: A Variational Approach for Global Optimization

    [https://arxiv.org/abs/2402.04689](https://arxiv.org/abs/2402.04689)

    Stein Boltzmann抽样（SBS）是一种全局优化的变分方法，通过从Boltzmann分布中采样，由Stein Variational Gradient Descent算法实现，具有渐近收敛性，并在各种基准函数上比较了几种最先进的全局优化算法，尤其适合作为高效全局优化方法的延续，能够产生更好的解决方案并有效利用预算。

    

    在本文中，我们介绍了一种新的基于流的方法，用于Lipschitz函数的全局优化，称为Stein Boltzmann抽样（SBS）。我们的方法从Boltzmann分布中采样，该分布在优化函数的最小值集合上渐近均匀。候选解通过Stein Variational Gradient Descent算法进行采样。我们证明了我们方法的渐近收敛性，引入了两种SBS变体，并在各种基准函数上与几种最先进的全局优化算法进行了详细比较。我们方法的设计、理论结果和实验表明，SBS特别适合作为高效全局优化方法的延续，因为它可以在很好地利用预算的同时产生更好的解决方案。

    In this paper, we introduce a new flow-based method for global optimization of Lipschitz functions, called Stein Boltzmann Sampling (SBS). Our method samples from the Boltzmann distribution that becomes asymptotically uniform over the set of the minimizers of the function to be optimized. Candidate solutions are sampled via the \emph{Stein Variational Gradient Descent} algorithm. We prove the asymptotic convergence of our method, introduce two SBS variants, and provide a detailed comparison with several state-of-the-art global optimization algorithms on various benchmark functions. The design of our method, the theoretical results, and our experiments, suggest that SBS is particularly well-suited to be used as a continuation of efficient global optimization methods as it can produce better solutions while making a good use of the budget.
    
[^20]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^21]: 离线约束强化学习中一种基本对偶算法在低秩MDPs上的应用

    A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs

    [https://arxiv.org/abs/2402.04493](https://arxiv.org/abs/2402.04493)

    这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。

    

    离线强化学习旨在通过预先收集的数据集学习一种最大化期望累积奖励的策略。最近，对于低秩MDPs或一般函数逼近的离线强化学习进行了广泛研究，但是现有算法在找到$\epsilon$-优化策略的样本复杂度为$O(\epsilon^{-2})$时，要么需要均匀的数据覆盖假设，要么计算效率低下。在本文中，我们提出了一种在折扣无穷时段设置下，用于低秩MDPs的离线强化学习的基本对偶算法。我们的算法是在部分数据覆盖假设下，该设置中第一个样本复杂度达到$O(\epsilon^{-2})$的计算有效的算法。这优于最近的一项工作，其需要$O(\epsilon^{-4})$个样本。此外，我们的算法通过支持额外奖励信号的约束，将之前的工作扩展到离线约束强化学习设置中。

    Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.
    
[^22]: SMOTE的理论和实验研究：关于重新平衡策略的限制和比较

    Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies

    [https://arxiv.org/abs/2402.03819](https://arxiv.org/abs/2402.03819)

    SMOTE是一种处理不平衡数据集的常用重新平衡策略，它通过复制原始少数样本来重新生成原始分布。本研究证明了SMOTE的密度在少数样本分布的边界附近逐渐减小，从而验证了BorderLine SMOTE策略的合理性。此外，研究还提出了两种新的SMOTE相关策略，并与其他重新平衡方法进行了比较。最终发现，在数据集极度不平衡的情况下，SMOTE、提出的方法或欠采样程序是最佳的策略。

    

    SMOTE（Synthetic Minority Oversampling Technique）是处理不平衡数据集常用的重新平衡策略。我们证明了在渐进情况下，SMOTE（默认参数）通过简单复制原始少数样本来重新生成原始分布。我们还证明了在少数样本分布的支持边界附近，SMOTE的密度会减小，从而验证了常见的BorderLine SMOTE策略。随后，我们提出了两种新的SMOTE相关策略，并将它们与现有的重新平衡方法进行了比较。我们发现，只有当数据集极度不平衡时才需要重新平衡策略。对于这种数据集，SMOTE、我们提出的方法或欠采样程序是最佳的策略。

    Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
    
[^23]: 变分自动编码器进行异常检测的统计测试

    Statistical Test for Anomaly Detections by Variational Auto-Encoders

    [https://arxiv.org/abs/2402.03724](https://arxiv.org/abs/2402.03724)

    本研究提出了一种利用变分自动编码器进行异常检测的统计测试方法（VAE-AD测试），通过量化异常区域的可靠性，可以控制误检的概率到所期望的水平。

    

    在本研究中，我们考虑使用变分自动编码器（VAE）进行异常检测（AD）的可靠性评估。在过去的十年里，基于VAE的AD已经在各个角度进行了积极的研究，从方法开发到应用研究。然而，当AD的结果用于高风险的决策时，如医学诊断，需要确保检测到的异常的可靠性。在本研究中，我们提出了VAE-AD测试作为在统计检验框架下量化基于VAE的AD的统计可靠性的方法。利用VAE-AD测试，可以以p值的形式量化VAE检测到的异常区域的可靠性。这意味着，如果在p值低于某个阈值时宣布为异常，则可以将误检的概率控制在所期望的水平。由于VAE-AD测试是基于一种称为选择性推理的新统计推断框架构建的，其有效性是确保被证明的。

    In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is 
    
[^24]: 图机器学习基础的未来方向

    Future Directions in Foundations of Graph Machine Learning

    [https://arxiv.org/abs/2402.02287](https://arxiv.org/abs/2402.02287)

    图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。

    

    随着图数据在不同学科（从生命科学到社会科学和工程科学）上的广泛应用，图机器学习，尤其是使用图神经网络（GNNs），引起了人们浓厚的兴趣。尽管在实际应用中取得了成功，但我们对GNNs性质的理论理解仍然非常不完整。最近的理论发展主要集中在阐明GNNs粗粒度表达能力方面，主要采用组合技巧。然而，这些研究与实践并不完全一致，特别是在使用随机一阶优化技术训练GNNs时，对GNNs的泛化行为的理解。在这篇定位论文中，我们认为图机器学习领域需要将注意力转移到发展一个更加均衡的图机器学习理论上来，重点关注表达能力、泛化和优化的相互关系的更全面的理解。

    Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
    
[^25]: 《在大规模人工智能时代的贝叶斯深度学习》的立场论文

    Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI

    [https://arxiv.org/abs/2402.00809](https://arxiv.org/abs/2402.00809)

    《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。

    

    在当前的深度学习研究领域中，人们主要关注在涉及大规模图像和语言数据集的监督任务中实现高预测准确性。然而，更广泛的视角揭示了许多被忽视的度量标准、任务和数据类型，如不确定性、主动和持续学习以及科学数据，这些方面需要关注。贝叶斯深度学习（BDL）是一条有前景的道路，可以在这些不同的设置中提供优势。本文认为BDL可以提升深度学习的能力。它重新审视了BDL的优势、承认了现有的挑战，并重点介绍了一些旨在解决这些障碍的有趣的研究方向。展望未来，讨论集中在可能的方式上，将大规模基础模型与BDL相结合，以充分发挥它们的潜力。

    In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
    
[^26]: 具有部分动态知识的样本高效强化学习

    Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge

    [https://arxiv.org/abs/2312.12558](https://arxiv.org/abs/2312.12558)

    研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。

    

    在本文中，我们研究了在线Q学习方法的样本复杂度，当某些关于动态的先前知识可用或可以有效学习时。我们专注于按照加性干扰模型演变的系统，在有限的分集马尔可夫决策过程设置下，我们提出了一种乐观的Q学习算法，在对$f$的完美知识条件下实现了$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$的遗憾，其中$T$是与系统进行交互的总次数。

    arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with
    
[^27]: 使用SHAP和LIME进行可证明稳定的特征排名

    Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])

    [http://arxiv.org/abs/2401.15800](http://arxiv.org/abs/2401.15800)

    这项研究提出了一种通过利用多重假设检验的思想，来设计可靠地排名机器学习模型中最重要特征的特征归因方法，旨在解决SHAP和LIME等常用方法由于随机采样导致的高度不稳定性问题。实验证明了该方法的有效性和计算效率。

    

    特征归因是了解机器学习模型预测的普遍工具。然而，用于评分输入变量的常用方法，如SHAP和LIME，由于随机采样而具有高度不稳定性。借鉴多重假设检验的思想，我们设计了能够以高概率正确排名最重要特征的归因方法。我们的算法RankSHAP保证$K$个最高Shapley值具有超过$1-\alpha$的正确排序概率。实证结果证明了其有效性和令人印象深刻的计算效率。我们还在之前的工作基础上为LIME提供了类似的结果，确保以正确顺序选择最重要的特征。

    Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
    
[^28]: 具有结构化变分族的可证伸缩性黑盒变分推断

    Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])

    [http://arxiv.org/abs/2401.10989](http://arxiv.org/abs/2401.10989)

    本文研究了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族，并通过理论证明结构化变分族可以在迭代复杂性上表现更好，缩放效果更好。

    

    已知具有满秩协方差逼近的变分族在黑盒变分推断中表现不佳，无论是从实证上还是理论上。事实上，最近对黑盒变分推断的计算复杂性结果表明，与均值场变分族相比，满秩变分族在问题的维度上扩展得很差。这对具有本地变量的分层贝叶斯模型尤为关键，它们的维度随着数据集的大小而增加。因此，迭代复杂性对数据集大小N存在明确的O(N^2)依赖。在本文中，我们探索了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族。我们严格证明了某些尺度矩阵结构可以实现更好的迭代复杂性O(N)，从而与N的缩放更好地匹配。我们在现实中验证了我们的理论结果

    Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
    
[^29]: 健壮和共轭高斯过程回归

    Robust and Conjugate Gaussian Process Regression. (arXiv:2311.00463v1 [stat.ML])

    [http://arxiv.org/abs/2311.00463](http://arxiv.org/abs/2311.00463)

    本文提出了一种健壮和共轭的高斯过程（RCGP）回归方法，通过泛化贝叶斯推断实现了可靠的闭式更新，适用于各种实际应用场景。

    

    为了实现闭式条件，高斯过程（GP）回归的常见假设是独立同分布的高斯观测噪声。然而，这种强假设在实际中经常被违反，导致不可靠的推断和不确定性量化。本文中，我们展示了如何使用泛化贝叶斯推断以几乎没有额外代价实现可靠和共轭的高斯过程（RCGP）回归。RCGP具有很高的灵活性，可以在标准GP适用的所有情况下进行精确的共轭闭式更新。为了展示其强大的实证性能，我们将RCGP应用于从贝叶斯优化到稀疏变分高斯过程的各种问题中。

    To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.
    
[^30]: Local Discovery by Partitioning: 在有限先验知识下的多项式时间因果发现方法

    Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])

    [http://arxiv.org/abs/2310.17816](http://arxiv.org/abs/2310.17816)

    在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。

    

    该研究解决了在有限先验知识下自动变量选择的问题。给定一个{X,Y}的曝光-结果对和一个未知因果结构的变量集合Z，局部分区发现（LDP）算法将Z划分成与{X,Y}相关的子集。我们列举了任意Z的8个穷举且互不重复的分区，并利用这个分类法区分混淆因素和其他变量类型。LDP的动机是有效的调整集识别，但避免了自动变量选择方法中常见的预处理假设。我们提供了理论保证，LDP对于满足足够图形条件的任何Z都返回一个有效的调整集。在更强的条件下，我们证明了分区标签的渐近正确性。总独立性测试在|Z|的最坏情况下是二次的，经验上观察到次二次的运行时间。我们在合成数据上对理论保证进行了数值验证。

    This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
    
[^31]: 通过二阶透镜看Adam

    Adam through a Second-Order Lens. (arXiv:2310.14963v1 [cs.LG])

    [http://arxiv.org/abs/2310.14963](http://arxiv.org/abs/2310.14963)

    该论文提出了AdamQLR，它是一个通过将K-FAC中的技术与Adam的更新方法相结合的优化器，通过考虑二阶数据上的Adam行为而得到启发。在回归和分类任务上进行了评估，结果显示AdamQLR在运行时间和推广性能方面表现出良好的竞争力。

    

    深度学习优化研究存在一种紧张状态，即第一阶梯度法（如SGD和Adam）的计算效率与第二阶曲率法（如拟牛顿方法和K-FAC）的理论效率之间的紧张关系。我们试图将这两种方法的优点结合到一个计算上高效的算法中。注意到二阶方法通常依赖于稳定的启发式方法（如Levenberg-Marquardt阻尼），我们提出AdamQLR：一个将K-FAC中的阻尼和学习率选择技术与Adam提出的更新方向相结合的优化器，通过考虑Adam在二阶数据上的表现而得到启发。我们在各种规模的回归和分类任务上评估了AdamQLR，在运行时间与竞争性推广性能之间取得了竞争性的结果。

    Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical efficiency of second-order, curvature-based methods (such as quasi-Newton methods and K-FAC). We seek to combine the benefits of both approaches into a single computationally-efficient algorithm. Noting that second-order methods often depend on stabilising heuristics (such as Levenberg-Marquardt damping), we propose AdamQLR: an optimiser combining damping and learning rate selection techniques from K-FAC (Martens and Grosse, 2015) with the update directions proposed by Adam, inspired by considering Adam through a second-order lens. We evaluate AdamQLR on a range of regression and classification tasks at various scales, achieving competitive generalisation performance vs runtime.
    
[^32]: 机器学习中的鼓励对象选择：在学生助学金续签领域实验中的因果与预测性目标定位

    Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal. (arXiv:2310.08672v1 [econ.EM])

    [http://arxiv.org/abs/2310.08672](http://arxiv.org/abs/2310.08672)

    通过对学生进行因果与预测性定位的比较，研究探讨了在学生助学金续签领域实验中鼓励对象选择的价值。这项大规模实地实验揭示了定位干预对于不同学生的效果，为干预策略的优化提供了参考。

    

    在许多情境下，干预可能对某些人比其他人更有效，因此定位干预可能是有益的。我们通过一个规模庞大的实地实验（超过53,000名大学生）来分析在学生助学金续签前使用“鼓励”策略的价值。我们首先使用基线方法进行定位。首先，我们基于一个估计异质处理效应的因果森林进行定位，并根据估计出的拥有最高处理效应的学生来进行处理。接下来，我们评估两种替代的定位策略，一种是针对在没有干预的情况下预测到低助学金续签概率的学生，另一种是针对预测到高概率的学生。预测的基线结果并不是定位的理想标准，而且在先验上也不清楚是优先考虑低、高还是中间的预测。

    In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predic
    
[^33]: 神经扩散模型

    Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])

    [http://arxiv.org/abs/2310.08337](http://arxiv.org/abs/2310.08337)

    本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。

    

    扩散模型在许多生成任务上表现出色。然而，尽管最近取得了一些成功，大多数扩散模型只允许对数据分布进行线性转换，受到了一定的限制。相比之下，更广泛的变换家族可能有助于更有效地训练生成分布，简化逆过程并缩小真实负对数似然和变分近似之间的差距。本文介绍了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在一个无需模拟的设置中使用变分界对NDMs进行优化。此外，我们导出了NDMs的时间连续形式，通过使用现成的数值ODE和SDE求解器，可以快速可靠地进行推理。最后，我们通过在标准图像生成任务上的实验展示了可学习变换的NDMs的实用性。

    Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
    
[^34]: 在流形上通过黎曼扩散过程的混合生成模型

    Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes. (arXiv:2310.07216v1 [cs.LG])

    [http://arxiv.org/abs/2310.07216](http://arxiv.org/abs/2310.07216)

    通过混合黎曼扩散过程的原则性框架，我们提出了一种在流形上构建生成过程的方法，与现有的生成模型相比，该方法具有更高的效率和更广泛的适用性。

    

    在非欧几里得空间中建模数据的分布对于来自不同科学领域的许多应用都至关重要。然而，现有的流形上的生成模型存在着计算复杂的散度或依赖于热核的近似的问题。这些限制限制了它们在简单几何形状上的适用性，并阻碍了在高维空间中的可扩展性。在这项工作中，我们引入了黎曼扩散混合模型，这是一个在流形上构建生成过程的原则性框架，它是一组以端点条件扩散过程作为混合的生成过程，而不依赖于先前扩散模型的去噪方法，对于这些模型，生成过程的特性是它的漂移导向与流形的几何形状相对应的最可能的终点。我们进一步提出了一个简单而高效的训练目标，用于学习混合过程，它可以直接应用于一般流形。我们的方法表现得很好。

    Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outp
    
[^35]: 学习温度条件下尺度标量化的GFlowNets

    Learning to Scale Logits for Temperature-Conditional GFlowNets. (arXiv:2310.02823v1 [cs.LG])

    [http://arxiv.org/abs/2310.02823](http://arxiv.org/abs/2310.02823)

    这项研究提出了一种名为LSL-GFN的新型架构设计，可以大大加速温度条件下GFlowNets的训练，从而提高GFlowNets的探索和利用能力。

    

    GFlowNets是一种概率模型，通过学习随机策略来顺序生成组合结构，例如分子图。它们的训练目标是按比例采样具有相应温度调节的对象的奖励。在GFlowNets中，温度条件下的GFlowNets代表了一系列由温度索引的策略，每个策略与相应的温度调节奖励函数相关联。温度条件下的GFlowNets的主要优势在于通过调整温度来控制对GFlowNets的探索和利用。我们提出了一种名为学习温度条件下尺度标量化的GFlowNets（LSL-GFN）的新型架构设计，它极大地加速了温度条件下GFlowNets的训练。它基于一个思想，即之前提出的温度条件方法在深度网络的训练中引入了数值挑战，因为不同的温度可能导致非常不同的情况。

    GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very differe
    
[^36]: 对称性导致学习的结构性约束

    Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])

    [http://arxiv.org/abs/2309.16932](http://arxiv.org/abs/2309.16932)

    本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。

    

    由于常见的架构设计，对称性在当代神经网络中广泛存在。在这项工作中，我们揭示了损失函数对称性对影响机器学习模型的学习行为的重要性。我们证明了损失函数的每个镜像对称性都会导致一种结构性约束，当权重衰减或梯度噪声较大时，这种约束将成为首选解。作为直接推论，我们展示了重新缩放对称性导致稀疏性，旋转对称性导致低秩性，置换对称性导致同质集成。然后，我们展示了理论框架可以解释神经网络中的可塑性丧失和各种崩溃现象，并提出了如何利用对称性设计可微分实施硬性约束的算法。

    Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
    
[^37]: PAGER: 一种用于深度回归模型故障分析的框架

    PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])

    [http://arxiv.org/abs/2309.10977](http://arxiv.org/abs/2309.10977)

    PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。

    

    安全部署AI模型需要主动检测潜在的预测故障，以防止昂贵的错误。尽管分类问题的故障检测已经引起了广泛关注，但在回归任务中表征故障模式更加复杂且较少研究。现有方法依赖于认识不确定性或与训练分布的特征不一致来表征模型风险。然而，我们表明，仅靠不确定性无法准确表征故障，这是由于各种误差源的存在。在本文中，我们提出了PAGER（回归器的原则性泛化错误分析），这是一个系统检测和表征深度回归模型故障的框架。基于最近提出的深度模型锚定思想，PAGER将认识不确定性和新颖的、互补的不一致分数统一起来，将样本组织成不同的风险区域，从而提供全面的分析。

    Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys
    
[^38]: 核去偏插值估计

    Kernel Debiased Plug-in Estimation. (arXiv:2306.08598v1 [stat.ME])

    [http://arxiv.org/abs/2306.08598](http://arxiv.org/abs/2306.08598)

    本文提出了一种高效、不需要实现影响函数且可计算的去偏插值估计方法。

    

    本文考虑在干扰参数存在的情况下估计标量目标参数的问题。采用非参数估计器（例如机器学习（ML）模型）替换未知干扰参数是方便的，但因存在较大偏差而效率低下。为了避免偏差-方差权衡的次优选择，现代方法会进行插值预估的去偏差操作，如有目标最小损失估计（TMLE）和双机器学习（DML）等。现有的去偏方法需要将目标参数的影响函数（IF）作为输入，然而，IF的推导需要专业知识，从而阻碍了这些方法的适应性。我们提出了一种新的去偏插入估计器的方法，它（i）高效、（ii）不需要实现IF、（iii）可计算。

    We consider the problem of estimating a scalar target parameter in the presence of nuisance parameters. Replacing the unknown nuisance parameter with a nonparametric estimator, e.g.,a machine learning (ML) model, is convenient but has shown to be inefficient due to large biases. Modern methods, such as the targeted minimum loss-based estimation (TMLE) and double machine learning (DML), achieve optimal performance under flexible assumptions by harnessing ML estimates while mitigating the plug-in bias. To avoid a sub-optimal bias-variance trade-off, these methods perform a debiasing step of the plug-in pre-estimate. Existing debiasing methods require the influence function of the target parameter as input. However, deriving the IF requires specialized expertise and thus obstructs the adaptation of these methods by practitioners. We propose a novel way to debias plug-in estimators which (i) is efficient, (ii) does not require the IF to be implemented, (iii) is computationally tractable, a
    
[^39]: 利用欧几里得距离函数解释和改进扩散模型

    Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])

    [http://arxiv.org/abs/2306.04848](http://arxiv.org/abs/2306.04848)

    本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。

    

    去噪直觉上与投影有关。事实上，在流形假设下，添加随机噪声近似等价于正交扰动。因此，学习去噪近似于学习投影。本文利用这一观察结果，将去噪扩散模型解释为应用于欧几里得距离函数的近似梯度下降。随后，我们基于对去噪器投影误差的简单假设，提供DDIM（Denoising Diffusion Implicit Models）采样器的简单收敛分析。最后，我们基于理论结果的洞见提出一种基于对DDIM的两个简单修改的新采样器。仅需要5-10个函数评估，我们的采样器就能在预训练的CIFAR-10和CelebA模型上达到最先进的FID得分，并且可以在潜在扩散模型上生成高质量的样本。

    Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
    
[^40]: 关于黎曼流形上无投影在线学习的研究

    On Riemannian Projection-free Online Learning. (arXiv:2305.19349v1 [cs.LG])

    [http://arxiv.org/abs/2305.19349](http://arxiv.org/abs/2305.19349)

    本文提出了一种针对非凸约束集情况下的曲线空间在线测地凸优化的无投影算法，获得了次线性遗憾保证。

    

    投影操作是许多优化算法（例如在线梯度下降[OGD]）中强制约束和实现最优遗憾边界所必需的关键组成部分。然而，当处理高维设置或具有病态约束集时，它会受到计算复杂度限制。无投影算法通过用更有效的优化子程序取代投影预测来解决此问题。但到目前为止，这些方法主要在欧几里得设置中开发，并且虽然越来越多地关注黎曼流形上的优化，但在尝试利用无投影工具方面基本上没有工作。一个明显的问题是，在这些领域中，非平凡的仿射函数通常是非凸的。在本文中，我们提出了一种方法，在曲线空间上进行在线测地凸优化，以获得两种情况下的次线性遗憾保证：当我们访问（a）时

    The projection operation is a critical component in a wide range of optimization algorithms, such as online gradient descent (OGD), for enforcing constraints and achieving optimal regret bounds. However, it suffers from computational complexity limitations in high-dimensional settings or when dealing with ill-conditioned constraint sets. Projection-free algorithms address this issue by replacing the projection oracle with more efficient optimization subroutines. But to date, these methods have been developed primarily in the Euclidean setting, and while there has been growing interest in optimization on Riemannian manifolds, there has been essentially no work in trying to utilize projection-free tools here. An apparent issue is that non-trivial affine functions are generally non-convex in such domains. In this paper, we present methods for obtaining sub-linear regret guarantees in online geodesically convex optimization on curved spaces for two scenarios: when we have access to (a) a s
    
[^41]: 低秩矩阵逼近的梯度下降全局快速收敛性

    Fast global convergence of gradient descent for low-rank matrix approximation. (arXiv:2305.19206v1 [math.OC])

    [http://arxiv.org/abs/2305.19206](http://arxiv.org/abs/2305.19206)

    本文证明了对于低秩矩阵逼近问题，使用小随机初始化的梯度下降方法具有非常快的全局收敛性，尤其是当最大特征值相同时。退化自由算法比Riemannian梯度下降方法更有效，在运行时间上可以减少29％。

    

    本文研究了使用梯度下降求解低秩矩阵逼近问题。我们首先证明了对于对称矩阵逼近，梯度下降的局部线性收敛性。在此基础上，我们证明了梯度下降在使用随机小值初始化时具有非常快的全局收敛性。值得注意的是，我们表明，即使是在中等大小的随机初始化情况下，包括使用小随机初始化的特殊情况在内，当最大特征值相同时，梯度下降在快速收敛方面也具有很好的效果。此外，我们扩展了我们的分析以解决非对称矩阵逼近问题，并调查了一种不依赖投影的特征空间计算方法的有效性。数值实验强烈支持我们的理论。特别地，退化自由算法优于对应的Riemannian梯度下降方法，导致运行时间显着减少了29％。

    This paper investigates gradient descent for solving low-rank matrix approximation problems. We begin by establishing the local linear convergence of gradient descent for symmetric matrix approximation. Building on this result, we prove the rapid global convergence of gradient descent, particularly when initialized with small random values. Remarkably, we show that even with moderate random initialization, which includes small random initialization as a special case, gradient descent achieves fast global convergence in scenarios where the top eigenvalues are identical. Furthermore, we extend our analysis to address asymmetric matrix approximation problems and investigate the effectiveness of a retraction-free eigenspace computation method. Numerical experiments strongly support our theory. In particular, the retraction-free algorithm outperforms the corresponding Riemannian gradient descent method, resulting in a significant 29\% reduction in runtime.
    
[^42]: 基于岭正则化的线性数据最小二乘去噪问题的欠参数化双谷效应

    Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line. (arXiv:2305.14689v1 [stat.ML])

    [http://arxiv.org/abs/2305.14689](http://arxiv.org/abs/2305.14689)

    本文研究了线性数据最小二乘岭正则化的去噪问题，证明了在欠参数化情况下会出现双峰谷现象。

    

    研究了训练数据点数、统计模型参数数和模型的泛化能力之间的关系。已有的工作表明，过度参数化情况下可能出现双峰谷现象，而在欠参数化情况下则普遍存在标准偏差-方差权衡。本文提出了一个简单的例子，可以证明欠参数化情况下可以发生双峰谷现象。考虑嵌入高维空间中的线性数据最小二乘去噪问题中的岭正则化，通过推导出一种渐近准确的广义误差公式，我们发现了样本和参数的双谷效应，双峰谷位于插值点和过度参数化区域之间。此外，样本双谷曲线的高峰对应于估计量的范数曲线的高峰。

    The relationship between the number of training data points, the number of parameters in a statistical model, and the generalization capabilities of the model has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime, and believe that the standard bias-variance trade-off holds in the under-parameterized regime. In this paper, we present a simple example that provably exhibits double descent in the under-parameterized regime. For simplicity, we look at the ridge regularized least squares denoising problem with data on a line embedded in high-dimension space. By deriving an asymptotically accurate formula for the generalization error, we observe sample-wise and parameter-wise double descent with the peak in the under-parameterized regime rather than at the interpolation point or in the over-parameterized regime.  Further, the peak of the sample-wise double descent curve corresponds to a peak in the curve for the norm of the estimator,
    
[^43]: 设计中的反事实：一种模型无关的设计建议方法

    Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations. (arXiv:2305.11308v1 [cs.AI])

    [http://arxiv.org/abs/2305.11308](http://arxiv.org/abs/2305.11308)

    本文介绍了一种多目标设计反事实(MCD)方法，可帮助设计师识别设计修改，提高功能性能。MCD通过支持多目标查询和解耦反事实搜索和采样过程来提高效率并改进现有的反事实搜索方法，证明其在自行车设计案例中的有效性。

    

    本文介绍了一种新型的设计问题反事实优化方法——多目标设计反事实(MCD)。反事实是指可能导致不同决策或选择的假设情况。本文将反事实搜索问题框架化为设计建议工具，可以帮助识别对设计进行修改，从而提高功能性能。MCD通过支持多目标查询和解耦反事实搜索和采样过程来提高效率并促进目标权衡可视化，改进了现有的反事实搜索方法。本文使用二维测试案例证明了MCD的核心功能，然后通过三个自行车设计案例研究展示了MCD在实际设计问题中的有效性。在第一个案例研究中，MCD在推荐对查询设计进行修改方面表现出色，可以显著提高自行车的性能。

    We introduce Multi-Objective Counterfactuals for Design (MCD), a novel method for counterfactual optimization in design problems. Counterfactuals are hypothetical situations that can lead to a different decision or choice. In this paper, the authors frame the counterfactual search problem as a design recommendation tool that can help identify modifications to a design, leading to better functional performance. MCD improves upon existing counterfactual search methods by supporting multi-objective queries, which are crucial in design problems, and by decoupling the counterfactual search and sampling processes, thus enhancing efficiency and facilitating objective tradeoff visualization. The paper demonstrates MCD's core functionality using a two-dimensional test case, followed by three case studies of bicycle design that showcase MCD's effectiveness in real-world design problems. In the first case study, MCD excels at recommending modifications to query designs that can significantly enha
    
[^44]: NUBO：一个透明的 Python 包用于贝叶斯优化

    NUBO: A Transparent Python Package for Bayesian Optimisation. (arXiv:2305.06709v1 [cs.LG])

    [http://arxiv.org/abs/2305.06709](http://arxiv.org/abs/2305.06709)

    NUBO是一个透明的Python包，用于优化昂贵的黑盒函数，它利用高斯过程做代理模型以及获取函数来指导选择候选点，专注于透明度和用户体验。

    

    NUBO（Newcastle University Bayesian Optimisation）是一个贝叶斯优化框架，用于优化昂贵的黑盒函数，比如物理实验和计算机模拟器。它利用高斯过程做代理模型、并通过获取函数来选择用于全局最优化的候选点。NUBO专注于透明度和用户体验，以便让不同领域的研究人员更容易使用贝叶斯优化。

    NUBO, short for Newcastle University Bayesian Optimisation, is a Bayesian optimisation framework for the optimisation of expensive-to-evaluate black-box functions, such as physical experiments and computer simulators. Bayesian optimisation is a cost-efficient optimisation strategy that uses surrogate modelling via Gaussian processes to represent an objective function and acquisition functions to guide the selection of candidate points to approximate the global optimum of the objective function. NUBO itself focuses on transparency and user experience to make Bayesian optimisation easily accessible to researchers from all disciplines. Clean and understandable code, precise references, and thorough documentation ensure transparency, while user experience is ensured by a modular and flexible design, easy-to-write syntax, and careful selection of Bayesian optimisation algorithms. NUBO allows users to tailor Bayesian optimisation to their specific problem by writing the optimisation loop the
    
[^45]: 使用热度图回归进行深度单张图片摄像机标定，在曼哈顿世界假设下在不模糊的情况下还原鱼眼图片

    Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under ManhattanWorld AssumptionWithout Ambiguity. (arXiv:2303.17166v1 [cs.CV])

    [http://arxiv.org/abs/2303.17166](http://arxiv.org/abs/2303.17166)

    本文提出一种基于学习的标定方法，使用热度图回归来消除曼哈顿世界假设下鱼眼图片中横向角度歧义，同时恢复旋转和消除鱼眼失真。该方法使用优化的对角线点缓解图像中缺乏消失点的情况，并在实验证明其性能优于现有技术。

    

    在正交世界坐标系中，曼哈顿世界沿着长方体建筑物广泛用于各种计算机视觉任务。然而，曼哈顿世界需要改进，因为图像中的横向角度的原点是任意的，即具有四倍轮换对称的横向角度的歧义。为了解决这个问题，我们提出了一个基于摄像机和行驶方向的道路方向的平角定义。我们提出了一种基于学习的标定方法，它使用热度图回归来消除歧义，类似于姿态估计关键点。与此同时，我们的两个分支网络恢复旋转并从一般场景图像中消除鱼眼失真。为了缓解图像中缺乏消失点的情况，我们引入了具有空间均匀性最佳的对角线点。大量实验证明，我们的方法在曼哈顿世界假设下对鱼眼图像的深度单张图片摄像机标定优于现有技术，没有歧义。

    In orthogonal world coordinates, a Manhattan world lying along cuboid buildings is widely useful for various computer vision tasks. However, the Manhattan world has much room for improvement because the origin of pan angles from an image is arbitrary, that is, four-fold rotational symmetric ambiguity of pan angles. To address this problem, we propose a definition for the pan-angle origin based on the directions of the roads with respect to a camera and the direction of travel. We propose a learning-based calibration method that uses heatmap regression to remove the ambiguity by each direction of labeled image coordinates, similar to pose estimation keypoints. Simultaneously, our two-branched network recovers the rotation and removes fisheye distortion from a general scene image. To alleviate the lack of vanishing points in images, we introduce auxiliary diagonal points that have the optimal 3D arrangement of spatial uniformity. Extensive experiments demonstrated that our method outperf
    
[^46]: 持续同调在图学习中的表达性

    On the Expressivity of Persistent Homology in Graph Learning. (arXiv:2302.09826v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09826](http://arxiv.org/abs/2302.09826)

    本文通过在图学习任务中的理论讨论和实证分析，证明了持续同调技术在捕捉具有显著拓扑结构的数据集中的长程图性质方面表现出的高表达性。

    

    近来，计算拓扑学中的一项技术，持续同调展现出在图分类方面强大的实证性能。它能够通过高阶拓扑特征——如任意长度的环——以及多尺度拓扑描述符捕捉长程图性质，从而提高对具有显著拓扑结构的数据集——如分子——的预测性能。与此同时，持续同调的理论性质在这个背景下尚未得到正式评估。本文旨在通过提供持续同调在图中的简要介绍以及对其在图学习任务中的表达性进行理论讨论和实证分析，弥合计算拓扑学和图机器学习之间的差距。

    Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.
    
[^47]: 通过元学习在哈密顿流形中识别普遍的神经表示

    Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01168](http://arxiv.org/abs/2212.01168)

    通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。

    

    最近物理学中深度学习的进展集中在通过将物理先验或归纳偏见引入神经网络来发现目标系统的共享表示。然而，这些方法特定于系统，不允许轻松适应由不同物理法则驱动的新物理系统。例如，训练于质点弹簧系统的神经网络无法准确预测双体系统或任何具有不同物理法则的系统的行为。在本研究中，我们使用图神经网络模拟我们的系统，并采用元学习算法使模型在一系列任务中积累经验，并使其适应新的物理系统。我们的方法旨在学习跨各种哈密顿流形的通用表示，这是哈密顿系统数据分布的共同特征。我们使用由不同系统组成的数据集训练模型，每个系统都有其自身固有的动力学，并评估其性能。

    Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
    
[^48]: GFlowNets与变分贝叶斯的因果结构和机制学习

    Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes. (arXiv:2211.02763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02763](http://arxiv.org/abs/2211.02763)

    本文介绍了一种新的方法，使用GFlowNets和变分贝叶斯联合学习因果模型的结构和机制，不仅能够处理非线性和非高斯数据，在模拟数据上也能与几个基线方法相竞争。

    

    贝叶斯因果结构学习旨在学习有向无环图（DAG）上的后验分布和定义父变量和子变量之间关系的机制。本文引入一种新的方法，使用变分贝叶斯联合学习因果模型的结构和机制，称为变分贝叶斯DAG-GFlowNet（VBG）。我们使用GFlowNets扩展了贝叶斯因果结构学习的方法，不仅学习结构的后验分布，还学习线性高斯模型的参数。我们在模拟数据上的结果表明，VBG在建模DAG和机制的后验分布时，不仅能够处理非线性和非高斯数据，而且还能与几个基线方法相竞争。

    Bayesian causal structure learning aims to learn a posterior distribution over directed acyclic graphs (DAGs), and the mechanisms that define the relationship between parent and child variables. By taking a Bayesian approach, it is possible to reason about the uncertainty of the causal model. The notion of modelling the uncertainty over models is particularly crucial for causal structure learning since the model could be unidentifiable when given only a finite amount of observational data. In this paper, we introduce a novel method to jointly learn the structure and mechanisms of the causal model using Variational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We extend the method of Bayesian causal structure learning using GFlowNets to learn not only the posterior distribution over the structure, but also the parameters of a linear-Gaussian model. Our results on simulated data suggest that VBG is competitive against several baselines in modelling the posterior over DAGs an
    
[^49]: 揭开黑盒子：调控算法决策

    Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2110.03443](http://arxiv.org/abs/2110.03443)

    本文研究如何在代理使用复杂的“黑盒”预测函数进行决策的情况下，对算法决策进行最优调控。研究发现，限制代理使用透明度足够高的预测函数是低效的，而针对激励偏差源头的目标化工具可以提供次优解决方案，从而改善福利。

    

    我们展示了如何在一个代理使用复杂的“黑盒”预测函数进行决策（如贷款、医疗测试或招聘）且委托人在了解代理的黑盒模型方面有限的情况下，最优地调控预测算法。我们证明，只要诱导不足，且最优预测函数足够复杂，将代理限制在足够透明的预测函数中是低效的。算法审计有助于提高福利，但其收益取决于审计工具的设计。许多解释工具倾向于最小化整体信息损失，但这通常是低效的，因为它们集中于解释预测函数的平均行为。针对性的工具，如针对激励偏差源头（如过多的假阳性或种族差异）的工具，可以提供次优解决方案。我们提供了对我们理论的实证支持。

    We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
    

