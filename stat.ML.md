# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Likelihood-free neural Bayes estimators for censored peaks-over-threshold models.](http://arxiv.org/abs/2306.15642) | 该论文提出了一种基于神经网络的无似然贝叶斯估计方法，用于构建高效的截尾超阈值模型估计器。该方法挑战了传统的基于截尾似然的空间极值推理，并在计算和统计效率上取得了显著的提升。 |
| [^2] | [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models.](http://arxiv.org/abs/2306.15626) | 本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。 |
| [^3] | [Approximate Message Passing for the Matrix Tensor Product Model.](http://arxiv.org/abs/2306.15580) | 本文提出了一种针对矩阵张量乘积模型的近似传递消息（AMP）算法，并分析了其性能。该算法通过权衡和组合多个估计来优化算法迭代过程。通过对非可分函数的AMP收敛定理和状态演化的研究，我们给出了恢复目标信号所需的必要和充分条件。该算法适用于多种类型的两两观测。 |
| [^4] | [PyBADS: Fast and robust black-box optimization in Python.](http://arxiv.org/abs/2306.15576) | PyBADS是Python中一种快速而稳健的黑盒优化算法，适用于解决目标函数粗糙、计算代价高、可能存在噪声且梯度信息不可用的困难优化问题。它支持高达20个连续输入参数的黑盒函数，并提供易于使用的Python接口。 |
| [^5] | [Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate.](http://arxiv.org/abs/2306.15444) | 有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。 |
| [^6] | [Simulating counterfactuals.](http://arxiv.org/abs/2306.15328) | 该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。 |
| [^7] | [Adaptive Annealed Importance Sampling with Constant Rate Progress.](http://arxiv.org/abs/2306.15283) | 本文提出了一种自适应退火重要性采样算法，通过推导出恒定速率离散化进展计划，实现了在退火过程中样本在各个分布之间的自适应移动，从而提升了采样效率。 |
| [^8] | [Variational Latent Discrete Representation for Time Series Modelling.](http://arxiv.org/abs/2306.15282) | 本文介绍了一种变分潜在离散表示模型，其中离散状态采用马尔可夫链，并在建筑管理数据集和电力变压器数据集上进行了性能评估。 |
| [^9] | [Exploiting Inferential Structure in Neural Processes.](http://arxiv.org/abs/2306.15169) | 这项工作提供了一个框架，利用推理结构在神经过程中进行扩展。我们为NPs的潜变量提供了丰富的先验，并提出了适当的上下文集聚合策略。此外，我们还描述了一种消息传递过程，可以进行端到端优化，并通过使用混合和学生-t假设改善了函数建模和测试时间的鲁棒性。 |
| [^10] | [Wasserstein Generative Regression.](http://arxiv.org/abs/2306.15163) | 我们提出了一种新的统一的非参数回归和条件分布学习方法，使用生成学习框架同时估计回归函数和条件生成器，并使用深度神经网络建模条件生成器。我们的方法可以处理具有多元输出和协变量的问题，可以构建预测区间，并通过理论保证和数值实验证明了方法的有效性和优越性。 |
| [^11] | [Evaluation of machine learning architectures on the quantification of epistemic and aleatoric uncertainties in complex dynamical systems.](http://arxiv.org/abs/2306.15159) | 这项研究评估了多种机器学习技术在复杂动态系统的UQ中的准确性，为降低训练数据集规模和安全因子提供了成本节约的机会。 |
| [^12] | [Off-Policy Evaluation of Ranking Policies under Diverse User Behavior.](http://arxiv.org/abs/2306.15098) | 本文提出了一种新的离策略评估方法Adaptive IPS (AIPS)，针对排名策略的离策略评估问题，通过考虑用户行为的多样性和上下文的变化，有效降低了估计中的偏差和方差。 |
| [^13] | [BatchGFN: Generative Flow Networks for Batch Active Learning.](http://arxiv.org/abs/2306.15058) | BatchGFN是一种用于批量主动学习的新颖方法，通过使用生成流网络根据批量奖励采样数据点集合，能够以原则性的方式构建高度信息量的批量，用于主动学习。通过在推理时间内进行单次前向传递来采样近乎最优效用的批量，减轻了面向批量的算法的计算复杂性，并消除了贪婪近似的需求。提出了跨获取步骤分摊训练的早期结果，实现了对实际任务的扩展。 |
| [^14] | [Optimal Differentially Private Learning with Public Data.](http://arxiv.org/abs/2306.15056) | 本论文研究了具有公共数据的最优差分隐私学习，并解决了在训练差分隐私模型时如何利用公共数据提高准确性的问题。 |
| [^15] | [Equivariant flow matching.](http://arxiv.org/abs/2306.15030) | 本文介绍了一种基于最优输运流匹配的等变CNF训练目标，可以提高等变CNF的可扩展性和实际应用。 |
| [^16] | [Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures.](http://arxiv.org/abs/2306.15012) | 本论文提出了一种用于从噪声混合物中恢复目标信号的统计分量分离方法，并且在图像降噪任务中展示了其优于标准降噪方法的表现。 |
| [^17] | [Uncertainty Estimation for Molecules: Desiderata and Methods.](http://arxiv.org/abs/2306.14916) | 该论文研究了分子力场中不确定性估计的要求和方法，并发现先前的研究方法无法满足所有标准。为了解决这个问题，提出了一种基于局部神经核的高斯过程模型（LNK）。 |
| [^18] | [Molecule Design by Latent Space Energy-Based Modeling and Gradual Distribution Shifting.](http://arxiv.org/abs/2306.14902) | 本文提出了一种概率生成模型来设计具有所需化学和生物性质的分子，并通过逐步分布变换采样算法搜索具有所需性质的分子。实验证明该方法在分子设计任务上表现出很强的性能。 |
| [^19] | [A Bayesian Take on Gaussian Process Networks.](http://arxiv.org/abs/2306.11380) | 该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。 |
| [^20] | [Effect-Invariant Mechanisms for Policy Generalization.](http://arxiv.org/abs/2306.10983) | 本文提出了一种松弛了完全不变性的方法，称为效果不变性，证明它足以进行零样本策略概括，并讨论了基于少量样本的扩展。 |
| [^21] | [Conditional expectation via compact kernels.](http://arxiv.org/abs/2306.10592) | 本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。 |
| [^22] | [Fast Optimal Locally Private Mean Estimation via Random Projections.](http://arxiv.org/abs/2306.04444) | 提出了一种名为ProjUnit的算法框架，用于实现高效的本地隐私均值估计，通过随机投影低维空间实现最优解，且具有低通信复杂度和快速的服务器运行时间。 |
| [^23] | [On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models.](http://arxiv.org/abs/2305.17583) | 本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。 |
| [^24] | [Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation.](http://arxiv.org/abs/2305.06563) | 本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。 |
| [^25] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^26] | [Non-asymptotic convergence bounds for Sinkhorn iterates and their gradients: a coupling approach.](http://arxiv.org/abs/2304.06549) | 本文提出了一种新的耦合方法分析了在$d$维环面$\mathbb{T}_L^d$上定义的在Haar测度相对于密度可允许的概率度量的Sinkhorn算法的收敛性，证明了Sinkhorn迭代及其梯度的点常指数收敛性，同时导出了其非渐近误差界限，这些界限与环境维数$d$和离散元素代价矩阵的尺寸无关。 |
| [^27] | [The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning.](http://arxiv.org/abs/2302.04658) | 本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。 |
| [^28] | [Conformal inference is (almost) free for neural networks trained with early stopping.](http://arxiv.org/abs/2301.11556) | 本文介绍了一种将早停与conformal校准相结合的新方法，以解决使用早停训练的神经网络在缺乏独立校准数据时无法提供准确统计保证的问题。 |
| [^29] | [Explainable Performance: Measuring the Driving Forces of Predictive Performance.](http://arxiv.org/abs/2212.05866) | XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。 |
| [^30] | [Bayesian Learning via Q-Exponential Process.](http://arxiv.org/abs/2210.07987) | 该论文研究了基于Q-指数过程的贝叶斯学习，通过推广Q-指数分布为Q-指数过程，来对函数的L_q正则化进行建模，并选择一致的多元q-指数分布。 |
| [^31] | [Event-Triggered Time-Varying Bayesian Optimization.](http://arxiv.org/abs/2208.10790) | 本文提出了一种事件触发算法ET-GP-UCB，用于解决时变贝叶斯优化中的探索和开发的权衡问题。通过基于高斯过程回归的概率均匀误差界，算法能够在未知变化速率的情况下自适应地适应实际的时间变化。数值实验结果表明，ET-GP-UCB在合成和实际数据上优于现有算法。 |
| [^32] | [The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks.](http://arxiv.org/abs/2112.09036) | 双重PC算法通过利用协方差和精度矩阵之间的反向关系，实现了CI测试，能够恢复正确的等价类，并可对互补调节集的偏相关进行测试。 |
| [^33] | [Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum.](http://arxiv.org/abs/2111.06171) | 本论文研究了带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，展示了SPPAM相比于随机近端点算法（SPPA）具有更快的线性收敛和更好的收缩因子。 |
| [^34] | [Denoising and change point localisation in piecewise-constant high-dimensional regression coefficients.](http://arxiv.org/abs/2110.14298) | 该论文研究了在线性回归模型中融合套索程序的理论性质，并提出了新的设计矩阵受限等距条件，得出了融合套索的估计界限。研究发现，估计误差的主导因素取决于非零系数和分段常数段的数量。 |
| [^35] | [FedPower: Privacy-Preserving Distributed Eigenspace Estimation.](http://arxiv.org/abs/2103.00704) | 本文提出了一种称为FedPower的算法，在联邦学习框架内解决了特征空间估计的隐私和通信效率问题。算法利用幂法进行本地迭代和全局聚合，采用正交Procrustes变换加权以实现对齐，并引入差分隐私以保护数据隐私。 |
| [^36] | [Learning Deep Features in Instrumental Variable Regression.](http://arxiv.org/abs/2010.07154) | 本论文提出了一种名为深度特征工具变量回归（DFIV）的方法，用于处理观测数据中工具变量、处理变量和结果变量之间的非线性关系。通过训练深度神经网络来定义工具变量和处理变量上的非线性特征，我们提供了一种交替训练机制以获得良好的端到端性能。这种方法可以在计算上获得高度灵活的特征映射。 |

# 详细

[^1]: 无似然神经贝叶斯估计的截尾超阈值模型

    Likelihood-free neural Bayes estimators for censored peaks-over-threshold models. (arXiv:2306.15642v1 [stat.ME])

    [http://arxiv.org/abs/2306.15642](http://arxiv.org/abs/2306.15642)

    该论文提出了一种基于神经网络的无似然贝叶斯估计方法，用于构建高效的截尾超阈值模型估计器。该方法挑战了传统的基于截尾似然的空间极值推理，并在计算和统计效率上取得了显著的提升。

    

    在高维度下，对于空间极值依赖模型的推理往往因其依赖于难以处理的或截尾的似然函数而造成计算负担。利用最近在无似然推理方面的进展，我们通过在神经网络架构中编码截尾信息，为截尾超阈值模型构建了高效的估计器。我们的新方法对于传统的基于截尾似然的空间极值推理提出了挑战。我们的模拟研究表明，在推断流行的极值依赖模型（如最大稳定模型、r-帕累托模型和随机比例混合过程）时，相对于竞争的基于似然的方法，我们的新估计器在计算和统计效率方面提供了显著的提升。

    Inference for spatial extremal dependence models can be computationally burdensome in moderate-to-high dimensions due to their reliance on intractable and/or censored likelihoods. Exploiting recent advances in likelihood-free inference with neural Bayes estimators (that is, neural estimators that target Bayes estimators), we develop a novel approach to construct highly efficient estimators for censored peaks-over-threshold models by encoding censoring information in the neural network architecture. Our new method provides a paradigm shift that challenges traditional censored likelihood-based inference for spatial extremes. Our simulation studies highlight significant gains in both computational and statistical efficiency, relative to competing likelihood-based approaches, when applying our novel estimators for inference of popular extremal dependence models, such as max-stable, $r$-Pareto, and random scale mixture processes. We also illustrate that it is possible to train a single esti
    
[^2]: LeanDojo: 检索增强语言模型的定理证明

    LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])

    [http://arxiv.org/abs/2306.15626](http://arxiv.org/abs/2306.15626)

    本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。

    

    大型语言模型（LLM）已经显示出在使用Lean等证明助手证明形式定理方面的潜力。然而，由于私有代码、数据和大量计算要求，现有的方法很难复制或建立在其基础上，这给定理证明的机器学习方法的研究带来了巨大的障碍。本文通过引入LeanDojo来消除这些障碍：一个包含工具包、数据、模型和基准测试的开放源代码的Lean游乐场。LeanDojo从Lean中提取数据，并使得可以通过编程与证明环境进行交互。它包含证明中命题的细粒度注释，为命题选择提供了有价值的数据：这是定理证明中的一个关键瓶颈。利用这些数据，我们开发出了ReProver（检索增强的证明器）：它是第一个使用LLM的证明器，通过检索从庞大的数学库中选择命题。它成本低廉，只需要一周的GPU训练时间。我们的检索器利用了LeanDojo的pro相关功能。

    Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
    
[^3]: 近似传递消息在矩阵张量乘积模型中的应用

    Approximate Message Passing for the Matrix Tensor Product Model. (arXiv:2306.15580v1 [stat.ML])

    [http://arxiv.org/abs/2306.15580](http://arxiv.org/abs/2306.15580)

    本文提出了一种针对矩阵张量乘积模型的近似传递消息（AMP）算法，并分析了其性能。该算法通过权衡和组合多个估计来优化算法迭代过程。通过对非可分函数的AMP收敛定理和状态演化的研究，我们给出了恢复目标信号所需的必要和充分条件。该算法适用于多种类型的两两观测。

    

    我们提出了一种针对矩阵张量乘积模型的近似传递消息（AMP）算法，并进行了分析。该模型是标准尖峰矩阵模型的推广，允许对一组潜在变量进行多种类型的两两观测。该算法的一个关键创新是在每次迭代中通过一种方法对多个估计进行最优权重和组合。借助非可分函数的AMP收敛定理，我们证明了非可分函数的状态演化，从而在高维极限下提供了其性能的渐近精确描述。我们利用这个状态演化结果给出了恢复目标信号所需的必要和充分条件。这些条件取决于从我们模型的适当推广中导出的线性算子的奇异值，这个线性算子是一个适当推广的信噪比的一个重要组成部分。我们的结果作为特殊情况涵盖了一些最近提出的背景模型方法（例如，covariance-）

    We propose and analyze an approximate message passing (AMP) algorithm for the matrix tensor product model, which is a generalization of the standard spiked matrix models that allows for multiple types of pairwise observations over a collection of latent variables. A key innovation for this algorithm is a method for optimally weighing and combining multiple estimates in each iteration. Building upon an AMP convergence theorem for non-separable functions, we prove a state evolution for non-separable functions that provides an asymptotically exact description of its performance in the high-dimensional limit. We leverage this state evolution result to provide necessary and sufficient conditions for recovery of the signal of interest. Such conditions depend on the singular values of a linear operator derived from an appropriate generalization of a signal-to-noise ratio for our model. Our results recover as special cases a number of recently proposed methods for contextual models (e.g., cova
    
[^4]: PyBADS：Python中快速而稳健的黑盒优化

    PyBADS: Fast and robust black-box optimization in Python. (arXiv:2306.15576v1 [stat.ML])

    [http://arxiv.org/abs/2306.15576](http://arxiv.org/abs/2306.15576)

    PyBADS是Python中一种快速而稳健的黑盒优化算法，适用于解决目标函数粗糙、计算代价高、可能存在噪声且梯度信息不可用的困难优化问题。它支持高达20个连续输入参数的黑盒函数，并提供易于使用的Python接口。

    

    PyBADS是Bayesian Adaptive Direct Search（BADS）算法的Python实现，用于快速而稳健的黑盒优化（Acerbi和Ma，2017）。BADS是一种针对目标函数粗糙（非凸、非光滑）、计算代价较高（例如函数评估需要超过0.1秒）、可能存在噪声且梯度信息不可用的困难优化问题而设计的优化算法。通过BADS，这些问题得到了很好的解决，使其成为使用最大似然估计等方法拟合计算模型的优秀选择。该算法在具有高达$D \approx 20$个连续输入参数的黑盒函数上具有高效的扩展性，并支持边界约束或无约束。PyBADS提供了一个易于使用的Python接口，用于运行算法和检查其结果。PyBADS只需要用户提供一个用于评估目标函数的Python函数，以及其他约束（可选）。

    PyBADS is a Python implementation of the Bayesian Adaptive Direct Search (BADS) algorithm for fast and robust black-box optimization (Acerbi and Ma 2017). BADS is an optimization algorithm designed to efficiently solve difficult optimization problems where the objective function is rough (non-convex, non-smooth), mildly expensive (e.g., the function evaluation requires more than 0.1 seconds), possibly noisy, and gradient information is unavailable. With BADS, these issues are well addressed, making it an excellent choice for fitting computational models using methods such as maximum-likelihood estimation. The algorithm scales efficiently to black-box functions with up to $D \approx 20$ continuous input parameters and supports bounds or no constraints. PyBADS comes along with an easy-to-use Pythonic interface for running the algorithm and inspecting its results. PyBADS only requires the user to provide a Python function for evaluating the target function, and optionally other constraint
    
[^5]: 有限内存贪婪拟牛顿方法与非渐进超线性收敛速率

    Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate. (arXiv:2306.15444v1 [math.OC])

    [http://arxiv.org/abs/2306.15444](http://arxiv.org/abs/2306.15444)

    有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。

    

    非渐进收敛分析表明，拟牛顿方法的显式超线性速率为O$((1/\sqrt{t})^t)$。然而，获得这一速率的方法存在一个众所周知的缺点：它们需要存储先前的黑塞近似矩阵，或者存储所有过去的曲率信息以形成当前的黑塞逆近似。有限内存的拟牛顿方法（如著名的L-BFGS）通过利用有限窗口的过去曲率信息来构造黑塞逆近似，从而缓解了这个问题。因此，它们的每次迭代复杂度和存储需求为O$(\tau d)$，其中$\tau \le d$ 是窗口的大小，$d$ 是问题的维数，从而降低了标准拟牛顿方法的O$(d^2)$ 计算成本和内存需求。然而，据我们所知，没有结果表明有限内存拟牛顿方法存在非渐进超线性收敛速率。

    Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit superlinear rate of O$((1/\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is O$(\tau d)$ where $\tau \le d$ is the size of the window and $d$ is the problem dimension reducing the O$(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for a
    
[^6]: 模拟反事实情况

    Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])

    [http://arxiv.org/abs/2306.15328](http://arxiv.org/abs/2306.15328)

    该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。

    

    反事实推断考虑了在与实际世界存在一些证据的平行世界中进行的假设性干预。如果证据在流形上指定了条件分布，反事实可能是解析难解的。我们提出了一种算法，用于从反事实分布中模拟值，其中可以对离散和连续变量设定条件。我们表明，所提出的算法可以被呈现为粒子滤波器，从而导致渐近有效的推断。该算法被应用于信用评分中的公平性分析。

    Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
    
[^7]: 自适应退火重要性采样与恒定速率进展

    Adaptive Annealed Importance Sampling with Constant Rate Progress. (arXiv:2306.15283v1 [stat.ML])

    [http://arxiv.org/abs/2306.15283](http://arxiv.org/abs/2306.15283)

    本文提出了一种自适应退火重要性采样算法，通过推导出恒定速率离散化进展计划，实现了在退火过程中样本在各个分布之间的自适应移动，从而提升了采样效率。

    

    退火重要性采样(AIS)能够在给定难以计算的概率分布的非规范化密度函数的情况下合成加权样本。该算法依赖于一系列插值分布，将目标分布与初始易于计算的分布进行衔接，其中著名的几何平均路径被认为通常是次优的。本文证明了几何退火是当前粒子分布与目标分布之间KL散度最小的路径。我们基于这一观察，推导出了这一退火序列的恒定速率离散化进展计划，根据样本在初始和目标分布之间移动的难度调整进展计划。我们进一步将结果推广至f-散度，并提出了相应的退火序列动力学。

    Annealed Importance Sampling (AIS) synthesizes weighted samples from an intractable distribution given its unnormalized density function. This algorithm relies on a sequence of interpolating distributions bridging the target to an initial tractable distribution such as the well-known geometric mean path of unnormalized distributions which is assumed to be suboptimal in general. In this paper, we prove that the geometric annealing corresponds to the distribution path that minimizes the KL divergence between the current particle distribution and the desired target when the feasible change in the particle distribution is constrained. Following this observation, we derive the constant rate discretization schedule for this annealing sequence, which adjusts the schedule to the difficulty of moving samples between the initial and the target distributions. We further extend our results to $f$-divergences and present the respective dynamics of annealing sequences based on which we propose the C
    
[^8]: 变分潜在离散表示在时间序列建模中的应用

    Variational Latent Discrete Representation for Time Series Modelling. (arXiv:2306.15282v1 [stat.ML])

    [http://arxiv.org/abs/2306.15282](http://arxiv.org/abs/2306.15282)

    本文介绍了一种变分潜在离散表示模型，其中离散状态采用马尔可夫链，并在建筑管理数据集和电力变压器数据集上进行了性能评估。

    

    最近，离散潜在空间模型在深度变分推断中的性能与其连续对应物相媲美。虽然它们仍然面临各种实现挑战，但这些模型为潜在空间的更好解释提供了机会，同时更直接地表示自然离散现象。最近的一些方法建议分别在离散潜在数据上训练非常高维的先验模型，这本身就是一个具有挑战性的任务。在本文中，我们介绍了一种潜在数据模型，其中离散状态是一个马尔可夫链，它允许快速端到端训练。我们对我们的生成模型在建筑管理数据集和公开可用的电力变压器数据集上进行了性能评估。

    Discrete latent space models have recently achieved performance on par with their continuous counterparts in deep variational inference. While they still face various implementation challenges, these models offer the opportunity for a better interpretation of latent spaces, as well as a more direct representation of naturally discrete phenomena. Most recent approaches propose to train separately very high-dimensional prior models on the discrete latent data which is a challenging task on its own. In this paper, we introduce a latent data model where the discrete state is a Markov chain, which allows fast end-to-end training. The performance of our generative model is assessed on a building management dataset and on the publicly available Electricity Transformer Dataset.
    
[^9]: 利用推理结构在神经过程中进行扩展

    Exploiting Inferential Structure in Neural Processes. (arXiv:2306.15169v1 [cs.LG])

    [http://arxiv.org/abs/2306.15169](http://arxiv.org/abs/2306.15169)

    这项工作提供了一个框架，利用推理结构在神经过程中进行扩展。我们为NPs的潜变量提供了丰富的先验，并提出了适当的上下文集聚合策略。此外，我们还描述了一种消息传递过程，可以进行端到端优化，并通过使用混合和学生-t假设改善了函数建模和测试时间的鲁棒性。

    

    神经过程(NPs)由于其能够基于上下文集执行快速适应而具有吸引力。这个集合由一个潜变量编码，通常假设该变量遵循一个简单的分布。然而，在现实世界的情况下，上下文集可能来自具有多个模式、重尾等丰富分布的抽样。在这项工作中，我们提供了一个框架，允许给予NPs潜变量一个由图模型定义的丰富先验。这些分布假设直接转化为适合上下文集的适当聚合策略。此外，我们描述了一种消息传递过程，仍然可以通过随机梯度进行端到端优化。我们通过使用混合和学生-t假设来证明我们框架的普适性，从而改善了函数建模和测试时间的鲁棒性。

    Neural Processes (NPs) are appealing due to their ability to perform fast adaptation based on a context set. This set is encoded by a latent variable, which is often assumed to follow a simple distribution. However, in real-word settings, the context set may be drawn from richer distributions having multiple modes, heavy tails, etc. In this work, we provide a framework that allows NPs' latent variable to be given a rich prior defined by a graphical model. These distributional assumptions directly translate into an appropriate aggregation strategy for the context set. Moreover, we describe a message-passing procedure that still allows for end-to-end optimization with stochastic gradients. We demonstrate the generality of our framework by using mixture and Student-t assumptions that yield improvements in function modelling and test-time robustness.
    
[^10]: Wasserstein生成回归

    Wasserstein Generative Regression. (arXiv:2306.15163v1 [stat.ML])

    [http://arxiv.org/abs/2306.15163](http://arxiv.org/abs/2306.15163)

    我们提出了一种新的统一的非参数回归和条件分布学习方法，使用生成学习框架同时估计回归函数和条件生成器，并使用深度神经网络建模条件生成器。我们的方法可以处理具有多元输出和协变量的问题，可以构建预测区间，并通过理论保证和数值实验证明了方法的有效性和优越性。

    

    在这篇论文中，我们提出了一种新的统一的非参数回归和条件分布学习方法。我们的方法同时估计回归函数和条件生成器，使用生成学习框架，其中条件生成器是一个可以从条件分布生成样本的函数。主要思想是估计一个满足产生良好回归函数估计的约束条件的条件生成器。我们使用深度神经网络来建模条件生成器。我们的方法可以处理具有多元输出和协变量的问题，并且可以用来构建预测区间。我们通过得出非渐近误差界和在适当的假设下我们方法的分布一致性来提供理论保证。我们还使用模拟和实际数据进行数值实验，以展示我们的方法在某些现有方法上的有效性和优越性。

    In this paper, we propose a new and unified approach for nonparametric regression and conditional distribution learning. Our approach simultaneously estimates a regression function and a conditional generator using a generative learning framework, where a conditional generator is a function that can generate samples from a conditional distribution. The main idea is to estimate a conditional generator that satisfies the constraint that it produces a good regression function estimator. We use deep neural networks to model the conditional generator. Our approach can handle problems with multivariate outcomes and covariates, and can be used to construct prediction intervals. We provide theoretical guarantees by deriving non-asymptotic error bounds and the distributional consistency of our approach under suitable assumptions. We also perform numerical experiments with simulated and real data to demonstrate the effectiveness and superiority of our approach over some existing approaches in va
    
[^11]: 在复杂动态系统的UQ中评估机器学习架构对关于认知和测试误差的量化

    Evaluation of machine learning architectures on the quantification of epistemic and aleatoric uncertainties in complex dynamical systems. (arXiv:2306.15159v1 [stat.ML])

    [http://arxiv.org/abs/2306.15159](http://arxiv.org/abs/2306.15159)

    这项研究评估了多种机器学习技术在复杂动态系统的UQ中的准确性，为降低训练数据集规模和安全因子提供了成本节约的机会。

    

    机器学习方法用于构建数据驱动的降阶模型，被广泛应用于工程领域，特别是作为昂贵计算流体力学的替代方法。对于模型可靠性的重要检验是不确定性量化（UQ），它是对模型误差的自我评估。准确的UQ可以通过减少训练数据集的规模和安全因子来实现成本节约，而较差的UQ则会阻碍用户对模型预测的信任。我们研究了几种机器学习技术，包括高斯过程和一系列增强UQ的神经网络：集成神经网络（ENN）、贝叶斯神经网络（BNN）、Dropout神经网络（D-NN）和高斯神经网络（G-NN）。我们使用两个指标来评估UQ准确性（与模型准确性不同）：验证数据上归一化残差的分布和估计分布。

    Machine learning methods for the construction of data-driven reduced order model models are used in an increasing variety of engineering domains, especially as a supplement to expensive computational fluid dynamics for design problems. An important check on the reliability of surrogate models is Uncertainty Quantification (UQ), a self assessed estimate of the model error. Accurate UQ allows for cost savings by reducing both the required size of training data sets and the required safety factors, while poor UQ prevents users from confidently relying on model predictions. We examine several machine learning techniques, including both Gaussian processes and a family UQ-augmented neural networks: Ensemble neural networks (ENN), Bayesian neural networks (BNN), Dropout neural networks (D-NN), and Gaussian neural networks (G-NN). We evaluate UQ accuracy (distinct from model accuracy) using two metrics: the distribution of normalized residuals on validation data, and the distribution of estima
    
[^12]: 不同用户行为下的排名策略的离策略评估

    Off-Policy Evaluation of Ranking Policies under Diverse User Behavior. (arXiv:2306.15098v1 [stat.ML])

    [http://arxiv.org/abs/2306.15098](http://arxiv.org/abs/2306.15098)

    本文提出了一种新的离策略评估方法Adaptive IPS (AIPS)，针对排名策略的离策略评估问题，通过考虑用户行为的多样性和上下文的变化，有效降低了估计中的偏差和方差。

    

    在线平台上到处都是排名界面。因此，对于使用记录数据进行排名策略的准确性能评估的离策略评估（OPE）越来越感兴趣。OPE的一种事实上的方法是倒数倾向得分法（IPS），它提供了一个无偏且一致的值估计。然而，在排名设置中，由于在大型行为空间下具有较高的方差，它变得极其不准确。为了解决这个问题，先前的研究假设用户行为是独立的或级联的，从而产生了一些IPS的排名版本。尽管这些估计器在减少方差方面有一定的效果，但所有现有的估计器都对每个用户应用了一个单一的通用假设，导致过度的偏差和方差。因此，这项工作探索了更通用的公式，其中用户行为是多样的，并且可以根据用户上下文的不同而变化。我们展示出由此产生的估计器，我们称之为自适应IPS（AIPS），可以更准确地进行离策略评估和排名策略。

    Ranking interfaces are everywhere in online platforms. There is thus an ever growing interest in their Off-Policy Evaluation (OPE), aiming towards an accurate performance evaluation of ranking policies using logged data. A de-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides an unbiased and consistent value estimate. However, it becomes extremely inaccurate in the ranking setup due to its high variance under large action spaces. To deal with this problem, previous studies assume either independent or cascade user behavior, resulting in some ranking versions of IPS. While these estimators are somewhat effective in reducing the variance, all existing estimators apply a single universal assumption to every user, causing excessive bias and variance. Therefore, this work explores a far more general formulation where user behavior is diverse and can vary depending on the user context. We show that the resulting estimator, which we call Adaptive IPS (AIPS), can be unb
    
[^13]: BatchGFN: 用于批量主动学习的生成流网络

    BatchGFN: Generative Flow Networks for Batch Active Learning. (arXiv:2306.15058v1 [cs.LG])

    [http://arxiv.org/abs/2306.15058](http://arxiv.org/abs/2306.15058)

    BatchGFN是一种用于批量主动学习的新颖方法，通过使用生成流网络根据批量奖励采样数据点集合，能够以原则性的方式构建高度信息量的批量，用于主动学习。通过在推理时间内进行单次前向传递来采样近乎最优效用的批量，减轻了面向批量的算法的计算复杂性，并消除了贪婪近似的需求。提出了跨获取步骤分摊训练的早期结果，实现了对实际任务的扩展。

    

    我们引入了BatchGFN - 一种新颖的基于池的主动学习方法，该方法使用生成流网络根据批量奖励采样数据点集合。通过适当的奖励函数来量化获取批量的效用，如批量与模型参数之间的联合互信息，BatchGFN能够以原则性的方式构建高度信息量的批量，用于主动学习。我们展示了我们的方法在玩具回归问题中可以在推理时间内通过对批量中每个点进行单次前向传递来采样近乎最优效用的批量，这减轻了面向批量的算法的计算复杂性，并消除了寻找批量奖励最大化器的贪婪近似的需求。我们还提出了跨获取步骤分摊训练的早期结果，这将实现对实际任务的扩展。

    We introduce BatchGFN -- a novel approach for pool-based active learning that uses generative flow networks to sample sets of data points proportional to a batch reward. With an appropriate reward function to quantify the utility of acquiring a batch, such as the joint mutual information between the batch and the model parameters, BatchGFN is able to construct highly informative batches for active learning in a principled way. We show our approach enables sampling near-optimal utility batches at inference time with a single forward pass per point in the batch in toy regression problems. This alleviates the computational complexity of batch-aware algorithms and removes the need for greedy approximations to find maximizers for the batch reward. We also present early results for amortizing training across acquisition steps, which will enable scaling to real-world tasks.
    
[^14]: 具有公共数据的最优差分隐私学习

    Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])

    [http://arxiv.org/abs/2306.15056](http://arxiv.org/abs/2306.15056)

    本论文研究了具有公共数据的最优差分隐私学习，并解决了在训练差分隐私模型时如何利用公共数据提高准确性的问题。

    

    差分隐私能够确保训练机器学习模型不泄漏私密数据。然而，差分隐私的代价是模型的准确性降低或样本复杂度增加。在实践中，我们可能可以访问不涉及隐私问题的辅助公共数据。这促使了最近研究公共数据在提高差分隐私模型准确性方面的作用。在本研究中，我们假设有一定数量的公共数据，并解决以下基本开放问题：1.在有公共数据的情况下，训练基于私有数据集的差分隐私模型的最优（最坏情况）误差是多少？哪些算法是最优的？2.如何利用公共数据在实践中改进差分隐私模型训练？我们在本地模型和中心模型的差分隐私问题下考虑这些问题。为了回答第一个问题，我们证明了对三个基本问题的最优误差率的紧密（最高常数因子）下界和上界。这三个问题是：均值估计，经验风险最小化和凸奇化。

    Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
    
[^15]: 等变流匹配

    Equivariant flow matching. (arXiv:2306.15030v1 [stat.ML])

    [http://arxiv.org/abs/2306.15030](http://arxiv.org/abs/2306.15030)

    本文介绍了一种基于最优输运流匹配的等变CNF训练目标，可以提高等变CNF的可扩展性和实际应用。

    

    标准化流是一类特别适用于物理学中概率分布建模的深度生成模型。其中，流的准确似然性质可以实现对已知目标能量函数的加权重重和无偏观测量的计算。例如，Boltzmann生成器通过训练流生成处于平衡状态的多体系统（如小分子和蛋白质）样本，解决了统计物理学中长期存在的采样问题。为了构建有效的模型，也很关键将目标能量的对称性纳入模型中，这可以通过等变连续标准化流（CNF）来实现。然而，CNF的训练和样本生成的计算开销较大，这限制了它们的可扩展性和实际应用。在本文中，我们引入了等变流匹配，一种新的等变CNF训练目标，其基于最近提出的最优输运流匹配方法。

    Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivarian
    
[^16]: 用于噪声混合物中目标信号恢复的统计分量分离

    Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v1 [stat.ML])

    [http://arxiv.org/abs/2306.15012](http://arxiv.org/abs/2306.15012)

    本论文提出了一种用于从噪声混合物中恢复目标信号的统计分量分离方法，并且在图像降噪任务中展示了其优于标准降噪方法的表现。

    

    当只对给定信号的特定属性感兴趣时，从一个加性混合物中分离信号可能是一个不必要地困难的问题。在本工作中，我们解决了更简单的“统计分量分离”问题，该问题专注于从噪声混合物中恢复目标信号的预定义统计描述量。假设可以获得噪声过程的样本，我们研究了一种方法，该方法旨在使受噪声样本污染的解决方案候选的统计特性与观测的混合物的统计特性匹配。首先，我们使用具有解析可追踪计算的简单示例分析了该方法的行为。然后，我们将其应用于图像降噪环境中，使用了1）基于小波的描述符，2）针对天体物理和ImageNet数据的ConvNet-based描述符。在第一种情况下，我们展示了我们的方法在大多数情况下比标准降噪方法更好地恢复了目标数据的描述符。此外，尽管不是为此目的构建的，它也表现出对目标信号描述符恢复的潜力。

    Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it pe
    
[^17]: 分子的不确定性估计: 期望和方法

    Uncertainty Estimation for Molecules: Desiderata and Methods. (arXiv:2306.14916v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.14916](http://arxiv.org/abs/2306.14916)

    该论文研究了分子力场中不确定性估计的要求和方法，并发现先前的研究方法无法满足所有标准。为了解决这个问题，提出了一种基于局部神经核的高斯过程模型（LNK）。

    

    图神经网络（GNNs）作为量子力学计算的有希望替代方法，在分子动力学（MD）轨迹集合上建立了前所未有的低误差。由于它们快速的推理时间，它们承诺加速计算化学应用。然而，尽管在分布（ID）误差上可低，但这些GNNs在分布外（OOD）样本上可能完全错误。不确定性估计（UE）可以在这种情况下帮助通过传达模型对其预测的确定性。在这里，我们着眼于这个问题，并确定了分子力场不确定性估计中的六个关键要求，三个是“物理信息”方面的，三个是“应用焦点”方面的。为了概述该领域，我们调查了现有的UE方法，并分析它们如何符合这些要求。通过我们的分析，我们得出结论，先前的研究都无法满足所有标准。为了填补这个空白，我们提出了基于局部神经核（LNK）的高斯过程（GP）模型。

    Graph Neural Networks (GNNs) are promising surrogates for quantum mechanical calculations as they establish unprecedented low errors on collections of molecular dynamics (MD) trajectories. Thanks to their fast inference times they promise to accelerate computational chemistry applications. Unfortunately, despite low in-distribution (ID) errors, such GNNs might be horribly wrong for out-of-distribution (OOD) samples. Uncertainty estimation (UE) may aid in such situations by communicating the model's certainty about its prediction. Here, we take a closer look at the problem and identify six key desiderata for UE in molecular force fields, three 'physics-informed' and three 'application-focused' ones. To overview the field, we survey existing methods from the field of UE and analyze how they fit to the set desiderata. By our analysis, we conclude that none of the previous works satisfies all criteria. To fill this gap, we propose Localized Neural Kernel (LNK) a Gaussian Process (GP)-based
    
[^18]: 通过潜在空间能量建模和分布逐步变换进行分子设计

    Molecule Design by Latent Space Energy-Based Modeling and Gradual Distribution Shifting. (arXiv:2306.14902v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.14902](http://arxiv.org/abs/2306.14902)

    本文提出了一种概率生成模型来设计具有所需化学和生物性质的分子，并通过逐步分布变换采样算法搜索具有所需性质的分子。实验证明该方法在分子设计任务上表现出很强的性能。

    

    为了药物研发，生成具有所需化学和生物性质（如高药物样性，高亲和力）的分子至关重要。本文提出了一个概率生成模型，用于捕捉分子和其性质的联合分布。我们的模型在潜在空间中基于能量建模（EBM）。在潜在向量的条件下，分子及其性质被分别建模为分子生成模型和性质回归模型。为了搜索具有所需性质的分子，我们提出了逐步分布变换采样（SGDS）算法，使得在最初学习现有分子及其性质的训练数据后，该算法逐渐将模型分布转移到支持所需性质值的分子区域。我们的实验结果表明，我们的方法在各种分子设计任务上表现出很强的性能。

    Generation of molecules with desired chemical and biological properties such as high drug-likeness, high binding affinity to target proteins, is critical for drug discovery. In this paper, we propose a probabilistic generative model to capture the joint distribution of molecules and their properties. Our model assumes an energy-based model (EBM) in the latent space. Conditional on the latent vector, the molecule and its properties are modeled by a molecule generation model and a property regression model respectively. To search for molecules with desired properties, we propose a sampling with gradual distribution shifting (SGDS) algorithm, so that after learning the model initially on the training data of existing molecules and their properties, the proposed algorithm gradually shifts the model distribution towards the region supported by molecules with desired values of properties. Our experiments show that our method achieves very strong performances on various molecule design tasks.
    
[^19]: 高斯过程网络的贝叶斯方法

    A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])

    [http://arxiv.org/abs/2306.11380](http://arxiv.org/abs/2306.11380)

    该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。

    

    高斯过程网络（GPNs）是一类有向图模型，其使用高斯过程作为网络中每个变量给定其父变量的条件期望的先验分布。该模型允许以紧凑但灵活的方式描述连续联合分布，对变量之间的依赖关系仅做最少的参数假设。GPNs的贝叶斯结构学习需要计算网络结构的后验分布，即使在低维情况下，这也是计算上不可行的。本文实现了蒙特卡罗和马尔可夫链蒙特卡罗方法来从网络结构的后验分布中采样。因此，该方法遵循贝叶斯范式，通过边缘似然比较模型，并计算GPN特征的后验概率。模拟研究表明，我们的方法在恢复网络的图形结构方面优于最先进的算法，并提供其后验的准确近似。

    Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
    
[^20]: 策略概括中的效果不变机制

    Effect-Invariant Mechanisms for Policy Generalization. (arXiv:2306.10983v1 [stat.ML])

    [http://arxiv.org/abs/2306.10983](http://arxiv.org/abs/2306.10983)

    本文提出了一种松弛了完全不变性的方法，称为效果不变性，证明它足以进行零样本策略概括，并讨论了基于少量样本的扩展。

    

    策略学习是许多实际学习系统的重要组成部分。策略学习的一个主要挑战是如何有效地适应未见过的环境或任务。最近，有人建议利用不变的条件分布来学习更好地概括未见过环境的模型。然而，假设整个条件分布是不变的（我们称之为完全不变性），在实践中可能是一个太强的假设。在本文中，我们引入了一种松弛完全不变性的方法，称为效果不变性（简称e-不变性），并证明它是足够的（在适当的假设下），用于零样本策略概括。我们还讨论了一种扩展，它在测试环境中只有少量样本时利用e-不变性，从而实现了少样本策略推广。我们的工作不假设存在一个基础因果图，也不假设数据是由结构因果模型生成的。相反，我们开发了测试过程来测试e-不变性。

    Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance dir
    
[^21]: 基于紧核的条件期望估计

    Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])

    [http://arxiv.org/abs/2306.10592](http://arxiv.org/abs/2306.10592)

    本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。

    

    去噪、条件期望和流形学习任务通常可以在寻找两个随机变量积的条件期望的公共环境下表述。本文针对这个更一般的问题，描述了一种算子理论方法来估计条件期望。核积分算子被用作紧致化工具，将估计问题设置为在再生核希尔伯特空间中的线性逆问题。该方程的解被证明对数值逼近是稳定的，从而确保了数据驱动实现的收敛性。总体技术易于实现，还展示了其在一些实际问题中的成功应用。

    The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
    
[^22]: 通过随机投影快速获得最优的本地隐私均值估计

    Fast Optimal Locally Private Mean Estimation via Random Projections. (arXiv:2306.04444v1 [cs.LG])

    [http://arxiv.org/abs/2306.04444](http://arxiv.org/abs/2306.04444)

    提出了一种名为ProjUnit的算法框架，用于实现高效的本地隐私均值估计，通过随机投影低维空间实现最优解，且具有低通信复杂度和快速的服务器运行时间。

    

    本文研究了欧几里得空间中高维向量的本地隐私均值估计问题。现有算法要么产生次优误差，要么具有高通信和/或运行时间复杂度。我们提出了一种新的算法框架ProjUnit，用于隐私均值估计的算法具有计算效率高、通信复杂度低且误差与最优解之间的差距最大为1 + o(1)。我们的框架实现起来非常简单：每个随机化器将其输入投影到一个随机的低维子空间中，对结果进行归一化，然后在低维空间中运行一个最优算法，例如PrivUnitG。此外，我们展示了通过适当地协调设备之间的随机投影矩阵，可以实现快速的服务器运行时间。我们通过随机投影的性质分析了算法的误差，并研究了两种实例。最后，我们的实验结果表明，ProjUnit相比现有方法具有显著的性能优势，特别是在高维高斯混合数据集上表现出色。

    We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace, normalizes the result, and then runs an optimal algorithm such as PrivUnitG in the lower-dimensional space. In addition, we show that, by appropriately correlating the random projection matrices across devices, we can achieve fast server run-time. We mathematically analyze the error of the algorithm in terms of properties of the random projections, and study two instantiations. Lastly, our experiments for private mean estimation and pr
    
[^23]: 关于神经网络作为无限树状概率图模型的论文研究

    On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])

    [http://arxiv.org/abs/2305.17583](http://arxiv.org/abs/2305.17583)

    本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    

    深度神经网络(DNNs)缺乏概率图模型(PGMs)的精确语义和明确定义的概率解释。本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决这个问题。我们的研究揭示了DNNs在前向传播期间确实执行PGM推断的近似，这与曾经的神经网络描述为核机器或无限大小的高斯过程的现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似。潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
    
[^24]: 基于流形正则化 Tucker 分解的时空交通数据填充方法

    Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])

    [http://arxiv.org/abs/2305.06563](http://arxiv.org/abs/2305.06563)

    本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。

    

    时空交通数据填充(STDI)是数据驱动智能交通系统中不可避免和具有挑战性的任务，在部分观测到的交通数据中估计丢失数据。由于交通数据具有多维和时空性质，我们将丢失数据填充视为张量完成问题。过去十年中，许多关于基于张量分解的 STDI 的研究已经展开。然而，如何利用时空相关性和核张量稀疏性来改善填充性能仍然需要解决。本文重新构造了3/4阶汉克尔张量，并提出了一种创新的流形正则化 Tucker 分解(maniRTD)模型用于STDI。明确地，我们通过引入多维延迟嵌入变换将传感交通状态数据表示为3/4阶张量。然后，ManiRTD使用稀疏正则化项改善了Tucker核的稀疏性，并使用流形正则化和时间约束项来优化张量的填充性能。

    Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
    
[^25]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^26]: 针对Sinkhorn迭代及其梯度的非渐进收敛界限: 一种耦合方法

    Non-asymptotic convergence bounds for Sinkhorn iterates and their gradients: a coupling approach. (arXiv:2304.06549v1 [math.PR])

    [http://arxiv.org/abs/2304.06549](http://arxiv.org/abs/2304.06549)

    本文提出了一种新的耦合方法分析了在$d$维环面$\mathbb{T}_L^d$上定义的在Haar测度相对于密度可允许的概率度量的Sinkhorn算法的收敛性，证明了Sinkhorn迭代及其梯度的点常指数收敛性，同时导出了其非渐近误差界限，这些界限与环境维数$d$和离散元素代价矩阵的尺寸无关。

    

    最近，计算最优传输（OT）作为一个在各个领域应用的强有力的框架出现了。本文着重讨论对原始OT问题的一种放松形式，即熵OT问题，它可以实现高效的实际算法解决方案，即使在高维情况下也是如此。这个公式也被称为Schr\"odinger桥问题，显然与随机最优控制（SOC）相关，并且可以用流行的Sinkhorn算法来解决。对于离散状态空间的情况，已知该算法具有指数收敛性；然而，在更一般的情况下实现类似的收敛速率仍然是一个积极的研究领域。在本文中，我们分析了在$d$维环面$\mathbb{T}_L^d$上定义的在Haar测度相对于密度可允许的概率度量的Sinkhorn算法的收敛性。特别的，在选择一定范围内的熵参数时，我们证明了Sinkhorn迭代及其梯度的点常指数收敛性。我们的证明技术依赖于一种耦合方法，受到[C. E. Brubaker, M. Fathi和G. Peyré, NeurIPS 2020]的最新工作的启发。我们还推导了Sinkhorn算法的新收敛速率，从而导出了Sinkhorn迭代和耦合的非渐近误差界限，这些界限独立于环境维数$d$和离散元素代价矩阵的尺寸。

    Computational optimal transport (OT) has recently emerged as a powerful framework with applications in various fields. In this paper we focus on a relaxation of the original OT problem, the entropic OT problem, which allows to implement efficient and practical algorithmic solutions, even in high dimensional settings. This formulation, also known as the Schr\"odinger Bridge problem, notably connects with Stochastic Optimal Control (SOC) and can be solved with the popular Sinkhorn algorithm. In the case of discrete-state spaces, this algorithm is known to have exponential convergence; however, achieving a similar rate of convergence in a more general setting is still an active area of research. In this work, we analyze the convergence of the Sinkhorn algorithm for probability measures defined on the $d$-dimensional torus $\mathbb{T}_L^d$, that admit densities with respect to the Haar measure of $\mathbb{T}_L^d$. In particular, we prove pointwise exponential convergence of Sinkhorn iterat
    
[^27]: 近似拒绝采样的样本复杂度及其在平滑在线学习中的应用

    The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04658](http://arxiv.org/abs/2302.04658)

    本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。

    

    假设我们可以访问来自分布μ的n个独立样本，并且我们希望输出其中一个样本，使得输出的分布尽可能接近目标分布ν。在这项工作中，我们展示了在所有具有有界f-散度Df(ν|μ)≤D的ν,μ对中，关于n的最优总变差距离由Θ(~(D/f'(n)))给出。之前，这个问题只研究了ν相对于μ的Radon-Nikodym导数一致有界的情况。我们还考虑了似乎非常不同的平滑在线学习领域的一个应用，我们展示了最小化遗憾和具有oracle效率的算法的遗憾即使在对手有边界f-散度（而不是有界Radon-Nikodym导数）的松弛约束下，仍然成立。最后，我们还研究了在均匀估计中用于平均估计的重要性采样的效果。

    Suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\Theta(\frac{D}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $D_f(\nu\|\mu)\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o
    
[^28]: 对于使用早停的神经网络，conformal推理是几乎免费的。

    Conformal inference is (almost) free for neural networks trained with early stopping. (arXiv:2301.11556v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11556](http://arxiv.org/abs/2301.11556)

    本文介绍了一种将早停与conformal校准相结合的新方法，以解决使用早停训练的神经网络在缺乏独立校准数据时无法提供准确统计保证的问题。

    

    基于保留数据的早停是一种流行的正则化技术，用于减轻过拟合并提高神经网络的预测准确性。使用早停训练的模型通常提供相对准确的预测，但除非进一步使用独立的保留数据进行校准，否则它们通常仍然缺乏精确的统计保证。本文通过将早停与conformal校准相结合，同时高效地重复使用相同的保留数据，解决了上述限制。这导致了既准确又能够提供精确预测推断的模型，而无需多次数据拆分或过于保守的调整。为不同的学习任务（异常值检测，多类分类，回归）开发了实际实现，并在真实数据上展示了它们的竞争性能。

    Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.
    
[^29]: 可解释的性能：衡量预测性能的驱动力

    Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.05866](http://arxiv.org/abs/2212.05866)

    XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。

    

    我们引入了XPER（eXplainable PERformance）方法来衡量输入特征对模型预测性能的具体贡献。我们的方法在理论上基于Shapley值，既不依赖于模型，也不依赖于性能度量。此外，XPER可在模型级别或个体级别实现。我们证明XPER具有标准解释性方法（SHAP）的特殊情况。在贷款违约预测应用中，我们展示了如何利用XPER处理异质性问题，并显著提高样本外性能。为此，我们通过基于个体XPER值对他们进行聚类来构建同质化的个体群体。我们发现估计群体特定的模型比一个模型适用于所有个体具有更高的预测精度。

    We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
    
[^30]: 基于Q-指数过程的贝叶斯学习

    Bayesian Learning via Q-Exponential Process. (arXiv:2210.07987v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.07987](http://arxiv.org/abs/2210.07987)

    该论文研究了基于Q-指数过程的贝叶斯学习，通过推广Q-指数分布为Q-指数过程，来对函数的L_q正则化进行建模，并选择一致的多元q-指数分布。

    

    正则化是优化、统计和机器学习中最基础的主题之一。为了在估计参数$u\in\mbR^d$时获得稀疏性，在目标函数中通常会添加$\ell_q$惩罚项，即$\Vert u\Vert_q$。这样的$\ell_q$惩罚对应的概率分布是什么？当我们对函数$u\in L^q$建模时，$\Vert u\Vert_q$对应的正确随机过程是什么？这对于统计建模大维度对象（例如图像）并保留确定性特性（例如图像边缘）的惩罚非常重要。在这项工作中，我们将$Q$-指数分布（密度正比于$\exp{(- \half|u|^q)}$）推广为一种称为\emph{$Q$-指数（Q-EP）过程}的随机过程，它对应于函数的$L_q$正则化。关键步骤是通过从大型椭圆轮廓分布族中选择来指定一致的多元$q$-指数分布。

    Regularization is one of the most fundamental topics in optimization, statistics and machine learning. To get sparsity in estimating a parameter $u\in\mbR^d$, an $\ell_q$ penalty term, $\Vert u\Vert_q$, is usually added to the objective function. What is the probabilistic distribution corresponding to such $\ell_q$ penalty? What is the correct stochastic process corresponding to $\Vert u\Vert_q$ when we model functions $u\in L^q$? This is important for statistically modeling large dimensional objects, e.g. images, with penalty to preserve certainty properties, e.g. edges in the image. In this work, we generalize the $q$-exponential distribution (with density proportional to) $\exp{(- \half|u|^q)}$ to a stochastic process named \emph{$Q$-exponential (Q-EP) process} that corresponds to the $L_q$ regularization of functions. The key step is to specify consistent multivariate $q$-exponential distributions by choosing from a large family of elliptic contour distributions. The work is closel
    
[^31]: 事件触发的时变贝叶斯优化

    Event-Triggered Time-Varying Bayesian Optimization. (arXiv:2208.10790v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10790](http://arxiv.org/abs/2208.10790)

    本文提出了一种事件触发算法ET-GP-UCB，用于解决时变贝叶斯优化中的探索和开发的权衡问题。通过基于高斯过程回归的概率均匀误差界，算法能够在未知变化速率的情况下自适应地适应实际的时间变化。数值实验结果表明，ET-GP-UCB在合成和实际数据上优于现有算法。

    

    我们考虑使用时变贝叶斯优化（TVBO）顺序优化时变目标函数的问题。其中，关键挑战是在时间变化下的勘探与开发的权衡。当前的TVBO方法需要对变化速率有先验知识。然而，在实践中，变化速率通常是未知的。我们提出了一种事件触发算法ET-GP-UCB，它将优化问题视为静态问题，直到在线检测到目标函数的变化并重置数据集。这使得算法能够适应实际的时间变化，而不需要先验知识。事件触发基于高斯过程回归中使用的概率均匀误差界。我们给出了ET-GP-UCB的遗憾界，并通过数值实验表明，它在合成和实际数据上优于现有算法。此外，这些结果表明ET-GP-UCB可广泛应用于不同的设定。

    We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Here, the key challenge is the exploration-exploitation trade-off under time variations. Current approaches to TVBO require prior knowledge of a constant rate of change. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function online and then resets the dataset. This allows the algorithm to adapt to realized temporal changes without the need for prior knowledge. The event-trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We provide regret bounds for ET-GP-UCB and show in numerical experiments that it outperforms state-of-the-art algorithms on synthetic and real-world data. Furthermore, these results demonstrate that ET-GP-UCB is readily applicable to various set
    
[^32]: 双重PC算法及其对高斯性质在贝叶斯网络结构学习中的作用

    The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks. (arXiv:2112.09036v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.09036](http://arxiv.org/abs/2112.09036)

    双重PC算法通过利用协方差和精度矩阵之间的反向关系，实现了CI测试，能够恢复正确的等价类，并可对互补调节集的偏相关进行测试。

    

    学习贝叶斯网络的图形结构是描述许多复杂应用程序中的数据生成机制的关键，但面临着巨大的计算挑战。在某些假设下，流行的PC算法可以通过逆向工程变量分布中所具有的条件独立关系来一致地恢复正确的等价类。双重PC算法是一种新颖的方案，通过利用协方差和精度矩阵之间的反向关系来进行PC算法中的CI测试。通过利用块矩阵求逆，我们还可以对互补（或双重）调节集的偏相关进行测试。双重PC算法的多个CI测试首先考虑边缘和完全排序CI关系。

    Learning the graphical structure of Bayesian networks is key to describing data-generating mechanisms in many complex applications but poses considerable computational challenges. Observational data can only identify the equivalence class of the directed acyclic graph underlying a Bayesian network model, and a variety of methods exist to tackle the problem. Under certain assumptions, the popular PC algorithm can consistently recover the correct equivalence class by reverse-engineering the conditional independence (CI) relationships holding in the variable distribution. The dual PC algorithm is a novel scheme to carry out the CI tests within the PC algorithm by leveraging the inverse relationship between covariance and precision matrices. By exploiting block matrix inversions we can also perform tests on partial correlations of complementary (or dual) conditioning sets. The multiple CI tests of the dual PC algorithm proceed by first considering marginal and full-order CI relationships a
    
[^33]: 带有动量的随机近端点算法的收敛性和稳定性研究

    Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum. (arXiv:2111.06171v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2111.06171](http://arxiv.org/abs/2111.06171)

    本论文研究了带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，展示了SPPAM相比于随机近端点算法（SPPA）具有更快的线性收敛和更好的收缩因子。

    

    随机梯度下降与动量（SGDM）是许多优化场景中的主要算法，包括凸优化和非凸神经网络训练。然而，在随机设置中，动量会干扰梯度噪声，通常需要特定的步长和动量选择才能保证收敛，但加速往往被忽略。相对而言，近端点方法因其数值稳定性和对不完美调整的弹性而受到了广泛关注。然而，它们的随机加速变体却受到了限制的关注：动量如何影响（随机）近端点方法的稳定性仍然未经研究。为了解决这个问题，我们关注带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，并展示了SPPAM相比于带有更好收缩因子的随机近端点算法（SPPA）具有更快的线性收敛到一个邻域。

    Stochastic gradient descent with momentum (SGDM) is the dominant algorithm in many optimization scenarios, including convex optimization instances and non-convex neural network training. Yet, in the stochastic setting, momentum interferes with gradient noise, often leading to specific step size and momentum choices in order to guarantee convergence, set aside acceleration. Proximal point methods, on the other hand, have gained much attention due to their numerical stability and elasticity against imperfect tuning. Their stochastic accelerated variants though have received limited attention: how momentum interacts with the stability of (stochastic) proximal point methods remains largely unstudied. To address this, we focus on the convergence and stability of the stochastic proximal point algorithm with momentum (SPPAM), and show that SPPAM allows a faster linear convergence to a neighborhood compared to the stochastic proximal point algorithm (SPPA) with a better contraction factor, und
    
[^34]: 在分段常数高维回归系数中进行去噪和变点定位

    Denoising and change point localisation in piecewise-constant high-dimensional regression coefficients. (arXiv:2110.14298v2 [math.ST] CROSS LISTED)

    [http://arxiv.org/abs/2110.14298](http://arxiv.org/abs/2110.14298)

    该论文研究了在线性回归模型中融合套索程序的理论性质，并提出了新的设计矩阵受限等距条件，得出了融合套索的估计界限。研究发现，估计误差的主导因素取决于非零系数和分段常数段的数量。

    

    我们研究了在线性回归模型中融合套索程序的理论特性。该模型假设回归系数是有序且稀疏的分段常数。尽管该方法很受欢迎，但据我们所知，目前仅在设计矩阵为单位矩阵的简单情况下才能得到高维情况下的估计误差界限。我们提出了一种新颖的设计矩阵受限等距条件，针对融合套索估计器，并得出了融合套索的约束版本和惩罚版本的估计界限。我们观察到，估计误差可能由套索或融合套索的速率主导，这取决于非零系数的数量是否大于分段常数段的数量。最后，我们设计了一个后处理程序来恢复...

    We study the theoretical properties of the fused lasso procedure originally proposed by \cite{tibshirani2005sparsity} in the context of a linear regression model in which the regression coefficient are totally ordered and assumed to be sparse and piecewise constant. Despite its popularity, to the best of our knowledge, estimation error bounds in high-dimensional settings have only been obtained for the simple case in which the design matrix is the identity matrix. We formulate a novel restricted isometry condition on the design matrix that is tailored to the fused lasso estimator and derive estimation bounds for both the constrained version of the fused lasso assuming dense coefficients and for its penalised version. We observe that the estimation error can be dominated by either the lasso or the fused lasso rate, depending on whether the number of non-zero coefficient is larger than the number of piece-wise constant segments. Finally, we devise a post-processing procedure to recover t
    
[^35]: FedPower: 隐私保护的分布式特征空间估计

    FedPower: Privacy-Preserving Distributed Eigenspace Estimation. (arXiv:2103.00704v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.00704](http://arxiv.org/abs/2103.00704)

    本文提出了一种称为FedPower的算法，在联邦学习框架内解决了特征空间估计的隐私和通信效率问题。算法利用幂法进行本地迭代和全局聚合，采用正交Procrustes变换加权以实现对齐，并引入差分隐私以保护数据隐私。

    

    特征空间估计是机器学习和统计学中的基础问题，在主成分分析、维度约简和聚类等领域都有应用。现代机器学习通常假设数据来自不同的组织且属于不同组织。数据的低通信能力和可能的隐私泄漏使得特征空间的计算具有挑战性。为了解决这些挑战，我们在联邦学习（FL）框架内提出了一类算法，称为FedPower。FedPower利用了著名的幂法，通过交替进行多个本地幂迭代和全局聚合步骤，从而提高通信效率。在聚合中，我们提出使用正交Procrustes变换（OPT）对每个本地特征向量矩阵进行加权以实现更好的对齐。为了确保强大的隐私保护，我们在每次迭代中添加高斯噪声，采用差分隐私（DP）的概念。我们提供了...

    Eigenspace estimation is fundamental in machine learning and statistics, which has found applications in PCA, dimension reduction, and clustering, among others. The modern machine learning community usually assumes that data come from and belong to different organizations. The low communication power and the possible privacy breaches of data make the computation of eigenspace challenging. To address these challenges, we propose a class of algorithms called \textsf{FedPower} within the federated learning (FL) framework. \textsf{FedPower} leverages the well-known power method by alternating multiple local power iterations and a global aggregation step, thus improving communication efficiency. In the aggregation, we propose to weight each local eigenvector matrix with {\it Orthogonal Procrustes Transformation} (OPT) for better alignment. To ensure strong privacy protection, we add Gaussian noise in each iteration by adopting the notion of \emph{differential privacy} (DP). We provide conve
    
[^36]: 在工具变量回归中学习深度特征

    Learning Deep Features in Instrumental Variable Regression. (arXiv:2010.07154v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.07154](http://arxiv.org/abs/2010.07154)

    本论文提出了一种名为深度特征工具变量回归（DFIV）的方法，用于处理观测数据中工具变量、处理变量和结果变量之间的非线性关系。通过训练深度神经网络来定义工具变量和处理变量上的非线性特征，我们提供了一种交替训练机制以获得良好的端到端性能。这种方法可以在计算上获得高度灵活的特征映射。

    

    工具变量（IV）回归是一种在观测数据中利用仅通过处理影响结果的工具变量来学习混淆的处理和结果变量之间因果关系的标准策略。在传统的IV回归中，学习分为两个阶段：阶段1从工具变量到处理变量进行线性回归；阶段2在工具变量的条件下，从处理变量到结果变量进行线性回归。我们提出了一种新的方法，即深度特征工具变量回归（DFIV），用于处理工具变量、处理变量和结果变量之间可能是非线性关系的情况。在这种情况下，我们训练深度神经网络来定义工具变量和处理变量上的信息性非线性特征。我们提出了一种交替训练机制来训练这些特征，以保证通过组合阶段1和阶段2获得良好的端到端性能，从而在计算上获得高度灵活的特征映射。

    Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally eff
    

