# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bridging RL Theory and Practice with the Effective Horizon.](http://arxiv.org/abs/2304.09853) | 本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。 |
| [^2] | [Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts.](http://arxiv.org/abs/2304.09836) | 本研究通过有限样本和功率分析确定了多元概率时间序列预测评分规则的可靠性区域，并在电力生产问题上评估了结果对真实世界任务的普适性。 |
| [^3] | [Generalization and Estimation Error Bounds for Model-based Neural Networks.](http://arxiv.org/abs/2304.09802) | 基于模型的神经网络在稀疏恢复中表现出较高的泛化能力，复杂度量有助于提高其泛化和估计误差界限 |
| [^4] | [Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit.](http://arxiv.org/abs/2304.09663) | 本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。 |
| [^5] | [Leveraging the two timescale regime to demonstrate convergence of neural networks.](http://arxiv.org/abs/2304.09576) | 研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。 |
| [^6] | [Denoising Cosine Similarity: A Theory-Driven Approach for Efficient Representation Learning.](http://arxiv.org/abs/2304.09552) | 本文提出了一种去噪余弦相似度（dCS）损失函数， 可以用于在原始数据集中学习鲁棒的表示形式。 |
| [^7] | [Loss minimization yields multicalibration for large neural networks.](http://arxiv.org/abs/2304.09424) | 本文展示了对于大型神经网络大小，最优地最小化损失会导致多校准，以提供公平的预测结果。 |
| [^8] | [Minimax Signal Detection in Sparse Additive Models.](http://arxiv.org/abs/2304.09398) | 本研究针对稀疏加性模型中的信号检测问题建立了极小极大分离速率，揭示了稀疏性和函数空间选择之间的非平凡交互作用，并研究了对稀疏性的自适应性和其在通用函数空间中的适用性。在Sobolev空间设置下，我们还讨论了对稀疏性和平滑性的自适应性。 |
| [^9] | [The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties.](http://arxiv.org/abs/2304.09310) | 本文提出了一种新型鲁棒的自适应 $\tau$-Lasso 估计器，同时采用自适应 $\ell_1$-范数惩罚项以降低真实回归系数的偏差。它具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。 |
| [^10] | [A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions.](http://arxiv.org/abs/2304.09278) | 本文提出了一种利用序列学习来高效优化多个相互冲突目标的复杂系统的数据驱动贝叶斯优化框架。 |
| [^11] | [A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition.](http://arxiv.org/abs/2304.09242) | 本文提出了一种使用非线性函数构建交叉相关器的方法，并使用Price定理和分段线性分解提出了一个数学框架来分析这种交叉相关器。 |
| [^12] | [Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks.](http://arxiv.org/abs/2304.09221) | 本文通过随机梯度下降算法研究了解析度函数为非凸的深度神经网络的全局收敛性，证明了当机器学习噪声的尺度与目标函数相等时，在局部区域内初始化后，以正的概率能够收敛到该区域内的全局最小值。 |
| [^13] | [Understanding Overfitting in Adversarial Training in Kernel Regression.](http://arxiv.org/abs/2304.06326) | 本文研究了核回归的对抗训练和带噪声的数据增强，发现如果没有适当的正则化，这两种方法可能会导致过拟合现象，但适当的正则化可以缓解这种现象，提高性能。 |
| [^14] | [Sampling with Barriers: Faster Mixing via Lewis Weights.](http://arxiv.org/abs/2303.00480) | 本文介绍了一种RHMC采样多面体的新方法，可实现更快的混合速度，其中混合速度由$m^{1/3}n^{4/3}$界限。 |
| [^15] | [Fine-tuning Neural-Operator architectures for training and generalization.](http://arxiv.org/abs/2301.11509) | 本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。 |
| [^16] | [Statistical inference for transfer learning with high-dimensional quantile regression.](http://arxiv.org/abs/2211.14578) | 本研究提出了一种高维分位数回归模型中的转移学习方法，以适应源域和目标域中的异质性和重尾分布。根据精心选择的可转移源域建立了转移学习估计量的误差界限，并提出了有效的置信区间和假设检验程序，以实现一步完成。 |
| [^17] | [Differentially private partitioned variational inference.](http://arxiv.org/abs/2209.11595) | 本论文提出了差分隐私的分区变分推断算法，是第一种在联邦贝叶斯学习环境下实现差分隐私的方法。 |
| [^18] | [A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors.](http://arxiv.org/abs/2207.11621) | 本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、测试损失和训练损失之间的关系，发现测试数据上表现出色的模型要么是经典的，要么是现代的。同时提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析，使得分析更加精确。 |
| [^19] | [Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward.](http://arxiv.org/abs/2206.06426) | 本文提出了一种离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代，用于解决轨迹奖励难以很好地利用的问题。 |
| [^20] | [A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems.](http://arxiv.org/abs/2203.01387) | 本文综述了离线强化学习中的分类与最新算法突破，离线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。 |
| [^21] | [Gaining Outlier Resistance with Progressive Quantiles: Fast Algorithms and Theoretical Studies.](http://arxiv.org/abs/2112.08471) | 本文提出了一个离群值抗性估计的框架，通过渐进分位数算法解决离群值问题，并开发可伸缩算法来保证快速收敛性和超越M-估计的非渐进分析。 |
| [^22] | [Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach.](http://arxiv.org/abs/2109.12701) | 本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。 |
| [^23] | [Continuous Time Bandits With Sampling Costs.](http://arxiv.org/abs/2107.05289) | 本文研究了带采样成本的连续时间多臂赌博机问题，在连续时间里，学习者要在获得更高奖励和承担采样成本之间进行有效平衡。本文提出了一个达到下界的算法，并揭示了与传统多臂赌博机问题不同的特殊现象，具有广泛应用价值。 |
| [^24] | [An Analysis of Robustness of Non-Lipschitz Networks.](http://arxiv.org/abs/2010.06154) | 本文研究了深度非利普希茨网络的鲁棒性问题，定义了一个攻击模型帮助理解内在属性，证明了此类攻击者可以战胜所有必须对其输入进行分类的算法，但也提出了克服此类攻击者的方法，进一步提供了理论保证并为最近邻算法提供了新的鲁棒性保证。 |

# 详细

[^1]: 用有效的视野连接强化学习理论和实践

    Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])

    [http://arxiv.org/abs/2304.09853](http://arxiv.org/abs/2304.09853)

    本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。

    

    深度强化学习在某些环境中表现出色，但在其他环境中却失败得非常严重。理想情况下，强化学习理论应该能够解释这种现象，提供预测实际性能的界限。不幸的是，当前的理论还没有这种能力。本文通过引入包含155个MDP的新数据集BRIDGE，将标准的深度强化学习算法与之前的样本复杂度先前界进行比较，并发现了一个意想不到的性质：当最高Q值的动作在随机策略下的Q值也是最高的时，深度强化学习往往会成功；反之，失败的可能性较高。基于这一性质，我们将其概括为一个新的MDP复杂度度量，称为有效的视野。

    Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
    
[^2]: 多元概率预测评估中的可靠性区域研究

    Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts. (arXiv:2304.09836v1 [cs.LG])

    [http://arxiv.org/abs/2304.09836](http://arxiv.org/abs/2304.09836)

    本研究通过有限样本和功率分析确定了多元概率时间序列预测评分规则的可靠性区域，并在电力生产问题上评估了结果对真实世界任务的普适性。

    

    在多元概率时间序列预测的评估中，通常使用适当的评分规则进行评估，即对于基准分布期望最小的函数。然而，在非渐进情况下，这一属性不能保证具有良好的区分度。在本文中，我们提供了第一篇系统的有限样本适当评分规则研究，通过功率分析，我们确定了一个分数规则的“可靠性区域”，即它可以可靠地识别预测误差的一组实际条件。我们在一个全面的人造基准测试上进行了分析，该测试专门设计以测试基准分布与预测分布之间的几个关键差异，并通过在电力生产问题上应用来评估我们的结果对真实世界任务的普适性。我们的结果揭示了在多元概率预测的评估中的重大缺陷。

    Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as co
    
[^3]: 基于模型的神经网络的泛化和估计误差界限

    Generalization and Estimation Error Bounds for Model-based Neural Networks. (arXiv:2304.09802v1 [cs.LG])

    [http://arxiv.org/abs/2304.09802](http://arxiv.org/abs/2304.09802)

    基于模型的神经网络在稀疏恢复中表现出较高的泛化能力，复杂度量有助于提高其泛化和估计误差界限

    

    基于模型的神经网络在各种任务（如稀疏编码和压缩感知问题）中提供了无与伦比的性能。由于与传感模型的强关联，这些网络是可解释的并继承了问题的先前结构。实践中，与ReLU神经网络相比，基于模型的神经网络表现出更高的泛化能力。然而，这种现象在理论上还没有得到解决。在本文中，我们利用复杂度量（包括全局和局部Rademacher复杂度）的方法，为基于模型的网络提供泛化和估计误差的上限。我们表明，基于模型的网络在稀疏恢复方面的泛化能力优于常规的ReLU网络，并导出允许构建具有高保证性的基于模型的网络的实际设计规则。我们通过一系列实验演示了我们的理论见解为深度学习实践者提供了一些启示。

    Model-based neural networks provide unparalleled performance for various tasks, such as sparse coding and compressed sensing problems. Due to the strong connection with the sensing model, these networks are interpretable and inherit prior structure of the problem. In practice, model-based neural networks exhibit higher generalization capability compared to ReLU neural networks. However, this phenomenon was not addressed theoretically. Here, we leverage complexity measures including the global and local Rademacher complexities, in order to provide upper bounds on the generalization and estimation errors of model-based networks. We show that the generalization abilities of model-based networks for sparse recovery outperform those of regular ReLU networks, and derive practical design rules that allow to construct model-based networks with guaranteed high generalization. We demonstrate through a series of experiments that our theoretical insights shed light on a few behaviours experienced 
    
[^4]: 通过最优输运和投影追踪的时变密度生成建模

    Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit. (arXiv:2304.09663v1 [stat.ML])

    [http://arxiv.org/abs/2304.09663](http://arxiv.org/abs/2304.09663)

    本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。

    

    受到流行的深度学习算法对于时态密度生成建模所带来的计算困难的启发，我们提出了一种便宜的替代方案，它需要最少的超参数调整，并且可以很好地扩展到高维问题。具体地，我们使用基于投影的最优输运求解器 [Meng等，2019] 来连接连续的样本，然后使用传输样条 [Chewi等，2020] 来插值演化的密度。当采样频率足够高时，最优映射接近于恒等映射，因此计算效率高。此外，训练过程可以高度并行化，因为所有最优映射是独立的，因此可以同时学习。最后，该方法仅基于数值线性代数而不是最小化非凸目标函数，这使我们能够轻松分析和控制算法。我们在合成和真实数据集上进行了几个数值实验。

    Motivated by the computational difficulties incurred by popular deep learning algorithms for the generative modeling of temporal densities, we propose a cheap alternative which requires minimal hyperparameter tuning and scales favorably to high dimensional problems. In particular, we use a projection-based optimal transport solver [Meng et al., 2019] to join successive samples and subsequently use transport splines [Chewi et al., 2020] to interpolate the evolving density. When the sampling frequency is sufficiently high, the optimal maps are close to the identity and are thus computationally efficient to compute. Moreover, the training process is highly parallelizable as all optimal maps are independent and can thus be learned simultaneously. Finally, the approach is based solely on numerical linear algebra rather than minimizing a nonconvex objective function, allowing us to easily analyze and control the algorithm. We present several numerical experiments on both synthetic and real-w
    
[^5]: 利用双时间尺度制度证明神经网络的收敛性研究

    Leveraging the two timescale regime to demonstrate convergence of neural networks. (arXiv:2304.09576v1 [math.OC])

    [http://arxiv.org/abs/2304.09576](http://arxiv.org/abs/2304.09576)

    研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。

    

    我们研究了浅层神经网络的训练动态，在内层步长远小于外层步长的双时间尺度制度下。在这个制度下，在简单的单变量环境中，我们证明了梯度流收敛于非凸优化问题的全局最优解。我们的结果不需要神经元数量趋于无限，这使我们的结果不同于最近流行的方法，如神经切向核或平均场制度。我们提供实验说明，显示随机梯度下降按照我们对梯度流的描述进行行为，并因此在双时间尺度制度下收敛于全局最优解，但在此制度之外可能失败。

    We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.
    
[^6]: 去噪余弦相似度：一种理论驱动的有效表示学习方法

    Denoising Cosine Similarity: A Theory-Driven Approach for Efficient Representation Learning. (arXiv:2304.09552v1 [stat.ML])

    [http://arxiv.org/abs/2304.09552](http://arxiv.org/abs/2304.09552)

    本文提出了一种去噪余弦相似度（dCS）损失函数， 可以用于在原始数据集中学习鲁棒的表示形式。

    

    表示学习已经在机器学习的研究和实践中发挥着越来越大的影响，因为它能够学习出可以有效应用于各种下游任务的表示形式。然而，最近的研究很少关注到表示学习阶段中使用的真实数据集通常会受到噪声污染，这可能会降低学习出的表示形式的质量。本文解决了在原始数据集中学习鲁棒表示形式的问题。为此，受到最近去噪相关工作和基于余弦相似度目标函数在表示学习中的成功的启发，我们提出了去噪余弦相似度（dCS）损失函数。dCS损失是一种修改过的余弦相似度损失函数，具有去噪属性，这一点得到了我们的理论和实证研究的支持。为了使dCS损失可实现，我们还构建了具有统计保证的dCS损失估计器。

    Representation learning has been increasing its impact on the research and practice of machine learning, since it enables to learn representations that can apply to various downstream tasks efficiently. However, recent works pay little attention to the fact that real-world datasets used during the stage of representation learning are commonly contaminated by noise, which can degrade the quality of learned representations. This paper tackles the problem to learn robust representations against noise in a raw dataset. To this end, inspired by recent works on denoising and the success of the cosine-similarity-based objective functions in representation learning, we propose the denoising Cosine-Similarity (dCS) loss. The dCS loss is a modified cosine-similarity loss and incorporates a denoising property, which is supported by both our theoretical and empirical findings. To make the dCS loss implementable, we also construct the estimators of the dCS loss with statistical guarantees. Finally,
    
[^7]: 大型神经网络的多校准可最小化损失

    Loss minimization yields multicalibration for large neural networks. (arXiv:2304.09424v1 [cs.LG])

    [http://arxiv.org/abs/2304.09424](http://arxiv.org/abs/2304.09424)

    本文展示了对于大型神经网络大小，最优地最小化损失会导致多校准，以提供公平的预测结果。

    

    多校准是一种公平性概念，旨在提供跨大量团体的准确预测。即使对于简单的预测器，如线性函数，多校准也被认为是与最小化损失不同的目标。在本文中，我们展示了对于（几乎所有的）大型神经网络大小，最优地最小化平方误差会导致多校准。我们的结果关于神经网络的表征方面，而不是关于算法或样本复杂性考虑。以前的这样的结果仅适用于几乎贝叶斯最优的预测器，因此是表征无关的。我们强调，我们的结果不适用于优化神经网络的特定算法，如 SGD，并且不应解释为“公平性从优化神经网络中获得免费的好处”。

    Multicalibration is a notion of fairness that aims to provide accurate predictions across a large set of groups. Multicalibration is known to be a different goal than loss minimization, even for simple predictors such as linear functions. In this note, we show that for (almost all) large neural network sizes, optimally minimizing squared error leads to multicalibration. Our results are about representational aspects of neural networks, and not about algorithmic or sample complexity considerations. Previous such results were known only for predictors that were nearly Bayes-optimal and were therefore representation independent. We emphasize that our results do not apply to specific algorithms for optimizing neural networks, such as SGD, and they should not be interpreted as "fairness comes for free from optimizing neural networks".
    
[^8]: 稀疏加性模型中的极小极大信号检测

    Minimax Signal Detection in Sparse Additive Models. (arXiv:2304.09398v1 [math.ST])

    [http://arxiv.org/abs/2304.09398](http://arxiv.org/abs/2304.09398)

    本研究针对稀疏加性模型中的信号检测问题建立了极小极大分离速率，揭示了稀疏性和函数空间选择之间的非平凡交互作用，并研究了对稀疏性的自适应性和其在通用函数空间中的适用性。在Sobolev空间设置下，我们还讨论了对稀疏性和平滑性的自适应性。

    

    在高维度的建模需求中，稀疏加性模型是一种有吸引力的选择。我们研究了信号检测问题，并建立了一个稀疏加性信号检测的极小极大分离速率。我们的结果是非渐近的，并适用于单变量分量函数属于一般再生核希尔伯特空间的情况。与估计理论不同，极小极大分离速率揭示了稀疏性和函数空间选择之间的非平凡交互作用。我们还研究了对稀疏性的自适应性，并建立了一个通用函数空间的自适应测试速率；在某些空间中，自适应性是可能的，而在其他空间中则会产生不可避免的代价。最后，我们在Sobolev空间设置下研究了对稀疏性和平滑性的自适应性，并更正了文献中存在的一些说法。

    Sparse additive models are an attractive choice in circumstances calling for modelling flexibility in the face of high dimensionality. We study the signal detection problem and establish the minimax separation rate for the detection of a sparse additive signal. Our result is nonasymptotic and applicable to the general case where the univariate component functions belong to a generic reproducing kernel Hilbert space. Unlike the estimation theory, the minimax separation rate reveals a nontrivial interaction between sparsity and the choice of function space. We also investigate adaptation to sparsity and establish an adaptive testing rate for a generic function space; adaptation is possible in some spaces while others impose an unavoidable cost. Finally, adaptation to both sparsity and smoothness is studied in the setting of Sobolev space, and we correct some existing claims in the literature.
    
[^9]: 自适应 $\tau$-Lasso：其健壮性和最优性质。

    The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])

    [http://arxiv.org/abs/2304.09310](http://arxiv.org/abs/2304.09310)

    本文提出了一种新型鲁棒的自适应 $\tau$-Lasso 估计器，同时采用自适应 $\ell_1$-范数惩罚项以降低真实回归系数的偏差。它具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。

    

    本文介绍了一种用于分析高维数据集的新型正则化鲁棒 $\tau$-回归估计器，以应对响应变量和协变量的严重污染。我们称这种估计器为自适应 $\tau$-Lasso，它对异常值和高杠杆点具有鲁棒性，同时采用自适应 $\ell_1$-范数惩罚项来减少真实回归系数的偏差。具体而言，该自适应 $\ell_1$-范数惩罚项为每个回归系数分配一个权重。对于固定数量的预测变量 $p$，我们显示出自适应 $\tau$-Lasso 具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。然后我们通过有限样本断点和影响函数来表征其健壮性。我们进行了广泛的模拟来比较不同的估计器的性能。

    This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
    
[^10]: 一种数据驱动的序列学习框架，用于加速和优化多目标制造决策

    A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions. (arXiv:2304.09278v1 [cs.LG])

    [http://arxiv.org/abs/2304.09278](http://arxiv.org/abs/2304.09278)

    本文提出了一种利用序列学习来高效优化多个相互冲突目标的复杂系统的数据驱动贝叶斯优化框架。

    

    制造具有特定性质或性质组合的先进材料和产品通常是必要的。为了实现这一点，找到能够生成这些性质理想组合的最佳配方或处理条件至关重要。大多数时候，需要进行足够数量的实验才能生成Pareto前沿。然而，制造实验通常很昂贵，甚至进行一次实验也可能是一个耗时的过程。因此，确定最佳数据收集位置以获得对过程的最全面理解非常关键。序列学习是一种有前途的方法，可以从进行中的实验中主动学习，迭代更新基础优化例程，并随时调整数据收集过程。本文提出了一种新颖的基于数据驱动的贝叶斯优化框架，利用序列学习来高效优化具有多个相互冲突目标的复杂系统。

    Manufacturing advanced materials and products with a specific property or combination of properties is often warranted. To achieve that it is crucial to find out the optimum recipe or processing conditions that can generate the ideal combination of these properties. Most of the time, a sufficient number of experiments are needed to generate a Pareto front. However, manufacturing experiments are usually costly and even conducting a single experiment can be a time-consuming process. So, it's critical to determine the optimal location for data collection to gain the most comprehensive understanding of the process. Sequential learning is a promising approach to actively learn from the ongoing experiments, iteratively update the underlying optimization routine, and adapt the data collection process on the go. This paper presents a novel data-driven Bayesian optimization framework that utilizes sequential learning to efficiently optimize complex systems with multiple conflicting objectives. 
    
[^11]: 使用Price定理和分段线性分解分析在线交叉相关器的框架。

    A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition. (arXiv:2304.09242v1 [cs.LG])

    [http://arxiv.org/abs/2304.09242](http://arxiv.org/abs/2304.09242)

    本文提出了一种使用非线性函数构建交叉相关器的方法，并使用Price定理和分段线性分解提出了一个数学框架来分析这种交叉相关器。

    

    精确估计两个随机变量之间的交叉相关或相似度是信号检测、高维计算、联想记忆和神经网络的核心问题。本文提出了一种能够构建具有更高信噪比（SNR）的交叉相关器的大量简单非线性函数的方法，并使用Price定理和分段线性分解提出了一个数学框架，以分析使用混合分段线性函数构建的交叉相关器。

    Precise estimation of cross-correlation or similarity between two random variables lies at the heart of signal detection, hyperdimensional computing, associative memories, and neural networks. Although a vast literature exists on different methods for estimating cross-correlations, the question what is the best and simplest method to estimate cross-correlations using finite samples ? is still not clear. In this paper, we first argue that the standard empirical approach might not be the optimal method even though the estimator exhibits uniform convergence to the true cross-correlation. Instead, we show that there exists a large class of simple non-linear functions that can be used to construct cross-correlators with a higher signal-to-noise ratio (SNR). To demonstrate this, we first present a general mathematical framework using Price's Theorem that allows us to analyze cross-correlators constructed using a mixture of piece-wise linear functions. Using this framework and high-dimensiona
    
[^12]: 基于局部Lajasiewicz条件的随机梯度下降在深度神经网络中的收敛性研究

    Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks. (arXiv:2304.09221v1 [cs.LG])

    [http://arxiv.org/abs/2304.09221](http://arxiv.org/abs/2304.09221)

    本文通过随机梯度下降算法研究了解析度函数为非凸的深度神经网络的全局收敛性，证明了当机器学习噪声的尺度与目标函数相等时，在局部区域内初始化后，以正的概率能够收敛到该区域内的全局最小值。

    

    本文考虑了解析度函数为非凸的情况下，通过随机梯度下降算法对深度神经网络的全局收敛性进行了研究。在有限宽的神经网络中，通过加入最小的额外假设并保证机器学习噪声的尺度与目标函数相等，证明了在局部区域内初始化时，以正的概率随机梯度下降迭代收敛到该区域内的全局最小值。本文的关键是确保随机梯度下降的整个轨迹以正的概率保留在局部区域内。文章提供了负面分析，表明使用Robbins-Monro类型的步长之间具有有界噪声的假设不足以保持该关键部分的有效性。

    We extend the global convergence result of Chatterjee \cite{chatterjee2022convergence} by considering the stochastic gradient descent (SGD) for non-convex objective functions. With minimal additional assumptions that can be realized by finitely wide neural networks, we prove that if we initialize inside a local region where the \L{}ajasiewicz condition holds, with a positive probability, the stochastic gradient iterates converge to a global minimum inside this region. A key component of our proof is to ensure that the whole trajectories of SGD stay inside the local region with a positive probability. For that, we assume the SGD noise scales with the objective function, which is called machine learning noise and achievable in many real examples. Furthermore, we provide a negative argument to show why using the boundedness of noise with Robbins-Monro type step sizes is not enough to keep the key component valid.
    
[^13]: 理解核回归对抗训练中的过拟合现象

    Understanding Overfitting in Adversarial Training in Kernel Regression. (arXiv:2304.06326v1 [stat.ML])

    [http://arxiv.org/abs/2304.06326](http://arxiv.org/abs/2304.06326)

    本文研究了核回归的对抗训练和带噪声的数据增强，发现如果没有适当的正则化，这两种方法可能会导致过拟合现象，但适当的正则化可以缓解这种现象，提高性能。

    

    对抗训练和带噪声的数据增强是提高神经网络性能的常见方法。本文研究了在再生希尔伯特空间（RKHS）中正则化回归的对抗训练和带噪声的数据增强。当攻击和噪声大小以及正则化参数趋向于零时，建立了这些技术的极限公式。根据该极限公式，分析了特定情况并证明了，如果没有适当的正则化，这两种方法可能具有大于标准核回归的广义误差和Lipschitz常数。然而，通过选择适当的正则化参数，这两种方法可以优于标准核回归，达到更小的广义误差和Lipschitz常数。这些发现支持对抗训练可能导致过拟合的经验观察，以及适当的正则化方法能够缓解这种过拟合现象。

    Adversarial training and data augmentation with noise are widely adopted techniques to enhance the performance of neural networks. This paper investigates adversarial training and data augmentation with noise in the context of regularized regression in a reproducing kernel Hilbert space (RKHS). We establish the limiting formula for these techniques as the attack and noise size, as well as the regularization parameter, tend to zero. Based on this limiting formula, we analyze specific scenarios and demonstrate that, without appropriate regularization, these two methods may have larger generalization error and Lipschitz constant than standard kernel regression. However, by selecting the appropriate regularization parameter, these two methods can outperform standard kernel regression and achieve smaller generalization error and Lipschitz constant. These findings support the empirical observations that adversarial training can lead to overfitting, and appropriate regularization methods, suc
    
[^14]: 带障碍的采样：通过Lewis权重实现更快的混合

    Sampling with Barriers: Faster Mixing via Lewis Weights. (arXiv:2303.00480v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2303.00480](http://arxiv.org/abs/2303.00480)

    本文介绍了一种RHMC采样多面体的新方法，可实现更快的混合速度，其中混合速度由$m^{1/3}n^{4/3}$界限。

    

    本文分析了用Riemannian Hamiltonian Monte Carlo（RHMC）采样由$\R^n$中$m$个不等式定义的多面体的方法。其中的度量按具有凸障碍函数的海森矩阵定义。RHMC相对于欧几里得方法（如球行走，hit-and-run和Dikin行走）的优势在于能够迈出更长的步伐。然而，在先前所有的工作中，混合速度都与不等式数量成线性关系。我们介绍了一个Lewis权重障碍和标准对数障碍的混合，并证明了相应RHMC的混合速度由$\tilde O(m^{1/3}n^{4/3})$界限，改进了先前的最佳界限$\tilde O(mn^{2/3})$（基于对数障碍）。这继续了最优化和采样之间的一般相似之处，后者通常导致新工具和更精细的分析。为了证明我们的主要结果，我们必须克服涉及Hamiltonian曲线的平滑性和自共轭性的几个挑战。

    We analyze Riemannian Hamiltonian Monte Carlo (RHMC) for sampling a polytope defined by $m$ inequalities in $\R^n$ endowed with the metric defined by the Hessian of a convex barrier function. The advantage of RHMC over Euclidean methods such as the ball walk, hit-and-run and the Dikin walk is in its ability to take longer steps. However, in all previous work, the mixing rate has a linear dependence on the number of inequalities. We introduce a hybrid of the Lewis weights barrier and the standard logarithmic barrier and prove that the mixing rate for the corresponding RHMC is bounded by $\tilde O(m^{1/3}n^{4/3})$, improving on the previous best bound of $\tilde O(mn^{2/3})$ (based on the log barrier). This continues the general parallels between optimization and sampling, with the latter typically leading to new tools and more refined analysis. To prove our main results, we have to overcomes several challenges relating to the smoothness of Hamiltonian curves and the self-concordance pro
    
[^15]: 细调神经算符结构以提高训练和泛化能力

    Fine-tuning Neural-Operator architectures for training and generalization. (arXiv:2301.11509v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11509](http://arxiv.org/abs/2301.11509)

    本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。

    

    本篇论文全面分析了神经算符（NOs）及其衍生结构的泛化特性。通过对测试损失的经验评估、基于复杂性的泛化界限的分析以及对损失景观可视化的定性评估，我们研究了旨在提高NOs泛化能力的修改。受Transformer的成功启发，我们提出了${\textit{s}}{\text{NO}}+\varepsilon$，该方法引入了一个核积分算符来代替自关注机制。我们的结果显示，伴随着损失景观可视化的定性变化，性能显著提高了，适用于各种数据集和初始化。我们猜测，Transformer的布局使优化算法能够找到更好的极小值，并且随机深度可以提高泛化性能。由于训练动态的严格分析是深度学习最突出的未解决问题之一，因此我们还推出了一个新的训练课程，重点是逐渐增加模型容量，从而显著提高了泛化能力。

    This work provides a comprehensive analysis of the generalization properties of Neural Operators (NOs) and their derived architectures. Through empirical evaluation of the test loss, analysis of the complexity-based generalization bounds, and qualitative assessments of the visualization of the loss landscape, we investigate modifications aimed at enhancing the generalization capabilities of NOs. Inspired by the success of Transformers, we propose ${\textit{s}}{\text{NO}}+\varepsilon$, which introduces a kernel integral operator in lieu of self-Attention. Our results reveal significantly improved performance across datasets and initializations, accompanied by qualitative changes in the visualization of the loss landscape. We conjecture that the layout of Transformers enables the optimization algorithm to find better minima, and stochastic depth, improve the generalization performance. As a rigorous analysis of training dynamics is one of the most prominent unsolved problems in deep lear
    
[^16]: 高维分位数回归中的转移学习统计推断

    Statistical inference for transfer learning with high-dimensional quantile regression. (arXiv:2211.14578v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14578](http://arxiv.org/abs/2211.14578)

    本研究提出了一种高维分位数回归模型中的转移学习方法，以适应源域和目标域中的异质性和重尾分布。根据精心选择的可转移源域建立了转移学习估计量的误差界限，并提出了有效的置信区间和假设检验程序，以实现一步完成。

    

    转移学习已经成为一种重要的技术，用于利用源域中的信息来提高目标任务的性能。尽管高维数据普遍存在异质性和/或重尾分布，但目前的转移学习方法未能充分考虑这些问题，可能会影响结果的性能。我们在高维分位数回归模型框架下提出了一种转移学习过程，以适应源域和目标域中的异质性和重尾分布。我们根据精心选择的可转移源域建立了转移学习估计量的误差界限，显示在关键选择标准和较大的源任务样本量下可以实现更低的误差界限。我们进一步提出了一个有效的置信区间和假设检验程序，用于高维分位数回归系数的各个分量，通过倡导双重转移学习估计量，实现一步完成。

    Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step 
    
[^17]: 差分隐私的分区变分推断方法

    Differentially private partitioned variational inference. (arXiv:2209.11595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11595](http://arxiv.org/abs/2209.11595)

    本论文提出了差分隐私的分区变分推断算法，是第一种在联邦贝叶斯学习环境下实现差分隐私的方法。

    

    从分布在多个设备上的敏感数据中学习隐私保护模型是一个日益重要的问题。该问题通常在联邦学习背景下进行规划，目标是在保持数据分布的同时学习单个全局模型。此外，贝叶斯学习是一种流行的建模方法，因为它自然地支持可靠的不确定性估计。然而，即使对于集中的非隐私数据，贝叶斯学习也通常是不可操作的，因此变分推断等近似技术是必需的。近期，通过分区变分推断算法，变分推断已经扩展到非隐私联邦学习的情况。对于隐私保护，目前的黄金标准被称为差分隐私。差分隐私在数学上定义了一个强的隐私保护概念。本文提出了差分隐私的分区变分推断方法，是第一种通过分区变分推断算法在联邦贝叶斯学习环境下实现差分隐私的方法。我们在人造和真实基准测试中展示了我们提出的方法的有效性和效率。

    Learning a privacy-preserving model from sensitive data which are distributed across multiple devices is an increasingly important problem. The problem is often formulated in the federated learning context, with the aim of learning a single global model while keeping the data distributed. Moreover, Bayesian learning is a popular approach for modelling, since it naturally supports reliable uncertainty estimates. However, Bayesian learning is generally intractable even with centralised non-private data and so approximation techniques such as variational inference are a necessity. Variational inference has recently been extended to the non-private federated learning setting via the partitioned variational inference algorithm. For privacy protection, the current gold standard is called differential privacy. Differential privacy guarantees privacy in a strong, mathematically clearly defined sense.  In this paper, we present differentially private partitioned variational inference, the first
    
[^18]: 线性预测器的模型大小、测试损失和训练损失之间的通用权衡

    A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors. (arXiv:2207.11621v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11621](http://arxiv.org/abs/2207.11621)

    本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、测试损失和训练损失之间的关系，发现测试数据上表现出色的模型要么是经典的，要么是现代的。同时提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析，使得分析更加精确。

    

    本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、超额测试损失和训练损失之间的关系。我们发现，在测试数据上表现出色的模型要么是“经典”的——其训练损失接近噪声水平，要么是“现代的”——其参数数量远远超过仅仅能精确拟合训练数据所需的最小参数数。同时，我们还提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析。值得注意的是，在插值顶点附近，即参数数量刚好足以拟合训练数据时，Marchenko-Pastur分析更加精确，而随着过度参数化程度的增加，它恰好与无分布限制的理论上界相一致。

    In this work we establish an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either "classical" -- have training loss close to the noise level, or are "modern" -- have a much larger number of parameters compared to the minimum needed to fit the training data exactly.  We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko-Pastur. Remarkably, while the Marchenko-Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparametrization increases.
    
[^19]: 带轨迹奖励的离线强化学习的可证效率性

    Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward. (arXiv:2206.06426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06426](http://arxiv.org/abs/2206.06426)

    本文提出了一种离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代，用于解决轨迹奖励难以很好地利用的问题。

    

    强化学习（RL）的显著成功严重依赖于观测每个访问的状态-动作对的奖励。然而在许多真实世界应用中，代理只能观察表示整个轨迹质量的得分，称为"轨迹奖励"。在这种情况下，标准的RL方法很难很好地利用轨迹奖励，并且可能会在策略评估中产生大的偏差和方差误差。在本文中，我们提出一种新的离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代。为了确保PARTED构建的值函数始终对最优值函数悲观，我们设计了一个新的惩罚项来抵消代理奖励的不确定性。

    The remarkable success of reinforcement learning (RL) heavily relies on observing the reward of every visited state-action pair. In many real world applications, however, an agent can observe only a score that represents the quality of the whole trajectory, which is referred to as the {\em trajectory-wise reward}. In such a situation, it is difficult for standard RL methods to well utilize trajectory-wise reward, and large bias and variance errors can be incurred in policy evaluation. In this work, we propose a novel offline RL algorithm, called Pessimistic vAlue iteRaTion with rEward Decomposition (PARTED), which decomposes the trajectory return into per-step proxy rewards via least-squares-based reward redistribution, and then performs pessimistic value iteration based on the learned proxy reward. To ensure the value functions constructed by PARTED are always pessimistic with respect to the optimal ones, we design a new penalty term to offset the uncertainty of the proxy reward. For 
    
[^20]: 离线强化学习综述：分类、回顾和未解决问题

    A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. (arXiv:2203.01387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01387](http://arxiv.org/abs/2203.01387)

    本文综述了离线强化学习中的分类与最新算法突破，离线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。

    

    随着深度学习的广泛应用，强化学习（RL）在解决以往无法处理的问题方面取得了显著进展，如从像素观察中玩复杂游戏、与人类进行对话以及控制机器人智能体。然而，仍有许多领域由于与环境互动的高成本和危险而无法用RL解决。离线RL是一种范式，它仅从以前收集的交互的静态数据集中学习，因此可以从大型和多样化的培训数据集中提取策略。有效的离线RL算法比在线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。在本文中，我们提出了一个统一的分类法，对离线RL方法进行分类。此外，我们还对该领域最新的算法突破进行了全面回顾。

    With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field usin
    
[^21]: 在渐进分位数中获得离群值抗性：快速算法和理论研究

    Gaining Outlier Resistance with Progressive Quantiles: Fast Algorithms and Theoretical Studies. (arXiv:2112.08471v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2112.08471](http://arxiv.org/abs/2112.08471)

    本文提出了一个离群值抗性估计的框架，通过渐进分位数算法解决离群值问题，并开发可伸缩算法来保证快速收敛性和超越M-估计的非渐进分析。

    

    离群值在大数据应用中普遍存在，可能严重影响统计估计和推断。本文介绍了一个离群值抗性估计的框架，用于增强任意给定损失函数的鲁棒性。它与修剪方法有着密切的联系，并为所有样本提供了显式的离群指标，从而方便计算、理论和参数调整。为了解决凸性和不光滑性问题，我们开发了可伸缩的算法，并保证了快速收敛性。特别地，我们提出了一种新技术，以减轻对起始点的要求，使得在常规数据集上，数据重新采样的次数可以大大减少。基于统计和计算的综合处理，我们能够进行超越M-估计的非渐进分析。得到的鲁棒估计量，虽然不一定是全局甚至是局部最优，但在广泛的统计模型中具有渐近的最小大误差率最优性，包括参数和非参数设置。

    Outliers widely occur in big-data applications and may severely affect statistical estimation and inference. In this paper, a framework of outlier-resistant estimation is introduced to robustify an arbitrarily given loss function. It has a close connection to the method of trimming and includes explicit outlyingness parameters for all samples, which in turn facilitates computation, theory, and parameter tuning. To tackle the issues of nonconvexity and nonsmoothness, we develop scalable algorithms with implementation ease and guaranteed fast convergence. In particular, a new technique is proposed to alleviate the requirement on the starting point such that on regular datasets, the number of data resamplings can be substantially reduced. Based on combined statistical and computational treatments, we are able to perform nonasymptotic analysis beyond M-estimation. The obtained resistant estimators, though not necessarily globally or even locally optimal, enjoy minimax rate optimality in bo
    
[^22]: 稀疏加低秩矩阵分解: 一种离散优化方法

    Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. (arXiv:2109.12701v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.12701](http://arxiv.org/abs/2109.12701)

    本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。

    

    本文研究稀疏加低秩分解问题(SLR)，即将损坏的数据矩阵分解为包含基本真值的低秩矩阵和包含扰动的稀疏矩阵。 SLR是运筹学和机器学习领域的基础问题，在数据压缩、潜在语义索引、协同过滤和医学成像等各种应用中出现。我们提出了一种新的离散模型，并设计了交替最小化启发式算法以及新的半定松弛算法来解决这个问题。此外，我们还开发了一个自定义分支定界算法，利用我们的启发式算法和凸松弛来解决小规模的SLR问题。我们的启发式算法可以解决 $n=10000$ 的问题规模。

    We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in m
    
[^23]: 带采样成本的连续时间多臂赌博机问题研究

    Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.05289](http://arxiv.org/abs/2107.05289)

    本文研究了带采样成本的连续时间多臂赌博机问题，在连续时间里，学习者要在获得更高奖励和承担采样成本之间进行有效平衡。本文提出了一个达到下界的算法，并揭示了与传统多臂赌博机问题不同的特殊现象，具有广泛应用价值。

    

    本文研究了连续时间下的多臂赌博机问题(Continuous Time Multi-arm Bandit Problem，CTMAB)。在给定时间段内，学习者可以对臂进行任意次采样，每次采样都能获得随机奖励，但采样频率的提高会带来额外的惩罚/成本。因此，存在获得更高奖励与承担采样成本之间的平衡。本文旨在设计一种学习算法，使遗憾（regret，定义为学习算法与理论最优策略收益之间的差值）达到最小。CTMAB与通常的多臂赌博机问题(Multi-armed Bandit Problem，MAB)有根本的区别，例如，在CTMAB中，单臂情况都不是微不足道的，因为最优采样频率取决于臂的均值，而该均值需要被估计。本文首先建立了所有算法可达到的遗憾下界，然后提出了一种在对数因子上达到下界的算法。对于单臂情况，我们证明了下限和上限大致符合，并提出了一个计算效率高的算法。我们的结果揭示了在经典MAB问题中不存在的令人惊讶的现象，并在各个领域的顺序决策问题中具有广泛的应用。

    We consider a continuous-time multi-arm bandit problem (CTMAB), where the learner can sample arms any number of times in a given interval and obtain a random reward from each sample, however, increasing the frequency of sampling incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining large reward and incurring sampling cost as a function of the sampling frequency. The goal is to design a learning algorithm that minimizes regret, that is defined as the difference of the payoff of the oracle policy and that of the learning algorithm. CTMAB is fundamentally different than the usual multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial in CTMAB, since the optimal sampling frequency depends on the mean of the arm, which needs to be estimated. We first establish lower bounds on the regret achievable with any algorithm and then propose algorithms that achieve the lower bound up to logarithmic factors. For the single-arm case, we show that the lower
    
[^24]: 非利普希茨网络的鲁棒性分析

    An Analysis of Robustness of Non-Lipschitz Networks. (arXiv:2010.06154v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.06154](http://arxiv.org/abs/2010.06154)

    本文研究了深度非利普希茨网络的鲁棒性问题，定义了一个攻击模型帮助理解内在属性，证明了此类攻击者可以战胜所有必须对其输入进行分类的算法，但也提出了克服此类攻击者的方法，进一步提供了理论保证并为最近邻算法提供了新的鲁棒性保证。

    

    尽管已经取得了显著进展，但深度网络仍然极易受到对抗攻击的影响。其中一个根本性的挑战是：即使输入略微扰动，也可能会产生网络最终层特征空间中的大幅移动。在本文中，我们定义了一个攻击模型来抽象这个挑战，以帮助理解它的内在属性。在我们的模型中，对手可以在特征空间中的任意距离上移动数据，但只能在随机的低维子空间内进行操作。我们证明了这种攻击者可以非常强大：它们可以战胜任何必须对其收到的所有输入进行分类的算法。然而，通过允许算法放弃处理不寻常的输入，我们表明当类在特征空间中相对分离得很好时，这种攻击者是可以被克服的。我们进一步提供了强有力的理论保证，以使用数据驱动方法设置算法参数以优化精度-放弃权衡。我们的结果为最近邻算法提供了新的鲁棒性保证。

    Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, a
    

