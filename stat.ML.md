# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Metric Learning from Limited Pairwise Preference Comparisons](https://arxiv.org/abs/2403.19629) | 在有限成对偏好比较下研究度量学习，表明虽然无法学习单个理想项目，但当比较对象表现出低维结构时，每个用户可以帮助学习限制在低维子空间中的度量。 |
| [^2] | [Top-$k$ Classification and Cardinality-Aware Prediction](https://arxiv.org/abs/2403.19625) | 该论文研究了Top-$k$分类任务，并提出了基于基数感知的损失函数，通过实例相关的成本敏感学习，引入了新的基数感知算法 |
| [^3] | [Taming the Interactive Particle Langevin Algorithm -- the superlinear case](https://arxiv.org/abs/2403.19587) | 我们提出了一种新颖的 tamed interactive particle Langevin algorithms（tIPLA）类算法，能够在多项式增长情况下获得稳定且具有最佳速率的非渐近收敛误差估计。 |
| [^4] | [Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering](https://arxiv.org/abs/2403.19516) | 本文研究了有向图聚类问题，提出了一种基于有向随机块模型的最大似然估计方法，并引入了两种高效且可解释的有向聚类算法。 |
| [^5] | [Tensor Network-Constrained Kernel Machines as Gaussian Processes](https://arxiv.org/abs/2403.19500) | 本文证明了Canonical Polyadic Decomposition和Tensor Train约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程，并且通过实验证明了Tensor Train模型相对于Canonical Polyadic Decomposition模型具有更多高斯过程行为。 |
| [^6] | [Regression with Multi-Expert Deferral](https://arxiv.org/abs/2403.19494) | 这是一个回归问题的新框架，涉及将预测推迟给多个专家，提出了单阶段和双阶段情景的全面分析，并引入了新的代理损失函数及其支持的一致性界限。 |
| [^7] | [$H$-Consistency Guarantees for Regression](https://arxiv.org/abs/2403.19480) | 提出了通用的$H$一致性界工具来分析回归问题，并针对平方损失推导出一系列新颖的$H$一致性界；基于对$H$一致性的分析，为对抗性回归提供了有原则的代理损失。 |
| [^8] | [Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients](https://arxiv.org/abs/2403.19448) | 研究了基于Fisher信息矩阵的自然梯度方法在线性规划中的应用，展示了线性收敛性，提出了改进现有结果的熵正则化误差估计，并对扰动的Fisher-Rao梯度流和自然梯度流的次线性收敛性进行了研究。 |
| [^9] | [On Uncertainty Quantification for Near-Bayes Optimal Algorithms](https://arxiv.org/abs/2403.19381) | 该论文提出了一种基于常用机器学习算法的近似贝叶斯最优方法，可以恢复由未知任务分布定义的贝叶斯后验，并提出了一种通用的不确定性量化方法。 |
| [^10] | [Causal-StoNet: Causal Inference for High-Dimensional Complex Data](https://arxiv.org/abs/2403.18994) | 本文提出了一种基于深度学习技术的新颖因果推断方法，用于处理高维复杂数据。 |
| [^11] | [A Geometric Explanation of the Likelihood OOD Detection Paradox](https://arxiv.org/abs/2403.18910) | 高似然区域将不会被生成如果它们包含最小概率质量，基于此观察提出了一种通过本地固有维度估计进行离群检测的方法 |
| [^12] | [When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings](https://arxiv.org/abs/2403.12984) | 将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性 |
| [^13] | [Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates](https://arxiv.org/abs/2403.11687) | 在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。 |
| [^14] | [CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control](https://arxiv.org/abs/2403.07728) | CAS框架允许在在线选择性预测中控制FCR，通过自适应选择和校准集构造输出符合预测区间 |
| [^15] | [Sharp bounds for the max-sliced Wasserstein distance](https://arxiv.org/abs/2403.00666) | 对于可分希尔伯特空间上的概率测度与其经验分布之间的最大切片1-Wasserstein距离，得到了尖锐的上下界限。 |
| [^16] | [Imbalanced Data Clustering using Equilibrium K-Means](https://arxiv.org/abs/2402.14490) | Equilibrium K-Means（EKM）是一种新颖且简单的K均值类型算法，通过减少聚类中心在大类簇中心聚集的倾向，显著改善了不平衡数据的聚类结果。 |
| [^17] | [Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees](https://arxiv.org/abs/2301.13375) | 引入了基于最优输运扰动的安全强化学习框架，通过构建最坏情况的虚拟状态转换以提升鲁棒性能和安全性。 |
| [^18] | [Bi-objective Ranking and Selection Using Stochastic Kriging](https://arxiv.org/abs/2209.03919) | 提出了一种使用随机克里金的贝叶斯双目标排序和选择方法，以减少在识别具有最佳期望性能解时的错误分类 |
| [^19] | [Max-Utility Based Arm Selection Strategy For Sequential Query Recommendations](https://arxiv.org/abs/2108.13810) | 提出一种基于最大效用的臂选择策略，以减少在序列查询推荐中的累积遗憾。 |
| [^20] | [Random Vector Functional Link Networks for Function Approximation on Manifolds](https://arxiv.org/abs/2007.15776) | 本文提供了对于连续函数在紧致域上的通用逼近器的严格证明，填补了单层神经网络随机向量功能链接网络在实践中成功的理论缺口 |
| [^21] | [SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping.](http://arxiv.org/abs/2310.01201) | SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。 |
| [^22] | [Leveraging Variational Autoencoders for Parameterized MMSE Channel Estimation.](http://arxiv.org/abs/2307.05352) | 本文提出利用变分自编码器进行信道估计，通过对条件高斯信道模型的内部结构进行参数化逼近来获得均方根误差最优信道估计器，同时给出了基于变分自编码器的估计器的实用性考虑和三种不同训练方式的估计器变体。 |
| [^23] | [Differentially Private Distributed Estimation and Learning.](http://arxiv.org/abs/2306.15865) | 本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。 |
| [^24] | [Targeted collapse regularized autoencoder for anomaly detection: black hole at the center.](http://arxiv.org/abs/2306.12627) | 本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。 |

# 详细

[^1]: 有限成对偏好比较下的度量学习

    Metric Learning from Limited Pairwise Preference Comparisons

    [https://arxiv.org/abs/2403.19629](https://arxiv.org/abs/2403.19629)

    在有限成对偏好比较下研究度量学习，表明虽然无法学习单个理想项目，但当比较对象表现出低维结构时，每个用户可以帮助学习限制在低维子空间中的度量。

    

    我们研究了在理想点模型下的偏好比较中的度量学习，其中用户如果一个项目比其潜在理想项目更接近，则更喜欢该项目。这些项目嵌入到具有未知马氏距离的$\mathbb{R}^d$中，该距离在用户间共享。尽管最近的工作表明，通过每个用户$\mathcal{O}(d)$个成对比较可以同时恢复度量和理想项目，但在实践中，我们经常有$o(d)$的有限比较预算。我们研究了即使已知学习单个理想项目现在不再可能，度量是否仍然可以恢复。我们发现一般来说，$o(d)$比较不会揭示有关度量的信息，即使用户数量无限。然而，当比较的项目表现出低维结构时，每个用户都可以有助于学习限制在低维子空间中的度量，这样度量就可以被恢复。

    arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric
    
[^2]: 基于Top-$k$分类和基数感知预测

    Top-$k$ Classification and Cardinality-Aware Prediction

    [https://arxiv.org/abs/2403.19625](https://arxiv.org/abs/2403.19625)

    该论文研究了Top-$k$分类任务，并提出了基于基数感知的损失函数，通过实例相关的成本敏感学习，引入了新的基数感知算法

    

    我们详细研究了Top-$k$分类，即预测输入的$k$个最有可能的类别，超越了单类别预测。我们证明了多类别分类中几种流行的替代损失函数，如comp-sum和约束损失，对于Top-$k$损失具有关于$h$-一致性边界的支持。这些边界保证了关于假设集$H$的一致性，由于它们的非渐近和特定假设集性质，提供比贝叶斯一致性更强的保证。为了解决准确性和基数$k$之间的折衷，我们进一步通过实例相关的成本敏感学习引入了基数感知损失函数。对于这些函数，我们推导了成本敏感comp-sum和约束替代损失，建立了它们的$H$-一致性边界和贝叶斯一致性。最小化这些损失导致了新的基数感知算法

    arXiv:2403.19625v1 Announce Type: new  Abstract: We present a detailed study of top-$k$ classification, the task of predicting the $k$ most probable classes for an input, extending beyond single-class prediction. We demonstrate that several prevalent surrogate loss functions in multi-class classification, such as comp-sum and constrained losses, are supported by $H$-consistency bounds with respect to the top-$k$ loss. These bounds guarantee consistency in relation to the hypothesis set $H$, providing stronger guarantees than Bayes-consistency due to their non-asymptotic and hypothesis-set specific nature. To address the trade-off between accuracy and cardinality $k$, we further introduce cardinality-aware loss functions through instance-dependent cost-sensitive learning. For these functions, we derive cost-sensitive comp-sum and constrained surrogate losses, establishing their $H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to new cardinality-aware algorithm
    
[^3]: 驯服交互式粒子 Langevin 算法 -- 超线性情况

    Taming the Interactive Particle Langevin Algorithm -- the superlinear case

    [https://arxiv.org/abs/2403.19587](https://arxiv.org/abs/2403.19587)

    我们提出了一种新颖的 tamed interactive particle Langevin algorithms（tIPLA）类算法，能够在多项式增长情况下获得稳定且具有最佳速率的非渐近收敛误差估计。

    

    最近在随机优化方面的进展产生了交互式粒子 Langevin 算法（IPLA），该算法利用交互粒子系统（IPS）的概念来高效地从近似后验密度中抽样。这在期望最大化（EM）框架中变得尤为关键，其中 E 步骤在计算上具有挑战性甚至是难以处理的。尽管先前的研究侧重于梯度最多线性增长的凸情况，我们的工作将此框架扩展到包括多项式增长。采用驯服技术生成明确的离散化方案，从而产生一类稳定的、在这种非线性情况下，称为驯服交互式粒子 Langevin 算法（tIPLA）的算法。我们获得了新类在 Wasserstein-2 距离下的非渐近收敛误差估计，具有最佳速率。

    arXiv:2403.19587v1 Announce Type: cross  Abstract: Recent advances in stochastic optimization have yielded the interactive particle Langevin algorithm (IPLA), which leverages the notion of interacting particle systems (IPS) to efficiently sample from approximate posterior densities. This becomes particularly crucial within the framework of Expectation-Maximization (EM), where the E-step is computationally challenging or even intractable. Although prior research has focused on scenarios involving convex cases with gradients of log densities that grow at most linearly, our work extends this framework to include polynomial growth. Taming techniques are employed to produce an explicit discretization scheme that yields a new class of stable, under such non-linearities, algorithms which are called tamed interactive particle Langevin algorithms (tIPLA). We obtain non-asymptotic convergence error estimates in Wasserstein-2 distance for the new class under an optimal rate.
    
[^4]: 针对有向图聚类问题的随机块模型最大似然估计

    Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering

    [https://arxiv.org/abs/2403.19516](https://arxiv.org/abs/2403.19516)

    本文研究了有向图聚类问题，提出了一种基于有向随机块模型的最大似然估计方法，并引入了两种高效且可解释的有向聚类算法。

    

    本文通过统计学的视角研究了有向图聚类问题，将聚类问题建模为有向随机块模型（DSBM）中潜在社区的估计。我们对DSBM进行最大似然估计（MLE），从而确定给定观察到的图结构时最可能的社区分配。除了统计观点外，我们进一步建立了这种MLE公式与一种新颖的流优化启发式之间的等价性，该启发式同时考虑了两个重要的有向图统计量：边密度和边方向。基于这种有向聚类的新公式，我们引入了两种高效且可解释的有向聚类算法，分别是谱聚类算法和基于半定规划的聚类算法。我们为谱聚类算法的错误聚类顶点数提供了一个理论上界。

    arXiv:2403.19516v1 Announce Type: cross  Abstract: This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation. Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algor
    
[^5]: 张量网络约束的核机器作为高斯过程

    Tensor Network-Constrained Kernel Machines as Gaussian Processes

    [https://arxiv.org/abs/2403.19500](https://arxiv.org/abs/2403.19500)

    本文证明了Canonical Polyadic Decomposition和Tensor Train约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程，并且通过实验证明了Tensor Train模型相对于Canonical Polyadic Decomposition模型具有更多高斯过程行为。

    

    张量网络（TNs）最近被用来通过约束模型权重加快核机器的速度，产生了指数级的计算和存储节约。在本文中，我们证明Canonical Polyadic Decomposition（CPD）和Tensor Train（TT）约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程（GP），我们完全表征了这一过程。我们分析了CPD和TT约束模型的收敛性，并展示了TT相对于CPD具有更多GP行为的模型，而模型参数的数量相同。我们通过两个数值实验在两个方面实证观察了这一行为，分别是分析到GP的收敛性和预测性能。因此，我们建立了张量网络约束的核机器和高斯过程之间的联系。

    arXiv:2403.19500v1 Announce Type: new  Abstract: Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.
    
[^6]: 具有多专家推迟的回归

    Regression with Multi-Expert Deferral

    [https://arxiv.org/abs/2403.19494](https://arxiv.org/abs/2403.19494)

    这是一个回归问题的新框架，涉及将预测推迟给多个专家，提出了单阶段和双阶段情景的全面分析，并引入了新的代理损失函数及其支持的一致性界限。

    

    学习与多个专家推迟是一个框架，其中学习者可以选择将预测推迟给多个专家。虽然在分类情境中该问题得到了重视，但由于标签空间的无限和连续特性，它在回归中面临独特挑战。在这项工作中，我们引入了一个具有推迟的回归新框架，其中涉及将预测推迟给多个专家。我们针对单阶段情景和双阶段情景进行了全面分析，前者涉及预测器和推迟函数的同时学习，后者涉及具有已训练预测器和学习推迟函数。我们为两种情景引入了新的代理损失函数，并证明它们受到$H$-一致性界限的支持。这些界限提供了比贝叶斯一致性更强的一致性保证，因为它们是非渐近的，且假设集

    arXiv:2403.19494v1 Announce Type: new  Abstract: Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set
    
[^7]: 回归问题中的$H$一致性保证

    $H$-Consistency Guarantees for Regression

    [https://arxiv.org/abs/2403.19480](https://arxiv.org/abs/2403.19480)

    提出了通用的$H$一致性界工具来分析回归问题，并针对平方损失推导出一系列新颖的$H$一致性界；基于对$H$一致性的分析，为对抗性回归提供了有原则的代理损失。

    

    我们对回归问题中的$H$一致性界进行了详细研究。我们首先提出了一些新的定理，这些定理推广了先前用于建立$H$一致性界的工具。这种概括对于分析针对回归问题特定的$H$一致性界至关重要。接下来，我们在对称分布和有界假设集的条件下，证明了一系列关于平方损失的新颖$H$一致性界，包括Huber损失、所有$\ell_p$损失（$p \geq 1$）、平方$\epsilon$-不敏感损失的正结果，以及对于在平方支持向量回归（SVR）中使用的$\epsilon$-不敏感损失的负结果。我们进一步利用对回归问题中$H$一致性的分析，并提出了针对对抗回归的有原则的代理损失（第5节）。这为对抗性回归建立了新颖的算法，我们报告了有利的实验结果。

    arXiv:2403.19480v1 Announce Type: new  Abstract: We present a detailed study of $H$-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency bounds specific to regression. Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all $\ell_p$ losses, $p \geq 1$, the squared $\epsilon$-insensitive loss, as well as a negative result for the $\epsilon$-insensitive loss used in squared Support Vector Regression (SVR). We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable exp
    
[^8]: Fisher-Rao线性规划和状态-动作自然策略梯度的梯度流

    Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients

    [https://arxiv.org/abs/2403.19448](https://arxiv.org/abs/2403.19448)

    研究了基于Fisher信息矩阵的自然梯度方法在线性规划中的应用，展示了线性收敛性，提出了改进现有结果的熵正则化误差估计，并对扰动的Fisher-Rao梯度流和自然梯度流的次线性收敛性进行了研究。

    

    Kakade的自然策略梯度方法近年来得到广泛研究，表明在有或无正则化的情况下具有线性收敛性。我们研究了另一种基于状态-动作分布的Fisher信息矩阵的自然梯度方法，但在理论方面接受度较低。在这里，状态-动作分布在状态-动作多面体内遵循Fisher-Rao梯度流，相对于线性势。因此，我们更全面地研究线性规划的Fisher-Rao梯度流，并显示了线性收敛性，其速率取决于线性规划的几何特性。换句话说，这提供了线性规划的熵正则化引起的误差估计，这改进了现有结果。我们拓展了这些结果，并展示了对扰动的Fisher-Rao梯度流和自然梯度流的次线性收敛性，直到逼近误差。

    arXiv:2403.19448v1 Announce Type: cross  Abstract: Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particul
    
[^9]: 关于近贝叶斯最优算法的不确定性量化

    On Uncertainty Quantification for Near-Bayes Optimal Algorithms

    [https://arxiv.org/abs/2403.19381](https://arxiv.org/abs/2403.19381)

    该论文提出了一种基于常用机器学习算法的近似贝叶斯最优方法，可以恢复由未知任务分布定义的贝叶斯后验，并提出了一种通用的不确定性量化方法。

    

    贝叶斯建模允许对预测不确定性进行量化，在安全关键应用中至关重要。然而，对于许多机器学习（ML）算法，构建或实现它们的贝叶斯对应是困难的。 在这项工作中，我们提出了一种解决这一挑战的有前途的方法，该方法基于常用的ML算法在各种任务中高效，并且可能在未知任务分布下接近贝叶斯最优。我们证明了通过使用该算法构建一个鞅后验，可以恢复由任务分布定义的贝叶斯后验，在这种设置中是未知但最优的。我们进一步提出了一种适用于通用ML算法的实用不确定性量化方法。基于各种非NN和NN算法的实验表明了我们方法的效果。

    arXiv:2403.19381v1 Announce Type: cross  Abstract: Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.
    
[^10]: Causal-StoNet: 高维复杂数据的因果推断

    Causal-StoNet: Causal Inference for High-Dimensional Complex Data

    [https://arxiv.org/abs/2403.18994](https://arxiv.org/abs/2403.18994)

    本文提出了一种基于深度学习技术的新颖因果推断方法，用于处理高维复杂数据。

    

    随着数据科学的发展，收集越来越复杂的数据集已经变得司空见惯。在这些数据集中，数据维度可能非常高，并且潜在的数据生成过程可能是未知的，高度非线性的。因此，在许多领域，如医学、计量经济学和社会科学，对高维复杂数据进行因果推断的任务已经成为一个基本问题。然而，现有的因果推断方法通常是在假设数据维度较低或潜在数据生成过程为线性或近似线性的情况下开发的。为了解决这些挑战，本文提出了一种新颖的因果推断方法，用于处理高维复杂数据。所提出的方法基于深度学习技术，包括最近发展的稀疏深度学习理论和随机神经网络。

    arXiv:2403.18994v1 Announce Type: cross  Abstract: With the advancement of data science, the collection of increasingly complex datasets has become commonplace. In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear. As a result, the task of making causal inference with high-dimensional complex data has become a fundamental problem in many disciplines, such as medicine, econometrics, and social science. However, the existing methods for causal inference are frequently developed under the assumption that the data dimension is low or that the underlying data generation process is linear or approximately linear. To address these challenges, this paper proposes a novel causal inference approach for dealing with high-dimensional complex data. The proposed approach is based on deep learning techniques, including sparse deep learning theory and stochastic neural networks, that have been developed in recent lit
    
[^11]: 对离群数据检测悖论的似然几何解释

    A Geometric Explanation of the Likelihood OOD Detection Paradox

    [https://arxiv.org/abs/2403.18910](https://arxiv.org/abs/2403.18910)

    高似然区域将不会被生成如果它们包含最小概率质量，基于此观察提出了一种通过本地固有维度估计进行离群检测的方法

    

    基于似然的深度生成模型(DGMs)通常表现出令人困惑的行为：当在相对复杂的数据集上训练时，它们会给来自更简单来源的离群数据赋予更高的似然值。更使人感到神秘的是，尽管具有更高的似然值，但这些DGMs从未生成过离群样本。这个双管齐下的悖论尚未得到最终解释，使得基于似然的离群检测不可靠。我们的主要观察是，如果高似然区域中包含了最小概率质量，那么这些区域将不会被生成。我们演示了在围绕低维流形数据的地方可能出现大密度但低概率质量的看似矛盾情况。我们还展示了通过本地固有维度(LID)估计可以识别这种场景，并提出了一种通过预训练的DGM获得的似然和LID估计相配对的离群检测方法。

    arXiv:2403.18910v1 Announce Type: cross  Abstract: Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can b
    
[^12]: 当SMILES拥有语言：使用文本分类方法对药物SMILES字符串进行药物分类

    When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings

    [https://arxiv.org/abs/2403.12984](https://arxiv.org/abs/2403.12984)

    将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性

    

    复杂的化学结构，如药物，通常由SMILES字符串来定义，作为分子和键的序列。这些SMILES字符串在不同的基于机器学习的药物相关研究和表示工作中使用。在这项工作中，我们摆脱复杂的表示法，提出了一个问题：如果我们将药物SMILES视为常规句子，并进行文本分类以进行药物分类会怎样？我们的实验证实了这种可能性，获得了非常有竞争力的分数。该研究探讨了将每个原子和键视为句子组件的概念，利用基本的自然语言处理方法对药物类型进行分类，表明复杂的问题也可以用更简单的视角来解决。数据和代码可在此处找到：https://github.com/azminewasi/Drug-Classification-NLP。

    arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
    
[^13]: 非光滑隐式微分：确定性和随机收敛速率

    Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates

    [https://arxiv.org/abs/2403.11687](https://arxiv.org/abs/2403.11687)

    在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。

    

    我们研究了有效计算参数化不可微收缩映射固定点导数的问题。这个问题在机器学习中有广泛的应用，包括超参数优化、元学习和数据污染攻击。我们分析了两种流行的方法：迭代微分（ITD）和近似隐式微分（AID）。在非光滑设置中的一个关键挑战是链规则不再成立。在Bolte等人（2022）最近的工作基础上，他们证明了不可微分ITD的线性收敛，我们提供了确定性情况下ITD和AID的改进线性收敛速率。我们进一步介绍了NSID，一种新的方法，用于在固定点被定义为只通过随机无偏估计器访问的外映射和内映射的组合时计算隐式导数。我们建立了该方法的收敛速率。

    arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
    
[^14]: CAS: 一种具有FCR控制的在线选择性符合预测的通用算法

    CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control

    [https://arxiv.org/abs/2403.07728](https://arxiv.org/abs/2403.07728)

    CAS框架允许在在线选择性预测中控制FCR，通过自适应选择和校准集构造输出符合预测区间

    

    我们研究了在线方式下后选择预测推断的问题。为了避免将资源耗费在不重要的单位上，在报告其预测区间之前对当前个体进行初步选择在在线预测任务中是常见且有意义的。由于在线选择导致所选预测区间中存在时间多重性，因此控制实时误覆盖陈述率（FCR）来测量平均误覆盖误差是重要的。我们开发了一个名为CAS（适应性选择后校准）的通用框架，可以包裹任何预测模型和在线选择规则，以输出后选择的预测区间。如果选择了当前个体，我们首先对历史数据进行自适应选择来构建校准集，然后为未观察到的标签输出符合预测区间。我们为校准集提供了可行的构造方式

    arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
    
[^15]: 最大切片Wasserstein距离的尖锐界限

    Sharp bounds for the max-sliced Wasserstein distance

    [https://arxiv.org/abs/2403.00666](https://arxiv.org/abs/2403.00666)

    对于可分希尔伯特空间上的概率测度与其经验分布之间的最大切片1-Wasserstein距离，得到了尖锐的上下界限。

    

    我们得到了关于在可分希尔伯特空间上的概率测度与从$n$个样本中获得的经验分布之间期望的最大切片1-Wasserstein距离的尖锐上下界。我们还得到了一个适用于Banach空间上的概率测度的版本。

    arXiv:2403.00666v1 Announce Type: cross  Abstract: We obtain sharp upper and lower bounds for the expected max-sliced 1-Wasserstein distance between a probability measure on a separable Hilbert space and its empirical distribution from $n$ samples. A version of this result for probability measures on Banach spaces is also obtained.
    
[^16]: 使用Equilibrium K-Means进行不平衡数据聚类

    Imbalanced Data Clustering using Equilibrium K-Means

    [https://arxiv.org/abs/2402.14490](https://arxiv.org/abs/2402.14490)

    Equilibrium K-Means（EKM）是一种新颖且简单的K均值类型算法，通过减少聚类中心在大类簇中心聚集的倾向，显著改善了不平衡数据的聚类结果。

    

    不平衡数据指的是数据点在不同类别之间分布不均衡，这给传统的硬聚类算法和模糊聚类算法（如硬K均值（HKM，或者Lloyd算法）和模糊K均值（FKM，或者Bezdek算法））带来了挑战。本文介绍了一种新颖且简单的K均值类型算法——Equilibrium K-Means（EKM），它在两个步骤之间交替进行，显著改善了不平衡数据的聚类结果，减少了聚类中心向大类簇中心聚集的倾向。我们还提出了对HKM、FKM和EKM的统一视角，表明它们本质上是具有明确关系的牛顿方法的梯度下降算法。EKM具有与FKM相同的时间和空间复杂度，但对其成员定义提供了更清晰的物理意义。我们在两个合成数据集和十个真实数据集上展示了EKM的性能，并将其与各种聚类算法进行了比较。

    arXiv:2402.14490v1 Announce Type: new  Abstract: Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various cl
    
[^17]: 使用带有鲁棒性保证的最优输运扰动进行安全强化学习

    Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees

    [https://arxiv.org/abs/2301.13375](https://arxiv.org/abs/2301.13375)

    引入了基于最优输运扰动的安全强化学习框架，通过构建最坏情况的虚拟状态转换以提升鲁棒性能和安全性。

    

    基于最优输运成本不确定性集，引入了一个安全强化学习框架，通过应用最优输运扰动来构建最坏情况的虚拟状态转换，提供了一种有效的实现方法。在连续控制任务的实验中，我们的方法展示了鲁棒性能，并显著提高了部署时的安全性。

    arXiv:2301.13375v2 Announce Type: replace-cross  Abstract: Robustness and safety are critical for the trustworthy deployment of deep reinforcement learning. Real-world decision making applications require algorithms that can guarantee robust performance and safety in the presence of general environment disturbances, while making limited assumptions on the data collection process during training. In order to accomplish this goal, we introduce a safe reinforcement learning framework that incorporates robustness through the use of an optimal transport cost uncertainty set. We provide an efficient implementation based on applying Optimal Transport Perturbations to construct worst-case virtual state transitions, which does not impact data collection during training and does not require detailed simulator access. In experiments on continuous control tasks with safety constraints, our approach demonstrates robust performance while significantly improving safety at deployment time compared to 
    
[^18]: 使用随机克里金的双目标排序和选择

    Bi-objective Ranking and Selection Using Stochastic Kriging

    [https://arxiv.org/abs/2209.03919](https://arxiv.org/abs/2209.03919)

    提出了一种使用随机克里金的贝叶斯双目标排序和选择方法，以减少在识别具有最佳期望性能解时的错误分类

    

    我们考虑双目标排序和选择问题，目标是在观察到两个目标结果具有不确定性的候选集中正确识别帕累托最优解，例如，在运行多目标随机模拟优化过程之后。在识别这些解时，观测性能的噪声扰动可能导致两种错误：真正帕累托最优的解可能被错误地认为是被支配的，而真正被支配的解可能被错误地认为是帕累托最优的。我们提出了一种新颖的贝叶斯双目标排序和选择方法，通过顺序分配额外样本给竞争解，以减少在识别具有最佳期望性能的解时的错误分类。该方法使用随机克里金构建客观预测分布。

    arXiv:2209.03919v3 Announce Type: replace-cross  Abstract: We consider bi-objective ranking and selection problems, where the goal is to correctly identify the Pareto optimal solutions among a finite set of candidates for which the two objective outcomes have been observed with uncertainty (e.g., after running a multiobjective stochastic simulation optimization procedure). When identifying these solutions, the noise perturbing the observed performance may lead to two types of errors: solutions that are truly Pareto-optimal can be wrongly considered dominated, and solutions that are truly dominated can be wrongly considered Pareto-optimal. We propose a novel Bayesian bi-objective ranking and selection method that sequentially allocates extra samples to competitive solutions, in view of reducing the misclassification errors when identifying the solutions with the best expected performance. The approach uses stochastic kriging to build reliable predictive distributions of the objective ou
    
[^19]: 基于最大效用的序列查询推荐中的臂选择策略

    Max-Utility Based Arm Selection Strategy For Sequential Query Recommendations

    [https://arxiv.org/abs/2108.13810](https://arxiv.org/abs/2108.13810)

    提出一种基于最大效用的臂选择策略，以减少在序列查询推荐中的累积遗憾。

    

    我们考虑在闭环交互式学习设置中的查询推荐问题，例如在线信息收集和探索分析。该问题可以自然地使用多臂老虎机（MAB）框架来建模，其中有可数个臂。标准的可数臂MAB算法从选择一个随机的候选臂集开始，然后在这个候选集合上应用标准的MAB算法，例如UCB。我们展示了这样的选择策略通常会导致更高的累积遗憾，为此，我们提出了一种基于臂的最大效用的选择策略。我们展示了在诸如在线信息收集这样的任务中，其中采用了序列查询推荐，查询序列是相关的，并且通过选择相对于当前执行查询具有最大效用的查询，可以将潜在最佳查询的数量减少到一个可管理的大小。

    arXiv:2108.13810v1 Announce Type: cross  Abstract: We consider the query recommendation problem in closed loop interactive learning settings like online information gathering and exploratory analytics. The problem can be naturally modelled using the Multi-Armed Bandits (MAB) framework with countably many arms. The standard MAB algorithms for countably many arms begin with selecting a random set of candidate arms and then applying standard MAB algorithms, e.g., UCB, on this candidate set downstream. We show that such a selection strategy often results in higher cumulative regret and to this end, we propose a selection strategy based on the maximum utility of the arms. We show that in tasks like online information gathering, where sequential query recommendations are employed, the sequences of queries are correlated and the number of potentially optimal queries can be reduced to a manageable size by selecting queries with maximum utility with respect to the currently executing query. Our
    
[^20]: 用于流形上函数逼近的随机向量功能链接网络

    Random Vector Functional Link Networks for Function Approximation on Manifolds

    [https://arxiv.org/abs/2007.15776](https://arxiv.org/abs/2007.15776)

    本文提供了对于连续函数在紧致域上的通用逼近器的严格证明，填补了单层神经网络随机向量功能链接网络在实践中成功的理论缺口

    

    feed-forward神经网络的学习速度因慢而著名，在深度学习应用中已经成为瓶颈数十年。为了应对这一问题，研究人员和实践者尝试引入随机性来减少学习需求。基于Igelnik和Pao的原始构造，具有随机输入到隐藏层权重和偏置的单层神经网络在实践中取得成功，但缺乏必要的理论证明。本文填补了这一理论空白。我们提供了一个（更正的）严格证明，证明Igelnik和Pao的构造是一个连续函数在紧致域上的通用逼近器，逼近误差像渐近衰减。

    arXiv:2007.15776v3 Announce Type: replace-cross  Abstract: The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically lik
    
[^21]: SWoTTeD:张量分解在时间表征中的扩展

    SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping. (arXiv:2310.01201v1 [cs.LG])

    [http://arxiv.org/abs/2310.01201](http://arxiv.org/abs/2310.01201)

    SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。

    

    张量分解最近在机器学习领域对于个体追踪数据的分析，如电子健康记录(EHR)，引起了人们的关注。然而，当数据遵循复杂的时间模式时，这个任务变得更加困难。本文引入了时间表征的概念，即一组随时间变化的特征，并提出了SWoTTeD (Sliding Window for Temporal Tensor Decomposition)方法，一种发现隐藏时间模式的新方法。SWoTTeD集成了多种约束和正则化方法，以增强提取到的表征的可解释性。我们使用合成数据集和真实世界数据集进行验证，并提供了使用巴黎大学医院的数据的原始用例。结果表明，SWoTTeD能够至少与最新的基于张量分解的模型一样准确地重建数据，并提取到对临床医生有意义的时间表征。

    Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
    
[^22]: 利用变分自编码器进行参数化MMSE信道估计

    Leveraging Variational Autoencoders for Parameterized MMSE Channel Estimation. (arXiv:2307.05352v1 [eess.SP])

    [http://arxiv.org/abs/2307.05352](http://arxiv.org/abs/2307.05352)

    本文提出利用变分自编码器进行信道估计，通过对条件高斯信道模型的内部结构进行参数化逼近来获得均方根误差最优信道估计器，同时给出了基于变分自编码器的估计器的实用性考虑和三种不同训练方式的估计器变体。

    

    在本文中，我们提出利用基于生成神经网络的变分自编码器进行信道估计。变分自编码器以一种新颖的方式将真实但未知的信道分布建模为条件高斯分布。所得到的信道估计器利用变分自编码器的内部结构对来自条件高斯信道模型的均方误差最优估计器进行参数化逼近。我们提供了严格的分析，以确定什么条件下基于变分自编码器的估计器是均方误差最优的。然后，我们提出了使基于变分自编码器的估计器实用的考虑因素，并提出了三种不同的估计器变体，它们在训练和评估阶段对信道知识的获取方式不同。特别地，仅基于噪声导频观测进行训练的所提出的估计器变体非常值得注意，因为它不需要获取信道训练。

    In this manuscript, we propose to utilize the generative neural network-based variational autoencoder for channel estimation. The variational autoencoder models the underlying true but unknown channel distribution as a conditional Gaussian distribution in a novel way. The derived channel estimator exploits the internal structure of the variational autoencoder to parameterize an approximation of the mean squared error optimal estimator resulting from the conditional Gaussian channel models. We provide a rigorous analysis under which conditions a variational autoencoder-based estimator is mean squared error optimal. We then present considerations that make the variational autoencoder-based estimator practical and propose three different estimator variants that differ in their access to channel knowledge during the training and evaluation phase. In particular, the proposed estimator variant trained solely on noisy pilot observations is particularly noteworthy as it does not require access
    
[^23]: 差分隐私分布式估计和学习

    Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])

    [http://arxiv.org/abs/2306.15865](http://arxiv.org/abs/2306.15865)

    本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。

    

    我们研究了在网络环境中的分布式估计和学习问题，其中代理通过交换信息来估计从其私下观察的样本中未知的统计属性。通过交换私有观测信息，代理可以集体估计未知数量，但他们也面临隐私风险。我们的聚合方案的目标是在时间和网络中高效地组合观测数据，同时满足代理的隐私需求，而不需要任何超越他们本地附近的协调。我们的算法使参与的代理能够从离线或随时间在线获取的私有信号中估计完整的充分统计量，并保护其信号和网络附近的隐私。这是通过线性聚合方案和调整的随机化方案实现的，将噪声添加到交换的估计数据中以满足差分隐私（DP）。

    We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
    
[^24]: 针对异常检测的目标塌缩正则化自编码器：中心的黑洞

    Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])

    [http://arxiv.org/abs/2306.12627](http://arxiv.org/abs/2306.12627)

    本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。

    

    自编码器已广泛用于最近的异常检测技术开发中。它们的应用前提是在正常训练数据上训练自编码器后，异常输入将表现出显著的重构误差。因此，这使得正常和异常样本之间有了明显的区别。然而，在实践中观察到，自编码器可以一定程度上泛化到正常类之外，并在一些异常样本上实现较小的重构误差。为了改善性能，各种技术提出了其他组件和更复杂的训练程序。在这项工作中，我们提出了一个非常简单的替代方法：不是添加神经网络组件、涉及计算和繁琐的训练，而是通过在潜在空间中调节表示的范数，用一个计算简单的项来补充重构损失。我们方法的简单性最小化了复杂性。

    Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
    

