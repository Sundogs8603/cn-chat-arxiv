# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PyVBMC: Efficient Bayesian inference in Python.](http://arxiv.org/abs/2303.09519) | PyVBMC是一种高效的Python工具，用于黑盒计算模型的贝叶斯推断和模型选择，可以处理连续参数不超过约10-15个的计算或统计模型。 |
| [^2] | [Challenges and Opportunities in Quantum Machine Learning.](http://arxiv.org/abs/2303.09491) | 量子机器学习（QML）是机器学习和量子计算交叉领域中的前沿研究方向，具有应用于量子材料、生物化学和高能物理等领域的潜力，但其模型的可训练性仍有挑战，需要进一步解决。本文回顾了当前QML方法与应用，并探讨了QML的量子优势机会。 |
| [^3] | [Gradient flow on extensive-rank positive semi-definite matrix denoising.](http://arxiv.org/abs/2303.09474) | 本文提出了一种新的方法来分析广义秩和高维情况下正半定矩阵去噪问题的梯度流，揭示了其中的连续相变。 |
| [^4] | [Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels.](http://arxiv.org/abs/2303.09470) | 本文提出了结合类中心距离和异常值折扣的方法，用于解决在存在噪声标签的情况下训练机器学习模型的问题，并通过实验证明了其有效性 。 |
| [^5] | [On the Existence of a Complexity in Fixed Budget Bandit Identification.](http://arxiv.org/abs/2303.09468) | 该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。 |
| [^6] | [Distributionally Robust Optimization using Cost-Aware Ambiguity Sets.](http://arxiv.org/abs/2303.09408) | 本文提出了一种新的用于分布鲁棒优化的模糊集，称为成本感知模糊集，它通过半空间定义，只排除那些预计对所获得的最坏情况成本产生重大影响的分布，实现了高置信度上界和一致估计。 |
| [^7] | [On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits.](http://arxiv.org/abs/2303.09390) | 本文研究了线性情境赌博机在错误规定的情境下的算法问题，提出一种新算法，将在一定水平内误差和最小次优间隙相互制约，以常数误差上限实现间隙相关的度量。 |
| [^8] | [Unsupervised domain adaptation by learning using privileged information.](http://arxiv.org/abs/2303.09350) | 本文提出利用特权信息进行领域适应（DALUPI）算法，以在学习中放宽假设条件并提高样本效率，通过减少错误来促进医学图像分析等应用的发展。 |
| [^9] | [Orthogonal Directions Constrained Gradient Method: from non-linear equality constraints to Stiefel manifold.](http://arxiv.org/abs/2303.09261) | 提出了一种新的算法——ODCGM，可以优化在光滑流形上最小化非凸函数的问题。算法的实现要简单得多，并且在确定性和随机情况下具有近似最优的误差复杂度。此外，我们还证明，在适当选择投影度量的情况下，方法可以恢复出Ablin 和 Peyr\'e (2022)的Landing算法，后者是一种最近用于优化Stiefel流形的算法。 |
| [^10] | [Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation.](http://arxiv.org/abs/2303.09154) | 本文数学上澄清了带有概念瓶颈结构和多任务组成的线性神经网络的贝叶斯泛化误差和自由能。 |
| [^11] | [High-Dimensional Penalized Bernstein Support Vector Machines.](http://arxiv.org/abs/2303.09066) | 提出一种适用于高维度情况下的平滑支持向量机铰链损失函数，即Bernstein支持向量机（BernSVM），并提出两种有效算法求解该方法，实验结果表明该方法在现有竞争对手中具有优越性。 |
| [^12] | [Learning ground states of gapped quantum Hamiltonians with Kernel Methods.](http://arxiv.org/abs/2303.08902) | 本文提出了一种利用 kernel 方法学习具有能隙的量子哈密顿量基态的统计学习方法，理论上需要多项式资源实现，通过数值模拟证明了该方法的有效性，并展示了方法的灵活性。 |
| [^13] | [Bayesian Quadrature for Neural Ensemble Search.](http://arxiv.org/abs/2303.08874) | 本论文介绍了一种使用贝叶斯积分的新方法，可以在架构似然表面有分散、狭窄峰时构建加权集成神经网络，相比当前同类方法，在测试似然性、准确性和期望校准误差方面更为优秀。 |
| [^14] | [Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction.](http://arxiv.org/abs/2212.07201) | 本文研究了环面坐标算法的问题，提出了圆值图形的几何相关性概念，并描述了一个系统性的过程，用于构建最小能量的环面值图。 |
| [^15] | [Neural Networks Efficiently Learn Low-Dimensional Representations with SGD.](http://arxiv.org/abs/2209.14863) | 本文研究使用随机梯度下降训练任意宽度的两层神经网络的问题，当输入为高斯分布，目标为多指数模型时，NN的第一层权重会收敛到真实模型中$k$维主子空间, 可以通过在子空间上使用均匀收敛建立广义误差边界为$O(\sqrt{{kd}/{T}})$, SGD训练的ReLU NN可以学习形如$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$的单指数目标。 |
| [^16] | [Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models.](http://arxiv.org/abs/2207.06950) | 本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。 |
| [^17] | [FibeRed: Fiberwise Dimensionality Reduction of Topologically Complex Data with Vector Bundles.](http://arxiv.org/abs/2206.06513) | FibeRed提出了一种通过向量丛描述拓扑复杂数据的纤维降维方法，利用该方法可以降低数据维度，同时保留其大规模拓扑特征。算法包含从局部线性降维得到的局部表示与初始全局表示相结合的过程，并在动力学系统和化学领域的数据集上表现出良好的效果。 |
| [^18] | [SRMD: Sparse Random Mode Decomposition.](http://arxiv.org/abs/2204.06108) | 该论文提出了一种通过构建纤细近似于谱图的稀疏来分析时间序列数据的方法，这种方法在信号分解和多尺度信号分析方面是先进的。 |
| [^19] | [Noisy Low-rank Matrix Optimization: Geometry of Local Minima and Convergence Rate.](http://arxiv.org/abs/2203.03899) | 本文提出了一个新的数学框架来处理噪声低秩矩阵优化问题，对受限等距常数的限制要少得多，并且只要无噪声目标的受限等距条件小于1/3，任何错误的局部优化解必须接近于真实解，可以在多项式时间内找到近似解。 |
| [^20] | [Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis.](http://arxiv.org/abs/2109.09500) | 本文研究了针对大规模验证项目因素分析的参数估计与拟合度检验方法，提出了基于深度学习的算法和扩展测试与指标，具有高效准确和有效性的特点。 |
| [^21] | [A Stochastic Sequential Quadratic Optimization Algorithm for Nonlinear Equality Constrained Optimization with Rank-Deficient Jacobians.](http://arxiv.org/abs/2106.13015) | 本研究提出了一种针对非线性等式约束优化的顺序二次优化算法，具备使用随机梯度和处理秩缺失雅可比矩阵的能力，实验结果表明其性能优于其他方法。 |
| [^22] | [Towards Lower Bounds on the Depth of ReLU Neural Networks.](http://arxiv.org/abs/2105.14835) | 该研究运用数学和优化理论方法，就 ReLU 神经网络的深度下界做了探究，有助于更好地理解这种网络所能表示的函数类的性质。此外，该研究还肯定了一项旧的分段线性函数猜想。 |
| [^23] | [Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes.](http://arxiv.org/abs/2105.08232) | 本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。 |
| [^24] | [Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems.](http://arxiv.org/abs/2007.03481) | 本文提出了一个Bayesian停时问题的逆强化学习框架，结合微观经济学中的Bayesian揭示偏好思路，通过观察Bayesian决策者的行动，确定其的最优性。并且通过两个停时问题示例得到了验证，并且已在一个真实的例子中得到了高精度地预测用户参与度。 |
| [^25] | [Sequential Gaussian Processes for Online Learning of Nonstationary Functions.](http://arxiv.org/abs/1905.10003) | 本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。 |

# 详细

[^1]: PyVBMC：Python中高效的贝叶斯推断

    PyVBMC: Efficient Bayesian inference in Python. (arXiv:2303.09519v1 [stat.ML])

    [http://arxiv.org/abs/2303.09519](http://arxiv.org/abs/2303.09519)

    PyVBMC是一种高效的Python工具，用于黑盒计算模型的贝叶斯推断和模型选择，可以处理连续参数不超过约10-15个的计算或统计模型。

    

    PyVBMC是Variational Bayesian Monte Carlo（VBMC）算法的Python实现，用于黑盒计算模型的后验和模型推断。VBMC是一种用于高效参数估计和模型评估的近似推断方法，当模型评估是有点到非常昂贵（例如第二次或更多次）和/或嘈杂时。具体而言，VBMC计算：

    PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo (VBMC) algorithm for posterior and model inference for black-box computational models (Acerbi, 2018, 2020). VBMC is an approximate inference method designed for efficient parameter estimation and model assessment when model evaluations are mildly-to-very expensive (e.g., a second or more) and/or noisy. Specifically, VBMC computes:  - a flexible (non-Gaussian) approximate posterior distribution of the model parameters, from which statistics and posterior samples can be easily extracted;  - an approximation of the model evidence or marginal likelihood, a metric used for Bayesian model selection.  PyVBMC can be applied to any computational or statistical model with up to roughly 10-15 continuous parameters, with the only requirement that the user can provide a Python function that computes the target log likelihood of the model, or an approximation thereof (e.g., an estimate of the likelihood obtained via simulation
    
[^2]: 量子机器学习中的挑战与机遇

    Challenges and Opportunities in Quantum Machine Learning. (arXiv:2303.09491v1 [quant-ph])

    [http://arxiv.org/abs/2303.09491](http://arxiv.org/abs/2303.09491)

    量子机器学习（QML）是机器学习和量子计算交叉领域中的前沿研究方向，具有应用于量子材料、生物化学和高能物理等领域的潜力，但其模型的可训练性仍有挑战，需要进一步解决。本文回顾了当前QML方法与应用，并探讨了QML的量子优势机会。

    

    量子机器学习（QML）位于机器学习和量子计算的交叉领域，具有加速数据分析的潜力，特别是在量子数据应用于量子材料、生物化学和高能物理方面。然而，QML模型的可训练性仍然存在挑战。本文回顾了当前的QML方法与应用，并突出了量子机器学习与经典机器学习之间的差异，重点关注量子神经网络和量子深度学习。最后，我们讨论了QML的量子优势机会。

    At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.
    
[^3]: 广义秩正半定矩阵去噪的梯度流分析

    Gradient flow on extensive-rank positive semi-definite matrix denoising. (arXiv:2303.09474v1 [stat.ML])

    [http://arxiv.org/abs/2303.09474](http://arxiv.org/abs/2303.09474)

    本文提出了一种新的方法来分析广义秩和高维情况下正半定矩阵去噪问题的梯度流，揭示了其中的连续相变。

    

    本文提出了一种新的方法来分析广义秩和高维情况下正半定矩阵去噪问题的梯度流。我们使用最近的线性矩阵理论技术推导出固定点方程，跟踪问题的矩阵均方差的完整时间演化。所得到的固定点方程的预测结果通过数值实验得到验证。我们通过举例简要说明了我们的形式主义的几个预测，特别是我们揭示了广义秩和高维情况下的连续相变，这些相变在适当的极限下与低秩问题的经典相变有关。该形式主义有比本文更广泛的应用。

    In this work, we present a new approach to analyze the gradient flow for a positive semi-definite matrix denoising problem in an extensive-rank and high-dimensional regime. We use recent linear pencil techniques of random matrix theory to derive fixed point equations which track the complete time evolution of the matrix-mean-square-error of the problem. The predictions of the resulting fixed point equations are validated by numerical experiments. In this short note we briefly illustrate a few predictions of our formalism by way of examples, and in particular we uncover continuous phase transitions in the extensive-rank and high-dimensional regime, which connect to the classical phase transitions of the low-rank problem in the appropriate limit. The formalism has much wider applicability than shown in this communication.
    
[^4]: 结合类中心距离和异常值折扣的方法，提高在存在噪声标签的情况下训练机器学习模型的效果

    Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels. (arXiv:2303.09470v1 [cs.LG])

    [http://arxiv.org/abs/2303.09470](http://arxiv.org/abs/2303.09470)

    本文提出了结合类中心距离和异常值折扣的方法，用于解决在存在噪声标签的情况下训练机器学习模型的问题，并通过实验证明了其有效性 。

    

    本文提出了一种新的方法，用于解决在存在噪声标签的情况下训练机器学习模型的挑战。通过在物品的潜在空间中巧妙地使用距离类中心的方法，再结合折扣策略以减少距离所有类中心（即异常值）远的样本的重要性，我们的方法有效解决了噪声标签的问题。我们的方法是基于这样的想法：在训练的早期阶段，距离各自类中心更远的样本更可能是噪声。通过在几个流行的基准数据集上进行广泛实验，我们证明了我们的方法的有效性。结果表明，我们的方法在存在噪声标签的情况下，可以明显提高分类准确性，表现优于当前领域的最优方法。

    In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels.
    
[^5]: 固定预算赌博机标识中的复杂度存在问题

    On the Existence of a Complexity in Fixed Budget Bandit Identification. (arXiv:2303.09468v1 [stat.ML])

    [http://arxiv.org/abs/2303.09468](http://arxiv.org/abs/2303.09468)

    该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。

    

    在固定预算赌博机标识中，算法按顺序观察来自多个分布的样本，直到给定最终时间。然后，它回答关于分布集的查询。一个好的算法将有小的错误概率。虽然这个概率随着最终时间的增加呈指数级下降，但对于大多数标识任务，最佳可达率并非精确已知。我们展示了如果固定预算任务接受复杂度（定义为单个算法在所有赌博问题中实现的错误概率的下限），则该复杂度由该问题的最佳非自适应抽样过程确定。我们证明了对于几个固定预算识别任务，包括具有两个臂的伯努利最佳臂识别，不存在这样的复杂度：没有单个算法能够随处实现最佳可能速率。

    In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by a single algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate.
    
[^6]: 使用成本感知的模糊集的分布鲁棒优化

    Distributionally Robust Optimization using Cost-Aware Ambiguity Sets. (arXiv:2303.09408v1 [math.OC])

    [http://arxiv.org/abs/2303.09408](http://arxiv.org/abs/2303.09408)

    本文提出了一种新的用于分布鲁棒优化的模糊集，称为成本感知模糊集，它通过半空间定义，只排除那些预计对所获得的最坏情况成本产生重大影响的分布，实现了高置信度上界和一致估计。

    

    我们提出了一种新的用于分布鲁棒优化（DRO）的模糊集类别，称为成本感知的模糊集，其定义为取决于在独立估计的最优解处评估成本函数的半空间，因此仅排除那些预计对所获得的最坏情况成本产生重大影响的分布。我们表明，由此产生的DRO方法提供了高置信度的上界和样本外预期成本的一致估计，并且经验证明，它与基于散度的模糊集相比，可以产生更少保守的解。

    We present a novel class of ambiguity sets for distributionally robust optimization (DRO). These ambiguity sets, called cost-aware ambiguity sets, are defined as halfspaces which depend on the cost function evaluated at an independent estimate of the optimal solution, thus excluding only those distributions that are expected to have significant impact on the obtained worst-case cost. We show that the resulting DRO method provides both a high-confidence upper bound and a consistent estimator of the out-of-sample expected cost, and demonstrate empirically that it results in less conservative solutions compared to divergence-based ambiguity sets.
    
[^7]: 关于线性情境赌博机中错误规定与次优间隙的相互作用研究

    On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits. (arXiv:2303.09390v1 [cs.LG])

    [http://arxiv.org/abs/2303.09390](http://arxiv.org/abs/2303.09390)

    本文研究了线性情境赌博机在错误规定的情境下的算法问题，提出一种新算法，将在一定水平内误差和最小次优间隙相互制约，以常数误差上限实现间隙相关的度量。

    

    本文研究了线性情境赌博机在错误规定的情境下，期望奖励函数可以以线性函数类来逼近的情况。我们提出了一种基于新的数据选择方案的算法，该算法仅选择具有大不确定性的情境向量进行在线回归。当误差规定水平被$\zeta>0$控制时，我们的算法的误差上限与好的指定情况下的结果相同。我们证明了一个现有的算法也可以在不知道亚优间隙$\Delta$的情况下实现间隙相关的常数误差上限。在Lattimore et al.（2020）的作品基础上，我们提供了一个下界，表明了错误规定和次优间隙之间的相互作用。

    We study linear contextual bandits in the misspecified setting, where the expected reward function can be approximated by a linear function class up to a bounded misspecification level $\zeta>0$. We propose an algorithm based on a novel data selection scheme, which only selects the contextual vectors with large uncertainty for online regression. We show that, when the misspecification level $\zeta$ is dominated by $\tilde O (\Delta / \sqrt{d})$ with $\Delta$ being the minimal sub-optimality gap and $d$ being the dimension of the contextual vectors, our algorithm enjoys the same gap-dependent regret bound $\tilde O (d^2/\Delta)$ as in the well-specified setting up to logarithmic factors. In addition, we show that an existing algorithm SupLinUCB (Chu et al., 2011) can also achieve a gap-dependent constant regret bound without the knowledge of sub-optimality gap $\Delta$. Together with a lower bound adapted from Lattimore et al. (2020), our result suggests an interplay between misspecific
    
[^8]: 利用特权信息进行无监督领域自适应

    Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])

    [http://arxiv.org/abs/2303.09350](http://arxiv.org/abs/2303.09350)

    本文提出利用特权信息进行领域适应（DALUPI）算法，以在学习中放宽假设条件并提高样本效率，通过减少错误来促进医学图像分析等应用的发展。

    

    成功的无监督领域自适应（UDA）只在强假设条件下得以实现，如协变量移位和输入领域之间的重叠。后者在高维应用中经常被违反，比如图像分类，在面对这种挑战时，图像分类仍然是算法开发的灵感和基准。本文表明，获取源域和目标域样本的有关信息能够帮助放宽这些假设，并在学习中提高样本效率，代价是收集更丰富的变量集。我们称之为利用特权信息进行领域适应（DALUPI）。为此，我们提出了一个简单的两阶段学习算法，并提出了一个针对多标签图像分类的实用端到端算法，受到我们分析的启发。通过一系列实验，包括医学图像分析的应用，我们证明了在学习过程中加入特权信息可以减少错误。

    Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
    
[^9]: 正交方向约束梯度法：从非线性等式约束到Stiefel流形

    Orthogonal Directions Constrained Gradient Method: from non-linear equality constraints to Stiefel manifold. (arXiv:2303.09261v1 [math.OC])

    [http://arxiv.org/abs/2303.09261](http://arxiv.org/abs/2303.09261)

    提出了一种新的算法——ODCGM，可以优化在光滑流形上最小化非凸函数的问题。算法的实现要简单得多，并且在确定性和随机情况下具有近似最优的误差复杂度。此外，我们还证明，在适当选择投影度量的情况下，方法可以恢复出Ablin 和 Peyr\'e (2022)的Landing算法，后者是一种最近用于优化Stiefel流形的算法。

    

    本文考虑在光滑流形 $\mathcal{M}$ 上最小化一个非凸函数的问题。我们提出了一种新的算法——正交方向约束梯度法(ODCGM)，该算法只需要计算向量空间上的投影。ODCGM虽然不可行，但迭代过程始终被拉向流形，确保了ODCGM收敛到 $\mathcal{M}$。相比需要计算收缩算子的传统方法，ODCGM的实现要简单得多。此外，我们证明了ODCGM在确定性和随机情况下具有近似最优的误差复杂度，分别为 $\mathcal{O}(1/\varepsilon^2)$ 和 $\mathcal{O}(1/\varepsilon^4)$。此外，我们还证明，在适当选择投影度量的情况下，我们的方法可以恢复出Ablin 和 Peyr\'e (2022)的Landing算法，后者是一种最近用于优化Stiefel流形的算法。因此，我们显著扩展了Ablin 和 Peyr\'e的分析。

    We consider the problem of minimizing a non-convex function over a smooth manifold $\mathcal{M}$. We propose a novel algorithm, the Orthogonal Directions Constrained Gradient Method (ODCGM) which only requires computing a projection onto a vector space. ODCGM is infeasible but the iterates are constantly pulled towards the manifold, ensuring the convergence of ODCGM towards $\mathcal{M}$. ODCGM is much simpler to implement than the classical methods which require the computation of a retraction. Moreover, we show that ODCGM exhibits the near-optimal oracle complexities $\mathcal{O}(1/\varepsilon^2)$ and $\mathcal{O}(1/\varepsilon^4)$ in the deterministic and stochastic cases, respectively. Furthermore, we establish that, under an appropriate choice of the projection metric, our method recovers the landing algorithm of Ablin and Peyr\'e (2022), a recently introduced algorithm for optimization over the Stiefel manifold. As a result, we significantly extend the analysis of Ablin and Peyr\
    
[^10]: 带有概念瓶颈结构和多任务组成的线性神经网络的贝叶斯泛化误差。

    Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation. (arXiv:2303.09154v1 [stat.ML])

    [http://arxiv.org/abs/2303.09154](http://arxiv.org/abs/2303.09154)

    本文数学上澄清了带有概念瓶颈结构和多任务组成的线性神经网络的贝叶斯泛化误差和自由能。

    

    概念瓶颈模型（CBM）是一种广泛使用的方法，可以使用概念解释神经网络。在CBM中，概念被插入到输出层和最后一个中间层之间作为可观察值。这有助于理解神经网络生成输出的原因：最后一个隐藏层到输出层的概念对应的权重。然而，在CBM中理解泛化误差行为尚不可能，因为神经网络通常是奇异的统计模型。当模型是奇异的时，从参数到概率分布的一一映射不能创建。这种不可识别性使得分析泛化性能变得困难。在本次研究中，我们数学上澄清了CBM的贝叶斯泛化误差和自由能，当其架构是三层的线性神经网络时。我们还考虑了一个多任务问题，在该问题中，神经网络的输出不再只是一个标签，而是一组任务。

    Concept bottleneck model (CBM) is a ubiquitous method that can interpret neural networks using concepts. In CBM, concepts are inserted between the output layer and the last intermediate layer as observable values. This helps in understanding the reason behind the outputs generated by the neural networks: the weights corresponding to the concepts from the last hidden layer to the output layer. However, it has not yet been possible to understand the behavior of the generalization error in CBM since a neural network is a singular statistical model in general. When the model is singular, a one to one map from the parameters to probability distributions cannot be created. This non-identifiability makes it difficult to analyze the generalization performance. In this study, we mathematically clarify the Bayesian generalization error and free energy of CBM when its architecture is three-layered linear neural networks. We also consider a multitask problem where the neural network outputs not on
    
[^11]: 高维度惩罚伯恩斯坦支持向量机

    High-Dimensional Penalized Bernstein Support Vector Machines. (arXiv:2303.09066v1 [stat.ML])

    [http://arxiv.org/abs/2303.09066](http://arxiv.org/abs/2303.09066)

    提出一种适用于高维度情况下的平滑支持向量机铰链损失函数，即Bernstein支持向量机（BernSVM），并提出两种有效算法求解该方法，实验结果表明该方法在现有竞争对手中具有优越性。

    

    支持向量机(SVM)是一种用于二分类的强大分类器，以提高预测精度。然而，在高维设置中，SVM铰链损失函数的不可微性可能导致计算困难。为了克服这个问题，我们依赖伯恩斯坦多项式，提出了一种新的平滑的SVM铰链损失函数版本，称为Bernstein支持向量机（BernSVM），适用于高维$p>> n$情况。由于BernSVM目标损失函数属于$C^2$类，因此我们提出了两种计算惩罚BernSVM解的有效算法。第一个算法基于最大化-主导（MM）原理的坐标下降法，第二个算法是IRLS类型算法（迭代重新加权最小二乘法）。在标准假设下，我们推导出一个锥条件和一个限制性强凸性，以建立加权Lasso BernSVM估计器的上界。使用局部线性逼近，我们提出了两个模型选择标准，用于调整BernSVM超参数。进行了广泛的数值实验，以证明所提出的方法在现有竞争对手中具有优越性。

    The support vector machines (SVM) is a powerful classifier used for binary classification to improve the prediction accuracy. However, the non-differentiability of the SVM hinge loss function can lead to computational difficulties in high dimensional settings. To overcome this problem, we rely on Bernstein polynomial and propose a new smoothed version of the SVM hinge loss called the Bernstein support vector machine (BernSVM), which is suitable for the high dimension $p >> n$ regime. As the BernSVM objective loss function is of the class $C^2$, we propose two efficient algorithms for computing the solution of the penalized BernSVM. The first algorithm is based on coordinate descent with maximization-majorization (MM) principle and the second one is IRLS-type algorithm (iterative re-weighted least squares). Under standard assumptions, we derive a cone condition and a restricted strong convexity to establish an upper bound for the weighted Lasso BernSVM estimator. Using a local linear ap
    
[^12]: 用 Kernel 方法学习具有能隙的量子哈密顿量的基态

    Learning ground states of gapped quantum Hamiltonians with Kernel Methods. (arXiv:2303.08902v1 [quant-ph])

    [http://arxiv.org/abs/2303.08902](http://arxiv.org/abs/2303.08902)

    本文提出了一种利用 kernel 方法学习具有能隙的量子哈密顿量基态的统计学习方法，理论上需要多项式资源实现，通过数值模拟证明了该方法的有效性，并展示了方法的灵活性。

    

    近年来，利用神经网络来近似量子哈密顿量基态的方法需要解决高度非线性的优化问题。本文提出了一种利用 kernel 方法来使优化变得简单的统计学习方法。我们的方案是功率法的一种近似实现，其中通过监督学习来学习功率迭代的下一步。我们证明，假设监督学习是有效的，那么可以使用多项式资源实现对任意具有能隙的量子哈密顿量的基态性质的计算。我们使用 kernel ridge 回归，通过对一维和二维的几个典型相互作用多体量子系统进行基态的寻找，提供了基于数值模拟的证据，证明了学习假设的有效性，展示了我们方法的灵活性。

    Neural network approaches to approximate the ground state of quantum hamiltonians require the numerical solution of a highly nonlinear optimization problem. We introduce a statistical learning approach that makes the optimization trivial by using kernel methods. Our scheme is an approximate realization of the power method, where supervised learning is used to learn the next step of the power iteration. We show that the ground state properties of arbitrary gapped quantum hamiltonians can be reached with polynomial resources under the assumption that the supervised learning is efficient. Using kernel ridge regression, we provide numerical evidence that the learning assumption is verified by applying our scheme to find the ground states of several prototypical interacting many-body quantum systems, both in one and two dimensions, showing the flexibility of our approach.
    
[^13]: 基于贝叶斯积分的神经网络集成搜索

    Bayesian Quadrature for Neural Ensemble Search. (arXiv:2303.08874v1 [stat.ML])

    [http://arxiv.org/abs/2303.08874](http://arxiv.org/abs/2303.08874)

    本论文介绍了一种使用贝叶斯积分的新方法，可以在架构似然表面有分散、狭窄峰时构建加权集成神经网络，相比当前同类方法，在测试似然性、准确性和期望校准误差方面更为优秀。

    

    集成方法可以提高神经网络的性能，但现有方法在架构似然表面有分散、狭窄峰时效果不佳。此外，现有方法构建均等加权的集成，这可能容易受到较弱架构的失效模式的影响。通过将集成视为近似边缘化架构，我们使用贝叶斯积分的工具构建集成方法——这些工具非常适合探索架构似然表面有分散、狭窄峰的情况。此外，由此产生的集成由体现其性能的架构加权权重组成。我们通过实证研究——在测试似然性、准确性和期望校准误差方面——表明我们的方法优于现有的基线，并通过削减研究验证其各成分的独立性能。

    Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.
    
[^14]: 环面坐标：格点约化实现循环坐标解耦

    Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction. (arXiv:2212.07201v2 [cs.CG] UPDATED)

    [http://arxiv.org/abs/2212.07201](http://arxiv.org/abs/2212.07201)

    本文研究了环面坐标算法的问题，提出了圆值图形的几何相关性概念，并描述了一个系统性的过程，用于构建最小能量的环面值图。

    

    这篇论文研究了应用环面坐标算法的问题，研究发现当应用于多个上同调类时，即使所选定的上同调类是线性无关的，输出的环面值图也可能会被几何相关。研究表明，通过适当的上同调类的整数线性组合可以获得较不相关的映射。本文提出了圆值图形的几何相关性概念，该概念在黎曼流形情况下对应于狄利克雷形式，这是从狄利克雷能量导出的双线性形式。作者描述了一个系统性的过程，用于构建最小能量的环面值图。

    The circular coordinates algorithm of de Silva, Morozov, and Vejdemo-Johansson takes as input a dataset together with a cohomology class representing a $1$-dimensional hole in the data; the output is a map from the data into the circle that captures this hole, and that is of minimum energy in a suitable sense. However, when applied to several cohomology classes, the output circle-valued maps can be "geometrically correlated" even if the chosen cohomology classes are linearly independent. It is shown in the original work that less correlated maps can be obtained with suitable integer linear combinations of the cohomology classes, with the linear combinations being chosen by inspection. In this paper, we identify a formal notion of geometric correlation between circle-valued maps which, in the Riemannian manifold case, corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet energy. We describe a systematic procedure for constructing low energy torus-valued maps on d
    
[^15]: 神经网络通过随机梯度下降有效地学习低维表示

    Neural Networks Efficiently Learn Low-Dimensional Representations with SGD. (arXiv:2209.14863v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14863](http://arxiv.org/abs/2209.14863)

    本文研究使用随机梯度下降训练任意宽度的两层神经网络的问题，当输入为高斯分布，目标为多指数模型时，NN的第一层权重会收敛到真实模型中$k$维主子空间, 可以通过在子空间上使用均匀收敛建立广义误差边界为$O(\sqrt{{kd}/{T}})$, SGD训练的ReLU NN可以学习形如$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$的单指数目标。

    

    本文研究了使用随机梯度下降训练任意宽度的两层神经网络(NN)的问题，其中输入$ \boldsymbol{x} \in \mathbb {R}^d $为高斯分布，目标$ y \in \mathbb {R}$遵循多指数模型，即 $ y = g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ ，其中函数$g$为有噪声的连接函数，我们证明当使用带有权重衰减的在线SGD进行训练时，NN的第一层权重会收敛到真实模型中$ \boldsymbol{u_1},...,\boldsymbol{u_k}$的$k$维主子空间，当$k \ll d$ 时，该现象有几个重要的影响。首先，通过在这个更小的子空间上使用均匀收敛，我们建立了在SGD进行$T$次迭代后的广义误差边界为$ O(\sqrt{{kd}/{T}})$，这不依赖于NN的宽度。我们进一步证明，SGD训练的ReLU NN可以学习形如$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$的单指数目标.

    We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\boldsymbol{x}\in \mathbb{R}^d$ is Gaussian and the target $y \in \mathbb{R}$ follows a multiple-index model, i.e., $y=g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\boldsymbol{u_1},...,\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $O(\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f
    
[^16]: 使用基于模型的树和提升方法拟合低阶函数ANOVA模型

    Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.06950](http://arxiv.org/abs/2207.06950)

    本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。

    

    低阶函数ANOVA模型已经被机器学习社区重新发现，并称之为内在可解释的机器学习。本文提出了一种新算法GAMI-Tree，类似于EBM，但具有一些趋向更好性能的特性。我们采用模型为基础的树，并融入一种新的交互过滤方法，提高了对底层交互的捕捉。此外，我们的迭代训练方法收敛于具有更好预测性能的模型，并确保相互作用在分层意义上正交于主效应。该算法不需要广泛的调整，并且实现快速高效。我们使用模拟和真实数据集进行比较。

    Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
    
[^17]: FibeRed: 通过向量丛描述拓扑复杂数据的纤维降维方法

    FibeRed: Fiberwise Dimensionality Reduction of Topologically Complex Data with Vector Bundles. (arXiv:2206.06513v2 [cs.CG] UPDATED)

    [http://arxiv.org/abs/2206.06513](http://arxiv.org/abs/2206.06513)

    FibeRed提出了一种通过向量丛描述拓扑复杂数据的纤维降维方法，利用该方法可以降低数据维度，同时保留其大规模拓扑特征。算法包含从局部线性降维得到的局部表示与初始全局表示相结合的过程，并在动力学系统和化学领域的数据集上表现出良好的效果。

    

    具有非平凡大规模拓扑结构的数据集可能很难用现有的降维算法嵌入到低维欧几里得空间中。我们提出使用向量丛来模拟具有拓扑复杂性的数据集，这样基空间可以考虑到大规模拓扑，而纤维可以考虑到局部几何。这允许我们降低纤维的维度，同时保留大规模拓扑结构。我们形式化了这个观点，并作为一个应用，我们提出了一个算法，该算法将数据集及其在欧几里得空间中的初始表示作为输入，假设它能恢复部分大规模拓扑结构，并输出一个新表示，该表示将局部线性降维得到的局部表示与初始全局表示相结合。我们在动力学系统和化学领域的例子中证明了这个算法。在这些例子中，我们的算法能够学习拓扑结构和几何结构之间的联系。

    Datasets with non-trivial large scale topology can be hard to embed in low-dimensional Euclidean space with existing dimensionality reduction algorithms. We propose to model topologically complex datasets using vector bundles, in such a way that the base space accounts for the large scale topology, while the fibers account for the local geometry. This allows one to reduce the dimensionality of the fibers, while preserving the large scale topology. We formalize this point of view, and, as an application, we describe an algorithm which takes as input a dataset together with an initial representation of it in Euclidean space, assumed to recover part of its large scale topology, and outputs a new representation that integrates local representations, obtained through local linear dimensionality reduction, along the initial global representation. We demonstrate this algorithm on examples coming from dynamical systems and chemistry. In these examples, our algorithm is able to learn topologica
    
[^18]: SRMD：稀疏随机模式分解

    SRMD: Sparse Random Mode Decomposition. (arXiv:2204.06108v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2204.06108](http://arxiv.org/abs/2204.06108)

    该论文提出了一种通过构建纤细近似于谱图的稀疏来分析时间序列数据的方法，这种方法在信号分解和多尺度信号分析方面是先进的。

    

    信号分解和多尺度信号分析提供了许多用于时频分析的有用工具。我们提出了一种随机特征方法，通过构建纤细近似于谱图的稀疏来分析时间序列数据。随机化同时发生在时间窗口位置和频率采样上，这降低了总体采样和计算成本。谱图的稀疏化导致时频聚类之间的尖锐分离，使识别固有模式更容易，因此引导了新的数据驱动模式分解。应用包括信号表征，异常值去除和模式分解。在基准测试中，我们展示了我们的方法优于其他最先进的分解方法。

    Signal decomposition and multiscale signal analysis provide many useful tools for time-frequency analysis. We proposed a random feature method for analyzing time-series data by constructing a sparse approximation to the spectrogram. The randomization is both in the time window locations and the frequency sampling, which lowers the overall sampling and computational cost. The sparsification of the spectrogram leads to a sharp separation between time-frequency clusters which makes it easier to identify intrinsic modes, and thus leads to a new data-driven mode decomposition. The applications include signal representation, outlier removal, and mode decomposition. On the benchmark tests, we show that our approach outperforms other state-of-the-art decomposition methods.
    
[^19]: 噪声低秩矩阵优化: 局部极小值的几何形状和收敛速度

    Noisy Low-rank Matrix Optimization: Geometry of Local Minima and Convergence Rate. (arXiv:2203.03899v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2203.03899](http://arxiv.org/abs/2203.03899)

    本文提出了一个新的数学框架来处理噪声低秩矩阵优化问题，对受限等距常数的限制要少得多，并且只要无噪声目标的受限等距条件小于1/3，任何错误的局部优化解必须接近于真实解，可以在多项式时间内找到近似解。

    

    本文关注于低秩矩阵优化问题，该问题在机器学习中应用广泛。在矩阵感知特例中，该问题已经通过受限等距性质的概念进行了广泛研究，导致了大量关于问题的几何景观和常见算法的收敛速度的结果。然而，现有结果仅在受限等距常数接近于0的情况下能够处理仅具有噪声数据的一般目标函数问题。在本文中，我们开发了一个新的数学框架来解决以上问题，该框架对受限等距常数的限制要少得多。我们证明，只要无噪声目标的受限等距条件小于1/3，任何错误的局部优化解必须接近于真实解。通过严格的鞍点特性，我们还证明在多项式时间内可以找到近似解。

    This paper is concerned with low-rank matrix optimization, which has found a wide range of applications in machine learning. This problem in the special case of matrix sensing has been studied extensively through the notion of Restricted Isometry Property (RIP), leading to a wealth of results on the geometric landscape of the problem and the convergence rate of common algorithms. However, the existing results can handle the problem in the case with a general objective function subject to noisy data only when the RIP constant is close to 0. In this paper, we develop a new mathematical framework to solve the above-mentioned problem with a far less restrictive RIP constant. We prove that as long as the RIP constant of the noiseless objective is less than $1/3$, any spurious local solution of the noisy optimization problem must be close to the ground truth solution. By working through the strict saddle property, we also show that an approximate solution can be found in polynomial time. We 
    
[^20]: 基于深度学习的大规模验证项目因素分析的参数估计和拟合度检验方法研究

    Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis. (arXiv:2109.09500v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.09500](http://arxiv.org/abs/2109.09500)

    本文研究了针对大规模验证项目因素分析的参数估计与拟合度检验方法，提出了基于深度学习的算法和扩展测试与指标，具有高效准确和有效性的特点。

    

    本论文研究了针对大规模验证项目因素分析中的参数估计和拟合度检验方法。对于参数估计，我们将Urban和Bauer（2021）的深度学习算法扩展到验证性因素分析领域，并展示了如何处理因子载荷和因子相关性的限制。对于拟合度检验，我们探索基于模拟的测试和指标，扩展了分类器两个样本测试（C2ST），该方法测试深度神经网络能否区分来自拟合的IFA模型的观测数据和合成数据。所提出的扩展包括近似拟合检验，其中用户指定观测数据和合成数据中应有多少占可区分部分的百分比，以及相对拟合指数（RFI），该指数类似于结构方程建模中使用的RFI。通过模拟研究，我们展示了：（1）Urban和Bauer的深度学习算法的验证性扩展即使存在高相关因子也可以准确地估计模型参数；（2）所提出的拟合度指标可以有效地检测模型不良拟合的重要信息，对于大规模验证IFA提供了有效的解决方案。

    We investigate novel parameter estimation and goodness-of-fit (GOF) assessment methods for large-scale confirmatory item factor analysis (IFA) with many respondents, items, and latent factors. For parameter estimation, we extend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to the confirmatory setting by showing how to handle constraints on loadings and factor correlations. For GOF assessment, we explore simulation-based tests and indices that extend the classifier two-sample test (C2ST), a method that tests whether a deep neural network can distinguish between observed data and synthetic data sampled from a fitted IFA model. Proposed extensions include a test of approximate fit wherein the user specifies what percentage of observed and synthetic data should be distinguishable as well as a relative fit index (RFI) that is similar in spirit to the RFIs used in structural equation modeling. Via simulation studies, we show that: (1) the confirmatory extension of Urb
    
[^21]: 一种用于具有秩缺失雅可比矩阵的非线性等式约束优化的随机顺序二次优化算法

    A Stochastic Sequential Quadratic Optimization Algorithm for Nonlinear Equality Constrained Optimization with Rank-Deficient Jacobians. (arXiv:2106.13015v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2106.13015](http://arxiv.org/abs/2106.13015)

    本研究提出了一种针对非线性等式约束优化的顺序二次优化算法，具备使用随机梯度和处理秩缺失雅可比矩阵的能力，实验结果表明其性能优于其他方法。

    

    本文提出了一种针对定义为随机函数期望的平滑非线性等式约束优化问题的顺序二次优化算法。该算法的结构基于一种在文献上被广泛证明在实践中非常有效的步骤分解策略，其中每个搜索方向被计算为正常步骤（朝向线性化可行性）和切向步骤（朝向约束雅可比矩阵零空间内目标下降）的总和。然而，所提出的方法在文献中是独一无二的，因为它既允许使用随机目标梯度估计，又在约束雅可比矩阵可能秩缺失的情况下具有收敛保证。 数值实验的结果表明，与流行的替代方案相比，该算法具有更强的性能。

    A sequential quadratic optimization algorithm is proposed for solving smooth nonlinear equality constrained optimization problems in which the objective function is defined by an expectation of a stochastic function. The algorithmic structure of the proposed method is based on a step decomposition strategy that is known in the literature to be widely effective in practice, wherein each search direction is computed as the sum of a normal step (toward linearized feasibility) and a tangential step (toward objective decrease in the null space of the constraint Jacobian). However, the proposed method is unique from others in the literature in that it both allows the use of stochastic objective gradient estimates and possesses convergence guarantees even in the setting in which the constraint Jacobians may be rank deficient. The results of numerical experiments demonstrate that the algorithm offers superior performance when compared to popular alternatives.
    
[^22]: 关于 ReLU 神经网络深度下界的探究

    Towards Lower Bounds on the Depth of ReLU Neural Networks. (arXiv:2105.14835v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.14835](http://arxiv.org/abs/2105.14835)

    该研究运用数学和优化理论方法，就 ReLU 神经网络的深度下界做了探究，有助于更好地理解这种网络所能表示的函数类的性质。此外，该研究还肯定了一项旧的分段线性函数猜想。

    

    我们运用混合整数优化、多面体理论和热带几何学等技术，为理解具有 ReLU 激活和给定结构的神经网络所能表示的函数类做出了更好的贡献。尽管普适逼近定理认为单层隐藏层就足以学习任何函数，但我们提供了一个数学的对称性，并详细探讨了添加更多层（无大小限制）时是否严格增加了可表示函数的类。作为研究副产品，我们肯定了 Wang 和 Sun（2005）有关分段线性函数的一个旧猜想。我们还给出了表示具有对数深度函数所需的神经网络大小上界。

    We contribute to a better understanding of the class of functions that can be represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning any function. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). As a by-product of our investigations, we settle an old conjecture about piecewise linear functions by Wang and Sun (2005) in the affirmative. We also present upper bounds on the sizes of neural networks required to represent functions with logarithmic depth.
    
[^23]: 噪声低秩矩阵恢复的几何分析在精确参数化和过度参数化区间中的应用

    Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes. (arXiv:2105.08232v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.08232](http://arxiv.org/abs/2105.08232)

    本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。

    

    矩阵感知问题是一种重要的低秩优化问题，在矩阵补全、相位同步/恢复、稳健PCA和电力系统状态估计等领域都有广泛应用。本文研究了通过线性测量损坏的噪声低秩矩阵感知问题。我们考虑了搜索秩r等于未知真实秩r*的情况（精确参数化情况），以及r大于r*的情况（过度参数化情况）。我们量化了约束等异性性质（restricted isometry property，RIP）在塑造非凸分解公式的整体景观和帮助局部搜索算法成功方面的作用。首先，我们在RIP常数小于 1/(1+sqrt(r*/r))的假设下，对非凸问题的任意局部极小值和真实值之间的最大距离进行了全局保证。然后，我们提出了一种新颖的方法，称为鲁棒错误定位几何分析（Robust Error-Locating Geometric Analysis，RELGA）算法，用于实现在存在噪声的情况下的精确低秩矩阵恢复。RELGA算法通过组合错误定位机制和几何分析，提供了理论保证，即使在噪声水平相对较大的情况下，也可以实现精确的矩阵恢复。对于过度参数化情况，我们提出了一种局部搜索算法，称为连续子空间优化（Successive Subspace Optimization，SSO）算法，在噪声水平和RIP常数的一定条件下，可以收敛到真实解。我们的分析揭示了SSO的成功取决于初始化、非退化性和几何条件的组合。

    The matrix sensing problem is an important low-rank optimization problem that has found a wide range of applications, such as matrix completion, phase synchornization/retrieval, robust PCA, and power system state estimation. In this work, we focus on the general matrix sensing problem with linear measurements that are corrupted by random noise. We investigate the scenario where the search rank $r$ is equal to the true rank $r^*$ of the unknown ground truth (the exact parametrized case), as well as the scenario where $r$ is greater than $r^*$ (the overparametrized case). We quantify the role of the restricted isometry property (RIP) in shaping the landscape of the non-convex factorized formulation and assisting with the success of local search algorithms. First, we develop a global guarantee on the maximum distance between an arbitrary local minimizer of the non-convex problem and the ground truth under the assumption that the RIP constant is smaller than $1/(1+\sqrt{r^*/r})$. We then p
    
[^24]: 《Bayesian停时问题的逆强化学习的充分必要条件》

    Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.03481](http://arxiv.org/abs/2007.03481)

    本文提出了一个Bayesian停时问题的逆强化学习框架，结合微观经济学中的Bayesian揭示偏好思路，通过观察Bayesian决策者的行动，确定其的最优性。并且通过两个停时问题示例得到了验证，并且已在一个真实的例子中得到了高精度地预测用户参与度。

    

    本文提出了一个Bayesian停时问题的逆强化学习（IRL）框架。通过观察Bayesian决策者的行动，我们提供了一种必要且充分的条件来确定这些行动是否与优化成本函数一致。在Bayesian（部分观察）情况下，逆向学习者能够最好地确定针对观察到的策略的最优性。我们的IRL算法确定最优性，然后构建成本函数的估计值，是一个集合值。为了实现这样一个IRL目标，我们使用了来自微观经济学的Bayesian揭示偏好的新思路。我们通过两个重要的停时问题示例，即，顺序假设检验和Bayesian搜索，说明了所提议的IRL方案。作为一个真实世界的例子，我们使用来自190000个视频的元数据的YouTube数据集说明了所提议的IRL方法如何高精度地预测在线多媒体平台中用户的参与度。最后，对于该算法的未来研究方向做了最后讨论。

    This paper presents an inverse reinforcement learning~(IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function. In a Bayesian (partially observed) setting, the inverse learner can at best identify optimality wrt the observed strategies. Our IRL algorithm identifies optimality and then constructs set-valued estimates of the cost function.To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. We illustrate the proposed IRL scheme using two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. As a real-world example, we illustrate using a YouTube dataset comprising metadata from 190000 videos how the proposed IRL method predicts user engagement in online multimedia platforms with high accuracy. Finally, for
    
[^25]: 用于在线学习非平稳函数的连续高斯过程

    Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1905.10003](http://arxiv.org/abs/1905.10003)

    本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。

    

    许多机器学习问题可以在估计函数的上下文中得到解决，通常这些函数是时间相关的函数，并且是实时地随着观测的到来而估计的。高斯过程是建模实值非线性函数的一个有吸引力的选择，由于其灵活性和不确定性量化。然而，典型的高斯过程回归模型存在若干不足：1）传统高斯过程推断的复杂度$O(N^{3})$随着观测值的个数N成增长；2）逐步更新高斯过程模型不容易；3）协方差核通常对函数施加平稳性约束，而具有非平稳协方差核的高斯过程通常难以在实践中使用。为了克服这些问题，我们提出了一个顺序蒙特卡罗算法来拟合无限混合高斯过程，以捕捉非平稳行为，同时允许在线、分布推断。我们的方法在实验中优于现有最先进方法的性能。

    Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo
    

