# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach](https://rss.arxiv.org/abs/2402.01454) | 本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。 |
| [^2] | [Variational Linearized Laplace Approximation for Bayesian Deep Learning](https://rss.arxiv.org/abs/2302.12565) | 本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。 |
| [^3] | [Conformal prediction for multi-dimensional time series by ellipsoidal sets](https://arxiv.org/abs/2403.03850) | 开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于在多元时间序列中构建预测区域，具有更小的预测区域和有效的覆盖。 |
| [^4] | [On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models](https://arxiv.org/abs/2403.02957) | 本论文通过严格证明一个特定的DPM去噪策略在大量扩散步数下收敛到均方误差最优条件均值估计器，突出了DPM由渐近最优的去噪器组成，同时具有强大生成器的独特视角。 |
| [^5] | [Prediction-Powered Ranking of Large Language Models](https://arxiv.org/abs/2402.17826) | 该研究提出了一种统计框架，可以衡量人类与模型偏好之间的不确定性，从而进行大型语言模型的预测排名。 |
| [^6] | [Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers](https://arxiv.org/abs/2402.13380) | 这项研究利用变压器模型解决混合整数规划问题，首次采用变压器预测二进制变量，提出的算法在解决时间上超越了传统CPLEX和LSTM。 |
| [^7] | [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875) | 思维链赋予变压器模型执行固有串行计算的能力，提高了变压器在算术和符号推理任务中的准确性。 |
| [^8] | [Collaborative Learning with Different Labeling Functions](https://arxiv.org/abs/2402.10445) | 研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。 |
| [^9] | [Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems](https://arxiv.org/abs/2402.08193) | 高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。 |
| [^10] | [Hypergraph Node Classification With Graph Neural Networks](https://arxiv.org/abs/2402.05569) | 本研究提出了一种简单高效的框架，利用加权子图扩展的图神经网络(WCE-GNN)实现了超图节点分类。实验证明，WCE-GNN具有优秀的预测效果和较低的计算复杂度。 |
| [^11] | [Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects](https://arxiv.org/abs/2402.04906) | 本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。 |
| [^12] | [A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets](https://arxiv.org/abs/2402.03985) | 本研究通过对多个合成数据集进行偏差-方差分解，增加了对其理论理解。实验证明多个合成数据集对于高方差的下游预测器特别有益，并提供了一个简单的经验法则用于选择适当的合成数据集数量。 |
| [^13] | [Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation](https://arxiv.org/abs/2402.03687) | PARD是一种将扩散模型与自回归方法相结合的置换不变性自回归扩散模型，通过使用图中的部分顺序以块逐块的自回归方式生成图。 |
| [^14] | [Effective Acquisition Functions for Active Correlation Clustering](https://arxiv.org/abs/2402.03587) | 本文提出了三种有效的获取函数用于主动相关聚类，分别基于不一致性概念和信息论量。 |
| [^15] | [Enhancing Compositional Generalization via Compositional Feature Alignment](https://arxiv.org/abs/2402.02851) | 通过组合特征对齐，增强了模型的组合通用性，使其能够推广到未见过的领域-类别组合。 |
| [^16] | [Causal Bayesian Optimization via Exogenous Distribution Learning](https://arxiv.org/abs/2402.02277) | 本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。 |
| [^17] | [Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection](https://arxiv.org/abs/2402.02239) | 本文提出了一种新的分布约简方法，利用格罗莫夫-瓦瑟斯坦投影统一了降维和聚类，通过优化问题同时解决降维和聚类，实验证明了该方法在多个领域表现出卓越性能。 |
| [^18] | [Sample, estimate, aggregate: A recipe for causal discovery foundation models](https://arxiv.org/abs/2402.01929) | 本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。 |
| [^19] | [BootsTAP: Bootstrapped Training for Tracking-Any-Point](https://arxiv.org/abs/2402.00847) | 该论文展示了如何通过使用大规模、无标签、未筛选的真实世界数据，以及采用自监督的师生设置，来改进Tracking-Any-Point（TAP）模型的性能。通过在TAP-Vid基准测试上取得了state-of-the-art的结果，该方法在TAP任务上取得了显著的改进。 |
| [^20] | [Continuous Treatment Effects with Surrogate Outcomes](https://arxiv.org/abs/2402.00168) | 本文研究了在部分缺失主要结果的情况下，使用替代结果来估计连续治疗效果，并提出了一种双重稳健方法，通过使用标记和未标记数据，可以有效地纳入替代结果并避免选择偏误问题。该方法的估计值渐近正态性，并在方差方面可能比仅使用标记数据的方法有所改进。模拟实验证明了该方法的良好实证性能。 |
| [^21] | [Optimal Multi-Distribution Learning.](http://arxiv.org/abs/2312.05134) | 本论文提出了一种最优化多分布学习的方法，通过自适应采样来实现数据高效的学习。针对Vapnik-Chervonenkis (VC)维数为d的假设类，算法可以生成一个ε-最优随机假设，并且样本复杂度与最佳下界保持一致。同时，该算法的思想和理论还被进一步扩展以适应Rademacher类。最终提出的算法是奥拉克尔高效的，仅访问假设类。 |
| [^22] | [A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks.](http://arxiv.org/abs/2311.18672) | 该论文比较了不变和等变的经典和量子图神经网络，探讨了它们在高能物理数据分析中的应用，以及如何利用量子计算提供快速而高效的计算范式。同时，研究还指出通过使用不变输入和等变层，可以增强深度网络的有效性和鲁棒性。 |
| [^23] | [Bayesian Optimization with Hidden Constraints via Latent Decision Models.](http://arxiv.org/abs/2310.18449) | 本文介绍了一种基于潜在决策模型的贝叶斯优化方法，通过利用变分自编码器学习可行决策的分布，在原始空间和潜在空间之间实现了双向映射，从而解决了公共决策制定中的隐藏约束问题。 |
| [^24] | [Inside the black box: Neural network-based real-time prediction of US recessions.](http://arxiv.org/abs/2310.17571) | 本研究使用神经网络对美国衰退进行实时预测，发现长短期记忆（LSTM）和门控循环单元（GRU）模型在长期预测任务中表现优于其他模型，并通过SHAP方法对GRU模型进行特征重要性评估。 |
| [^25] | [From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling.](http://arxiv.org/abs/2310.11011) | 本文综述了因果生成建模的技术，其中分为因果表示学习和可控反事实生成两个部分，这些模型融合了因果理论，解决了深度生成模型的一些根本性缺点，并提供了分布偏移鲁棒性、公平性和互操作性等有益属性。 |
| [^26] | [On the sample complexity of estimation in logistic regression.](http://arxiv.org/abs/2307.04191) | 本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。 |
| [^27] | [Improved sampling via learned diffusions.](http://arxiv.org/abs/2307.01198) | 通过学习扩散的方法改进了采样过程，引入了基于变分形式的路径空间度量，提出了对数方差损失，优化了采样性能。 |
| [^28] | [Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision.](http://arxiv.org/abs/2306.16564) | 本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。 |
| [^29] | [Lower Complexity Adaptation for Empirical Entropic Optimal Transport.](http://arxiv.org/abs/2306.13580) | 本文研究了经验熵正则化最优输运的统计表现，并证明了它遵循低复杂度适应原则，推导出了其统计界限及参数化速率。 |
| [^30] | [Learning Costs for Structured Monge Displacements.](http://arxiv.org/abs/2306.11895) | 本文提出了一种新的方法，即学习合适的成本结构来鼓励映射沿着特定的工程特征来传送点，其在扩展 Monge-Bregman-Occam 管道方面做出了重大贡献，并在匹配直方图方面表现出了优异的性能。 |
| [^31] | [Individual Treatment Effects in Extreme Regimes.](http://arxiv.org/abs/2306.11697) | 本文提出了一种新的框架，通过测量潜在结果在存在或缺乏治疗的情况下的尾部衰减率变化，来估计极端环境下的个体治疗效果（ITE$_2$）。 |
| [^32] | [Provably Efficient Bayesian Optimization with Unbiased Gaussian Process Hyperparameter Estimation.](http://arxiv.org/abs/2306.06844) | 针对贝叶斯优化中常见的数据偏差问题，提出了一种新方法，在无需事先知道真实高斯过程超参数的情况下，使用多臂老虎机技术向BO过程中添加随机数据点，采用新的训练损失函数进行超参数估计，以达到次线性收敛到全局最优解的目的。 |
| [^33] | [Pareto Front Identification with Regret Minimization.](http://arxiv.org/abs/2306.00096) | 本文提出了适用于线性Bandit的Pareto前沿识别算法，该算法使用所有动作的上下文信息，具有较低的样本复杂度，并同时保证了Pareto前沿识别和Pareto遗憾最小化。 |
| [^34] | [Better Batch for Deep Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.17028) | 该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。 |
| [^35] | [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning.](http://arxiv.org/abs/2305.15912) | 本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。 |
| [^36] | [Counterfactual Explanation with Missing Values.](http://arxiv.org/abs/2304.14606) | 本论文提出了一种新的反事实解释方法（CEPIA），可以处理数据中的缺失值，使得用户可以获得有效的行动建议并了解缺失值对建议的影响。 |
| [^37] | [Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning.](http://arxiv.org/abs/2304.07278) | 本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。 |
| [^38] | [Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism.](http://arxiv.org/abs/2211.07482) | 本文提出使用融合图来设计等变神经网络的新组件，为解决涉及全局空间和置换对称性的学习任务提供了一种图形化的方法。 |
| [^39] | [GeONet: a neural operator for learning the Wasserstein geodesic.](http://arxiv.org/abs/2209.14440) | GeONet是一个不受网格影响的深度神经算子网络，学习了从初始和终端分布到连接两个端点分布的Wasserstein测地的非线性映射。通过学习鞍点优化条件，GeONet可以快速进行实时预测，并在仿真示例和测试数据上取得了与标准OT求解器相当的准确性。 |

# 详细

[^1]: 在因果发现中集成大型语言模型: 一种统计因果方法

    Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach

    [https://rss.arxiv.org/abs/2402.01454](https://rss.arxiv.org/abs/2402.01454)

    本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。

    

    在实际的统计因果发现（SCD）中，将领域专家知识作为约束嵌入到算法中被广泛接受，因为这对于创建一致有意义的因果模型是重要的，尽管识别背景知识的挑战被认可。为了克服这些挑战，本文提出了一种新的因果推断方法，即通过将LLM的“统计因果提示（SCP）”与SCD方法和基于知识的因果推断（KBCI）相结合，对SCD进行先验知识增强。实验证明，GPT-4可以使LLM-KBCI的输出与带有LLM-KBCI的先验知识的SCD结果接近真实情况，如果GPT-4经历了SCP，那么SCD的结果还可以进一步改善。而且，即使LLM不含有数据集的信息，LLM仍然可以通过其背景知识来改进SCD。

    In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
    
[^2]: 变分线性化Laplace近似在贝叶斯深度学习中的应用

    Variational Linearized Laplace Approximation for Bayesian Deep Learning

    [https://rss.arxiv.org/abs/2302.12565](https://rss.arxiv.org/abs/2302.12565)

    本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。

    

    最近，线性化Laplace近似（LLA）被用来对预训练深度神经网络（DNNs）的预测进行不确定性估计。然而，在训练点或DNN参数较多的情况下，其广泛应用受到了计算成本的限制。因此，其他LLA的近似方法，如Kronecker分解或对角线GGN矩阵的近似，被使用，可能会影响模型的性能。为了解决这些挑战，我们提出了一种基于变分稀疏高斯过程（GP）的LLA近似方法。我们的方法基于GP的对偶RKHS公式，并保留了原始DNN的预测均值。此外，它允许有效的随机优化，从而在训练数据集的大小中实现子线性训练时间。具体而言，其训练成本与训练点的数量无关。我们比较了我们的方法与其他近似方法的性能，并展示了其在准确性和计算效率方面的优势。

    The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose
    
[^3]: 利用椭球集进行多维时间序列的合规预测

    Conformal prediction for multi-dimensional time series by ellipsoidal sets

    [https://arxiv.org/abs/2403.03850](https://arxiv.org/abs/2403.03850)

    开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于在多元时间序列中构建预测区域，具有更小的预测区域和有效的覆盖。

    

    合规预测（CP）因其无需假设分布、不受模型限制且在理论上可靠而成为一种流行的不确定性量化方法。对于监督学习中的预测问题，大多数CP方法专注于为单变量响应构建预测区间。在本文中，我们开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于为多元响应构建预测区域，特别是在不可交换的多元时间序列环境中。在理论上，我们估计了条件覆盖间隙的有限样本高概率界限。在实证方面，我们证明了$\texttt{MultiDimSPCI}$在各种多元时间序列上保持有效覆盖，同时产生比CP和非CP基线更小的预测区域。

    arXiv:2403.03850v1 Announce Type: cross  Abstract: Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.
    
[^4]: 关于扩散概率模型渐近均方误差最优性的研究

    On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models

    [https://arxiv.org/abs/2403.02957](https://arxiv.org/abs/2403.02957)

    本论文通过严格证明一个特定的DPM去噪策略在大量扩散步数下收敛到均方误差最优条件均值估计器，突出了DPM由渐近最优的去噪器组成，同时具有强大生成器的独特视角。

    

    最近，扩散概率模型（DPMs）在去噪任务中展现出巨大潜力。尽管它们在实际应用中很有用，但它们的理论理解存在明显的差距。本文通过严格证明特定DPM去噪策略在大量扩散步数下收敛到均方误差（MSE）最优条件均值估计器（CME），为该领域提供了新的理论见解。研究的基于DPM的去噪器在训练过程中与DPMs共享，但在训练后的逆推理过程中仅传递条件均值。我们强调了DPM由渐近最优的去噪器组成的独特视角，同时通过在逆过程中切换重新采样的方式继承了一个强大的生成器。通过数值结果验证了理论发现。

    arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
    
[^5]: 大型语言模型的预测排名

    Prediction-Powered Ranking of Large Language Models

    [https://arxiv.org/abs/2402.17826](https://arxiv.org/abs/2402.17826)

    该研究提出了一种统计框架，可以衡量人类与模型偏好之间的不确定性，从而进行大型语言模型的预测排名。

    

    大型语言模型通常根据其与人类偏好的一致性水平进行排名--如果一个模型的输出更受人类偏好，那么它就比其他模型更好。本文提出了一种统计框架来弥合人类与模型偏好之间可能引入的不一致性。

    arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
    
[^6]: 迈向变压器：用变压器彻底改变混合整数规划的解决方案

    Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers

    [https://arxiv.org/abs/2402.13380](https://arxiv.org/abs/2402.13380)

    这项研究利用变压器模型解决混合整数规划问题，首次采用变压器预测二进制变量，提出的算法在解决时间上超越了传统CPLEX和LSTM。

    

    在这项研究中，我们引入了一种创新的深度学习框架，利用变压器模型来解决混合整数规划的挑战，特别是专注于容量限制批量生产问题（CLSP）。据我们所知，我们的方法是首个利用变压器来预测混合整数规划问题中的二进制变量。具体而言，我们的方法利用编码器-解码器变压器处理顺序数据的能力，非常适合预测每个CLSP周期中表示生产设置决策的二进制变量。这个问题本质上是动态的，我们需要在约束条件下处理顺序决策。我们提出了一种有效的算法，通过变压器神经网络学习CLSP解决方案。所提出的后处理变压器算法在解决时间上超越了最先进的求解器CPLEX和长短期记忆（LSTM）。

    arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
    
[^7]: 思维链激发变压器解决固有串行问题的能力

    Chain of Thought Empowers Transformers to Solve Inherently Serial Problems

    [https://arxiv.org/abs/2402.12875](https://arxiv.org/abs/2402.12875)

    思维链赋予变压器模型执行固有串行计算的能力，提高了变压器在算术和符号推理任务中的准确性。

    

    指导模型生成一系列中间步骤，即思维链（CoT），是提高大型语言模型（LLMs）在算术和符号推理任务上准确性的高效方法。然而，CoT背后的机制仍不清楚。这项工作通过表达性的视角提供了对解码器专用变压器的CoT能力的理论理解。在概念上，CoT赋予模型执行固有串行计算的能力，而这种能力在变压器中缺乏，特别是当深度较低时。先前的作品已经表明，在没有CoT的情况下，具有有限精度$\mathsf{poly}(n)$嵌入尺寸的恒定深度变压器只能在$\mathsf{TC}^0$中解决问题。我们首先展示了具有常数位精度的恒定深度变压器的更紧密的表达性上界，它只能解决$\mathsf{AC}^0$中的问题。

    arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
    
[^8]: 使用不同标注函数的协作学习

    Collaborative Learning with Different Labeling Functions

    [https://arxiv.org/abs/2402.10445](https://arxiv.org/abs/2402.10445)

    研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。

    

    我们研究了一种 Collaborative PAC Learning 的变体，在这种情况下，我们旨在学习每个$n$个数据分布的准确分类器，同时最小化从它们总共抽取的样本数量。与通常的协作学习设置不同，不假设存在一个同时对所有分布准确的单一分类器。我们表明，当数据分布满足较弱的可实现性假设时，仍然可以实现高效的学习。我们给出了一种基于经验风险最小化(ERM)的学习算法，应用于假设类的一个自然增强，分析依赖于对该增强类的VC维的上界。

    arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
    
[^9]: 高斯模型集成置信传播用于高维系统中的高效推断

    Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems

    [https://arxiv.org/abs/2402.08193](https://arxiv.org/abs/2402.08193)

    高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。

    

    高维模型中的高效推断仍然是机器学习中的一个核心挑战。本文介绍了一种名为高斯模型集成置信传播（GEnBP）算法的方法，该方法是集成卡尔曼滤波器和高斯置信传播（GaBP）方法的结合。GEnBP通过在图模型结构中传递低秩本地信息来更新集成模型。这种组合继承了每种方法的有利特性。集成技术使得GEnBP能够处理高维状态、参数和复杂的、嘈杂的黑箱生成过程。在图模型结构中使用本地信息确保了该方法适用于分布式计算，并能高效地处理复杂的依赖结构。当集成大小远小于推断维度时，GEnBP特别有优势。这种情况在空时建模、图像处理和物理模型反演等领域经常出现。GEnBP可以应用于一般性问题。

    Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
    
[^10]: 使用图神经网络进行超图节点分类

    Hypergraph Node Classification With Graph Neural Networks

    [https://arxiv.org/abs/2402.05569](https://arxiv.org/abs/2402.05569)

    本研究提出了一种简单高效的框架，利用加权子图扩展的图神经网络(WCE-GNN)实现了超图节点分类。实验证明，WCE-GNN具有优秀的预测效果和较低的计算复杂度。

    

    超图是用来模拟现实世界数据中的高阶相互作用的关键。图神经网络（GNNs）的成功揭示了神经网络处理具有成对交互的数据的能力。这激发了使用神经网络处理具有高阶相互作用的数据的想法，从而导致了超图神经网络（HyperGNNs）的发展。GNNs和HyperGNNs通常被认为是不同的，因为它们被设计用于处理不同几何拓扑的数据。然而，在本文中，我们在理论上证明，在节点分类的上下文中，大多数HyperGNNs可以使用带有超图的加权子图扩展的GNN来近似。这导致了WCE-GNN，一种简单高效的框架，包括一个GNN和一个加权子图扩展（WCE），用于超图节点分类。对于九个真实世界的超图节点分类数据集的实验表明，WCE-GNN不仅具有优秀的预测效果，而且具有较低的计算复杂度。

    Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
    
[^11]: 预测个体治疗效果的一致性蒙特卡洛元学习模型

    Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects

    [https://arxiv.org/abs/2402.04906](https://arxiv.org/abs/2402.04906)

    本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。

    

    认识干预效果，即治疗效果，对于决策至关重要。用条件平均治疗效果 (CATE) 估计等方法通常只提供治疗效果的点估计，而常常需要额外的不确定性量化。因此，我们提出了一个新方法，即一致性蒙特卡洛 (CMC) 元学习模型，利用一致性预测系统、蒙特卡洛采样和 CATE 元学习模型，来产生可用于个性化决策的预测分布。此外，我们展示了结果噪声分布的特定假设如何严重影响这些不确定性预测。尽管如此，CMC框架展示了强大的实验覆盖范围，同时保持较小的区间宽度，以提供真实个体治疗效果的估计。

    Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
    
[^12]: 对多个合成数据集的集成进行偏差-方差分解

    A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets

    [https://arxiv.org/abs/2402.03985](https://arxiv.org/abs/2402.03985)

    本研究通过对多个合成数据集进行偏差-方差分解，增加了对其理论理解。实验证明多个合成数据集对于高方差的下游预测器特别有益，并提供了一个简单的经验法则用于选择适当的合成数据集数量。

    

    最近的研究强调了为监督学习生成多个合成数据集的好处，包括增加准确性、更有效的模型选择和不确定性估计。这些好处在经验上有明确的支持，但对它们的理论理解目前非常有限。我们通过推导使用多个合成数据集的几种设置的偏差-方差分解，来增加理论理解。我们的理论预测，对于高方差的下游预测器，多个合成数据集将特别有益，并为均方误差和Brier分数的情况提供了一个简单的经验法则来选择合适的合成数据集数量。我们通过评估一个集成在多个合成数据集和几个真实数据集以及下游预测器上的性能来研究我们的理论在实践中的效果。结果验证了我们的理论，表明我们的洞察也在实践中具有相关性。

    Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.
    
[^13]: Pard: 具有置换不变性的自回归扩散用于图生成

    Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation

    [https://arxiv.org/abs/2402.03687](https://arxiv.org/abs/2402.03687)

    PARD是一种将扩散模型与自回归方法相结合的置换不变性自回归扩散模型，通过使用图中的部分顺序以块逐块的自回归方式生成图。

    

    尽管自回归模型对于图的顺序敏感，但其简单有效，在图生成领域一直占据主导地位。然而，扩散模型因其置换不变性而越来越受关注。目前的图扩散模型一次性生成图，但需要额外的特征和成千上万步的去噪才能达到最佳性能。我们引入了PARD，一种将扩散模型与自回归方法相结合的置换不变性自回归扩散模型。PARD利用自回归模型的效果和效率，同时保持置换不变性，无需关注图的顺序敏感性。具体来说，我们发现与集合不同，图中的元素并不是完全无序的，节点和边有一个独特的部分顺序。利用这个部分顺序，PARD以块逐块的自回归方式生成图，其中每个块的概率为c。

    Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
    
[^14]: 主动相关聚类的有效获取函数

    Effective Acquisition Functions for Active Correlation Clustering

    [https://arxiv.org/abs/2402.03587](https://arxiv.org/abs/2402.03587)

    本文提出了三种有效的获取函数用于主动相关聚类，分别基于不一致性概念和信息论量。

    

    相关聚类是一种强大的无监督学习范例，支持正和负的相似性。本文假设相似性事先未知，而是采用主动学习以一种成本有效的方式迭代地查询相似性。具体而言，我们开发了三种有效的获取函数用于在此设置下使用。其中一种基于不一致性概念（即当相似性违反传递性时）。其余两个基于信息论量，即熵和信息增益。

    Correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ active learning to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.
    
[^15]: 通过组合特征对齐增强组合通用性

    Enhancing Compositional Generalization via Compositional Feature Alignment

    [https://arxiv.org/abs/2402.02851](https://arxiv.org/abs/2402.02851)

    通过组合特征对齐，增强了模型的组合通用性，使其能够推广到未见过的领域-类别组合。

    

    机器学习模型在现实世界的应用中经常面临数据分布偏移的问题，即训练数据和测试数据分布之间存在差异。在常见的多领域多类别设置中，随着类别和领域数量的增加，很难为每个领域-类别组合收集训练数据。这个挑战自然地引发了对具备组合通用性（CG）能力的模型的探索，即模型可以推广到未见过的领域-类别组合。为了深入研究CG挑战，我们开发了CG-Bench，这是一套从现有实际图像数据集派生的CG基准测试，并观察到目前在基础模型（如CLIP和DINOv2）上流行的预训练-微调范式在这个挑战中存在困难。为了解决这个挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，它通过在预训练的编码器上学习两个正交线性头部来对齐类别和领域的标签。

    Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab
    
[^16]: 因果贝叶斯优化通过外源分布学习

    Causal Bayesian Optimization via Exogenous Distribution Learning

    [https://arxiv.org/abs/2402.02277](https://arxiv.org/abs/2402.02277)

    本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。

    

    在结构化因果模型中，将目标变量最大化作为操作目标是一个重要的问题。现有的因果贝叶斯优化（CBO）方法要么依赖于改变因果结构以最大化奖励的硬干预，要么引入动作节点到内生变量中，以调整数据生成机制以实现目标。本文引入了一种新的方法来学习外源变量的分布，这在现有方法中通常被忽略或通过期望进行边缘化。外源分布学习提高了通常通过有限观测数据训练的代理模型中的结构化因果模型的近似精度。此外，学习到的外源分布将现有的CBO扩展到超出加性噪声模型（ANM）的一般因果方案。恢复外源变量使我们能够为噪声或未观测到的隐藏变量使用更灵活的先验。引入了一种新的CBO方法。

    Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
    
[^17]: 分布约简：用格罗莫夫-瓦瑟斯坦投影统一降维和聚类

    Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection

    [https://arxiv.org/abs/2402.02239](https://arxiv.org/abs/2402.02239)

    本文提出了一种新的分布约简方法，利用格罗莫夫-瓦瑟斯坦投影统一了降维和聚类，通过优化问题同时解决降维和聚类，实验证明了该方法在多个领域表现出卓越性能。

    

    无监督学习旨在捕捉潜在的大规模和高维数据集的结构。传统上，这涉及使用降维方法将数据投影到可解释的空间上，或将数据点组织成有意义的聚类。在实践中，这些方法通常是按顺序使用的，而不能保证聚类与降维相一致。在这项工作中，我们提出了一个新的观点：使用分布。通过利用最优输运的工具，特别是格罗莫夫-瓦瑟斯坦距离，我们将聚类和降维统一为一个称为分布约简的单一框架。这使我们能够通过单个优化问题同时解决聚类和降维。通过全面的实验证明了我们方法的多功能性和解释性，并表明它在各种图像和基因组数据集上优于现有方法。

    Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.
    
[^18]: 样本、估计、聚合：因果发现基础模型的一种方法

    Sample, estimate, aggregate: A recipe for causal discovery foundation models

    [https://arxiv.org/abs/2402.01929](https://arxiv.org/abs/2402.01929)

    本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。

    

    因果发现是从数据中推断因果结构的任务，它可以加速科学研究、指导决策等。然而，现有因果发现算法的每个数据集的特性使它们变得缓慢、需要大量数据并且脆弱。受基础模型的启发，我们提出了一种因果发现框架，其中深度学习模型预训练用于处理在较小的变量子集上运行的经典发现算法的预测。这种方法可以利用以下观察结果：经典算法的输出在小问题上计算速度快，对（边际）数据结构具有信息量，且它们的输出结构作为对象在数据集之间可以进行比较。我们的方法在合成和实际数据集上实现了最先进的性能，可以推广到训练期间未见过的数据生成机制，并且提供比现有模型快几个数量级的推理速度。

    Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.
    
[^19]: BootsTAP: 针对Tracking-Any-Point的自举训练

    BootsTAP: Bootstrapped Training for Tracking-Any-Point

    [https://arxiv.org/abs/2402.00847](https://arxiv.org/abs/2402.00847)

    该论文展示了如何通过使用大规模、无标签、未筛选的真实世界数据，以及采用自监督的师生设置，来改进Tracking-Any-Point（TAP）模型的性能。通过在TAP-Vid基准测试上取得了state-of-the-art的结果，该方法在TAP任务上取得了显著的改进。

    

    为了使模型对物理和运动有更深入的理解，让它们能够感知实景中固体表面的移动和变形是很有用的。这可以形式化为Tracking-Any-Point (TAP)，要求算法能够追踪视频中与固体表面对应的任意点，可能是在空间和时间上密集的。目前，TAP需要大规模的真实数据进行训练，但目前只能在模拟环境中获得有限种类的对象和运动。在这项工作中，我们演示了如何使用大规模、无标签、未筛选的真实世界数据，在仅进行最小架构更改的情况下提高TAP模型的性能，采用了自监督的师生设置。我们在TAP-Vid基准测试上展示了超过以往成果的最先进性能：例如，TAP-Vid-DAVIS的性能从61.3%提升到66.4%，TAP-Vid-Kinetics从57.2%提升到61.5%。

    To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.
    
[^20]: 使用替代结果进行连续治疗效果研究

    Continuous Treatment Effects with Surrogate Outcomes

    [https://arxiv.org/abs/2402.00168](https://arxiv.org/abs/2402.00168)

    本文研究了在部分缺失主要结果的情况下，使用替代结果来估计连续治疗效果，并提出了一种双重稳健方法，通过使用标记和未标记数据，可以有效地纳入替代结果并避免选择偏误问题。该方法的估计值渐近正态性，并在方差方面可能比仅使用标记数据的方法有所改进。模拟实验证明了该方法的良好实证性能。

    

    在许多实际因果推断应用中，主要结果（标签）常常是部分缺失的，特别是如果它们很昂贵或很难收集。如果缺失依赖于协变量（即缺失不完全随机），仅基于完全观测样本的分析可能存在偏误。在这种情况下，结合与主要结果相关的完全观测的治疗后变量（替代结果）可以改进估计。在本文中，我们研究了替代结果在估计连续治疗效果中的作用，并提出了一种双重稳健方法，以高效地将替代结果纳入分析中，该方法使用了标记和未标记数据，并且不会受到上述选择偏误问题的影响。重要的是，我们建立了所提估计器的渐近正态性，并展示了与仅使用标记数据的方法相比，方差的可能改进。广泛的模拟显示我们的方法具有吸引人的经验性能。

    In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.
    
[^21]: 最优化多分布学习

    Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05134](http://arxiv.org/abs/2312.05134)

    本论文提出了一种最优化多分布学习的方法，通过自适应采样来实现数据高效的学习。针对Vapnik-Chervonenkis (VC)维数为d的假设类，算法可以生成一个ε-最优随机假设，并且样本复杂度与最佳下界保持一致。同时，该算法的思想和理论还被进一步扩展以适应Rademacher类。最终提出的算法是奥拉克尔高效的，仅访问假设类。

    

    多分布学习（MDL）旨在学习一个共享模型，使得在k个不同的数据分布下，最小化最坏情况风险，已成为适应健壮性、公平性、多组合作等需求的统一框架。实现数据高效的MDL需要在学习过程中进行自适应采样，也称为按需采样。然而，最优样本复杂度的上下界之间存在较大差距。针对Vapnik-Chervonenkis（VC）维数为d的假设类，我们提出了一种新颖的算法，可生成一个ε-最优随机假设，其样本复杂度接近于（d+k）/ε^2（在某些对数因子中），与已知的最佳下界匹配。我们的算法思想和理论被进一步扩展，以适应Rademacher类。提出的算法是奥拉克尔高效的，仅仅访问假设类

    Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
    
[^22]: 不变和等变的经典和量子图神经网络的比较

    A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2311.18672](http://arxiv.org/abs/2311.18672)

    该论文比较了不变和等变的经典和量子图神经网络，探讨了它们在高能物理数据分析中的应用，以及如何利用量子计算提供快速而高效的计算范式。同时，研究还指出通过使用不变输入和等变层，可以增强深度网络的有效性和鲁棒性。

    

    机器学习算法在理解CERN大型强子对撞机(LHC)上产生的大量高能粒子碰撞数据时起着重要作用。这些碰撞事件的数据可以自然地用图结构表示。因此，深度几何方法，如图神经网络(GNNs)，已经在高能物理数据分析的各种任务中得到应用。一个典型的任务是喷注标记，其中喷注被视为具有不同特征和其组成粒子之间的边连接的点云。LHC粒子数据集的规模和复杂性的增加，以及用于其分析的计算模型，大大促进了开发替代快速且高效的计算范式，如量子计算。此外，为了增强深度网络的有效性和鲁棒性，可以通过使用不变输入和等变层来利用数据中存在的基本对称性。

    Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In t
    
[^23]: 基于潜在决策模型的具有隐藏约束的贝叶斯优化方法

    Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])

    [http://arxiv.org/abs/2310.18449](http://arxiv.org/abs/2310.18449)

    本文介绍了一种基于潜在决策模型的贝叶斯优化方法，通过利用变分自编码器学习可行决策的分布，在原始空间和潜在空间之间实现了双向映射，从而解决了公共决策制定中的隐藏约束问题。

    

    贝叶斯优化（BO）已经成为解决复杂决策问题的强大工具，尤其在公共政策领域如警察划区方面。然而，由于定义可行区域的复杂性和决策的高维度，其在公共决策制定中的广泛应用受到了阻碍。本文介绍了一种新的贝叶斯优化方法——隐藏约束潜在空间贝叶斯优化（HC-LSBO），该方法集成了潜在决策模型。该方法利用变分自编码器来学习可行决策的分布，实现了原始决策空间与较低维度的潜在空间之间的双向映射。通过这种方式，HC-LSBO捕捉了公共决策制定中固有的隐藏约束的细微差别，在潜在空间中进行优化的同时，在原始空间中评估目标。我们通过对合成数据集和真实数据集进行数值实验来验证我们的方法，特别关注大规模问题。

    Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
    
[^24]: 黑匣子内部：基于神经网络的实时预测美国衰退的研究

    Inside the black box: Neural network-based real-time prediction of US recessions. (arXiv:2310.17571v1 [econ.EM])

    [http://arxiv.org/abs/2310.17571](http://arxiv.org/abs/2310.17571)

    本研究使用神经网络对美国衰退进行实时预测，发现长短期记忆（LSTM）和门控循环单元（GRU）模型在长期预测任务中表现优于其他模型，并通过SHAP方法对GRU模型进行特征重要性评估。

    

    本研究使用前馈神经网络（FFN）和两种特定类型的循环神经网络，即长短期记忆（LSTM）和门控循环单元（GRU），对1967年至2021年的美国衰退进行建模。然后利用估计的模型对美国的大衰退和Covid-19衰退进行实时预测，将其预测性能与传统线性模型、逻辑回归模型（有和无岭回归惩罚）进行比较。外样本表现表明，LSTM和GRU在衰退预测领域具有应用潜力，特别适用于长期预测任务。相对于不同类型的统计性能指标，在5个预测周期内，它们优于其他类型的模型。而用Shapley增量解释（SHAP）方法评估衡量GRU在不同预测周期内的重要特征，以深入了解特征重要性。

    Feedforward neural network (FFN) and two specific types of recurrent neural network, long short-term memory (LSTM) and gated recurrent unit (GRU), are used for modeling US recessions in the period from 1967 to 2021. The estimated models are then employed to conduct real-time predictions of the Great Recession and the Covid-19 recession in US. Their predictive performances are compared to those of the traditional linear models, the logistic regression model both with and without the ridge penalty. The out-of-sample performance suggests the application of LSTM and GRU in the area of recession forecasting, especially for the long-term forecasting tasks. They outperform other types of models across 5 forecasting horizons with respect to different types of statistical performance metrics. Shapley additive explanations (SHAP) method is applied to the fitted GRUs across different forecasting horizons to gain insight into the feature importance. The evaluation of predictor importance differs b
    
[^25]: 从可识别的因果表示到可控的反事实生成：因果生成建模综述

    From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])

    [http://arxiv.org/abs/2310.11011](http://arxiv.org/abs/2310.11011)

    本文综述了因果生成建模的技术，其中分为因果表示学习和可控反事实生成两个部分，这些模型融合了因果理论，解决了深度生成模型的一些根本性缺点，并提供了分布偏移鲁棒性、公平性和互操作性等有益属性。

    

    深度生成模型在数据密度估计和从有限样本中生成数据方面取得了巨大的成功。然而，这些模型存在一些根本性的缺点，如缺乏可解释性、引入虚假相关性和差劲的超出分布的外推能力。为了解决这些挑战，可以将因果理论融入深度生成建模中。结构因果模型描述了数据生成过程，并对系统中变量之间的复杂因果关系和机制进行建模。因此，结构因果模型可以与深度生成模型自然地结合。因果模型为深度生成模型提供了几个有益的属性，如分布偏移鲁棒性、公平性和互操作性。本文提供了对因果生成建模的技术综述，分为因果表示学习和可控反事实生成两个部分。

    Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
    
[^26]: 关于逻辑回归中参数估计的样本复杂度研究

    On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])

    [http://arxiv.org/abs/2307.04191](http://arxiv.org/abs/2307.04191)

    本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。

    

    逻辑回归模型是噪声二元分类问题中最常见的数据生成模型之一。本文研究了在标准正态协变量下，以$\ell_2$误差为限，估计逻辑回归模型参数的样本复杂度，考虑了维度和逆温度的影响。逆温度控制了数据生成过程中的信噪比。虽然逻辑回归的广义界限和渐近性能已经有了深入研究，但关于参数估计的非渐近样本复杂度在之前的分析中没有讨论其与误差和逆温度的依赖关系。我们展示了样本复杂度曲线在逆温度方面具有两个转折点（或临界点），明确划分了低、中和高温度区域。

    The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
    
[^27]: 通过学习扩散改进采样

    Improved sampling via learned diffusions. (arXiv:2307.01198v1 [cs.LG])

    [http://arxiv.org/abs/2307.01198](http://arxiv.org/abs/2307.01198)

    通过学习扩散的方法改进了采样过程，引入了基于变分形式的路径空间度量，提出了对数方差损失，优化了采样性能。

    

    最近，一系列论文提出了基于深度学习的方法，使用控制扩散过程从非标准化目标密度中采样。在本研究中，我们将这些方法视为Schrödinger桥问题的特例，寻求给定先验分布和指定目标之间最可能的随机演化。我们进一步通过引入基于时间反演扩散过程的路径空间度量之间的差异的变分形式来推广这个框架。这个抽象的视角导致了可以通过梯度优化的实际损失，并将先前的目标作为特例。与此同时，它允许我们考虑除了已知存在模式坍缩问题的反向Kullback-Leibler差别之外的其他差别。特别地，我们提出了所谓的对数方差损失，它具有良好的数值特性，并显著提高了在所有考虑的情况下的性能。

    Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered a
    
[^28]: 通过Pareto Optimal自监督实现大型语言模型的自动校准和错误修正

    Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])

    [http://arxiv.org/abs/2306.16564](http://arxiv.org/abs/2306.16564)

    本文介绍了一种Pareto Optimal自监督框架，利用可用的编程监督将大型语言模型(LLM)的响应进行系统校准，通过为每个响应生成风险评分，而无需额外的手动工作。

    

    大型语言模型(LLM)已经展现了出色的能力，适用于广泛的应用领域，但是准确性仍然是一个重要的增长领域，特别是在生物医学等关键领域。一种有效的方法，用于校准LLM响应的置信水平，对于自动检测错误并促进人机协作验证至关重要。一个重要的校准信号来源是专家指定的编程监督，通常具有较低的成本，但也有其自身的局限性，如噪声和覆盖范围。在本文中，我们引入了一种Pareto Optimal自监督框架，可以利用可用的编程监督来系统地校准LLM响应，通过为每个响应生成风险评分，而不需要任何额外的手动工作。这通过学习一个调和模型来实现，将LLM输出与其他可用的监督来源相协调，将更不确定的响应分配更高的风险评分。

    Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
    
[^29]: 经验熵正则化最优输运的低复杂度适应性

    Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])

    [http://arxiv.org/abs/2306.13580](http://arxiv.org/abs/2306.13580)

    本文研究了经验熵正则化最优输运的统计表现，并证明了它遵循低复杂度适应原则，推导出了其统计界限及参数化速率。

    

    经验熵正则化最优输运 (EOT) 是优化输运 (OT) 的一种有效且计算可行的替代方案，对大规模数据分析有着广泛的应用。本文推导出了 EOT 成本的新的统计界限，并显示它们在熵正则化参数 $\epsilon$ 和样本大小 $n$ 的统计性能仅取决于两个概率测度之中较简单的那个。例如，在充分平滑的成本下，这会产生具有$\epsilon^{-d/2}$因子的参数化速率$n^{-1/2}$，其中$d$是两个总体测度的最小维度。这确认了经验EOT也遵循了最近才为未规则化OT确认的低复杂度适应原则的标志性特征。根据我们的理论，我们展示了欧几里得空间上的测度的经验熵Gromov-Wasserstein距离及其未规则化版本也遵循此原则。

    Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip
    
[^30]: 学习结构蒙日位移的成本

    Learning Costs for Structured Monge Displacements. (arXiv:2306.11895v1 [stat.ML])

    [http://arxiv.org/abs/2306.11895](http://arxiv.org/abs/2306.11895)

    本文提出了一种新的方法，即学习合适的成本结构来鼓励映射沿着特定的工程特征来传送点，其在扩展 Monge-Bregman-Occam 管道方面做出了重大贡献，并在匹配直方图方面表现出了优异的性能。

    

    最优输运理论为机器学习提供了多种推断样本间密度前向映射的工具。尽管最近在机器学习领域中该理论已经见证了许多方法的发展，但其实际实现仍然极其困难，因为它同时面临计算和统计上的挑战。现有方法很少有不使用默认选择来估计这些映射的情况，其中简单的平方欧氏距离作为地面费用$c(x,y)=\|x-y\|^2_2$。我们在本文中采取不同的方法，以\emph{学习}合适的成本结构，鼓励映射沿着特定的工程特征来传送点。我们将最近提出的 Monge-Bregman-Occam 管道~\citep{cuturi2023monge} 的范式进行了扩展，该范式基于替代的成本公式$c(x,y)=h(x-y)$ ，它也是成本不变的，但采用更一般的形式$h=\tfrac12 \ell_2^2+\tau$，其中$\tau$是适当的凸规则项。

    Optimal transport theory has provided machine learning with several tools to infer a push-forward map between densities from samples. While this theory has recently seen tremendous methodological developments in machine learning, its practical implementation remains notoriously difficult, because it is plagued by both computational and statistical challenges. Because of such difficulties, existing approaches rarely depart from the default choice of estimating such maps with the simple squared-Euclidean distance as the ground cost, $c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the motivation of \emph{learning} a suitable cost structure to encourage maps to transport points along engineered features. We extend the recently proposed Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is an approp
    
[^31]: 极端环境下的个体治疗效果

    Individual Treatment Effects in Extreme Regimes. (arXiv:2306.11697v1 [stat.ME])

    [http://arxiv.org/abs/2306.11697](http://arxiv.org/abs/2306.11697)

    本文提出了一种新的框架，通过测量潜在结果在存在或缺乏治疗的情况下的尾部衰减率变化，来估计极端环境下的个体治疗效果（ITE$_2$）。

    

    了解极端环境下的个体治疗效果对于描述不同干预策略的风险至关重要。但极端环境数据很难收集，因为它在实践中很少被观察到。为了解决这个问题，我们提出了一个新的框架来估计极端环境下的个体治疗效果（ITE$_2$）。具体而言，我们通过测量潜在结果在存在或缺乏治疗的情况下的尾部衰减率变化来量化这种效果。随后，我们建立了 ITE$_2$ 的计算条件，并开发了计算算法。我们在各种合成和半合成数据集上演示了我们提出的方法的功效。

    Understanding individual treatment effects in extreme regimes is important for characterizing risks associated with different interventions. This is hindered by the fact that extreme regime data may be hard to collect, as it is scarcely observed in practice. In addressing this issue, we propose a new framework for estimating the individual treatment effect in extreme regimes (ITE$_2$). Specifically, we quantify this effect by the changes in the tail decay rates of potential outcomes in the presence or absence of the treatment. Subsequently, we establish conditions under which ITE$_2$ may be calculated and develop algorithms for its computation. We demonstrate the efficacy of our proposed method on various synthetic and semi-synthetic datasets.
    
[^32]: 具有无偏高斯过程超参数估计的可证明高效贝叶斯优化

    Provably Efficient Bayesian Optimization with Unbiased Gaussian Process Hyperparameter Estimation. (arXiv:2306.06844v1 [stat.ML])

    [http://arxiv.org/abs/2306.06844](http://arxiv.org/abs/2306.06844)

    针对贝叶斯优化中常见的数据偏差问题，提出了一种新方法，在无需事先知道真实高斯过程超参数的情况下，使用多臂老虎机技术向BO过程中添加随机数据点，采用新的训练损失函数进行超参数估计，以达到次线性收敛到全局最优解的目的。

    

    基于高斯过程的贝叶斯优化是一种有效优化黑盒函数的方法。该方法的实际性能和理论保证，取决于正确估计高斯过程超参数值。但在实践中，由于常用于贝叶斯优化的数据采样策略可能会引起数据偏差，从而导致超参数估计错误。为了解决这个问题，我们提出了一种新的贝叶斯优化方法，即使在事先不知道真实高斯过程超参数并需要从观察数据中进行估计时，该方法也能够次线性收敛到目标函数的全局最优解。我们的方法使用多臂老虎机技术(EXP3)向BO过程中添加随机数据点，并使用新的训练损失函数用于高斯过程超参数估计过程的训练。

    Gaussian process (GP) based Bayesian optimization (BO) is a powerful method for optimizing black-box functions efficiently. The practical performance and theoretical guarantees associated with this approach depend on having the correct GP hyperparameter values, which are usually unknown in advance and need to be estimated from the observed data. However, in practice, these estimations could be incorrect due to biased data sampling strategies commonly used in BO. This can lead to degraded performance and break the sub-linear global convergence guarantee of BO. To address this issue, we propose a new BO method that can sub-linearly converge to the global optimum of the objective function even when the true GP hyperparameters are unknown in advance and need to be estimated from the observed data. Our method uses a multi-armed bandit technique (EXP3) to add random data points to the BO process, and employs a novel training loss function for the GP hyperparameter estimation process that ens
    
[^33]: 通过遗憾最小化方法进行Pareto前沿识别

    Pareto Front Identification with Regret Minimization. (arXiv:2306.00096v1 [stat.ML])

    [http://arxiv.org/abs/2306.00096](http://arxiv.org/abs/2306.00096)

    本文提出了适用于线性Bandit的Pareto前沿识别算法，该算法使用所有动作的上下文信息，具有较低的样本复杂度，并同时保证了Pareto前沿识别和Pareto遗憾最小化。

    

    本文考虑线性Bandit情况下的Pareto前沿识别（PFILin），其目标是在平均奖励向量作为环境的线性函数的情况下，识别一组奖励向量不被其他任何向量所占优。PFILin包括最佳动作识别和多目标主动学习等特殊情况。我们提出的算法的样本复杂度为$\tilde{O}(d/\Delta^2)$，其中$d$是上下文的维数，$\Delta$是问题复杂性的一种度量。我们的样本复杂度在对数因子上是最优的。本算法的一个新特点是使用所有动作的上下文信息。除了有效地识别Pareto前沿之外，我们的算法还保证，在样本数大于$\Omega(d\log dL)$时，对于$L$维矢量奖励，瞬时Pareto遗憾的$\tilde{O}(\sqrt{d/t})$界限。通过使用所有动作的上下文信息，我们提出的算法同时为线性Bandit提供了有效的Pareto前沿识别和Pareto遗憾最小化。

    We consider Pareto front identification for linear bandits (PFILin) where the goal is to identify a set of arms whose reward vectors are not dominated by any of the others when the mean reward vector is a linear function of the context. PFILin includes the best arm identification problem and multi-objective active learning as special cases. The sample complexity of our proposed algorithm is $\tilde{O}(d/\Delta^2)$, where $d$ is the dimension of contexts and $\Delta$ is a measure of problem complexity. Our sample complexity is optimal up to a logarithmic factor. A novel feature of our algorithm is that it uses the contexts of all actions. In addition to efficiently identifying the Pareto front, our algorithm also guarantees $\tilde{O}(\sqrt{d/t})$ bound for instantaneous Pareto regret when the number of samples is larger than $\Omega(d\log dL)$ for $L$ dimensional vector rewards. By using the contexts of all arms, our proposed algorithm simultaneously provides efficient Pareto front ide
    
[^34]: 深度概率时间序列预测的更好Batch方法

    Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])

    [http://arxiv.org/abs/2305.17028](http://arxiv.org/abs/2305.17028)

    该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。

    

    深度概率时间序列预测因其能够提供有价值的不确定性量化而受到广泛关注。然而，许多现有模型过于简单化问题，假设误差过程是与时间无关的，从而忽略了误差过程中的序列相关性。这可能会降低预测的准确性，使这些模型对决策性任务的有效性减弱。为了克服这一限制，我们提出了一种创新的训练方法，将误差自相关性纳入考虑，以增强概率预测的准确性。我们的方法涉及构造一个mini-batch，作为$D$个连续时间序列段进行模型训练，并显式地学习一个协方差矩阵，覆盖了相邻时间步之间的误差相关性。由此产生的协方差矩阵可用于提高预测准确性和增强不确定性的量化。

    Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
    
[^35]: 改进ReLU网络特征学习的神经特征激活值分析

    Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])

    [http://arxiv.org/abs/2305.15912](http://arxiv.org/abs/2305.15912)

    本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。

    

    本文研究了神经网络中单个ReLU单元的特征激活值。我们将ReLU单元在输入空间中对应的特征激活值集合称为ReLU单元的特征激活集。我们建立了特征激活集与ReLU网络中学习特征之间的明确联系，并揭示了现代深度学习架构中使用的各种神经网络规范化技术如何规范化和稳定SGD优化。利用这些洞见，我们提出了一种几何方法来参数化ReLU网络以改进特征学习。我们经验性地验证了其有用性，使用了不那么精心选择的初始化方案和更大的学习率。我们报告了更好的优化稳定性，更快的收敛速度和更好的泛化性能。

    We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
    
[^36]: 缺失值下的反事实解释方法

    Counterfactual Explanation with Missing Values. (arXiv:2304.14606v1 [cs.LG])

    [http://arxiv.org/abs/2304.14606](http://arxiv.org/abs/2304.14606)

    本论文提出了一种新的反事实解释方法（CEPIA），可以处理数据中的缺失值，使得用户可以获得有效的行动建议并了解缺失值对建议的影响。

    

    反事实解释是一种提供扰动以改变分类器预测结果的事后解释方法。现有方法需要完整信息的输入，但实际情况中往往会有缺失值。本文提出一种新的CE框架（称为CEPIA），使用户可以在有缺失值的情况下获得有效的操作，并阐明缺失值对操作的影响。

    Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an "action" to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation ca
    
[^37]: 强化学习中的极小极大最优无关奖励探索

    Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])

    [http://arxiv.org/abs/2304.07278](http://arxiv.org/abs/2304.07278)

    本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。

    

    本文研究了强化学习中的无关奖励探索，设计了一种算法来改进现有技术。研究了具有S个状态，A个动作和有限时间水平H的非平稳马尔科夫决策过程，并收集了一定数量的无引导奖励信息的样本集，在保证收集的数量满足多项式级别时，算法能够发现所有这些奖励函数的最小值，实现了可证明的极小极大最优探索方案。

    This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log f
    
[^38]: 用张量网络形式统一O(3)等变神经网络设计

    Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07482](http://arxiv.org/abs/2211.07482)

    本文提出使用融合图来设计等变神经网络的新组件，为解决涉及全局空间和置换对称性的学习任务提供了一种图形化的方法。

    

    许多学习任务，包括从从第一原理计算中学习势能面，涉及到全局空间对称性和原子或一般粒子之间的置换对称性。等变图神经网络是解决这类问题的标准方法之一，其中最成功的方法之一是使用在空间群下变换的各种张量之间的张量积。然而，随着不同张量的数量和它们之间关系的复杂性增加，保持简洁和等变性变得越来越具有挑战性。在本文中，我们提出使用融合图，一种广泛用于模拟SU(2)对称量子多体问题的技术，来为等变神经网络设计新的等变组件。这导致了一种基于图的方法来构建新的神经网络架构。当应用于给定局部邻域中的粒子时，我们称之为“融合块”的结果组件起到了

    Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
    
[^39]: GeONet：一种学习Wasserstein测地的神经算子

    GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14440](http://arxiv.org/abs/2209.14440)

    GeONet是一个不受网格影响的深度神经算子网络，学习了从初始和终端分布到连接两个端点分布的Wasserstein测地的非线性映射。通过学习鞍点优化条件，GeONet可以快速进行实时预测，并在仿真示例和测试数据上取得了与标准OT求解器相当的准确性。

    

    最优传输(OT)提供了一种将复杂数据分布进行几何上有意义比较的通用框架。传统的计算概率测度的Wasserstein距离和测地的方法需要网格依赖的域离散化，同时受到维度灾难的影响。我们提出了GeONet，一种不受网格影响的深度神经算子网络，它学习了将输入的初始和终端分布映射到连接两个端点分布的Wasserstein测地的非线性映射。在脱机训练阶段，GeONet通过耦合的PDE系统表征的原始和对偶空间中的动态最优条件学习了OT问题的鞍点优化条件。后续的推理阶段是瞬时完成的，并可以在在线学习设置中用于实时预测。我们证明了GeONet在仿真示例和...

    Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
    

