# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Generalizable Reinforcement Learning for Trade Execution.](http://arxiv.org/abs/2307.11685) | 本论文提出了一种面向通用化的交易执行的强化学习方法。研究表明，现有的强化学习方法存在过拟合问题，阻碍了实际应用。作者通过使用离线强化学习和动态上下文建模来解决过拟合问题，并提出了学习上下文的紧凑表示方法。 |
| [^2] | [General regularization in covariate shift adaptation.](http://arxiv.org/abs/2307.11503) | 本文研究了协变量偏移自适应中的一般正则化方法，并通过组合已有结果得到了新的结果。在弱平滑条件下证明了实现与标准监督学习中相同精度所需的样本量要比现有分析证明的少。 |
| [^3] | [Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2307.11494) | 本研究提出了一种面向概率时间序列预测的自引导扩散模型，称为TSDiff。该模型不需要辅助网络或训练过程的改变，在预测、改进和合成数据生成等时间序列任务上展现出了竞争力。 |
| [^4] | [Attention to Entropic Communication.](http://arxiv.org/abs/2307.11423) | 该论文研究了在通信理论中结合注意力和相对熵的概念。研究发现，在通信中使用注意力导向的加权相对熵是不适当的，而适当的注意力通信可通过发送者仅需要了解接收者的效用函数来实现最佳通知。 |
| [^5] | [What can a Single Attention Layer Learn? A Study Through the Random Features Lens.](http://arxiv.org/abs/2307.11353) | 本研究通过随机特征分析，对单个注意层的学习和泛化进行了严格的理论研究。结果表明，在具有随机采样的关键矩阵和可训练值矩阵的情况下，随机特征注意层可以表示一类与关键向量置换无关的目标函数，并提供了学习这些目标函数的风险界限。 |
| [^6] | [Model-based Offline Reinforcement Learning with Count-based Conservatism.](http://arxiv.org/abs/2307.11352) | 本文提出了一种基于模型的离线强化学习方法$\texttt{Count-MORL}$，通过利用计数保守性来量化模型估计误差，证明了基于计数保守性在离线深度强化学习中的有效性，并展示了学习到的策略提供了接近最优性能的保证。 |
| [^7] | [Bounded P-values in Parametric Programming-based Selective Inference.](http://arxiv.org/abs/2307.11351) | 本研究提出了一种降低参数规划选择性推断计算成本的方法，通过计算p值的上界和下界来保证所需精度。 |
| [^8] | [Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models.](http://arxiv.org/abs/2307.11332) | 本研究探讨了机器学习和深度学习模型中的参数可识别性。通过一个运动传感器数据的参数估计案例研究，发现虽然某些参数可以从观测数据中识别出来，但其他参数仍然不可识别。这表明不可识别性是实验设置的固有限制，需要改变数据收集方法。 |
| [^9] | [Kernelized Offline Contextual Dueling Bandits.](http://arxiv.org/abs/2307.11288) | 本论文提出了基于内核的离线背景双向竞标者算法，用于解决基于偏好反馈的问题，并证明了算法的遗憾界。通过实验证明该方法优于使用均匀采样上下文的相似策略。 |
| [^10] | [On the Fisher-Rao Gradient of the Evidence Lower Bound.](http://arxiv.org/abs/2307.11249) | 本文研究了证据下界的Fisher-Rao梯度，揭示了它与目标分布的Kullback-Leibler散度梯度的关系，进一步证明了最小化主要目标函数与最大化ELBO的等价性。 |
| [^11] | [From Adaptive Query Release to Machine Unlearning.](http://arxiv.org/abs/2307.11228) | 该论文将机器取消学习问题形式化为设计高效的取消学习算法，给出了线性和前缀和查询类的高效取消学习算法，以及应用于随机凸优化问题的改进保证。 |
| [^12] | [Dense Sample Deep Learning.](http://arxiv.org/abs/2307.10991) | 密集样本深度学习是一种针对深度学习网络的研究方法，旨在揭示学习机制和表示的未知特性，并解决大规模数据和隐藏单元存在的问题。 |
| [^13] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^14] | [Bayesian taut splines for estimating the number of modes.](http://arxiv.org/abs/2307.05825) | 本研究提出了一种贝叶斯紧系数样条方法，用于估计概率密度函数中模式的数量。该方法结合了核估计器和组合样条，实现了特征探索、模型选择和模式检验，并允许引入专家判断。通过在体育分析中的案例研究中的验证，证明了该方法的实用性。 |
| [^15] | [Is Your Model "MADD"? A Novel Metric to Evaluate Algorithmic Fairness for Predictive Student Models.](http://arxiv.org/abs/2305.15342) | 本文提出一种新的度量标准，即MADD，可以独立于预测性能分析模型的歧视行为。研究者还提供了可视化分析的补充来帮助进行人类评估。 |
| [^16] | [Exact recovery for the non-uniform Hypergraph Stochastic Block Model.](http://arxiv.org/abs/2304.13139) | 本文首次建立了非均匀超图随机块模型（HSBM）下的精确恢复的尖锐阈值，提供了两种有效算法，并依赖于非均匀随机超图的邻接矩阵的集中和正则化进行理论分析。 |
| [^17] | [Inexact iterative numerical linear algebra for neural network-based spectral estimation and rare-event prediction.](http://arxiv.org/abs/2303.12534) | 本文开发了一种不精确的迭代线性代数方法，用于基于神经网络的谱估计和从短轨迹数据集中进行稀有事件的预测，这对于理解复杂系统的动态是具有挑战性的，并讨论了该方法对强化学习中的预测问题的影响。 |
| [^18] | [Modeling Events and Interactions through Temporal Processes -- A Survey.](http://arxiv.org/abs/2303.06067) | 该调查研究通过时间过程建模事件序列的概率模型，并通过简单、标记和时空点过程分类，对基于深度学习的现有方法进行系统回顾，并分析了应用于预测和建模方面的场景。 |
| [^19] | [Tight Bounds for $\gamma$-Regret via the Decision-Estimation Coefficient.](http://arxiv.org/abs/2303.03327) | 本论文通过决策估计系数对任意结构化赌臂问题的$\gamma$-遗憾进行了紧密界定，该界定是对函数类的统计复杂度参数$\gamma$-DEC的修改版本。作者发现$\gamma$-DEC是任何模型类$\mathcal{F}$的基本限制，对于任何算法都存在某个$f \in \mathcal{F}$，该算法的$\gamma$-遗憾与$\mathcal{F}$的$\gamma$-DEC几乎成正比。 |
| [^20] | [On Provable Copyright Protection for Generative Models.](http://arxiv.org/abs/2302.10870) | 该论文研究了可证明版权保护生成模型的问题，提出了近无阻碍性（NAF）的定义，并给出了满足该定义的模型输出与受版权保护数据相似样本的概率上限。同时，还提供了生成模型学习算法，能够高效输出具有强大版权保护能力的生成模型。最后，进行了有前景的语言和图像生成模型实验。 |
| [^21] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^22] | [SpArX: Sparse Argumentative Explanations for Neural Networks.](http://arxiv.org/abs/2301.09559) | 该论文提出了一种稀疏的神经网络论证解释方法SpArX，通过利用多层感知器和定量论证框架之间的关系，可以为神经网络的决策过程提供更忠实和深入的解释。 |
| [^23] | [A Convergence Rate for Manifold Neural Networks.](http://arxiv.org/abs/2212.12606) | 该论文研究了流形神经网络的收敛速率，通过建立一个与环境维度无关但与流形内在维度相关的收敛速率，对先前的工作进行了改进。 |
| [^24] | [Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization.](http://arxiv.org/abs/2210.11589) | 本文研究了在分布转移写下，期望模型在两个分布上性能存在单调关系的条件，利用岭正则化通用线性模型证明了平方误差的精确渐近线性关系和误分类误差的单调关系，以及线性逆问题的近似线性关系。 |
| [^25] | [Adversarial Bayesian Simulation.](http://arxiv.org/abs/2208.12113) | 本文提出了对抗贝叶斯模拟的方法，通过将近似贝叶斯计算与生成对抗网络和对抗变分贝叶斯相结合，开发了一种直接针对后验概率的贝叶斯GAN（B-GAN）采样器，通过解决一个对抗优化问题。通过使用数据驱动建议和重要性重新加权以及变分贝叶斯等后处理技术，进一步提高了模拟结果的准确性。 |
| [^26] | [The activity-weight duality in feed forward neural networks: The geometric determinants of generalization.](http://arxiv.org/abs/2203.10736) | 这项研究发现了在前馈神经网络中，神经元活动的变化与连接到下一层神经元的权重变化之间的准确对偶关系。通过这种对偶性，我们能够将输入数据的变化映射到对应的权重变化，并发现泛化损失可以通过解的损失函数的Hessian矩阵的特征方向的几何因子的乘积来表示。 |
| [^27] | [Is Homophily a Necessity for Graph Neural Networks?.](http://arxiv.org/abs/2106.06134) | “同质性对于良好的图神经网络性能是否必要的”这一问题在研究中得到了重新评估。实验证明，标准的图卷积网络（GCNs）在一些常用的异质性图上可以实现比精心设计的方法更好的性能。 |

# 详细

[^1]: 面向通用化的交易执行的强化学习方法

    Towards Generalizable Reinforcement Learning for Trade Execution. (arXiv:2307.11685v1 [q-fin.TR])

    [http://arxiv.org/abs/2307.11685](http://arxiv.org/abs/2307.11685)

    本论文提出了一种面向通用化的交易执行的强化学习方法。研究表明，现有的强化学习方法存在过拟合问题，阻碍了实际应用。作者通过使用离线强化学习和动态上下文建模来解决过拟合问题，并提出了学习上下文的紧凑表示方法。

    

    优化的交易执行是在给定时间内以最低的交易成本卖出（或买入）给定资产的过程。最近，强化学习方法被应用于优化的交易执行，以从市场数据中学习更智能的策略。然而，我们发现许多现有的强化学习方法存在显著的过拟合问题，从而阻碍了它们的实际应用。在本文中，我们对优化的交易执行中的过拟合问题进行了广泛研究。首先，我们将优化的交易执行建模为带有动态上下文（ORDC）的离线强化学习问题，其中上下文表示不能受到交易策略影响并以离线方式收集的市场变量。在这个框架下，我们推导了泛化界限，并发现过拟合问题是由于离线环境中上下文空间巨大且上下文样本有限所导致的。因此，我们提出了学习上下文的紧凑表示来解决过拟合问题，可以通过...

    Optimized trade execution is to sell (or buy) a given amount of assets in a given time with the lowest possible trading cost. Recently, reinforcement learning (RL) has been applied to optimized trade execution to learn smarter policies from market data. However, we find that many existing RL methods exhibit considerable overfitting which prevents them from real deployment. In this paper, we provide an extensive study on the overfitting problem in optimized trade execution. First, we model the optimized trade execution as offline RL with dynamic context (ORDC), where the context represents market variables that cannot be influenced by the trading policy and are collected in an offline manner. Under this framework, we derive the generalization bound and find that the overfitting issue is caused by large context space and limited context samples in the offline setting. Accordingly, we propose to learn compact representations for context to address the overfitting problem, either by levera
    
[^2]: 在协变量偏移自适应中的一般正则化方法

    General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])

    [http://arxiv.org/abs/2307.11503](http://arxiv.org/abs/2307.11503)

    本文研究了协变量偏移自适应中的一般正则化方法，并通过组合已有结果得到了新的结果。在弱平滑条件下证明了实现与标准监督学习中相同精度所需的样本量要比现有分析证明的少。

    

    样本重加权是纠正在再生核希尔伯特空间(RKHS)中由未来数据分布与训练数据分布不同引起的最小二乘学习算法错误的最常用方法之一。在实际情况中，样本权重是由未来数据分布对训练数据分布的估计Radon-Nikod\'ym导数的值确定的。本研究回顾了在RKHS中重新加权核回归的已知误差界限，并通过组合得到新的结果。我们在弱平滑条件下表明，为了实现与标准监督学习中数据分布差异相同精度的样本数目要比现有的分析证明的少。

    Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
    
[^3]: 预测、改进、合成：面向概率时间序列预测的自引导扩散模型

    Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])

    [http://arxiv.org/abs/2307.11494](http://arxiv.org/abs/2307.11494)

    本研究提出了一种面向概率时间序列预测的自引导扩散模型，称为TSDiff。该模型不需要辅助网络或训练过程的改变，在预测、改进和合成数据生成等时间序列任务上展现出了竞争力。

    

    扩散模型在各个领域的生成建模任务中取得了最先进的性能。之前关于时间序列扩散模型的研究主要集中在开发针对特定预测或填补任务的条件模型。在这项工作中，我们探索了面向多种时间序列应用的任务不可知条件下的扩散模型的潜力。我们提出了TSDiff，一种面向时间序列的无条件训练的扩散模型。我们的自引导机制在推理过程中使得TSDiff能够为下游任务进行条件设置，而无需辅助网络或改变训练过程。我们在三个不同的时间序列任务上展示了我们方法的有效性：预测、改进和合成数据生成。首先，我们表明TSDiff与几种任务特定的条件预测方法相竞争（预测）。其次，我们利用TSDiff学到的隐性概率密度来迭代地改进p

    Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
    
[^4]: 注意力对熵通信的影响

    Attention to Entropic Communication. (arXiv:2307.11423v1 [cs.IT])

    [http://arxiv.org/abs/2307.11423](http://arxiv.org/abs/2307.11423)

    该论文研究了在通信理论中结合注意力和相对熵的概念。研究发现，在通信中使用注意力导向的加权相对熵是不适当的，而适当的注意力通信可通过发送者仅需要了解接收者的效用函数来实现最佳通知。

    

    注意力的概念是指在人工智能中强调特定数据重要性的数值权重，在通信理论中相对熵（RE，也称为库尔巴克-勒布勒散度）发挥着核心作用。在这里，我们结合了这些概念，即注意力和RE。RE引导带宽有限通信中的最佳编码以及通过最大熵原理（MEP）进行最佳消息解码。在编码场景中，RE可以从四个要求中推导出来，即分析性、局部性、适当性和校准性。而用于通信中注意力导向的加权RE实际上是不适当的。为了看到适当的注意力通信是如何出现的，我们分析了一个场景，即消息发送者希望确保接收者能够执行知情的操作。如果接收者使用MEP解码消息，则发送者只需要知道接收者的效用函数来进行最佳通知，不需要知道接收者的策略。

    The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receive
    
[^5]: 一个单一的注意层能学到什么？通过随机特征视角的研究。

    What can a Single Attention Layer Learn? A Study Through the Random Features Lens. (arXiv:2307.11353v1 [cs.LG])

    [http://arxiv.org/abs/2307.11353](http://arxiv.org/abs/2307.11353)

    本研究通过随机特征分析，对单个注意层的学习和泛化进行了严格的理论研究。结果表明，在具有随机采样的关键矩阵和可训练值矩阵的情况下，随机特征注意层可以表示一类与关键向量置换无关的目标函数，并提供了学习这些目标函数的风险界限。

    

    注意层是Transformer架构的核心组成部分，通过将输入序列映射到输出序列，在现代人工智能领域取得了重要突破。本文对单个多头注意层进行了严格的理论研究，输入是一系列关键向量和一个独立的查询向量。我们考虑了一个随机特征设置，其中注意层具有大量头部，具有随机采样的冻结查询和关键矩阵以及可训练的值矩阵。我们展示了这样一个随机特征的注意层可以表示一类与关键向量置换无关的目标函数。我们进一步提供了有限样本情况下学习这些目标函数的定量过量风险界限的方法，使用有限数量的头部和随机特征注意层。我们的结果相比现有的随机线性功能模型有几个注意结构上的独特影响。

    Attention layers -- which map a sequence of inputs to a sequence of outputs -- are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.  Our results feature several implications unique to the attention structure compared with existing ra
    
[^6]: 基于模型的离线强化学习与基于计数保守性的结合方法

    Model-based Offline Reinforcement Learning with Count-based Conservatism. (arXiv:2307.11352v1 [cs.LG])

    [http://arxiv.org/abs/2307.11352](http://arxiv.org/abs/2307.11352)

    本文提出了一种基于模型的离线强化学习方法$\texttt{Count-MORL}$，通过利用计数保守性来量化模型估计误差，证明了基于计数保守性在离线深度强化学习中的有效性，并展示了学习到的策略提供了接近最优性能的保证。

    

    本文提出了一种名为$\texttt{Count-MORL}$的基于模型的离线强化学习方法，它利用了状态动作对的计数估计来量化模型估计误差，据我们所知，这是首个证明了基于计数保守性在基于模型的离线深度强化学习中的有效性的算法。我们首先展示了估计误差与状态动作对的频率成反比的关系。其次，我们证明了在基于计数保守模型下学习的策略提供了接近最优性能的保证。通过大量的数值实验，我们验证了使用哈希编码实现的$\texttt{Count-MORL}$在D4RL基准数据集上显著优于现有离线强化学习算法。代码可在$\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$上进行访问。

    In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.
    
[^7]: 参数规划的选择性推断中的有界P值

    Bounded P-values in Parametric Programming-based Selective Inference. (arXiv:2307.11351v1 [stat.ML])

    [http://arxiv.org/abs/2307.11351](http://arxiv.org/abs/2307.11351)

    本研究提出了一种降低参数规划选择性推断计算成本的方法，通过计算p值的上界和下界来保证所需精度。

    

    选择性推断（SI）作为一种适用于数据驱动的假设检验的有前景的框架，一直受到研究关注。SI的基本思想是在一个假设被选中的事件的条件下进行推断。为了进行SI，必须以可追踪的形式对这个事件进行描述。当选择事件难以描述时，可以引入额外的条件以使其可处理。这些额外的条件往往会导致功效的损失，这一问题被称为过度条件化。基于参数规划的SI（PP-based SI）被提出作为解决过度条件化问题的一种方法。PP-based SI的主要问题是由于需要完全地探索数据空间而导致计算成本高。本研究引入了一种降低计算成本的过程，同时保证所需精度，通过提出计算p值的上界和下界的方法。我们还提出了三种类型的搜索策略。

    Selective inference (SI) has been actively studied as a promising framework for statistical hypothesis testing for data-driven hypotheses. The basic idea of SI is to make inferences conditional on an event that a hypothesis is selected. In order to perform SI, this event must be characterized in a traceable form. When selection event is too difficult to characterize, additional conditions are introduced for tractability. This additional conditions often causes the loss of power, and this issue is referred to as over-conditioning. Parametric programming-based SI (PP-based SI) has been proposed as one way to address the over-conditioning issue. The main problem of PP-based SI is its high computational cost due to the need to exhaustively explore the data space. In this study, we introduce a procedure to reduce the computational cost while guaranteeing the desired precision, by proposing a method to compute the upper and lower bounds of p-values. We also proposed three types of search str
    
[^8]: 超越收敛性：机器学习和深度学习模型的可识别性

    Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models. (arXiv:2307.11332v1 [cs.LG])

    [http://arxiv.org/abs/2307.11332](http://arxiv.org/abs/2307.11332)

    本研究探讨了机器学习和深度学习模型中的参数可识别性。通过一个运动传感器数据的参数估计案例研究，发现虽然某些参数可以从观测数据中识别出来，但其他参数仍然不可识别。这表明不可识别性是实验设置的固有限制，需要改变数据收集方法。

    

    机器学习和深度学习模型被广泛应用于参数优化和回归问题。然而，并非所有机器学习的逆问题都是“可识别的”，这意味着模型参数可能无法从可用数据和数据模型的输入-输出关系中唯一确定。本研究通过一个以运动传感器数据的参数估计为重点的案例研究，探讨了模型参数可识别性的概念。利用双足弹簧质点人类行走动力学模型，我们生成了表示不同步态模式和条件的合成数据。通过使用深度神经网络，我们尝试估计个体参数，包括质量、刚度和平衡腿长。结果表明，虽然某些参数可以从观测数据中识别出来，但其他参数仍然不可识别，这凸显了不可识别性是实验设置的固有限制，需要改变数据收集方法。

    Machine learning (ML) and deep learning models are extensively used for parameter optimization and regression problems. However, not all inverse problems in ML are ``identifiable,'' indicating that model parameters may not be uniquely determined from the available data and the data model's input-output relationship. In this study, we investigate the notion of model parameter identifiability through a case study focused on parameter estimation from motion sensor data. Utilizing a bipedal-spring mass human walk dynamics model, we generate synthetic data representing diverse gait patterns and conditions. Employing a deep neural network, we attempt to estimate subject-wise parameters, including mass, stiffness, and equilibrium leg length. The results show that while certain parameters can be identified from the observation data, others remain unidentifiable, highlighting that unidentifiability is an intrinsic limitation of the experimental setup, necessitating a change in data collection a
    
[^9]: 基于内核的离线背景双向竞标者算法

    Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])

    [http://arxiv.org/abs/2307.11288](http://arxiv.org/abs/2307.11288)

    本论文提出了基于内核的离线背景双向竞标者算法，用于解决基于偏好反馈的问题，并证明了算法的遗憾界。通过实验证明该方法优于使用均匀采样上下文的相似策略。

    

    基于偏好的反馈在许多应用中非常重要，这些应用中无法直接评估奖励函数。在人们对大型语言模型的强化学习中，这是一个显著的最新实例。对于许多这些应用，获取人类反馈的成本可能相当高甚至不可行。在这项工作中，我们利用一个事实，即代理通常可以选择获得人类反馈的上下文，以最高效地确定一个良好策略，并引入了离线背景双向竞标者设置。我们为这个设置提供了一个上界置信区间样式的算法，并证明了一个遗憾上界。我们还通过经验证实这种方法胜过使用均匀采样上下文的类似策略。

    Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
    
[^10]: 关于证据下界的Fisher-Rao梯度研究

    On the Fisher-Rao Gradient of the Evidence Lower Bound. (arXiv:2307.11249v1 [cs.LG])

    [http://arxiv.org/abs/2307.11249](http://arxiv.org/abs/2307.11249)

    本文研究了证据下界的Fisher-Rao梯度，揭示了它与目标分布的Kullback-Leibler散度梯度的关系，进一步证明了最小化主要目标函数与最大化ELBO的等价性。

    

    本文研究了证据下界（ELBO）的Fisher-Rao梯度，也称为自然梯度，它在变分自动编码器理论、Helmholtz机和自由能原理中起着关键作用。ELBO的自然梯度与目标分布的Kullback-Leibler散度的自然梯度相关，后者是学习的主要目标函数。基于信息几何中梯度的不变性特性，提供了关于底层模型的条件，确保最小化主要目标函数与最大化ELBO的等价性。

    This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound, the ELBO, which plays a crucial role within the theory of the Variational Autonecoder, the Helmholtz Machine and the Free Energy Principle. The natural gradient of the ELBO is related to the natural gradient of the Kullback-Leibler divergence from a target distribution, the prime objective function of learning. Based on invariance properties of gradients within information geometry, conditions on the underlying model are provided that ensure the equivalence of minimising the prime objective function and the maximisation of the ELBO.
    
[^11]: 从自适应查询释放到机器取消学习

    From Adaptive Query Release to Machine Unlearning. (arXiv:2307.11228v1 [cs.LG])

    [http://arxiv.org/abs/2307.11228](http://arxiv.org/abs/2307.11228)

    该论文将机器取消学习问题形式化为设计高效的取消学习算法，给出了线性和前缀和查询类的高效取消学习算法，以及应用于随机凸优化问题的改进保证。

    

    我们将机器取消学习问题形式化为设计高效取消学习算法来对应从结构化查询类中选择自适应查询的学习算法。我们给出了线性和前缀和查询类的高效取消学习算法。作为应用，我们展示了在许多问题中，特别是随机凸优化（SCO）中的取消学习可以通过上述方法来减少，从而改善问题的保证。特别地，对于平滑的Lipschitz损失和任意的$\rho>0$，我们的结果给出了一个取消学习算法，其超出总体风险为$\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$，取消学习查询（梯度）复杂性为$\tilde O(\rho \cdot \text{重新训练复杂性})$，其中$d$是模型的维度，$n$是初始样本数。对于非平滑的Lipschitz损失，我们给出了一个取消学习算法，其超出总体风险为$\tilde O\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}$

    We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\rho>0$, our results yield an unlearning algorithm with excess population risk of $\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$ with unlearning query (gradient) complexity $\tilde O(\rho \cdot \text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\tilde O\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}
    
[^12]: 密集样本深度学习

    Dense Sample Deep Learning. (arXiv:2307.10991v1 [cs.AI])

    [http://arxiv.org/abs/2307.10991](http://arxiv.org/abs/2307.10991)

    密集样本深度学习是一种针对深度学习网络的研究方法，旨在揭示学习机制和表示的未知特性，并解决大规模数据和隐藏单元存在的问题。

    

    深度学习（DL）是20世纪80年代提出的一种神经网络算法的变体，在人工智能（AI）领域取得了令人惊讶的进展，包括语言翻译、蛋白质折叠、自动驾驶汽车，以及最近的类人语言模型（CHATbots）。尽管深度学习（DL）网络的使用越来越广泛，但对于使这些网络在如此广泛的应用中有效的学习机制和表示仍知之甚少。部分原因可能是其大规模架构和大规模数据的使用，但深度学习表示的本质仍然大部分未知。不幸的是，具有数百万或数十亿个标记的训练集存在未知的组合方式，同时数百万或数十亿个隐藏单元的网络难以可视化，其机制也难以揭示。在本文中，我们提出了一种密集样本深度学习的方法。

    Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently. Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications. Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987. But the nature of deep learned representations remain largely unknown. Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed. In this pap
    
[^13]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^14]: 贝叶斯紧系数样条估计模式的数量

    Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])

    [http://arxiv.org/abs/2307.05825](http://arxiv.org/abs/2307.05825)

    本研究提出了一种贝叶斯紧系数样条方法，用于估计概率密度函数中模式的数量。该方法结合了核估计器和组合样条，实现了特征探索、模型选择和模式检验，并允许引入专家判断。通过在体育分析中的案例研究中的验证，证明了该方法的实用性。

    

    概率密度函数中模式的数量代表模型的复杂性，也可以看作现有亚群体的数量。尽管其相关性，对其估计的研究非常有限。我们针对单变量情况提出一个新颖的方法，致力于预测准确性，受到了问题的一些被忽视的方面的启发。我们认为解决方案需要结构，模式的主观且不确定性，以及融合全局和局部密度特性的整体视图的便利性。我们的方法结合了灵活的核估计器和简洁的组合样条。特征探索、模型选择和模式检验都在贝叶斯推理范式中实现，为软解决方案提供了便利，并允许在过程中引入专家判断。我们的提议的实用性通过在体育分析中的案例研究中进行了验证，并展示了多个陪伴的可视化。

    The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
    
[^15]: 你的模型“MADD”了吗？一种用于评估预测性学生模型算法公平性的新指标。

    Is Your Model "MADD"? A Novel Metric to Evaluate Algorithmic Fairness for Predictive Student Models. (arXiv:2305.15342v1 [cs.LG])

    [http://arxiv.org/abs/2305.15342](http://arxiv.org/abs/2305.15342)

    本文提出一种新的度量标准，即MADD，可以独立于预测性能分析模型的歧视行为。研究者还提供了可视化分析的补充来帮助进行人类评估。

    

    由于其增强教育成果和支持利益相关者做出明智决策的能力，预测性学生模型在学习环境中越来越普遍。然而，预测模型可能存在偏见，导致对某些学生的潜在歧视和可能的有害长期影响。这促使了对公平性度量标准的研究，旨在捕捉和量化此类偏见。尽管如此，目前在教育领域使用的现有公平度量标准是面向预测性能的，重点是评估组间存在的有偏结果，而不考虑模型的行为以及结果中的偏见程度。因此，我们提出了一种新的度量标准，即“模型绝对密度距离”（MADD），以分析模型的歧视行为，独立于其预测性能。我们还提供了基于可视化分析的补充，以实现对模型行为的细粒度人类评估。

    Predictive student models are increasingly used in learning environments due to their ability to enhance educational outcomes and support stakeholders in making informed decisions. However, predictive models can be biased and produce unfair outcomes, leading to potential discrimination against some students and possible harmful long-term implications. This has prompted research on fairness metrics meant to capture and quantify such biases. Nonetheless, so far, existing fairness metrics used in education are predictive performance-oriented, focusing on assessing biased outcomes across groups of students, without considering the behaviors of the models nor the severity of the biases in the outcomes. Therefore, we propose a novel metric, the Model Absolute Density Distance (MADD), to analyze models' discriminatory behaviors independently from their predictive performance. We also provide a complementary visualization-based analysis to enable fine-grained human assessment of how the models
    
[^16]: 非均匀超图随机块模型的精确恢复

    Exact recovery for the non-uniform Hypergraph Stochastic Block Model. (arXiv:2304.13139v1 [math.ST])

    [http://arxiv.org/abs/2304.13139](http://arxiv.org/abs/2304.13139)

    本文首次建立了非均匀超图随机块模型（HSBM）下的精确恢复的尖锐阈值，提供了两种有效算法，并依赖于非均匀随机超图的邻接矩阵的集中和正则化进行理论分析。

    

    考虑在非均匀超图随机块模型（HSBM）下的随机超图中的社区检测问题，其中每个超边独立地以某些给定概率出现，该概率仅取决于其顶点的标签。我们在本文中首次建立了在这种非均匀情况下实现精确恢复的尖锐阈值，受到次要约束；尤其是，我们考虑了具有K类别的模型和对称二进制模型（K=2）。关键点是通过聚合所有均匀层的信息，即使在考虑每个层时似乎不可能实现精确恢复，我们也可以获得精确恢复。我们提供了两种有效算法，成功地在阈值以上实现了精确恢复。我们算法的理论分析依赖于非均匀随机超图的邻接矩阵的集中和正则化，这可能具有独立的兴趣。我们还解决了一些实际问题

    Consider the community detection problem in random hypergraphs under the non-uniform hypergraph stochastic block model (HSBM), where each hyperedge appears independently with some given probability depending only on the labels of its vertices. We establish, for the first time in the literature, a sharp threshold for exact recovery under this non-uniform case, subject to minor constraints; in particular, we consider the model with $K$ classes as well as the symmetric binary model ($K=2$). One crucial point here is that by aggregating information from all the uniform layers, we may obtain exact recovery even in cases when this may appear impossible if each layer were considered alone. Two efficient algorithms that successfully achieve exact recovery above the threshold are provided. The theoretical analysis of our algorithms relies on the concentration and regularization of the adjacency matrix for non-uniform random hypergraphs, which could be of independent interest. We also address so
    
[^17]: 不精确的迭代数值线性代数用于基于神经网络的谱估计和稀有事件预测

    Inexact iterative numerical linear algebra for neural network-based spectral estimation and rare-event prediction. (arXiv:2303.12534v1 [physics.comp-ph])

    [http://arxiv.org/abs/2303.12534](http://arxiv.org/abs/2303.12534)

    本文开发了一种不精确的迭代线性代数方法，用于基于神经网络的谱估计和从短轨迹数据集中进行稀有事件的预测，这对于理解复杂系统的动态是具有挑战性的，并讨论了该方法对强化学习中的预测问题的影响。

    

    由于复杂系统存在大量自由度，其中最重要的度量通常并不明显，因此理解其动态是具有挑战性的。转移算符的主要特征函数对于可视化很有用，它们可以为计算统计量（例如事件的可能性和平均时间）提供高效的基础（预测）。在本文中，我们开发了不精确的迭代线性代数方法来计算这些特征函数（谱估计）并从短轨迹数据集上进行预测。我们在便于可视化的低维模型和生物分子系统的高维模型上演示了该方法。我们还讨论了这些方法对强化学习中的预测问题的影响。

    Understanding dynamics in complex systems is challenging because there are many degrees of freedom, and those that are most important for describing events of interest are often not obvious. The leading eigenfunctions of the transition operator are useful for visualization, and they can provide an efficient basis for computing statistics such as the likelihood and average time of events (predictions). Here we develop inexact iterative linear algebra methods for computing these eigenfunctions (spectral estimation) and making predictions from a data set of short trajectories sampled at finite intervals. We demonstrate the methods on a low-dimensional model that facilitates visualization and a high-dimensional model of a biomolecular system. Implications for the prediction problem in reinforcement learning are discussed.
    
[^18]: 通过时间过程建模事件和相互作用 - 一项调查

    Modeling Events and Interactions through Temporal Processes -- A Survey. (arXiv:2303.06067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06067](http://arxiv.org/abs/2303.06067)

    该调查研究通过时间过程建模事件序列的概率模型，并通过简单、标记和时空点过程分类，对基于深度学习的现有方法进行系统回顾，并分析了应用于预测和建模方面的场景。

    

    在现实世界的场景中，许多现象以连续时间发生的一系列事件产生。点过程为建模这些事件序列提供了一种自然的数学框架。在本调查中，我们通过时间过程研究概率模型来建模事件序列。我们修订了事件建模的概念，并提供了表征相关文献的数学基础。我们定义了一个本体来以三个类别（简单、标记和时空点过程）对现有方法进行分类。对于每个类别，我们系统地回顾了基于深度学习的现有方法。最后，我们分析了提出的技术可以用于解决预测和建模方面的场景。

    In real-world scenario, many phenomena produce a collection of events that occur in continuous time. Point Processes provide a natural mathematical framework for modeling these sequences of events. In this survey, we investigate probabilistic models for modeling event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that characterize the literature on the topic. We define an ontology to categorize the existing approaches in terms of three families: simple, marked, and spatio-temporal point processes. For each family, we systematically review the existing approaches based based on deep learning. Finally, we analyze the scenarios where the proposed techniques can be used for addressing prediction and modeling aspects.
    
[^19]: 通过决策估计系数，对$\gamma$-遗憾进行紧密界定

    Tight Bounds for $\gamma$-Regret via the Decision-Estimation Coefficient. (arXiv:2303.03327v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03327](http://arxiv.org/abs/2303.03327)

    本论文通过决策估计系数对任意结构化赌臂问题的$\gamma$-遗憾进行了紧密界定，该界定是对函数类的统计复杂度参数$\gamma$-DEC的修改版本。作者发现$\gamma$-DEC是任何模型类$\mathcal{F}$的基本限制，对于任何算法都存在某个$f \in \mathcal{F}$，该算法的$\gamma$-遗憾与$\mathcal{F}$的$\gamma$-DEC几乎成正比。

    

    在这项工作中，我们给出了对任意结构化赌臂问题的$\gamma$-遗憾的统计描述，该遗憾是与$\gamma$倍最优解相比较时产生的遗憾。在函数类$\mathcal{F}$上的结构化赌臂问题中，寻找$f \in \mathcal{F}$的精确最优解是难以处理的。我们的描述是基于$\gamma$-DEC的，它是函数类$\mathcal{F}$的统计复杂度参数，是Foster et al.，2023的约束决策估计系数(DEC)的修改版本（与Foster et al.，2021的原始偏移DEC密切相关）。我们的下界表明，对于任何模型类$\mathcal{F}$，$\gamma$-DEC是一个基本限制：对于任何算法，存在一些$f \in \mathcal{F}$，该算法的$\gamma$-遗憾与$\mathcal{F}$的$\gamma$-DEC的规模（几乎）成正比。我们还提供了一个上界，证明了存在一些......

    In this work, we give a statistical characterization of the $\gamma$-regret for arbitrary structured bandit problems, the regret which arises when comparing against a benchmark that is $\gamma$ times the optimal solution. The $\gamma$-regret emerges in structured bandit problems over a function class $\mathcal{F}$ where finding an exact optimum of $f \in \mathcal{F}$ is intractable. Our characterization is given in terms of the $\gamma$-DEC, a statistical complexity parameter for the class $\mathcal{F}$, which is a modification of the constrained Decision-Estimation Coefficient (DEC) of Foster et al., 2023 (and closely related to the original offset DEC of Foster et al., 2021). Our lower bound shows that the $\gamma$-DEC is a fundamental limit for any model class $\mathcal{F}$: for any algorithm, there exists some $f \in \mathcal{F}$ for which the $\gamma$-regret of that algorithm scales (nearly) with the $\gamma$-DEC of $\mathcal{F}$. We provide an upper bound showing that there exist
    
[^20]: 关于可证明版权保护生成模型的研究

    On Provable Copyright Protection for Generative Models. (arXiv:2302.10870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10870](http://arxiv.org/abs/2302.10870)

    该论文研究了可证明版权保护生成模型的问题，提出了近无阻碍性（NAF）的定义，并给出了满足该定义的模型输出与受版权保护数据相似样本的概率上限。同时，还提供了生成模型学习算法，能够高效输出具有强大版权保护能力的生成模型。最后，进行了有前景的语言和图像生成模型实验。

    

    担心学习的条件生成模型可能输出与其训练集中的某些受版权保护的数据$C$极为相似的样本。我们给出了“近无阻碍性（NAF）”的正式定义，并证明了满足这个定义的模型输出与$C$相似样本的概率上限，即使$C$包含在其训练集中。粗略地说，生成模型$p$是“$k$-NAF”的，如果对于每一个潜在的受版权保护的数据$C$，$p$的输出与一个完全未访问$C$的模型$q$的输出之间的差别最大为$k$比特。我们还提供了生成模型学习算法，以黑盒方式高效修改原始生成模型学习算法，输出具有强大的保护内容采样概率上限的生成模型。此外，我们还针对语言（Transformer）和图像（Diffusion）生成模型进行了有前景的实验。

    There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models
    
[^21]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^22]: SpArX: 稀疏的神经网络论证解释

    SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09559](http://arxiv.org/abs/2301.09559)

    该论文提出了一种稀疏的神经网络论证解释方法SpArX，通过利用多层感知器和定量论证框架之间的关系，可以为神经网络的决策过程提供更忠实和深入的解释。

    

    神经网络在人工智能中有各种应用，但解释它们的决策仍然具有挑战性。现有方法通常关注解释改变单个输入如何影响神经网络的输出。然而，一个与神经网络的输入输出行为一致的解释未必忠实于其实际机制。在本文中，我们利用多层感知器和定量论证框架之间的关系，为多层感知器的机制创建了论证性解释。我们的SpArX方法首先将多层感知器稀疏化，同时保持尽可能多的原始结构。然后将稀疏的多层感知器转化为等效的定量论证框架，以揭示多层感知器的潜在决策过程，产生全局和/或局部解释。我们通过实验证明，SpArX比现有方法可以给出更忠实的解释，同时提供更深入的洞察实际推理过程。

    Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
    
[^23]: 流形神经网络的收敛速率

    A Convergence Rate for Manifold Neural Networks. (arXiv:2212.12606v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12606](http://arxiv.org/abs/2212.12606)

    该论文研究了流形神经网络的收敛速率，通过建立一个与环境维度无关但与流形内在维度相关的收敛速率，对先前的工作进行了改进。

    

    高维数据在许多应用中产生，并且几何深度学习这一快速发展的领域致力于开发神经网络架构，以便在非欧几里德领域（如图和流形）中分析此类数据。弗丁·王、唐坤·卢以及亚历山大·里贝罗最近的工作引入了一种利用拉普拉斯-贝尔特拉米算子的谱分解构建流形神经网络的方法。此外，本文作者提供了一种数值方案，以在流形未知且仅有有限数量样本点可获得的情况下实施此类神经网络。作者证明了这种方案，依靠构建数据驱动的图，当样本点数量趋于无穷大时，收敛到连续极限。在这里，我们在此结果的基础上建立了一个收敛速率，该收敛速率取决于流形的内在维度，但与环境维度无关。我们还讨论了收敛速率如何依赖于...

    High-dimensional data arises in numerous applications, and the rapidly developing field of geometric deep learning seeks to develop neural network architectures to analyze such data in non-Euclidean domains, such as graphs and manifolds. Recent work by Z. Wang, L. Ruiz, and A. Ribeiro has introduced a method for constructing manifold neural networks using the spectral decomposition of the Laplace Beltrami operator. Moreover, in this work, the authors provide a numerical scheme for implementing such neural networks when the manifold is unknown and one only has access to finitely many sample points. The authors show that this scheme, which relies upon building a data-driven graph, converges to the continuum limit as the number of sample points tends to infinity. Here, we build upon this result by establishing a rate of convergence that depends on the intrinsic dimension of the manifold but is independent of the ambient dimension. We also discuss how the rate of convergence depends on the
    
[^24]: 在分布偏移下正则化风险最小化的单调风险关系

    Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization. (arXiv:2210.11589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11589](http://arxiv.org/abs/2210.11589)

    本文研究了在分布转移写下，期望模型在两个分布上性能存在单调关系的条件，利用岭正则化通用线性模型证明了平方误差的精确渐近线性关系和误分类误差的单调关系，以及线性逆问题的近似线性关系。

    

    机器学习系统通常应用于与训练分布不同的数据。最近的研究表明，在各种分类和信号重建问题中，超出分布的性能与内部分布的性能强烈线性相关。如果存在这种关系或更一般的单调关系，将产生重要的影响。例如，它允许将一个分布上的性能优化作为另一个分布上性能的代理。在本文中，我们研究了在两个分布上模型性能之间预期存在单调关系的条件。我们在协变量转移下，证明了岭正则化通用线性模型的平方误差的精确渐近线性关系和误分类误差的单调关系，以及线性逆问题的近似线性关系。

    Machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. Recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. If this relationship or more generally a monotonic one holds, it has important consequences. For example, it allows to optimize performance on one distribution as a proxy for performance on the other. In this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. We prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems.
    
[^25]: 对抗贝叶斯模拟

    Adversarial Bayesian Simulation. (arXiv:2208.12113v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.12113](http://arxiv.org/abs/2208.12113)

    本文提出了对抗贝叶斯模拟的方法，通过将近似贝叶斯计算与生成对抗网络和对抗变分贝叶斯相结合，开发了一种直接针对后验概率的贝叶斯GAN（B-GAN）采样器，通过解决一个对抗优化问题。通过使用数据驱动建议和重要性重新加权以及变分贝叶斯等后处理技术，进一步提高了模拟结果的准确性。

    

    在没有明确或可计算的似然函数的情况下，贝叶斯推断常常使用近似贝叶斯计算（ABC）。我们的工作将ABC与基于生成对抗网络（GANs）和对抗变分贝叶斯的深度神经隐式采样器相结合。ABC和GANs分别比较观测数据和虚假数据的各个方面以模拟后验概率和似然函数。我们开发了一种贝叶斯GAN（B-GAN）采样器，通过解决一个对抗优化问题直接对准目标后验概率。B-GAN由在ABC参考上通过条件GANs学习的确定性映射驱动。一旦映射被训练好，通过滤除噪声以几乎没有额外代价的方式获得独立同分布的后验样本。我们提出了两个后处理局部改进方法，分别使用了（1）数据驱动的建议和重要性重新加权，和（2）变分贝叶斯。我们用频率学-贝叶斯的结果支持我们的发现，显示真实值与一个常见的总变差距离之间的差异

    In the absence of explicit or tractable likelihoods, Bayesians often resort to approximate Bayesian computation (ABC) for inference. Our work bridges ABC with deep neural implicit samplers based on generative adversarial networks (GANs) and adversarial variational Bayes. Both ABC and GANs compare aspects of observed and fake data to simulate from posteriors and likelihoods, respectively. We develop a Bayesian GAN (B-GAN) sampler that directly targets the posterior by solving an adversarial optimization problem. B-GAN is driven by a deterministic mapping learned on the ABC reference by conditional GANs. Once the mapping has been trained, iid posterior samples are obtained by filtering noise at a negligible additional cost. We propose two post-processing local refinements using (1) data-driven proposals with importance reweighting, and (2) variational Bayes. We support our findings with frequentist-Bayesian results, showing that the typical total variation distance between the true and a
    
[^26]: 前馈神经网络中的活动-权重对偶性：泛化性的几何决定因素

    The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.10736](http://arxiv.org/abs/2203.10736)

    这项研究发现了在前馈神经网络中，神经元活动的变化与连接到下一层神经元的权重变化之间的准确对偶关系。通过这种对偶性，我们能够将输入数据的变化映射到对应的权重变化，并发现泛化损失可以通过解的损失函数的Hessian矩阵的特征方向的几何因子的乘积来表示。

    

    机器学习中一个基本的问题是泛化性。在具有大量权重（参数）的神经网络模型中，可以找到很多解来很好地拟合训练数据。关键问题是哪个解能够描述不在训练集中的测试数据。在这里，我们报告了在任何前馈神经网络的密集连接层中，给定层神经元活动的变化与连接到下一层神经元的权重变化之间的确切对偶（等价）关系的发现。活动-权重（A-W）对偶性使我们能够将输入（数据）的变化映射到相应的对偶权重的变化。通过使用这种映射，我们表明泛化损失可以分解为在权重空间中的解的损失函数的Hessian矩阵的不同特征方向的贡献之和。给定特征方向的贡献是两个几何因子（行列式）的乘积：尖锐度

    One of the fundamental problems in machine learning is generalization. In neural network models with a large number of weights (parameters), many solutions can be found to fit the training data equally well. The key question is which solution can describe testing data not in the training set. Here, we report the discovery of an exact duality (equivalence) between changes in activities in a given layer of neurons and changes in weights that connect to the next layer of neurons in a densely connected layer in any feed forward neural network. The activity-weight (A-W) duality allows us to map variations in inputs (data) to variations of the corresponding dual weights. By using this mapping, we show that the generalization loss can be decomposed into a sum of contributions from different eigen-directions of the Hessian matrix of the loss function at the solution in weight space. The contribution from a given eigen-direction is the product of two geometric factors (determinants): the sharpn
    
[^27]: “同质性对于图神经网络是必要的吗？”

    Is Homophily a Necessity for Graph Neural Networks?. (arXiv:2106.06134v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06134](http://arxiv.org/abs/2106.06134)

    “同质性对于良好的图神经网络性能是否必要的”这一问题在研究中得到了重新评估。实验证明，标准的图卷积网络（GCNs）在一些常用的异质性图上可以实现比精心设计的方法更好的性能。

    

    图神经网络（GNN）在学习适用于众多基于图的机器学习任务的表示方面显示出极强的能力。当应用于半监督节点分类时，由于同质性假设（“类似相互吸引”），人们普遍认为GNN可以很好地工作，但在异质性图中（连接不相似节点的图）无法泛化。最近的研究通过设计新的架构来克服这种与异质性相关的限制，引用了贫弱的基准性能以及在一些异质性图基准数据集上的新架构改进作为证据。在我们的实验证明，标准的图卷积网络（GCN）实际上在一些常用的异质性图上可以实现比这些精心设计的方法更好的性能。这激励我们重新思考同质性是否真正对于良好的GNN性能是必要的。我们发现这种说法并不完全正确，事实上，GCN可以在异质性图上实现强大的性能。

    Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption ("like attracts like"), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on h
    

