# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms.](http://arxiv.org/abs/2306.12383) | 本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。 |
| [^2] | [An efficient, provably exact algorithm for the 0-1 loss linear classification problem.](http://arxiv.org/abs/2306.12344) | 该研究详细介绍了一种名为增量单元枚举（ICE）的算法，该算法可以精确解决定维度0-1损失线性分类问题。 |
| [^3] | [Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training.](http://arxiv.org/abs/2306.12230) | 该论文对动态稀疏训练中的剪枝位置进行了实证分析，发现在低密度范围内，最简单的大小方法提供了最佳性能。 |
| [^4] | [More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity.](http://arxiv.org/abs/2306.12214) | 本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。 |
| [^5] | [Explaining human body responses in random vibration: Effect of motion direction, sitting posture, and anthropometry.](http://arxiv.org/abs/2306.12115) | 本文研究了人体形态、生物性别和姿势对于平移振动中人体动力学响应的影响，并创建了多个线性回归模型以确定最具影响的预测因子。 |
| [^6] | [Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions.](http://arxiv.org/abs/2306.12067) | 本文提出了一种新型随机双层优化的最优算法，避免了使用高阶光滑性假设，能够更好地适应非凸设置。 |
| [^7] | [Sampling Individually-Fair Rankings that are Always Group Fair.](http://arxiv.org/abs/2306.11964) | 该论文提出了一种有效算法，从个体公平分布中采样排名，同时确保每个输出的排名都满足群体公平性约束。输出排名的期望效用至少是最优公平解的效用的$\alpha$倍，其中$\alpha$是一个量化公平约束紧度的参数。 |
| [^8] | [Open Problem: Learning with Variational Objectives on Measures.](http://arxiv.org/abs/2306.11928) | 本文探讨了在测度上编写变分目标的动机，提出通过此类目标推导实用算法，以解决超出分布的泛化和弱监督学习等问题的开放问题。 |
| [^9] | [Structure-Aware Robustness Certificates for Graph Classification.](http://arxiv.org/abs/2306.11915) | 该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。 |
| [^10] | [Accelerating Generalized Random Forests with Fixed-Point Trees.](http://arxiv.org/abs/2306.11908) | 本文提出一种新的树生长规则，使广义随机森林在无梯度优化的情况下大大节省了时间。 |
| [^11] | [Learning Costs for Structured Monge Displacements.](http://arxiv.org/abs/2306.11895) | 本文提出了一种新的方法，即学习合适的成本结构来鼓励映射沿着特定的工程特征来传送点，其在扩展 Monge-Bregman-Occam 管道方面做出了重大贡献，并在匹配直方图方面表现出了优异的性能。 |
| [^12] | [Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations.](http://arxiv.org/abs/2306.11839) | 本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。 |
| [^13] | [Topological Parallax: A Geometric Specification for Deep Perception Models.](http://arxiv.org/abs/2306.11835) | 拓扑视差是一种比较训练模型和参考数据集多尺度几何结构相似性的理论和计算工具，它可以估计模型中的拓扑特征，有助于理解深度学习模型的行为和性能。 |
| [^14] | [Any Deep ReLU Network is Shallow.](http://arxiv.org/abs/2306.11827) | 该论文证明了任何深度的ReLU网络都可以被重写为一个具有透明性的浅层网络。这一结论有助于解释模型行为。 |
| [^15] | [Time-Varying Transition Matrices with Multi-task Gaussian Processes.](http://arxiv.org/abs/2306.11772) | 本文介绍了一种基于核的多任务高斯过程模型，用于逼近个体的移动状态的潜在函数，并考虑了转移概率之间的相关性和时间可变性，通过强制执行随机性和非负性约束来实现研究目标。 |
| [^16] | [Samplet basis pursuit.](http://arxiv.org/abs/2306.10180) | 本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。 |
| [^17] | [Leveraging Evolutionary Changes for Software Process Quality.](http://arxiv.org/abs/2305.18061) | 本文提出了一种利用演进变化来改善软件开发过程质量的方法，其包括使用统计过程控制和机器学习技术来分析应用程序生命周期管理所捕获的变更数据，实验表明该方法是有效的。 |
| [^18] | [On the Partial Convexification for Low-Rank Spectral Optimization: Rank Bounds and Algorithms.](http://arxiv.org/abs/2305.07638) | 研究低秩谱最优化问题的部分凸化的实力，提出了派生任何极端点的秩界，并证明了对于不同矩阵空间的域集合的紧致性，同时开发了一个包含矢量凸定价神谕的列生成算法以有效解决此问题。 |
| [^19] | [Integrating nearest neighbors on neural network models for treatment effect estimation.](http://arxiv.org/abs/2305.06789) | 本论文提出了一种新的方法NNCI，用于将最近邻居信息集成到神经网络模型中，以更准确地估计治疗效果。 |
| [^20] | [Impact Study of Numerical Discretization Accuracy on Parameter Reconstructions and Model Parameter Distributions.](http://arxiv.org/abs/2305.02663) | 本论文研究了数值离散化参数对光学纳米计量领域参数重建和模型参数分布的影响，确定了数值参数可以允许高精度重建。 |
| [^21] | [ChemCrow: Augmenting large-language models with chemistry tools.](http://arxiv.org/abs/2304.05376) | 本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。 |
| [^22] | [Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression.](http://arxiv.org/abs/2304.01561) | 本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。 |
| [^23] | [Finding Competence Regions in Domain Generalization.](http://arxiv.org/abs/2303.09989) | 该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。 |
| [^24] | [Detection-Recovery Gap for Planted Dense Cycles.](http://arxiv.org/abs/2302.06737) | 本文研究了在Erdős-Rényi图中种植一个具有期望带宽n*tau和边密度p的稠密环，对应的检测和恢复问题在低次多项式算法类的计算阈值存在差距。 |
| [^25] | [Algorithmic Collective Action in Machine Learning.](http://arxiv.org/abs/2302.04262) | 本文研究了机器学习中的算法集体行动的理论模型，并在大量实验中验证了该算法可以大大提高分类准确性，特别是在数据结构复杂和集体规模大的情况下。 |
| [^26] | [Bayes-optimal Learning of Deep Random Networks of Extensive-width.](http://arxiv.org/abs/2302.00375) | 本文研究了深度随机网络的学习问题，提出了 Bayes 最优测试误差的闭式表达式。岭回归和核回归能够达到最优表现，而神经网络的测试误差也可以从平方级的样本数量中获得接近于零的结果。 |
| [^27] | [Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge.](http://arxiv.org/abs/2301.11214) | 本文研究了如何将有向无环图中的概率因果知识应用于回归问题，提出了一种碰撞回归框架，并证明了其在假设空间为再生核希尔伯特空间时具有严格正的广义收益，在合成和气候模型数据上的实验结果表明其可以提高预测性能。 |
| [^28] | [flexBART: Flexible Bayesian regression trees with categorical predictors.](http://arxiv.org/abs/2211.04459) | 本论文提出了一种新的灵活贝叶斯回归树模型flexBART，可以在划分分类水平的离散集时，将多个水平分配给决策树节点的两个分支，从而实现了对分类预测变量的灵活建模，跨级别的数据部分汇总能力也得到了改进。 |
| [^29] | [Convergence Rates for Regularized Optimal Transport via Quantization.](http://arxiv.org/abs/2208.14391) | 本文研究了随着正则化参数消失，发散-正则化最优输运的收敛性，并通过量化和鞅耦合技术得到了一些一般发散性、包括相对熵或 $L^{p}$ 正则化、一般运输成本和多边问题的尖锐速率结果。 |
| [^30] | [Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals.](http://arxiv.org/abs/2203.13887) | 本文提出了一种自动去偏机器学习方法，通过递归Riesz表征嵌套均值回归，避免了在动态治疗方案中需要解决辅助倾向模型的问题。 |
| [^31] | [RiskNet: Neural Risk Assessment in Networks of Unreliable Resources.](http://arxiv.org/abs/2201.12263) | 本文提出了一种基于GNN算法的方法，用于在通信网络中处理连接故障引起的罚款分布，并可以准确地模拟各种现有拓扑结构中的惩罚，在实践中还获得了超过12,000倍的速度提高。 |
| [^32] | [Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions.](http://arxiv.org/abs/2201.02958) | 本文提出了一种嵌套模拟的新方法，它能够在保持条件期望足够平滑的情况下，有效地缓解高维度中的维度灾难，以桥接标准嵌套模拟的立方根收敛率和标准蒙特卡洛模拟的平方根收敛率之间的差距。 |
| [^33] | [Self-Supervised Graph Representation Learning for Neuronal Morphologies.](http://arxiv.org/abs/2112.12482) | 本文提出了GraphDINO，一种自监督的图形表示学习方法，可用于从未标记的大规模数据集中学习三维神经元形态的低维表示。该方法使用了一系列数据增强策略和新型的注意力机制AC-Attention，在多个大脑区域内，GraphDINO 显示出了优于其它最先进方法的表现。 |
| [^34] | [Generalization in the Face of Adaptivity: A Bayesian Perspective.](http://arxiv.org/abs/2106.10761) | 本文提出面对自适应选择数据样本引起的过度拟合问题，使用噪声加算法可以提供不依赖于查询规模的误差保证，这一结果表明适应数据分析的问题在于新查询与过去查询的协方差。 |

# 详细

[^1]: 二次型赌臂机的样本复杂度：Hessian相关性界限和最优算法

    Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])

    [http://arxiv.org/abs/2306.12383](http://arxiv.org/abs/2306.12383)

    本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。

    

    在随机零阶优化中，了解如何充分利用底层目标函数的局部几何结构是一个实际相关的问题。我们考虑一种基本情况，即目标函数是二次型的，并且提供了最优Hessian相关样本复杂度的第一个紧密刻画。我们的贡献具有双重性质。首先，从信息论的角度出发，通过引入一种称为能量分配的概念来捕捉搜索算法和目标函数几何结构之间的交互，证明了Hessian相关复杂度的紧密下界。通过解决最优能量谱，得到了配套的上限。其次，算法方面，我们展示了存在一种Hessian无关的算法，能够普遍实现所有Hessian实例的渐近最优样本复杂度。我们算法能够实现的渐近最优样本复杂度对于重尾噪声分布仍然有效。

    In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
    
[^2]: 一种有效且可证明精确的0-1损失线性分类问题算法

    An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])

    [http://arxiv.org/abs/2306.12344](http://arxiv.org/abs/2306.12344)

    该研究详细介绍了一种名为增量单元枚举（ICE）的算法，该算法可以精确解决定维度0-1损失线性分类问题。

    

    解决线性分类问题的算法具有悠久的历史，至少可以追溯到1936年的线性判别分析。对于线性可分数据，许多算法可以有效地得到相应的0-1损失分类问题的精确解，但对于非线性可分数据，已经证明这个问题在完全范围内是NP难的。所有替代方法都涉及某种形式的近似，包括使用0-1损失的代理（例如hinge或logistic损失）或近似的组合搜索，这些都不能保证完全解决问题。找到解决定维度0-1损失线性分类问题的全局最优解的有效算法仍然是一个未解决的问题。在本研究中，我们详细介绍了一个新算法的构建过程，增量单元枚举（ICE），它可以精确解决0-1损失分类问题。

    Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
    
[^3]: 奇妙的权重及其查找方法：动态稀疏训练中的剪枝位置

    Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])

    [http://arxiv.org/abs/2306.12230](http://arxiv.org/abs/2306.12230)

    该论文对动态稀疏训练中的剪枝位置进行了实证分析，发现在低密度范围内，最简单的大小方法提供了最佳性能。

    

    动态稀疏训练（DST）是一个快速发展的研究领域，旨在通过在训练过程中调整神经网络的拓扑结构来优化其稀疏初始化。已经证明，在特定条件下，DST能够胜过密集模型。该框架的关键组成部分是剪枝和生长标准，这些标准在训练过程中被反复应用以调整网络的稀疏连接。虽然生长标准对DST性能的影响相对较好地研究了，但剪枝标准的影响仍然被忽视。为解决这个问题，我们设计并进行了对各种剪枝标准的广泛实证分析，以更好地了解它们对 DST 解决方案动态的影响。令人惊讶的是，我们发现大多数研究方法都产生类似的结果。在低密度范围内，最简单的技术——基于大小的方法，提供了最佳性能。

    Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
    
[^4]: 更多的PAC-Bayes Bounds：从有界损失到具有一般性尾部行为的损失，到任何时间均有效的损失。

    More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])

    [http://arxiv.org/abs/2306.12214](http://arxiv.org/abs/2306.12214)

    本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。

    

    本文针对不同类型的损失提出了新的高概率PAC-Bayes界限。首先，针对有界范围的损失，我们提出了Catoni界的加强版本，适用于所有参数值的统一界。这导致了新的快速速率和混合速率上限，这些上限可解释性强且比文献中先前界限更紧。其次，针对更一般的尾部行为的损失，我们引入了两个新的无参数上限：当损失的累积生成函数有界时，我们引入了一个PAC-Bayes Chernoff类比，另一个上限是损失的二阶矩有界。这两个上限是利用一种基于可能事件空间的离散化的新技术获得的，“在概率”参数优化问题。最后，我们使用一种适用于任何现有界限的简单技术将所有先前结果扩展到任何时间有效的上限。

    In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
    
[^5]: 随机振动中人体反应的解释：运动方向、坐姿和人体测量的影响

    Explaining human body responses in random vibration: Effect of motion direction, sitting posture, and anthropometry. (arXiv:2306.12115v1 [stat.ML])

    [http://arxiv.org/abs/2306.12115](http://arxiv.org/abs/2306.12115)

    本文研究了人体形态、生物性别和姿势对于平移振动中人体动力学响应的影响，并创建了多个线性回归模型以确定最具影响的预测因子。

    

    本研究调查了人体形态、生物性别和姿势对于平移振动中人体动力学响应的影响。35名参与者坐在标准汽车座椅上，使用基于运动的平台施加0.1至12.0赫兹的随机信号振动，RMS加速度为0.3 m / s2，持续60秒。创建了多个线性回归模型，以确定频域内每个身体段（骨盆、躯干和头部）的峰值平移增益最有影响的预测因子。模型将实验操作因子（运动方向、姿势、测量的人体测量属性和生物性别）作为预测因子。评估了包括预测因子对模型拟合的影响。

    This study investigates the effects of anthropometric attributes, biological sex, and posture on translational body kinematic responses in translational vibrations. In total, 35 participants were recruited. Perturbations were applied on a standard car seat using a motion-based platform with 0.1 to 12.0 Hz random noise signals, with 0.3 m/s2 rms acceleration, for 60 seconds. Multiple linear regression models (three basic models and one advanced model, including interactions between predictors) were created to determine the most influential predictors of peak translational gains in the frequency domain per body segment (pelvis, trunk, and head). The models introduced experimentally manipulated factors (motion direction, posture, measured anthropometric attributes, and biological sex) as predictors. Effects of included predictors on the model fit were estimated. Basic linear regression models could explain over 70% of peak body segments' kinematic body response (where the R2 adjusted was 
    
[^6]: 松弛光滑条件下随机双层优化的最优算法

    Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions. (arXiv:2306.12067v1 [math.OC])

    [http://arxiv.org/abs/2306.12067](http://arxiv.org/abs/2306.12067)

    本文提出了一种新型随机双层优化的最优算法，避免了使用高阶光滑性假设，能够更好地适应非凸设置。

    

    随机双层优化通常涉及最小化依赖于强凸下层函数的下层函数（LL）最小值的上层（UL）函数。几种算法利用Neumann级数来近似估计隐式梯度（超梯度）的矩阵逆。最先进的随机双层算法（SOBA）[16]改用随机梯度下降步骤来解决与显式矩阵反演相关的线性系统。该修改使SOBA能够在非凸设置中匹配单层对应项的样本复杂度下限。不幸的是，当前SOBA的分析依赖于UL和LL函数的高阶光滑性假设以实现最优性。本文介绍了随机双层优化的一种新型完全单循环且无Hessian反演算法框架，并在标准光滑性假设下提供了更紧密的分析。

    Stochastic Bilevel optimization usually involves minimizing an upper-level (UL) function that is dependent on the arg-min of a strongly-convex lower-level (LL) function. Several algorithms utilize Neumann series to approximate certain matrix inverses involved in estimating the implicit gradient of the UL function (hypergradient). The state-of-the-art StOchastic Bilevel Algorithm (SOBA) [16] instead uses stochastic gradient descent steps to solve the linear system associated with the explicit matrix inversion. This modification enables SOBA to match the lower bound of sample complexity for the single-level counterpart in non-convex settings. Unfortunately, the current analysis of SOBA relies on the assumption of higher-order smoothness for the UL and LL functions to achieve optimality. In this paper, we introduce a novel fully single-loop and Hessian-inversion-free algorithmic framework for stochastic bilevel optimization and present a tighter analysis under standard smoothness assumpti
    
[^7]: 采样个体公平且满足群体公平的排名

    Sampling Individually-Fair Rankings that are Always Group Fair. (arXiv:2306.11964v1 [cs.CY])

    [http://arxiv.org/abs/2306.11964](http://arxiv.org/abs/2306.11964)

    该论文提出了一种有效算法，从个体公平分布中采样排名，同时确保每个输出的排名都满足群体公平性约束。输出排名的期望效用至少是最优公平解的效用的$\alpha$倍，其中$\alpha$是一个量化公平约束紧度的参数。

    

    在线平台上的排名可以帮助用户快速找到相关信息，如人物、新闻、媒体和产品。公平排名是一种为了满足群体公平性约束而优化一组项目排名的任务，已经在算法公平性、信息检索和机器学习领域引起了广泛关注。然而，近期的研究表明项目效用的不确定性是不公平的主要原因，并建议在输出中引入随机性。这种随机性经过仔细选择，以确保对每个项目进行充分且合理的代表（同时考虑不确定性）。然而，由于这种随机性，输出的排名可能会违反群体公平性约束。我们提出了一个有效的算法，从一个个体公平分布中抽样排名，同时确保每个输出的排名都满足群体公平性约束。输出排名的期望效用至少是最优公平解的效用的 $\alpha$ 倍，其中 $\alpha$ 是一个量化公平约束紧度的参数。我们在真实世界数据集上进行实验，证明了我们算法的高效性和有效性。

    Rankings on online platforms help their end-users find the relevant information -- people, news, media, and products -- quickly. Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in the Algorithmic Fairness, Information Retrieval, and Machine Learning literature. Recent works, however, identify uncertainty in the utilities of items as a primary cause of unfairness and propose introducing randomness in the output. This randomness is carefully chosen to guarantee an adequate representation of each item (while accounting for the uncertainty). However, due to this randomness, the output rankings may violate group fairness constraints. We give an efficient algorithm that samples rankings from an individually-fair distribution while ensuring that every output ranking is group fair. The expected utility of the output ranking is at least $\alpha$ times the utility of the optimal fair solut
    
[^8]: 开放问题：基于变分目标的测度学习 (arXiv:2306.11928v1 [stat.ML])

    Open Problem: Learning with Variational Objectives on Measures. (arXiv:2306.11928v1 [stat.ML])

    [http://arxiv.org/abs/2306.11928](http://arxiv.org/abs/2306.11928)

    本文探讨了在测度上编写变分目标的动机，提出通过此类目标推导实用算法，以解决超出分布的泛化和弱监督学习等问题的开放问题。

    

    统计学习理论关注的是基于函数的变分目标。本文讨论了在测度上编写类似目标的动机，特别是讨论了超出分布的泛化和弱监督学习。这引发了一个自然的问题：能否将通常的统计学习结果转化为基于测量表达的目标？结果构建是否会导致新的实用算法？

    The theory of statistical learning has focused on variational objectives expressed on functions. In this note, we discuss motivations to write similar objectives on measures, in particular to discuss out-of-distribution generalization and weakly-supervised learning. It raises a natural question: can one cast usual statistical learning results to objectives expressed on measures? Does the resulting construction lead to new algorithms of practical interest?
    
[^9]: 图分类问题中结构感知的鲁棒性认证

    Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])

    [http://arxiv.org/abs/2306.11915](http://arxiv.org/abs/2306.11915)

    该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。

    

    对于基于图的机器学习模型进行鲁棒性认证是保证安全性的一个至关重要的挑战。目前用于图分类器的鲁棒性证明保证与节点对翻转（添加或删除边缘）的总数有关，这相当于以邻接矩阵为中心的l0球。尽管从理论上看很有吸引力，但这种各向同性的结构噪声在实际场景中可能过于严格，因为有些节点对于确定分类器的输出更为关键。在这种情况下，证书给出了对图模型鲁棒性的悲观描述。为了解决这个问题，我们开发了一种基于随机平滑的方法，将非各向同性的噪声分布添加到输入图结构中。我们展示了我们的过程为分类器生成了结构感知的证书，因此鲁棒性证书的大小可以在图的不同预定义结构之间变化。我们在几个基准图分类任务上展示了我们方法的优势，在对抗性攻击的鲁棒性方面取得了最先进的结果。

    Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
    
[^10]: 基于定点树的广义随机森林加速

    Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])

    [http://arxiv.org/abs/2306.11908](http://arxiv.org/abs/2306.11908)

    本文提出一种新的树生长规则，使广义随机森林在无梯度优化的情况下大大节省了时间。

    

    广义随机森林建立在传统随机森林的基础上，通过将其作为自适应核加权算法来构建估算器，并通过基于梯度的树生长过程来实现。我们提出了一种新的树生长规则，基于定点迭代近似表示梯度近似，实现了无梯度优化，并为此开发了渐近理论。这有效地节省了时间，尤其是在目标量的维度适中时。

    Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
    
[^11]: 学习结构蒙日位移的成本

    Learning Costs for Structured Monge Displacements. (arXiv:2306.11895v1 [stat.ML])

    [http://arxiv.org/abs/2306.11895](http://arxiv.org/abs/2306.11895)

    本文提出了一种新的方法，即学习合适的成本结构来鼓励映射沿着特定的工程特征来传送点，其在扩展 Monge-Bregman-Occam 管道方面做出了重大贡献，并在匹配直方图方面表现出了优异的性能。

    

    最优输运理论为机器学习提供了多种推断样本间密度前向映射的工具。尽管最近在机器学习领域中该理论已经见证了许多方法的发展，但其实际实现仍然极其困难，因为它同时面临计算和统计上的挑战。现有方法很少有不使用默认选择来估计这些映射的情况，其中简单的平方欧氏距离作为地面费用$c(x,y)=\|x-y\|^2_2$。我们在本文中采取不同的方法，以\emph{学习}合适的成本结构，鼓励映射沿着特定的工程特征来传送点。我们将最近提出的 Monge-Bregman-Occam 管道~\citep{cuturi2023monge} 的范式进行了扩展，该范式基于替代的成本公式$c(x,y)=h(x-y)$ ，它也是成本不变的，但采用更一般的形式$h=\tfrac12 \ell_2^2+\tau$，其中$\tau$是适当的凸规则项。

    Optimal transport theory has provided machine learning with several tools to infer a push-forward map between densities from samples. While this theory has recently seen tremendous methodological developments in machine learning, its practical implementation remains notoriously difficult, because it is plagued by both computational and statistical challenges. Because of such difficulties, existing approaches rarely depart from the default choice of estimating such maps with the simple squared-Euclidean distance as the ground cost, $c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the motivation of \emph{learning} a suitable cost structure to encourage maps to transport points along engineered features. We extend the recently proposed Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is an approp
    
[^12]: 是否应该停止：具有异质种群的早期停止方法

    Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])

    [http://arxiv.org/abs/2306.11839](http://arxiv.org/abs/2306.11839)

    本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。

    

    随机实验由于治疗造成意外的有害影响，因此往往需要提前停止。目前确定何时提前终止实验的现有方法通常适用于总体数据，不考虑治疗效应的异质性。本文研究了针对异质种群有害实验的早期停止方法。我们首先确定现有方法在治疗对少数参与者造成伤害时往往无法停止实验。然后使用因果机器学习开发了CLASH，这是首个广泛适用于异质早期停止的方法。我们在模拟和实际数据上展示了CLASH的表现，并证明它在临床试验和A/B测试中都能有效提前停止。

    Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
    
[^13]: 拓扑视差：深度感知模型的几何规范说明

    Topological Parallax: A Geometric Specification for Deep Perception Models. (arXiv:2306.11835v1 [cs.LG])

    [http://arxiv.org/abs/2306.11835](http://arxiv.org/abs/2306.11835)

    拓扑视差是一种比较训练模型和参考数据集多尺度几何结构相似性的理论和计算工具，它可以估计模型中的拓扑特征，有助于理解深度学习模型的行为和性能。

    

    为了保证人工智能系统的安全性和鲁棒性，我们引入拓扑视差作为比较已训练模型和参考数据集的多尺度几何结构相似性的理论和计算工具。我们的证明和例子表明，数据集和模型之间的这种几何相似性对于可信的插值和扰动至关重要，并且我们猜测，这个新概念将为深度学习应用中过拟合和泛化之间不明确的关系的当前讨论增添价值。在典型的DNN应用中，模型的显式几何描述是不可能的，但视差可以通过检查使用参考数据集的测地畸变对Rips复合体的影响来估计模型中的拓扑特征（组件、周期、空洞等）。因此，视差指示模型与数据集是否共享类似的多尺度几何特征。视差通过拓扑数据分析理论，提供了从不同角度观察数据的直观概念，并为理解深度感知模型的行为和性能提供了新的视角。

    For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between overfitting and generalization in applications of deep-learning. In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topolo
    
[^14]: 任何深度ReLU网络都是浅层网络

    Any Deep ReLU Network is Shallow. (arXiv:2306.11827v1 [cs.LG])

    [http://arxiv.org/abs/2306.11827](http://arxiv.org/abs/2306.11827)

    该论文证明了任何深度的ReLU网络都可以被重写为一个具有透明性的浅层网络。这一结论有助于解释模型行为。

    

    我们构造性地证明了每个深度的ReLU网络可以被重写为一个函数上等价的三层网络，其中权重值为延迟实数。基于此证明，我们提供了一个算法，可以给出一个深度ReLU网络对应的显式权重。由此得到的浅层网络是透明的，并用于生成模型行为的解释。

    We constructively prove that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals. Based on this proof, we provide an algorithm that, given a deep ReLU network, finds the explicit weights of the corresponding shallow network. The resulting shallow network is transparent and used to generate explanations of the model s behaviour.
    
[^15]: 多任务高斯过程中的时间变化转移矩阵

    Time-Varying Transition Matrices with Multi-task Gaussian Processes. (arXiv:2306.11772v1 [stat.ML])

    [http://arxiv.org/abs/2306.11772](http://arxiv.org/abs/2306.11772)

    本文介绍了一种基于核的多任务高斯过程模型，用于逼近个体的移动状态的潜在函数，并考虑了转移概率之间的相关性和时间可变性，通过强制执行随机性和非负性约束来实现研究目标。

    

    本文提出了一种基于核的多任务高斯过程模型，用于逼近个体的移动状态的潜在函数，该模型使用具有两个状态（移动和停留）的时间不均匀马尔科夫过程。我们的方法通过创建任务之间的协方差矩阵来考虑转移概率之间的相关性。我们还引入了时间可变性，假设个体的转移概率随着外部变量而随时间变化。通过将约束点集成到高斯过程中，我们强制执行马尔科夫过程中概率的随机性和非负性约束。我们还讨论了利用Toeplitz和Kronecker乘积结构在此上下文中加速GP估计和推断的机会。我们的数值实验表明，我们的公式可以强制执行所需的约束条件，同时学习转移概率的函数形式。

    In this paper, we present a kernel-based, multi-task Gaussian Process (GP) model for approximating the underlying function of an individual's mobility state using a time-inhomogeneous Markov Process with two states: moves and pauses. Our approach accounts for the correlations between the transition probabilities by creating a covariance matrix over the tasks. We also introduce time-variability by assuming that an individual's transition probabilities vary over time in response to exogenous variables. We enforce the stochasticity and non-negativity constraints of probabilities in a Markov process through the incorporation of a set of constraint points in the GP. We also discuss opportunities to speed up GP estimation and inference in this context by exploiting Toeplitz and Kronecker product structures. Our numerical experiments demonstrate the ability of our formulation to enforce the desired constraints while learning the functional form of transition probabilities.
    
[^16]: 基于Samplet基 Pursuit 的核学习方法

    Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])

    [http://arxiv.org/abs/2306.10180](http://arxiv.org/abs/2306.10180)

    本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。

    

    本文考虑了基于l1正则化的Samplet坐标下的核学习问题。在Samplet基的系数上，应用l1正则化项可以强制增加稀疏性。因此，我们称这种方法为Samplet基 Pursuit。Samplet基是波形类型的有符号测度，专门用于散乱数据。它们具有与小波相似的本地化、多分辨率分析和数据压缩性质。可以在Samplet基上稀疏地表示的信号类比单尺度基上能够表示稀疏的信号类别要大得多。特别地，仅用基函数映射的几个特征叠加即可表示的所有信号也可以在Samplet坐标下实现稀疏表示。我们提出了一种高效解决该问题的方法，将软阈值和半光滑牛顿法相结合，并将该方法与快速迭代收缩阈值算法进行了比较。实验结果表明了该方法在稀疏性和预测精度方面的优势。

    We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
    
[^17]: 利用演进变化提高软件过程质量。

    Leveraging Evolutionary Changes for Software Process Quality. (arXiv:2305.18061v1 [cs.SE])

    [http://arxiv.org/abs/2305.18061](http://arxiv.org/abs/2305.18061)

    本文提出了一种利用演进变化来改善软件开发过程质量的方法，其包括使用统计过程控制和机器学习技术来分析应用程序生命周期管理所捕获的变更数据，实验表明该方法是有效的。

    

    现实世界中的软件应用必须不断演进才能保持相关性。传统的软件质量控制方法涉及软件质量模型和持续的代码检查工具。然而，软件开发过程的质量与最终软件产品的质量之间存在强关联和因果关系。因此，间接提高软件产品的质量需要改善软件开发过程的质量。本文提出了一种利用开发过程的演进变化来提高软件质量的新方法。该方法包括使用统计过程控制和机器学习技术来分析应用程序生命周期管理所捕获的变更数据。实验结果显示了该方法的有效性。

    Real-world software applications must constantly evolve to remain relevant. This evolution occurs when developing new applications or adapting existing ones to meet new requirements, make corrections, or incorporate future functionality. Traditional methods of software quality control involve software quality models and continuous code inspection tools. These measures focus on directly assessing the quality of the software. However, there is a strong correlation and causation between the quality of the development process and the resulting software product. Therefore, improving the development process indirectly improves the software product, too. To achieve this, effective learning from past processes is necessary, often embraced through post mortem organizational learning. While qualitative evaluation of large artifacts is common, smaller quantitative changes captured by application lifecycle management are often overlooked. In addition to software metrics, these smaller changes can 
    
[^18]: 关于低秩谱最优化的部分凸化：秩界与算法

    On the Partial Convexification for Low-Rank Spectral Optimization: Rank Bounds and Algorithms. (arXiv:2305.07638v1 [math.OC])

    [http://arxiv.org/abs/2305.07638](http://arxiv.org/abs/2305.07638)

    研究低秩谱最优化问题的部分凸化的实力，提出了派生任何极端点的秩界，并证明了对于不同矩阵空间的域集合的紧致性，同时开发了一个包含矢量凸定价神谕的列生成算法以有效解决此问题。

    

    低秩谱最优化问题（LSOP）的目标是在低秩和谱约束的可行域内，最小化一个线性目标，满足多个双面线性矩阵不等式的交集。虽然解决LSOP在一般情况下是NP难的，但它的部分凸化（即用凸壳代替域集合），称为“LSOP-R”，通常是可处理的并产生高质量的解。这激励我们研究LSOP-R的实力。具体而言，我们为LSOP-R可行集的任何极端点派生秩界，并证明对于不同矩阵空间的域集合，它们的紧致性。所提出的秩界从新的角度恢复了文献中的两个众所周知的结果，并允许我们导出当松弛LSOP-R等价于原始LSOP的充分条件。为了有效地解决LSOP-R，我们开发了一个包含矢量凸定价神谕的列生成算法，配合一个秩降算法，它确保输出一个LSOP-R的准确解。

    A Low-rank Spectral Optimization Problem (LSOP) minimizes a linear objective subject to multiple two-sided linear matrix inequalities intersected with a low-rank and spectral constrained domain set. Although solving LSOP is, in general, NP-hard, its partial convexification (i.e., replacing the domain set by its convex hull) termed "LSOP-R," is often tractable and yields a high-quality solution. This motivates us to study the strength of LSOP-R. Specifically, we derive rank bounds for any extreme point of the feasible set of LSOP-R and prove their tightness for the domain sets with different matrix spaces. The proposed rank bounds recover two well-known results in the literature from a fresh angle and also allow us to derive sufficient conditions under which the relaxation LSOP-R is equivalent to the original LSOP. To effectively solve LSOP-R, we develop a column generation algorithm with a vector-based convex pricing oracle, coupled with a rank-reduction algorithm, which ensures the ou
    
[^19]: 在神经网络模型中集成最近邻居以估计治疗效果

    Integrating nearest neighbors on neural network models for treatment effect estimation. (arXiv:2305.06789v1 [stat.ML])

    [http://arxiv.org/abs/2305.06789](http://arxiv.org/abs/2305.06789)

    本论文提出了一种新的方法NNCI，用于将最近邻居信息集成到神经网络模型中，以更准确地估计治疗效果。

    

    治疗效果估计对于许多科学和工业领域的研究人员和从业者来说具有高度重要性。观察数据的丰富性使它们越来越受到研究人员用于因果效应的估计。然而，这些数据存在偏差和其他弱点，导致如果不正确处理，估计因果效应会不准确。因此，提出了几种机器学习技术，其中大部分都专注于利用神经网络模型的预测能力，以达到更精确的因果效应估计。在本文中，我们提出了一种名为最近邻居信息用于因果推断（NNCI）的新方法，用于将有价值的最近邻居信息集成到基于神经网络的模型中，以估计治疗效果。提出的NNCI方法被应用于一些最广泛使用的基于神经网络的治疗效果估计模型，其使用观察数据。

    Treatment effect estimation is of high-importance for both researchers and practitioners across many scientific and industrial domains. The abundance of observational data makes them increasingly used by researchers for the estimation of causal effects. However, these data suffer from biases, from several weaknesses, leading to inaccurate causal effect estimations, if not handled properly. Therefore, several machine learning techniques have been proposed, most of them focusing on leveraging the predictive power of neural network models to attain more precise estimation of causal effects. In this work, we propose a new methodology, named Nearest Neighboring Information for Causal Inference (NNCI), for integrating valuable nearest neighboring information on neural network-based models for estimating treatment effects. The proposed NNCI methodology is applied to some of the most well established neural network-based models for treatment effect estimation with the use of observational data
    
[^20]: 数值离散化精度对参数重建和模型参数分布的影响研究

    Impact Study of Numerical Discretization Accuracy on Parameter Reconstructions and Model Parameter Distributions. (arXiv:2305.02663v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.02663](http://arxiv.org/abs/2305.02663)

    本论文研究了数值离散化参数对光学纳米计量领域参数重建和模型参数分布的影响，确定了数值参数可以允许高精度重建。

    

    数值模型在光学纳米计量领域的参数重建中得到广泛应用。通过使用贝叶斯目标向量优化方法将有限元数值模型拟合到实验数据集，可以获得纳米结构线光栅的几何参数。在重建过程中，使用高斯过程代理模型进行训练。然后，我们利用马尔可夫链蒙特卡罗抽样器对代理模型进行抽样，以确定重建模型参数的完整模型参数分布。数值离散化参数的选择，如有限元解答函数的多项式阶数，影响正演模型的数值离散化误差。在本研究中，我们研究了正演问题的数值离散化参数对重构参数以及模型参数分布的影响。我们表明这样的收敛研究可以确定允许高精度重建的数值参数。

    Numerical models are used widely for parameter reconstructions in the field of optical nano metrology. To obtain geometrical parameters of a nano structured line grating, we fit a finite element numerical model to an experimental data set by using the Bayesian target vector optimization method. Gaussian process surrogate models are trained during the reconstruction. Afterwards, we employ a Markov chain Monte Carlo sampler on the surrogate models to determine the full model parameter distribution for the reconstructed model parameters. The choice of numerical discretization parameters, like the polynomial order of the finite element ansatz functions, impacts the numerical discretization error of the forward model. In this study we investigate the impact of numerical discretization parameters of the forward problem on the reconstructed parameters as well as on the model parameter distributions. We show that such a convergence study allows to determine numerical parameters which allow for
    
[^21]: ChemCrow:用化学工具增强大型语言模型

    ChemCrow: Augmenting large-language models with chemistry tools. (arXiv:2304.05376v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.05376](http://arxiv.org/abs/2304.05376)

    本研究介绍了ChemCrow，一种LLM化学代理，通过整合13个专家设计的工具从而增强LLM在化学领域的性能，在化学任务中实现自动化，提高了效率和效果。

    

    近期大型语言模型(LLMs)在跨领域的任务表现出一定的优势，但在化学相关问题上却表现不佳。此外，这些模型缺乏访问外部知识源，限制了它们在科学应用中的有用性。在本研究中，我们介绍了ChemCrow，一种LLM化学代理，旨在完成有机合成、药物发现和材料设计等任务。通过整合13个专家设计的工具，ChemCrow提高了LLM在化学中的性能，并产生了新的能力。我们的评估，包括LLM和人类专家评估，证明了ChemCrow在自动化各种化学任务方面的有效性。令人惊讶的是，我们发现GPT-4作为评估器无法区分明显错误的GPT-4完成和GPT-4 + ChemCrow性能。这种工具的滥用有很大的风险，我们讨论了它们的潜在危害。在负责任的情况下，ChemCrow不仅可以帮助专业化学家并降低成本。

    Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers ba
    
[^22]: Shallow ReLU$^k$神经网络的逼近速率及其在非参数回归中的应用

    Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])

    [http://arxiv.org/abs/2304.01561](http://arxiv.org/abs/2304.01561)

    本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。

    

    本研究探讨与Shallow ReLU$^k$神经网络相关的变异空间的逼近容量。结果表明，在有限变异范数下，容纳了足够平滑的函数。对于较少平滑的函数，根据变异范数建立了逼近速率。运用这些结果，我们可以证明Shallow ReLU$^k$神经网络的最优逼近速率。同时阐明了这些结果如何用于推导深层神经网络和卷积神经网络(CNNs)的逼近界限。为应用研究，我们使用了三种ReLU神经网络模型：浅层神经网络，超参数神经网络和CNN进行非参数回归收敛速率研究。特别地，我们展示了浅层神经网络可以实现学习H\"older函数的最优渐进速率，这补充了深层神经网络的最近结果。

    We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
    
[^23]: 在领域泛化中找到能力区域

    Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])

    [http://arxiv.org/abs/2303.09989](http://arxiv.org/abs/2303.09989)

    该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。

    

    我们提出了一个“学习拒绝”框架来解决领域泛化中默默失败的问题，即测试分布与训练分布不同的情况。假设有一个温和的分布偏移，我们希望在模型估计的能力预示着可信响应时接受超出分布的数据，而不是直接拒绝超出分布的数据。可信度通过与分类器性能密切相关的代理无能分数进行预测。我们对分类的无能得分进行了全面的实验评估，并强调了拒绝率与准确率之间的权衡。为了与先前的工作进行比较，我们聚焦于标准领域泛化基准，并考虑在闭合和开放世界环境下通过不同的学习表示来衡量无能。我们的结果表明，增加无能分数确实预示着降低准确性，从而导致显着的...

    We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
    
[^24]: 揭示种植稠密环的检测和恢复差距

    Detection-Recovery Gap for Planted Dense Cycles. (arXiv:2302.06737v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2302.06737](http://arxiv.org/abs/2302.06737)

    本文研究了在Erdős-Rényi图中种植一个具有期望带宽n*tau和边密度p的稠密环，对应的检测和恢复问题在低次多项式算法类的计算阈值存在差距。

    

    种植稠密环是许多应用程序中出现的一种潜在结构，例如社会科学中的小世界网络和计算生物学中的序列组装。本文考虑在Erdős-Rényi图G(n，q)中种植一个具有期望带宽n*tau和边密度p的稠密环。我们表征了低次多项式算法类的相关检测和恢复问题的计算阈值。特别地，在某些参数范围内存在两个阈值之间的差距。例如，如果n^(-3/4)≪tau≪n^(-1/2)且p=Cq=Θ(1)，其中C>1是一个常数，则检测问题对于低次算法是计算容易的，而恢复问题对于低次算法是困难的。

    Planted dense cycles are a type of latent structure that appears in many applications, such as small-world networks in social sciences and sequence assembly in computational biology. We consider a model where a dense cycle with expected bandwidth $n \tau$ and edge density $p$ is planted in an Erd\H{o}s-R\'enyi graph $G(n,q)$. We characterize the computational thresholds for the associated detection and recovery problems for the class of low-degree polynomial algorithms. In particular, a gap exists between the two thresholds in a certain regime of parameters. For example, if $n^{-3/4} \ll \tau \ll n^{-1/2}$ and $p = C q = \Theta(1)$ for a constant $C>1$, the detection problem is computationally easy while the recovery problem is hard for low-degree algorithms.
    
[^25]: 机器学习中算法集体行动的研究

    Algorithmic Collective Action in Machine Learning. (arXiv:2302.04262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04262](http://arxiv.org/abs/2302.04262)

    本文研究了机器学习中的算法集体行动的理论模型，并在大量实验中验证了该算法可以大大提高分类准确性，特别是在数据结构复杂和集体规模大的情况下。

    

    我们对在采用机器学习算法的数字平台上进行算法集体行动进行了原则性研究。我们提出了一个简单的理论模型，描述了一群人与公司的学习算法进行交互的情况。集体汇聚参与个体的数据并通过一种算法策略指导参与者修改自己的数据以实现集体目标。我们在三种基本的学习理论设置下研究了这种模型的结果：非参数最优学习算法，参数风险最小化和梯度下降优化。在每个设置中，我们提出了协调的算法策略，并根据集体规模的大小来表征自然的成功标准。为了补充我们的理论，我们对涉及数以万计自由职业平台简历的技能分类任务进行了系统实验。通过 BERT 模型的两千多次训练运行，我们证明了我们的算法集体行动可以显著提高分类准确性，比集中式学习算法和独立修改数据的非协调方法要好得多。我们的实验表明，算法集体行动的有效性至关重要的依赖于集体的规模和数据的基本结构。

    We initiate a principled study of algorithmic collective action on digital platforms that deploy machine learning algorithms. We propose a simple theoretical model of a collective interacting with a firm's learning algorithm. The collective pools the data of participating individuals and executes an algorithmic strategy by instructing participants how to modify their own data to achieve a collective goal. We investigate the consequences of this model in three fundamental learning-theoretic settings: the case of a nonparametric optimal learning algorithm, a parametric risk minimizer, and gradient-based optimization. In each setting, we come up with coordinated algorithmic strategies and characterize natural success criteria as a function of the collective's size. Complementing our theory, we conduct systematic experiments on a skill classification task involving tens of thousands of resumes from a gig platform for freelancers. Through more than two thousand model training runs of a BERT
    
[^26]: 深度随机网络的 Bayes 最优学习

    Bayes-optimal Learning of Deep Random Networks of Extensive-width. (arXiv:2302.00375v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00375](http://arxiv.org/abs/2302.00375)

    本文研究了深度随机网络的学习问题，提出了 Bayes 最优测试误差的闭式表达式。岭回归和核回归能够达到最优表现，而神经网络的测试误差也可以从平方级的样本数量中获得接近于零的结果。

    

    本文考虑学习一个深度广度非线性神经网络的目标函数，其具有随机高斯权重。我们研究了样本数量、输入维数和网络宽度成比例增加时的渐近情况，并为回归和分类任务提出了 Bayes 最优测试误差的闭式表达式。此外，我们还计算了岭回归、核函数和随机特征回归的测试误差的闭式表达式。我们发现，最优正则化的岭回归以及核回归可以达到 Bayes 最优表现，而逻辑损失函数对于分类问题几乎能达到最优的测试误差。我们通过数字实验证明，当样本数量增长速度快于维数时，岭回归和核方法变得次优，而神经网络可以从平方级的样本数量中获得接近于零的测试误差。

    We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random Gaussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large. We propose a closed-form expression for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test error close to zero from quadratically many samples.
    
[^27]: 回报恩惠：回归如何从概率因果知识中受益

    Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge. (arXiv:2301.11214v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11214](http://arxiv.org/abs/2301.11214)

    本文研究了如何将有向无环图中的概率因果知识应用于回归问题，提出了一种碰撞回归框架，并证明了其在假设空间为再生核希尔伯特空间时具有严格正的广义收益，在合成和气候模型数据上的实验结果表明其可以提高预测性能。

    

    有向无环图(DAG)提供了有价值的先验知识，但在机器学习中回归任务中经常被忽略。本文表明，在DAG中由碰撞结构引起的独立性提供了有意义的归纳偏差，这可以限制回归假设空间并提高预测性能。我们介绍了碰撞回归，一种将一个碰撞中的概率因果知识纳入回归问题的框架。当假设空间为再生核希尔伯特空间时，我们证明在温和的假设下，具有严格正的广义收益，并提供了经验风险最小化的闭合形式估计量。在合成和气候模型数据上的实验表明，所提出的技术可以提高预测性能。

    A directed acyclic graph (DAG) provides valuable prior knowledge that is often discarded in regression tasks in machine learning. We show that the independences arising from the presence of collider structures in DAGs provide meaningful inductive biases, which constrain the regression hypothesis space and improve predictive performance. We introduce collider regression, a framework to incorporate probabilistic causal knowledge from a collider in a regression problem. When the hypothesis space is a reproducing kernel Hilbert space, we prove a strictly positive generalisation benefit under mild assumptions and provide closed-form estimators of the empirical risk minimiser. Experiments on synthetic and climate model data demonstrate performance gains of the proposed methodology.
    
[^28]: flexBART:具有分类预测变量的灵活贝叶斯回归树

    flexBART: Flexible Bayesian regression trees with categorical predictors. (arXiv:2211.04459v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2211.04459](http://arxiv.org/abs/2211.04459)

    本论文提出了一种新的灵活贝叶斯回归树模型flexBART，可以在划分分类水平的离散集时，将多个水平分配给决策树节点的两个分支，从而实现了对分类预测变量的灵活建模，跨级别的数据部分汇总能力也得到了改进。

    

    大多数贝叶斯加法回归树（BART）的实现方法采用独热编码将分类预测变量替换为多个二进制指标，每个指标对应于每个级别或类别。用这些指标构建的回归树通过反复删除一个级别来划分分类水平的离散集。然而，绝大多数分割不能使用此策略构建，严重限制了BART在跨级别的数据部分汇总方面的能力。受对棒球数据和邻里犯罪动态分析的启发，我们通过重新实现以能够将多个水平分配给决策树节点的两个分支的回归树来克服了这个限制。为了对聚合为小区域的空间数据建模，我们进一步提出了一个新的决策规则先验，通过从适当定义的网络的随机生成树中删除一个随机边来创建空间连续的区域。我们的重新实现，可在R的flexBART软件包中使用，允许灵活地建模分类预测变量并改进跨不同级别的数据部分汇总。

    Most implementations of Bayesian additive regression trees (BART) one-hot encode categorical predictors, replacing each one with several binary indicators, one for every level or category. Regression trees built with these indicators partition the discrete set of categorical levels by repeatedly removing one level at a time. Unfortunately, the vast majority of partitions cannot be built with this strategy, severely limiting BART's ability to partially pool data across groups of levels. Motivated by analyses of baseball data and neighborhood-level crime dynamics, we overcame this limitation by re-implementing BART with regression trees that can assign multiple levels to both branches of a decision tree node. To model spatial data aggregated into small regions, we further proposed a new decision rule prior that creates spatially contiguous regions by deleting a random edge from a random spanning tree of a suitably defined network. Our re-implementation, which is available in the flexBART
    
[^29]: 通过量化实现正则化最优输运的收敛速率研究

    Convergence Rates for Regularized Optimal Transport via Quantization. (arXiv:2208.14391v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.14391](http://arxiv.org/abs/2208.14391)

    本文研究了随着正则化参数消失，发散-正则化最优输运的收敛性，并通过量化和鞅耦合技术得到了一些一般发散性、包括相对熵或 $L^{p}$ 正则化、一般运输成本和多边问题的尖锐速率结果。

    

    本文研究了随着正则化参数消失，发散-正则化最优输运的收敛性。得到了一些一般发散性、包括相对熵或 $L^{p}$ 正则化、一般运输成本和多边问题的尖锐速率结果。一种新颖的方法，使用量化和鞅耦合技术，适用于非紧致边缘，并实现了所有有限 $(2+\delta)$-矩的边缘上的熵正则化 2-Wasserstein 距离的尖锐领先阶项。

    We study the convergence of divergence-regularized optimal transport as the regularization parameter vanishes. Sharp rates for general divergences including relative entropy or $L^{p}$ regularization, general transport costs and multi-marginal problems are obtained. A novel methodology using quantization and martingale couplings is suitable for non-compact marginals and achieves, in particular, the sharp leading-order term of entropically regularized 2-Wasserstein distance for all marginals with finite $(2+\delta)$-moment.
    
[^30]: 动态治疗效应和一般嵌套函数的自动去偏机器学习

    Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals. (arXiv:2203.13887v5 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2203.13887](http://arxiv.org/abs/2203.13887)

    本文提出了一种自动去偏机器学习方法，通过递归Riesz表征嵌套均值回归，避免了在动态治疗方案中需要解决辅助倾向模型的问题。

    

    我们将自动去偏机器学习的思想扩展到了动态治疗方案和更一般的嵌套函数上。我们展示了动态治疗方案离散处理的多重稳健公式可以用嵌套均值回归的递归 Riesz 表示来重新表述。然后，我们应用一种递归 Riesz 表示估计学习算法，估计去偏转化的修正，而无需描述校正项的形式，例如倒数概率加权项的乘积，如在动态机制中进行的双重稳健估计中所做的那样。我们的方法定义了一系列损失最小化问题，其最小化器是去偏转化的修正的乘数，从而避免了需要解决辅助倾向模型，并直接优化目标去偏性修正的均方误差。我们进一步将我们的方法应用于一些实际问题中。

    We extend the idea of automated debiased machine learning to the dynamic treatment regime and more generally to nested functionals. We show that the multiply robust formula for the dynamic treatment regime with discrete treatments can be re-stated in terms of a recursive Riesz representer characterization of nested mean regressions. We then apply a recursive Riesz representer estimation learning algorithm that estimates de-biasing corrections without the need to characterize how the correction terms look like, such as for instance, products of inverse probability weighting terms, as is done in prior work on doubly robust estimation in the dynamic regime. Our approach defines a sequence of loss minimization problems, whose minimizers are the mulitpliers of the de-biasing correction, hence circumventing the need for solving auxiliary propensity models and directly optimizing for the mean squared error of the target de-biasing correction. We provide further applications of our approach to
    
[^31]: RiskNet:不可靠资源网络的神经风险评估

    RiskNet: Neural Risk Assessment in Networks of Unreliable Resources. (arXiv:2201.12263v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2201.12263](http://arxiv.org/abs/2201.12263)

    本文提出了一种基于GNN算法的方法，用于在通信网络中处理连接故障引起的罚款分布，并可以准确地模拟各种现有拓扑结构中的惩罚，在实践中还获得了超过12,000倍的速度提高。

    

    我们提出了一种基于图神经网络（GNN）的方法，用于预测通信网络中的故障引起的罚款分布，其中连接受共享于工作路径和备用路径之间的资源保护。该GNN算法仅使用通过Barab\'asi-Albert模型生成的随机图进行训练。尽管如此，所得的测试结果表明我们可以准确地模拟各种现有拓扑结构中的惩罚。GNN消除了在研究网络拓扑时模拟复杂故障场景的需要。在实践中，整个设计操作仅受现代硬件的4ms的限制。这样，我们可以获得超过12,000倍的速度提高。

    We propose a graph neural network (GNN)-based method to predict the distribution of penalties induced by outages in communication networks, where connections are protected by resources shared between working and backup paths. The GNN-based algorithm is trained only with random graphs generated with the Barab\'asi-Albert model. Even though, the obtained test results show that we can precisely model the penalties in a wide range of various existing topologies. GNNs eliminate the need to simulate complex outage scenarios for the network topologies under study. In practice, the whole design operation is limited by 4ms on modern hardware. This way, we can gain as much as over 12,000 times in the speed improvement.
    
[^32]: 平滑的嵌套模拟方法：在高维度中桥接立方和平方根收敛率

    Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions. (arXiv:2201.02958v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2201.02958](http://arxiv.org/abs/2201.02958)

    本文提出了一种嵌套模拟的新方法，它能够在保持条件期望足够平滑的情况下，有效地缓解高维度中的维度灾难，以桥接标准嵌套模拟的立方根收敛率和标准蒙特卡洛模拟的平方根收敛率之间的差距。

    

    嵌套模拟是通过模拟来估计条件期望的功能。本文提出了一种基于核岭回归的新方法，以利用条件期望作为多维调节变量的平滑函数。渐近分析表明，只要条件期望足够平滑，所提出的方法能够在模拟次数增加时有效地减少维度灾难的影响。平滑性桥接了立方根收敛率（即标准嵌套模拟的最优收敛率）和平方根收敛率（即标准蒙特卡洛模拟的规范收敛率）之间的差距。我们通过组合风险管理和输入不确定性量化的数值示例，展示了所提出方法的性能。

    Nested simulation concerns estimating functionals of a conditional expectation via simulation. In this paper, we propose a new method based on kernel ridge regression to exploit the smoothness of the conditional expectation as a function of the multidimensional conditioning variable. Asymptotic analysis shows that the proposed method can effectively alleviate the curse of dimensionality on the convergence rate as the simulation budget increases, provided that the conditional expectation is sufficiently smooth. The smoothness bridges the gap between the cubic root convergence rate (that is, the optimal rate for the standard nested simulation) and the square root convergence rate (that is, the canonical rate for the standard Monte Carlo simulation). We demonstrate the performance of the proposed method via numerical examples from portfolio risk management and input uncertainty quantification.
    
[^33]: 自监督的图形表示学习在神经元形态学中的应用

    Self-Supervised Graph Representation Learning for Neuronal Morphologies. (arXiv:2112.12482v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.12482](http://arxiv.org/abs/2112.12482)

    本文提出了GraphDINO，一种自监督的图形表示学习方法，可用于从未标记的大规模数据集中学习三维神经元形态的低维表示。该方法使用了一系列数据增强策略和新型的注意力机制AC-Attention，在多个大脑区域内，GraphDINO 显示出了优于其它最先进方法的表现。

    

    无监督的图形表示学习在多个应用领域如神经科学中吸引了越来越多的关注，其中对大脑中细胞类型的多样形态进行建模是其中的关键挑战之一。本文提出了GraphDINO，一种用于从未标记的大规模数据集中学习三维神经元形态的低维表示的数据驱动方法。GraphDINO 是一种新的基于Transformer模型的用于空间嵌入式图形表示学习的方法。为了使Transformer模型能够进行自监督学习，我们 (1)开发了针对空间嵌入式图形的数据增强策略， (2) 对位置编码进行了修改， (3)引入了新型的注意力机制AC-Attention，它结合了节点间基于注意力的全局交互和传统的图形卷积处理。我们在两个不同种类的、跨越多个大脑区域的神经元数据上测试，结果表明，GraphDINO 在神经元形态学表示学习方面表现出色，优于目前最先进的方法。研究结果表明，所提出的自监督方法可以学习到捕捉神经元形态学相关特征的表示。

    Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain
    
[^34]: 面对适应性泛化：贝叶斯视角下的研究

    Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.10761](http://arxiv.org/abs/2106.10761)

    本文提出面对自适应选择数据样本引起的过度拟合问题，使用噪声加算法可以提供不依赖于查询规模的误差保证，这一结果表明适应数据分析的问题在于新查询与过去查询的协方差。

    

    自适应选择样本可能导致过度拟合，简单的噪声加算法可以避免这一问题。本文证明了噪声加算法可以提供不依赖于查询规模的误差保证，这一结果来源于更好地理解自适应数据分析的核心问题。我们表明适应数据分析的问题在于新查询与过去查询的协方差。

    Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries. However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.  In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a 
    

