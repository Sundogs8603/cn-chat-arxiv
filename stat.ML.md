# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription.](http://arxiv.org/abs/2307.15691) | ODTlearn是一个开源的Python包，用于学习预测和处方的最优决策树。它提供了多种优化方法，并支持各种问题和算法的扩展。 |
| [^2] | [A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data.](http://arxiv.org/abs/2307.15625) | 本文比较了不同机器学习方法在车道改变意图识别中的性能，结果表明集成方法能降低分类错误的影响，而LightGBM相比XGBoost算法在模型训练效率上有显著提升。 |
| [^3] | [From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs.](http://arxiv.org/abs/2307.15496) | 本论文通过使用张量列车方法结合回归类型方法来解决高维度偏微分方程的数值逼近问题。实验结果表明，该方法在精度和计算效率之间取得了有利的折中。 |
| [^4] | [Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis.](http://arxiv.org/abs/2307.15424) | 本文综述了近期合成数据生成的深度生成模型发展，重点关注表格数据集。通过使用深度生成模型，可以有效地生成隐私敏感数据的合成数据，并解决数据归一化、隐私和评估等方面的挑战。 |
| [^5] | [Stochastic automatic differentiation for Monte Carlo processes.](http://arxiv.org/abs/2307.15406) | 本文将自动微分技术扩展到蒙特卡洛过程中，通过使用哈密顿方法代替重加权方法，实现了对期望值的导数的方差减小，为发现其他期望值导数方差减小技术提供了新思路。 |
| [^6] | [Confident Feature Ranking.](http://arxiv.org/abs/2307.15361) | 提出了一种确定性特征排序的方法，该方法通过特征重要性值的两两比较，可以产生排序和同时的置信区间，并且可以选择前k个集合。 |
| [^7] | [Med-HALT: Medical Domain Hallucination Test for Large Language Models.](http://arxiv.org/abs/2307.15343) | Med-HALT是一个新的基准和数据集，用于评估和减少大规模语言模型中医疗领域的幻觉问题。这个数据集包括多种创新的测试模式，并评估了领先的LLMs在性能上的差异。 |
| [^8] | [Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks.](http://arxiv.org/abs/2307.15285) | 本论文解决了Zonoid的最优逼近和浅层神经网络的均匀逼近两个问题。对于Zonoid的逼近，我们填补了在$d=2,3$时的对数差距，实现了在所有维度上的解决方案。对于神经网络的逼近，我们的技术在$k \geq 1$时显著提高了目前的逼近率，并能够均匀逼近目标函数及其导数。 |
| [^9] | [Is this model reliable for everyone? Testing for strong calibration.](http://arxiv.org/abs/2307.15247) | 通过重新排序观测值的预期残差，我们引入了一种新的测试程序来评估模型的强校准性能。 |
| [^10] | [PCA, SVD, and Centering of Data.](http://arxiv.org/abs/2307.15213) | 本研究详细研究了PCA方法中数据居中化步骤的影响，分析了带居中化和不带居中化的两个PCA嵌入之间的对齐性，并探讨了其与奇异向量以及均值方向之间的关系。 |
| [^11] | [A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity.](http://arxiv.org/abs/2307.15154) | 本文研究了在非稳态环境中的线性赌博机的最佳臂识别问题，提出了一种具有鲁棒性的算法来解决。该算法通过在每个时间步从一个G-最优设计中随机选择臂来实现最佳臂的鲁棒识别。 |
| [^12] | [Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions.](http://arxiv.org/abs/2307.15073) | 这篇论文提出了一种基于领域信息的先验分布模型Q-SAVI，能够解决药物发现中标签数据稀缺和特征转移的问题，并提供了一种透明且概率上合理的数据驱动模型建模方式。 |
| [^13] | [Conformal prediction for frequency-severity modeling.](http://arxiv.org/abs/2307.13124) | 这个论文提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，扩展了split conformal prediction技术到两阶段频率-严重性建模领域，并通过使用随机森林作为严重性模型，利用了袋外机制消除了校准集的需要，并实现了具有自适应宽度的预测区间的生成。 |
| [^14] | [Proximal nested sampling with data-driven priors for physical scientists.](http://arxiv.org/abs/2307.00056) | 近端嵌套抽样方法允许物理科学家应用贝叶斯模型选择于高维问题中，并展示了如何通过数据驱动先验的支持来扩展该方法。 |
| [^15] | [Automating Model Comparison in Factor Graphs.](http://arxiv.org/abs/2306.05965) | 本文基于自定义混合节点 Forney 样式的因子图消息传递，实现了高效自动化贝叶斯模型平均、选择和组合，并缩短了模型设计周期。 |
| [^16] | [Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study.](http://arxiv.org/abs/2305.11164) | 本论文通过分析Hugging Face上1,417个ML模型及相关数据集的碳足迹测量情况，提出了有关如何报告和优化ML模型的碳效率的见解和建议。 |
| [^17] | [No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations.](http://arxiv.org/abs/2305.03824) | 本文提出了一种新颖的算法，CUQB，来解决复合函数（混合模型）的高效约束全局优化问题，并取得了良好的效果，在合成和真实的应用程序中均得到了验证，包括进行了最优控制的流体流量和拓扑结构优化，后者比当前最先进的设计强2倍。 |
| [^18] | [Design-based conformal prediction.](http://arxiv.org/abs/2303.01422) | 本文介绍了基于设计的符合性预测方法，它可以生成适用于各种预测模型的预测区间，并保证有限样本覆盖率。我们邀请调查方法学家使用和贡献这些方法，通过介绍如何将其应用于复杂样本调查设计数据，并指出调查方法学家可以在其中发挥专业知识的领域。实验证明了理论保证的有效性，并通过实际数据示例展示了符合性预测在实践中的应用。 |
| [^19] | [A Definition of Non-Stationary Bandits.](http://arxiv.org/abs/2302.12202) | 该论文提出了一个解决非稳态赌博机定义歧义的新定义，解决了之前定义中的问题并修复了代理设计中探索过度的情况。 |
| [^20] | [Post-Episodic Reinforcement Learning Inference.](http://arxiv.org/abs/2302.08854) | 我们提出了一种后期情节式强化学习推断的方法，能够评估反事实的自适应策略并估计动态处理效应，通过重新加权的$Z$-估计方法稳定情节变化的估计方差。 |
| [^21] | [Consistent Range Approximation for Fair Predictive Modeling.](http://arxiv.org/abs/2212.10839) | 本文提出了一个新颖的框架，用于验证基于有偏数据训练的预测模型的公平性。通过一致范围逼近的方法，在目标人群上构建了可证明公平的预测模型，并在真实数据上展示了明显的改进。 |
| [^22] | [Shapley Curves: A Smoothing Perspective.](http://arxiv.org/abs/2211.13289) | 本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。 |
| [^23] | [A Bi-level Nonlinear Eigenvector Algorithm for Wasserstein Discriminant Analysis.](http://arxiv.org/abs/2211.11891) | 本文提出了一种基于双层非线性特征向量算法（WDA-nepv）的Wasserstein判别分析方法，通过最大化不同类别的离散度，并最小化相同类别的离散度，充分利用WDA的双层优化结构，并在自洽场框架下高效求解。 |
| [^24] | [Resource frugal optimizer for quantum machine learning.](http://arxiv.org/abs/2211.04965) | 提出了一种名为Refoqus的资源节约的量子随机梯度下降优化器，通过同时随机采样数据集和测量操作，能够保存大量资源。 |
| [^25] | [Fail-Safe Adversarial Generative Imitation Learning.](http://arxiv.org/abs/2203.01696) | 提出了一种灵活而安全的模仿学习方法，包括一个安全层，使得生成连续策略的概率密度/梯度成为闭合形式，提供了端到端的生成对抗训练和最坏情况下的安全保证。采用对抗可达性分析和利普希茨连续性等方法，通过推断行动邻域的安全性来确定一组安全行动。在实际驾驶员交互数据的实验中，展示了该方法的可行性和鲁棒性优势。 |
| [^26] | [Fairness-aware Online Price Discrimination with Nonparametric Demand Models.](http://arxiv.org/abs/2111.08221) | 本论文研究了在公平性约束下的动态歧视性定价问题，针对在线零售中存在的价格歧视问题，提出了一种公平感知的解决方法，旨在在最大化收入的同时确保公平性。 |
| [^27] | [Dynamic Silos: Increased Modularity in Intra-organizational Communication Networks during the Covid-19 Pandemic.](http://arxiv.org/abs/2104.00641) | COVID-19大流行期间，世界各地的组织在沟通上变得更加孤立，模块化增加，员工开始更加灵活地进行沟通。 |

# 详细

[^1]: ODTlearn: 一个用于学习预测和处方的最优决策树的包

    ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription. (arXiv:2307.15691v1 [stat.ML])

    [http://arxiv.org/abs/2307.15691](http://arxiv.org/abs/2307.15691)

    ODTlearn是一个开源的Python包，用于学习预测和处方的最优决策树。它提供了多种优化方法，并支持各种问题和算法的扩展。

    

    ODTLearn是一个开源的Python包，提供了基于混合整数优化(MIO)框架的高风险预测和处方任务的最优决策树学习方法。该包的当前版本提供了学习最优分类树、公平最优分类树、鲁棒最优分类树和从观测数据学习最优处方树的实现。我们设计了该包以便于维护和扩展，当引入新的最优决策树问题类、重构策略和解决算法时，可以轻松更新。为此，该包遵循面向对象的设计原则，并支持商业(Gurobi)和开源(COIN-OR branch and cut)求解器。包的文档和详细用户指南可以在https://d3m-research-group.github.io/odtlearn/找到。

    ODTLearn is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in Aghaei et al. (2019) and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at https://d3m-research-group.github.io/odtlearn/. Additionally, users can view
    
[^2]: 通过车辆轨迹数据进行车道改变意图识别的机器学习方法的比较分析

    A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data. (arXiv:2307.15625v1 [stat.ML])

    [http://arxiv.org/abs/2307.15625](http://arxiv.org/abs/2307.15625)

    本文比较了不同机器学习方法在车道改变意图识别中的性能，结果表明集成方法能降低分类错误的影响，而LightGBM相比XGBoost算法在模型训练效率上有显著提升。

    

    准确地检测和预测车道改变过程可以帮助自动驾驶汽车更好地理解周围环境，识别潜在的安全隐患，并提高交通安全性。本文主要研究车道改变过程，并比较不同机器学习方法在从高维时间序列数据中识别车道改变意图方面的性能。为了验证所提模型的性能，从CitySim数据集中提取了总共1023个车辆轨迹。对于车道改变意图识别问题，结果表明，集成方法可以将II型和III型分类错误的影响降低到98%的分类准确率。在不损失识别准确率的情况下，LightGBM相比XGBoost算法在模型训练效率上提高了6倍。

    Accurately detecting and predicting lane change (LC)processes can help autonomous vehicles better understand their surrounding environment, recognize potential safety hazards, and improve traffic safety. This paper focuses on LC processes and compares different machine learning methods' performance to recognize LC intention from high-dimensionality time series data. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. For LC intention recognition issues, the results indicate that with ninety-eight percent of classification accuracy, ensemble methods reduce the impact of Type II and Type III classification errors. Without sacrificing recognition accuracy, the LightGBM demonstrates a sixfold improvement in model training efficiency than the XGBoost algorithm.
    
[^3]: 从连续时间表述到离散化方案：张量列车和鲁棒回归用于BSDEs和抛物线PDEs

    From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs. (arXiv:2307.15496v1 [cs.LG])

    [http://arxiv.org/abs/2307.15496](http://arxiv.org/abs/2307.15496)

    本论文通过使用张量列车方法结合回归类型方法来解决高维度偏微分方程的数值逼近问题。实验结果表明，该方法在精度和计算效率之间取得了有利的折中。

    

    在高维度中数值逼近偏微分方程（PDEs）面临着巨大的挑战，因为传统的基于网格的方法受到所谓维数诅咒的限制。最近的尝试依赖于蒙特卡罗方法和变分表述的组合，利用神经网络进行函数逼近。延续之前的工作（Richter等，2021），我们认为张量列车为抛物型PDE提供了一种吸引人的框架：通过以逆向随机微分方程和回归类型方法的形式重新表述，结合潜在的低秩结构，有望实现压缩和高效计算的目标。强调连续时间观点，我们开发了迭代方案，其在计算效率和鲁棒性方面有所不同。我们从理论和数值上都证明了我们的方法能够在精度和计算效率之间取得有利的折中。

    The numerical approximation of partial differential equations (PDEs) poses formidable challenges in high dimensions since classical grid-based methods suffer from the so-called curse of dimensionality. Recent attempts rely on a combination of Monte Carlo methods and variational formulations, using neural networks for function approximation. Extending previous work (Richter et al., 2021), we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation. Emphasizing a continuous-time viewpoint, we develop iterative schemes, which differ in terms of computational efficiency and robustness. We demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency. While previous 
    
[^4]: 深度生成模型、合成表格数据和差分隐私：综述与综合

    Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis. (arXiv:2307.15424v1 [cs.LG])

    [http://arxiv.org/abs/2307.15424](http://arxiv.org/abs/2307.15424)

    本文综述了近期合成数据生成的深度生成模型发展，重点关注表格数据集。通过使用深度生成模型，可以有效地生成隐私敏感数据的合成数据，并解决数据归一化、隐私和评估等方面的挑战。

    

    本文全面综述了通过深度生成模型生成合成数据的最新发展，重点关注表格数据集。我们特别概述了在隐私敏感数据背景下合成数据生成的重要性。此外，我们强调了使用深度生成模型相对于其他方法的优势，并详细解释了包括无监督学习、神经网络和生成模型在内的基本概念。论文涵盖了在使用深度生成模型处理表格数据集时涉及的挑战和考虑因素，如数据归一化、隐私问题和模型评估。本综述为对合成数据生成及其应用感兴趣的研究人员和实践者提供了宝贵的资源。

    This article provides a comprehensive synthesis of the recent developments in synthetic data generation via deep generative models, focusing on tabular datasets. We specifically outline the importance of synthetic data generation in the context of privacy-sensitive data. Additionally, we highlight the advantages of using deep generative models over other methods and provide a detailed explanation of the underlying concepts, including unsupervised learning, neural networks, and generative models. The paper covers the challenges and considerations involved in using deep generative models for tabular datasets, such as data normalization, privacy concerns, and model evaluation. This review provides a valuable resource for researchers and practitioners interested in synthetic data generation and its applications.
    
[^5]: 蒙特卡洛过程的随机自动微分

    Stochastic automatic differentiation for Monte Carlo processes. (arXiv:2307.15406v1 [hep-lat])

    [http://arxiv.org/abs/2307.15406](http://arxiv.org/abs/2307.15406)

    本文将自动微分技术扩展到蒙特卡洛过程中，通过使用哈密顿方法代替重加权方法，实现了对期望值的导数的方差减小，为发现其他期望值导数方差减小技术提供了新思路。

    

    蒙特卡洛方法是计算机科学的一个基石，它们能够高效地采样高维分布函数。本文考虑将自动微分（AD）技术扩展到蒙特卡洛过程，解决获取期望值的导数（以及泰勒级数）的问题。借鉴晶格场论社区的思想，我们考察了两种方法。一种是基于重加权的方法，另一种是扩展了混合蒙特卡洛（HMC）和类似算法中通常使用的哈密顿方法。我们展示了哈密顿方法可以被理解为重加权方法的变量变换，从而大大减小了泰勒级数的系数的方差。这项工作为获取期望值导数的其他方差减小技术开辟了新的思路。

    Monte Carlo methods represent a cornerstone of computer science. They allow to sample high dimensional distribution functions in an efficient way. In this paper we consider the extension of Automatic Differentiation (AD) techniques to Monte Carlo process, addressing the problem of obtaining derivatives (and in general, the Taylor series) of expectation values. Borrowing ideas from the lattice field theory community, we examine two approaches. One is based on reweighting while the other represents an extension of the Hamiltonian approach typically used by the Hybrid Monte Carlo (HMC) and similar algorithms. We show that the Hamiltonian approach can be understood as a change of variables of the reweighting approach, resulting in much reduced variances of the coefficients of the Taylor series. This work opens the door to find other variance reduction techniques for derivatives of expectation values.
    
[^6]: 确定性特征排序

    Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])

    [http://arxiv.org/abs/2307.15361](http://arxiv.org/abs/2307.15361)

    提出了一种确定性特征排序的方法，该方法通过特征重要性值的两两比较，可以产生排序和同时的置信区间，并且可以选择前k个集合。

    

    特征重要性的解释通常依赖于特征的相对顺序而不是数值本身，也就是排序。然而，由于计算重要性值时使用的样本量较小，排序可能不稳定。我们提出了一种事后重要性方法，可以产生一种排序和同时的置信区间。基于特征重要性值的两两比较，我们的方法可以保证高概率包含“真实”（无限样本）排序，并允许选择前k个集合。

    Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
    
[^7]: Med-HALT:大规模语言模型中医疗领域幻觉测试

    Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])

    [http://arxiv.org/abs/2307.15343](http://arxiv.org/abs/2307.15343)

    Med-HALT是一个新的基准和数据集，用于评估和减少大规模语言模型中医疗领域的幻觉问题。这个数据集包括多种创新的测试模式，并评估了领先的LLMs在性能上的差异。

    

    本研究论文关注大规模语言模型（LLMs）中幻觉问题的挑战，特别是在医疗领域的背景下。幻觉指这些模型生成了合理但未经验证或错误的信息，这可能对医疗应用产生严重影响。我们提出了一个新的基准和数据集，Med-HALT（医疗领域幻觉测试），专门设计用于评估和减少幻觉。Med-HALT提供了一个多元化的跨国数据集，这些数据集来自不同国家的医疗检查，包括多种创新的测试模式。Med-HALT包括两类测试：推理和基于记忆的幻觉测试，旨在评估LLMs的问题解决和信息检索能力。我们的研究评估了文本Davinci，GPT-3.5，LlaMa-2，MPT和Falcon等领先的LLMs，揭示了它们在性能上的显著差异。这篇论文提供了有关数据集的详细见解，促进了进一步的研究和发展。

    This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
    
[^8]: Zonoid的最优逼近和浅层神经网络的均匀逼近

    Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])

    [http://arxiv.org/abs/2307.15285](http://arxiv.org/abs/2307.15285)

    本论文解决了Zonoid的最优逼近和浅层神经网络的均匀逼近两个问题。对于Zonoid的逼近，我们填补了在$d=2,3$时的对数差距，实现了在所有维度上的解决方案。对于神经网络的逼近，我们的技术在$k \geq 1$时显著提高了目前的逼近率，并能够均匀逼近目标函数及其导数。

    

    我们研究了以下两个相关问题。第一个问题是确定一个任意的在$\mathbb{R}^{d+1}$空间中的Zonoid可以通过$n$个线段的Hausdorff距离来逼近的误差。第二个问题是确定浅层ReLU$^k$神经网络在其变分空间中的均匀范数的最优逼近率。第一个问题已经在$d \neq 2, 3$时得到解决，但当$d = 2, 3$时，最优上界和最优下界之间仍存在一个对数差距。我们填补了这个差距，完成了所有维度上的解决方案。对于第二个问题，我们的技术在$k \geq 1$时显著提高了现有的逼近率，并实现了目标函数及其导数的均匀逼近。

    We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
    
[^9]: 这个模型对每个人都可靠吗？测试强校准

    Is this model reliable for everyone? Testing for strong calibration. (arXiv:2307.15247v1 [cs.LG])

    [http://arxiv.org/abs/2307.15247](http://arxiv.org/abs/2307.15247)

    通过重新排序观测值的预期残差，我们引入了一种新的测试程序来评估模型的强校准性能。

    

    在一个校准良好的风险预测模型中，对于任何给定的子群体，平均预测概率与真实事件率接近。这样的模型适用于异质人群，并满足强算法公平性的概念。然而，对于强校准，对模型进行审核是一个已知困难的任务，特别是对于机器学习算法来说，由于潜在的子群体数量庞大。因此，常见做法是只根据少数预定义的子群体评估校准。最近在拟合度检验方面的发展提供了潜在的解决方案，但对于信号较弱或校准不良的子群体较小的情况，这些方法要么过度细分数据，要么根本不进行细分。我们引入了一种新的测试过程，基于以下洞察：如果我们能够按预期的残差对观测进行重新排序，预测值和观察值之间的关联性应该会发生变化。

    In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed resi
    
[^10]: PCA、SVD和数据居中化

    PCA, SVD, and Centering of Data. (arXiv:2307.15213v1 [stat.ME])

    [http://arxiv.org/abs/2307.15213](http://arxiv.org/abs/2307.15213)

    本研究详细研究了PCA方法中数据居中化步骤的影响，分析了带居中化和不带居中化的两个PCA嵌入之间的对齐性，并探讨了其与奇异向量以及均值方向之间的关系。

    

    本文详细研究了主成分分析（PCA），这是统计学和机器学习中常用的降维方法。奇异值分解（SVD）通常被用作计算PCA的主要方法，这个过程中必不可少地包含了数据居中化的步骤，即从数据集中减去均值位置。在我们的研究中，我们深入探讨了这个关键但常常被忽视或轻视的数据居中化步骤的影响。我们的研究精细地研究了在什么条件下，从带有居中化的SVD和不带居中化的SVD得到的两个PCA嵌入可以看作是对齐的。作为这个探索的一部分，我们分析了第一个奇异向量和均值方向之间的关系，随后将这一观察结果与中心化和非中心化矩阵的两个SVD之间的一致性联系起来。此外，我们还探讨了可能产生的相关影响。

    The research detailed in this paper scrutinizes Principal Component Analysis (PCA), a seminal method employed in statistics and machine learning for the purpose of reducing data dimensionality. Singular Value Decomposition (SVD) is often employed as the primary means for computing PCA, a process that indispensably includes the step of centering - the subtraction of the mean location from the data set. In our study, we delve into a detailed exploration of the influence of this critical yet often ignored or downplayed data centering step. Our research meticulously investigates the conditions under which two PCA embeddings, one derived from SVD with centering and the other without, can be viewed as aligned. As part of this exploration, we analyze the relationship between the first singular vector and the mean direction, subsequently linking this observation to the congruity between two SVDs of centered and uncentered matrices. Furthermore, we explore the potential implications arising fro
    
[^11]: A/B测试和具有非稳态鲁棒性的线性赌博机最佳臂识别问题

    A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity. (arXiv:2307.15154v1 [cs.LG])

    [http://arxiv.org/abs/2307.15154](http://arxiv.org/abs/2307.15154)

    本文研究了在非稳态环境中的线性赌博机的最佳臂识别问题，提出了一种具有鲁棒性的算法来解决。该算法通过在每个时间步从一个G-最优设计中随机选择臂来实现最佳臂的鲁棒识别。

    

    本文研究了在可能存在非稳态环境下的线性赌博机中的固定预算最佳臂识别问题。给定有限臂集合X，固定预算T以及不可预测的参数序列θ，算法的目标是以尽可能高的概率正确识别最佳臂x*。之前的工作已经在稳态设置下进行了研究，并且证明了错误概率随着预算的增加而指数下降。但在许多现实世界的A/B/n多变量测试场景中，环境是非稳态的，而一个期望稳态的算法很容易失败。为了具有鲁棒的识别能力，众所周知，如果在每个时间步从X的一个G-最优设计中以随机和非自适应的方式选择臂，那么可以实现最佳臂的鲁棒识别。

    We investigate the fixed-budget best-arm identification (BAI) problem for linear bandits in a potentially non-stationary environment. Given a finite arm set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an algorithm will aim to correctly identify the best arm $x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as high as possible. Prior work has addressed the stationary setting where $\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But in many real-world $A/B/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. For robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a G-optimal design over $\mathcal{X}$ at each 
    
[^12]: 在特征转移条件下利用基于领域信息的先验分布进行药物发现

    Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions. (arXiv:2307.15073v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.15073](http://arxiv.org/abs/2307.15073)

    这篇论文提出了一种基于领域信息的先验分布模型Q-SAVI，能够解决药物发现中标签数据稀缺和特征转移的问题，并提供了一种透明且概率上合理的数据驱动模型建模方式。

    

    加速发现新型和更有效的治疗药物是一个重要的制药问题，深度学习在其中扮演着愈发重要的角色。然而，真实世界的药物发现任务通常特点是标签数据稀缺和显著的特征转移，这对于标准的深度学习方法构成了挑战。在本论文中，我们提出了Q-SAVI，这是一个概率模型，能够通过将数据生成过程的显式先验知识编码为函数的先验分布，为研究人员提供了一种透明且概率上合理的方式来编码数据驱动的建模偏好。基于一个新颖的，高标准的生物活性数据集，使得在外推模式下能够进行有意义的模型比较，我们探索了不同的方法来诱导数据转移并构建一个具有挑战性的评估环境。然后我们证明了使用Q-SAVI可以在特征转移条件下更好地进行药物发现。

    Accelerating the discovery of novel and more effective therapeutics is an important pharmaceutical problem in which deep learning is playing an increasingly significant role. However, real-world drug discovery tasks are often characterized by a scarcity of labeled data and significant covariate shift$\unicode{x2013}\unicode{x2013}$a setting that poses a challenge to standard deep learning methods. In this paper, we present Q-SAVI, a probabilistic model able to address these challenges by encoding explicit prior knowledge of the data-generating process into a prior distribution over functions, presenting researchers with a transparent and probabilistically principled way to encode data-driven modeling preferences. Building on a novel, gold-standard bioactivity dataset that facilitates a meaningful comparison of models in an extrapolative regime, we explore different approaches to induce data shift and construct a challenging evaluation setup. We then demonstrate that using Q-SAVI to int
    
[^13]: 频率-严重性建模的符合性预测

    Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])

    [http://arxiv.org/abs/2307.13124](http://arxiv.org/abs/2307.13124)

    这个论文提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，扩展了split conformal prediction技术到两阶段频率-严重性建模领域，并通过使用随机森林作为严重性模型，利用了袋外机制消除了校准集的需要，并实现了具有自适应宽度的预测区间的生成。

    

    我们提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，将分割符合性预测技术扩展到两阶段频率-严重性建模领域。通过模拟和真实数据集展示了该框架的有效性。当基础严重性模型是随机森林时，我们扩展了两阶段分割符合性预测过程，展示了如何利用袋外机制消除校准集的需要，并实现具有自适应宽度的预测区间的生成。

    We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
    
[^14]: 用于物理科学家的基于数据驱动先验的近端嵌套抽样方法

    Proximal nested sampling with data-driven priors for physical scientists. (arXiv:2307.00056v1 [stat.ME])

    [http://arxiv.org/abs/2307.00056](http://arxiv.org/abs/2307.00056)

    近端嵌套抽样方法允许物理科学家应用贝叶斯模型选择于高维问题中，并展示了如何通过数据驱动先验的支持来扩展该方法。

    

    最近引入了近端嵌套抽样方法，以开辟贝叶斯模型选择在高维问题中的应用，例如计算成像。该框架适用于具有对数凸似然函数的模型，这在成像科学中非常普遍。本文有两个目的。首先，我们以教学的方式对近端嵌套抽样方法进行综述，以努力为物理科学家解释该框架。其次，我们展示了近端嵌套抽样方法如何在经验贝叶斯设置中扩展，以支持数据驱动的先验，如从训练数据中学习的深度神经网络。

    Proximal nested sampling was introduced recently to open up Bayesian model selection for high-dimensional problems such as computational imaging. The framework is suitable for models with a log-convex likelihood, which are ubiquitous in the imaging sciences. The purpose of this article is two-fold. First, we review proximal nested sampling in a pedagogical manner in an attempt to elucidate the framework for physical scientists. Second, we show how proximal nested sampling can be extended in an empirical Bayes setting to support data-driven priors, such as deep neural networks learned from training data.
    
[^15]: 在因子图中自动进行模型比较

    Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])

    [http://arxiv.org/abs/2306.05965](http://arxiv.org/abs/2306.05965)

    本文基于自定义混合节点 Forney 样式的因子图消息传递，实现了高效自动化贝叶斯模型平均、选择和组合，并缩短了模型设计周期。

    

    在文献中，贝叶斯状态和参数估计已经被有效自动化，但对于模型比较尚未如此，因此仍需要容易出错和耗时的手动推导。因此，模型比较经常被忽视和忽略，尽管它很重要。本文通过在Forney样式的因子图上使用自定义混合节点上的消息传递来高效地自动化贝叶斯模型平均、选择和组合。进而可使用缩放因子同时执行参数和状态推断以及模型比较。这种方法缩短了模型设计周期，同时允许简单地扩展到分层和时间模型先验，以适应建模复杂的时变过程。

    Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
    
[^16]: 探索抱抱脸ML模型的碳足迹：一项存储库挖掘研究

    Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study. (arXiv:2305.11164v1 [cs.LG])

    [http://arxiv.org/abs/2305.11164](http://arxiv.org/abs/2305.11164)

    本论文通过分析Hugging Face上1,417个ML模型及相关数据集的碳足迹测量情况，提出了有关如何报告和优化ML模型的碳效率的见解和建议。

    

    机器学习(ML)系统的崛起加剧了它们的碳足迹，这是由于其增加的能力和模型大小所致。然而，目前对ML模型的碳足迹如何实际测量、报告和评估的认识相对较少。因此，本论文旨在分析在Hugging Face上1,417个ML模型和相关数据集的碳足迹测量情况，Hugging Face是最受欢迎的预训练ML模型的存储库。目标是提供有关如何报告和优化ML模型的碳效率的见解和建议。该研究包括Hugging Face Hub API上有关碳排放的第一项存储库挖掘研究。本研究旨在回答两个研究问题：(1) ML模型的创建者如何在Hugging Face Hub上测量和报告碳排放？(2) 哪些方面影响了训练ML模型的碳排放？该研究得出了几个关键发现。其中包括碳排放报告模式比例的逐步下降等。

    The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models. The goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. The study includes the first repository mining study on the Hugging Face Hub API on carbon emissions. This study seeks to answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? The study yielded several key findings. These include a decreasing proportion of carbon emissions-reporting mode
    
[^17]: 无遗憾的约束贝叶斯优化方法用于带有噪声和昂贵混合模型的差分分位数函数逼近

    No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations. (arXiv:2305.03824v1 [stat.ML])

    [http://arxiv.org/abs/2305.03824](http://arxiv.org/abs/2305.03824)

    本文提出了一种新颖的算法，CUQB，来解决复合函数（混合模型）的高效约束全局优化问题，并取得了良好的效果，在合成和真实的应用程序中均得到了验证，包括进行了最优控制的流体流量和拓扑结构优化，后者比当前最先进的设计强2倍。

    

    本文研究了复合函数（混合模型）的高效约束全局优化问题，该模型的输入是具有矢量值输出和有噪声观测的昂贵黑盒函数，这在实际的科学、工程、制造和控制应用中经常出现。我们提出了一种新颖的算法Constrained Upper Quantile Bound（CUQB），用于解决这种问题，直接利用了我们展示的目标和约束函数的复合结构，从而大大提高了采样效率。CUQB的概念简单，避免了先前方法所使用的约束逼近。虽然CUQB的收购函数不在封闭形式中，但我们提出了一种新颖的可微随机逼近，使其能够有效地最大化。我们进一步得出了对于累积遗憾和约束违规的界限。由于在某些规则假设下这些界限对迭代次数具有次线性依赖性，因此我们的算法在渐近意义下无遗憾并满足约束条件。我们在几个合成和真实的应用程序中展示了CUQB的功效，包括桥架拓扑 - 在其中，我们发现的结构比当前最先进的设计强2倍 - 以及流体流量的最优控制，其中我们使用的方法比以前的方法少了3倍的模拟。

    This paper investigates the problem of efficient constrained global optimization of composite functions (hybrid models) whose input is an expensive black-box function with vector-valued outputs and noisy observations, which often arises in real-world science, engineering, manufacturing, and control applications. We propose a novel algorithm, Constrained Upper Quantile Bound (CUQB), to solve such problems that directly exploits the composite structure of the objective and constraint functions that we show leads substantially improved sampling efficiency. CUQB is conceptually simple and avoids the constraint approximations used by previous methods. Although the CUQB acquisition function is not available in closed form, we propose a novel differentiable stochastic approximation that enables it to be efficiently maximized. We further derive bounds on the cumulative regret and constraint violation. Since these bounds depend sublinearly on the number of iterations under some regularity assum
    
[^18]: 基于设计的符合性预测

    Design-based conformal prediction. (arXiv:2303.01422v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.01422](http://arxiv.org/abs/2303.01422)

    本文介绍了基于设计的符合性预测方法，它可以生成适用于各种预测模型的预测区间，并保证有限样本覆盖率。我们邀请调查方法学家使用和贡献这些方法，通过介绍如何将其应用于复杂样本调查设计数据，并指出调查方法学家可以在其中发挥专业知识的领域。实验证明了理论保证的有效性，并通过实际数据示例展示了符合性预测在实践中的应用。

    

    符合性预测是一种不依赖于假设的方法，用于生成无分布的预测区间或集合，适用于几乎任何预测模型，并且具有保证的有限样本覆盖率。符合性方法在统计和机器学习中是一个活跃的研究主题，但近期才被扩展到非可交换数据上。在本文中，我们邀请调查方法学家开始使用和贡献符合性方法。我们介绍了如何将符合性预测应用于几种常见的复杂样本调查设计数据上，通过有限总体设计推断的框架，同时指出调查方法学家可以有益地应用其专业知识的领域。我们的模拟实验从经验上验证了有限样本覆盖率的理论保证，我们的实际数据示例展示了符合性预测如何在实践中应用于复杂样本调查数据。

    Conformal prediction is an assumption-lean approach to generating distribution-free prediction intervals or sets, for nearly arbitrary predictive models, with guaranteed finite-sample coverage. Conformal methods are an active research topic in statistics and machine learning, but only recently have they been extended to non-exchangeable data. In this paper, we invite survey methodologists to begin using and contributing to conformal methods. We introduce how conformal prediction can be applied to data from several common complex sample survey designs, under a framework of design-based inference for a finite population, and we point out gaps where survey methodologists could fruitfully apply their expertise. Our simulations empirically bear out the theoretical guarantees of finite-sample coverage, and our real-data example demonstrates how conformal prediction can be applied to complex sample survey data in practice.
    
[^19]: 非稳态赌博机的定义

    A Definition of Non-Stationary Bandits. (arXiv:2302.12202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12202](http://arxiv.org/abs/2302.12202)

    该论文提出了一个解决非稳态赌博机定义歧义的新定义，解决了之前定义中的问题并修复了代理设计中探索过度的情况。

    

    尽管非稳态赌博机学习的课题吸引了最近的很多关注，但我们还没有找到一个能够区分非稳态赌博机和稳态赌博机的形式定义。之前的研究将非稳态赌博机定义为奖励分布随时间变化的赌博机。我们证明这个定义在将同一个赌博机同时分类为稳态和非稳态时存在歧义；这种歧义源于该定义对潜在奖励分布序列的依赖。此外，这个定义也导致了两种广泛使用的遗憾概念：动态遗憾和弱遗憾。这些概念在一些赌博机中并不能准确地反映代理的性能。此外，这个非稳态赌博机的定义还导致了探索过度的代理设计。我们引入了一个解决这些问题的非稳态赌博机的形式定义。

    Despite the subject of non-stationary bandit learning having attracted much recent attention, we have yet to identify a formal definition of non-stationarity that can consistently distinguish non-stationary bandits from stationary ones. Prior work has characterized non-stationary bandits as bandits for which the reward distribution changes over time. We demonstrate that this definition can ambiguously classify the same bandit as both stationary and non-stationary; this ambiguity arises in the existing definition's dependence on the latent sequence of reward distributions. Moreover, the definition has given rise to two widely used notions of regret: the dynamic regret and the weak regret. These notions are not indicative of qualitative agent performance in some bandits. Additionally, this definition of non-stationary bandits has led to the design of agents that explore excessively. We introduce a formal definition of non-stationary bandits that resolves these issues. Our new definition 
    
[^20]: 后期情节式强化学习推断

    Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08854](http://arxiv.org/abs/2302.08854)

    我们提出了一种后期情节式强化学习推断的方法，能够评估反事实的自适应策略并估计动态处理效应，通过重新加权的$Z$-估计方法稳定情节变化的估计方差。

    

    我们考虑从情节式强化学习算法收集的数据进行估计和推断；即在每个时期（也称为情节）以顺序方式与单个受试单元多次交互的自适应试验算法。我们的目标是在收集数据后能够评估反事实的自适应策略，并估计结构参数，如动态处理效应，这可以用于信用分配（例如，第一个时期的行动对最终结果的影响）。这些感兴趣的参数可以构成矩方程的解，但不是总体损失函数的最小化器，在静态数据情况下导致了$Z$-估计方法。然而，这样的估计量在自适应数据收集的情况下不能渐近正态。我们提出了一种重新加权的$Z$-估计方法，使用精心设计的自适应权重来稳定情节变化的估计方差，这是由非...

    We consider estimation and inference with data collected from episodic reinforcement learning (RL) algorithms; i.e. adaptive experimentation algorithms that at each period (aka episode) interact multiple times in a sequential manner with a single treated unit. Our goal is to be able to evaluate counterfactual adaptive policies after data collection and to estimate structural parameters such as dynamic treatment effects, which can be used for credit assignment (e.g. what was the effect of the first period action on the final outcome). Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to $Z$-estimation approaches in the case of static data. However, such estimators fail to be asymptotically normal in the case of adaptive data collection. We propose a re-weighted $Z$-estimation approach with carefully designed adaptive weights to stabilize the episode-varying estimation variance, which results from the non
    
[^21]: 公平预测建模的一种一致范围逼近方法

    Consistent Range Approximation for Fair Predictive Modeling. (arXiv:2212.10839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10839](http://arxiv.org/abs/2212.10839)

    本文提出了一个新颖的框架，用于验证基于有偏数据训练的预测模型的公平性。通过一致范围逼近的方法，在目标人群上构建了可证明公平的预测模型，并在真实数据上展示了明显的改进。

    

    本文提出了一个新颖的框架，用于验证基于有偏数据训练的预测模型的公平性。它借鉴了对不完整和不一致数据库的查询回答，以形式化公平查询在目标人群上的一致范围逼近（CRA）问题。该框架利用数据收集过程和有偏数据的背景知识，可以在有限的目标人群统计数据的情况下，计算公平查询的答案范围。通过CRA，该框架构建的预测模型可以在目标人群上获得可证明的公平性，而不受训练过程中外部数据的可用性限制。通过对真实数据的评估，验证了该框架的有效性，显示出明显优于现有最先进方法的改进。

    This paper proposes a novel framework for certifying the fairness of predictive models trained on biased data. It draws from query answering for incomplete and inconsistent databases to formulate the problem of consistent range approximation (CRA) of fairness queries for a predictive model on a target population. The framework employs background knowledge of the data collection process and biased data, working with or without limited statistics about the target population, to compute a range of answers for fairness queries. Using CRA, the framework builds predictive models that are certifiably fair on the target population, regardless of the availability of external data during training. The framework's efficacy is demonstrated through evaluations on real data, showing substantial improvement over existing state-of-the-art methods.
    
[^22]: Shapley曲线：一种平滑视角

    Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.13289](http://arxiv.org/abs/2211.13289)

    本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。

    

    源自合作博弈理论，Shapley值已成为应用机器学习中最广泛使用的变量重要性度量之一。然而，对Shapley值的统计理解仍然有限。本文以非参数(或平滑)的角度，引入Shapley曲线作为局部变量重要性的度量。我们提出了两种估计策略，并在特征独立和依赖的情况下都得出了一致性和渐近正态性。这样，我们可以构建置信区间并对估计的Shapley曲线进行推断。我们提出了一种新颖的野蛮引导程序版本，专门调整以获得Shapley曲线的良好有限样本覆盖。渐近结果在大量实验证实了。在实证应用中，我们分析了哪些属性驱动了车辆的价格。

    Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
    
[^23]: 基于双层非线性特征向量算法的Wasserstein判别分析

    A Bi-level Nonlinear Eigenvector Algorithm for Wasserstein Discriminant Analysis. (arXiv:2211.11891v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.11891](http://arxiv.org/abs/2211.11891)

    本文提出了一种基于双层非线性特征向量算法（WDA-nepv）的Wasserstein判别分析方法，通过最大化不同类别的离散度，并最小化相同类别的离散度，充分利用WDA的双层优化结构，并在自洽场框架下高效求解。

    

    如同经典的Fisher线性判别分析（LDA），最近提出的Wasserstein判别分析（WDA）是一种线性降维方法，通过双层优化来寻求一个投影矩阵，最大化不同数据类别的离散度，并最小化相同数据类别的离散度。与LDA不同，WDA利用最优传输的基本原理，可以考虑数据类别之间的全局和局部相互关联关系。本文提出了一种基于双层非线性特征向量算法（WDA-nepv）来充分利用WDA的双层优化结构。WDA-nepv的内部层用于计算最优传输矩阵，并被构造为一个依赖于特征向量的非线性特征值问题（NEPv），同时，外部层用于追踪比率优化，并被构造为另一个NEPv问题。这两个NEPv问题可以在自洽场（SCF）框架下高效计算。WDA-nepv是可导的。

    Much like the classical Fisher linear discriminant analysis (LDA), the recently proposed Wasserstein discriminant analysis (WDA) is a linear dimensionality reduction method that seeks a projection matrix to maximize the dispersion of different data classes and minimize the dispersion of same data classes via a bi-level optimization. In contrast to LDA, WDA can account for both global and local interconnections between data classes by using the underlying principles of optimal transport. In this paper, a bi-level nonlinear eigenvector algorithm (WDA-nepv) is presented to fully exploit the structures of the bi-level optimization of WDA. The inner level of WDA-nepv for computing the optimal transport matrices is formulated as an eigenvector-dependent nonlinear eigenvalue problem (NEPv), and meanwhile, the outer level for trace ratio optimizations is formulated as another NEPv. Both NEPvs can be computed efficiently under the self-consistent field (SCF) framework. WDA-nepv is derivative-fr
    
[^24]: 资源节约的量子机器学习优化器

    Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.04965](http://arxiv.org/abs/2211.04965)

    提出了一种名为Refoqus的资源节约的量子随机梯度下降优化器，通过同时随机采样数据集和测量操作，能够保存大量资源。

    

    量子增强的数据科学，也称为量子机器学习（QML），作为近期量子计算机的应用越来越受关注。变分QML算法在涉及量子数据时有能力解决实际问题。然而，训练这些算法可能具有挑战性，并需要定制的优化程序。具体而言，QML应用可能需要大量的采样次数，因为涉及大型数据集。在这项工作中，我们提倡对数据集和定义损失函数的测量操作进行同时随机采样。我们考虑了一个高度通用的损失函数，包括了许多QML应用，并展示了如何构建其梯度的无偏估计器。这使我们能够提出一种称为Refoqus（资源节约的量子随机梯度下降优化器）的节约采样梯度下降优化器。我们的数值结果表明，Refoqus能够节省几个数量级的资源。

    Quantum-enhanced data science, also known as quantum machine learning (QML), is of growing interest as an application of near-term quantum computers. Variational QML algorithms have the potential to solve practical problems on real hardware, particularly when involving quantum data. However, training these algorithms can be challenging and calls for tailored optimization procedures. Specifically, QML applications can require a large shot-count overhead due to the large datasets involved. In this work, we advocate for simultaneous random sampling over both the dataset as well as the measurement operators that define the loss function. We consider a highly general loss function that encompasses many QML applications, and we show how to construct an unbiased estimator of its gradient. This allows us to propose a shot-frugal gradient descent optimizer called Refoqus (REsource Frugal Optimizer for QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can save several orde
    
[^25]: 安全可靠的对抗生成模仿学习

    Fail-Safe Adversarial Generative Imitation Learning. (arXiv:2203.01696v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01696](http://arxiv.org/abs/2203.01696)

    提出了一种灵活而安全的模仿学习方法，包括一个安全层，使得生成连续策略的概率密度/梯度成为闭合形式，提供了端到端的生成对抗训练和最坏情况下的安全保证。采用对抗可达性分析和利普希茨连续性等方法，通过推断行动邻域的安全性来确定一组安全行动。在实际驾驶员交互数据的实验中，展示了该方法的可行性和鲁棒性优势。

    

    为了实现灵活而安全的模仿学习（IL），我们提出了一种理论和模块化方法，其中包括一个安全层，该层能够使安全生成连续策略的概率密度/梯度成为闭合形式，并提供端到端的生成对抗训练和最坏情况下的安全保证。安全层将所有行动映射到一组安全行动，并使用变量转换公式和度量的可加性来计算密度。通过对回退操作的对抗可达性分析，我们首先检查有限样本的行动安全性，然后通过利普希茨连续性等方法来推断这些行动邻域的安全性。我们提供了理论分析，表明与仅在测试时使用安全层（最多二次误差）相比，在训练过程中使用安全层的鲁棒性优势（模仿误差与时间序列线性相关）。在实际驾驶员交互数据的实验中，我们经验性地证明了该方法的可行性。

    For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, 
    
[^26]: 具有非参数需求模型的公平感知在线价格歧视

    Fairness-aware Online Price Discrimination with Nonparametric Demand Models. (arXiv:2111.08221v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08221](http://arxiv.org/abs/2111.08221)

    本论文研究了在公平性约束下的动态歧视性定价问题，针对在线零售中存在的价格歧视问题，提出了一种公平感知的解决方法，旨在在最大化收入的同时确保公平性。

    

    价格歧视是在网上零售中广泛使用的一种策略，指的是为不同的客户群体设定不同的价格。尽管它有助于提高网上零售商的收入，但可能引发关于公平性的严重担忧，甚至违反规定和法律。本文研究了在公平性约束下的动态歧视性定价问题。具体而言，我们考虑了一个有限销售周期长度为T的单一产品，有两组客户。每组客户都有其未知的需求函数需要学习。对于每个销售周期，卖家确定每组的价格并观察其购买行为。虽然现有文献主要关注最大化收入，但在动态定价文献中尚未充分探讨确保不同客户之间的公平性。本研究采用了Cohen等人（2022）的公平概念。对于价格公平性，我们提出了一种最优动态方案。

    Price discrimination, which refers to the strategy of setting different prices for different customer groups, has been widely used in online retailing. Although it helps boost the collected revenue for online retailers, it might create serious concerns about fairness, which even violates the regulation and laws. This paper studies the problem of dynamic discriminatory pricing under fairness constraints. In particular, we consider a finite selling horizon of length $T$ for a single product with two groups of customers. Each group of customers has its unknown demand function that needs to be learned. For each selling period, the seller determines the price for each group and observes their purchase behavior. While existing literature mainly focuses on maximizing revenue, ensuring fairness among different customers has not been fully explored in the dynamic pricing literature. This work adopts the fairness notion from Cohen et al. (2022). For price fairness, we propose an optimal dynamic 
    
[^27]: 动态沟通网络：COVID-19大流行期间组织内部沟通网络中的模块化增加

    Dynamic Silos: Increased Modularity in Intra-organizational Communication Networks during the Covid-19 Pandemic. (arXiv:2104.00641v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2104.00641](http://arxiv.org/abs/2104.00641)

    COVID-19大流行期间，世界各地的组织在沟通上变得更加孤立，模块化增加，员工开始更加灵活地进行沟通。

    

    COVID-19，相关的居家办公政策和远程工作的兴起，极大地改变了世界各地的工作场所沟通方式。为了了解这些变化，我们分析了来自全球4361个组织的3600亿封电子邮件的聚合、匿名化元数据。通过比较每月和年度的指标，我们对COVID-19之前和之后24个月内网络社群结构的变化进行了研究。我们还研究了单个全球组织内多种沟通媒体（电子邮件、即时消息、视频通话和日历软件）的变化，并将其与由于组织结构改变而推动的沟通变化进行了比较。我们发现，在2020年，世界各地的组织比2019年更加孤立，表现为增加的模块化。这一转变与模块内稳定性的降低同时发生。我们的分析共同表明，COVID-19爆发后，员工开始更加灵活地进行沟通。

    Workplace communications around the world were drastically altered by Covid-19, related work-from-home orders, and the rise of remote work. To understand these shifts, we analyzed aggregated, anonymized metadata from over 360 billion emails within 4,361 organizations worldwide. By comparing month-to-month and year-over-year metrics, we examined changes in network community structures over 24 months before and after Covid-19. We also examined shifts across multiple communication media (email, instant messages, video calls, and calendaring software) within a single global organization, and compared them to communications shifts that were driven by changes in formal organizational structure. We found that, in 2020, organizations around the world became more siloed than in 2019, evidenced by increased modularity. This shift was concurrent with decreased stability within silos. Collectively, our analyses indicate that following the onset of Covid-19, employees began to shift more dynamicall
    

