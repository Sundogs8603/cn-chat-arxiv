# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Gaussian Process Regression with Soft Inequality and Monotonicity Constraints](https://arxiv.org/abs/2404.02873) | 引入类量子 Hamilton Monte Carlo 方法到不等式和单调性约束的高斯过程回归中，在概率意义上提高了模型准确性并减少了方差 |
| [^2] | [Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds](https://arxiv.org/abs/2404.02866) | 通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度 |
| [^3] | [Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect](https://arxiv.org/abs/2404.02591) | 学习者倾向于估计值为正的替代方案，避开估计值为负的替代方案，导致高估误差纠正但低估误差无法纠正，同时研究发现在一些情况下负估计会导致选择较少的样本量，这种消极偏见同样存在于贝叶斯学习者中。 |
| [^4] | [Convergence Analysis of Flow Matching in Latent Space with Transformers](https://arxiv.org/abs/2404.02538) | 该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。 |
| [^5] | [Masked Completion via Structured Diffusion with White-Box Transformers](https://arxiv.org/abs/2404.02446) | 该论文提出了一种可以应用于大规模无监督表示学习的白盒设计范例。 |
| [^6] | [From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives](https://arxiv.org/abs/2404.02438) | 该论文提出了一种利用最先进的NLP技术从口述验尸文本中预测死因并进行有效推断的方法。 |
| [^7] | [Improved model-free bounds for multi-asset options using option-implied information and deep learning](https://arxiv.org/abs/2404.02343) | 利用深度学习和期权隐含信息改进多资产期权的无模型上界计算方法 |
| [^8] | [On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning](https://arxiv.org/abs/2404.02254) | 提出了更强的平均情况计算分离，对于“典型”情况下的学习任务实例，单模态学习在计算上是困难的，但多模态学习却很容易。 |
| [^9] | [Distributed and Rate-Adaptive Feature Compression](https://arxiv.org/abs/2404.02179) | 设计一种适应不断变化的通信约束的特征压缩方案，同时最大化融合中心的推断性能。 |
| [^10] | [Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning](https://arxiv.org/abs/2404.00015) | 提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。 |
| [^11] | [Taming the Interactive Particle Langevin Algorithm -- the superlinear case](https://arxiv.org/abs/2403.19587) | 我们提出了一种新颖的 tamed interactive particle Langevin algorithms（tIPLA）类算法，能够在多项式增长情况下获得稳定且具有最佳速率的非渐近收敛误差估计。 |
| [^12] | [Tradeoffs of Diagonal Fisher Information Matrix Estimators](https://arxiv.org/abs/2402.05379) | 本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。 |
| [^13] | [ReCoRe: Regularized Contrastive Representation Learning of World Model](https://arxiv.org/abs/2312.09056) | 通过正则化对比度表示学习世界模型，该方法提高了样本效率和泛化性能，解决了在视觉导航等日常任务中出现外观变化时的挑战。 |
| [^14] | [Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates](https://arxiv.org/abs/2311.13447) | 该论文研究了在KL条件下具有最优速率的差分私有非凸优化问题，并提出了针对不同情况的新算法，实现了接近最优的速率。 |
| [^15] | [Will My Robot Achieve My Goals? Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target](https://arxiv.org/abs/2211.16462) | 本文提出了一种能够预测MDP策略达到用户指定行为目标概率的方法，并通过对符合预测进行反转来计算概率估计。 |
| [^16] | [Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery](https://arxiv.org/abs/2211.13715) | 提出了一种基于梯度的干预目标定位方法，GIT，在因果发现中能够通过信号梯度估计器降低干预次数，在低数据量情况下优于竞争基线。 |
| [^17] | [Global Momentum Compression for Sparse Communication in Distributed Learning](https://arxiv.org/abs/1905.12948) | 本文提出了一种全局动量压缩（GMC）方法，用于稀疏通信，与现有的局部动量方法不同，GMC利用全局动量来提高分布式学习性能。 |
| [^18] | [Model-Based Reinforcement Learning for Atari](https://arxiv.org/abs/1903.00374) | 本研究探索了如何利用视频预测模型实现基于模型的深度RL算法SimPLe，在Atari游戏中比无模型方法更有效地解决问题，并通过实验验证了新颖模型体系结构在这一背景下取得最佳结果。 |
| [^19] | [Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index](https://arxiv.org/abs/1603.09326) | 利用现代数据集中大量中间结果的事实，即使没有单个替代指标满足统计替代条件，使用多个替代指标也可能是有效的。 |
| [^20] | [Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias.](http://arxiv.org/abs/2310.14814) | 本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。 |
| [^21] | [Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems.](http://arxiv.org/abs/2310.00728) | 提出了一种基于物理信息的图神经网络（GNN）框架GraPhyR，用于解决电力系统的动态重构（DyR）问题。该框架将运营和连接约束直接融入GNN框架中，并进行端到端的训练，能够有效地优化DyR任务。 |
| [^22] | [Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints.](http://arxiv.org/abs/2305.15558) | 本文研究了一个面向随机网络资源分配的在线问题，并提出了一种近乎最优的解决方案，该方案在数字模拟中表现优异。 |
| [^23] | [Shapley Curves: A Smoothing Perspective.](http://arxiv.org/abs/2211.13289) | 本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。 |
| [^24] | [Enhanced Bayesian Neural Networks for Macroeconomics and Finance.](http://arxiv.org/abs/2211.04752) | 该论文提出的增强贝叶斯神经网络能够模拟大量宏观经济和金融变量的通用非线性和时间变化，具有潜在的政策决策应用价值。 |
| [^25] | [Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records.](http://arxiv.org/abs/2110.09680) | 本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。 |

# 详细

[^1]: 高斯过程回归与软不等式和单调性约束

    Gaussian Process Regression with Soft Inequality and Monotonicity Constraints

    [https://arxiv.org/abs/2404.02873](https://arxiv.org/abs/2404.02873)

    引入类量子 Hamilton Monte Carlo 方法到不等式和单调性约束的高斯过程回归中，在概率意义上提高了模型准确性并减少了方差

    

    Gaussian process（GP）回归是一种非参数、贝叶斯框架，用于逼近复杂模型。标准的GP回归可能导致模型无界，导致某些点采用不可行的值。我们介绍了一种新的GP方法，以概率方式强制执行物理约束。该GP模型通过类量子启发的 Hamilton Monte Carlo（QHMC）进行训练。QHMC是从各种分布中高效抽样的方法。与标准的 Hamilton Monte Carlo 算法不同，其中粒子具有固定质量，QHMC允许粒子具有随机质量矩阵并带有概率分布。将 QHMC 方法引入概率意义上的不等式和单调性约束的 GP 回归，我们的方法提高了结果 GP 模型的准确性并减少了方差。根据我们在几个数据集上的实验，所提出的方法作为一种高效方法可以加速...

    arXiv:2404.02873v1 Announce Type: cross  Abstract: Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accel
    
[^2]: 通过Hammersley-Chapman-Robbins界限保证机密性

    Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds

    [https://arxiv.org/abs/2404.02866](https://arxiv.org/abs/2404.02866)

    通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度

    

    在深度神经网络推断过程中通过向最后几层的激活添加噪声来保护隐私是可能的。这些层中的激活被称为“特征”（少见的称为“嵌入”或“特征嵌入”）。添加的噪声有助于防止从嘈杂的特征中重建输入。通过对所有可能的无偏估计量的方差进行下限估计，量化了由此添加的噪声产生的机密性。经典不等式Hammersley和Chapman以及Robbins提供便利的、可计算的界限-- HCR界限。数值实验表明，对于包含10个类别的图像分类数据集“MNIST”和“CIFAR-10”，HCR界限在小型神经网络上表现良好。HCR界限似乎单独无法保证

    arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
    
[^3]: 自适应采样政策意味着存在偏见信念: 一般化的热炉效应

    Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect

    [https://arxiv.org/abs/2404.02591](https://arxiv.org/abs/2404.02591)

    学习者倾向于估计值为正的替代方案，避开估计值为负的替代方案，导致高估误差纠正但低估误差无法纠正，同时研究发现在一些情况下负估计会导致选择较少的样本量，这种消极偏见同样存在于贝叶斯学习者中。

    

    热炉效应是由于学习的适应性特性所导致的一种消极偏见。研究发现，追求估计值为正的替代方案，避开估计值为负的替代方案的学习算法将纠正高估误差，但无法纠正低估误差。本文将热炉效应的理论推广到负估计不一定导致避免而是导致样本量较小的情况（即，如果B被认为是次优的，学习者会选择更少的替代方案B而非完全避免B）。我们形式上证明了在这种设置中消极偏见仍然存在。我们还表明，贝叶斯学习者存在消极偏见，即大多数这样的学习者低估替代方案的预期价值。

    arXiv:2404.02591v1 Announce Type: new  Abstract: The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning. The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation. Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B). We formally demonstrate that the negativity bias remains in this set-up. We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative.
    
[^4]: 流匹配在潜空间中的收敛性分析与Transformer

    Convergence Analysis of Flow Matching in Latent Space with Transformers

    [https://arxiv.org/abs/2404.02538](https://arxiv.org/abs/2404.02538)

    该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。

    

    我们提出了ODE-based生成模型，特别是流匹配的理论收敛性保证。我们使用预训练的自编码器网络将高维原始输入映射到低维潜空间，其中一个Transformer网络被训练来预测从标准正态分布到目标潜空间分布的变换速度场。我们的误差分析展示了这种方法的有效性，表明通过估计的ODE流生成样本的分布在温斯坦-2距离下收敛到目标分布，这在温和且实际的假设下成立。此外，我们展示了具有利普希茨连续性的Transformer网络可以有效地逼近任意光滑函数，这可能是独立感兴趣的。

    arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.
    
[^5]: 通过带有白盒变换器的结构扩散进行蒙面补全

    Masked Completion via Structured Diffusion with White-Box Transformers

    [https://arxiv.org/abs/2404.02446](https://arxiv.org/abs/2404.02446)

    该论文提出了一种可以应用于大规模无监督表示学习的白盒设计范例。

    

    现代学习框架通常使用大量无标签数据训练深度神经网络，通过解决简单的前置任务学习表示，然后将这些表示用作下游任务的基础。本文提供了第一个白盒设计范例的实例，可以应用于大规模无监督表示学习。

    arXiv:2404.02446v1 Announce Type: new  Abstract: Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving
    
[^6]: 从叙述到数字：利用口述验尸叙述的语言模型预测进行有效推断

    From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives

    [https://arxiv.org/abs/2404.02438](https://arxiv.org/abs/2404.02438)

    该论文提出了一种利用最先进的NLP技术从口述验尸文本中预测死因并进行有效推断的方法。

    

    在大部分死亡事件发生在医疗系统外的场景中，口述验尸（VAs）是监测死因趋势的常用工具。VAs是与幸存的照料者或亲属进行的访谈，用于预测逝者的死因。将VAs转化为研究人员和决策者可行的见解需要两个步骤：（i）使用VA访谈预测可能的死因，（ii）使用预测的死因进行推断（例如，使用死亡样本对死因按人口统计因素分解的建模）。在本文中，我们开发了一种利用最先进的NLP技术从自由文本预测结果（在我们的案例中为死因）进行有效推断的方法。我们将这种方法称为multiPPI++，将最近的“预测驱动推断”工作扩展到多项分类。我们利用一系列NLP技术进行COD预测，并通过对VA数据的实证分析，展示了我们方法的有效性。

    arXiv:2404.02438v1 Announce Type: new  Abstract: In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in "prediction-powered inference" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our 
    
[^7]: 利用期权隐含信息和深度学习改进多资产期权的无模型界限

    Improved model-free bounds for multi-asset options using option-implied information and deep learning

    [https://arxiv.org/abs/2404.02343](https://arxiv.org/abs/2404.02343)

    利用深度学习和期权隐含信息改进多资产期权的无模型上界计算方法

    

    我们考虑在将依赖性不确定性与有关依赖结构的额外信息相结合的情况下计算多资产期权的无模型界限。 更具体地，我们考虑了边际分布已知且市场上也有多资产期权价格的部分信息的情形。 我们在这种情况下给出了资产定价的基本定理，以及一种超对冲对偶，能够将在概率度量上的最大化问题转化为在交易策略上的更易处理的最小化问题。 后者是通过结合罚款方法和借助人工神经网络的深度学习逼近来解决的。 数值方法快速，并且计算时间与交易资产数量成线性比例。 最后，我们检验了各种额外信息的重要性。

    arXiv:2404.02343v1 Announce Type: cross  Abstract: We consider the computation of model-free bounds for multi-asset options in a setting that combines dependence uncertainty with additional information on the dependence structure. More specifically, we consider the setting where the marginal distributions are known and partial information, in the form of known prices for multi-asset options, is also available in the market. We provide a fundamental theorem of asset pricing in this setting, as well as a superhedging duality that allows to transform the maximization problem over probability measures in a more tractable minimization problem over trading strategies. The latter is solved using a penalization approach combined with a deep learning approximation using artificial neural networks. The numerical method is fast and the computational time scales linearly with respect to the number of traded assets. We finally examine the significance of various pieces of additional information. Em
    
[^8]: 关于多模态与单模态机器学习之间更强的计算分离

    On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning

    [https://arxiv.org/abs/2404.02254](https://arxiv.org/abs/2404.02254)

    提出了更强的平均情况计算分离，对于“典型”情况下的学习任务实例，单模态学习在计算上是困难的，但多模态学习却很容易。

    

    在多模态机器学习中，将多种数据模态（例如文本和图像）结合起来以促进更好的机器学习模型的学习，这仍然适用于相应的单模态任务（例如文本生成）。最近，多模态机器学习取得了巨大的经验成功（例如GPT-4）。受到为这种经验成功开发理论基础的动机，Lu（NeurIPS '23，ALT '24）提出了一种多模态学习理论，并考虑了多模态和单模态学习的理论模型之间可能的分离。特别是Lu（ALT '24）展示了一种计算分离，这对学习任务的最坏情况实例是相关的。

    arXiv:2404.02254v1 Announce Type: cross  Abstract: In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for "typical" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how "organic" the average-cas
    
[^9]: 分布式和速率自适应特征压缩

    Distributed and Rate-Adaptive Feature Compression

    [https://arxiv.org/abs/2404.02179](https://arxiv.org/abs/2404.02179)

    设计一种适应不断变化的通信约束的特征压缩方案，同时最大化融合中心的推断性能。

    

    我们研究了分布式和速率自适应特征压缩问题，针对线性回归。一组分布式传感器收集回归器数据的不相交特征。假定融合中心包含一个在整个未压缩数据集上训练的预训练线性回归模型。在推断时，传感器压缩其观测结果，并通过通信受限的信道将其发送到融合中心，这些信道的速率可以随时间变化。我们的目标是设计一种能够适应不断变化的通信约束的特征压缩方案，同时在融合中心最大化推断性能。我们首先获得了假定了对底层回归器数据分布知识的最优量化器的形式。在一个实际合理的近似下，我们提出了一种通过对传感器数据的一维投影进行量化工作的分布式压缩方案。

    arXiv:2404.02179v1 Announce Type: cross  Abstract: We study the problem of distributed and rate-adaptive feature compression for linear regression. A set of distributed sensors collect disjoint features of regressor data. A fusion center is assumed to contain a pretrained linear regression model, trained on a dataset of the entire uncompressed data. At inference time, the sensors compress their observations and send them to the fusion center through communication-constrained channels, whose rates can change with time. Our goal is to design a feature compression {scheme} that can adapt to the varying communication constraints, while maximizing the inference performance at the fusion center. We first obtain the form of optimal quantizers assuming knowledge of underlying regressor data distribution. Under a practically reasonable approximation, we then propose a distributed compression scheme which works by quantizing a one-dimensional projection of the sensor data. We also propose a simp
    
[^10]: 利用量子增强机器学习赋能信用评分系统

    Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning

    [https://arxiv.org/abs/2404.00015](https://arxiv.org/abs/2404.00015)

    提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。

    

    Quantum Kernels被认为在量子机器学习的早期阶段提供了有用性。然而，在利用庞大数据集时，高度复杂的经典模型很难超越，特别是在理解力方面。尽管如此，一旦数据稀缺且倾斜，经典模型就会遇到困难。量子特征空间被预计在这样具有挑战性的情景中能够找到更好的数据特征和目标类别之间的联系，最重要的是增强了泛化能力。在这项工作中，我们提出了一种名为Systemic Quantum Score (SQS)的新方法，并提供了初步结果，表明在金融行业生产级应用案例中，SQS可能比纯经典模型具有优势。我们的具体研究表明，SQS能够从较少的数据点中提取出模式，并且在数据需求量大的算法（如XGBoost）上表现出更好的性能，带来优势。

    arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
    
[^11]: 驯服交互式粒子 Langevin 算法 -- 超线性情况

    Taming the Interactive Particle Langevin Algorithm -- the superlinear case

    [https://arxiv.org/abs/2403.19587](https://arxiv.org/abs/2403.19587)

    我们提出了一种新颖的 tamed interactive particle Langevin algorithms（tIPLA）类算法，能够在多项式增长情况下获得稳定且具有最佳速率的非渐近收敛误差估计。

    

    最近在随机优化方面的进展产生了交互式粒子 Langevin 算法（IPLA），该算法利用交互粒子系统（IPS）的概念来高效地从近似后验密度中抽样。这在期望最大化（EM）框架中变得尤为关键，其中 E 步骤在计算上具有挑战性甚至是难以处理的。尽管先前的研究侧重于梯度最多线性增长的凸情况，我们的工作将此框架扩展到包括多项式增长。采用驯服技术生成明确的离散化方案，从而产生一类稳定的、在这种非线性情况下，称为驯服交互式粒子 Langevin 算法（tIPLA）的算法。我们获得了新类在 Wasserstein-2 距离下的非渐近收敛误差估计，具有最佳速率。

    arXiv:2403.19587v1 Announce Type: cross  Abstract: Recent advances in stochastic optimization have yielded the interactive particle Langevin algorithm (IPLA), which leverages the notion of interacting particle systems (IPS) to efficiently sample from approximate posterior densities. This becomes particularly crucial within the framework of Expectation-Maximization (EM), where the E-step is computationally challenging or even intractable. Although prior research has focused on scenarios involving convex cases with gradients of log densities that grow at most linearly, our work extends this framework to include polynomial growth. Taming techniques are employed to produce an explicit discretization scheme that yields a new class of stable, under such non-linearities, algorithms which are called tamed interactive particle Langevin algorithms (tIPLA). We obtain non-asymptotic convergence error estimates in Wasserstein-2 distance for the new class under an optimal rate.
    
[^12]: 对角费舍尔信息矩阵估计器的权衡

    Tradeoffs of Diagonal Fisher Information Matrix Estimators

    [https://arxiv.org/abs/2402.05379](https://arxiv.org/abs/2402.05379)

    本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。

    

    费舍尔信息矩阵描述了神经网络参数空间中的局部几何性质，它提供了理论和工具来理解和优化神经网络。鉴于其计算成本高，实践者通常使用随机估计器，并仅评估对角线条目。我们研究了两种这样的估计器，其准确性和样本复杂性取决于它们关联的方差。我们推导了方差的界限，并在回归和分类网络中实例化它们。我们通过分析和数值研究来权衡这两个估计器。我们发现方差量取决于关于不同参数组的非线性，当估计费舍尔信息时不能忽视它们。

    The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
    
[^13]: ReCoRe: 正则化对比度表示学习的世界模型

    ReCoRe: Regularized Contrastive Representation Learning of World Model

    [https://arxiv.org/abs/2312.09056](https://arxiv.org/abs/2312.09056)

    通过正则化对比度表示学习世界模型，该方法提高了样本效率和泛化性能，解决了在视觉导航等日常任务中出现外观变化时的挑战。

    

    近期的无模型强化学习（RL）方法在游戏环境中已经展示出与人类水平相当的有效性，但在视觉导航等日常任务中的成功受到限制，特别是在出现显著外观变化的情况下。为了解决这些挑战，我们提出了一种世界模型，通过（i）对比度无监督学习和（ii）介入不变正则化来学习不变特征。学习世界动力学的显式表示，即世界模型，提高了样本效率，而对比度学习隐含地强化了学习不变特征，改善了泛化性能。然而，简单地将对比度损失集成到世界模型中是不够的，因为基于世界模型的RL方法独立优化表示学习和智能体策略。

    arXiv:2312.09056v2 Announce Type: replace-cross  Abstract: While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the na\"ive integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overc
    
[^14]: 在KL条件下具有最优速率的差分私有非凸优化

    Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates

    [https://arxiv.org/abs/2311.13447](https://arxiv.org/abs/2311.13447)

    该论文研究了在KL条件下具有最优速率的差分私有非凸优化问题，并提出了针对不同情况的新算法，实现了接近最优的速率。

    

    我们研究了满足$(\gamma,\kappa)$-Kurdyka-Lojasiewicz (KL)条件的损失函数的私有经验风险最小化（ERM）问题。Polyak-Lojasiewicz (PL)条件是这个条件的特例，当$\kappa=2$时。具体来说，我们研究了在$\rho$零集中差分隐私（zCDP）约束下的问题。当$\kappa\in[1,2]$且损失函数在足够大的区域内是Lipschitz和光滑的时，我们提出了一种基于方差减少梯度下降的新算法，其在超额经验风险上实现了速率$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$，其中$n$是数据集大小，$d$是维度。我们进一步展示了这个速率几乎是最优的。当$\kappa \geq 2$且损失函数代替是Lipschitz和弱凸时，我们展示了通过私有方法可以实现速率$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$。

    arXiv:2311.13447v2 Announce Type: replace  Abstract: We study private empirical risk minimization (ERM) problem for losses satisfying the $(\gamma,\kappa)$-Kurdyka-{\L}ojasiewicz (KL) condition. The Polyak-{\L}ojasiewicz (PL) condition is a special case of this condition when $\kappa=2$. Specifically, we study this problem under the constraint of $\rho$ zero-concentrated differential privacy (zCDP). When $\kappa\in[1,2]$ and the loss function is Lipschitz and smooth over a sufficiently large region, we provide a new algorithm based on variance reduced gradient descent that achieves the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ on the excess empirical risk, where $n$ is the dataset size and $d$ is the dimension. We further show that this rate is nearly optimal. When $\kappa \geq 2$ and the loss is instead Lipschitz and weakly convex, we show it is possible to achieve the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ with a privat
    
[^15]: 我的机器人会实现我的目标吗？预测MDP策略达到用户指定行为目标的概率

    Will My Robot Achieve My Goals? Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target

    [https://arxiv.org/abs/2211.16462](https://arxiv.org/abs/2211.16462)

    本文提出了一种能够预测MDP策略达到用户指定行为目标概率的方法，并通过对符合预测进行反转来计算概率估计。

    

    当自主系统执行任务时，应保持对实现用户目标概率的校准估计。如果该概率低于某个期望水平，应向用户发出警报，以便采取适当干预。本文考虑了用户将目标规定为实值性能摘要的目标区间的设置，例如在固定时间段$H$内测量的累积奖励。在每个时间$t \in \{0, \ldots, H-1\}$，我们的方法会产生一个校准概率估计，即最终累积奖励落在用户指定目标区间$[y^-,y^+]$内的概率。利用这一估计，自主系统可以在概率低于指定阈值时发出警报。我们通过反转符合预测来计算概率估计。我们的出发点是Romano等人的Quantile Regression (CQR)方法，该方法应用了spli

    arXiv:2211.16462v2 Announce Type: replace  Abstract: As an autonomous system performs a task, it should maintain a calibrated estimate of the probability that it will achieve the user's goal. If that probability falls below some desired level, it should alert the user so that appropriate interventions can be made. This paper considers settings where the user's goal is specified as a target interval for a real-valued performance summary, such as the cumulative reward, measured at a fixed horizon $H$. At each time $t \in \{0, \ldots, H-1\}$, our method produces a calibrated estimate of the probability that the final cumulative reward will fall within a user-specified target interval $[y^-,y^+].$ Using this estimate, the autonomous system can raise an alarm if the probability drops below a specified threshold. We compute the probability estimates by inverting conformal prediction. Our starting point is the Conformalized Quantile Regression (CQR) method of Romano et al., which applies spli
    
[^16]: 相信您的 $\nabla$: 基于梯度的干预目标定位用于因果发现

    Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery

    [https://arxiv.org/abs/2211.13715](https://arxiv.org/abs/2211.13715)

    提出了一种基于梯度的干预目标定位方法，GIT，在因果发现中能够通过信号梯度估计器降低干预次数，在低数据量情况下优于竞争基线。

    

    从数据中推断因果结构是科学中一项具有基础重要性的挑战性任务。观测数据通常不足以唯一确定系统的因果结构。虽然进行干预（即实验）可以改善可识别性，但这些样本通常难以获得且成本高昂。因此，因果发现的实验设计方法旨在通过估计最具信息性的干预目标来最小化干预次数。在这项工作中，我们提出了一种新颖的基于梯度的干预目标定位方法，简称为GIT，它‘相信’了基于梯度的因果发现框架的梯度估计器，以提供干预采集函数的信号。我们在模拟和真实世界数据集上进行了大量实验，并证明GIT在低数据量情况下表现与竞争基线相当，甚至在某些情况下超越它们。

    arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
    
[^17]: 全局动量压缩用于分布式学习中的稀疏通信

    Global Momentum Compression for Sparse Communication in Distributed Learning

    [https://arxiv.org/abs/1905.12948](https://arxiv.org/abs/1905.12948)

    本文提出了一种全局动量压缩（GMC）方法，用于稀疏通信，与现有的局部动量方法不同，GMC利用全局动量来提高分布式学习性能。

    

    随着数据的快速增长，分布式动量随机梯度下降（DMSGD）在分布式学习中得到了广泛应用，特别是用于训练大规模深度模型。由于网络的延迟和带宽有限，通信成为分布式学习的瓶颈。使用稀疏梯度进行通信压缩，简称为“稀疏通信”，已被广泛应用以降低通信成本。所有关于DMSGD中稀疏通信的现有工作都使用本地动量，其中动量仅累积每个工作者在本地计算的随机梯度。在本文中，我们提出了一种新方法，称为\emph{全局动量压缩}（GMC），用于稀疏通信。不同于现有工作中使用的局部动量，GMC使用全局动量。

    arXiv:1905.12948v3 Announce Type: replace-cross  Abstract: With the rapid growth of data, distributed momentum stochastic gradient descent~(DMSGD) has been widely used in distributed learning, especially for training large-scale deep models. Due to the latency and limited bandwidth of the network, communication has become the bottleneck of distributed learning. Communication compression with sparsified gradient, abbreviated as \emph{sparse communication}, has been widely employed to reduce communication cost. All existing works about sparse communication in DMSGD employ local momentum, in which the momentum only accumulates stochastic gradients computed by each worker locally. In this paper, we propose a novel method, called \emph{\underline{g}}lobal \emph{\underline{m}}omentum \emph{\underline{c}}ompression~(GMC), for sparse communication. Different from existing works that utilize local momentum, GMC utilizes global momentum. Furthermore, to enhance the convergence performance when u
    
[^18]: 基于模型的强化学习在Atari中的应用

    Model-Based Reinforcement Learning for Atari

    [https://arxiv.org/abs/1903.00374](https://arxiv.org/abs/1903.00374)

    本研究探索了如何利用视频预测模型实现基于模型的深度RL算法SimPLe，在Atari游戏中比无模型方法更有效地解决问题，并通过实验验证了新颖模型体系结构在这一背景下取得最佳结果。

    

    无模型的强化学习（RL）可以用于从图像观察中学习有效的策略，例如Atari游戏，但通常需要非常大量的交互——实际上，远远超过人类学习相同游戏所需的数量。人们是如何如此快速学习的？答案的一部分可能是人们可以学习游戏运行的方式，并预测哪些动作会产生期望的结果。本文探讨了视频预测模型如何使代理能够在比无模型方法交互更少的情况下解决Atari游戏。我们描述了Simulated Policy Learning（SimPLe），这是一个基于视频预测模型的完整的基于模型的深度RL算法，并对几种模型体系结构进行了比较，包括一个在我们的情境中取得最佳结果的新颖结构。我们的实验评估了SimPLe在100k低数据条件下的一系列Atari游戏中的表现。

    arXiv:1903.00374v5 Announce Type: replace  Abstract: Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k
    
[^19]: 利用多个替代指标估计治疗效果：替代分数和替代指数的作用

    Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index

    [https://arxiv.org/abs/1603.09326](https://arxiv.org/abs/1603.09326)

    利用现代数据集中大量中间结果的事实，即使没有单个替代指标满足统计替代条件，使用多个替代指标也可能是有效的。

    

    估计治疗效果长期作用是许多领域感兴趣的问题。 估计此类治疗效果的一个常见挑战在于长期结果在需要做出政策决策的时间范围内是未观察到的。 克服这种缺失数据问题的一种方法是分析治疗效果对中间结果的影响，通常称为统计替代指标，如果满足条件：在统计替代指标的条件下，治疗和结果是独立的。  替代条件的有效性经常是有争议的。 在现代数据集中，研究人员通常观察到大量中间结果，可能是数百个或数千个，被认为位于治疗和长期感兴趣的结果之间的因果链上或附近。 即使没有个别代理满足统计替代条件，使用多个代理也可以。

    arXiv:1603.09326v4 Announce Type: replace-cross  Abstract: Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be 
    
[^20]: 在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练

    Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14814](http://arxiv.org/abs/2310.14814)

    本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。

    

    自训练是半监督学习中一种众所周知的方法。它包括对模型自信度高的未标记数据进行伪标签分配，并将其视为标记样本进行处理。对于神经网络，通常使用softmax预测概率作为自信度度量，尽管已知它们对错误预测也过于自信。当数据标注受到某种约束时，这种现象尤为明显，即样本选择偏差存在。为了解决这个问题，我们提出了一种新的自信度度量方法，称为$\mathcal{T}$-相似度，它基于线性分类器的集成预测多样性。我们通过研究稳定点并描述单个成员的多样性与其性能之间的关系来提供我们方法的理论分析。我们通过对三种不同伪标签策略的实验验证了我们自信度度量的好处。

    Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
    
[^21]: 基于物理信息的图神经网络用于电力系统的动态重构

    Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems. (arXiv:2310.00728v1 [cs.LG])

    [http://arxiv.org/abs/2310.00728](http://arxiv.org/abs/2310.00728)

    提出了一种基于物理信息的图神经网络（GNN）框架GraPhyR，用于解决电力系统的动态重构（DyR）问题。该框架将运营和连接约束直接融入GNN框架中，并进行端到端的训练，能够有效地优化DyR任务。

    

    为了保持可靠的电网，我们需要快速的决策算法来解决动态重构（DyR）等复杂问题。DyR实时优化配电网开关设置，以最小化电网损耗，并分派资源以满足可用发电量的负载需求。DyR是一个混合整数问题，对于大规模电网和快速时间尺度来说，可能计算难以解决。我们提出了GraPhyR，一种专为DyR而设计的基于物理信息的图神经网络（GNN）框架。我们直接将基本的运营和连接约束融入到GNN框架中，并进行端到端的训练。我们的结果表明，GraPhyR能够学习优化DyR任务。

    To maintain a reliable grid we need fast decision-making algorithms for complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution grid switch settings in real-time to minimize grid losses and dispatches resources to supply loads with available generation. DyR is a mixed-integer problem and can be computationally intractable to solve for large grids and at fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network (GNNs) framework tailored for DyR. We incorporate essential operational and connectivity constraints directly within the GNN framework and train it end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR task.
    
[^22]: 面向带有长期约束的随机网络资源分配的在线优化问题

    Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints. (arXiv:2305.15558v1 [math.OC])

    [http://arxiv.org/abs/2305.15558](http://arxiv.org/abs/2305.15558)

    本文研究了一个面向随机网络资源分配的在线问题，并提出了一种近乎最优的解决方案，该方案在数字模拟中表现优异。

    

    本论文研究了一个简单通信网络中的在线资源预留问题。网络由两个计算节点组成，通过本地通信链路连接。系统在离散时间内运行；在每个时间段，管理员会在实际作业请求之前为服务器预留资源，这些预留会产生成本。然后，在观察到客户端请求之后，作业可能会从一个服务器转移到另一个服务器，以最好地适应需求，但这会产生额外的传输成本。如果无法满足某些作业请求，则会产生违规成本，需要为每个被阻止的作业支付成本。目标是在有限的时间内最小化总预订成本，同时在一定预算限制下维护累积违规和传输成本。为了研究这个问题，我们将其形式化为一个反复博弈问题，针对一系列提议的策略按随机顺序进行预订。然后，我们设计了一种在线算法，该算法可以实现接近最优的性能保证，以期望的总成本为基础，为任何有限的T时间段。数字模拟表明，我们的算法优于几种基线算法。

    In this paper, we study an optimal online resource reservation problem in a simple communication network. The network is composed of two compute nodes linked by a local communication link. The system operates in discrete time; at each time slot, the administrator reserves resources for servers before the actual job requests are known. A cost is incurred for the reservations made. Then, after the client requests are observed, jobs may be transferred from one server to the other to best accommodate the demands by incurring an additional transport cost. If certain job requests cannot be satisfied, there is a violation that engenders a cost to pay for each of the blocked jobs. The goal is to minimize the overall reservation cost over finite horizons while maintaining the cumulative violation and transport costs under a certain budget limit. To study this problem, we first formalize it as a repeated game against nature where the reservations are drawn randomly according to a sequence of pro
    
[^23]: Shapley曲线：一种平滑视角

    Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.13289](http://arxiv.org/abs/2211.13289)

    本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。

    

    源自合作博弈理论，Shapley值已成为应用机器学习中最广泛使用的变量重要性度量之一。然而，对Shapley值的统计理解仍然有限。本文以非参数(或平滑)的角度，引入Shapley曲线作为局部变量重要性的度量。我们提出了两种估计策略，并在特征独立和依赖的情况下都得出了一致性和渐近正态性。这样，我们可以构建置信区间并对估计的Shapley曲线进行推断。我们提出了一种新颖的野蛮引导程序版本，专门调整以获得Shapley曲线的良好有限样本覆盖。渐近结果在大量实验证实了。在实证应用中，我们分析了哪些属性驱动了车辆的价格。

    Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
    
[^24]: 增强贝叶斯神经网络在宏观经济和金融领域的应用

    Enhanced Bayesian Neural Networks for Macroeconomics and Finance. (arXiv:2211.04752v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2211.04752](http://arxiv.org/abs/2211.04752)

    该论文提出的增强贝叶斯神经网络能够模拟大量宏观经济和金融变量的通用非线性和时间变化，具有潜在的政策决策应用价值。

    

    我们开发了贝叶斯神经网络 (BNNs)，它们能够模拟可能包含大量宏观经济和金融变量的通用非线性和时间变化。从方法论上讲，我们允许对网络进行一般规格的说明，可以应用于密集或稀疏数据集，并结合各种激活功能、可能非常多的神经元和误差项的随机波动。从计算的角度来看，我们为引入的通用BNNs开发了快速高效的估计算法。从实证的角度来看，我们用模拟数据和一组常见的宏观金融应用来展示我们的BNNs可以实际使用，特别是对于观测目标变量的横截面或时间序列分布的尾部，该方法特别适用于在不寻常的时间做出决策方面具有信息量。

    We develop Bayesian neural networks (BNNs) that permit to model generic nonlinearities and time variation for (possibly large sets of) macroeconomic and financial variables. From a methodological point of view, we allow for a general specification of networks that can be applied to either dense or sparse datasets, and combines various activation functions, a possibly very large number of neurons, and stochastic volatility (SV) for the error term. From a computational point of view, we develop fast and efficient estimation algorithms for the general BNNs we introduce. From an empirical point of view, we show both with simulated data and with a set of common macro and financial applications that our BNNs can be of practical use, particularly so for observations in the tails of the cross-sectional or time series distributions of the target variables, which makes the method particularly informative for policy making in uncommon times.
    
[^25]: 大规模医疗数据记录中的多层次随机优化填补方法

    Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records. (arXiv:2110.09680v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.09680](http://arxiv.org/abs/2110.09680)

    本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。

    

    探索和分析大规模数据集最近在研究和发展社区中引起了越来越多的关注。长期以来，人们一直认识到许多数据集中包含大量缺失的数值数据。我们引入了一种基于Kriging理论的数学原则随机优化填补方法，该方法被证明是一种强大的填补方法。然而，其计算成本和潜在的数值不稳定性会导致昂贵和/或不可靠的预测，可能限制其在大规模数据集上的使用。在本文中，我们将最近开发的多层次随机优化方法应用于大规模医疗记录中的填补问题。该方法基于计算应用数学技术，并具有高精度。特别地，对于最佳线性无偏预测器（BLUP），该多层次形式化是精确的，而且计算速度更快，数值稳定性更高。

    Exploration and analysis of massive datasets has recently generated increasing interest in the research and development communities. It has long been a recognized problem that many datasets contain significant levels of missing numerical data. We introduce a mathematically principled stochastic optimization imputation method based on the theory of Kriging. This is shown to be a powerful method for imputation. However, its computational effort and potential numerical instabilities produce costly and/or unreliable predictions, potentially limiting its use on large scale datasets. In this paper, we apply a recently developed multi-level stochastic optimization approach to the problem of imputation in massive medical records. The approach is based on computational applied mathematics techniques and is highly accurate. In particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level formulation is exact, and is also significantly faster and more numerically stable. This permits
    

