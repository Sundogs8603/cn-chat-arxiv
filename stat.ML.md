# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method.](http://arxiv.org/abs/2305.16284) | 本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。 |
| [^2] | [Non-adversarial training of Neural SDEs with signature kernel scores.](http://arxiv.org/abs/2305.16274) | 该研究提出一种非对抗性训练神经SDE的新方法，使用基于签名内核的评分规则代替传统的GAN方法。新方法由于不需要进行动态规划运算，具有计算效率更高的特点。 |
| [^3] | [Incentivizing Honesty among Competitors in Collaborative Learning and Optimization.](http://arxiv.org/abs/2305.16272) | 这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。 |
| [^4] | [Koopman Kernel Regression.](http://arxiv.org/abs/2305.16215) | 提出了一种基于Koopman核的回归方法，用于预测非线性动力系统的时间演变。该方法在机器人操作，视频预测和交通预测等各种应用中均有优异表现，并具有可证明的学习理论保证。 |
| [^5] | [Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders.](http://arxiv.org/abs/2305.16189) | 该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。 |
| [^6] | [Unifying GANs and Score-Based Diffusion as Generative Particle Models.](http://arxiv.org/abs/2305.16150) | 本文提出了一个新框架，将生成器训练作为粒子模型的一个推广，从而统一了粒子和对抗生成模型。这个框架可以将生成器集成到基于分数扩散模型中，并在没有生成器的情况下训练GAN。 |
| [^7] | [Demystifying Oversmoothing in Attention-Based Graph Neural Networks.](http://arxiv.org/abs/2305.16102) | 本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。 |
| [^8] | [FAVAS: Federated AVeraging with ASynchronous clients.](http://arxiv.org/abs/2305.16099) | 本研究提出了FAVAS算法，是一种用于在资源有限环境下训练DNNs的新型中心化异步联邦学习框架。实验结果表明FAVAS算法优于当前方法。 |
| [^9] | [On Influence Functions, Classification Influence, Relative Influence, Memorization and Generalization.](http://arxiv.org/abs/2305.16094) | 本论文通过简化涉及计算的影响函数，提高了大规模机器学习系统的训练效率并保证了模型的准确性，同时影响值的符号可以指示训练点是用于记忆还是泛化。 |
| [^10] | [An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond.](http://arxiv.org/abs/2305.16041) | 提出一种新颖的采样规则EB-TC $\varepsilon$，用于随机赌博机中的$\varepsilon$-最佳臂的辨识。该规则可用于确定固定置信度或固定预算标识且具备自适应调整勘探参数的渐近最优性。在仿真实验中表现良好，适用于不同问题领域。 |
| [^11] | [Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank.](http://arxiv.org/abs/2305.16038) | 在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。 |
| [^12] | [How many samples are needed to leverage smoothness?.](http://arxiv.org/abs/2305.16014) | 本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。 |
| [^13] | [Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo Algorithms.](http://arxiv.org/abs/2305.15988) | 本文研究了从非对数凸分布进行近似抽样的问题，并通过 Langevin Monte Carlo 算法解决。此外，研究了两种非光滑情况，这些任务源于贝叶斯推断和图像反问题。数值模拟比较了最常用的 Langevin Monte Carlo 算法的性能。 |
| [^14] | [First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities.](http://arxiv.org/abs/2305.15938) | 本论文研究了涉及马尔科夫噪声的随机优化问题，提出了一种适用于非凸和强凸最小化问题的一阶梯度方法，使用基于多层蒙特卡罗方法的随机批处理方案以获得最优线性关系，并消除了以前研究中的限制条件。在马尔可夫噪声下对变分不等式的扩展是原创性的。 |
| [^15] | [On the Identifiability of Markov Switching Models.](http://arxiv.org/abs/2305.15925) | 本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。 |
| [^16] | [Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks.](http://arxiv.org/abs/2305.15920) | 本文使用GAN来学习一个原型晶格上的随机过程，并提出一种合适的多模型程序，可以显著提高精度。GAN似乎是处理复杂统计动力学问题的有前途的工具。 |
| [^17] | [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning.](http://arxiv.org/abs/2305.15912) | 本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。 |
| [^18] | [Exponential Smoothing for Off-Policy Learning.](http://arxiv.org/abs/2305.15877) | 本文研究了离线学习中最小化风险的倒数倾向评分(IPS)的平滑正则化，推导出了可处理、可扩展、可解释的学习证明，并确定了在何种情况下不需要正则化IPS。 |
| [^19] | [Learning Robust Statistics for Simulation-based Inference under Model Misspecification.](http://arxiv.org/abs/2305.15871) | 本研究提出首个通用的方法来处理基于模拟的推论（如ABC和NPE）中由于模型错误引起的不可靠推论。通过约束统计量的选择，我们的方法通过惩罚与数据和模型之间不匹配的统计量来防止不可靠推论结果。我们在高维时间序列模型上进行了实验，证明了本方法的优越性能。 |
| [^20] | [Embeddings between Barron spaces with higher order activation functions.](http://arxiv.org/abs/2305.15839) | 本文研究了不同激活函数的Barron空间之间的嵌入，并证明了Barron空间的层次结构类似于Sobolev空间$H^m$。其中，修正功率单位激活函数在这个研究中特别重要。 |
| [^21] | [Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness.](http://arxiv.org/abs/2305.15807) | 本文提出了带总成本限制的上下文信息决策问题（CBwK），通过对术语进行重新组合，对CBwK进行了优化，支持小于$T^{3/4}$的总成本约束，并通过对偶策略实现了平等的成本限制。 |
| [^22] | [Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting.](http://arxiv.org/abs/2305.15786) | 本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。 |
| [^23] | [Differentially Private Latent Diffusion Models.](http://arxiv.org/abs/2305.15759) | 本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。 |
| [^24] | [Bayesian Analysis for Over-parameterized Linear Model without Sparsity.](http://arxiv.org/abs/2305.15754) | 本文提出了一种基于数据的特征向量的先验方法，用于处理非稀疏超参数线性模型。从导出的后验分布收缩率和开发的截断高斯近似两个方面来证明了该方法的有效性，可以解决之前的先验稀疏性限制。 |
| [^25] | [Assessing the Spatial Structure of the Association between Attendance at Preschool and Childrens Developmental Vulnerabilities in Queensland Australia.](http://arxiv.org/abs/2305.15746) | 该研究使用澳大利亚早期发展普查数据，研究了出席幼儿园与儿童发展脆弱性的关系，发现在出席幼儿园比例较高的地区，儿童至少存在一个发展性脆弱性的比例较低。使用数据分析和机器学习技术，研究人员确定了昆士兰省内的三个不同群集，每个群集特点是不同的社会人口统计变量影响幼儿园出席与发展脆弱性之间的关系。 |
| [^26] | [Counterfactual Generative Models for Time-Varying Treatments.](http://arxiv.org/abs/2305.15742) | 本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。 |
| [^27] | [The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning.](http://arxiv.org/abs/2305.15703) | 通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。 |
| [^28] | [Interpretable Machine Learning based on Functional ANOVA Framework: Algorithms and Comparisons.](http://arxiv.org/abs/2305.15670) | 本论文探讨了机器学习中可解释性的重要性，并介绍了函数ANOVA框架及其在可解释机器学习中的应用。此外，还概述了两种新开发的可解释性技术，并提出了一种新的算法——基于FANOVA和GAM的可解释的监督学习算法（FANGAM-EBM）。 |
| [^29] | [Federated Composite Saddle Point Optimization.](http://arxiv.org/abs/2305.15643) | FeDualEx是第一种在联邦学习范式下同时处理鞍点优化和复合目标的方法，具有效性和高效性。 |
| [^30] | [Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning.](http://arxiv.org/abs/2305.15612) | 该论文提出了一种基于密度比估计和半监督学习的贝叶斯优化方法，通过使用监督分类器代替密度比来估计全局最优解的两组数据的类别概率，避免了分类器对全局解决方案过于自信的问题。 |
| [^31] | [Variational Gradient Descent using Local Linear Models.](http://arxiv.org/abs/2305.15577) | 本文提出了使用局部线性模型实现目标和粒子分布KL散度降低的新估计器，可以使用样本进行计算而不需要目标得分函数，具有比SVGD更简单有效的计算方法，对于高维度情况下的模型也有优化，提升估计精度。 |
| [^32] | [Deep Stochastic Processes via Functional Markov Transition Operators.](http://arxiv.org/abs/2305.15574) | 基于马尔科夫转移算子的神经随机过程MNPs，通过在函数空间中堆叠神经参数化的算子构建，不影响一致性或添加限制，提供了更大的灵活性和表现力。在实验中MNPs表现出优异的性能。 |
| [^33] | [The Behavior and Convergence of Local Bayesian Optimization.](http://arxiv.org/abs/2305.15572) | 本文研究了贝叶斯本地优化策略的行为和收敛性，并在高维问题上提供了强大的实证性能。统计数据表明，单个高斯过程样本路径的本地解比全局方法恢复的预期值更好。Müller等人提出的贝叶斯本地优化算法的收敛速率在有噪音和无噪音的情况下都有推导。 |
| [^34] | [Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints.](http://arxiv.org/abs/2305.15558) | 本文研究了一个面向随机网络资源分配的在线问题，并提出了一种近乎最优的解决方案，该方案在数字模拟中表现优异。 |
| [^35] | [Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time.](http://arxiv.org/abs/2305.15546) | 该论文提出了一种无模型算法，采用方差缩减和自适应执行策略转换技术，在短烧化时间MDPs上实现了遗憾最优，解决了现有算法无法实现最优性和需要付出高昂内存计算成本的问题。 |
| [^36] | [Kernel Interpolation with Sparse Grids.](http://arxiv.org/abs/2305.14451) | 本文提出利用稀疏网格优化结构化核插值方法（SKI），在保证插值准确性的同时，有效解决了输入点维度较高带来的计算困难，提出了相应的稀疏网格矩阵乘法算法以及高效插值方案。 |
| [^37] | [Dimensionality Reduction as Probabilistic Inference.](http://arxiv.org/abs/2304.07658) | 该论文提出了ProbDR变分框架，将经典降维算法解释为概率推断算法，通过优化一个证据下界来完成推断操作。该框架不仅可以完成常规降维算法，还支持使用概率编程语言进行降维操作，具有强大的表达能力。 |
| [^38] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^39] | [On the Learnability of Multilabel Ranking.](http://arxiv.org/abs/2304.03337) | 研究了一系列排名损失函数下多标签排名问题在批处理和在线设置下的可学习性，并首次给出基于可学习性的排名损失函数的等价类。 |
| [^40] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^41] | [Operator learning with PCA-Net: upper and lower complexity bounds.](http://arxiv.org/abs/2303.16317) | 本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。 |
| [^42] | [Reimagining Demand-Side Management with Mean Field Learning.](http://arxiv.org/abs/2302.08190) | 本文提出了一种新的需求侧管理（DSM）方法，即控制大量电气设备遵循所需消费信号的问题，称为MD-MFC算法。该算法直接解决目标跟踪问题，具有较高的有效性和理论保证。 |
| [^43] | [Variable Selection for Kernel Two-Sample Tests.](http://arxiv.org/abs/2302.07415) | 本文提出了一种解决双样本检验中变量选择问题的框架，利用核最大均值差异统计量，以最大化方差正则化的MMD统计量。实验结果证明其超群表现。 |
| [^44] | [Sequential Underspecified Instrument Selection for Cause-Effect Estimation.](http://arxiv.org/abs/2302.05684) | 该研究提出一种序列未确定仪器选择方法，用于处理高维处理变量和仅有有限仪器的因果推断问题，并能够可靠地估计处理效应在插补子空间中的投影。 |
| [^45] | [Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR.](http://arxiv.org/abs/2302.03201) | 本文提出了基于CVaR的风险敏感强化学习算法，在针对多臂老虎机和标签化马尔可夫决策过程问题上，通过提出一种新的伯恩斯坦奖励算法和基于价值迭代的算法，实现了最优或接近最优的风险。 |
| [^46] | [Memory-Based Meta-Learning on Non-Stationary Distributions.](http://arxiv.org/abs/2302.03067) | 本文研究基于记忆的元学习在非平稳分布上的应用，重点关注在部分可观察环境中的自然语言和动作-观察序列，研究表明各种类型的基于记忆的神经模型可以准确地逼近贝叶斯最优算法，并执行潜在参数的贝叶斯推断。 |
| [^47] | [A theory of continuous generative flow networks.](http://arxiv.org/abs/2301.12594) | 本文提出了一种通用 GFlowNets 的理论，可以适用于连续或混合状态空间，通过实验证明其与非 GFlowNet 基线相比表现出很强的结果，极大地扩展了将 GFlowNets 应用于概率推理和各种建模设置的视角。 |
| [^48] | [Minimizing Trajectory Curvature of ODE-based Generative Models.](http://arxiv.org/abs/2301.12003) | 本论文提出通过训练正向过程以最小化生成轨迹的曲率，来优化ODE/SDE-based生成模型的采样速度，实验表明此方法有效。 |
| [^49] | [Minimax estimation of discontinuous optimal transport maps: The semi-discrete case.](http://arxiv.org/abs/2301.11302) | 本文研究了间断最优输运映射的估计问题，在半离散的情况下，提出了一种计算上有效的估计器，以极小极大速率$n^{-1/2}$收敛，不依赖于维度，为解决间断映射的估计问题提供了新思路。 |
| [^50] | [An Analysis of Quantile Temporal-Difference Learning.](http://arxiv.org/abs/2301.04462) | 本文证明了量化时间差分学习（QTD）在一定状态下的收敛概率为1，建立了QTD与非线性微分包含式之间的联系。 |
| [^51] | [Your diffusion model secretly knows the dimension of the data manifold.](http://arxiv.org/abs/2212.12611) | 本研究提出了一种新的方法，利用扩散模型估算数据流形的维度并且在实验中表现出色。 |
| [^52] | [Sliced Optimal Partial Transport.](http://arxiv.org/abs/2212.08049) | 本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。 |
| [^53] | [Simulating first-order phase transition with hierarchical autoregressive networks.](http://arxiv.org/abs/2212.04955) | 本文通过分层自回归神经网络采样算法成功模拟了二维 $Q$ 状态 Potts 模型中的一级相变，并发现该方法在统计不确定性方面有显着改进。预训练技术可以有效地训练大型神经网络。 |
| [^54] | [Generalized Balancing Weights via Deep Neural Networks.](http://arxiv.org/abs/2211.07533) | 本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。 |
| [^55] | [On Proper Learnability between Average- and Worst-case Robustness.](http://arxiv.org/abs/2211.05656) | 本研究研究了在对最坏情况下鲁棒损失的放松下适当的学习问题，提出了鲁棒损失的放宽使得VC分类可适当地用PAC学习算法进行学习，并给出了对于对抗鲁棒性经验风险最小化算法的新的泛化保证。 |
| [^56] | [Deep importance sampling using tensor trains with application to a priori and a posteriori rare event estimation.](http://arxiv.org/abs/2209.01941) | 本论文提出了一种使用张量列进行深度重要性采样的方法。采用平方张量列分解和顺序保持变换组合进行参考分布的推送，通过张量列的可扩展答案构建保序高维变换，设计了比率估计器来计算未归一化概率分布上的期望值。该方法在高维稀有事件估计问题中表现出了更好的方差减小效果。 |
| [^57] | [Lost in the Shuffle: Testing Power in the Presence of Errorful Network Vertex Labels.](http://arxiv.org/abs/2208.08638) | 研究了当网络中存在不匹配/标签混乱的顶点时，两个样本图假设检验中的功率损失，并通过多个实验加以验证。 |
| [^58] | [Deeply-Learned Generalized Linear Models with Missing Data.](http://arxiv.org/abs/2207.08911) | 本文提出了一种深度学习广义线性模型 (dlglm) 及其在处理缺失数据中的应用，其中的方法能够灵活地处理输入特征和响应的可忽略和不可忽略的缺失模式。我们通过统计模拟证明，在缺失数据不随机的情况下，该方法优于现有的监督学习方法，可用于生物医学科学等领域中。 |
| [^59] | [When are Post-hoc Conceptual Explanations Identifiable?.](http://arxiv.org/abs/2206.13872) | 本论文提出了可识别的概念发现方法，可以恢复出多个已知的概念，以确保解释的可靠性。对于具有依赖关系的概念，提出了两种新的方法，利用图像生成过程的功能组合性质。该方法明显优于现有方法。 |
| [^60] | [ForestPrune: Compact Depth-Controlled Tree Ensembles.](http://arxiv.org/abs/2206.00128) | ForestPrune是一种可以通过修剪深度图层来优化树集成的新颖算法框架，它能够在中等规模的数据集和集成中显著压缩模型，从而实现更快的预测速度。 |
| [^61] | [Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey.](http://arxiv.org/abs/2205.08099) | 本文是一项关于在深度学习模型训练过程中通过剪枝和冻结网络参数减少已训练权重数量的调查研究。剪枝方法可以分为初始化时的剪枝、奖励彩票和动态稀疏训练，而冻结权重同样能够减少已训练的权重数量，这些技术可以在降低存储和传输成本的同时提高训练效率。 |
| [^62] | [A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods.](http://arxiv.org/abs/2108.13097) | 本文提出了一种新的无限宽度限制——贝叶斯表示学习限制，旨在解决标准无限宽度限制消除表示学习的问题。该方法可以实现类似于有限宽度模型中的表示学习效果，并保留标准无限宽度限制的简单性。 |
| [^63] | [Near Optimal Adversarial Attack on UCB Bandits.](http://arxiv.org/abs/2008.09312) | 本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。 |
| [^64] | [Quality Inference in Federated Learning with Secure Aggregation.](http://arxiv.org/abs/2007.06236) | 本文研究了在联邦学习中应用安全聚合后，单个训练集的质量信息仍可能被推断并归因于具体参与者的问题，通过图像识别实验找出了参与者相对的质量排序，进而用于检测不良行为、稳定训练性能以及测量参与者的个人贡献。 |

# 详细

[^1]: DoWG展示：一种高效的通用无参数梯度下降方法

    DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])

    [http://arxiv.org/abs/2305.16284](http://arxiv.org/abs/2305.16284)

    本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。

    

    本文提出了一种新的易于实现的无参数梯度优化器：DoWG（Weighted Gradients的距离）。我们证明了该方法是高效的——在不调整任何参数的情况下，匹配优化凸优化中最优调的梯度下降的收敛速度，直到对数因子，并且是通用的——自动适应平滑和非平滑问题。与AdaGrad，Adam或DoG等流行算法计算平方梯度的运行平均值不同，DoWG保持运行平均值的一种新的基于距离的加权版本，这对于实现所需的性质至关重要。据我们所知，DoWG是第一个不需要回溯搜索过程的无参数，高效和通用算法。它还是第一个适应于平稳优化的无参数AdaGrad样式算法。为了补充我们的理论，我们还通过实验证明DoWG在稳定的边缘训练，并证明其在实践中的有效性。

    This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
    
[^2]: 无对抗训练的神经SDE与签名内核评分

    Non-adversarial training of Neural SDEs with signature kernel scores. (arXiv:2305.16274v1 [stat.ML])

    [http://arxiv.org/abs/2305.16274](http://arxiv.org/abs/2305.16274)

    该研究提出一种非对抗性训练神经SDE的新方法，使用基于签名内核的评分规则代替传统的GAN方法。新方法由于不需要进行动态规划运算，具有计算效率更高的特点。

    

    神经SDE是用于生成序列数据的连续时间生成模型。之前通过对抗性训练，将这些模型作为GAN获得了无规则时间序列生成的最新成果。然而，像GAN体系结构一样，训练极不稳定，经常受到模式崩溃的困扰，并需要专门的技术，如重量剪切和梯度惩罚，以缓解这些问题。在本文中，我们介绍了一种基于签名内核的路径空间上新型的评分规则，并将其作为非对抗性训练神经SDE的目标。通过展示此类内核分数的严格适当以及相应的估计器的一致性，我们为极小化器提供了存在和唯一性保证。通过这种公式，评估生成器-判别器对等于解决一组线性路径相关的PDE，这允许记忆效率的伴随反向传播。此外，因为所提出的内核分数，我们不需要通过动态编程方法对评分进行微分，这对于计算效率更高。

    Neural SDEs are continuous-time generative models for sequential data. State-of-the-art performance for irregular time series generation has been previously obtained by training these models adversarially as GANs. However, as typical for GAN architectures, training is notoriously unstable, often suffers from mode collapse, and requires specialised techniques such as weight clipping and gradient penalty to mitigate these issues. In this paper, we introduce a novel class of scoring rules on pathspace based on signature kernels and use them as objective for training Neural SDEs non-adversarially. By showing strict properness of such kernel scores and consistency of the corresponding estimators, we provide existence and uniqueness guarantees for the minimiser. With this formulation, evaluating the generator-discriminator pair amounts to solving a system of linear path-dependent PDEs which allows for memory-efficient adjoint-based backpropagation. Moreover, because the proposed kernel score
    
[^3]: 在协同学习和优化中激励竞争对手诚实行为的研究

    Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])

    [http://arxiv.org/abs/2305.16272](http://arxiv.org/abs/2305.16272)

    这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。

    

    协同学习技术能够让机器学习模型的训练比仅利用单一数据源的模型效果更好。然而，在许多情况下，潜在的参与者是下游任务中的竞争对手，如每个都希望通过提供最佳推荐来吸引客户的公司。这可能会激励不诚实的更新，损害其他参与者的模型，从而可能破坏协作的好处。在这项工作中，我们制定了一个模型来描述这种交互，并在该框架内研究了两个学习任务：单轮均值估计和强凸目标的多轮 SGD。对于一类自然的参与者行为，我们发现理性的客户会被激励强烈地操纵他们的更新，从而防止学习。然后，我们提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。最后，我们通过实验证明了这一点。

    Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
    
[^4]: Koopman核回归

    Koopman Kernel Regression. (arXiv:2305.16215v1 [cs.LG])

    [http://arxiv.org/abs/2305.16215](http://arxiv.org/abs/2305.16215)

    提出了一种基于Koopman核的回归方法，用于预测非线性动力系统的时间演变。该方法在机器人操作，视频预测和交通预测等各种应用中均有优异表现，并具有可证明的学习理论保证。

    

    许多决策制定的机器学习方法，如强化学习，依赖于模拟器或预测模型来预测感兴趣的量的时间演变，例如智能体的状态或策略的奖励。这些复杂现象的预测通常由高度非线性的动力系统描述，使得它们在基于优化的决策制定中的使用具有挑战性。Koopman算子理论通过通过线性动态系统描述预测来解决这个问题。这使得系统分析和长期预测变得简单--只涉及矩阵乘法。然而，将其转化为线性系统通常是非平凡的和未知的，需要基于学习的方法。虽然存在各种方法，但它们通常缺乏关键的学习理论保证，因此所获得的模型在数据和维度增加时的行为通常不清楚。通过提出一种新颖的基于Koopman核的回归方法，我们解决了上述挑战，该方法直接从历史观察中学习到未来预测在Koopman算子空间中的映射。我们的方法享有可证明的学习理论保证，并在广泛的应用中与现有的最先进方法匹配（或优于），包括机器人操作，视频预测和交通预测。

    Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear dynamical systems. This makes system analysis and long-term predictions simple -- involving only matrix multiplications. However, the transformation to a linear system is generally non-trivial and unknown, requiring learning-based approaches. While there exists a variety of approaches, they usually lack crucial learning-theoretic guarantees, such that the behavior of the obtained models with increasing data and dimensionality is often unclear. We address the aforemention
    
[^5]: 火星时间序列分解：一种多尺度嵌套方法中的因子变分自编码器

    Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])

    [http://arxiv.org/abs/2305.16189](http://arxiv.org/abs/2305.16189)

    该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。

    

    无监督的源分离涉及通过混合操作记录的未知源信号的分解，其中对源的先验知识有限，仅可以访问信号混合数据集。这个问题本质上是不适用的，并且进一步受到时间序列数据中源展现出的多种时间尺度的挑战。为了解决这个问题，我们提出了一种无监督的多尺度聚类和源分离框架，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程。在这个表示空间中，我们开发了一个因子高斯混合变分自动编码器，它被训练用于(1)概率地对不同时间尺度上的源进行聚类和逐层非监督源分离，(2)在每个时间尺度上提取低维表示，(3)学习源信号的因子表示，(4)在表示空间中进行采样，以生成未知源信号。我们在MRO上的三个频道的可见数据集上进行了评估，结果表明所提出的方法比目前最先进的技术具有更好的性能。

    Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
    
[^6]: 统一GAN和基于分数扩散的粒子生成模型

    Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v1 [cs.LG])

    [http://arxiv.org/abs/2305.16150](http://arxiv.org/abs/2305.16150)

    本文提出了一个新框架，将生成器训练作为粒子模型的一个推广，从而统一了粒子和对抗生成模型。这个框架可以将生成器集成到基于分数扩散模型中，并在没有生成器的情况下训练GAN。

    

    基于粒子的深度生成模型，例如梯度流和基于分数的扩散模型，由于其惊人的性能而最近受到关注。传统上，通过微分方程来移动粒子分布的方法被普遍认为是与以前广泛使用的生成对抗网络（GAN）相对立的，后者涉及到训练一个向前的生成器网络。在本文中，我们质疑这种解释，并提出了一个统一粒子和对抗生成模型的新框架，通过将生成器训练作为粒子模型的推广。这表明，生成器是任何这样的生成模型的可选附件。因此，将生成器集成到基于分数扩散模型中，并在没有生成器的情况下训练GAN自然地出现在我们的框架中。我们通过实证测试这些原始模型的可行性，这些模型是我们框架可能应用的概念证明。

    Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions by differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper, we challenge this interpretation and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework.
    
[^7]: 揭示基于注意力的图神经网络中的平滑过度现象

    Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])

    [http://arxiv.org/abs/2305.16102](http://arxiv.org/abs/2305.16102)

    本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。

    

    图神经网络中的平滑过度指的是增加网络深度导致节点表示变得相同的现象。尽管之前的研究已经证实了图卷积网络(GCN)会指数级失去表达能力，但是图注意力机制是否可以缓解平滑过度问题还存在争议。本文通过将基于注意力的图神经网络视为非线性时变动态系统，并结合非齐次矩阵乘积和联合谱半径理论的工具和技术，对这个问题进行了严格的数学分析，提出了一个明确的答案。我们证明了与流行观点相反，图注意力机制不能防止平滑过度现象，并且呈指数级失去表达能力。所提出的框架将对称GCN的平滑过度问题扩展到了更广泛的GNN模型类别中。特别地，我们的分析考虑了在现实应用中普遍存在的不对称、状态相关和有向图结构。

    Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
    
[^8]: FAVAS: 带有异步客户端的联邦平均的新型中心化框架

    FAVAS: Federated AVeraging with ASynchronous clients. (arXiv:2305.16099v1 [cs.LG])

    [http://arxiv.org/abs/2305.16099](http://arxiv.org/abs/2305.16099)

    本研究提出了FAVAS算法，是一种用于在资源有限环境下训练DNNs的新型中心化异步联邦学习框架。实验结果表明FAVAS算法优于当前方法。

    

    本文提出了一种新型的中心化异步联邦学习框架FAVAS，用于在资源有限的环境下训练深度神经网络。尽管联邦学习越来越受欢迎，但在大型无线网络上伸缩同步通信变得越来越困难。此外，由于客户端通常具有不同的计算资源和计算速度，异步更新可能会导致显着的偏差（对“快速”客户端更有利）。因此，FL的实际部署需要处理在通信/资源受限的环境中具有强烈变化的计算速度的用户。我们提供了FAVAS在平滑的非凸环境中的收敛性保证，并仔细比较了获得的收敛保证与现有边界（如果有）的差异。实验结果表明，FAVAS算法在标准基准测试中优于当前方法。

    In this paper, we propose a novel centralized Asynchronous Federated Learning (FL) framework, FAVAS, for training Deep Neural Networks (DNNs) in resource-constrained environments. Despite its popularity, ``classical'' federated learning faces the increasingly difficult task of scaling synchronous communication over large wireless networks. Moreover, clients typically have different computing resources and therefore computing speed, which can lead to a significant bias (in favor of ``fast'' clients) when the updates are asynchronous. Therefore, practical deployment of FL requires to handle users with strongly varying computing speed in communication/resource constrained setting. We provide convergence guarantees for FAVAS in a smooth, non-convex environment and carefully compare the obtained convergence guarantees with existing bounds, when they are available. Experimental results show that the FAVAS algorithm outperforms current methods on standard benchmarks.
    
[^9]: 论影响函数、分类影响、相对影响、记忆和泛化

    On Influence Functions, Classification Influence, Relative Influence, Memorization and Generalization. (arXiv:2305.16094v1 [cs.LG])

    [http://arxiv.org/abs/2305.16094](http://arxiv.org/abs/2305.16094)

    本论文通过简化涉及计算的影响函数，提高了大规模机器学习系统的训练效率并保证了模型的准确性，同时影响值的符号可以指示训练点是用于记忆还是泛化。

    

    通常情况下，诸如大规模推荐系统或自然语言处理系统之类的机器学习系统都要在数十亿的训练数据和数百亿或万亿个参数的支持下进行训练。如何改进学习过程，降低训练负荷，提高模型准确性是非常必要的。本文介绍了从简化涉及计算的影响函数的角度出发，探索解决这个问题的第一步。我们讨论了影响计算可以在显著更少的参数下进行的假设。我们还证明了影响值的符号可以指示训练点是用于记忆还是泛化。为此，我们正式定义了对于训练点而言什么是记忆和泛化。我们得出结论，即使对于大型机器学习系统，影响函数也可以实用化。

    Machine learning systems such as large scale recommendation systems or natural language processing systems are usually trained on billions of training points and are associated with hundreds of billions or trillions of parameters. Improving the learning process in such a way that both the training load is reduced and the model accuracy improved is highly desired. In this paper we take a first step toward solving this problem, studying influence functions from the perspective of simplifying the computations they involve. We discuss assumptions, under which influence computations can be performed on significantly fewer parameters. We also demonstrate that the sign of the influence value can indicate whether a training point is to memorize, as opposed to generalize upon. For this purpose we formally define what memorization means for a training point, as opposed to generalization. We conclude that influence functions can be made practical, even for large scale machine learning systems, an
    
[^10]: 一种用于确定固定置信度和以上的 $\varepsilon$-最佳臂辨识算法

    An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond. (arXiv:2305.16041v1 [stat.ML])

    [http://arxiv.org/abs/2305.16041](http://arxiv.org/abs/2305.16041)

    提出一种新颖的采样规则EB-TC $\varepsilon$，用于随机赌博机中的$\varepsilon$-最佳臂的辨识。该规则可用于确定固定置信度或固定预算标识且具备自适应调整勘探参数的渐近最优性。在仿真实验中表现良好，适用于不同问题领域。

    

    我们提出了一种新颖的采样规则EB-TC $\varepsilon$，该规则用于随机赌博机中的$\varepsilon$-最佳臂的辨识。这是第一种用于近似最佳臂辨识的Top Two算法分析实例。 EB-TC $\varepsilon$ 是一种“随时可用”的采样规则，因此可以在没有预算知识的情况下用于确定固定置信度或固定预算标识（无需修改）。我们为EB-TC $\varepsilon$ 提供了三种理论保证。首先，我们证明其在固定置信度设置中预期的样本复杂度上有界，特别是在其勘探参数的自适应调整与组合的情况下呈现其渐近最优性。我们通过在任何时间和对于任何误差参数的概率上界来补充这些发现，这进一步产生其任何时间的简单遗憾上界。最后，我们通过数值模拟表明，与现有算法相比，EB-TC $\varepsilon$ 的性能表现优秀，且适用于不同问题领域。

    We propose EB-TC$\varepsilon$, a novel sampling rule for $\varepsilon$-best arm identification in stochastic bandits. It is the first instance of Top Two algorithm analyzed for approximate best arm identification. EB-TC$\varepsilon$ is an *anytime* sampling rule that can therefore be employed without modification for fixed confidence or fixed budget identification (without prior knowledge of the budget). We provide three types of theoretical guarantees for EB-TC$\varepsilon$. First, we prove bounds on its expected sample complexity in the fixed confidence setting, notably showing its asymptotic optimality in combination with an adaptive tuning of its exploration parameter. We complement these findings with upper bounds on its probability of error at any time and for any error parameter, which further yield upper bounds on its simple regret at any time. Finally, we show through numerical simulations that EB-TC$\varepsilon$ performs favorably compared to existing algorithms, in different
    
[^11]: $L_{2}$正则线性深度神经网络中隐性SGD偏差：从高秩到低秩的单向跳跃。

    Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])

    [http://arxiv.org/abs/2305.16038](http://arxiv.org/abs/2305.16038)

    在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。

    

    具有多个隐藏层的深度线性网络（DLN）的$L_{2}$正则化损失具有多个局部最小值，对应于具有不同秩的矩阵。在矩阵完成等任务中，目标是收敛到最小秩局部最小值，该局部最小值仍适合训练数据。虽然可以轻松避免低估秩的局部最小值，因为它们不适合数据，但梯度下降可能会陷入高估秩的局部最小值。我们证明，使用SGD，总是有从更高秩最小值跳跃到更低秩最小值的概率，但跳回的概率为零。更精确地说，我们定义了一系列集合$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$，使得$B_{r}$包含秩$r$或更少的所有最小值（而不是更多），对于足够小的岭参数$\lambda$和学习率$\eta$，它们是吸收的：SGD离开$B_{r}$的概率为0，从任何起点开始，SGD进入$B_{r}$的概率非零。

    The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
    
[^12]: 当前机器学习需要多少样本才能利用平滑性？

    How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])

    [http://arxiv.org/abs/2305.16014](http://arxiv.org/abs/2305.16014)

    本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。

    

    统计学习的核心原则之一是，目标函数的平滑性可以打破维度灾难。然而，通过泰勒展开学习平滑函数需要足够接近一起的样本来获得高阶导数的有意义估计，这在数据量相对较小的机器学习问题中似乎很困难。本文通过推导广义泛化误差的新的下界，研究了常数和瞬态区域在实践中通常被忽略却发挥了主导作用的问题。

    A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
    
[^13]: 非对数凸和非光滑采样的 Langevin Monte Carlo 算法研究

    Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo Algorithms. (arXiv:2305.15988v1 [stat.ML])

    [http://arxiv.org/abs/2305.15988](http://arxiv.org/abs/2305.15988)

    本文研究了从非对数凸分布进行近似抽样的问题，并通过 Langevin Monte Carlo 算法解决。此外，研究了两种非光滑情况，这些任务源于贝叶斯推断和图像反问题。数值模拟比较了最常用的 Langevin Monte Carlo 算法的性能。

    

    本文研究了从非对数凸分布（例如高斯混合分布）进行近似抽样的问题。我们通过离散过度阻尼 Langevin 扩散所导出的马尔可夫链蒙特卡罗（MCMC）方法来解决这个问题，这些方法通常称为 Langevin Monte Carlo 算法。此外，我们还研究了两种非光滑情况，其中已经开发了大量的近端 MCMC 方法：(i) 考虑到非光滑的先验和高斯混合似然；(ii) 拉普拉斯混合分布。这样的非光滑和非对数凸采样任务源于广泛的贝叶斯推断和图像反问题，如图像反褶积中。我们进行了数值模拟以比较最常用的 Langevin Monte Carlo 算法的性能。

    We study the problem of approximate sampling from non-log-concave distributions, e.g., Gaussian mixtures, which is often challenging even in low dimensions due to their multimodality. We focus on performing this task via Markov chain Monte Carlo (MCMC) methods derived from discretizations of the overdamped Langevin diffusions, which are commonly known as Langevin Monte Carlo algorithms. Furthermore, we are also interested in two nonsmooth cases for which a large class of proximal MCMC methods have been developed: (i) a nonsmooth prior is considered with a Gaussian mixture likelihood; (ii) a Laplacian mixture distribution. Such nonsmooth and non-log-concave sampling tasks arise from a wide range of applications to Bayesian inference and imaging inverse problems such as image deconvolution. We perform numerical simulations to compare the performance of most commonly used Langevin Monte Carlo algorithms.
    
[^14]: 具有马尔可夫噪声的一阶方法：从加速到变分不等式

    First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities. (arXiv:2305.15938v1 [math.OC])

    [http://arxiv.org/abs/2305.15938](http://arxiv.org/abs/2305.15938)

    本论文研究了涉及马尔科夫噪声的随机优化问题，提出了一种适用于非凸和强凸最小化问题的一阶梯度方法，使用基于多层蒙特卡罗方法的随机批处理方案以获得最优线性关系，并消除了以前研究中的限制条件。在马尔可夫噪声下对变分不等式的扩展是原创性的。

    

    本文研究涉及马尔可夫噪声的随机优化问题。我们提出了一个统一的方法来理论分析一阶梯度方法用于解决随机优化和变分不等式的问题。我们的方法涵盖了非凸和强凸最小化问题的情况。为了实现一个依赖于底层噪声序列混合时间的最优(线性)关系，我们使用基于多层蒙特卡罗方法的随机批处理方案。此外，我们的技术允许我们消除以前关于马尔可夫噪声的研究中的限制条件，例如需要有界域和均匀有界随机梯度。我们在马尔可夫噪声下对变分不等式的扩展是原创性的。此外，我们提供了匹配强凸优化问题的理论最优解的下限。

    This paper delves into stochastic optimization problems that involve Markovian noise. We present a unified approach for the theoretical analysis of first-order gradient methods for stochastic optimization and variational inequalities. Our approach covers scenarios for both non-convex and strongly convex minimization problems. To achieve an optimal (linear) dependence on the mixing time of the underlying noise sequence, we use the randomized batching scheme, which is based on the multilevel Monte Carlo method. Moreover, our technique allows us to eliminate the limiting assumptions of previous research on Markov noise, such as the need for a bounded domain and uniformly bounded stochastic gradients. Our extension to variational inequalities under Markovian noise is original. Additionally, we provide lower bounds that match the oracle complexity of our method in the case of strongly convex optimization problems.
    
[^15]: 关于马尔科夫转换模型的可辨识性研究

    On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])

    [http://arxiv.org/abs/2305.15925](http://arxiv.org/abs/2305.15925)

    本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。

    

    最近，潜变量模型的可辨识性因其在可解释性或分布泛化方面的应用而备受关注。本文探讨了作为将最近的结果扩展到序列潜变量模型的第一步的马尔科夫转换模型的可辨识性。我们在第一阶段马尔科夫依赖结构中提出了可辨识性条件，并通过非线性高斯参数化迁移分布。我们的实验展示了我们方法在依赖于政权的因果发现和高维时间序列分割方面的适用性。

    Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
    
[^16]: 基于多模型生成对抗网络的随机动力学学习和精确生成

    Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks. (arXiv:2305.15920v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2305.15920](http://arxiv.org/abs/2305.15920)

    本文使用GAN来学习一个原型晶格上的随机过程，并提出一种合适的多模型程序，可以显著提高精度。GAN似乎是处理复杂统计动力学问题的有前途的工具。

    

    生成对抗网络（GAN）已经在远离物理领域，如文本和图像生成方面展示出了巨大的潜力。本文使用GAN来学习一个原型晶格上的随机过程。通过合理地向原始数据添加噪声，我们成功地将生成器和鉴别器损失函数的值带到了它们的理想值附近。然而，像对抗性方法一样，震荡仍然存在。这会破坏模型选择和生成轨迹的质量。我们展示了，一种合适的多模型程序，在每一步随机选择生成器推进随机轨迹，可以显著提高精度。基于以上发现，GAN似乎是处理复杂统计动力学问题的有前途的工具。

    Generative Adversarial Networks (GANs) have shown immense potential in fields far from physics, such as in text and image generation. Here we use GANs to learn a prototypical stochastic process on a lattice. By suitably adding noise to the original data we succeed in bringing both the Generator and the Discriminator loss functions close to their ideal value. However, as typical for adversarial approaches, oscillations persist. This undermines model selection and the quality of the generated trajectory. We demonstrate that a suitable multi-model procedure where stochastic trajectories are advanced at each step upon randomly selecting a Generator leads to a remarkable increase in accuracy. Based on the reported findings GANs appears as a promising tool to tackle complex statistical dynamics.
    
[^17]: 改进ReLU网络特征学习的神经特征激活值分析

    Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])

    [http://arxiv.org/abs/2305.15912](http://arxiv.org/abs/2305.15912)

    本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。

    

    本文研究了神经网络中单个ReLU单元的特征激活值。我们将ReLU单元在输入空间中对应的特征激活值集合称为ReLU单元的特征激活集。我们建立了特征激活集与ReLU网络中学习特征之间的明确联系，并揭示了现代深度学习架构中使用的各种神经网络规范化技术如何规范化和稳定SGD优化。利用这些洞见，我们提出了一种几何方法来参数化ReLU网络以改进特征学习。我们经验性地验证了其有用性，使用了不那么精心选择的初始化方案和更大的学习率。我们报告了更好的优化稳定性，更快的收敛速度和更好的泛化性能。

    We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
    
[^18]: 指数平滑用于离线策略学习

    Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])

    [http://arxiv.org/abs/2305.15877](http://arxiv.org/abs/2305.15877)

    本文研究了离线学习中最小化风险的倒数倾向评分(IPS)的平滑正则化，推导出了可处理、可扩展、可解释的学习证明，并确定了在何种情况下不需要正则化IPS。

    

    离线策略学习旨在通过最小化风险的倒数倾向评分（IPS）来寻找改进的策略，通常使用记录的赌博数据。在本文中，我们研究了IPS的平滑正则化，推导出了一个双向PAC-Bayes泛化界限。该界限是可处理的、可扩展的、可解释的并提供了学习证明。我们通过一系列学习任务展示了我们方法的相关性和有利的性能。由于我们的界限适用于标准IPS，因此我们能够提供关于何时正则化IPS有用的见解。即，我们确定了不需要正则化的情况。这与在实践中，剪辑IPS常常比OPL中的标准IPS表现更好的信念相反。

    Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
    
[^19]: 学习鲁棒统计用于模型错误情况下的基于模拟推论

    Learning Robust Statistics for Simulation-based Inference under Model Misspecification. (arXiv:2305.15871v1 [stat.ML])

    [http://arxiv.org/abs/2305.15871](http://arxiv.org/abs/2305.15871)

    本研究提出首个通用的方法来处理基于模拟的推论（如ABC和NPE）中由于模型错误引起的不可靠推论。通过约束统计量的选择，我们的方法通过惩罚与数据和模型之间不匹配的统计量来防止不可靠推论结果。我们在高维时间序列模型上进行了实验，证明了本方法的优越性能。

    

    基于模拟的推论方法（如近似贝叶斯计算（ABC），合成似然性和神经后验估计（NPE））依赖于模拟统计量以推断难以计算的似然模型的参数。然而，已知这种方法在模型错误情况下会产生不可信和误导性的推论结果，从而阻碍了它们的广泛应用。在本文中，我们提出了第一个通用方法来处理跨不同类别的SBI方法的模型错误情况。利用统计量的选择确定SBI中的误差程度，我们引入了一个正则化损失函数，惩罚那些增加数据和模型之间不匹配的统计量。以NPE和ABC为应用案例，我们展示了我们的方法在人工错误规范化的高维时间序列模型上表现出优越的性能。我们还将我们的方法应用于来自无线电传播领域的实际数据。

    Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalises those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagat
    
[^20]: 具有高阶激活函数的Barron空间之间的嵌入

    Embeddings between Barron spaces with higher order activation functions. (arXiv:2305.15839v1 [stat.ML])

    [http://arxiv.org/abs/2305.15839](http://arxiv.org/abs/2305.15839)

    本文研究了不同激活函数的Barron空间之间的嵌入，并证明了Barron空间的层次结构类似于Sobolev空间$H^m$。其中，修正功率单位激活函数在这个研究中特别重要。

    

    无限宽浅层神经网络的逼近性质很大程度上取决于激活函数的选择。为了了解这种影响，我们研究了具有不同激活函数的Barron空间之间的嵌入。通过提供用于表示函数$f$的测量$\mu$上的推进映射来证明这些嵌入。一种特别感兴趣的激活函数是给定为$\operatorname{RePU}_s(x)=\max(0,x)^s$的修正功率单位($\operatorname{RePU}$)。对于许多常用的激活函数，可以使用众所周知的泰勒余项定理构造推进映射，这使我们能够证明相关Barron空间嵌入到具有$\operatorname{RePU}$作为激活函数的Barron空间中。此外，与$\operatorname{RePU}_s$相关的Barron空间具有类似于Sobolev空间$H^m$的分层结构。

    The approximation properties of infinitely wide shallow neural networks heavily depend on the choice of the activation function. To understand this influence, we study embeddings between Barron spaces with different activation functions. These embeddings are proven by providing push-forward maps on the measures $\mu$ used to represent functions $f$. An activation function of particular interest is the rectified power unit ($\operatorname{RePU}$) given by $\operatorname{RePU}_s(x)=\max(0,x)^s$. For many commonly used activation functions, the well-known Taylor remainder theorem can be used to construct a push-forward map, which allows us to prove the embedding of the associated Barron space into a Barron space with a $\operatorname{RePU}$ as activation function. Moreover, the Barron spaces associated with the $\operatorname{RePU}_s$ have a hierarchical structure similar to the Sobolev spaces $H^m$.
    
[^21]: 带小总成本限制的上下文信息决策问题与背包问题的相关性，及其对公平性的应用

    Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness. (arXiv:2305.15807v1 [stat.ML])

    [http://arxiv.org/abs/2305.15807](http://arxiv.org/abs/2305.15807)

    本文提出了带总成本限制的上下文信息决策问题（CBwK），通过对术语进行重新组合，对CBwK进行了优化，支持小于$T^{3/4}$的总成本约束，并通过对偶策略实现了平等的成本限制。

    

    本文考虑了带有背包问题的上下文信息决策问题（CBwK），每一轮获得一个标量奖励和一个向量值的成本。我们的目标是最大化累计的奖励，并确保累计成本低于某个预定的成本限制。我们假设环境来自一个连续集合，成本可以带符号，并且未知的期望奖励和成本函数可以被一致地估计，这是文献中的一个典型假设。在这种情况下，迄今为止总成本约束至少要为$T^{3/4}$，其中$T$是轮数，并且甚至通常被假定为与$T$线性相关。然而，我们受到鼓舞，使用CBwK来强制实施实现组之间平均成本平等的公平性约束：与相应成本约束相关的预算应尽可能接近于阶数为$\sqrt{T}$级别的自然偏差。为此，我们介绍了一种基于对偶策略的方法。

    We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated -- a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order $T^{3/4}$, where $T$ is the number of rounds, and were even typically assumed to depend linearly on $T$. We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order $\sqrt{T}$. To that end, we introduce a dual strategy based on 
    
[^22]: 学习集合策略的理论保证及其在时间序列预测中的应用

    Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])

    [http://arxiv.org/abs/2305.15786](http://arxiv.org/abs/2305.15786)

    本文研究了学习集合策略在时间序列预测中的应用，证明了在有限或有限维叠加泛化模型中选择基于交叉验证性能的最优叠加泛化与最优解性能相近。

    

    集合是机器学习中最常用的工具之一，由于其能够有效地减少方差，从而提高泛化性能。针对黑盒基学习器的大多数集合方法都属于“叠加泛化”范畴，即训练一个接受基学习器推理作为输入的机器学习算法。虽然叠加泛化在实践中广泛应用，但其理论性质仍然不为人所知。本文证明了一个新的结果，表明选择基于交叉验证性能的“有限或有限维”叠加泛化中的最佳叠加泛化并不比最优解表现“差得多”。这一结果加强和大大扩展了Van der Laan等人（2007年）的结果。受到理论分析的启发，我们在概率预测的背景下进一步提出了一系列不同敏感性的叠加泛化模型。

    Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
    
[^23]: 差分隐私潜在扩散模型

    Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])

    [http://arxiv.org/abs/2305.15759](http://arxiv.org/abs/2305.15759)

    本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。

    

    扩散模型(DMs)被广泛用于生成高质量图像数据集。然而，由于它们直接在高维像素空间中运行，DMs的优化计算成本高，需要长时间的训练。这导致由于差分隐私的可组合性属性，大量噪音注入到差分隐私学习过程中。为了解决这个挑战，我们提出使用差分隐私训练潜在扩散模型(LDMs)。LDMs使用强大的预训练自编码器将高维像素空间减少到更低维的潜在空间，使训练DMs更加高效和快速。与[Ghalebikesabi等人，2023]预先用公共数据预训练DMs，然后再用隐私数据进行微调不同，我们仅微调LDMs中不同层的注意力模块以获得隐私敏感数据，相对于整个DM微调，可减少大约96%的可训练参数数量。

    Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
    
[^24]: 非稀疏超参数线性模型的贝叶斯分析

    Bayesian Analysis for Over-parameterized Linear Model without Sparsity. (arXiv:2305.15754v1 [math.ST])

    [http://arxiv.org/abs/2305.15754](http://arxiv.org/abs/2305.15754)

    本文提出了一种基于数据的特征向量的先验方法，用于处理非稀疏超参数线性模型。从导出的后验分布收缩率和开发的截断高斯近似两个方面来证明了该方法的有效性，可以解决之前的先验稀疏性限制。

    

    在高维贝叶斯统计学中，发展了许多方法，包括许多先验分布，它们导致估计参数的稀疏性。然而，这种先验在处理数据的谱特征向量结构方面有局限性，因此不适用于分析最近发展的不假设稀疏性的高维线性模型。本文介绍了一种贝叶斯方法，它使用一个依赖于数据协方差矩阵的特征向量的先验，但不会引起参数的稀疏性。我们还提供了导出的后验分布的收缩率，并开发了后验分布的截断高斯近似。前者证明了后验估计的效率，而后者则使用Bernstein-von Mises类型方法来量化参数不确定性。这些结果表明，任何能够处理谱特征向量的贝叶斯方法，都可以用于非稀疏超参数线性模型分析，从而解决了先前的限制。

    In high-dimensional Bayesian statistics, several methods have been developed, including many prior distributions that lead to the sparsity of estimated parameters. However, such priors have limitations in handling the spectral eigenvector structure of data, and as a result, they are ill-suited for analyzing over-parameterized models (high-dimensional linear models that do not assume sparsity) that have been developed in recent years. This paper introduces a Bayesian approach that uses a prior dependent on the eigenvectors of data covariance matrices, but does not induce the sparsity of parameters. We also provide contraction rates of derived posterior distributions and develop a truncated Gaussian approximation of the posterior distribution. The former demonstrates the efficiency of posterior estimation, while the latter enables quantification of parameter uncertainty using a Bernstein-von Mises-type approach. These results indicate that any Bayesian method that can handle the spectrum
    
[^25]: 评估澳大利亚昆士兰省出席幼儿园与儿童发展脆弱性的空间结构（arXiv:2305.15746v1 [stat.ML]）

    Assessing the Spatial Structure of the Association between Attendance at Preschool and Childrens Developmental Vulnerabilities in Queensland Australia. (arXiv:2305.15746v1 [stat.ML])

    [http://arxiv.org/abs/2305.15746](http://arxiv.org/abs/2305.15746)

    该研究使用澳大利亚早期发展普查数据，研究了出席幼儿园与儿童发展脆弱性的关系，发现在出席幼儿园比例较高的地区，儿童至少存在一个发展性脆弱性的比例较低。使用数据分析和机器学习技术，研究人员确定了昆士兰省内的三个不同群集，每个群集特点是不同的社会人口统计变量影响幼儿园出席与发展脆弱性之间的关系。

    

    本研究探讨了出席幼儿园（全日制学校前一年）对儿童在其第一年学校期间发展的影响。使用澳大利亚早期发展普查收集的数据，发现出席幼儿园比例较高的地区往往有较低比例的儿童至少存在一个发展性脆弱性。发展性脆弱性包括不能应对整个学校日（疲惫、饥饿、精力低下）、不能与他人相处或者具有攻击性，以及阅读/写作或数学方面存在问题。这些发现当然会因地区而异。使用数据分析和机器学习，研究人员能够识别出昆士兰省内的三个不同的群集，每个群集的特点是不同的社会人口统计变量影响幼儿园出席与发展脆弱性之间的关系。这些分析有助于理解高脆弱性地区和该分析提供了有关如何在这些区域应用资源的见解。

    The research explores the influence of preschool attendance (one year before full-time school) on the development of children during their first year of school. Using data collected by the Australian Early Development Census, the findings show that areas with high proportions of preschool attendance tended to have lower proportions of children with at least one developmental vulnerability. Developmental vulnerablities include not being able to cope with the school day (tired, hungry, low energy), unable to get along with others or aggressive behaviour, trouble with reading/writing or numbers. These findings, of course, vary by region. Using Data Analysis and Machine Learning, the researchers were able to identify three distinct clusters within Queensland, each characterised by different socio-demographic variables influencing the relationship between preschool attendance and developmental vulnerability. These analyses contribute to understanding regions with high vulnerability and the 
    
[^26]: 时间变化处理的反事实生成模型

    Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])

    [http://arxiv.org/abs/2305.15742](http://arxiv.org/abs/2305.15742)

    本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。

    

    估计平均因果效应是测试新疗法的常用做法。然而，平均效应会掩盖反事实分布中重要的个体特征，可能会引起安全、公平和道德方面的担忧。这个问题在时间设置中更加严重，因为处理是时序的和时变的，对反事实分布产生了错综复杂的影响。本文提出了一种新的条件生成建模方法，以捕获整个反事实分布，允许对反事实分布的某些统计量进行有效推断。这使得所提出的方法尤其适用于医疗保健和公共政策制定领域。我们的生成建模方法通过边际结构模型谨慎地解决了观察数据和目标反事实分布之间的分布不匹配。在合成和真实数据上，我们的方法优于现有的基线方法。

    Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
    
[^27]: 分布式强化学习的好处：小损失边界

    The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])

    [http://arxiv.org/abs/2305.15703](http://arxiv.org/abs/2305.15703)

    通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。

    

    虽然分布式强化学习已经取得了实证成果，但其何时何地有益的问题尚未得到回答。在这项工作中，通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，我们的边界会比非分布式方法更强。作为热身，我们展示了学习成本分布会在情境展开（CB）中导致小损失后悔边界，我们发现分布式CB在三个具有挑战性的任务上比最先进的技术在实证上表现更好。对于在线RL，我们提出了一个分布式版本空间算法，该算法使用最大似然估计构建置信区间，并证明了它在表格MDP中实现了小损失后悔，同时在潜变量模型中享有小损失PAC边界。以类似的见解为基础，我们提出了一个分布式离线RL算法

    While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
    
[^28]: 基于函数ANOVA框架的可解释机器学习: 算法及比较

    Interpretable Machine Learning based on Functional ANOVA Framework: Algorithms and Comparisons. (arXiv:2305.15670v1 [stat.ML])

    [http://arxiv.org/abs/2305.15670](http://arxiv.org/abs/2305.15670)

    本论文探讨了机器学习中可解释性的重要性，并介绍了函数ANOVA框架及其在可解释机器学习中的应用。此外，还概述了两种新开发的可解释性技术，并提出了一种新的算法——基于FANOVA和GAM的可解释的监督学习算法（FANGAM-EBM）。

    

    在机器学习早期，重点是开发复杂算法以获得最佳预测性能。为了理解和解释模型结果，必须依靠事后解释技术，这些技术已经被证明存在一定限制。最近，随着认识到可解释性同样重要，研究人员开始做出妥协来开发固有可解释性的算法而不是追求极致预测表现。在此过程中，机器学习社区重新发掘了函数ANOVA低阶模型的使用方法，这种方法在统计文献中已知。本文首先描述了事后可解释性面临的挑战，并重点介绍了主效应和二阶相互作用。接下来，概述了两种新开发的技术:可解释的增强机器（EBM）（Lou等人，2013）和GAMI-Net（Yang等人，2021b)。最后，本文提出了一个新算法，即基于FANOVA和GAM的可解释监督学习算法（FANGAM-EBM）。

    In the early days of machine learning (ML), the emphasis was on developing complex algorithms to achieve best predictive performance. To understand and explain the model results, one had to rely on post hoc explainability techniques, which are known to have limitations. Recently, with the recognition that interpretability is just as important, researchers are compromising on small increases in predictive performance to develop algorithms that are inherently interpretable. While doing so, the ML community has rediscovered the use of low-order functional ANOVA (fANOVA) models that have been known in the statistical literature for some time. This paper starts with a description of challenges with post hoc explainability and reviews the fANOVA framework with a focus on main effects and second-order interactions. This is followed by an overview of two recently developed techniques: Explainable Boosting Machines or EBM (Lou et al., 2013) and GAMI-Net (Yang et al., 2021b). The paper proposes 
    
[^29]: 联邦复合鞍点优化

    Federated Composite Saddle Point Optimization. (arXiv:2305.15643v1 [cs.LG])

    [http://arxiv.org/abs/2305.15643](http://arxiv.org/abs/2305.15643)

    FeDualEx是第一种在联邦学习范式下同时处理鞍点优化和复合目标的方法，具有效性和高效性。

    

    最近，由于在机器学习中的关键作用，解决鞍点问题的联邦学习方法变得越来越流行。然而现有的方法主要针对欧几里得空间中的平滑无约束目标函数，而机器学习问题往往涉及约束或非平滑正则化，这导致需要进行复合优化。为了应对这些问题，我们提出了联邦对偶外推（FeDualEx），这是一种额外步骤的原始—对偶算法，是第一种在联邦学习范式下包括了鞍点优化和复合目标的方法。收敛分析和实证评估都证明了 FeDualEx 在这些具有挑战性的场景下的有效性。此外，即使对于 FeDualEx 的顺序版本，我们也为随机复合鞍点设置提供了速率，这是据我们所知之前的文献中没有找到的。

    Federated learning (FL) approaches for saddle point problems (SPP) have recently gained in popularity due to the critical role they play in machine learning (ML). Existing works mostly target smooth unconstrained objectives in Euclidean space, whereas ML problems often involve constraints or non-smooth regularization, which results in a need for composite optimization. Addressing these issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-step primal-dual algorithm, which is the first of its kind that encompasses both saddle point optimization and composite objectives under the FL paradigm. Both the convergence analysis and the empirical evaluation demonstrate the effectiveness of FeDualEx in these challenging settings. In addition, even for the sequential version of FeDualEx, we provide rates for the stochastic composite saddle point setting which, to our knowledge, are not found in prior literature.
    
[^30]: 基于密度比估计的半监督学习贝叶斯优化

    Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])

    [http://arxiv.org/abs/2305.15612](http://arxiv.org/abs/2305.15612)

    该论文提出了一种基于密度比估计和半监督学习的贝叶斯优化方法，通过使用监督分类器代替密度比来估计全局最优解的两组数据的类别概率，避免了分类器对全局解决方案过于自信的问题。

    

    贝叶斯优化在科学与工程的多个领域受到了广泛关注，因为它能高效地找到昂贵黑盒函数的全局最优解。通常，一个概率回归模型，如高斯过程、随机森林和贝叶斯神经网络，被广泛用作替代函数，用于模拟在给定输入和训练数据集的情况下函数评估的显式分布。除了基于概率回归的贝叶斯优化，基于密度比估计的贝叶斯优化已被提出来估计相对于全局最优解相对接近和相对远离的两组密度比。为了进一步发展这一研究，可以使用监督分类器来估计这两组的类别概率，而不是密度比。然而，此策略中使用的监督分类器倾向于对全局解决方案过于自信。

    Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
    
[^31]: 采用局部线性模型的变分梯度下降

    Variational Gradient Descent using Local Linear Models. (arXiv:2305.15577v1 [stat.ML])

    [http://arxiv.org/abs/2305.15577](http://arxiv.org/abs/2305.15577)

    本文提出了使用局部线性模型实现目标和粒子分布KL散度降低的新估计器，可以使用样本进行计算而不需要目标得分函数，具有比SVGD更简单有效的计算方法，对于高维度情况下的模型也有优化，提升估计精度。

    

    Stein Variational Gradient Descent (SVGD) 能够沿着轨迹传输粒子，从而减少目标和粒子分布之间的KL散度，但需要目标得分函数来计算更新。我们提出了一种新的SVGD视角，将其视为反向KL梯度流的局部估计器。这种视角启发我们提出了使用局部线性模型来实现相同目的的新估计器。这些提议的估计器可以仅使用目标和粒子分布的样本进行计算，而不需要目标得分函数。我们提议的变分梯度估计器利用了局部线性模型，从而在保持估计偏差与SVGD相当的效果的同时具有计算简便性。此外，我们证明，在温和的假设下，高维梯度流的估计可以转化为一个低维估计问题，从而导致更好的估计精度。我们对提议的方法进行了验证，并对其进行了比较。

    Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We vali
    
[^32]: 基于功能马尔科夫转移算子的深度随机过程

    Deep Stochastic Processes via Functional Markov Transition Operators. (arXiv:2305.15574v1 [stat.ML])

    [http://arxiv.org/abs/2305.15574](http://arxiv.org/abs/2305.15574)

    基于马尔科夫转移算子的神经随机过程MNPs，通过在函数空间中堆叠神经参数化的算子构建，不影响一致性或添加限制，提供了更大的灵活性和表现力。在实验中MNPs表现出优异的性能。

    

    我们引入了一种新的随机过程类别称为马尔科夫神经过程(MNPs)，这种随机过程通过在函数空间中堆叠神经参数化的马尔科夫转移算子构建。我们证明这些马尔科夫转移算子可以保持随机过程的可交换性和一致性。因此，在不妨碍一致性或添加限制的情况下，提出的迭代构建为神经过程(NPs)的原始框架增加了实质性的灵活性和表现力。我们的实验说明MNPs在各种任务中比基准模型具有明显的优势。

    We introduce Markov Neural Processes (MNPs), a new class of Stochastic Processes (SPs) which are constructed by stacking sequences of neural parameterised Markov transition operators in function space. We prove that these Markov transition operators can preserve the exchangeability and consistency of SPs. Therefore, the proposed iterative construction adds substantial flexibility and expressivity to the original framework of Neural Processes (NPs) without compromising consistency or adding restrictions. Our experiments demonstrate clear advantages of MNPs over baseline models on a variety of tasks.
    
[^33]: 本地贝叶斯优化的行为和收敛性

    The Behavior and Convergence of Local Bayesian Optimization. (arXiv:2305.15572v1 [cs.LG])

    [http://arxiv.org/abs/2305.15572](http://arxiv.org/abs/2305.15572)

    本文研究了贝叶斯本地优化策略的行为和收敛性，并在高维问题上提供了强大的实证性能。统计数据表明，单个高斯过程样本路径的本地解比全局方法恢复的预期值更好。Müller等人提出的贝叶斯本地优化算法的收敛速率在有噪音和无噪音的情况下都有推导。

    

    贝叶斯优化中一项最新的发展是使用本地优化策略，与传统的全局策略相比，可以在高维问题上提供强大的实证性能。文献中的“传统智慧”是，专注于本地优化规避了维度诅咒。然而，对于贝叶斯本地优化例程的预期行为或收敛性了解甚少。我们首先研究了本地方法的行为，并发现高斯过程样本路径单个本地解的统计数据与从全局方法恢复的预期值相比非常好。然后，我们展示了最近由Müller等人提出的基于贝叶斯本地优化算法的第一次严格分析，并在有噪音和无噪音的情况下推导出收敛速率。

    A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\"uller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.
    
[^34]: 面向带有长期约束的随机网络资源分配的在线优化问题

    Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints. (arXiv:2305.15558v1 [math.OC])

    [http://arxiv.org/abs/2305.15558](http://arxiv.org/abs/2305.15558)

    本文研究了一个面向随机网络资源分配的在线问题，并提出了一种近乎最优的解决方案，该方案在数字模拟中表现优异。

    

    本论文研究了一个简单通信网络中的在线资源预留问题。网络由两个计算节点组成，通过本地通信链路连接。系统在离散时间内运行；在每个时间段，管理员会在实际作业请求之前为服务器预留资源，这些预留会产生成本。然后，在观察到客户端请求之后，作业可能会从一个服务器转移到另一个服务器，以最好地适应需求，但这会产生额外的传输成本。如果无法满足某些作业请求，则会产生违规成本，需要为每个被阻止的作业支付成本。目标是在有限的时间内最小化总预订成本，同时在一定预算限制下维护累积违规和传输成本。为了研究这个问题，我们将其形式化为一个反复博弈问题，针对一系列提议的策略按随机顺序进行预订。然后，我们设计了一种在线算法，该算法可以实现接近最优的性能保证，以期望的总成本为基础，为任何有限的T时间段。数字模拟表明，我们的算法优于几种基线算法。

    In this paper, we study an optimal online resource reservation problem in a simple communication network. The network is composed of two compute nodes linked by a local communication link. The system operates in discrete time; at each time slot, the administrator reserves resources for servers before the actual job requests are known. A cost is incurred for the reservations made. Then, after the client requests are observed, jobs may be transferred from one server to the other to best accommodate the demands by incurring an additional transport cost. If certain job requests cannot be satisfied, there is a violation that engenders a cost to pay for each of the blocked jobs. The goal is to minimize the overall reservation cost over finite horizons while maintaining the cumulative violation and transport costs under a certain budget limit. To study this problem, we first formalize it as a repeated game against nature where the reservations are drawn randomly according to a sequence of pro
    
[^35]: 短烧化时间MDPs上具有遗憾最优的无模型强化学习

    Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time. (arXiv:2305.15546v1 [cs.LG])

    [http://arxiv.org/abs/2305.15546](http://arxiv.org/abs/2305.15546)

    该论文提出了一种无模型算法，采用方差缩减和自适应执行策略转换技术，在短烧化时间MDPs上实现了遗憾最优，解决了现有算法无法实现最优性和需要付出高昂内存计算成本的问题。

    

    强化学习中一个关键问题是学习最优策略。我们在在线设置下研究了在表格无限时段折扣马尔科夫决策过程中的最优策略学习。现有算法要么无法实现遗憾最优性，要么需要付出高昂的内存和计算成本。此外，在现有的最优算法中，为了实现最优样本效率，所有算法都要经过较长的烧化时间，即只有样本容量超过一个高阈值才能保证最优性。我们通过引入一种无模型算法来解决这两个开放性问题，该算法采用方差缩减和一种慢而自适应的执行策略转换技术。这是折扣设置下第一个具有遗憾最优的无模型算法，并具有低烧化时间的额外优势。

    A crucial problem in reinforcement learning is learning the optimal policy. We study this in tabular infinite-horizon discounted Markov decision processes under the online setting. The existing algorithms either fail to achieve regret optimality or have to incur a high memory and computational cost. In addition, existing optimal algorithms all require a long burn-in time in order to achieve optimal sample efficiency, i.e., their optimality is not guaranteed unless sample size surpasses a high threshold. We address both open problems by introducing a model-free algorithm that employs variance reduction and a novel technique that switches the execution policy in a slow-yet-adaptive manner. This is the first regret-optimal model-free algorithm in the discounted setting, with the additional benefit of a low burn-in time.
    
[^36]: 稀疏网格的核插值

    Kernel Interpolation with Sparse Grids. (arXiv:2305.14451v1 [cs.LG])

    [http://arxiv.org/abs/2305.14451](http://arxiv.org/abs/2305.14451)

    本文提出利用稀疏网格优化结构化核插值方法（SKI），在保证插值准确性的同时，有效解决了输入点维度较高带来的计算困难，提出了相应的稀疏网格矩阵乘法算法以及高效插值方案。

    

    结构化核插值（SKI）通过使用感应点的密集网格插值核协方差函数，加速高斯过程（GP）推断。对应的核矩阵高度结构化，因此易于快速进行线性代数计算。不幸的是，SKI在输入点的维度方面的规模表现不佳，因为密集网格大小随着维度的增加呈指数级增长。为了减轻这个问题，我们建议在SKI框架内使用稀疏网格。这些网格能够进行准确的插值，但点数随维度的增加更慢。我们贡献了一种新的近似线性时间的稀疏网格核矩阵矢量乘法算法。接下来，我们描述了如何将稀疏网格与基于简单形式的高效插值方案相结合。通过这些改进，我们展示了SKI可以扩展到更高的维度而保持准确性。

    Structured kernel interpolation (SKI) accelerates Gaussian process (GP) inference by interpolating the kernel covariance function using a dense grid of inducing points, whose corresponding kernel matrix is highly structured and thus amenable to fast linear algebra. Unfortunately, SKI scales poorly in the dimension of the input points, since the dense grid size grows exponentially with the dimension. To mitigate this issue, we propose the use of sparse grids within the SKI framework. These grids enable accurate interpolation, but with a number of points growing more slowly with dimension. We contribute a novel nearly linear time matrix-vector multiplication algorithm for the sparse grid kernel matrix. Next, we describe how sparse grids can be combined with an efficient interpolation scheme based on simplices. With these changes, we demonstrate that SKI can be scaled to higher dimensions while maintaining accuracy.
    
[^37]: 作为概率推断的降维方法

    Dimensionality Reduction as Probabilistic Inference. (arXiv:2304.07658v1 [stat.ML])

    [http://arxiv.org/abs/2304.07658](http://arxiv.org/abs/2304.07658)

    该论文提出了ProbDR变分框架，将经典降维算法解释为概率推断算法，通过优化一个证据下界来完成推断操作。该框架不仅可以完成常规降维算法，还支持使用概率编程语言进行降维操作，具有强大的表达能力。

    

    降维算法将高维数据压缩到低维表示中，同时保留数据的重要特征。降维是许多分析流程中的关键步骤，因为它实现了数据的可视化、噪声降低和高效的下游处理。在本文中，我们引入了ProbDR变分框架，将广泛的经典DR算法解释为该框架中的概率推断算法。ProbDR包括PCA、CMDS、LLE、LE、MVU、扩散映射、kPCA、Isomap、(t-)SNE和UMAP。在我们的框架中，一个低维潜变量用于构建协方差、精度或图拉普拉斯矩阵，可以作为数据的生成模型的一部分。推断是通过优化一个证据下界来完成的。我们展示了我们框架的内部一致性，并表明它支持使用概率编程语言（PPL）进行DR。此外，我们证明了该框架可以完成常规DR算法的操作，并赋予了它通过概率变分推断的强大表达力。

    Dimensionality reduction (DR) algorithms compress high-dimensional data into a lower dimensional representation while preserving important features of the data. DR is a critical step in many analysis pipelines as it enables visualisation, noise reduction and efficient downstream processing of the data. In this work, we introduce the ProbDR variational framework, which interprets a wide range of classical DR algorithms as probabilistic inference algorithms in this framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps, kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latent variable is used to construct a covariance, precision, or a graph Laplacian matrix, which can be used as part of a generative model for the data. Inference is done by optimizing an evidence lower bound. We demonstrate the internal consistency of our framework and show that it enables the use of probabilistic programming languages (PPLs) for DR. Additionally, we illustrate that the f
    
[^38]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^39]: 关于多标签排名的可学习性研究

    On the Learnability of Multilabel Ranking. (arXiv:2304.03337v1 [cs.LG])

    [http://arxiv.org/abs/2304.03337](http://arxiv.org/abs/2304.03337)

    研究了一系列排名损失函数下多标签排名问题在批处理和在线设置下的可学习性，并首次给出基于可学习性的排名损失函数的等价类。

    

    在机器学习中，多标签排名是一项重要任务，广泛应用于网络搜索、新闻报道、推荐系统等领域。但是，关于多标签排名设置中可学习性的最基本问题仍未解答。本文研究了一系列排名损失函数下多标签排名问题在批处理和在线设置下的可学习性，同时也首次给出了基于可学习性的排名损失函数的等价类。

    Multilabel ranking is a central task in machine learning with widespread applications to web search, news stories, recommender systems, etc. However, the most fundamental question of learnability in a multilabel ranking setting remains unanswered. In this paper, we characterize the learnability of multilabel ranking problems in both the batch and online settings for a large family of ranking losses. Along the way, we also give the first equivalence class of ranking losses based on learnability.
    
[^40]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^41]: PCA-Net：操作学习的复杂性上下界

    Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])

    [http://arxiv.org/abs/2303.16317](http://arxiv.org/abs/2303.16317)

    本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。

    

    神经算子在计算科学和工程中备受关注。PCA-Net是一种最近提出的神经算子架构，它将主成分分析(PCA)与神经网络相结合，以逼近潜在的算子。本文对这种方法进行了近似理论的发展，改进并显着扩展了此方向的以前的工作。在定性界限方面，本文得出了新颖的通用逼近结果，在对潜在算子和数据生成分布的最小假设的前提下。在定量限制方面，本文识别了使用PCA-Net进行高效操作学习的两个潜在障碍，通过导出下界进行了严格证明，第一个障碍与输出分布的复杂性有关，由PCA特征值的缓慢衰减来衡量；另一个障碍涉及无限维输入和输出空间之间的算子空间的内在复杂性。

    Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
    
[^42]: 利用均值场学习重新构想需求侧管理

    Reimagining Demand-Side Management with Mean Field Learning. (arXiv:2302.08190v2 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2302.08190](http://arxiv.org/abs/2302.08190)

    本文提出了一种新的需求侧管理（DSM）方法，即控制大量电气设备遵循所需消费信号的问题，称为MD-MFC算法。该算法直接解决目标跟踪问题，具有较高的有效性和理论保证。

    

    在平衡供需的同时将可再生能源纳入电网是一个复杂问题，鉴于其间歇性。需求侧管理（DSM）为解决这一挑战提供了解决方案。我们提出了一种新的DSM方法，特别是控制大量电气设备遵循所需消费信号的问题。我们将其建模为一个有限时间段马尔科夫均值场控制问题。我们开发了一个新算法，MD-MFC，为凸性和Lipschitz目标函数提供理论保证。MD-MFC与现有负载控制文献的区别在于其在不使用主问题的正则化技术的情况下直接解决目标跟踪问题的有效性。镜像下降方案上的非标准Bregman距离允许使用动态规划获得简单的闭式解。此外，我们展示了一般的均值场博弈算法可以应用于此问题，从而扩展了现有技术。

    Integrating renewable energy into the power grid while balancing supply and demand is a complex issue, given its intermittent nature. Demand side management (DSM) offers solutions to this challenge. We propose a new method for DSM, in particular the problem of controlling a large population of electrical devices to follow a desired consumption signal. We model it as a finite horizon Markovian mean field control problem. We develop a new algorithm, MD-MFC, which provides theoretical guarantees for convex and Lipschitz objective functions. What distinguishes MD-MFC from the existing load control literature is its effectiveness in directly solving the target tracking problem without resorting to regularization techniques on the main problem. A non-standard Bregman divergence on a mirror descent scheme allows dynamic programming to be used to obtain simple closed-form solutions. In addition, we show that general mean-field game algorithms can be applied to this problem, which expands the p
    
[^43]: 变量选择在核双样本检验中的应用

    Variable Selection for Kernel Two-Sample Tests. (arXiv:2302.07415v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.07415](http://arxiv.org/abs/2302.07415)

    本文提出了一种解决双样本检验中变量选择问题的框架，利用核最大均值差异统计量，以最大化方差正则化的MMD统计量。实验结果证明其超群表现。

    

    本文考虑了两样本检验中的变量选择问题，旨在选择区分两组样本的最有信息变量。为了解决该问题，我们提出了一种基于核最大均值差异（MMD）的框架。我们的方法寻求一组变量，其预先确定的大小最大化方差正则化的MMD统计量。这种计算形式也对应于在文献中研究的控制类型I错误的同时最小化异质类型II错误。我们介绍了混合整数编程公式，并提供了线性和二次类型内核函数的精确和近似算法，并具有性能保证。实验结果证明了我们的框架的卓越性能。

    We consider the variable selection problem for two-sample tests, aiming to select the most informative variables to distinguish samples from two groups. To solve this problem, we propose a framework based on the kernel maximum mean discrepancy (MMD). Our approach seeks a group of variables with a pre-specified size that maximizes the variance-regularized MMD statistics. This formulation also corresponds to the minimization of asymptotic type-II error while controlling type-I error, as studied in the literature. We present mixed-integer programming formulations and offer exact and approximation algorithms with performance guarantees for linear and quadratic types of kernel functions. Experimental results demonstrate the superior performance of our framework.
    
[^44]: 序列未确定仪器选择用于因果推断估计

    Sequential Underspecified Instrument Selection for Cause-Effect Estimation. (arXiv:2302.05684v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.05684](http://arxiv.org/abs/2302.05684)

    该研究提出一种序列未确定仪器选择方法，用于处理高维处理变量和仅有有限仪器的因果推断问题，并能够可靠地估计处理效应在插补子空间中的投影。

    

    工具变量（IV）方法用于估计存在未观测混淆的设置中的因果效应，在这种设置中，我们无法直接对处理变量进行实验。仪器是仅通过处理变量间接影响结果的变量。大多数IV应用集中在低维处理上，并且关键需要至少与处理相同数量的仪器。这种假设是有限制性的：在自然科学中，我们经常寻求推断高维处理（例如，基因表达或微生物群在健康和疾病上的影响）的因果效应，但只能使用有限数量的仪器（例如，药物或抗生素）进行少量实验。在这种未指定的问题中，即使在线性情况下，也无法在单个实验中确定完整的处理效应。我们展示了人们仍然可以可靠地恢复处理效应在插补子空间中的投影，并开发了技术来连续组合这样的部分估计。

    Instrumental variable (IV) methods are used to estimate causal effects in settings with unobserved confounding, where we cannot directly experiment on the treatment variable. Instruments are variables which only affect the outcome indirectly via the treatment variable(s). Most IV applications focus on low-dimensional treatments and crucially require at least as many instruments as treatments. This assumption is restrictive: in the natural sciences we often seek to infer causal effects of high-dimensional treatments (e.g., the effect of gene expressions or microbiota on health and disease), but can only run few experiments with a limited number of instruments (e.g., drugs or antibiotics). In such underspecified problems, the full treatment effect is not identifiable in a single experiment even in the linear case. We show that one can still reliably recover the projection of the treatment effect onto the instrumented subspace and develop techniques to consistently combine such partial es
    
[^45]: 基于CVaR的风险敏感强化学习的近最小化风险算法

    Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR. (arXiv:2302.03201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03201](http://arxiv.org/abs/2302.03201)

    本文提出了基于CVaR的风险敏感强化学习算法，在针对多臂老虎机和标签化马尔可夫决策过程问题上，通过提出一种新的伯恩斯坦奖励算法和基于价值迭代的算法，实现了最优或接近最优的风险。

    

    本文研究了风险敏感强化学习(Reinforcement Learning)的目标条件风险价值(CVaR)，并针对多臂老虎机和标签化马尔可夫决策过程问题，提出了一种新的基于上置信界算法的伯恩斯坦奖励算法，以及一种基于价值迭代的算法。我们在连续性假设下证明了我们的算法达到了最优或者接近最优的风险。这些算法都是基于CVaR所实现的。

    In this paper, we study risk-sensitive Reinforcement Learning (RL), focusing on the objective of Conditional Value at Risk (CVaR) with risk tolerance $\tau$. Starting with multi-arm bandits (MABs), we show the minimax CVaR regret rate is $\Omega(\sqrt{\tau^{-1}AK})$, where $A$ is the number of actions and $K$ is the number of episodes, and that it is achieved by an Upper Confidence Bound algorithm with a novel Bernstein bonus. For online RL in tabular Markov Decision Processes (MDPs), we show a minimax regret lower bound of $\Omega(\sqrt{\tau^{-1}SAK})$ (with normalized cumulative rewards), where $S$ is the number of states, and we propose a novel bonus-driven Value Iteration procedure. We show that our algorithm achieves the optimal regret of $\widetilde O(\sqrt{\tau^{-1}SAK})$ under a continuity assumption and in general attains a near-optimal regret of $\widetilde O(\tau^{-1}\sqrt{SAK})$, which is minimax-optimal for constant $\tau$. This improves on the best available bounds. By di
    
[^46]: 基于记忆的元学习在非平稳分布上的应用

    Memory-Based Meta-Learning on Non-Stationary Distributions. (arXiv:2302.03067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03067](http://arxiv.org/abs/2302.03067)

    本文研究基于记忆的元学习在非平稳分布上的应用，重点关注在部分可观察环境中的自然语言和动作-观察序列，研究表明各种类型的基于记忆的神经模型可以准确地逼近贝叶斯最优算法，并执行潜在参数的贝叶斯推断。

    

    基于记忆的元学习是一种逼近贝叶斯最优预测器的技术。在相当一般的条件下，通过最小化顺序预测误差（由对数损失度量）会导致隐式元学习。本文的目标是调查当前序列预测模型和训练方案能否实现这种解释的深度。重点关注具有未观察到的切换点的分段平稳源，很可能捕捉到部分可观察环境中的自然语言和动作-观察序列的一个重要特征。我们展示了各种类型的基于记忆的神经模型（包括变形金刚模型、LSTM和RNN）可以学习准确地逼近已知的贝叶斯最优算法，并表现得好像在每个段内对潜在切换点和控制数据分布的潜在参数执行贝叶斯推断。

    Memory-based meta-learning is a technique for approximating Bayes-optimal predictors. Under fairly general conditions, minimizing sequential prediction error, measured by the log loss, leads to implicit meta-learning. The goal of this work is to investigate how far this interpretation can be realized by current sequence prediction models and training regimes. The focus is on piecewise stationary sources with unobserved switching-points, which arguably capture an important characteristic of natural language and action-observation sequences in partially observable environments. We show that various types of memory-based neural models, including Transformers, LSTMs, and RNNs can learn to accurately approximate known Bayes-optimal algorithms and behave as if performing Bayesian inference over the latent switching-points and the latent parameters governing the data distribution within each segment.
    
[^47]: 连续生成流网络的理论

    A theory of continuous generative flow networks. (arXiv:2301.12594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12594](http://arxiv.org/abs/2301.12594)

    本文提出了一种通用 GFlowNets 的理论，可以适用于连续或混合状态空间，通过实验证明其与非 GFlowNet 基线相比表现出很强的结果，极大地扩展了将 GFlowNets 应用于概率推理和各种建模设置的视角。

    

    生成流网络（GFlowNets）是一种常规化变分推理算法，被训练用于从组合对象的未归一化目标分布中进行采样。GFlowNets 的一个关键限制到目前为止一直是它们仅适用于离散空间。我们提出了一个通用 GFlowNets 的理论，它包括现有的离散 GFlowNets 和连续或混合状态空间中的 GFlowNets，并通过两个目标进行实验。首先，我们阐述了理论的关键点以及各种假设的重要性。其次，我们在几个之前研究的任务中，证明了关于离散 GFlowNets 的观察结果如何转化为连续情况，并与非 GFlowNet 基线相比表现出很强的结果。这项工作极大地扩展了将 GFlowNets 应用于概率推理和各种建模设置的视角。

    Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks. This work greatly widens the perspectives for the application of GFlowNets in probabilistic inference and various modeling settings.
    
[^48]: 最小化ODE生成模型轨迹的曲率

    Minimizing Trajectory Curvature of ODE-based Generative Models. (arXiv:2301.12003v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12003](http://arxiv.org/abs/2301.12003)

    本论文提出通过训练正向过程以最小化生成轨迹的曲率，来优化ODE/SDE-based生成模型的采样速度，实验表明此方法有效。

    

    最近，基于ODE / SDE的生成模型，如扩散模型，矩形流和流匹配，将生成过程定义为固定正向过程的时间反演。尽管这些模型在大规模数据集上表现出令人印象深刻的性能，但数值模拟需要多次评估神经网络，导致采样速度缓慢。我们认为原因在于学习到的生成轨迹具有很高的曲率，因为它与数值求解器的截断误差直接相关。基于正向过程和曲率之间的关系，本文提出一种有效的方法，通过训练正向过程以最小化生成轨迹的曲率，而无需进行任何ODE / SDE模拟。实验表明，我们的方法实现了比先前模型更低的曲率，并因此降低了采样成本，同时保持了竞争性能。

    Recent ODE/SDE-based generative models, such as diffusion models, rectified flows, and flow matching, define a generative process as a time reversal of a fixed forward process. Even though these models show impressive performance on large-scale datasets, numerical simulation requires multiple evaluations of a neural network, leading to a slow sampling speed. We attribute the reason to the high curvature of the learned generative trajectories, as it is directly related to the truncation error of a numerical solver. Based on the relationship between the forward process and the curvature, here we present an efficient method of training the forward process to minimize the curvature of generative trajectories without any ODE/SDE simulation. Experiments show that our method achieves a lower curvature than previous models and, therefore, decreased sampling costs while maintaining competitive performance. Code is available at https://github.com/sangyun884/fast-ode.
    
[^49]: 半离散情况下间断最优输运映射的极小极大估计

    Minimax estimation of discontinuous optimal transport maps: The semi-discrete case. (arXiv:2301.11302v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2301.11302](http://arxiv.org/abs/2301.11302)

    本文研究了间断最优输运映射的估计问题，在半离散的情况下，提出了一种计算上有效的估计器，以极小极大速率$n^{-1/2}$收敛，不依赖于维度，为解决间断映射的估计问题提供了新思路。

    

    本文研究在独立同分布样本的基础上估计两个概率分布P和Q在$\mathbb{R}^d$上的最优输运映射的问题。现有的所有统计分析都要求输运映射是Lipschitz的，这是一个强的要求，特别是排除了输运映射是间断的情况。为了开发间断映射的估计程序，我们考虑了数据分布Q是支撑在$\mathbb{R}^d$上有限点集上的离散测度的重要特例。我们研究了一种计算上有效的估计器，该估计器最初由Pooladian和Niles-Weed(2021)提出，基于熵最优输运，并在半离散情况下证明，它以极小极大速率$n^{-1/2}$收敛，不依赖于维度。其他标准的映射估计技术在这种情况下缺乏有限样本保证，并且被证明受到诅咒的影响

    We consider the problem of estimating the optimal transport map between two probability distributions, $P$ and $Q$ in $\mathbb R^d$, on the basis of i.i.d. samples. All existing statistical analyses of this problem require the assumption that the transport map is Lipschitz, a strong requirement that, in particular, excludes any examples where the transport map is discontinuous. As a first step towards developing estimation procedures for discontinuous maps, we consider the important special case where the data distribution $Q$ is a discrete measure supported on a finite number of points in $\mathbb R^d$. We study a computationally efficient estimator initially proposed by Pooladian and Niles-Weed (2021), based on entropic optimal transport, and show in the semi-discrete setting that it converges at the minimax-optimal rate $n^{-1/2}$, independent of dimension. Other standard map estimation techniques both lack finite-sample guarantees in this setting and provably suffer from the curse 
    
[^50]: 量化时间差分学习的分析

    An Analysis of Quantile Temporal-Difference Learning. (arXiv:2301.04462v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04462](http://arxiv.org/abs/2301.04462)

    本文证明了量化时间差分学习（QTD）在一定状态下的收敛概率为1，建立了QTD与非线性微分包含式之间的联系。

    

    本文分析了一个分布式强化学习算法：量化时间差分学习（QTD），该算法已成为多个成功的强化学习大规模应用的关键组成部分。尽管在实证方面取得了成功，但到目前为止，QTD的理论认识一直难以捉摸。与可以使用标准随机逼近工具来进行分析的经典TD学习不同，QTD的更新并不近似于收缩算子，高度非线性并且可能具有多个不动点。本文的核心结果是证明在与一类动态规划程序的不动点相应的状态下，QTD的收敛概率为1，从而让QTD在理论上得到了确定性的基础。证明通过随机逼近理论和非光滑分析将QTD与非线性微分包含式建立了联系。

    We analyse quantile temporal-difference learning (QTD), a distributional reinforcement learning algorithm that has proven to be a key component in several successful large-scale applications of reinforcement learning. Despite these empirical successes, a theoretical understanding of QTD has proven elusive until now. Unlike classical TD learning, which can be analysed with standard stochastic approximation tools, QTD updates do not approximate contraction mappings, are highly non-linear, and may have multiple fixed points. The core result of this paper is a proof of convergence to the fixed points of a related family of dynamic programming procedures with probability 1, putting QTD on firm theoretical footing. The proof establishes connections between QTD and non-linear differential inclusions through stochastic approximation theory and non-smooth analysis.
    
[^51]: 扩散模型暗中识别数据流形的维度

    Your diffusion model secretly knows the dimension of the data manifold. (arXiv:2212.12611v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12611](http://arxiv.org/abs/2212.12611)

    本研究提出了一种新的方法，利用扩散模型估算数据流形的维度并且在实验中表现出色。

    

    本研究提出了一种使用训练过的扩散模型估算数据流形维度的新框架。扩散模型逐渐逼近目标分布的梯度，即噪声污染版本的对数密度的梯度，不同级别的污染程度对应不同的梯度。我们证明，如果数据集聚焦于高维环境空间中嵌入的流形，那么随着噪声污染程度的降低，梯度会指向流形，因为这个方向是最大似然增加的方向。因此，在污染程度较低时，扩散模型为我们提供了数据流形正常向量的逼近。这使我们能够估计切空间的维度，也就是数据流形的内在维度。据我们所知，我们的方法是基于扩散模型的数据流形维度的第一个估算器，并且胜过了已经成熟的统计方法。

    In this work, we propose a novel framework for estimating the dimension of the data manifold using a trained diffusion model. A diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that, if the data concentrates around a manifold embedded in the high-dimensional ambient space, then as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, for small levels of corruption, the diffusion model provides us with access to an approximation of the normal bundle of the data manifold. This allows us to estimate the dimension of the tangent space, thus, the intrinsic dimension of the data manifold. To the best of our knowledge, our method is the first estimator of the data manifold dimension based on diffusion models and it outperforms well established statis
    
[^52]: 切片最优偏转运输

    Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08049](http://arxiv.org/abs/2212.08049)

    本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。

    

    最优传输（OT）已经在机器学习、数据科学和计算机视觉中变得极其流行。OT问题的核心假设是源和目标测度的总质量相等，这限制了它的应用。最优偏转运输（OPT）是最近提出的解决这个限制的方法。与OT问题类似，OPT的计算依赖于解决线性规划问题（通常在高维度中），这可能会变得计算上困难。在本文中，我们提出了一种计算一维非负测度之间OPT问题的有效算法。接下来，遵循切片OT距离的思想，我们利用切片定义了切片OPT距离。最后，我们展示了切片OPT-based方法在各种数值实验中的计算和精度优势。特别是，我们展示了我们提出的Sliced-OPT在噪声点云配准中的应用。

    Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
    
[^53]: 基于分层自回归网络的一级相变模拟

    Simulating first-order phase transition with hierarchical autoregressive networks. (arXiv:2212.04955v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2212.04955](http://arxiv.org/abs/2212.04955)

    本文通过分层自回归神经网络采样算法成功模拟了二维 $Q$ 状态 Potts 模型中的一级相变，并发现该方法在统计不确定性方面有显着改进。预训练技术可以有效地训练大型神经网络。

    

    我们将分层自回归神经（HAN）网络采样算法应用于二维 $Q$ 状态 Potts 模型，并在 $Q=12$ 时绕过相变进行模拟。我们量化了该方法在一级相变的附近表现，并将其与 Wolff 簇算法进行了比较。我们发现在相似的数值带中，该方法在统计不确定性方面有显着改进。为了有效地训练大型神经网络，我们引入了预训练技术。它使得可以使用较小的系统大小训练某些神经网络，然后将它们用作更大系统大小的起始配置。这是由于我们的分层方法的递归构造而实现的。我们的结果证明了分层方法在具有双峰分布的系统上的性能。此外，我们还提供了在相变附近的自由能和熵的估计。

    We apply the Hierarchical Autoregressive Neural (HAN) network sampling algorithm to the two-dimensional $Q$-state Potts model and perform simulations around the phase transition at $Q=12$. We quantify the performance of the approach in the vicinity of the first-order phase transition and compare it with that of the Wolff cluster algorithm. We find a significant improvement as far as the statistical uncertainty is concerned at a similar numerical effort. In order to efficiently train large neural networks we introduce the technique of pre-training. It allows to train some neural networks using smaller system sizes and then employing them as starting configurations for larger system sizes. This is possible due to the recursive construction of our hierarchical approach. Our results serve as a demonstration of the performance of the hierarchical approach for systems exhibiting bimodal distributions. Additionally, we provide estimates of the free energy and entropy in the vicinity of the ph
    
[^54]: 基于深度神经网络的广义平衡权重

    Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07533](http://arxiv.org/abs/2211.07533)

    本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。

    

    从观测数据中估计因果效应是许多领域中的一个中心问题。一种广泛使用的方法是平衡协变量的权重，使得数据的分布类似于随机化。我们提出了一种称为神经平衡权重（NBW）的广义平衡权重，以估计任意混合离散和连续干预的因果效应。通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重。为此，我们选择了 $\alpha$-差异作为优化的目标函数，因为它具有样本复杂度独立于其地面实况值和无偏小批量梯度的估计器，而且对于梯度消失问题具有优势。此外，我们提供了以下两种方法来估计平衡权重：提高平衡权重的泛化性能和检查其效果。

    Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking 
    
[^55]: 在平均鲁棒性和最坏情况下鲁棒性之间的适当可学习性研究

    On Proper Learnability between Average- and Worst-case Robustness. (arXiv:2211.05656v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05656](http://arxiv.org/abs/2211.05656)

    本研究研究了在对最坏情况下鲁棒损失的放松下适当的学习问题，提出了鲁棒损失的放宽使得VC分类可适当地用PAC学习算法进行学习，并给出了对于对抗鲁棒性经验风险最小化算法的新的泛化保证。

    

    最近，Montasser等人[2019]表明，有限VC维度不足以实现适当的对抗鲁棒PAC学习。鉴于这种困难，学界开始研究在对最坏情况下鲁棒损失的放宽下的适当学习。我们给出了一系列鲁棒损失的放宽，使得VC分类可适当地用PAC学习算法进行学习，其样本复杂度接近于标准PAC学习设置所需的复杂度。另一方面，我们证明了对于一种现有的和自然的最坏情况下鲁棒损失的放宽，有限的VC维度不足以实现适当的学习。最后，我们给出了对于对抗鲁棒性经验风险最小化算法的新的泛化保证。

    Recently, Montasser et al. [2019] showed that finite VC dimension is not sufficient for proper adversarially robust PAC learning. In light of this hardness, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the worst-case robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learnable with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the worst-case robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer.
    
[^56]: 使用张量列进行深度重要性采样及其在先验和后验极端事件估计中的应用

    Deep importance sampling using tensor trains with application to a priori and a posteriori rare event estimation. (arXiv:2209.01941v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.01941](http://arxiv.org/abs/2209.01941)

    本论文提出了一种使用张量列进行深度重要性采样的方法。采用平方张量列分解和顺序保持变换组合进行参考分布的推送，通过张量列的可扩展答案构建保序高维变换，设计了比率估计器来计算未归一化概率分布上的期望值。该方法在高维稀有事件估计问题中表现出了更好的方差减小效果。

    

    我们提出了一种适用于高维问题中估计稀有事件概率的深度重要性采样方法。我们将一般重要性采样问题中的最优重要性分布近似为一个由平方张量列分解形成的顺序保持变换组合下的参考分布推送。张量列提供了一个可扩展的答案，用于通过密度近似构建保序高维变换。沿着一系列过渡密度的映射组成的地图合成减轻了直接近似浓缩密度函数的困难。为了计算未归一化概率分布上的期望值，我们设计了一个比率估计器，使用单独构建的张量列格式的变换组合构建另一个重要性分布来估计归一化常数。与传统的重要性采样相比，这提供了更好的方差减小。我们展示了我们的方法在几个高维稀有事件估计问题上的有效性，包括使用神经网络模型进行的先验和后验估计。

    We propose a deep importance sampling method that is suitable for estimating rare event probabilities in high-dimensional problems. We approximate the optimal importance distribution in a general importance sampling problem as the pushforward of a reference distribution under a composition of order-preserving transformations, in which each transformation is formed by a squared tensor-train decomposition. The squared tensor-train decomposition provides a scalable ansatz for building order-preserving high-dimensional transformations via density approximations. The use of composition of maps moving along a sequence of bridging densities alleviates the difficulty of directly approximating concentrated density functions. To compute expectations over unnormalized probability distributions, we design a ratio estimator that estimates the normalizing constant using a separate importance distribution, again constructed via a composition of transformations in tensor-train format. This offers bett
    
[^57]: 在错误标记的网络顶点存在的情况下进行功耗测试

    Lost in the Shuffle: Testing Power in the Presence of Errorful Network Vertex Labels. (arXiv:2208.08638v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.08638](http://arxiv.org/abs/2208.08638)

    研究了当网络中存在不匹配/标签混乱的顶点时，两个样本图假设检验中的功率损失，并通过多个实验加以验证。

    

    许多两个样本的网络假设检验方法都是在顶点对应在网络之间的隐含假设下运行的。在本文中，我们考虑了当网络中存在不匹配/标签混乱的顶点时，两个样本图假设检验中的功率损失。在随机点乘积和随机块模型网络的背景下，我们在理论上探讨了由于混洗对基于估计的边缘概率矩阵或邻接矩阵之间的Frobenius范数差异的一对假设检验的功率损失。功耗测试的损失通过众多模拟和实验进一步加强，在文献中比较了多个最近提出的测试中的功率损失，在随机块模型和随机点乘积图模型中均有体现。最后，我们通过来自神经科学和社交网络分析的两个例子展示了混洗可能对真实数据测试的影响。

    Many two-sample network hypothesis testing methodologies operate under the implicit assumption that the vertex correspondence across networks is a priori known. In this paper, we consider the degradation of power in two-sample graph hypothesis testing when there are misaligned/label-shuffled vertices across networks. In the context of random dot product and stochastic block model networks, we theoretically explore the power loss due to shuffling for a pair of hypothesis tests based on Frobenius norm differences between estimated edge probability matrices or between adjacency matrices. The loss in testing power is further reinforced by numerous simulations and experiments, both in the stochastic block model and in the random dot product graph model, where we compare the power loss across multiple recently proposed tests in the literature. Lastly, we demonstrate the impact that shuffling can have in real-data testing in a pair of examples from neuroscience and from social network analysi
    
[^58]: 深度学习广义线性模型及其在缺失数据中的应用

    Deeply-Learned Generalized Linear Models with Missing Data. (arXiv:2207.08911v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.08911](http://arxiv.org/abs/2207.08911)

    本文提出了一种深度学习广义线性模型 (dlglm) 及其在处理缺失数据中的应用，其中的方法能够灵活地处理输入特征和响应的可忽略和不可忽略的缺失模式。我们通过统计模拟证明，在缺失数据不随机的情况下，该方法优于现有的监督学习方法，可用于生物医学科学等领域中。

    

    深度学习方法在过去几年中在生物医学科学的监督学习问题中得到了显著应用，但现代生物医学数据集中缺失数据的更加普遍和复杂性给深度学习方法带来了重大挑战。本文提出了一种深度学习广义线性模型(dlglm)的正式处理方法，在训练时能够灵活地处理输入特征和响应的可忽略和不可忽略的缺失模式。我们通过统计模拟证明，在缺失数据不随机的情况下，我们的方法优于现有的监督学习方法。最后，我们以UCI机器学习中的银行营销数据集为例进行了案例研究。

    Deep Learning (DL) methods have dramatically increased in popularity in recent years, with significant growth in their application to supervised learning problems in the biomedical sciences. However, the greater prevalence and complexity of missing data in modern biomedical datasets present significant challenges for DL methods. Here, we provide a formal treatment of missing data in the context of deeply learned generalized linear models, a supervised DL architecture for regression and classification problems. We propose a new architecture, \textit{dlglm}, that is one of the first to be able to flexibly account for both ignorable and non-ignorable patterns of missingness in input features and response at training time. We demonstrate through statistical simulation that our method outperforms existing approaches for supervised learning tasks in the presence of missing not at random (MNAR) missingness. We conclude with a case study of a Bank Marketing dataset from the UCI Machine Learnin
    
[^59]: 后验概念解释何时可识别？

    When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.13872](http://arxiv.org/abs/2206.13872)

    本论文提出了可识别的概念发现方法，可以恢复出多个已知的概念，以确保解释的可靠性。对于具有依赖关系的概念，提出了两种新的方法，利用图像生成过程的功能组合性质。该方法明显优于现有方法。

    

    学习嵌入通常需要通过概念解释来理解和分解，这种需求在解释中不包含有效概念标签的情况下尤为显著。为了提供后验解释，概念发现方法会在已训练的嵌入空间中搜索解释性强的概念，例如物体形状或颜色。与之前的工作不同，我们认为概念发现应该是可识别的，这意味着可以被证明地恢复出多个已知的概念，以确保解释的可靠性。为了作为一个起点，我们明确地将概念发现与传统方法（例如主成分分析和独立成分分析）联系起来，并通过表明它们可以恢复具有非高斯分布的独立概念来阐明这一点。对于具有依赖关系的概念，我们提出了两种新的方法，利用图像生成过程的功能组合性质。我们的可证明可识别的概念发现方法明显优于现有方法。

    Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
    
[^60]: ForestPrune: 紧凑的深度可控树集成

    ForestPrune: Compact Depth-Controlled Tree Ensembles. (arXiv:2206.00128v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.00128](http://arxiv.org/abs/2206.00128)

    ForestPrune是一种可以通过修剪深度图层来优化树集成的新颖算法框架，它能够在中等规模的数据集和集成中显著压缩模型，从而实现更快的预测速度。

    

    树集成是一种强大的模型，能够实现出色的预测性能，但可能会变得庞大而难以控制。这些集成通常会进行后处理(修剪)，以减少内存占用并提高可解释性。我们提出了一种新颖的优化框架ForestPrune，通过从单个树中修剪深度图层，对树集成进行后处理。由于决策树中节点数量随着树的深度呈指数增长，深树的修剪可显著压缩集成。我们开发了一种专门的优化算法，能够在ForestPrune下高效地获得高质量解决方案。我们的算法通常可以在中等规模的数据集和集成中在几秒钟内获得良好的解决方案，具有数万行和数百棵树，结果比现有的方法快得多。我们的实验表明，ForestPrune产生的简洁模型优于现有后处理算法提取的模型。

    Tree ensembles are powerful models that achieve excellent predictive performances, but can grow to unwieldy sizes. These ensembles are often post-processed (pruned) to reduce memory footprint and improve interpretability. We present ForestPrune, a novel optimization framework to post-process tree ensembles by pruning depth layers from individual trees. Since the number of nodes in a decision tree increases exponentially with tree depth, pruning deep trees drastically compactifies ensembles. We develop a specialized optimization algorithm to efficiently obtain high-quality solutions to problems under ForestPrune. Our algorithm typically reaches good solutions in seconds for medium-size datasets and ensembles, with 10000s of rows and 100s of trees, resulting in significant speedups over existing approaches. Our experiments demonstrate that ForestPrune produces parsimonious models that outperform models extracted by existing post-processing algorithms.
    
[^61]: 剪枝和冻结深度神经网络的一部分以进行降维优化的训练方法: 一项调查研究

    Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.08099](http://arxiv.org/abs/2205.08099)

    本文是一项关于在深度学习模型训练过程中通过剪枝和冻结网络参数减少已训练权重数量的调查研究。剪枝方法可以分为初始化时的剪枝、奖励彩票和动态稀疏训练，而冻结权重同样能够减少已训练的权重数量，这些技术可以在降低存储和传输成本的同时提高训练效率。

    

    最先进的深度学习模型的参数计数达到了千亿级别。训练、存储和传输这样的模型是耗费能量和时间的，因此代价很高。其中很大一部分成本是由网络的训练引起的。模型压缩可以降低存储和传输成本，并通过减少前向或后向传递中的计算数量，进一步使训练更有效。因此，在训练时压缩网络并保持高性能是一个重要的研究课题。本文是关于如何在深度学习模型的训练过程中减少已训练权重数量的方法的调查研究。介绍的大多数方法将网络参数设置为零，这被称为剪枝。所介绍的剪枝方法被归类为初始化时的剪枝、奖励彩票和动态稀疏训练。此外，我们还讨论了在随机初始化时冻结网络一部分的方法。通过冻结权重，也可以减少已训练的权重数量。在本次调研中，我们提供了深度神经网络降维优化训练中剪枝和冻结网络参数的当前最先进技术的概述。

    State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of
    
[^62]: 一种深度神经网络中表示学习的理论给出了核方法的深度泛化。

    A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods. (arXiv:2108.13097v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.13097](http://arxiv.org/abs/2108.13097)

    本文提出了一种新的无限宽度限制——贝叶斯表示学习限制，旨在解决标准无限宽度限制消除表示学习的问题。该方法可以实现类似于有限宽度模型中的表示学习效果，并保留标准无限宽度限制的简单性。

    

    现代深度机器学习方法的成功基于它们跨多个层次对输入进行变换以建立良好的高级表示能力。因此，理解这种表示学习过程至关重要。然而，常规的理论方法（正式为NNGPs）涉及无限宽限制消除了表示学习。因此，我们开发了一种新的无限宽限制——贝叶斯表示学习限制，它展现了在有限宽度模型中镜像表示学习的效果，同时保留了一些标准无限宽度限制的简单性。特别地，我们表明在贝叶斯表示学习极限下的深层高斯过程（DGPs）具有确切的多元高斯后验分布，后验协方差可以通过优化一种可解释目标得到，该目标结合了增强性能的对数似然和一系列的KL-散度，使得后验分布接近先验分布。

    The successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a log-likelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We
    
[^63]: UCB Bandits在对抗攻击中的近乎最优攻击策略

    Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.09312](http://arxiv.org/abs/2008.09312)

    本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。

    

    本文考虑了一种随机多臂赌博问题，其中奖励受到对抗性破坏。我们提出了一种新颖的攻击策略，通过操作UCB原则来拉动一些非最优目标臂$T-o(T)$次，累积成本的标度为$\sqrt{\log T}$，其中$T$为回合数。我们还证明了累积攻击成本的第一个下界。我们的下界与我们的上界匹配，除了$\log\log T$因子，表明我们的攻击策略近乎是最优的。

    We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
    
[^64]: 带有安全聚合的联邦学习中的质量推断

    Quality Inference in Federated Learning with Secure Aggregation. (arXiv:2007.06236v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.06236](http://arxiv.org/abs/2007.06236)

    本文研究了在联邦学习中应用安全聚合后，单个训练集的质量信息仍可能被推断并归因于具体参与者的问题，通过图像识别实验找出了参与者相对的质量排序，进而用于检测不良行为、稳定训练性能以及测量参与者的个人贡献。

    

    为了保障个人和商业数据的隐私和机密性，联邦学习算法既考虑了效率，也注重不共享数据。然而，最近的研究表明，这种机制仍可能泄漏敏感信息。因此，在许多实际情况下，采用安全聚合来防止归因于特定参与者。本文重点研究单个训练数据集的质量，并显示即使应用了安全聚合，这样的质量信息仍可能被推断并归因于具体参与者。具体而言，通过一系列图像识别实验，我们推断参与者的相对质量排序。此外，我们将推断出的质量信息应用于检测不良行为、稳定训练性能以及测量参与者的个人贡献。

    Federated learning algorithms are developed both for efficiency reasons and to ensure the privacy and confidentiality of personal and business data, respectively. Despite no data being shared explicitly, recent studies showed that the mechanism could still leak sensitive information. Hence, secure aggregation is utilized in many real-world scenarios to prevent attribution to specific participants. In this paper, we focus on the quality of individual training datasets and show that such quality information could be inferred and attributed to specific participants even when secure aggregation is applied. Specifically, through a series of image recognition experiments, we infer the relative quality ordering of participants. Moreover, we apply the inferred quality information to detect misbehaviours, to stabilize training performance, and to measure the individual contributions of participants.
    

