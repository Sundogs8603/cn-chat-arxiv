# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Conformal Prediction for Time Series with Modern Hopfield Networks.](http://arxiv.org/abs/2303.12783) | 该论文提出了一种名为 HopCPT 的新一致性时间序列预测方法，不仅能够处理时间结构，而且能够利用其优势，已在多种真实世界的时间序列数据集上证明了优于现有方法的性能。 |
| [^2] | [Adaptive Conformal Prediction by Reweighting Nonconformity Score.](http://arxiv.org/abs/2303.12695) | 该论文提出了一种新方法，利用分位数回归森林来学习非拟合分数的分布，并利用其权重分配更多的重要性给残差与测试点相似的样本，从而实现更符合模型的不确定性的预测区间。 |
| [^3] | [EDGI: Equivariant Diffusion for Planning with Embodied Agents.](http://arxiv.org/abs/2303.12410) | EDGI是一种基于模型的强化学习和规划算法，通过等变扩散处理内在对称性，具有更高效的采样和更好的泛化能力，适用于具有内在对称性的机器人操作任务。 |
| [^4] | [Non-asymptotic analysis of Langevin-type Monte Carlo algorithms.](http://arxiv.org/abs/2303.12407) | 本文提出了一种新的Langevin型算法并应用于吉布斯分布。通过提出的2-Wasserstein距离上限，我们发现势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性可以保证算法具有接近零的误差。新的Langevin型算法还可以应用于无凸性或连续可微性的势函数。 |
| [^5] | [Hardness of Independent Learning and Sparse Equilibrium Computation in Markov Games.](http://arxiv.org/abs/2303.12287) | 本文研究了分散式多智能体强化学习的问题，证明了在标准马尔可夫博弈框架下不存在可获得纳什均衡且可独立学习的算法。 |
| [^6] | [Information-Based Sensor Placement for Data-Driven Estimation of Unsteady Flows.](http://arxiv.org/abs/2303.12260) | 本文提出了一种基于数据驱动的流场估计的传感器选择框架，能够使用少量传感器高效地估计高攻角下机翼后流的流场。 |
| [^7] | [A Random Projection k Nearest Neighbours Ensemble for Classification via Extended Neighbourhood Rule.](http://arxiv.org/abs/2303.12210) | 本文提出了一种随机投影的kNN集成分类器，使用扩展邻域规则和降维来增加基本学习者的随机性并保留特征信息。 |
| [^8] | [Universal Approximation Property of Hamiltonian Deep Neural Networks.](http://arxiv.org/abs/2303.12147) | 本文研究了离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络的通用逼近能力，证明了其中的一部分流可以逐渐逼近紧致域上的任何连续函数，为实际使用提供了理论基础。 |
| [^9] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^10] | [On Finite-Step Convergence of the Non-Greedy Algorithm and Proximal Alternating Minimization Method with Extrapolation for $L_1$-Norm PCA.](http://arxiv.org/abs/2302.07712) | 本文研究了非贪婪算法和具有外推的近端交替最小化方法在 $L_1$-范数PCA中的有限步收敛性，证明了在一定条件下，算法生成的迭代点将在有限步骤中保持不变。通过将算法视为交替最优化方法，同时优化条件下固定目标值。 |
| [^11] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^12] | [On Penalty-based Bilevel Gradient Descent Method.](http://arxiv.org/abs/2302.05185) | 本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。 |
| [^13] | [Unconstrained Dynamic Regret via Sparse Coding.](http://arxiv.org/abs/2301.13349) | 本文探讨了在线线性优化（OLO）涉及无约束问题和动态遗憾问题的复杂性，提出了一种通过重新构造问题为稀疏编码的复杂度度量方式，在适应性和应用上有较好的应用价值。 |
| [^14] | [Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification.](http://arxiv.org/abs/2301.08403) | 本文探究了一种单次生成模型的多样性，主要聚焦于子序列相似性如何影响整个序列相似性，并通过生成子序列相似的序列来增强数据集。 |
| [^15] | [Retire: Robust Expectile Regression in High Dimensions.](http://arxiv.org/abs/2212.05562) | 本文提出了一种适用于高维数据的健壮expectile回归方法（Robust Retire），并针对迭代重新加权l1惩罚提出了oracle属性解决了惩罚分位数的非平滑和expectile回归敏感误差分布的问题。实验表明这种方法在预测和变量选择方面的表现都优于其他现有方法。 |
| [^16] | [Unbiased Supervised Contrastive Learning.](http://arxiv.org/abs/2211.05568) | 本文提出了一种新的监督对比损失形式（epsilon-SupInfoNCE）以及一种新的去偏正则化损失（FairKL），旨在解决从有偏数据中学习无偏模型的问题。 |
| [^17] | [Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-free RL.](http://arxiv.org/abs/2206.14057) | 本文提出 Safe reWard-frEe ExploraTion (SWEET)框架，在RF-RL任务中可将安全约束和探索效率同时实现，使得安全探索几乎不会增加额外的样本复杂度。 |
| [^18] | [When Doubly Robust Methods Meet Machine Learning for Estimating Treatment Effects from Real-World Data: A Comparative Study.](http://arxiv.org/abs/2204.10969) | 本研究比较了多种常用的双重稳健方法，探讨了它们使用治疗模型和结果模型的策略异同，并研究了如何结合机器学习技术以提高其性能。 |
| [^19] | [Near-optimal inference in adaptive linear regression.](http://arxiv.org/abs/2107.02266) | 本文提出了一些在线去偏估计的方法来修正自适应线性回归中的渐近偏差，利用数据集中的协方差结构提供更锐利的估计。 |
| [^20] | [Robust and flexible learning of a high-dimensional classification rule using auxiliary outcomes.](http://arxiv.org/abs/2011.05493) | 该论文提出了一种利用所有结果共享的信息来学习高维线性决策规则的方法，其中包括使用多任务学习（MTL）来提高效率，并使用校准步骤来纠正估计偏差。 |
| [^21] | [The generalization error of max-margin linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime.](http://arxiv.org/abs/1911.01544) | 这篇论文在深入探讨现代机器学习分类器的最大间隔线性分类器问题上发现了一些关于过度拟合以及高维渐近性的泛化误差。 |
| [^22] | [Bayesian stochastic blockmodeling.](http://arxiv.org/abs/1705.10225) | 本论文介绍了如何利用贝叶斯推断从复杂网络中提取大规模模块结构，并探讨了其潜在应用，同时展示了将其应用于预测网络中缺失和虚假链接的能力。 |

# 详细

[^1]: 基于现代 Hopfield 网络的时间序列一致性预测方法

    Conformal Prediction for Time Series with Modern Hopfield Networks. (arXiv:2303.12783v1 [cs.LG])

    [http://arxiv.org/abs/2303.12783](http://arxiv.org/abs/2303.12783)

    该论文提出了一种名为 HopCPT 的新一致性时间序列预测方法，不仅能够处理时间结构，而且能够利用其优势，已在多种真实世界的时间序列数据集上证明了优于现有方法的性能。

    

    为了量化不确定性，一致性预测方法受到越来越多的关注，并已成功应用于各个领域。然而，它们难以应用于时间序列，因为时间序列的自相关结构违反了一致性预测所需的基本假设。我们提出了 HopCPT，一种新的基于 Hopfield 网络的时间序列一致性预测方法，不仅能够应对时间结构，而且能够利用它们。我们证明了我们的方法在存在时间依赖性的时间序列中在理论上是有很好的理论基础的。在实验中，我们证明了我们的新方法在四个不同领域的多个真实世界时间序列数据集上优于现有的最先进的一致性预测方法。

    To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
    
[^2]: 非拟合分数重新权重实现自适应一致性预测

    Adaptive Conformal Prediction by Reweighting Nonconformity Score. (arXiv:2303.12695v1 [stat.ML])

    [http://arxiv.org/abs/2303.12695](http://arxiv.org/abs/2303.12695)

    该论文提出了一种新方法，利用分位数回归森林来学习非拟合分数的分布，并利用其权重分配更多的重要性给残差与测试点相似的样本，从而实现更符合模型的不确定性的预测区间。

    

    尽管具有吸引人的理论保证和实际成功，但由一致性预测（CP）给出的预测区间（PI）可能无法反映给定模型的不确定性。这种限制源于CP方法对所有测试点使用常数修正，无视它们的不确定性，以确保覆盖特性。为了解决这个问题，我们提出使用分位数回归森林（QRF）来学习非拟合分数的分布，并利用QRF的权重将更多的重要性分配给残差与测试点相似的样本。这种方法导致的PI长度更符合模型的不确定性。此外，QRF学习到的权重提供了特征空间的划分，通过组合一致化可以实现更高效的计算和改进PI的适应性。我们的方法享有基于样本和基于训练条件的无假设有限覆盖率，并在适当的假设下，也可以

    Despite attractive theoretical guarantees and practical successes, Predictive Interval (PI) given by Conformal Prediction (CP) may not reflect the uncertainty of a given model. This limitation arises from CP methods using a constant correction for all test points, disregarding their individual uncertainties, to ensure coverage properties. To address this issue, we propose using a Quantile Regression Forest (QRF) to learn the distribution of nonconformity scores and utilizing the QRF's weights to assign more importance to samples with residuals similar to the test point. This approach results in PI lengths that are more aligned with the model's uncertainty. In addition, the weights learnt by the QRF provide a partition of the features space, allowing for more efficient computations and improved adaptiveness of the PI through groupwise conformalization. Our approach enjoys an assumption-free finite sample marginal and training-conditional coverage, and under suitable assumptions, it also
    
[^3]: EDGI: 内在对称性规划的等变扩散

    EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v1 [cs.LG])

    [http://arxiv.org/abs/2303.12410](http://arxiv.org/abs/2303.12410)

    EDGI是一种基于模型的强化学习和规划算法，通过等变扩散处理内在对称性，具有更高效的采样和更好的泛化能力，适用于具有内在对称性的机器人操作任务。

    

    内在对称性是时空和排列上的，大多数计划和基于模型的强化学习算法没有考虑这种丰富的几何结构，导致采样效率低和泛化能力弱。本文提出了一种内在对称性规划的等变扩散算法(EDGI), 可用于基于模型的强化学习和规划，并引入一种新的支持多种表示形式的扩散模型。

    Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model i
    
[^4]: Langevin型Monte Carlo算法的非渐进分析

    Non-asymptotic analysis of Langevin-type Monte Carlo algorithms. (arXiv:2303.12407v1 [math.ST])

    [http://arxiv.org/abs/2303.12407](http://arxiv.org/abs/2303.12407)

    本文提出了一种新的Langevin型算法并应用于吉布斯分布。通过提出的2-Wasserstein距离上限，我们发现势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性可以保证算法具有接近零的误差。新的Langevin型算法还可以应用于无凸性或连续可微性的势函数。

    

    本文研究了Langevin型算法应用于吉布斯分布的情况，其中势函数是耗散的，且其弱梯度具有有限的连续性模量。我们的主要结果是2-Wasserstein距离上限的非渐进性，它衡量了吉布斯分布与基于Liptser-Shiryaev理论和函数不等式的Langevin型算法的一般分布之间的距离。我们应用这个上限来展示势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性是充分的，可以通过适当控制参数来获得Langevin Monte Carlo算法的收敛性。我们还针对无凸性或连续可微性的势函数提出了球形平滑技术的Langevin型算法。

    We study the Langevin-type algorithms for Gibbs distributions such that the potentials are dissipative and their weak gradients have the finite moduli of continuity. Our main result is a non-asymptotic upper bound of the 2-Wasserstein distance between the Gibbs distribution and the law of general Langevin-type algorithms based on the Liptser--Shiryaev theory and functional inequalities. We apply this bound to show that the dissipativity of the potential and the $\alpha$-H\"{o}lder continuity of the gradient with $\alpha>1/3$ are sufficient for the convergence of the Langevin Monte Carlo algorithm with appropriate control of the parameters. We also propose Langevin-type algorithms with spherical smoothing for potentials without convexity or continuous differentiability.
    
[^5]: 独立学习和稀疏均衡计算在马尔可夫博弈中的难度

    Hardness of Independent Learning and Sparse Equilibrium Computation in Markov Games. (arXiv:2303.12287v1 [cs.LG])

    [http://arxiv.org/abs/2303.12287](http://arxiv.org/abs/2303.12287)

    本文研究了分散式多智能体强化学习的问题，证明了在标准马尔可夫博弈框架下不存在可获得纳什均衡且可独立学习的算法。

    

    本文研究了马尔可夫博弈中分散式多智能体强化学习的问题。一个基本问题是，是否存在算法，当所有代理采用并在分散方式下独立运行时，每个玩家都可以不后悔地进展，类似于正常形式游戏中的著名收敛结果。虽然最近的研究表明，在受限制的情况下（特别是当后悔与马尔可夫策略的偏离有关时），这种算法存在，但是独立的不后悔学习是否能在标准的马尔可夫博弈框架下实现是值得探讨的问题。我们从计算和统计角度相应地提出了一个明确的否定解决这个问题。

    We consider the problem of decentralized multi-agent reinforcement learning in Markov games. A fundamental question is whether there exist algorithms that, when adopted by all agents and run independently in a decentralized fashion, lead to no-regret for each player, analogous to celebrated convergence results in normal-form games. While recent work has shown that such algorithms exist for restricted settings (notably, when regret is defined with respect to deviations to Markovian policies), the question of whether independent no-regret learning can be achieved in the standard Markov game framework was open. We provide a decisive negative resolution this problem, both from a computational and statistical perspective. We show that:  - Under the widely-believed assumption that PPAD-hard problems cannot be solved in polynomial time, there is no polynomial-time algorithm that attains no-regret in general-sum Markov games when executed independently by all players, even when the game is kno
    
[^6]: 基于信息的传感器放置用于无定常流量的数据驱动估计

    Information-Based Sensor Placement for Data-Driven Estimation of Unsteady Flows. (arXiv:2303.12260v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2303.12260](http://arxiv.org/abs/2303.12260)

    本文提出了一种基于数据驱动的流场估计的传感器选择框架，能够使用少量传感器高效地估计高攻角下机翼后流的流场。

    

    对飞行器周围的无定常流场的估计可能会改善流场交互并导致飞行器性能的提高。尽管流场表示可以是非常高维的，但它们的动态可以具有低阶表示，并且可以使用一些适当放置的测量来估计。本文提出了一个传感器选择框架，用于数据驱动的流场估计。该框架结合了数据驱动建模、稳态卡尔曼滤波器设计和一种用于顺序选择传感器的稀疏化技术。本文还使用传感器选择框架设计了传感器阵列，可以在各种操作条件下表现良好。数值数据上的流量估计结果表明，所提出的框架使用嵌入式压力传感器能够高效地估计高攻角下机翼后流的流场。流场分析表明

    Estimation of unsteady flow fields around flight vehicles may improve flow interactions and lead to enhanced vehicle performance. Although flow-field representations can be very high-dimensional, their dynamics can have low-order representations and may be estimated using a few, appropriately placed measurements. This paper presents a sensor-selection framework for the intended application of data-driven, flow-field estimation. This framework combines data-driven modeling, steady-state Kalman Filter design, and a sparsification technique for sequential selection of sensors. This paper also uses the sensor selection framework to design sensor arrays that can perform well across a variety of operating conditions. Flow estimation results on numerical data show that the proposed framework produces arrays that are highly effective at flow-field estimation for the flow behind and an airfoil at a high angle of attack using embedded pressure sensors. Analysis of the flow fields reveals that pa
    
[^7]: 随机投影k最近邻集成分类器 via 扩展邻域规则

    A Random Projection k Nearest Neighbours Ensemble for Classification via Extended Neighbourhood Rule. (arXiv:2303.12210v1 [stat.ML])

    [http://arxiv.org/abs/2303.12210](http://arxiv.org/abs/2303.12210)

    本文提出了一种随机投影的kNN集成分类器，使用扩展邻域规则和降维来增加基本学习者的随机性并保留特征信息。

    

    基于k最近邻（kNN）的集成将许多基本学习者组合在一起，每个学习者都是基于给定训练数据中的一个样本构建的。这篇论文提出了一种新颖的随机投影扩展邻域规则（RPExNRule）集成，其中来自给定训练数据的自举样本在降低的维度中被随机投影，以增加基本模型的随机性并保留特征信息，并使用扩展邻域规则（ExNRule）将kNN作为基本学习者拟合随机投影的自举样本。

    Ensembles based on k nearest neighbours (kNN) combine a large number of base learners, each constructed on a sample taken from a given training data. Typical kNN based ensembles determine the k closest observations in the training data bounded to a test sample point by a spherical region to predict its class. In this paper, a novel random projection extended neighbourhood rule (RPExNRule) ensemble is proposed where bootstrap samples from the given training data are randomly projected into lower dimensions for additional randomness in the base models and to preserve features information. It uses the extended neighbourhood rule (ExNRule) to fit kNN as base learners on randomly projected bootstrap samples.
    
[^8]: Hamiltonian深度神经网络的万能逼近性质研究

    Universal Approximation Property of Hamiltonian Deep Neural Networks. (arXiv:2303.12147v1 [cs.LG])

    [http://arxiv.org/abs/2303.12147](http://arxiv.org/abs/2303.12147)

    本文研究了离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络的通用逼近能力，证明了其中的一部分流可以逐渐逼近紧致域上的任何连续函数，为实际使用提供了理论基础。

    

    本文研究了由离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络（HDNN）的通用逼近能力。最近的研究表明，HDNN因设计而具有非消失梯度，在训练过程中提供数值稳定性。然而，尽管在几个应用中HDNN已经展示了最先进的性能，但缺少量化其表现力的全面研究。因此，我们提供了一个HDNN的通用逼近定理，并证明了HDNN的一部分流可以逐渐逼近紧致域上的任何连续函数。此结果为实际使用HDNN提供了牢固的理论基础。

    This paper investigates the universal approximation capabilities of Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of Hamiltonian Neural Ordinary Differential Equations. Recently, it has been shown that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical stability during training. However, although HDNNs have demonstrated state-of-the-art performance in several applications, a comprehensive study to quantify their expressivity is missing. In this regard, we provide a universal approximation theorem for HDNNs and prove that a portion of the flow of HDNNs can approximate arbitrary well any continuous function over a compact domain. This result provides a solid theoretical foundation for the practical use of HDNNs.
    
[^9]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^10]: 非贪婪算法和具有外推的近端交替最小化方法在 $L_1$-范数PCA中的有限步收敛性

    On Finite-Step Convergence of the Non-Greedy Algorithm and Proximal Alternating Minimization Method with Extrapolation for $L_1$-Norm PCA. (arXiv:2302.07712v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.07712](http://arxiv.org/abs/2302.07712)

    本文研究了非贪婪算法和具有外推的近端交替最小化方法在 $L_1$-范数PCA中的有限步收敛性，证明了在一定条件下，算法生成的迭代点将在有限步骤中保持不变。通过将算法视为交替最优化方法，同时优化条件下固定目标值。

    

    本文重访了经典的非贪婪算法（NGA）和最近提出的具有外推的近端交替最小化（PAMe）方法在 $L_1$-范数PCA中的应用，并研究了它们的有限步收敛性。文章首先证明了NGA可以作为条件次梯度或者交替最大化方法进行解释。通过将其视为条件次梯度，我们证明了在一定条件下，算法生成的迭代点将在有限步骤中保持不变；当投射维数为一时，可以消除这种条件的限制。而将算法视为交替最优化方法，然后证明目标值在满足一定优化条件的情况下最多在 $\left\lceil\frac{F^{\max}}{\tau_0} \right\rceil$ 步后固定不变。接着，文章分析了NGA的改进形式及其收敛性。

    The classical non-greedy algorithm (NGA) and the recently proposed proximal alternating minimization method with extrapolation (PAMe) for $L_1$-norm PCA are revisited and their finite-step convergence are studied. It is first shown that NGA can be interpreted as a conditional subgradient or an alternating maximization method. By recognizing it as a conditional subgradient, we prove that the iterative points generated by the algorithm will be constant in finitely many steps under a certain full-rank assumption; such an assumption can be removed when the projection dimension is one. By treating the algorithm as an alternating maximization, we then prove that the objective value will be fixed after at most $\left\lceil\frac{F^{\max}}{\tau_0} \right\rceil$ steps, where the stopping point satisfies certain optimality conditions. Then, a slight modification of NGA with improved convergence properties is analyzed. It is shown that the iterative points generated by the modified algorithm will 
    
[^11]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^12]: 基于惩罚的双层梯度下降方法研究

    On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05185](http://arxiv.org/abs/2302.05185)

    本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。

    This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.

    双层优化在超参数优化、元学习和强化学习等领域有广泛应用，但是双层优化问题难以解决。最近的可扩展双层算法主要集中在下层目标函数是强凸或无约束的双层优化问题上。在本文中，我们通过惩罚方法来解决双层问题。我们证明，在一定条件下，惩罚重构可以恢复原始双层问题的解。此外，我们提出了基于惩罚的双层梯度下降（PBGD）算法，并证明了其在下层非强凸约束双层问题上的有限时间收敛性。实验展示了所提出的PBGD算法的效率。

    Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
    
[^13]: 通过稀疏编码实现无约束动态遗憾

    Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13349](http://arxiv.org/abs/2301.13349)

    本文探讨了在线线性优化（OLO）涉及无约束问题和动态遗憾问题的复杂性，提出了一种通过重新构造问题为稀疏编码的复杂度度量方式，在适应性和应用上有较好的应用价值。

    

    受时间序列预测的影响，本研究探讨了在线线性优化（OLO）在两个问题结构的耦合下的情况：域无界，而算法的性能是通过动态遗憾来衡量的。处理任一问题都要求遗憾界限依赖于比较序列的某些复杂度量度 - 特别是无约束OLO中的比较器范数，以及动态遗憾中的路径长度。与最近一篇文章(Jacobsen& Cutkosky，2022)适应这两个复杂度量度相比，我们提出了一种通过重新构造问题为稀疏编码的复杂度度量方式。可以通过一个简单的模块化框架实现适应性，这个框架自然地利用了环境更复杂的前置知识。同时，我们还提出了一种新的静态无约束OLO梯度自适应算法，使用了新颖的连续时间机制设计。这可能是具有独立兴趣的。

    Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
    
[^14]: 通过子序列相似性生成序列：理论及其在无人机识别中的应用

    Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification. (arXiv:2301.08403v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08403](http://arxiv.org/abs/2301.08403)

    本文探究了一种单次生成模型的多样性，主要聚焦于子序列相似性如何影响整个序列相似性，并通过生成子序列相似的序列来增强数据集。

    

    生成人工合成序列的能力在广泛的应用中至关重要，而深度学习架构和生成框架的最新进展已经极大地促进了这一过程。本文使用一种单次生成模型来采样，通过相似性生成子序列，并证明了子序列相似性对整个序列相似性的影响，给出了相应的界限。我们使用一种一次性生成模型来从单个序列的范围内取样，并生成子序列相似的序列，证明了数据集增强方面的实用性。

    The ability to generate synthetic sequences is crucial for a wide range of applications, and recent advances in deep learning architectures and generative frameworks have greatly facilitated this process. Particularly, unconditional one-shot generative models constitute an attractive line of research that focuses on capturing the internal information of a single image or video to generate samples with similar contents. Since many of those one-shot models are shifting toward efficient non-deep and non-adversarial approaches, we examine the versatility of a one-shot generative model for augmenting whole datasets. In this work, we focus on how similarity at the subsequence level affects similarity at the sequence level, and derive bounds on the optimal transport of real and generated sequences based on that of corresponding subsequences. We use a one-shot generative model to sample from the vicinity of individual sequences and generate subsequence-similar ones and demonstrate the improvem
    
[^15]: 高维数据中的健壮性expectile回归方法Robust Retire

    Retire: Robust Expectile Regression in High Dimensions. (arXiv:2212.05562v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2212.05562](http://arxiv.org/abs/2212.05562)

    本文提出了一种适用于高维数据的健壮expectile回归方法（Robust Retire），并针对迭代重新加权l1惩罚提出了oracle属性解决了惩罚分位数的非平滑和expectile回归敏感误差分布的问题。实验表明这种方法在预测和变量选择方面的表现都优于其他现有方法。

    

    高维数据经常表现出由异方差方差或不均匀协变量效应引起的异质性。惩罚的分位数和expectile回归方法提供了在高维数据中检测异方差性的有用工具。前者由于check loss的非平滑特性而具有计算上的挑战性，而后者对重尾误差分布敏感。在本文中，我们提出并研究了（惩罚）健壮expectile回归（retire），重点是迭代重新加权$\ell_1$惩罚，它减少了 $\ell_1$惩罚的估计偏差，并导致了oracle属性。理论上，我们在两种情况下建立了retire估计器的统计性质：（i）当$d\ll n$时的低维情景；（ii）当$s\ll n\ll d$，其中$s$表示显著预测变量的数量。在高维度设定下，我们仔细描述了iteratively reweighted l1-penalized retire estimator的解路径，并开发了有效的算法来解决所得到的优化问题。模拟和实际数据分析表明，所提出的方法在预测精度和变量选择方面优于其他现有的惩罚分位数和expectile回归方法。

    High-dimensional data can often display heterogeneity due to heteroscedastic variance or inhomogeneous covariate effects. Penalized quantile and expectile regression methods offer useful tools to detect heteroscedasticity in high-dimensional data. The former is computationally challenging due to the non-smooth nature of the check loss, and the latter is sensitive to heavy-tailed error distributions. In this paper, we propose and study (penalized) robust expectile regression (retire), with a focus on iteratively reweighted $\ell_1$-penalization which reduces the estimation bias from $\ell_1$-penalization and leads to oracle properties. Theoretically, we establish the statistical properties of the retire estimator under two regimes: (i) low-dimensional regime in which $d \ll n$; (ii) high-dimensional regime in which $s\ll n\ll d$ with $s$ denoting the number of significant predictors. In the high-dimensional setting, we carefully characterize the solution path of the iteratively reweight
    
[^16]: 无偏的监督对比学习

    Unbiased Supervised Contrastive Learning. (arXiv:2211.05568v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05568](http://arxiv.org/abs/2211.05568)

    本文提出了一种新的监督对比损失形式（epsilon-SupInfoNCE）以及一种新的去偏正则化损失（FairKL），旨在解决从有偏数据中学习无偏模型的问题。

    

    许多数据集存在偏差，即它们包含仅在数据集中与目标类高度相关的易于学习的特征，但不在真实的数据分布中。因此，从有偏数据中学习无偏模型已成为近年来非常相关的研究课题。在这项工作中，我们解决了学习对偏差具有鲁棒性的表征的问题。我们首先提出了一种基于边缘的理论框架，可以帮助我们澄清为什么最近的对比损失（InfoNCE，SupCon等）在处理偏差数据时可能失败。基于此，我们推导出了一种新的监督对比损失形式（epsilon-SupInfoNCE），提供了更准确的对正负样本之间最小距离的控制。此外，由于我们的理论框架，我们还提出了FairKL，一种新的去偏正则化损失，即使在极度偏差的数据情况下也可以很好地工作。我们在标准的视觉数据集上验证了所提出的损失。

    Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets inc
    
[^17]: 安全探索在没有奖励反馈的强化学习任务中几乎不会增加样本的复杂度

    Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-free RL. (arXiv:2206.14057v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14057](http://arxiv.org/abs/2206.14057)

    本文提出 Safe reWard-frEe ExploraTion (SWEET)框架，在RF-RL任务中可将安全约束和探索效率同时实现，使得安全探索几乎不会增加额外的样本复杂度。

    

    无奖励反馈的强化学习（RF-RL）是最近引入的一种强化学习范式，依靠随机采取行动来探索未知的环境，没有任何奖励反馈信息。虽然RF-RL中探索阶段的主要目标是在最少轨迹数量的情况下减少对估计模型的不确定性，但在实践中，智能体经常需要同时遵守某些安全约束。目前尚不明确这种安全探索要求会如何影响相应的样本复杂度，以便在规划中实现所得到策略的所需最优性。在本文中，我们首次尝试回答这个问题。具体而言，我们考虑已知一个安全基线策略的情况，提出了一个统一的安全无奖励探索（SWEET）框架。我们然后特化SWEET框架到表格和低秩MDP设置中，并分别开发了被称为Tabular-SWEET和Low-rank-SWEET的算法。这两种算法能够在RF-RL的探索阶段中并入安全约束，并在某些假设下保证可证明的安全性，同时实现所需的性能。我们表明，在我们提出的算法中，安全探索引发的附加样本复杂度几乎为零，这表明安全约束和最优性目标可以同时实现而不会太大地降低样本效率。

    Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Bot
    
[^18]: 当双重稳健方法遇到机器学习：用于从实际数据中估计治疗效果的比较研究

    When Doubly Robust Methods Meet Machine Learning for Estimating Treatment Effects from Real-World Data: A Comparative Study. (arXiv:2204.10969v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2204.10969](http://arxiv.org/abs/2204.10969)

    本研究比较了多种常用的双重稳健方法，探讨了它们使用治疗模型和结果模型的策略异同，并研究了如何结合机器学习技术以提高其性能。

    

    观察性队列研究越来越常用于比较效果研究，以评估治疗方法的安全性。最近，各种双重稳健方法已被提出，通过匹配、加权和回归等不同方式，通过组合治疗模型和结果模型来估计平均治疗效应。双重稳健估计器的关键优势在于，它们要求治疗模型或结果模型之一被正确规定，以获得平均治疗效果的一致估计值，从而导致更准确、通常更精确的推断。然而，很少有工作去理解双重稳健估计器由于使用治疗和结果模型的独特策略如何不同以及如何结合机器学习技术以提高其性能。在这里，我们检查了多个受欢迎的双重稳健方法，并使用不同的治疗和结果模型比较它们的性能。

    Observational cohort studies are increasingly being used for comparative effectiveness research to assess the safety of therapeutics. Recently, various doubly robust methods have been proposed for average treatment effect estimation by combining the treatment model and the outcome model via different vehicles, such as matching, weighting, and regression. The key advantage of doubly robust estimators is that they require either the treatment model or the outcome model to be correctly specified to obtain a consistent estimator of average treatment effects, and therefore lead to a more accurate and often more precise inference. However, little work has been done to understand how doubly robust estimators differ due to their unique strategies of using the treatment and outcome models and how machine learning techniques can be combined to boost their performance. Here we examine multiple popular doubly robust methods and compare their performance using different treatment and outcome modeli
    
[^19]: 自适应线性回归中的近最优推断

    Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2107.02266](http://arxiv.org/abs/2107.02266)

    本文提出了一些在线去偏估计的方法来修正自适应线性回归中的渐近偏差，利用数据集中的协方差结构提供更锐利的估计。

    

    当数据以自适应方式收集时，即使是最简单的方法如普通最小二乘法也可能表现出非正常的渐近行为。 作为不良后果，基于渐近正常性的假设检验和置信区间可能导致错误结果。 我们提出了一些在线去偏估计的方法来纠正这些误差，并利用数据集中存在的协方差结构，在其更多信息已累积的方向上提供更锐利的估计。 我们在数据收集过程的温和条件下证明了我们提出的在线去偏估计的渐近正态性质，并提供了渐近精确的置信区间。 我们还针对自适应线性回归问题证明了最小化下界，从而提供了比较估计器的基线。 在我们提出的估计器达到最小值的各种条件下，最小化下界均实现。

    When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals. We additionally prove a minimax lower bound for the adaptive linear regression problem, thereby providing a baseline by which to compare estimators. There are various conditions under which our proposed estimators achieve the minimax lo
    
[^20]: 利用辅助结果强韧灵活地学习高维分类规则

    Robust and flexible learning of a high-dimensional classification rule using auxiliary outcomes. (arXiv:2011.05493v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2011.05493](http://arxiv.org/abs/2011.05493)

    该论文提出了一种利用所有结果共享的信息来学习高维线性决策规则的方法，其中包括使用多任务学习（MTL）来提高效率，并使用校准步骤来纠正估计偏差。

    

    相关结果在许多实际问题中很常见。在某些情况下，一个结果特别重要，而其他结果是辅助的。为了利用所有结果共享的信息，传统的多任务学习（MTL）最小化了所有结果的平均损失函数，这可能会在MTL模型被错误说明时导致目标结果的估计偏差。基于对估计偏差的分解，我们提出了一种强韧的转移学习方法，用于估计具有辅助结果存在的高维线性决策规则，该方法包括使用所有结果进行MTL步骤以获得效率，以及随后使用仅感兴趣的结果进行校准步骤，以修正两种类型的偏差。我们表明，最终的估计器可以实现比仅使用单个感兴趣结果的估计器具有更低的估计误差。

    Correlated outcomes are common in many practical problems. In some settings, one outcome is of particular interest, and others are auxiliary. To leverage information shared by all the outcomes, traditional multi-task learning (MTL) minimizes an averaged loss function over all the outcomes, which may lead to biased estimation for the target outcome, especially when the MTL model is mis-specified. In this work, based on a decomposition of estimation bias into two types, within-subspace and against-subspace, we develop a robust transfer learning approach to estimating a high-dimensional linear decision rule for the outcome of interest with the presence of auxiliary outcomes. The proposed method includes an MTL step using all outcomes to gain efficiency, and a subsequent calibration step using only the outcome of interest to correct both types of biases. We show that the final estimator can achieve a lower estimation error than the one using only the single outcome of interest. Simulations
    
[^21]: 最大间隔线性分类器的泛化误差：过度拟合和超参数区域的高维渐近性。

    The generalization error of max-margin linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime. (arXiv:1911.01544v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/1911.01544](http://arxiv.org/abs/1911.01544)

    这篇论文在深入探讨现代机器学习分类器的最大间隔线性分类器问题上发现了一些关于过度拟合以及高维渐近性的泛化误差。

    

    现代机器学习分类器通常在训练集上表现出消失的分类误差。它们通过学习将数据映射到线性可分的类别的输入的非线性表示来实现这一点。受这些现象的启发，我们重新审视线性可分数据的高维最大间隔分类问题。我们考虑这样一个设定: 数据$(y_i,{\boldsymbol x}_i)$， $i\le n$独立同分布，其中${\boldsymbol x}_i\sim \mathsf{N}({\boldsymbol 0},{\boldsymbol \Sigma})$为$p$维高斯特征向量，$y_i \in\{+1,-1\}$表示标签，其分布取决于协变量${\boldsymbol \theta}_*$ 和 ${\boldsymbol x}_i$的线性组合。虽然高斯模型可能看起来非常简单，但普遍性论据可用于表明在这个设置中得出的结果也适用于某些非线性映射的输出。我们考虑比例渐近关系$n,p\to\infty$，且$p/n\to\psi$，

    Modern machine learning classifiers often exhibit vanishing classification error on the training set. They achieve this by learning nonlinear representations of the inputs that maps the data into linearly separable classes.  Motivated by these phenomena, we revisit high-dimensional maximum margin classification for linearly separable data. We consider a stylized setting in which data $(y_i,{\boldsymbol x}_i)$, $i\le n$ are i.i.d. with ${\boldsymbol x}_i\sim\mathsf{N}({\boldsymbol 0},{\boldsymbol \Sigma})$ a $p$-dimensional Gaussian feature vector, and $y_i \in\{+1,-1\}$ a label whose distribution depends on a linear combination of the covariates $\langle {\boldsymbol \theta}_*,{\boldsymbol x}_i \rangle$. While the Gaussian model might appear extremely simplistic, universality arguments can be used to show that the results derived in this setting also apply to the output of certain nonlinear featurization maps.  We consider the proportional asymptotics $n,p\to\infty$ with $p/n\to \psi$,
    
[^22]: 贝叶斯随机块模型

    Bayesian stochastic blockmodeling. (arXiv:1705.10225v9 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1705.10225](http://arxiv.org/abs/1705.10225)

    本论文介绍了如何利用贝叶斯推断从复杂网络中提取大规模模块结构，并探讨了其潜在应用，同时展示了将其应用于预测网络中缺失和虚假链接的能力。

    

    本章节提供了一个自包含的介绍，教授如何使用贝叶斯推断从网络数据中提取大规模模块结构，基于随机块模型(SBM)，以及其校正度量和重叠推广。我们着重于非参数的表述，允许它们在有效防止过拟合和实现模型选择的情况下推断。我们讨论了先验选择的方面，特别是如何通过增加贝叶斯层次结构避免欠拟合，并对比了从后验分布中采样网络分区和寻找最大化后验分布的单个点估计之间的任务，同时描述了执行任何一个任务的高效算法。我们还展示了如何利用推断SBM来预测缺失和虚假链接，并揭示了网络中模块化结构的可检测性的基本限制。

    This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic blockmodel (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects of the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks.
    

