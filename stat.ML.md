# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And Efficient Combinatorial Materials Design.](http://arxiv.org/abs/2310.15124) | 本论文针对工程设计中存在的混合变量问题，开发了一种基于元模型的混合变量全局敏感性分析方法，通过数值案例研究验证了该方法的有效性。 |
| [^2] | [Evaluating machine learning models in non-standard settings: An overview and new findings.](http://arxiv.org/abs/2310.15108) | 本文综述了在非标准环境中评估机器学习模型的准则和方法，包括聚类数据、空间数据、不均匀采样概率、概念漂移和层次结构化结果。其中的统一原则是在重采样过程中使用的测试数据应反映出模型将要应用的新观测数据，而训练数据应代表整个数据集。 |
| [^3] | [Fast 2D Bicephalous Convolutional Autoencoder for Compressing 3D Time Projection Chamber Data.](http://arxiv.org/abs/2310.15026) | 本研究提出了一种快速的二头卷积自编码器用于压缩三维时间投影室数据，在压缩率和重建精度方面优于传统方法。 |
| [^4] | [The Fundamental Dilemma of Bayesian Active Meta-learning.](http://arxiv.org/abs/2310.14968) | 在贝叶斯主动元学习中，贪婪追求可转移知识可能会损害对可转移参数的估计，学习者面临任务识别和可转移知识获取之间的困境。 |
| [^5] | [Adam through a Second-Order Lens.](http://arxiv.org/abs/2310.14963) | 该论文提出了AdamQLR，它是一个通过将K-FAC中的技术与Adam的更新方法相结合的优化器，通过考虑二阶数据上的Adam行为而得到启发。在回归和分类任务上进行了评估，结果显示AdamQLR在运行时间和推广性能方面表现出良好的竞争力。 |
| [^6] | [Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks.](http://arxiv.org/abs/2310.14901) | 本文提出了一种以Hessian-Vector乘积系列为基础的优化算法，通过平方根和求逆操作实现了高效可伸缩的优化方法，并相对于其他一阶和二阶优化方法在运行时间和性能上具有可比性。 |
| [^7] | [Boosting for Bounding the Worst-class Error.](http://arxiv.org/abs/2310.14890) | 该论文提出了一种基于Boosting的算法，可以保证最差类别训练误差的上界，并降低了最差类别的测试误差率。 |
| [^8] | [Sharp error bounds for imbalanced classification: how many examples in the minority class?.](http://arxiv.org/abs/2310.14826) | 该论文提出了两个在稀有类概率趋近于零的情况下的新贡献，分别是一种非渐近快速率概率界限和一种一致上界估计方法，这些发现为在实际情况下的类别加权提供了更清晰的理解，为进一步的研究提供了新的方向。 |
| [^9] | [The evolving of Data Science and the Saudi Arabia case. How much have we changed in 13 years?.](http://arxiv.org/abs/2310.14808) | 过去13年中的数据科学词汇使用情况进行了全面考察，研究发现了引入新词汇的文献，并探索了这些词汇如何被纳入科学文献中。同时分析了沙特阿拉伯的科学出版物与整体结果之间的比较。 |
| [^10] | [Principled Approaches for Learning to Defer with Multiple Experts.](http://arxiv.org/abs/2310.14774) | 我们研究了多个专家学习推迟问题的代理损失和算法，并证明了这些代理损失函数具有强H一致性界限。我们展示了几个实际应用的代理损失函数，并设计了基于最小化这些损失函数的新的学习推迟算法。我们还进行了在SVHN和CIFAR-10数据集上的实验。 |
| [^11] | [Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms.](http://arxiv.org/abs/2310.14772) | 我们研究了多类别分类设置中的学习与放弃框架，并提出了一系列新的理论和算法结果，解决了两个现存的开放问题。这些保证为基于最小化放弃损失的新的多类别放弃算法提供了启示。 |
| [^12] | [Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention.](http://arxiv.org/abs/2310.14770) | 本文提出了基于分数的多类放弃的理论基础损失函数和算法，包括引入了新的代理损失函数族群以及证明了这些代理损失的一致性保证。我们通过实验证明了这些算法的实际意义。 |
| [^13] | [Externally Valid Policy Evaluation Combining Trial and Observational Data.](http://arxiv.org/abs/2310.14763) | 这项研究提出了一种结合试验和观察数据的外部有效策略评估方法，利用试验数据对目标人群上的政策结果进行有效推断，并给出了可验证的评估结果。 |
| [^14] | [Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks.](http://arxiv.org/abs/2310.14720) | 本研究提出了一种名为EDAIn的扩展深度自适应输入规范化层，通过以端到端的方式学习如何适当地规范化时序数据，而不是使用固定的规范化方案，来提高深度神经网络在时序预测和分类任务中的性能。实验证明该方法在不同数据集上都取得了良好效果。 |
| [^15] | [Random Forest Dissimilarity for High-Dimension Low Sample Size Classification.](http://arxiv.org/abs/2310.14710) | 针对高维度低样本(HDLSS)分类问题，本论文提出了一种基于随机森林相似度的学习预计算SVM核方法(RFSVM)，通过在40个公共HDLSS分类数据集上的实验证明，该方法在HDLSS问题上优于现有方法并且保持了非常一致的结果。 |
| [^16] | [Estimation of forest height and biomass from open-access multi-sensor satellite imagery and GEDI Lidar data: high-resolution maps of metropolitan France.](http://arxiv.org/abs/2310.14662) | 本研究利用开放获取的卫星图像和GEDI激光雷达数据，采用机器学习方法，在法国的大范围内估计了森林高度和生物量，并生成了2020年的高分辨率地图。 |
| [^17] | [Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy.](http://arxiv.org/abs/2310.14661) | 本文介绍了一种基于纯差分隐私和高斯差分隐私的可计算MCMC私有学习方法，通过引入近似采样扰动算法，结合Metropolis-Hastings算法和局部化步骤，实现了对隐私的保护并获得了较好的收敛性能。 |
| [^18] | [CAD-DA: Controllable Anomaly Detection after Domain Adaptation by Statistical Inference.](http://arxiv.org/abs/2310.14608) | 本论文提出了CAD-DA方法，可在领域适应的情况下进行可控异常检测。该方法通过利用条件选择性推断来处理领域适应的影响，并在合成和真实数据集上进行了评估。 |
| [^19] | [Trigonometric Quadrature Fourier Features for Scalable Gaussian Process Regression.](http://arxiv.org/abs/2310.14544) | 本研究提出了一种新的三角积分傅里叶特征（TQFF）方法，通过使用一种特定于所需傅里叶变换的新颖非高斯积分规则，解决了现有傅里叶特征逼近方法的性能限制问题，并展示了其相对于随机傅里叶特征（RFF）和高斯积分傅里叶特征（QFF）的改进性能。 |
| [^20] | [On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers.](http://arxiv.org/abs/2310.14421) | 本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。 |
| [^21] | [DePAint: A Decentralized Safe Multi-Agent Reinforcement Learning Algorithm considering Peak and Average Constraints.](http://arxiv.org/abs/2310.14348) | 本文提出了一种分散的安全多智能体强化学习算法，能够在没有中央控制器的情况下，通过智能体之间的邻居通信来最大化累积奖励总和，并同时满足每个智能体的安全约束限制。 |
| [^22] | [Finite-Sample Analysis of the Temporal Difference Learning.](http://arxiv.org/abs/2310.14286) | 本文提出了一种时间差异学习方法，通过线性函数逼近策略评估，在在线推断中获得了接近最优的性能，并提供了相应的样本复杂度界限。 |
| [^23] | [A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts.](http://arxiv.org/abs/2310.14188) | 该论文提出了一种通用理论，用于研究Softmax Gating Multinomial Logistic Mixture of Experts模型。通过建立模型的收敛速度，揭示了softmax gating和专家函数之间存在的互作用，同时提出了一种修改后的softmax gating函数。 |
| [^24] | [Are LSTMs Good Few-Shot Learners?.](http://arxiv.org/abs/2310.14139) | 本研究重新考察了使用LSTM进行少样本学习的方法，在简单的回归问题上表现优于流行的元学习技术MAML，但在复杂的图像分类问题上不及预期。研究提出了一种名为OP-LSTM的新方法来解决这些问题，并取得了显著的性能改进。 |
| [^25] | [Optimal Batched Best Arm Identification.](http://arxiv.org/abs/2310.14129) | 本论文研究了最佳批处理武器识别问题，在渐近和非渐近设置中提出了Tri-BBAI和Opt-BBAI算法，分别实现了最优和几乎最优的样本和批处理复杂度。 |
| [^26] | [On discretisation drift and smoothness regularisation in neural network training.](http://arxiv.org/abs/2310.14036) | 本研究旨在提高对深度学习优化和模型正则化的理解，研究了梯度下降算法的动力学特性和离散漂移问题。 |
| [^27] | [ASBART:Accelerated Soft Bayes Additive Regression Trees.](http://arxiv.org/abs/2310.13975) | ASBART是基于贝叶斯加法回归树（BART）的一种加速软件，相较于传统的软BART方法，在保持准确度的情况下提高了计算速度。 |
| [^28] | [Distributed Linear Regression with Compositional Covariates.](http://arxiv.org/abs/2310.13969) | 本文提出了针对大规模组合数据中的分布式稀疏惩罚线性对比模型的两种分布式优化技术，分别采用集中式和分散式拓扑结构，以获得具有较低通信开销的正则化估计。 |
| [^29] | [Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression.](http://arxiv.org/abs/2310.13966) | 本文主要研究了在再生核希尔伯特空间中的非参数回归的传递学习问题，提出了两种情况下的解决方法，并分别给出了统计性质和最优性结果。 |
| [^30] | [Distributionally Robust Optimization with Bias and Variance Reduction.](http://arxiv.org/abs/2310.13863) | 本论文提出了一种可以减小偏差和方差的分布鲁棒优化方法，通过谱风险和$f$-散度罚项来解决风险敏感学习问题。提出的Prospect算法只需调节一个学习率超参数，并证明在平滑正则化损失情况下具有线性收敛性。实验证明，Prospect算法在分布偏移和公平性基准上能以2-3倍的速度收敛。 |
| [^31] | [Gradual Domain Adaptation: Theory and Algorithms.](http://arxiv.org/abs/2310.13852) | 本文研究了渐进域自适应中的渐进自训练算法，提出了一个改进的泛化界限，并指出了中间域在源域和目标域之间均匀放置的重要性。 |
| [^32] | [Geometric Learning with Positively Decomposable Kernels.](http://arxiv.org/abs/2310.13821) | 本文提出了使用正可分解核的几何学习方法，该方法通过在RKKS中学习而不需要访问核的分解，为非欧几里德数据的核学习提供了一条路径，并为RKKS方法提供了理论基础。 |
| [^33] | [Fundamental Limits of Membership Inference Attacks on Machine Learning Models.](http://arxiv.org/abs/2310.13786) | 本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。 |
| [^34] | [Compositional Deep Probabilistic Models of DNA Encoded Libraries.](http://arxiv.org/abs/2310.13769) | 本研究提出了一种组合深度概率模型DEL-Compose，用于对DNA编码库的数据进行建模和分析，以发现潜在的结构和信息，并通过改进观察模型来更好地处理数据噪声。 |
| [^35] | [Generative Flow Networks as Entropy-Regularized RL.](http://arxiv.org/abs/2310.12934) | 本研究将生成流网络的学习任务重新定义为具有特定奖励和正则化器结构的熵正则化强化学习问题，并证明熵正则化强化学习方法在生成流网络训练中具有实际效率和竞争力。 |
| [^36] | [Sensitivity-Aware Amortized Bayesian Inference.](http://arxiv.org/abs/2310.11122) | 本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。 |
| [^37] | [Distributed Variational Inference for Online Supervised Learning.](http://arxiv.org/abs/2309.02606) | 本文提出了一种适用于智能传感器网络的分布式变分推断算法，可以解决连续变量、大规模实时数据和难以处理的后验概率的推断问题。通过推导出可分离的较低下界，实现了在传感器网络中进行一跳通信的分布式变分推断，并提出了处理二进制问题的方法。 |
| [^38] | [Amortized Variational Inference: When and Why?.](http://arxiv.org/abs/2307.11018) | 本文研究了分期变分推断作为近似后验推断的一种通用替代方法，探讨了何时能够达到与传统的因子化变分推断相同的最优解。 |
| [^39] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^40] | [Disentanglement via Latent Quantization.](http://arxiv.org/abs/2305.18378) | 本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。 |
| [^41] | [The Crucial Role of Normalization in Sharpness-Aware Minimization.](http://arxiv.org/abs/2305.15287) | 这篇论文提出的Sharpness-Aware Minimization算法大大提高了深度神经网络的预测性能，而其中规范化起着关键作用，通过稳定算法和使其漂移沿着一系列极小值提升性能，并使算法具有鲁棒性。 |
| [^42] | [A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods.](http://arxiv.org/abs/2305.15027) | 本论文建立了深度学习在不确定性量化中所使用的深度集成和（变分）贝叶斯方法的统一理论，通过将非凸优化问题转化为概率测度空间上的凸优化问题，并提出一族交互式深度集成方案，并在实验中验证了理论结果。 |
| [^43] | [ELSA -- Enhanced latent spaces for improved collider simulations.](http://arxiv.org/abs/2305.07696) | 本文提出了多种增强模拟精确度的方法，包括在模拟链的末尾进行干预、在模拟链的开头进行干预和潜空间细化，通过 W+jets矩阵元代理模拟以及使用正则流和生成模型等策略进行研究，实验证明这些方法能显著提高精确度。 |
| [^44] | [Policy Gradient Algorithms Implicitly Optimize by Continuation.](http://arxiv.org/abs/2305.06851) | 本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。 |
| [^45] | [Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally.](http://arxiv.org/abs/2305.02247) | 本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。 |
| [^46] | [Learning curves for deep structured Gaussian feature models.](http://arxiv.org/abs/2303.00564) | 该论文针对深度高斯模型的特征各向异性展开研究。研究结果表明，在第一层特征行之间允许存在相关性可以促进泛化，而后续层的结构通常是不利的。 |
| [^47] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^48] | [The Geometry of Neural Nets' Parameter Spaces Under Reparametrization.](http://arxiv.org/abs/2302.07384) | 研究了神经网络在重参数化下的不变性，如果显式地表示度量并使用正确的相关变换规则，则不变性是任何神经网络的固有属性。 |
| [^49] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^50] | [ResMem: Learn what you can and memorize the rest.](http://arxiv.org/abs/2302.01576) | ResMem是一种通过显式记忆来改善模型泛化能力的方法，它通过拟合模型的残差来实现。在各种视觉和自然语言处理基准测试中，ResMem一致地改善了原始预测模型的测试集泛化能力。 |
| [^51] | [Doubly Adversarial Federated Bandits.](http://arxiv.org/abs/2301.09223) | 我们研究了一种新的非随机联邦多臂赌博问题，考虑了具有双重对抗性的设置。我们提供了任何联邦赌博算法的遗憾下界，并提出了一种接近最优的算法FEDEXP3。该算法解决了之前的开放性问题。 |
| [^52] | [A Characterization of Multioutput Learnability.](http://arxiv.org/abs/2301.02729) | 该论文研究了多输出函数类的学习问题，提出了多输出函数类学习的特征化判别标准，即每个单输出子类可学习时多输出函数类才可学习，在多标记分类和多输出回归领域取得了重要进展。 |
| [^53] | [Generalized Gradient Flows with Provable Fixed-Time Convergence and Fast Evasion of Non-Degenerate Saddle Points.](http://arxiv.org/abs/2212.03765) | 该论文介绍了一种广义梯度流算法，它能够在固定时间内收敛到非凸函数的最优解，并且能够快速逃逸非退化鞍点。 |
| [^54] | [On Regret-optimal Cooperative Nonstochastic Multi-armed Bandits.](http://arxiv.org/abs/2211.17154) | 本研究考虑了具有延迟通信网络的合作非随机多智能体多臂老虎机问题，在合适的正则化器和通信协议下，采用"FTRL"算法可以达到个体后悔的最小化，且具有后悔最优性。在数值实验中验证了理论结果并展示了算法的优越性。 |
| [^55] | [On the Ability of Graph Neural Networks to Model Interactions Between Vertices.](http://arxiv.org/abs/2211.16494) | 本文研究了GNN模拟顶点间相互作用的能力，通过一个被称为分离秩的度量标准来量化这种能力，结果表明模拟相互作用的能力主要取决于分区的行走指数，即从分界线开始的行走数量，同时设计了一种名为WISA的边稀疏化算法以提高GNNs的处理效率和表达能力。 |
| [^56] | [Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism.](http://arxiv.org/abs/2211.07482) | 本文提出使用融合图来设计等变神经网络的新组件，为解决涉及全局空间和置换对称性的学习任务提供了一种图形化的方法。 |
| [^57] | [Revisiting Le Cam's Equation: Exact Minimax Rates over Convex Density Classes.](http://arxiv.org/abs/2210.11436) | 本文通过研究在凸密度类上进行密度估计的极小极大速率，扩展了已有结果，确定了任何凸密度类的精确极小极大速率，并且提出了自适应估计器。 |
| [^58] | [Hindsight Learning for MDPs with Exogenous Inputs.](http://arxiv.org/abs/2207.06272) | 提出了一种数据高效的带有外部输入的MDPs算法，名为追溯学习（HL）。HL算法通过利用外部变量样本使得过去的决策在回溯中可以加速策略改进，在资源管理问题中表现出良好的性能。 |
| [^59] | [Employing Feature Selection Algorithms to Determine the Immune State of a Mouse Model of Rheumatoid Arthritis.](http://arxiv.org/abs/2207.05882) | 该论文研究了使用特征选择算法来确定类风湿性关节炎小鼠模型的免疫状态。研究目标是通过调节免疫状态中的调节作用因子，关闭自身免疫反应中的自身免疫通路。通过考虑胶原诱导性关节炎小鼠模型进行实验，作者探索了如何确定系统状态以提高免疫疗法效果。 |
| [^60] | [Statistical Inference for the Dynamic Time Warping Distance, with Application to Abnormal Time-Series Detection.](http://arxiv.org/abs/2202.06593) | 本研究提出了一种新的统计推断方法，用于解决在不确定环境下基于动态时间规整算法的时间序列相似性/距离问题。该方法能够提供有效的p值，对于异常时间序列检测等高风险决策具有重要意义。 |
| [^61] | [Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case.](http://arxiv.org/abs/2202.05069) | 本文提出了一种适用于线性回归情况的迁移学习算法，该算法能够将新数据与历史数据相结合，特别在新数据稀缺的情况下具有益处，并且在实验验证中表现出对负迁移学习的鲁棒性。 |
| [^62] | [Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions.](http://arxiv.org/abs/2004.08318) | 本文研究了基于结果抽样的因果推断，发现强无偏性不总是像随机抽样下那样强大，并且某些单调性假设在锐利识别间隔方面产生可比较的结果。 通过算法推断出参数并用实证例子证明了方法的贡献。 |
| [^63] | [Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy.](http://arxiv.org/abs/1911.09307) | 这项工作提出了一种名为Pani的通用正则化器，它在深度神经网络中进行非局部表示，并将邻域补丁特征进行线性插值，从而构建了一种通用且有效的正则化策略。 |
| [^64] | [TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer.](http://arxiv.org/abs/1811.09620) | TimbreTron是一种用于音乐音色转换的方法，将图像领域的风格转换应用到音频信号的时频表示上，然后使用条件WaveNet合成器生成高质量的波形。通过人类感知评估，证实了其可行性。 |

# 详细

[^1]: 混合变量全局敏感性分析用于知识发现和高效的组合材料设计

    Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And Efficient Combinatorial Materials Design. (arXiv:2310.15124v1 [stat.ML])

    [http://arxiv.org/abs/2310.15124](http://arxiv.org/abs/2310.15124)

    本论文针对工程设计中存在的混合变量问题，开发了一种基于元模型的混合变量全局敏感性分析方法，通过数值案例研究验证了该方法的有效性。

    

    全局敏感性分析（GSA）是研究任何给定输入对模型输出的影响的研究。在工程设计的背景下，GSA已被广泛用于理解设计变量对设计目标的单个和集体贡献。到目前为止，全局敏感性研究通常局限于只有定量（数字）设计变量的设计空间。然而，许多工程系统除了定量设计变量外，还包含有质量（分类）设计变量。在本文中，我们将潜变量高斯过程（LVGP）与Sobol分析相结合，开发了第一个基于元模型的混合变量GSA方法。通过数值案例研究，我们验证和证明了我们提出的混合变量问题的方法的有效性。此外，虽然所提出的GSA方法足够通用，可以用于各种工程设计应用，但我们将其与多目标贝叶斯方法相结合。

    Global Sensitivity Analysis (GSA) is the study of the influence of any given inputs on the outputs of a model. In the context of engineering design, GSA has been widely used to understand both individual and collective contributions of design variables on the design objectives. So far, global sensitivity studies have often been limited to design spaces with only quantitative (numerical) design variables. However, many engineering systems also contain, if not only, qualitative (categorical) design variables in addition to quantitative design variables. In this paper, we integrate Latent Variable Gaussian Process (LVGP) with Sobol' analysis to develop the first metamodel-based mixed-variable GSA method. Through numerical case studies, we validate and demonstrate the effectiveness of our proposed method for mixed-variable problems. Furthermore, while the proposed GSA method is general enough to benefit various engineering design applications, we integrate it with multi-objective Bayesian 
    
[^2]: 在非标准环境中评估机器学习模型：综述与新发现

    Evaluating machine learning models in non-standard settings: An overview and new findings. (arXiv:2310.15108v1 [stat.ML])

    [http://arxiv.org/abs/2310.15108](http://arxiv.org/abs/2310.15108)

    本文综述了在非标准环境中评估机器学习模型的准则和方法，包括聚类数据、空间数据、不均匀采样概率、概念漂移和层次结构化结果。其中的统一原则是在重采样过程中使用的测试数据应反映出模型将要应用的新观测数据，而训练数据应代表整个数据集。

    

    估计机器学习模型的泛化误差是基本的，采用重采样方法是最常见的方法。然而，在非标准环境中，特别是观测值不是独立同分布的情况下，使用简单的随机数据划分进行重采样可能导致偏倚的泛化误差估计。本文旨在提供在各种非标准环境中进行泛化误差估计的可靠准则：聚类数据、空间数据、不均匀采样概率、概念漂移和层次结构化结果。我们的综述将已建立的方法与我们所知的其他在这些特定情景中较少被考虑的方法相结合。这些技术之间的一个统一原则是，在重采样过程的每次迭代中使用的测试数据应反映出模型将要应用的新观测数据，而训练数据应代表整个数据集。

    Estimating the generalization error (GE) of machine learning models is fundamental, with resampling methods being the most common approach. However, in non-standard settings, particularly those where observations are not independently and identically distributed, resampling using simple random data divisions may lead to biased GE estimates. This paper strives to present well-grounded guidelines for GE estimation in various such non-standard settings: clustered data, spatial data, unequal sampling probabilities, concept drift, and hierarchically structured outcomes. Our overview combines well-established methodologies with other existing methods that, to our knowledge, have not been frequently considered in these particular settings. A unifying principle among these techniques is that the test data used in each iteration of the resampling procedure should reflect the new observations to which the model will be applied, while the training data should be representative of the entire data 
    
[^3]: 快速二头卷积自编码器用于压缩3D时间投影室数据

    Fast 2D Bicephalous Convolutional Autoencoder for Compressing 3D Time Projection Chamber Data. (arXiv:2310.15026v1 [stat.ML])

    [http://arxiv.org/abs/2310.15026](http://arxiv.org/abs/2310.15026)

    本研究提出了一种快速的二头卷积自编码器用于压缩三维时间投影室数据，在压缩率和重建精度方面优于传统方法。

    

    高能大型粒子对撞机在核物理领域以每秒1TB的速度产生数据，在高能物理领域以每秒PB的速度产生数据。开发实时数据压缩算法以减小数据量以适应永久存储引起了越来越多的关注。在相对论重离子对撞机（RHIC）的新建sPHENIX实验中，时间投影室被用作主要跟踪探测器，记录了三维（3D）圆柱体内的粒子轨迹。结果数据通常非常稀疏，占据率约为10.8%。这样的稀疏性对于传统的无损学习压缩算法（如SZ、ZFP和MGARD）提出了挑战。基于3D卷积神经网络（CNN）的方法，即二头卷积自编码器（BCAE），在压缩率和重建精度方面优于传统方法。BCAE还可以利用计算能力。

    High-energy large-scale particle colliders produce data at high speed in the order of 1 terabytes per second in nuclear physics and petabytes per second in high-energy physics. Developing real-time data compression algorithms to reduce such data at high throughput to fit permanent storage has drawn increasing attention. Specifically, at the newly constructed sPHENIX experiment at the Relativistic Heavy Ion Collider (RHIC), a time projection chamber is used as the main tracking detector, which records particle trajectories in a volume of a three-dimensional (3D) cylinder. The resulting data are usually very sparse with occupancy around 10.8%. Such sparsity presents a challenge to conventional learning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. The 3D convolutional neural network (CNN)-based approach, Bicephalous Convolutional Autoencoder (BCAE), outperforms traditional methods both in compression rate and reconstruction accuracy. BCAE can also utilize the computation
    
[^4]: 贝叶斯主动元学习的基本困境

    The Fundamental Dilemma of Bayesian Active Meta-learning. (arXiv:2310.14968v1 [cs.LG])

    [http://arxiv.org/abs/2310.14968](http://arxiv.org/abs/2310.14968)

    在贝叶斯主动元学习中，贪婪追求可转移知识可能会损害对可转移参数的估计，学习者面临任务识别和可转移知识获取之间的困境。

    

    许多应用需要估计在多个不同但相关的数据稀缺任务环境中推广的参数。贝叶斯主动元学习是一种顺序最优实验设计的形式，为解决这类问题提供了一个框架。主动元学习者的目标是在当前任务的特殊特征（任务特定参数）的情况下获得可转移的知识（估计可转移的参数）。我们证明，在这种情况下，贪婪追求这个目标实际上可能会损害对可转移参数的估计（引起所谓的负迁移）。学习者面临着一个类似但不同于勘探-利用困境的困境：他们应该花费他们的获取预算来追求可转移的知识，还是用来确定当前任务特定的参数？我们理论上证明，一些任务存在不可避免且任意大的负迁移威胁，任务的识别对于重新寻找可迁移参数至关重要。

    Many applications involve estimation of parameters that generalize across multiple diverse, but related, data-scarce task environments. Bayesian active meta-learning, a form of sequential optimal experimental design, provides a framework for solving such problems. The active meta-learner's goal is to gain transferable knowledge (estimate the transferable parameters) in the presence of idiosyncratic characteristics of the current task (task-specific parameters). We show that in such a setting, greedy pursuit of this goal can actually hurt estimation of the transferable parameters (induce so-called negative transfer). The learner faces a dilemma akin to but distinct from the exploration--exploitation dilemma: should they spend their acquisition budget pursuing transferable knowledge, or identifying the current task-specific parameters? We show theoretically that some tasks pose an inevitable and arbitrarily large threat of negative transfer, and that task identification is critical to re
    
[^5]: 通过二阶透镜看Adam

    Adam through a Second-Order Lens. (arXiv:2310.14963v1 [cs.LG])

    [http://arxiv.org/abs/2310.14963](http://arxiv.org/abs/2310.14963)

    该论文提出了AdamQLR，它是一个通过将K-FAC中的技术与Adam的更新方法相结合的优化器，通过考虑二阶数据上的Adam行为而得到启发。在回归和分类任务上进行了评估，结果显示AdamQLR在运行时间和推广性能方面表现出良好的竞争力。

    

    深度学习优化研究存在一种紧张状态，即第一阶梯度法（如SGD和Adam）的计算效率与第二阶曲率法（如拟牛顿方法和K-FAC）的理论效率之间的紧张关系。我们试图将这两种方法的优点结合到一个计算上高效的算法中。注意到二阶方法通常依赖于稳定的启发式方法（如Levenberg-Marquardt阻尼），我们提出AdamQLR：一个将K-FAC中的阻尼和学习率选择技术与Adam提出的更新方向相结合的优化器，通过考虑Adam在二阶数据上的表现而得到启发。我们在各种规模的回归和分类任务上评估了AdamQLR，在运行时间与竞争性推广性能之间取得了竞争性的结果。

    Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical efficiency of second-order, curvature-based methods (such as quasi-Newton methods and K-FAC). We seek to combine the benefits of both approaches into a single computationally-efficient algorithm. Noting that second-order methods often depend on stabilising heuristics (such as Levenberg-Marquardt damping), we propose AdamQLR: an optimiser combining damping and learning rate selection techniques from K-FAC (Martens and Grosse, 2015) with the update directions proposed by Adam, inspired by considering Adam through a second-order lens. We evaluate AdamQLR on a range of regression and classification tasks at various scales, achieving competitive generalisation performance vs runtime.
    
[^6]: 可行的无鞍牛顿优化神经网络的Hessian-Vector乘积系列

    Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks. (arXiv:2310.14901v1 [cs.LG])

    [http://arxiv.org/abs/2310.14901](http://arxiv.org/abs/2310.14901)

    本文提出了一种以Hessian-Vector乘积系列为基础的优化算法，通过平方根和求逆操作实现了高效可伸缩的优化方法，并相对于其他一阶和二阶优化方法在运行时间和性能上具有可比性。

    

    尽管拟牛顿方法在连续优化领域非常受欢迎，但在机器学习中应用仍具有挑战性，因为Hessian矩阵的规模过大。通过修改Hessian的特征值来处理非凸性，如无鞍牛顿方法，进一步增加了计算负担。我们提出了一种同时解决这两个问题的优化算法-据我们所知，这是首个可以渐近地使用精确（特征值修改后的）逆Hessian的高效可伸缩优化算法。我们的方法将问题表述为一个主要对Hessian进行平方根和求逆的级数，然后用它来预处理梯度向量，而无需显式计算或进行特征分解。对该无限级数的截断提供了一个新的可伸缩优化算法，其运行时间和优化性能与其他一阶和二阶优化方法相当。

    Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact (eigenvalue-modified) inverse Hessian. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performan
    
[^7]: Boosting用于界定最差分类误差

    Boosting for Bounding the Worst-class Error. (arXiv:2310.14890v1 [stat.ML])

    [http://arxiv.org/abs/2310.14890](http://arxiv.org/abs/2310.14890)

    该论文提出了一种基于Boosting的算法，可以保证最差类别训练误差的上界，并降低了最差类别的测试误差率。

    

    本文解决了最差类别误差率的问题，而不是针对所有类别的标准误差率的平均。例如，一个三类别分类任务，其中各类别的误差率分别为10％，10％和40％，其最差类别误差率为40％，而在类别平衡条件下的平均误差率为20％。最差类别错误在许多应用中很重要。例如，在医学图像分类任务中，对于恶性肿瘤类别具有40％的错误率而良性和健康类别具有10％的错误率是不能被接受的。我们提出了一种保证最差类别训练误差上界的提升算法，并推导出其泛化界。实验结果表明，该算法降低了最差类别的测试误差率，同时避免了对训练集的过拟合。

    This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\%, 10\%, and 40\% has a worst-class error rate of 40\%, whereas the average is 20\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\% error rate, while the benign and healthy classes have 10\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.
    
[^8]: 针对不平衡分类的尖锐误差界：少数类中有多少样本？

    Sharp error bounds for imbalanced classification: how many examples in the minority class?. (arXiv:2310.14826v1 [stat.ML])

    [http://arxiv.org/abs/2310.14826](http://arxiv.org/abs/2310.14826)

    该论文提出了两个在稀有类概率趋近于零的情况下的新贡献，分别是一种非渐近快速率概率界限和一种一致上界估计方法，这些发现为在实际情况下的类别加权提供了更清晰的理解，为进一步的研究提供了新的方向。

    

    在处理不平衡分类数据时，重新加权损失函数是一种标准的方法，可以在风险度量中平衡真正的正例率和真正的负例率。尽管在这个领域有重要的理论工作，但现有的结果并没有充分解决不平衡分类框架中的主要挑战，即相对于整个样本大小来说一个类的可忽略的大小以及需要通过趋近于零的概率来重新调整风险函数的问题。为了解决这个问题，我们在稀有类概率趋近于零的情况下提出了两个新的贡献：（1）用于约束平衡经验风险最小化的非渐近快速率概率界限，以及（2）用于平衡最近邻估计的一致上界。我们的发现更清楚地说明了类别加权在实际情况下的益处，为这个领域的进一步研究开辟了新的途径。

    When dealing with imbalanced classification data, reweighting the loss function is a standard procedure allowing to equilibrate between the true positive and true negative rates within the risk measure. Despite significant theoretical work in this area, existing results do not adequately address a main challenge within the imbalanced classification framework, which is the negligible size of one class in relation to the full sample size and the need to rescale the risk function by a probability tending to zero. To address this gap, we present two novel contributions in the setting where the rare class probability approaches zero: (1) a non asymptotic fast rate probability bound for constrained balanced empirical risk minimization, and (2) a consistent upper bound for balanced nearest neighbors estimates. Our findings provide a clearer understanding of the benefits of class-weighting in realistic settings, opening new avenues for further research in this field.
    
[^9]: 数据科学的演变与沙特阿拉伯案例——13年来我们发生了多大的变化？

    The evolving of Data Science and the Saudi Arabia case. How much have we changed in 13 years?. (arXiv:2310.14808v1 [stat.AP])

    [http://arxiv.org/abs/2310.14808](http://arxiv.org/abs/2310.14808)

    过去13年中的数据科学词汇使用情况进行了全面考察，研究发现了引入新词汇的文献，并探索了这些词汇如何被纳入科学文献中。同时分析了沙特阿拉伯的科学出版物与整体结果之间的比较。

    

    本文对过去13年中数据科学词汇的使用情况进行了全面考察。研究从一个包含16,018个摘要的数据集开始，这些摘要中出现了“数据科学”一词，无论是标题、摘要还是关键词。研究包括识别引入新词汇的文献，并进一步探索这些词汇如何被纳入科学文献中。为了实现这些目标，我采用了探索性数据分析、潜在语义分析、潜在狄利克雷分析和N-gram分析等技术。文章还展示了整体结果和沙特阿拉伯特定结果之间的科学出版物比较。根据词汇的使用方式，我们确定了代表性文章。

    A comprehensive examination of data science vocabulary usage over the past 13 years in this work is conducted. The investigation commences with a dataset comprising 16,018 abstracts that feature the term "data science" in either the title, abstract, or keywords. The study involves the identification of documents that introduce novel vocabulary and subsequently explores how this vocabulary has been incorporated into scientific literature. To achieve these objectives, I employ techniques such as Exploratory Data Analysis, Latent Semantic Analysis, Latent Dirichlet Analysis, and N-grams Analysis. A comparison of scientific publications between overall results and those specific to Saudi Arabia is presented. Based on how the vocabulary is utilized, representative articles are identified.
    
[^10]: 多个专家学习推迟的原则方法

    Principled Approaches for Learning to Defer with Multiple Experts. (arXiv:2310.14774v1 [cs.LG])

    [http://arxiv.org/abs/2310.14774](http://arxiv.org/abs/2310.14774)

    我们研究了多个专家学习推迟问题的代理损失和算法，并证明了这些代理损失函数具有强H一致性界限。我们展示了几个实际应用的代理损失函数，并设计了基于最小化这些损失函数的新的学习推迟算法。我们还进行了在SVHN和CIFAR-10数据集上的实验。

    

    我们提出了一项关于使用多个专家学习推迟问题的代理损失和算法的研究。我们首先引入了一类专门针对多专家设置的代理损失函数，其中预测和推迟函数同时学习。然后，我们证明了这些代理损失函数受益于强H一致性界限。我们通过几个实际代理损失函数的示例展示了我们分析的应用，并给出了明确的保证。这些损失函数直接导致了基于它们最小化的新的学习推迟算法的设计。虽然本工作的主要重点是理论分析，但我们还报告了在SVHN和CIFAR-10数据集上的多个实验结果。

    We present a study of surrogate losses and algorithms for the general problem of learning to defer with multiple experts. We first introduce a new family of surrogate losses specifically tailored for the multiple-expert setting, where the prediction and deferral functions are learned simultaneously. We then prove that these surrogate losses benefit from strong $H$-consistency bounds. We illustrate the application of our analysis through several examples of practical surrogate losses, for which we give explicit guarantees. These loss functions readily lead to the design of new learning to defer algorithms based on their minimization. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on SVHN and CIFAR-10 datasets.
    
[^11]: 预测-拒绝多类放弃：理论分析和算法

    Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms. (arXiv:2310.14772v1 [cs.LG])

    [http://arxiv.org/abs/2310.14772](http://arxiv.org/abs/2310.14772)

    我们研究了多类别分类设置中的学习与放弃框架，并提出了一系列新的理论和算法结果，解决了两个现存的开放问题。这些保证为基于最小化放弃损失的新的多类别放弃算法提供了启示。

    

    我们研究了多类别分类设置中的学习与放弃框架。在这种设置中，学习者可以选择以一定的预定义成本放弃进行预测。我们提出了一系列新的理论和算法结果，解决了预测-拒绝框架下的学习问题。我们引入了几个新的替代损失函数家族，证明了强非渐进和假设集特定的一致性保证，从而积极地解决了两个现存的开放问题。这些保证提供了放弃损失函数的估计误差的上界，与替代损失的误差相关。我们分析了同时学习预测器和拒绝器的单阶段设置，以及在应用中至关重要的两阶段设置，在第一阶段使用标准替代损失函数如交叉熵来学习预测器。这些保证为基于最小化放弃损失的新的多类别放弃算法提供了启示。

    We study the key framework of learning with abstention in the multi-class classification setting. In this setting, the learner can choose to abstain from making a prediction with some pre-defined cost. We present a series of new theoretical and algorithmic results for this learning problem in the predictor-rejector framework. We introduce several new families of surrogate losses for which we prove strong non-asymptotic and hypothesis set-specific consistency guarantees, thereby resolving positively two existing open questions. These guarantees provide upper bounds on the estimation error of the abstention loss function in terms of that of the surrogate loss. We analyze both a single-stage setting where the predictor and rejector are learned simultaneously and a two-stage setting crucial in applications, where the predictor is learned in a first stage using a standard surrogate loss such as cross-entropy. These guarantees suggest new multi-class abstention algorithms based on minimizing
    
[^12]: 基于分数的多类放弃的理论基础损失函数和算法

    Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention. (arXiv:2310.14770v1 [cs.LG])

    [http://arxiv.org/abs/2310.14770](http://arxiv.org/abs/2310.14770)

    本文提出了基于分数的多类放弃的理论基础损失函数和算法，包括引入了新的代理损失函数族群以及证明了这些代理损失的一致性保证。我们通过实验证明了这些算法的实际意义。

    

    学习中的放弃是一种重要的场景，学习者可以选择在某个代价下放弃进行预测。本文在多类别分类的设置下分析了基于分数的学习中的放弃。我们引入了放弃损失函数的新代理损失族群，其中包括单阶段设置中最先进的代理损失以及二阶段设置中的新型损失函数。我们证明了这些代理损失的强非渐近和假设集特定的一致性保证，这些保证上界了放弃损失函数的估计误差，与代理损失的估计误差相关。我们的上界可以帮助比较不同的基于分数的代理损失，指导通过最小化提出的代理损失函数来设计新的放弃算法。我们在CIFAR-10、CIFAR-100和SVHN数据集上对我们的新算法进行了实验评估，展示了我们的新代理损失函数的实际意义。

    Learning with abstention is a key scenario where the learner can abstain from making a prediction at some cost. In this paper, we analyze the score-based formulation of learning with abstention in the multi-class classification setting. We introduce new families of surrogate losses for the abstention loss function, which include the state-of-the-art surrogate losses in the single-stage setting and a novel family of loss functions in the two-stage setting. We prove strong non-asymptotic and hypothesis set-specific consistency guarantees for these surrogate losses, which upper-bound the estimation error of the abstention loss function in terms of the estimation error of the surrogate loss. Our bounds can help compare different score-based surrogates and guide the design of novel abstention algorithms by minimizing the proposed surrogate losses. We experimentally evaluate our new algorithms on CIFAR-10, CIFAR-100, and SVHN datasets and the practical significance of our new surrogate losse
    
[^13]: 外部验证策略评估结合试验和观察数据

    Externally Valid Policy Evaluation Combining Trial and Observational Data. (arXiv:2310.14763v1 [stat.ME])

    [http://arxiv.org/abs/2310.14763](http://arxiv.org/abs/2310.14763)

    这项研究提出了一种结合试验和观察数据的外部有效策略评估方法，利用试验数据对目标人群上的政策结果进行有效推断，并给出了可验证的评估结果。

    

    随机试验被广泛认为是评估决策策略影响的金 standard。然而，试验数据来自可能与目标人群不同的人群，这引发了外部效度（也称为泛化能力）的问题。在本文中，我们试图利用试验数据对目标人群上的政策结果进行有效推断。目标人群的额外协变量数据用于模拟试验研究中个体的抽样。我们开发了一种方法，在任何指定的模型未校准范围内产生可验证的基于试验的政策评估。该方法是非参数的，即使样本是有限的，有效性也得到保证。使用模拟和实际数据说明了认证的政策评估结果。

    Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. Additional covariate data from the target population is used to model the sampling of individuals in the trial study. We develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. The method is nonparametric and the validity is assured even with finite samples. The certified policy evaluations are illustrated using both simulated and real data.
    
[^14]: 扩展深度自适应输入规范化用于神经网络对时序数据的预处理

    Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks. (arXiv:2310.14720v1 [cs.LG])

    [http://arxiv.org/abs/2310.14720](http://arxiv.org/abs/2310.14720)

    本研究提出了一种名为EDAIn的扩展深度自适应输入规范化层，通过以端到端的方式学习如何适当地规范化时序数据，而不是使用固定的规范化方案，来提高深度神经网络在时序预测和分类任务中的性能。实验证明该方法在不同数据集上都取得了良好效果。

    

    数据预处理是任何机器学习流程中至关重要的一部分，它对性能和训练效率都有重要影响。当使用深度神经网络进行时序预测和分类时，这一点尤为明显：真实世界的时序数据通常表现出多样性、偏斜和异常值等不规则特征，如果不充分处理这些特征，模型性能很快会下降。在本研究中，我们提出了EDAIN（扩展深度自适应输入规范化）层，一种新颖的自适应神经层，它能够以端到端的方式学习如何适当地规范化不规则的时序数据，而不是使用固定的规范化方案。这通过使用反向传播算法，同时优化其未知参数和深度神经网络来实现。我们的实验证明，这一方法在使用合成数据、信用违约预测数据集和大规模限价单簿基准数据集时都取得了良好效果。

    Data preprocessing is a crucial part of any machine learning pipeline, and it can have a significant impact on both performance and training efficiency. This is especially evident when using deep neural networks for time series prediction and classification: real-world time series data often exhibit irregularities such as multi-modality, skewness and outliers, and the model performance can degrade rapidly if these characteristics are not adequately addressed. In this work, we propose the EDAIN (Extended Deep Adaptive Input Normalization) layer, a novel adaptive neural layer that learns how to appropriately normalize irregular time series data for a given task in an end-to-end fashion, instead of using a fixed normalization scheme. This is achieved by optimizing its unknown parameters simultaneously with the deep neural network using back-propagation. Our experiments, conducted using synthetic data, a credit default prediction dataset, and a large-scale limit order book benchmark datase
    
[^15]: 高维度低样本分类的随机森林差异性

    Random Forest Dissimilarity for High-Dimension Low Sample Size Classification. (arXiv:2310.14710v1 [stat.ML])

    [http://arxiv.org/abs/2310.14710](http://arxiv.org/abs/2310.14710)

    针对高维度低样本(HDLSS)分类问题，本论文提出了一种基于随机森林相似度的学习预计算SVM核方法(RFSVM)，通过在40个公共HDLSS分类数据集上的实验证明，该方法在HDLSS问题上优于现有方法并且保持了非常一致的结果。

    

    高维度低样本(HDLSS)问题在机器学习的实际应用中很常见。从医学影像到文本处理，传统的机器学习算法通常无法从这样的数据中学习到最佳的概念。我们在之前的工作中提出了一种基于差异性的多视角分类方法，即随机森林差异性(RFD)，该方法在这类问题上取得了最先进的结果。在本研究中，我们将该方法的核心原则转化为解决HDLSS分类问题的方法，通过使用随机森林相似度作为学习的预计算SVM核(RFSVM)。我们展示了这样的学习相似度度量在这种分类上特别适用和准确。通过对40个公共HDLSS分类数据集进行的实验，配合严格的统计分析，结果显示RFSVM方法在大多数HDLSS问题上优于现有方法，并且同时非常连贯。

    High dimension, low sample size (HDLSS) problems are numerous among real-world applications of machine learning. From medical images to text processing, traditional machine learning algorithms are usually unsuccessful in learning the best possible concept from such data. In a previous work, we proposed a dissimilarity-based approach for multi-view classification, the Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for such problems. In this work, we transpose the core principle of this approach to solving HDLSS classification problems, by using the RF similarity measure as a learned precomputed SVM kernel (RFSVM). We show that such a learned similarity measure is particularly suited and accurate for this classification context. Experiments conducted on 40 public HDLSS classification datasets, supported by rigorous statistical analyses, show that the RFSVM method outperforms existing methods for the majority of HDLSS problems and remains at the same time very co
    
[^16]: 使用开放获取的多传感器卫星图像和GEDI激光雷达数据估计森林高度和生物量：法国城市地区的高分辨率地图

    Estimation of forest height and biomass from open-access multi-sensor satellite imagery and GEDI Lidar data: high-resolution maps of metropolitan France. (arXiv:2310.14662v1 [stat.ML])

    [http://arxiv.org/abs/2310.14662](http://arxiv.org/abs/2310.14662)

    本研究利用开放获取的卫星图像和GEDI激光雷达数据，采用机器学习方法，在法国的大范围内估计了森林高度和生物量，并生成了2020年的高分辨率地图。

    

    映射森林资源和碳储量对于改善森林管理并实现碳储存和环境保护目标非常重要。航天遥感方法在提供大范围高空间分辨率的重复观测方面具有重要潜力来支持森林高度监测。本研究利用先前开发的机器学习方法，在法国国家范围等更大比例上推广了该方法。我们使用GEDI激光雷达任务作为参考高度数据，利用Sentinel-1、Sentinel-2和ALOS-2 PALSA-2卫星图像估计森林高度并产生2020年的法国地图。然后，利用通用方程将高度图转化为体积和地上生物量（AGB）。利用ALS数据的本地地图验证了高度图的准确性。

    Mapping forest resources and carbon is important for improving forest management and meeting the objectives of storing carbon and preserving the environment. Spaceborne remote sensing approaches have considerable potential to support forest height monitoring by providing repeated observations at high spatial resolution over large areas. This study uses a machine learning approach that was previously developed to produce local maps of forest parameters (basal area, height, diameter, etc.). The aim of this paper is to present the extension of the approach to much larger scales such as the French national coverage. We used the GEDI Lidar mission as reference height data, and the satellite images from Sentinel-1, Sentinel-2 and ALOS-2 PALSA-2 to estimate forest height and produce a map of France for the year 2020. The height map is then derived into volume and aboveground biomass (AGB) using allometric equations. The validation of the height map with local maps from ALS data shows an accur
    
[^17]: 基于纯差分隐私和高斯差分隐私的可计算MCMC私有学习方法。

    Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy. (arXiv:2310.14661v1 [cs.LG])

    [http://arxiv.org/abs/2310.14661](http://arxiv.org/abs/2310.14661)

    本文介绍了一种基于纯差分隐私和高斯差分隐私的可计算MCMC私有学习方法，通过引入近似采样扰动算法，结合Metropolis-Hastings算法和局部化步骤，实现了对隐私的保护并获得了较好的收敛性能。

    

    后验采样即从后验分布中采样的指数机制，提供ε-纯差分隐私（DP）保证，并不受（ε，δ）-近似DP引入的潜在无界隐私泄漏的影响。然而，在实践中，需要应用近似采样方法，如马尔科夫链蒙特卡洛（MCMC），从而重新引入了对隐私保证的δ-近似误差。为了弥合这一差距，我们提出了近似采样扰动（即ASAP）算法，该算法通过与满足纯DP或纯高斯DP（即δ=0）的参考分布有界Wasserstein无穷距离的MCMC样本加噪声。然后利用Metropolis-Hastings算法生成样本并证明算法在W$_\infty$距离上收敛。我们展示了通过将我们的新技术与细致的局部化步骤相结合，我们获得了第一个可计算MCMC私有学习方法。

    Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\varepsilon,\delta)$-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing $\delta$-approximation error into the privacy guarantees. To bridge this gap, we propose the Approximate SAample Perturbation (abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to its Wasserstein-infinity ($W_\infty$) distance from a reference distribution that satisfies pure DP or pure Gaussian DP (i.e., $\delta=0$). We then leverage a Metropolis-Hastings algorithm to generate the sample and prove that the algorithm converges in W$_\infty$ distance. We show that by combining our new techniques with a careful localization step, we obtain the first ne
    
[^18]: CAD-DA：统计推断下领域适应后可控异常检测

    CAD-DA: Controllable Anomaly Detection after Domain Adaptation by Statistical Inference. (arXiv:2310.14608v1 [stat.ML])

    [http://arxiv.org/abs/2310.14608](http://arxiv.org/abs/2310.14608)

    本论文提出了CAD-DA方法，可在领域适应的情况下进行可控异常检测。该方法通过利用条件选择性推断来处理领域适应的影响，并在合成和真实数据集上进行了评估。

    

    我们提出了一种新颖的统计方法，用于在领域适应下测试异常检测（AD）的结果，我们称之为CAD-DA-可控异常检测下的领域适应。CAD-DA的优势在于能够控制在预先指定的水平$\alpha$（例如0.05）下误识别异常的概率。在这个领域适应的设定中，挑战在于必须考虑领域适应的影响，以确保推断结果的有效性。我们的解决方案利用条件选择性推断的概念来处理领域适应的影响。据我们所知，这是第一个能够在领域适应的背景下进行有效统计推断的工作。我们在合成数据集和真实世界数据集上评估了CAD-DA方法的性能。

    We propose a novel statistical method for testing the results of anomaly detection (AD) under domain adaptation (DA), which we call CAD-DA -controllable AD under DA. The distinct advantage of the CAD-DA lies in its ability to control the probability of misidentifying anomalies under a pre-specified level $\alpha$ (e.g., 0.05). The challenge within this DA setting is the necessity to account for the influence of DA to ensure the validity of the inference results. Our solution to this challenge leverages the concept of conditional Selective Inference to handle the impact of DA. To our knowledge, this is the first work capable of conducting a valid statistical inference within the context of DA. We evaluate the performance of the CAD-DA method on both synthetic and real-world datasets.
    
[^19]: 可扩展的高斯过程回归中的三角积分傅里叶特征

    Trigonometric Quadrature Fourier Features for Scalable Gaussian Process Regression. (arXiv:2310.14544v1 [stat.ML])

    [http://arxiv.org/abs/2310.14544](http://arxiv.org/abs/2310.14544)

    本研究提出了一种新的三角积分傅里叶特征（TQFF）方法，通过使用一种特定于所需傅里叶变换的新颖非高斯积分规则，解决了现有傅里叶特征逼近方法的性能限制问题，并展示了其相对于随机傅里叶特征（RFF）和高斯积分傅里叶特征（QFF）的改进性能。

    

    傅里叶特征逼近已经成功应用于可扩展的高斯过程（GP）回归中。尤其是基于高斯积分规则的积分傅里叶特征（QFF）近年来因其改进的近似精度和更好的校准不确定性估计而受到关注，相较于随机傅里叶特征（RFF）方法。然而，QFF的一个关键限制是其性能受到与高频振荡的积分相关的已知病态的影响，导致近似精度有限的特征。我们通过一种新的三角积分傅里叶特征（TQFF）方法来解决这个关键问题，该方法使用了一种特别适用于所需傅里叶变换的新颖非高斯积分规则。我们导出了TQFF的精确积分规则，并给出了对应特征映射的核逼近误差界。然后，我们展示了我们的方法相对于RFF和高斯QFF的改进性能。

    Fourier feature approximations have been successfully applied in the literature for scalable Gaussian Process (GP) regression. In particular, Quadrature Fourier Features (QFF) derived from Gaussian quadrature rules have gained popularity in recent years due to their improved approximation accuracy and better calibrated uncertainty estimates compared to Random Fourier Feature (RFF) methods. However, a key limitation of QFF is that its performance can suffer from well-known pathologies related to highly oscillatory quadrature, resulting in mediocre approximation with limited features. We address this critical issue via a new Trigonometric Quadrature Fourier Feature (TQFF) method, which uses a novel non-Gaussian quadrature rule specifically tailored for the desired Fourier transform. We derive an exact quadrature rule for TQFF, along with kernel approximation error bounds for the resulting feature map. We then demonstrate the improved performance of our method over RFF and Gaussian QFF in
    
[^20]: 对AI分类器的对抗鲁棒性度量的存在性，唯一性和可扩展性研究

    On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])

    [http://arxiv.org/abs/2310.14421](http://arxiv.org/abs/2310.14421)

    本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。

    

    本文提出并证明了针对（局部）唯一可逆分类器、广义线性模型（GLM）和熵AI（EAI）具有最小对抗路径（MAP）和最小对抗距离（MAD）的存在性、唯一性和明确的分析计算的简单可验证的数学条件。在常见的合成基准测试数据集上，针对神经网络、提升随机森林、GLM和EAI等各类AI工具进行MAP和MAD的实际计算、比较和解释，包括双卷状螺旋线及其扩展以及两个生物医学数据问题（用于健康保险理赔预测和心脏病发作致死率分类）。在生物医学应用中，展示了MAP如何在预定义的可访问控制变量子集中提供唯一的最小患者特定风险缓解干预措施。

    Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
    
[^21]: DePAint：考虑峰值和平均限制的分散安全多智能体强化学习算法

    DePAint: A Decentralized Safe Multi-Agent Reinforcement Learning Algorithm considering Peak and Average Constraints. (arXiv:2310.14348v1 [cs.MA])

    [http://arxiv.org/abs/2310.14348](http://arxiv.org/abs/2310.14348)

    本文提出了一种分散的安全多智能体强化学习算法，能够在没有中央控制器的情况下，通过智能体之间的邻居通信来最大化累积奖励总和，并同时满足每个智能体的安全约束限制。

    

    安全的多智能体强化学习领域，尽管在无人机投递和车辆自动化等各个领域具有潜在应用，但仍然相对未被探索。在训练智能体学习最优策略以最大化奖励的同时考虑特定限制，特别是在训练过程中无法有中央控制器协调智能体的场景中，可能具有挑战性。本文针对分散环境下的多智能体策略优化问题进行了研究，其中智能体通过与邻居通信以最大化其累积奖励总和的同时满足每个智能体的安全约束。我们同时考虑了峰值和平均限制。在这种情况下，没有中央控制器协调智能体，奖励和约束只在每个智能体本地/私有可知。我们将问题形式化为分散约束的多智能体马尔可夫决策问题，并提出了一个算法。

    The field of safe multi-agent reinforcement learning, despite its potential applications in various domains such as drone delivery and vehicle automation, remains relatively unexplored. Training agents to learn optimal policies that maximize rewards while considering specific constraints can be challenging, particularly in scenarios where having a central controller to coordinate the agents during the training process is not feasible. In this paper, we address the problem of multi-agent policy optimization in a decentralized setting, where agents communicate with their neighbors to maximize the sum of their cumulative rewards while also satisfying each agent's safety constraints. We consider both peak and average constraints. In this scenario, there is no central controller coordinating the agents and both the rewards and constraints are only known to each agent locally/privately. We formulate the problem as a decentralized constrained multi-agent Markov Decision Problem and propose a 
    
[^22]: 时间差异学习的有限样本分析

    Finite-Sample Analysis of the Temporal Difference Learning. (arXiv:2310.14286v1 [stat.ML])

    [http://arxiv.org/abs/2310.14286](http://arxiv.org/abs/2310.14286)

    本文提出了一种时间差异学习方法，通过线性函数逼近策略评估，在在线推断中获得了接近最优的性能，并提供了相应的样本复杂度界限。

    

    本文考虑了在线策略评估中使用线性函数逼近的时间差异(TD)方法的性能。我们展示了一个简单的算法，通过使用通用且与实例无关的步长和Polyak-Ruppert尾平均，可以获得接近最优的方差和偏差项。我们还提供了相应的样本复杂度界限。我们的证明技巧基于线性随机逼近的精确误差界限以及TD类型递归产生的随机矩阵乘积的新稳定性结果。

    In this paper we consider the problem of obtaining sharp bounds for the performance of temporal difference (TD) methods with linear functional approximation for policy evaluation in discounted Markov Decision Processes. We show that a simple algorithm with a universal and instance-independent step size together with Polyak-Ruppert tail averaging is sufficient to obtain near-optimal variance and bias terms. We also provide the respective sample complexity bounds. Our proof technique is based on refined error bounds for linear stochastic approximation together with the novel stability result for the product of random matrices that arise from the TD-type recurrence.
    
[^23]: 一种Softmax Gating Multinomial Logistic Mixture of Experts的通用理论

    A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts. (arXiv:2310.14188v1 [stat.ML])

    [http://arxiv.org/abs/2310.14188](http://arxiv.org/abs/2310.14188)

    该论文提出了一种通用理论，用于研究Softmax Gating Multinomial Logistic Mixture of Experts模型。通过建立模型的收敛速度，揭示了softmax gating和专家函数之间存在的互作用，同时提出了一种修改后的softmax gating函数。

    

    Mixture-of-experts（MoE）模型通过门控函数将多个子模型的能力结合起来，在许多回归和分类应用中实现更好的性能。从理论上讲，虽然之前已经尝试通过高斯MoE模型中最大似然估计的收敛性分析来理解该模型在回归设置下的行为，但在分类问题的设置下缺乏相关分析。我们通过建立softmax gating multinomial logistic MoE模型的密度估计和参数估计的收敛速度来弥补这一空白。值得注意的是，当部分专家参数消失时，由于softmax gating和专家函数之间存在固有的偏微分方程互作用，这些收敛速度比多项式速度更慢。为了解决这个问题，我们提出使用一种新颖的修改softmax gating函数。

    Mixture-of-experts (MoE) model incorporates the power of multiple submodels via gating functions to achieve greater performance in numerous regression and classification applications. From a theoretical perspective, while there have been previous attempts to comprehend the behavior of that model under the regression settings through the convergence analysis of maximum likelihood estimation in the Gaussian MoE model, such analysis under the setting of a classification problem has remained missing in the literature. We close this gap by establishing the convergence rates of density estimation and parameter estimation in the softmax gating multinomial logistic MoE model. Notably, when part of the expert parameters vanish, these rates are shown to be slower than polynomial rates owing to an inherent interaction between the softmax gating and expert functions via partial differential equations. To address this issue, we propose using a novel class of modified softmax gating functions which 
    
[^24]: LSTM对于少样本学习是否有效？

    Are LSTMs Good Few-Shot Learners?. (arXiv:2310.14139v1 [cs.LG])

    [http://arxiv.org/abs/2310.14139](http://arxiv.org/abs/2310.14139)

    本研究重新考察了使用LSTM进行少样本学习的方法，在简单的回归问题上表现优于流行的元学习技术MAML，但在复杂的图像分类问题上不及预期。研究提出了一种名为OP-LSTM的新方法来解决这些问题，并取得了显著的性能改进。

    

    深度学习需要大量数据来很好地学习新任务，这限制了其在数据较少的领域的应用。元学习通过学习如何学习来克服这一限制。2001年，Hochreiter等人展示了一个经过不同任务上的反向传播训练的LSTM能够进行元学习。尽管该方法在小问题上取得了有希望的结果，并且最近在强化学习问题上也取得了成功，但在有监督的少样本学习设置中，该方法却受到了较少的关注。我们重新考察了这个方法，并在现代少样本学习基准上进行了测试。我们发现，令人惊讶的是，LSTM在一个简单的少样本正弦波回归基准上表现优于流行的元学习技术MAML，但在更复杂的少样本图像分类基准上不及预期。我们确定了两个潜在原因，并提出了一种名为Outer Product LSTM (OP-LSTM)的新方法来解决这些问题，并显示出显著的性能改进。

    Deep learning requires large amounts of data to learn new tasks well, limiting its applicability to domains where such data is available. Meta-learning overcomes this limitation by learning how to learn. In 2001, Hochreiter et al. showed that an LSTM trained with backpropagation across different tasks is capable of meta-learning. Despite promising results of this approach on small problems, and more recently, also on reinforcement learning problems, the approach has received little attention in the supervised few-shot learning setting. We revisit this approach and test it on modern few-shot learning benchmarks. We find that LSTM, surprisingly, outperform the popular meta-learning technique MAML on a simple few-shot sine wave regression benchmark, but that LSTM, expectedly, fall short on more complex few-shot image classification benchmarks. We identify two potential causes and propose a new method called Outer Product LSTM (OP-LSTM) that resolves these issues and displays substantial p
    
[^25]: 最佳武器识别的最佳批处理算法

    Optimal Batched Best Arm Identification. (arXiv:2310.14129v1 [cs.LG])

    [http://arxiv.org/abs/2310.14129](http://arxiv.org/abs/2310.14129)

    本论文研究了最佳批处理武器识别问题，在渐近和非渐近设置中提出了Tri-BBAI和Opt-BBAI算法，分别实现了最优和几乎最优的样本和批处理复杂度。

    

    我们研究了最佳批处理武器识别（BBAI）问题，其中学习者的目标是在尽量少地更换策略的同时识别出最佳武器。具体而言，我们的目标是以概率$1-\delta$找到最佳武器，其中$\delta>0$是一个小常数，同时最小化样本复杂度（武器拉取的总数）和批处理复杂度（批处理的总数）。我们提出了三批次最佳武器识别（Tri-BBAI）算法，这是第一个在渐近设置（即$\delta\rightarrow0$）中实现最优样本复杂度且仅在最多三个批次中运行的批处理算法。基于Tri-BBAI，我们进一步提出了几乎最优的批处理最佳武器识别（Opt-BBAI）算法，在非渐近设置（即$\delta>0$是任意固定的）中实现近似最优的样本和批处理复杂度，同时在$\delta$趋于零时享受与Tri-BBAI相同的批处理和样本复杂度。

    We study the batched best arm identification (BBAI) problem, where the learner's goal is to identify the best arm while switching the policy as less as possible. In particular, we aim to find the best arm with probability $1-\delta$ for some small constant $\delta>0$ while minimizing both the sample complexity (total number of arm pulls) and the batch complexity (total number of batches). We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., $\delta\rightarrow 0$) and runs only in at most $3$ batches. Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., $\delta>0$ is arbitrarily fixed), while enjoying the same batch and sample complexity as Tri-BBAI when $\delta$ tends to zer
    
[^26]: 关于神经网络训练中的离散漂移和平滑正则化的研究

    On discretisation drift and smoothness regularisation in neural network training. (arXiv:2310.14036v1 [stat.ML])

    [http://arxiv.org/abs/2310.14036](http://arxiv.org/abs/2310.14036)

    本研究旨在提高对深度学习优化和模型正则化的理解，研究了梯度下降算法的动力学特性和离散漂移问题。

    

    将现实世界问题转化为数学优化，并通过使用基于梯度的优化方法来训练深度神经网络的深度学习方法已被证明是有效的。然而，深度学习的具体工作原理的理解落后于其实际意义。我们的目标是在优化和模型正则化方面迈出改进的步伐。我们首先研究了梯度下降（GD），这是大多数流行的深度学习优化算法的离散时间算法的基础。理解GD的动力学特性一直受到离散漂移的阻碍，即GD与其常常研究的连续时间对应物——负梯度流（NGF）之间的数值积分误差。为了扩展研究GD的工具，我们推导了考虑离散漂移的新型连续时间流动。与NGF不同，这些新的流动可以用来描述离散漂移。

    The deep learning recipe of casting real-world problems as mathematical optimisation and tackling the optimisation by training deep neural networks using gradient-based optimisation has undoubtedly proven to be a fruitful one. The understanding behind why deep learning works, however, has lagged behind its practical significance. We aim to make steps towards an improved understanding of deep learning with a focus on optimisation and model regularisation. We start by investigating gradient descent (GD), a discrete-time algorithm at the basis of most popular deep learning optimisation algorithms. Understanding the dynamics of GD has been hindered by the presence of discretisation drift, the numerical integration error between GD and its often studied continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretisation drift. Unlike the NGF, these new flows can be used to describe 
    
[^27]: ASBART: 加速软贝叶斯加法回归树

    ASBART:Accelerated Soft Bayes Additive Regression Trees. (arXiv:2310.13975v1 [stat.ML])

    [http://arxiv.org/abs/2310.13975](http://arxiv.org/abs/2310.13975)

    ASBART是基于贝叶斯加法回归树（BART）的一种加速软件，相较于传统的软BART方法，在保持准确度的情况下提高了计算速度。

    

    贝叶斯加法回归树（BART）是一种非参数回归模型，由于其灵活性和高准确度的估计，近年来在统计和机器学习领域广受欢迎。软BART是BART的一种变种，从实际和理论上改进了现有的贝叶斯树总和模型。然而，软BART的一个瓶颈是在长的MCMC循环中速度较慢。与BART相比，它使用的计算时间默认情况下多了大约20倍。我们提出了一种名为加速软BART（ASBART）的BART变体。模拟研究表明，该新方法比具有可比准确度的软BART快约10倍。我们的代码是开源的，可在https://github.com/richael008/XSBART找到。

    Bayes additive regression trees(BART) is a nonparametric regression model which has gained wide-spread popularity in recent years due to its flexibility and high accuracy of estimation. Soft BART,one variation of BART,improves both practically and heoretically on existing Bayesian sum-of-trees models. One bottleneck for Soft BART is its slow speed in the long MCMC loop. Compared to BART,it use more than about 20 times to complete the calculation with the default setting. We proposed a variant of BART named accelerate Soft BART(ASBART). Simulation studies show that the new method is about 10 times faster than the Soft BART with comparable accuracy. Our code is open-source and available at https://github.com/richael008/XSBART.
    
[^28]: 具有组合协变量的分布式线性回归

    Distributed Linear Regression with Compositional Covariates. (arXiv:2310.13969v1 [stat.ML])

    [http://arxiv.org/abs/2310.13969](http://arxiv.org/abs/2310.13969)

    本文提出了针对大规模组合数据中的分布式稀疏惩罚线性对比模型的两种分布式优化技术，分别采用集中式和分散式拓扑结构，以获得具有较低通信开销的正则化估计。

    

    随着大规模数据集的可用性，解决分布式统计方法和计算问题变得在大数据领域日益关键。本文针对大规模组合数据中的分布式稀疏惩罚线性对比模型进行研究。具体而言，我们提出了两种不同约束凸优化问题的分布式优化技术，分别采用集中式和分散式拓扑结构。这两个算法都是基于交替方向乘子方法（ADMM）和坐标下降方法的框架。值得强调的是，在分散式拓扑中，我们引入了一个基于分组ADMM（GADMM）的分布式坐标下降算法，以获得具有较低通信开销的正则化估计。更具体地说，对于每个变量，我们引入了适应GADMM的协调下降方法。

    With the availability of extraordinarily huge data sets, solving the problems of distributed statistical methodology and computing for such data sets has become increasingly crucial in the big data area. In this paper, we focus on the distributed sparse penalized linear log-contrast model in massive compositional data. In particular, two distributed optimization techniques under centralized and decentralized topologies are proposed for solving the two different constrained convex optimization problems. Both two proposed algorithms are based on the frameworks of Alternating Direction Method of Multipliers (ADMM) and Coordinate Descent Method of Multipliers(CDMM, Lin et al., 2014, Biometrika). It is worth emphasizing that, in the decentralized topology, we introduce a distributed coordinate-wise descent algorithm based on Group ADMM(GADMM, Elgabli et al., 2020, Journal of Machine Learning Research) for obtaining a communication-efficient regularized estimation. Correspondingly, the conve
    
[^29]: 基于核非参数回归的最优极小化传递学习

    Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression. (arXiv:2310.13966v1 [stat.ML])

    [http://arxiv.org/abs/2310.13966](http://arxiv.org/abs/2310.13966)

    本文主要研究了在再生核希尔伯特空间中的非参数回归的传递学习问题，提出了两种情况下的解决方法，并分别给出了统计性质和最优性结果。

    

    近年来，传递学习在机器学习社区中受到了很大关注。它能够利用相关研究的知识来提高目标研究的泛化性能，使其具有很高的吸引力。本文主要研究在再生核希尔伯特空间中的非参数回归的传递学习问题，目的是缩小实际效果与理论保证之间的差距。具体考虑了两种情况：已知可传递的来源和未知的情况。对于已知可传递的来源情况，我们提出了一个两步核估计器，仅使用核岭回归。对于未知的情况，我们开发了一种基于高效聚合算法的新方法，可以自动检测并减轻负面来源的影响。本文提供了所需估计器的统计性质，并建立了该方法的最优性结果。

    In recent years, transfer learning has garnered significant attention in the machine learning community. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the 
    
[^30]: 具有偏差和方差减小的分布鲁棒优化

    Distributionally Robust Optimization with Bias and Variance Reduction. (arXiv:2310.13863v1 [stat.ML])

    [http://arxiv.org/abs/2310.13863](http://arxiv.org/abs/2310.13863)

    本论文提出了一种可以减小偏差和方差的分布鲁棒优化方法，通过谱风险和$f$-散度罚项来解决风险敏感学习问题。提出的Prospect算法只需调节一个学习率超参数，并证明在平滑正则化损失情况下具有线性收敛性。实验证明，Prospect算法在分布偏移和公平性基准上能以2-3倍的速度收敛。

    

    我们考虑使用谱风险基于不确定性集和$f$-散度罚项的分布鲁棒优化(DRO)问题。这个问题包括常见的风险敏感学习目标，如正则化条件风险价值(Conditional Value-at-Risk, CVaR)和平均top-$k$损失。我们提出了Prospect，一种仅需调节单个学习率超参数的随机梯度算法，并证明在平滑正则化损失情况下，它具有线性收敛性。这与之前的算法形成了对比，这些算法要么需要调节多个超参数，要么由于梯度估计存在偏差或不适当的正则化而导致失败。在实证方面，我们展示了Prospect在跨表格、视觉和语言领域的分布偏移和公平性基准上能以2-3倍的速度收敛，比基线方法如随机梯度和随机鞍点方法快。

    We consider the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty. This formulation includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3$\times$ faster than baselines such as stochastic gradient and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.
    
[^31]: 渐进域自适应：理论与算法

    Gradual Domain Adaptation: Theory and Algorithms. (arXiv:2310.13852v1 [cs.LG])

    [http://arxiv.org/abs/2310.13852](http://arxiv.org/abs/2310.13852)

    本文研究了渐进域自适应中的渐进自训练算法，提出了一个改进的泛化界限，并指出了中间域在源域和目标域之间均匀放置的重要性。

    

    无监督域自适应（UDA）是将模型从有标记的源域适应到无标记的目标域的一种一次性方法。尽管被广泛应用，但当源域和目标域之间的分布偏移较大时，UDA面临巨大挑战。渐进域自适应（GDA）通过使用中间域逐渐从源域适应到目标域来缓解这个限制。在这项工作中，我们首先从理论上分析了一种常见的GDA算法——渐进自训练，并提供了与Kumar等人（2020）相比显著改进的泛化界限。我们的理论分析得出一个有趣的观点：为了最小化目标域上的泛化误差，中间域的顺序应该均匀地放置在源域和目标域之间的Wasserstein测地线上。这个观点在中间域缺失或稀缺的情况下尤其有用，而这在现实世界的应用中经常出现。

    Unsupervised domain adaptation (UDA) adapts a model from a labeled source domain to an unlabeled target domain in a one-off way. Though widely applied, UDA faces a great challenge whenever the distribution shift between the source and the target is large. Gradual domain adaptation (GDA) mitigates this limitation by using intermediate domains to gradually adapt from the source to the target domain. In this work, we first theoretically analyze gradual self-training, a popular GDA algorithm, and provide a significantly improved generalization bound compared with Kumar et al. (2020). Our theoretical analysis leads to an interesting insight: to minimize the generalization error on the target domain, the sequence of intermediate domains should be placed uniformly along the Wasserstein geodesic between the source and target domains. The insight is particularly useful under the situation where intermediate domains are missing or scarce, which is often the case in real-world applications. Based
    
[^32]: 使用正可分解核的几何学习

    Geometric Learning with Positively Decomposable Kernels. (arXiv:2310.13821v1 [cs.LG])

    [http://arxiv.org/abs/2310.13821](http://arxiv.org/abs/2310.13821)

    本文提出了使用正可分解核的几何学习方法，该方法通过在RKKS中学习而不需要访问核的分解，为非欧几里德数据的核学习提供了一条路径，并为RKKS方法提供了理论基础。

    

    核方法是机器学习中强大的工具。经典的核方法基于正定核，将数据空间映射到重现核希尔伯特空间(RKHS)。对于非欧几里德数据空间，很难找到正定核。在这种情况下，我们提出使用基于重现核控制空间(RKKS)的方法，这些方法只需要具有正分解的核。我们证明了在RKKS中学习时，并不需要访问这个分解。然后我们研究了使核正可分解的条件。我们证明在可处理的正则性假设下，不变核在齐次空间上允许正分解。这使得它们比正定核更容易构造，为非欧几里德数据的核学习提供了一条路径。同样，这为RKKS方法提供了一般的理论基础。

    Kernel methods are powerful tools in machine learning. Classical kernel methods are based on positive-definite kernels, which map data spaces into reproducing kernel Hilbert spaces (RKHS). For non-Euclidean data spaces, positive-definite kernels are difficult to come by. In this case, we propose the use of reproducing kernel Krein space (RKKS) based methods, which require only kernels that admit a positive decomposition. We show that one does not need to access this decomposition in order to learn in RKKS. We then investigate the conditions under which a kernel is positively decomposable. We show that invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions. This makes them much easier to construct than positive-definite kernels, providing a route for learning with kernels for non-Euclidean data. By the same token, this provides theoretical foundations for RKKS-based methods in general.
    
[^33]: 机器学习模型的成员推断攻击的基本限制

    Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])

    [http://arxiv.org/abs/2310.13786](http://arxiv.org/abs/2310.13786)

    本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。

    

    成员推断攻击（MIA）可以揭示特定数据点是否是训练数据集的一部分，可能暴露个人的敏感信息。本文探讨了关于机器学习模型上MIA的基本统计限制。具体而言，我们首先推导了统计量，该统计量决定了这种攻击的有效性和成功率。然后，我们研究了几种情况，并对这个感兴趣的统计量提供了界限。这使我们能够根据样本数量和学习模型的其他结构参数推断潜在攻击的准确性，在某些情况下可以直接从数据集中估计。

    Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
    
[^34]: DNA编码库的组合深度概率模型

    Compositional Deep Probabilistic Models of DNA Encoded Libraries. (arXiv:2310.13769v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.13769](http://arxiv.org/abs/2310.13769)

    本研究提出了一种组合深度概率模型DEL-Compose，用于对DNA编码库的数据进行建模和分析，以发现潜在的结构和信息，并通过改进观察模型来更好地处理数据噪声。

    

    DNA编码库（DEL）已被证明是一种利用组合构建的小分子进行高效筛选的强大工具。这些选择实验涉及多个阶段的洗涤、洗脱，并通过唯一的DNA条形码鉴定出强效结合物质，往往产生复杂的数据。这种复杂性可能掩盖了潜在的信号，因此需要应用机器学习等计算工具来发现有价值的见解。我们引入了一个DEL数据的组合深度概率模型DEL-Compose，它将分子表示分解为它们的单合子、二合子和三合子构建块，并通过模拟嵌入合成物之间的潜在反应来利用这些分子的内在分层结构。此外，我们还研究了改进DEL计数数据的观察模型的方法，如整合协变因子以更有效地解释数据噪声。

    DNA-Encoded Library (DEL) has proven to be a powerful tool that utilizes combinatorially constructed small molecules to facilitate highly-efficient screening assays. These selection experiments, involving multiple stages of washing, elution, and identification of potent binders via unique DNA barcodes, often generate complex data. This complexity can potentially mask the underlying signals, necessitating the application of computational tools such as machine learning to uncover valuable insights. We introduce a compositional deep probabilistic model of DEL data, DEL-Compose, which decomposes molecular representations into their mono-synthon, di-synthon, and tri-synthon building blocks and capitalizes on the inherent hierarchical structure of these molecules by modeling latent reactions between embedded synthons. Additionally, we investigate methods to improve the observation models for DEL count data such as integrating covariate factors to more effectively account for data noise. Acro
    
[^35]: 生成流网络作为熵正则化强化学习

    Generative Flow Networks as Entropy-Regularized RL. (arXiv:2310.12934v1 [cs.LG])

    [http://arxiv.org/abs/2310.12934](http://arxiv.org/abs/2310.12934)

    本研究将生成流网络的学习任务重新定义为具有特定奖励和正则化器结构的熵正则化强化学习问题，并证明熵正则化强化学习方法在生成流网络训练中具有实际效率和竞争力。

    

    最近提出的生成流网络(GFlowNets)是一种训练策略以便样本具有与给定奖励成比例的组合离散对象的概率的方法，通过一系列的动作。 GFlowNets利用问题的序列性质，与强化学习(RL)进行类比。我们的工作将RL和GFlowNets之间的联系扩展到了一般情况。我们演示了如何将学习生成流网络的任务高效地重新定义为具有特定奖励和正则化器结构的熵正则化RL问题。此外，我们通过将标准的软RL算法应用于几个概率建模任务的GFlowNet训练，来说明这种重定义的实际效率。与先前报道的结果相反，我们表明熵正则化强化学习方法在与已有的GFlowNet训练方法竞争中具有竞争力。这个观点为将强化学习原则融入实际问题提供了直接途径。

    The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the real
    
[^36]: 敏感性感知的摊销贝叶斯推断

    Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])

    [http://arxiv.org/abs/2310.11122](http://arxiv.org/abs/2310.11122)

    本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。

    

    贝叶斯推断是在不确定性下进行概率推理和决策的强大框架。现代贝叶斯工作流程中的基本选择涉及似然函数和先验分布的规范、后验逼近器和数据。每个选择都可以显着影响基于模型的推断和后续决策，因此需要进行敏感性分析。在这项工作中，我们提出了一种多方面的方法，将敏感性分析整合到摊销贝叶斯推断（ABI，即基于神经网络的模拟推断）中。首先，我们利用权重共享在训练过程中编码替代似然和先验规范之间的结构相似性，以最小的计算开销。其次，我们利用神经网络的快速推断来评估对各种数据扰动或预处理程序的敏感性。与大多数其他贝叶斯方法相比，这两个步骤都避免了昂贵的计算。

    Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
    
[^37]: 分布式变分推断用于在线监督学习

    Distributed Variational Inference for Online Supervised Learning. (arXiv:2309.02606v1 [cs.LG])

    [http://arxiv.org/abs/2309.02606](http://arxiv.org/abs/2309.02606)

    本文提出了一种适用于智能传感器网络的分布式变分推断算法，可以解决连续变量、大规模实时数据和难以处理的后验概率的推断问题。通过推导出可分离的较低下界，实现了在传感器网络中进行一跳通信的分布式变分推断，并提出了处理二进制问题的方法。

    

    在智能传感器网络中开发高效的推断问题解决方案对于下一代定位、跟踪和地图服务至关重要。本文提出了一种适用于连续变量、难以处理的后验概率和大规模实时数据的可扩展分布式概率推断算法。在集中式设置中，变分推断是一种执行近似贝叶斯估计的基本技术，其中将难以处理的后验密度用参数化密度来近似。我们的主要贡献在于推导出一个可分离的较低下界，用于集中式估计目标，从而实现了在传感器网络中进行一跳通信的分布式变分推断。我们的分布式证据较低下界 (DELBO) 包括观测似然和距离先验密度的差值的加权和，其与测量证据的差距是由于共识和建模误差造成的。为了解决二进制问题，我们提出了一种处理方法

    Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary 
    
[^38]: 分期变分推断：何时以及为什么使用？

    Amortized Variational Inference: When and Why?. (arXiv:2307.11018v1 [stat.ML])

    [http://arxiv.org/abs/2307.11018](http://arxiv.org/abs/2307.11018)

    本文研究了分期变分推断作为近似后验推断的一种通用替代方法，探讨了何时能够达到与传统的因子化变分推断相同的最优解。

    

    分期变分推断（A-VI）是一种近似处理概率模型中的难以计算的后验分布的方法。A-VI的定义特点是学习一个全局推断函数，将每个观察映射到其局部潜变量的近似后验分布。这与更传统的分解（或均场）变分推断（F-VI）形成对比，后者直接学习每个潜变量的近似分布的参数。在深度生成模型中，A-VI用作加速局部潜变量推断的计算技巧。本文研究A-VI作为近似后验推断的一种通用替代方法。由于分期家族是分解家族的子集，A-VI无法产生比F-VI最优解更低的Kullback-Leibler散度的近似值。因此，一个核心的理论问题是刻画A-VI何时仍然达到F-VI的最优解。

    Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We deri
    
[^39]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^40]: 通过潜在量化进行解缠

    Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])

    [http://arxiv.org/abs/2305.18378](http://arxiv.org/abs/2305.18378)

    本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。

    

    在解缠表示学习中，模型需要将数据集的基础变化因素分开并独立地表示出来，而模型并没有提供有关这些因素的真实信息，归纳偏见在实现解缠方面发挥着重要作用。在本文中，我们通过施加严格的交流瓶颈和强大的模型规范化，构建了一种朝着组合编码和解码数据的归纳偏见。具体来说，我们对潜在维度进行可学习的离散编码，并为每个维度应用一个单独的标量码书。潜在量化迫使编码器在许多数据点上使用少量潜在值，从而使解码器能够为每个值分配一致的含义。规范化有助于将模型引向这种简明策略。我们在多个基准数据集上展示了该方法的广泛应用性，并且展示了我们的方法显著提高了一系列标准VAE模型学习的表征的可解释性。

    In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
    
[^41]: 规范化在Sharpness-Aware Minimization中的关键作用

    The Crucial Role of Normalization in Sharpness-Aware Minimization. (arXiv:2305.15287v1 [cs.LG])

    [http://arxiv.org/abs/2305.15287](http://arxiv.org/abs/2305.15287)

    这篇论文提出的Sharpness-Aware Minimization算法大大提高了深度神经网络的预测性能，而其中规范化起着关键作用，通过稳定算法和使其漂移沿着一系列极小值提升性能，并使算法具有鲁棒性。

    

    Sharpness-Aware Minimization（SAM）是一种基于梯度的优化器，极大地提高了深度神经网络的预测性能。本文研究了SAM更新中规范化这一关键组件的作用，从理论和实验两方面分析了规范化在SAM中对凸函数和非凸函数的影响，揭示了规范化发挥的两个关键作用：i）它有助于稳定算法；ii）它使算法能够沿着一系列极小值（流形）漂移，这是最近一些理论工作确定的性能提升关键性质。此外，我们还认为，这两个正常化的属性使SAM对超参数的选择具有鲁棒性，证实了SAM的实用性。各种实验证明了我们的结论。

    Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding the role played by normalization, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima -- a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.
    
[^42]: 深度集成与（变分）贝叶斯方法之间的严格联系

    A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods. (arXiv:2305.15027v1 [stat.ML])

    [http://arxiv.org/abs/2305.15027](http://arxiv.org/abs/2305.15027)

    本论文建立了深度学习在不确定性量化中所使用的深度集成和（变分）贝叶斯方法的统一理论，通过将非凸优化问题转化为概率测度空间上的凸优化问题，并提出一族交互式深度集成方案，并在实验中验证了理论结果。

    

    我们首次在数学上建立了贝叶斯、变分贝叶斯和集成方法之间的严格联系。其关键步骤是将在深度学习中通常遇到的非凸优化问题重新表述为概率测度空间中的凸优化问题。在技术层面上，我们的贡献是通过Wasserstein梯度流的透镜研究广义变分推断。结果是一个统一的理论，涵盖多种看似无关的方法，这些方法通常用于深度学习中的不确定性量化，包括深度集成和（变分）贝叶斯方法。这为深度集成胜过基于参数化变分推断的程序背后的原因提供了新的视角，并允许推导具有收敛保证的新集成方案。我们通过提出一族具有直接类比于物理学中粒子系统交互的交互式深度集成来展示这一点，并提供一系列实验证明了我们的理论结果。

    We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this it to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lense of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning -- including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on parameterised variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle sys
    
[^43]: ELSA -- 提高碰撞模拟精确度的增强潜空间

    ELSA -- Enhanced latent spaces for improved collider simulations. (arXiv:2305.07696v1 [hep-ph] CROSS LISTED)

    [http://arxiv.org/abs/2305.07696](http://arxiv.org/abs/2305.07696)

    本文提出了多种增强模拟精确度的方法，包括在模拟链的末尾进行干预、在模拟链的开头进行干预和潜空间细化，通过 W+jets矩阵元代理模拟以及使用正则流和生成模型等策略进行研究，实验证明这些方法能显著提高精确度。

    

    模拟在碰撞物理中具有关键作用。我们探索机器学习增强模拟精确度的各种方法，包括在模拟链的末尾进行干预（重新加权）、在模拟链的开头进行干预（预处理）以及在末尾和开头之间建立联系（潜空间细化）。为了清晰地说明我们的方法，我们使用基于正则流的W+jets矩阵元代理模拟作为原型示例。首先，使用机器学习分类器在数据空间中确定权重。然后，我们将数据空间权重回推到潜空间以产生无权重的样本，并使用哈密顿蒙特卡罗使用潜空间细化（LASER）协议。另一种方法是增强正则流，该方法允许在潜空间和目标空间中具有不同的维度。我们研究了各种预处理策略，包括使用生成模型生成增强样本的新的通用方法。结果表明，这些方法显着提高了W+jets矩阵元模拟的精确度。

    Simulations play a key role for inference in collider physics. We explore various approaches for enhancing the precision of simulations using machine learning, including interventions at the end of the simulation chain (reweighting), at the beginning of the simulation chain (pre-processing), and connections between the end and beginning (latent space refinement). To clearly illustrate our approaches, we use W+jets matrix element surrogate simulations based on normalizing flows as a prototypical example. First, weights in the data space are derived using machine learning classifiers. Then, we pull back the data-space weights to the latent space to produce unweighted examples and employ the Latent Space Refinement (LASER) protocol using Hamiltonian Monte Carlo. An alternative approach is an augmented normalizing flow, which allows for different dimensions in the latent and target spaces. These methods are studied for various pre-processing strategies, including a new and general method f
    
[^44]: 通过连续方式隐式优化的政策梯度算法

    Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])

    [http://arxiv.org/abs/2305.06851](http://arxiv.org/abs/2305.06851)

    本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。

    

    强化学习中的直接策略优化通常通过政策梯度算法解决，该算法通过随机梯度上升优化策略参数。本文提供了一种新的理论解释和证明这些算法的方法。首先，我们将直接策略优化问题建立在优化连续框架下。后者是一种用于优化非凸函数的框架，其中以连续的替代目标函数序列为基础。其次，我们证明了优化仿射高斯策略并执行熵正则化可以解释为通过连续隐式地优化确定性策略。基于这些理论结果，我们认为政策梯度算法中的探索包括计算当前的策略收益的连续函数，策略的方差应该是历史依赖性函数，以避免局部最值而不是仅仅最大化政策的收益。

    Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
    
[^45]: 毫不畏惧地选择：几乎所有的小批量训练方案都能够优化。

    Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])

    [http://arxiv.org/abs/2305.02247](http://arxiv.org/abs/2305.02247)

    本文证明了对于数据独立的批处理方案，几乎所有小批量梯度下降训练都能够优化，其中包括所有的确定性方案和随机方案。此外，所有这样的批量调度都能达到最优的一般化误差下限。

    

    我们证明了带有确定性或随机性、数据独立的小批量梯度下降训练的匹配上下一般化误差界限，但批量选择规则是任意的。我们考虑光滑的Lipschitz-凸性/非凸性/强凸性损失函数，并证明了随机梯度下降的经典上限界限也适用于这样任意的非自适应批量调度，包括所有确定性的调度方案。进一步地，对于凸和强凸的损失函数，我们直接证明了在上述批量调度类上一致的一般化误差下的匹配下限界限，表明所有这样的批量调度都能达到最优的一般化。最后，对于光滑的（非Lipschitz）非凸性损失函数，我们证明了在所考虑的类别内，包括所有随机批处理方案，全批量（确定性）梯度下降是最优的。

    We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.
    
[^46]: 深度结构高斯特征模型的学习曲线

    Learning curves for deep structured Gaussian feature models. (arXiv:2303.00564v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.00564](http://arxiv.org/abs/2303.00564)

    该论文针对深度高斯模型的特征各向异性展开研究。研究结果表明，在第一层特征行之间允许存在相关性可以促进泛化，而后续层的结构通常是不利的。

    

    近年来，深度学习理论中对于多层高斯随机特征模型的泛化性能分析引起了广泛关注。然而，很少有研究考虑特征各向异性的影响；大多数模型都假设特征是使用独立同分布的高斯权重生成的。在这篇论文中，我们为具有许多层结构高斯特征的模型导出了学习曲线。我们表明，允许第一层特征的行之间存在相关性可促进泛化，而后续层的结构通常是不利的。我们的结果揭示了权重结构如何影响可解模型的泛化性能。

    In recent years, significant attention in deep learning theory has been devoted to analyzing the generalization performance of models with multiple layers of Gaussian random features. However, few works have considered the effect of feature anisotropy; most assume that features are generated using independent and identically distributed Gaussian weights. Here, we derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.
    
[^47]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^48]: 重参数化下神经网络参数空间的几何学

    The Geometry of Neural Nets' Parameter Spaces Under Reparametrization. (arXiv:2302.07384v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07384](http://arxiv.org/abs/2302.07384)

    研究了神经网络在重参数化下的不变性，如果显式地表示度量并使用正确的相关变换规则，则不变性是任何神经网络的固有属性。

    

    模型重参数化是改善神经网络训练的一种流行方法，但也可能存在问题，如在Hessian平坦度测量、优化轨迹和概率密度模式等方面引入不一致性。这使得下游分析变得更为复杂：例如，由于任意的重参数化都可以改变二者之间的关系，因此无法明确地将平坦度与泛化联系起来。在本文中，我们从黎曼几何的角度研究了神经网络在重参数化下的不变性。从这个角度来看，如果我们显式地表示度量并使用正确的相关变换规则，那么不变性是任何神经网络的固有属性。这一点很重要，因为尽管度量始终存在，但通常被隐式地假定为单位矩阵，并因此从符号中省略，然后在重参数化下丢失了。我们讨论了衡量平坦度所带来的启示。

    Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net if one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of
    
[^49]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^50]: ResMem：学习可以的，记住剩下的。

    ResMem: Learn what you can and memorize the rest. (arXiv:2302.01576v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01576](http://arxiv.org/abs/2302.01576)

    ResMem是一种通过显式记忆来改善模型泛化能力的方法，它通过拟合模型的残差来实现。在各种视觉和自然语言处理基准测试中，ResMem一致地改善了原始预测模型的测试集泛化能力。

    

    现代神经网络所展现出的令人瞩目的泛化性能部分归功于其隐式记忆复杂的训练模式的能力。受此启发，我们探索了一种改进模型泛化能力的新机制，通过显式记忆来实现。具体地，我们提出了残差记忆（ResMem）算法，这是一种通过用基于$k$最近邻的回归器拟合模型的残差来增加现有预测模型（例如神经网络）的方法。最终预测是原始模型和拟合的残差回归器的和。通过构造，ResMem可以显式地记住训练标签。实证上，我们展示了ResMem在各种标准视觉和自然语言处理基准测试中一致地改善了原始预测模型的测试集泛化能力。理论上，我们构建了一个简化的线性回归问题，并严格证明了ResMem相对于基本预测模型具有更有利的测试风险。

    The impressive generalization performance of modern neural networks is attributed in part to their ability to implicitly memorize complex training patterns. Inspired by this, we explore a novel mechanism to improve model generalization via explicit memorization. Specifically, we propose the residual-memorization (ResMem) algorithm, a new method that augments an existing prediction model (e.g. a neural network) by fitting the model's residuals with a $k$-nearest neighbor based regressor. The final prediction is then the sum of the original model and the fitted residual regressor. By construction, ResMem can explicitly memorize the training labels. Empirically, we show that ResMem consistently improves the test set generalization of the original prediction model across various standard vision and natural language processing benchmarks. Theoretically, we formulate a stylized linear regression problem and rigorously show that ResMem results in a more favorable test risk over the base predi
    
[^51]: 双重对抗性联邦多臂赌博问题研究

    Doubly Adversarial Federated Bandits. (arXiv:2301.09223v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.09223](http://arxiv.org/abs/2301.09223)

    我们研究了一种新的非随机联邦多臂赌博问题，考虑了具有双重对抗性的设置。我们提供了任何联邦赌博算法的遗憾下界，并提出了一种接近最优的算法FEDEXP3。该算法解决了之前的开放性问题。

    

    我们研究了一种新的非随机联邦多臂赌博问题，多个代理通过通信网络进行协作。臂的损失由一个无意识的对手分配，该对手不仅指定每个时间步和每个代理的每个臂的损失，还具有“双重对抗性”。在这种设置下，不同的代理可能在同一时间步选择相同的臂，但观察到不同的反馈。每个代理的目标是找到一个全局最好的臂，使得在所有代理上平均累积损失最低，这需要代理之间的通信。我们针对不同设置提供了任何联邦赌博算法的遗憾下界，当代理有完全信息反馈或赌博反馈时。对于赌博反馈设置，我们提出了一种接近最优的联邦赌博算法称为FEDEXP3。我们的算法对Cesa-Bianchi等人（2016）提出的一个开放性问题给出了正面答案：FEDEXP3可以保证...

    We study a new non-stochastic federated multi-armed bandit problem with multiple agents collaborating via a communication network. The losses of the arms are assigned by an oblivious adversary that specifies the loss of each arm not only for each time step but also for each agent, which we call ``doubly adversarial". In this setting, different agents may choose the same arm in the same time step but observe different feedback. The goal of each agent is to find a globally best arm in hindsight that has the lowest cumulative loss averaged over all agents, which necessities the communication among agents. We provide regret lower bounds for any federated bandit algorithm under different settings, when agents have access to full-information feedback, or the bandit feedback. For the bandit feedback setting, we propose a near-optimal federated bandit algorithm called FEDEXP3. Our algorithm gives a positive answer to an open question proposed in Cesa-Bianchi et al. (2016): FEDEXP3 can guarante
    
[^52]: 多输出可学习性的特征化研究

    A Characterization of Multioutput Learnability. (arXiv:2301.02729v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02729](http://arxiv.org/abs/2301.02729)

    该论文研究了多输出函数类的学习问题，提出了多输出函数类学习的特征化判别标准，即每个单输出子类可学习时多输出函数类才可学习，在多标记分类和多输出回归领域取得了重要进展。

    

    本文考虑了批处理和在线学习中的多输出函数类学习问题。我们证明了，当且仅当函数类的每个单输出子类都可学习时，多输出函数类才是可学习的。这提供了多标记分类和多输出回归在批处理和在线学习中可学习性的完整特征化。作为扩展，我们还考虑了在赌博反馈环境下的多标记学习，并展示了与完全反馈环境下类似的特征化。

    We consider the problem of learning multioutput function classes in batch and online settings. In both settings, we show that a multioutput function class is learnable if and only if each single-output restriction of the function class is learnable. This provides a complete characterization of the learnability of multilabel classification and multioutput regression in both batch and online settings. As an extension, we also consider multilabel learnability in the bandit feedback setting and show a similar characterization as in the full-feedback setting.
    
[^53]: 可证明固定时间收敛和快速逃逸非退化鞍点的广义梯度流

    Generalized Gradient Flows with Provable Fixed-Time Convergence and Fast Evasion of Non-Degenerate Saddle Points. (arXiv:2212.03765v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03765](http://arxiv.org/abs/2212.03765)

    该论文介绍了一种广义梯度流算法，它能够在固定时间内收敛到非凸函数的最优解，并且能够快速逃逸非退化鞍点。

    

    基于梯度的一阶凸优化算法在各种领域中得到了广泛的应用，包括机器学习任务。受连续时间动力系统固定时间稳定性理论的最新进展的启发，我们引入了一个广义框架，用于设计具有最强收敛性保证的加速优化算法，这些算法进一步适用于非凸函数的子类。我们特别介绍了GenFlow算法及其动量变体，它们可证明在固定时间内收敛到满足Polyak-Lojasiewicz (PL)不等式的目标函数的最优解。此外，对于具有非退化鞍点的函数，我们证明了对于提出的GenFlow算法，躲避这些鞍点所需的时间在所有初始条件下都有一致的上界。最后，对于最优解为鞍点的强凸-强凹极小极大问题，类似的方案被证明可以达到。

    Gradient-based first-order convex optimization algorithms find widespread applicability in a variety of domains, including machine learning tasks. Motivated by the recent advances in fixed-time stability theory of continuous-time dynamical systems, we introduce a generalized framework for designing accelerated optimization algorithms with strongest convergence guarantees that further extend to a subclass of non-convex functions. In particular, we introduce the GenFlow algorithm and its momentum variant that provably converge to the optimal solution of objective functions satisfying the Polyak-{\L}ojasiewicz (PL) inequality in a fixed time. Moreover, for functions that admit non-degenerate saddle-points, we show that for the proposed GenFlow algorithm, the time required to evade these saddle-points is uniformly bounded for all initial conditions. Finally, for strongly convex-strongly concave minimax problems whose optimal solution is a saddle point, a similar scheme is shown to arrive a
    
[^54]: 关于后悔最小的合作非随机多臂老虎机问题

    On Regret-optimal Cooperative Nonstochastic Multi-armed Bandits. (arXiv:2211.17154v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.17154](http://arxiv.org/abs/2211.17154)

    本研究考虑了具有延迟通信网络的合作非随机多智能体多臂老虎机问题，在合适的正则化器和通信协议下，采用"FTRL"算法可以达到个体后悔的最小化，且具有后悔最优性。在数值实验中验证了理论结果并展示了算法的优越性。

    

    我们考虑了一个具有延迟通信网络的合作非随机多智能体多臂老虎机问题。我们证明了所有智能体的个体后悔的下界。我们证明当臂的数量相对于通信图中智能体的度数足够大时，采用适当的正则化器和通信协议，合作多个智能体的"FTRL"算法的个体后悔上界与下界相匹配，最多仅相差一个常数因子。我们还证明了一个具有适当正则化器的"FTRL"算法相对于边延迟参数的缩放具有后悔最优性。我们提供了数值实验来验证我们的理论结果，并展示了我们的算法优于先前提出的算法的情况。

    We consider the nonstochastic multi-agent multi-armed bandit problem with agents collaborating via a communication network with delays. We show a lower bound for individual regret of all agents. We show that with suitable regularizers and communication protocols, a collaborative multi-agent \emph{follow-the-regularized-leader} (FTRL) algorithm has an individual regret upper bound that matches the lower bound up to a constant factor when the number of arms is large enough relative to degrees of agents in the communication graph. We also show that an FTRL algorithm with a suitable regularizer is regret optimal with respect to the scaling with the edge-delay parameter. We present numerical experiments validating our theoretical results and demonstrate cases when our algorithms outperform previously proposed algorithms.
    
[^55]: 关于图神经网络模拟顶点间相互作用的研究

    On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16494](http://arxiv.org/abs/2211.16494)

    本文研究了GNN模拟顶点间相互作用的能力，通过一个被称为分离秩的度量标准来量化这种能力，结果表明模拟相互作用的能力主要取决于分区的行走指数，即从分界线开始的行走数量，同时设计了一种名为WISA的边稀疏化算法以提高GNNs的处理效率和表达能力。

    

    图神经网络(GNNs)被广泛用于建模由图中顶点表示的实体之间的复杂互动。尽管最近有一些理论分析GNNs表达能力的努力，但对其模拟相互作用的能力缺乏一个正式的描述。本文旨在填补这一空白。通过一个已知的度量标准——分离秩(separation rank)来规范化相互作用的强度，我们量化了某些GNNs模拟给定顶点子集及其补集之间交互的能力，即输入顶点组成的给定分区的两侧之间的互动。我们的结果表明，模拟相互作用的能力主要取决于分区的行走指数(walk index)——一个由分界线开始的行走数量定义的图形特征。常见GNN架构的实验证明了这一发现。作为我们理论的实际应用，我们设计了一种名为Walk Indexed Sparsification Algorithm (WISA)的边稀疏化算法，利用我们的研究结果提高处理大规模图形的GNNs效率同时保持它们的表达能力。

    Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
    
[^56]: 用张量网络形式统一O(3)等变神经网络设计

    Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07482](http://arxiv.org/abs/2211.07482)

    本文提出使用融合图来设计等变神经网络的新组件，为解决涉及全局空间和置换对称性的学习任务提供了一种图形化的方法。

    

    许多学习任务，包括从从第一原理计算中学习势能面，涉及到全局空间对称性和原子或一般粒子之间的置换对称性。等变图神经网络是解决这类问题的标准方法之一，其中最成功的方法之一是使用在空间群下变换的各种张量之间的张量积。然而，随着不同张量的数量和它们之间关系的复杂性增加，保持简洁和等变性变得越来越具有挑战性。在本文中，我们提出使用融合图，一种广泛用于模拟SU(2)对称量子多体问题的技术，来为等变神经网络设计新的等变组件。这导致了一种基于图的方法来构建新的神经网络架构。当应用于给定局部邻域中的粒子时，我们称之为“融合块”的结果组件起到了

    Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
    
[^57]: 重新考虑Le Cam方程：凸密度类的精确极小极大速率

    Revisiting Le Cam's Equation: Exact Minimax Rates over Convex Density Classes. (arXiv:2210.11436v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2210.11436](http://arxiv.org/abs/2210.11436)

    本文通过研究在凸密度类上进行密度估计的极小极大速率，扩展了已有结果，确定了任何凸密度类的精确极小极大速率，并且提出了自适应估计器。

    

    我们研究了在凸密度类上进行密度估计的极小极大速率的经典问题。在Le Cam（1973）、Birge（1983, 1986）、Wong和Shen（1995）以及Yang和Barron（1999）的开创性工作基础上，我们确定了任何凸密度类的精确（仅与常数相关）极小极大速率。这项工作通过证明密度类的局部度量熵总是捕捉到这些设置下的极小极大最优速率，从而扩展了这些已知结果。我们的界限在比以前考虑的密度类的丰富性方面提供了统一的视角，适用于参数化和非参数化的凸密度类。我们提出的“多阶段筛”MLE适用于任何这样的凸密度类。我们进一步证明了这个估计器对真实的潜在密度是自适应的。我们将我们的风险界应用于重新导出已知的极小极大速率，包括有界总变差和Holder密度类。

    We study the classical problem of deriving minimax rates for density estimation over convex density classes. Building on the pioneering work of Le Cam (1973), Birge (1983, 1986), Wong and Shen (1995), Yang and Barron (1999), we determine the exact (up to constants) minimax rate over any convex density class. This work thus extends these known results by demonstrating that the local metric entropy of the density class always captures the minimax optimal rates under such settings. Our bounds provide a unifying perspective across both parametric and nonparametric convex density classes, under weaker assumptions on the richness of the density class than previously considered. Our proposed `multistage sieve' MLE applies to any such convex density class. We further demonstrate that this estimator is also adaptive to the true underlying density of interest. We apply our risk bounds to rederive known minimax rates including bounded total variation, and Holder density classes. We further illust
    
[^58]: 带外部输入的MDPs的追溯学习

    Hindsight Learning for MDPs with Exogenous Inputs. (arXiv:2207.06272v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06272](http://arxiv.org/abs/2207.06272)

    提出了一种数据高效的带有外部输入的MDPs算法，名为追溯学习（HL）。HL算法通过利用外部变量样本使得过去的决策在回溯中可以加速策略改进，在资源管理问题中表现出良好的性能。

    

    许多资源管理问题需要在不确定性下做出迭代决策，其中影响决策结果的唯一不确定性是决策者控制之外的外部变量。我们将这些问题建模为带有外部输入的MDPs（马尔可夫决策过程），并设计了一类名为追溯学习（HL）的数据高效算法。我们的HL算法通过利用一个关键洞见实现了数据效率：通过外部变量的样本，过去的决策可以在回溯中重新审视，以推断出可以加速策略改进的反事实后果。我们将HL与多个基线算法在多个测试案例中进行比较，包括多秘书和航空公司收益管理问题。我们还将我们的算法扩展到业务关键的云资源管理问题——将虚拟机（VM）分配到物理机器上，并使用来自大型公共云提供商的真实数据集模拟其性能。我们发现HL算法优于基准算法。

    Many resource management problems require sequential decision-making under uncertainty, where the only uncertainty affecting the decision outcomes are exogenous variables outside the control of the decision-maker. We model these problems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and design a class of data-efficient algorithms for them termed Hindsight Learning (HL). Our HL algorithms achieve data efficiency by leveraging a key insight: having samples of the exogenous variables, past decisions can be revisited in hindsight to infer counterfactual consequences that can accelerate policy improvements. We compare HL against classic baselines in the multi-secretary and airline revenue management problems. We also scale our algorithms to a business-critical cloud resource management problem -- allocating Virtual Machines (VMs) to physical machines, and simulate their performance with real datasets from a large public cloud provider. We find that HL algorithms outperform d
    
[^59]: 使用特征选择算法确定类风湿性关节炎小鼠模型的免疫状态

    Employing Feature Selection Algorithms to Determine the Immune State of a Mouse Model of Rheumatoid Arthritis. (arXiv:2207.05882v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.05882](http://arxiv.org/abs/2207.05882)

    该论文研究了使用特征选择算法来确定类风湿性关节炎小鼠模型的免疫状态。研究目标是通过调节免疫状态中的调节作用因子，关闭自身免疫反应中的自身免疫通路。通过考虑胶原诱导性关节炎小鼠模型进行实验，作者探索了如何确定系统状态以提高免疫疗法效果。

    

    免疫反应是一个动态过程，通过该过程，机体确定抗原是自身还是非自身。这个动态过程的状态由组成决策过程的炎症和调节作用因子的相对平衡和种群定义。免疫疗法应用于类风湿性关节炎等，其目标是将免疫状态偏向调节作用因子，从而关闭自身免疫反应中的自身免疫通路。虽然已知有几种免疫疗法方法，但该疗法的有效性取决于该干预如何改变该状态的演变。不幸的是，这个过程不仅由过程的动态性确定，还由干预时系统的状态确定，而干预前很难甚至不可能确定系统的状态。为了确定这种状态，我们考虑了一个类风湿性关节炎小鼠模型（胶原诱导性关节炎），进行了免疫疗法；收集了高量级。。。

    The immune response is a dynamic process by which the body determines whether an antigen is self or nonself. The state of this dynamic process is defined by the relative balance and population of inflammatory and regulatory actors which comprise this decision making process. The goal of immunotherapy as applied to, e.g. Rheumatoid Arthritis (RA), then, is to bias the immune state in favor of the regulatory actors - thereby shutting down autoimmune pathways in the response. While there are several known approaches to immunotherapy, the effectiveness of the therapy will depend on how this intervention alters the evolution of this state. Unfortunately, this process is determined not only by the dynamics of the process, but the state of the system at the time of intervention - a state which is difficult if not impossible to determine prior to application of the therapy. To identify such states we consider a mouse model of RA (Collagen-Induced Arthritis (CIA)) immunotherapy; collect high di
    
[^60]: 动态时间规整距离的统计推断及其在异常时间序列检测中的应用

    Statistical Inference for the Dynamic Time Warping Distance, with Application to Abnormal Time-Series Detection. (arXiv:2202.06593v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.06593](http://arxiv.org/abs/2202.06593)

    本研究提出了一种新的统计推断方法，用于解决在不确定环境下基于动态时间规整算法的时间序列相似性/距离问题。该方法能够提供有效的p值，对于异常时间序列检测等高风险决策具有重要意义。

    

    本研究考虑了在不确定环境下基于动态时间规整（DTW）算法的两个时间序列之间的相似性/距离的统计推断问题，提出了一种统计假设检验方法。由于DTW距离是基于DTW算法的解得到的，其采样分布很难计算。为了解决这个问题，我们提出了条件选择推断框架，能够推导出一种对DTW距离进行有效推断的方法。据我们所知，这是第一种能够提供有效p值来量化DTW距离的统计显著性的方法，对于像异常时间序列检测等高风险决策非常有帮助。我们在合成和真实数据集上评估了所提出的推断方法的性能。

    We study statistical inference on the similarity/distance between two time-series under uncertain environment by considering a statistical hypothesis test on the distance obtained from Dynamic Time Warping (DTW) algorithm. The sampling distribution of the DTW distance is too difficult to derive because it is obtained based on the solution of the DTW algorithm, which is complicated. To circumvent this difficulty, we propose to employ the conditional selective inference framework, which enables us to derive a valid inference method on the DTW distance. To our knowledge, this is the first method that can provide a valid p-value to quantify the statistical significance of the DTW distance, which is helpful for high-stake decision making such as abnormal time-series detection problems. We evaluate the performance of the proposed inference method on both synthetic and real-world datasets.
    
[^61]: 不同输入维度数据集之间的迁移学习：线性回归情况下的算法和分析

    Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05069](http://arxiv.org/abs/2202.05069)

    本文提出了一种适用于线性回归情况的迁移学习算法，该算法能够将新数据与历史数据相结合，特别在新数据稀缺的情况下具有益处，并且在实验验证中表现出对负迁移学习的鲁棒性。

    

    随着新传感器和监测设备的发展，越来越多的数据源可以作为机器学习模型的输入。这些数据既可以帮助提高模型的准确性，但将这些新输入与历史数据相结合仍然是一个尚未详细研究的挑战。在本文中，我们提出了一种迁移学习算法，将新数据和历史数据结合起来，特别在新数据稀缺的情况下具有益处。我们将重点放在线性回归情况下，这使得我们能够对该方法的益处进行严格的理论研究。我们表明我们的方法对负迁移学习是具有鲁棒性的，并通过真实和模拟数据进行了实证验证。

    With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
    
[^62]: 基于结果抽样的因果推断和单调性假设

    Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions. (arXiv:2004.08318v5 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2004.08318](http://arxiv.org/abs/2004.08318)

    本文研究了基于结果抽样的因果推断，发现强无偏性不总是像随机抽样下那样强大，并且某些单调性假设在锐利识别间隔方面产生可比较的结果。 通过算法推断出参数并用实证例子证明了方法的贡献。

    

    本文研究了根据案例控制和案例群体取样进行因果推断的方法。具体而言，我们关注二元结果和二元治疗的情况，其中感兴趣的参数是通过潜在结果框架定义的因果相对和可归因风险。我们发现强无偏性不总是像随机抽样下那样强大，并且某些单调性假设在锐利识别间隔方面产生可比较的结果。具体而言，通常的比值比在单调治疗反应和单调治疗选择假设下被证明是因果相对风险的锐利识别上界。我们提供了聚合在协变量的真实人口分布上的因果参数推断算法。我们通过研究三个实证例子展示了我们方法的有用性：在巴基斯坦进入著名大学的私立学校受益问题上；在留在学校和放弃学校早期的关系问题上；在预测岗位培训效果问题上。

    We study causal inference under case-control and case-population sampling. Specifically, we focus on the binary-outcome and binary-treatment case, where the parameters of interest are causal relative and attributable risks defined via the potential outcome framework. It is shown that strong ignorability is not always as powerful as it is under random sampling and that certain monotonicity assumptions yield comparable results in terms of sharp identified intervals. Specifically, the usual odds ratio is shown to be a sharp identified upper bound on causal relative risk under the monotone treatment response and monotone treatment selection assumptions. We offer algorithms for inference on the causal parameters that are aggregated over the true population distribution of the covariates. We show the usefulness of our approach by studying three empirical examples: the benefit of attending private school for entering a prestigious university in Pakistan; the relationship between staying in sc
    
[^63]: Patch-level Neighborhood Interpolation: 一种通用且有效的基于图的正则化策略

    Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy. (arXiv:1911.09307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.09307](http://arxiv.org/abs/1911.09307)

    这项工作提出了一种名为Pani的通用正则化器，它在深度神经网络中进行非局部表示，并将邻域补丁特征进行线性插值，从而构建了一种通用且有效的正则化策略。

    

    正则化对于机器学习模型尤其是深度神经网络非常重要。现有的正则化技术主要依赖于独立同分布假设，并且仅考虑当前样本的知识，没有利用样本之间的邻居关系。在这项工作中，我们提出了一种称为“Patch-level Neighborhood Interpolation（Pani）”的通用正则化器，在网络计算中进行非局部表示。我们的提议明确地构建了不同层次的补丁级图，然后线性插值邻域补丁特征，作为一种通用且有效的正则化策略。此外，我们将我们的方法定制为两种流行的正则化方法，即虚拟对抗训练（VAT）和MixUp以及其变体。首先派生的“Pani VAT”通过使用补丁级插值扰动构建非局部对抗平滑度，提出了一种新颖的方法。

    Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \textbf{Patch-level Neighborhood Interpolation~(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. Th
    
[^64]: TimbreTron：用于音乐音色转换的WaveNet（CycleGAN（CQT（Audio）））流水线

    TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer. (arXiv:1811.09620v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/1811.09620](http://arxiv.org/abs/1811.09620)

    TimbreTron是一种用于音乐音色转换的方法，将图像领域的风格转换应用到音频信号的时频表示上，然后使用条件WaveNet合成器生成高质量的波形。通过人类感知评估，证实了其可行性。

    

    本研究解决了音乐音色转换的问题，目标是将一个乐器的声音样本的音色转换为另一个乐器的音色，同时保留其他音乐内容，如音高、节奏和音量。我们引入了TimbreTron，一种音乐音色转换方法，它将“图像”领域的风格转换应用到音频信号的时频表示上，然后使用条件WaveNet合成器生成高质量的波形。我们证明了Constant Q Transform（CQT）表示特别适合卷积结构，因其近似的音高等变性。基于人类感知评估，我们确认TimbreTron可以被识别出来。

    In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies "image" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably tra
    

