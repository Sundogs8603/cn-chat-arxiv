# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A duality framework for generalization analysis of random feature models and two-layer neural networks.](http://arxiv.org/abs/2305.05642) | 本文提出了一个针对随机特征模型和双层神经网络的泛化分析的对偶性框架，并证明了学习不会受到维数灾难的影响，使 RFMs 可以在核范围之外发挥作用。 |
| [^2] | [Metric Space Magnitude and Generalisation in Neural Networks.](http://arxiv.org/abs/2305.05611) | 本研究使用一种新的拓扑不变量——大小，来量化深度神经网络的学习过程，研究其内部表示并提出一个新方法来确定其泛化能力，实验证明此框架可作为泛化错误的指标。 |
| [^3] | [Accelerate Langevin Sampling with Birth-Death process and Exploration Component.](http://arxiv.org/abs/2305.05529) | 该论文提出了一种新的采样方法，在探索新模式和传递有用信息的过程中利用了Birth-Death过程和探索组件，具有高效和指数渐近收敛等优点。 |
| [^4] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^5] | [Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions.](http://arxiv.org/abs/2305.05400) | 本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。 |
| [^6] | [Towards Understanding Generalization of Macro-AUC in Multi-label Learning.](http://arxiv.org/abs/2305.05248) | 本研究探究了 multi-label 学习中常用的 Macro-AUC 的泛化性质，并发现数据集中标签不平衡对泛化界限有重要影响。未经变量处理的基于损失函数的算法可能由于对标签的不平衡更敏感而表现较差，这一结论在多个数据集上得到验证。 |
| [^7] | [Solving Linear Inverse Problems using Higher-Order Annealed Langevin Diffusion.](http://arxiv.org/abs/2305.05014) | 本论文提出了一种高阶 Langevin 动力学的算法，可更高效地采样未知变量后验分布，同时加入淬火过程，能应用于离散未知变量情况，数值实验表明在多个任务中相对竞争方法具有更高性能。 |
| [^8] | [FedHB: Hierarchical Bayesian Federated Learning.](http://arxiv.org/abs/2305.04979) | 该论文提出了一种层次贝叶斯联邦学习方法，通过块坐标下降分布式算法实现对客户端私有数据不透露的学习，在收敛速度上与正则化相同。 |
| [^9] | [Error Analysis of Kernel/GP Methods for Nonlinear and Parametric PDEs.](http://arxiv.org/abs/2305.04962) | 本文介绍了使用核/GP方法估计非线性和参数PDE通过Sobolev空间误差，并指出当PDE的解空间充分光滑时，具有维度友好的收敛速率。 |
| [^10] | [Gaussian process deconvolution.](http://arxiv.org/abs/2305.04871) | 本文提出了一种基于高斯过程的贝叶斯非参数方法，可以解决连续时间信号的去卷积问题，适用于观测值中可能存在缺失数据且信号滤波器未知的情况。 |
| [^11] | [Calibration Assessment and Boldness-Recalibration for Binary Events.](http://arxiv.org/abs/2305.03780) | 本研究提出了一种假设检验和贝叶斯模型选择方法来评估校准，并提供一种大胆再校准策略，使实践者能够在满足所需的校准水平的情况下负责任地增强预测。 |
| [^12] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^13] | [Non-asymptotic analysis of Langevin-type Monte Carlo algorithms.](http://arxiv.org/abs/2303.12407) | 本文提出了一种新的Langevin型算法并应用于吉布斯分布。通过提出的2-Wasserstein距离上限，我们发现势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性可以保证算法具有接近零的误差。新的Langevin型算法还可以应用于无凸性或连续可微性的势函数。 |
| [^14] | [Provably Convergent Plug-and-Play Quasi-Newton Methods.](http://arxiv.org/abs/2303.07271) | 本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。 |
| [^15] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^16] | [Proximal Causal Learning of Conditional Average Treatment Effects.](http://arxiv.org/abs/2301.10913) | 提出了P-学习器，一种用于学习处理效果异质性的定制两阶段损失函数，能够依靠代理变量进行因果推断，具有较高的灵活性和效率。 |
| [^17] | [A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation.](http://arxiv.org/abs/2202.08146) | 该论文提出了一种基于Wi-Fi信道数据的人与人互动识别的新方法，使用注意力双向门控循环神经网络实现高精度的实时处理，准确率达到98.22%，并提供了一个GUI应用程序方便实时应用。 |
| [^18] | [Proportional Fairness in Federated Learning.](http://arxiv.org/abs/2202.01666) | 本文提出了用于保证联邦学习中公平性的比例公平性 (PF) 概念，并提出了一种新颖易于实现的算法 PropFair，能够在所有客户端平均性能和最差 10% 客户端平均性能之间达到良好平衡。 |
| [^19] | [Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning.](http://arxiv.org/abs/2108.05660) | 本研究提出了一种风险低且高精度的堆叠集成机器学习模型，可以从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。 |
| [^20] | [Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning.](http://arxiv.org/abs/2107.04050) | 本文针对均场控制问题提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在未知系统动力学的情况下，该算法可平衡探索和利用，并实现了可证明的问题求解，具有较好的表现。 |
| [^21] | [A Kernel Stein Test for Comparing Latent Variable Models.](http://arxiv.org/abs/1907.00586) | 本研究提出了一种核Stein检验方法，可用于比较具有潜变量的模型，相比于当前方法，性能更好。 |

# 详细

[^1]: 随机特征模型和双层神经网络的泛化分析的对偶性框架

    A duality framework for generalization analysis of random feature models and two-layer neural networks. (arXiv:2305.05642v1 [stat.ML])

    [http://arxiv.org/abs/2305.05642](http://arxiv.org/abs/2305.05642)

    本文提出了一个针对随机特征模型和双层神经网络的泛化分析的对偶性框架，并证明了学习不会受到维数灾难的影响，使 RFMs 可以在核范围之外发挥作用。

    

    本文研究在高维分析中出现的自然函数空间 $\mathcal{F}_{p,\pi}$ 和 Barron 空间中学习函数的问题。通过对偶分析，我们揭示了这些空间的逼近和估计可以在某种意义下被视为等价的。这使得我们能够在研究这两种模型的泛化时更专注于更容易的逼近和估计问题。通过定义一种基于信息的复杂度来有效地控制估计误差，建立了对偶等价性。此外，我们通过对两个具体应用进行综合分析展示了我们的对偶性框架的灵活性。第一个应用是研究使用 RFMs 学习 $\mathcal{F}_{p,\pi}$ 中的函数。我们证明只要 $p>1$，学习不会受到维数灾难的影响，这意味着 RFMs 可以在核范围之外发挥作用。

    We consider the problem of learning functions in the $\mathcal{F}_{p,\pi}$ and Barron spaces, which are natural function spaces that arise in the high-dimensional analysis of random feature models (RFMs) and two-layer neural networks. Through a duality analysis, we reveal that the approximation and estimation of these spaces can be considered equivalent in a certain sense. This enables us to focus on the easier problem of approximation and estimation when studying the generalization of both models. The dual equivalence is established by defining an information-based complexity that can effectively control estimation errors. Additionally, we demonstrate the flexibility of our duality framework through comprehensive analyses of two concrete applications.  The first application is to study learning functions in $\mathcal{F}_{p,\pi}$ with RFMs. We prove that the learning does not suffer from the curse of dimensionality as long as $p>1$, implying RFMs can work beyond the kernel regime. Our 
    
[^2]: 度量空间大小和神经网络中的泛化性能

    Metric Space Magnitude and Generalisation in Neural Networks. (arXiv:2305.05611v1 [cs.LG])

    [http://arxiv.org/abs/2305.05611](http://arxiv.org/abs/2305.05611)

    本研究使用一种新的拓扑不变量——大小，来量化深度神经网络的学习过程，研究其内部表示并提出一个新方法来确定其泛化能力，实验证明此框架可作为泛化错误的指标。

    

    深度学习模型在许多应用中取得了重大成功，但它们的内部工作过程仍然是难以捉摸的。本文的目的是通过一种称为“大小”的新拓扑不变量的视角来量化深度神经网络的学习过程。大小是一种等距不变量；它的属性是研究的一个活跃领域，因为它编码了度量空间中许多已知的不变量。我们使用大小来研究神经网络的内部表示，并提出了一种确定它们泛化能力的新方法。此外，我们在理论上将大小维度和泛化错误连接起来，并实验性地证明，所提出的框架可以成为泛化错误的一个良好指标。

    Deep learning models have seen significant successes in numerous applications, but their inner workings remain elusive. The purpose of this work is to quantify the learning process of deep neural networks through the lens of a novel topological invariant called magnitude. Magnitude is an isometry invariant; its properties are an active area of research as it encodes many known invariants of a metric space. We use magnitude to study the internal representations of neural networks and propose a new method for determining their generalisation capabilities. Moreover, we theoretically connect magnitude dimension and the generalisation error, and demonstrate experimentally that the proposed framework can be a good indicator of the latter.
    
[^3]: 利用Birth-Death 过程和探索组件加速Langevin采样

    Accelerate Langevin Sampling with Birth-Death process and Exploration Component. (arXiv:2305.05529v1 [stat.CO])

    [http://arxiv.org/abs/2305.05529](http://arxiv.org/abs/2305.05529)

    该论文提出了一种新的采样方法，在探索新模式和传递有用信息的过程中利用了Birth-Death过程和探索组件，具有高效和指数渐近收敛等优点。

    

    在计算科学和工程中，采样已知概率分布是一项基本任务。针对多峰性，我们提出了一种新的采样方法，利用了Birth-Death过程和探索组件。该方法的主要思想是“三思而后行”。我们保留两组采样器，一组在较高温度下，一组在原始温度下。前者作为探索新模式和将有用信息传递给后者的先驱，后者在接收信息后对目标分布进行采样。我们推导了均场极限，并展示了探索过程如何决定采样效率。此外，在温和假设下，我们证明了指数渐近收敛。最后，我们对以前文献中的实验进行了测试，并将我们的方法与以前的方法进行了比较。

    Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both birth-death process and exploration component. The main idea of this method is \textit{look before you leap}. We keep two sets of samplers, one at warmer temperature and one at original temperature. The former one serves as pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration process determines sampling efficiency. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test on experiments from previous literature and compared our methodology to previous ones.
    
[^4]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^5]: 使用随机Lp范数失真探究图像分类器的腐败稳健性

    Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])

    [http://arxiv.org/abs/2305.05400](http://arxiv.org/abs/2305.05400)

    本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    

    稳健性是机器学习分类器实现安全和可靠的基本属性。在对图像分类模型的对抗稳健性和形式稳健性验证领域中，稳健性通常被定义为在Lp范数距离内对所有输入变化的稳定性。然而，对随机失真的稳健性通常通过在现实世界中观察到的变化来改进和评估，而很少考虑数学定义的Lp范数失真。本研究探讨了使用随机Lp范数失真来增强图像分类器的训练和测试数据。我们借鉴了对抗稳健性领域的方法来评估模型对不可感知随机失真的稳健性。我们实证和理论上研究了在不同Lp范数之间稳健性是否可转移，并得出结论，哪些Lp范数的失真应该用来训练和评估模型。我们发现训练数据增强可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
    
[^6]: 关于多标签学习中Macro-AUC的泛化理解探究

    Towards Understanding Generalization of Macro-AUC in Multi-label Learning. (arXiv:2305.05248v1 [cs.LG])

    [http://arxiv.org/abs/2305.05248](http://arxiv.org/abs/2305.05248)

    本研究探究了 multi-label 学习中常用的 Macro-AUC 的泛化性质，并发现数据集中标签不平衡对泛化界限有重要影响。未经变量处理的基于损失函数的算法可能由于对标签的不平衡更敏感而表现较差，这一结论在多个数据集上得到验证。

    

    在多标签学习中，Macro-AUC是类内AUC算术平均值，通常在实践中使用。然而，它的理论理解远远不足。为了解决这个问题，我们基于对应的代理损失函数表征各种学习算法的宏AUC的泛化属性。我们在理论上确定了影响泛化界限的数据集的关键因素：标签类别不平衡。我们对不平衡感知误差界限的结果表明，广泛使用的未经变量处理的基于损失函数的算法比提出的基于成对和重新加权的算法更敏感于标签类别的不平衡，这可能意味着它的性能较差。此外，各种数据集上的经验结果证实了我们的理论结果。就技术而言，我们提出了一种新的（更通用的）McDiarmid型集中不等式，这可能具有独立的兴趣。

    Macro-AUC is the arithmetic mean of the class-wise AUCs in multi-label learning and is commonly used in practice. However, its theoretical understanding is far lacking. Toward solving it, we characterize the generalization properties of various learning algorithms based on the corresponding surrogate losses w.r.t. Macro-AUC. We theoretically identify a critical factor of the dataset affecting the generalization bounds: \emph{the label-wise class imbalance}. Our results on the imbalance-aware error bounds show that the widely-used univariate loss-based algorithm is more sensitive to the label-wise class imbalance than the proposed pairwise and reweighted loss-based ones, which probably implies its worse performance. Moreover, empirical results on various datasets corroborate our theory findings. To establish it, technically, we propose a new (and more general) McDiarmid-type concentration inequality, which may be of independent interest.
    
[^7]: 使用高阶淬火随机漂移解决线性反问题

    Solving Linear Inverse Problems using Higher-Order Annealed Langevin Diffusion. (arXiv:2305.05014v1 [stat.ML])

    [http://arxiv.org/abs/2305.05014](http://arxiv.org/abs/2305.05014)

    本论文提出了一种高阶 Langevin 动力学的算法，可更高效地采样未知变量后验分布，同时加入淬火过程，能应用于离散未知变量情况，数值实验表明在多个任务中相对竞争方法具有更高性能。

    

    我们提出了一种基于高阶 Langevin 漂移的线性反问题解决方案。更具体地，我们提出了预处理的二阶和三阶 Langevin 动力学，这些动力学明显地从我们感兴趣的未知变量的后验分布中采样，同时比其一阶对应物和两种动力学的非预处理版本更具计算效率。此外，我们证明了两种预处理动力学是良定义的，并且具有与非预处理情况相同的唯一不变分布。我们还加入了一个淬火过程，这具有双重优点，一方面进一步加速了算法的收敛速度，另一方面，允许我们适应未知变量为离散的情况。在两个不同的任务（MIMO 符号检测和通道估计）的数值实验中，展示了我们方法的通用性，并说明了相对于竞争方法（包括基于学习的方法）所实现的高性能。

    We propose a solution for linear inverse problems based on higher-order Langevin diffusion. More precisely, we propose pre-conditioned second-order and third-order Langevin dynamics that provably sample from the posterior distribution of our unknown variables of interest while being computationally more efficient than their first-order counterpart and the non-conditioned versions of both dynamics. Moreover, we prove that both pre-conditioned dynamics are well-defined and have the same unique invariant distributions as the non-conditioned cases. We also incorporate an annealing procedure that has the double benefit of further accelerating the convergence of the algorithm and allowing us to accommodate the case where the unknown variables are discrete. Numerical experiments in two different tasks (MIMO symbol detection and channel estimation) showcase the generality of our method and illustrate the high performance achieved relative to competing approaches (including learning-based ones)
    
[^8]: FedHB: 层次贝叶斯联邦学习

    FedHB: Hierarchical Bayesian Federated Learning. (arXiv:2305.04979v1 [cs.LG])

    [http://arxiv.org/abs/2305.04979](http://arxiv.org/abs/2305.04979)

    该论文提出了一种层次贝叶斯联邦学习方法，通过块坐标下降分布式算法实现对客户端私有数据不透露的学习，在收敛速度上与正则化相同。

    

    本文提出了一种新的层次贝叶斯联邦学习方法，通过层次贝叶斯建模合理地描述了客户端本地数据的生成过程：构成客户端本地模型的随机变量，由更高水平的全局变量进行控制。有趣的是，我们贝叶斯模型中的变分推断导致了一个优化问题，其块坐标下降求解成为一个可分客户端的分布式算法，这使得客户端完全不需要透露自己的私有数据，因此与联邦学习完全兼容。我们还强调，我们的块坐标算法具有特定形式，包括Fed-Avg和Fed-Prox在内的众所周知的FL算法都可以作为其特例进行子归。除了引入新的建模和导出之外，我们还提供了收敛性分析，表明我们的块坐标FL算法以$O(1/\sqrt{t})$的速度收敛到目标的（本地）最优解，这与正则化具有相同的速率。

    We propose a novel hierarchical Bayesian approach to Federated Learning (FL), where our model reasonably describes the generative process of clients' local data via hierarchical Bayesian modeling: constituting random variables of local models for clients that are governed by a higher-level global variate. Interestingly, the variational inference in our Bayesian model leads to an optimisation problem whose block-coordinate descent solution becomes a distributed algorithm that is separable over clients and allows them not to reveal their own private data at all, thus fully compatible with FL. We also highlight that our block-coordinate algorithm has particular forms that subsume the well-known FL algorithms including Fed-Avg and Fed-Prox as special cases. Beyond introducing novel modeling and derivations, we also offer convergence analysis showing that our block-coordinate FL algorithm converges to an (local) optimum of the objective at the rate of $O(1/\sqrt{t})$, the same rate as regul
    
[^9]: 关于非线性和参数PDE的核/GP方法误差分析

    Error Analysis of Kernel/GP Methods for Nonlinear and Parametric PDEs. (arXiv:2305.04962v1 [math.NA])

    [http://arxiv.org/abs/2305.04962](http://arxiv.org/abs/2305.04962)

    本文介绍了使用核/GP方法估计非线性和参数PDE通过Sobolev空间误差，并指出当PDE的解空间充分光滑时，具有维度友好的收敛速率。

    

    我们引入了使用高斯过程和基于核的方法解决非线性和可能是参数PDE的先验Sobolev空间误差估计。主要的假设是：（1）核重现空间在足够正则的Sobolev空间中的连续嵌入；（2）微分算子和PDE的解映射在相应的Sobolev空间之间稳定。这些误差估计展示了在PDE的解空间充分光滑的情况下具有维度友好的收敛速率。

    We introduce a priori Sobolev-space error estimates for the solution of nonlinear, and possibly parametric, PDEs using Gaussian process and kernel based methods. The primary assumptions are: (1) a continuous embedding of the reproducing kernel Hilbert space of the kernel into a Sobolev space of sufficient regularity; and (2) the stability of the differential operator and the solution map of the PDE between corresponding Sobolev spaces. The proof is articulated around Sobolev norm error estimates for kernel interpolants and relies on the minimizing norm property of the solution. The error estimates demonstrate dimension-benign convergence rates if the solution space of the PDE is smooth enough. We illustrate these points with applications to high-dimensional nonlinear elliptic PDEs and parametric PDEs. Although some recent machine learning methods have been presented as breaking the curse of dimensionality in solving high-dimensional PDEs, our analysis suggests a more nuanced picture: t
    
[^10]: 高斯过程去卷积问题的贝叶斯非参数方法

    Gaussian process deconvolution. (arXiv:2305.04871v1 [stat.ML])

    [http://arxiv.org/abs/2305.04871](http://arxiv.org/abs/2305.04871)

    本文提出了一种基于高斯过程的贝叶斯非参数方法，可以解决连续时间信号的去卷积问题，适用于观测值中可能存在缺失数据且信号滤波器未知的情况。

    

    本文考虑去卷积问题，即从卷积处理的观测值 $\mathbf{y}$ 中恢复潜在信号 $x(\cdot)$，其中观测值 $\mathbf{y}$ 可能对应于 $y$ 的一部分缺失，滤波器 $h$ 可能未知且噪声 $\eta$ 可加性。当 $x$ 是连续时间信号时，我们采用高斯过程（GP）先验分布来解决这一问题，提出了一种闭合的贝叶斯非参数去卷积策略。我们首先分析了直接模型，以建立其良好定义的条件。然后，我们转向逆问题，研究了：（i）一些必要条件，使得贝叶斯去卷积计算有可能成立，以及（ii）在哪种程度上可以从数据中学习到滤波器 $h$，以及在盲去卷积情况下可以近似滤波器 $h$ 的程度。所提出的方法被称为高斯过程去卷积（GPDC），并与其他去卷积方法进行了比较。

    Let us consider the deconvolution problem, that is, to recover a latent source $x(\cdot)$ from the observations $\y = [y_1,\ldots,y_N]$ of a convolution process $y = x\star h + \eta$, where $\eta$ is an additive noise, the observations in $\y$ might have missing parts with respect to $y$, and the filter $h$ could be unknown. We propose a novel strategy to address this task when $x$ is a continuous-time signal: we adopt a Gaussian process (GP) prior on the source $x$, which allows for closed-form Bayesian nonparametric deconvolution. We first analyse the direct model to establish the conditions under which the model is well defined. Then, we turn to the inverse problem, where we study i) some necessary conditions under which Bayesian deconvolution is feasible, and ii) to which extent the filter $h$ can be learnt from data or approximated for the blind deconvolution case. The proposed approach, termed Gaussian process deconvolution (GPDC) is compared to other deconvolution methods concep
    
[^11]: 二元事件的校准评估和大胆再校准

    Calibration Assessment and Boldness-Recalibration for Binary Events. (arXiv:2305.03780v1 [stat.ME])

    [http://arxiv.org/abs/2305.03780](http://arxiv.org/abs/2305.03780)

    本研究提出了一种假设检验和贝叶斯模型选择方法来评估校准，并提供一种大胆再校准策略，使实践者能够在满足所需的校准水平的情况下负责任地增强预测。

    

    概率预测对于医学、经济、图像分类、体育分析、娱乐等许多领域中的决策制定至关重要。理想情况下，概率预测应该 (i) 校准良好 (ii) 准确 (iii) 大胆，即远离事件的基础频率。满足这三个条件的预测对于决策制定是有信息量的。然而，校准和大胆之间存在基本的紧张关系，因为当预测过于谨慎时(即非大胆)校准度量可以很高。本文的目的是开发一种假设检验和贝叶斯模型选择方法来评估校准，并提供一种大胆再校准策略，使实践者能够在满足所需的校准水平的情况下负责任地增强预测。具体而言，我们允许用户预先指定他们所需的后验校准概率，然后在此约束下最大化增强预测。我们通过模拟研究和实际数据应用验证了我们方法的性能。

    Probability predictions are essential to inform decision making in medicine, economics, image classification, sports analytics, entertainment, and many other fields. Ideally, probability predictions are (i) well calibrated, (ii) accurate, and (iii) bold, i.e., far from the base rate of the event. Predictions that satisfy these three criteria are informative for decision making. However, there is a fundamental tension between calibration and boldness, since calibration metrics can be high when predictions are overly cautious, i.e., non-bold. The purpose of this work is to develop a hypothesis test and Bayesian model selection approach to assess calibration, and a strategy for boldness-recalibration that enables practitioners to responsibly embolden predictions subject to their required level of calibration. Specifically, we allow the user to pre-specify their desired posterior probability of calibration, then maximally embolden predictions subject to this constraint. We verify the perfo
    
[^12]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^13]: Langevin型Monte Carlo算法的非渐进分析

    Non-asymptotic analysis of Langevin-type Monte Carlo algorithms. (arXiv:2303.12407v1 [math.ST])

    [http://arxiv.org/abs/2303.12407](http://arxiv.org/abs/2303.12407)

    本文提出了一种新的Langevin型算法并应用于吉布斯分布。通过提出的2-Wasserstein距离上限，我们发现势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性可以保证算法具有接近零的误差。新的Langevin型算法还可以应用于无凸性或连续可微性的势函数。

    

    本文研究了Langevin型算法应用于吉布斯分布的情况，其中势函数是耗散的，且其弱梯度具有有限的连续性模量。我们的主要结果是2-Wasserstein距离上限的非渐进性，它衡量了吉布斯分布与基于Liptser-Shiryaev理论和函数不等式的Langevin型算法的一般分布之间的距离。我们应用这个上限来展示势函数的耗散性以及梯度 $\alpha>1/3$ 下的 $\alpha$-H\"{o}lder连续性是充分的，可以通过适当控制参数来获得Langevin Monte Carlo算法的收敛性。我们还针对无凸性或连续可微性的势函数提出了球形平滑技术的Langevin型算法。

    We study the Langevin-type algorithms for Gibbs distributions such that the potentials are dissipative and their weak gradients have the finite moduli of continuity. Our main result is a non-asymptotic upper bound of the 2-Wasserstein distance between the Gibbs distribution and the law of general Langevin-type algorithms based on the Liptser--Shiryaev theory and functional inequalities. We apply this bound to show that the dissipativity of the potential and the $\alpha$-H\"{o}lder continuity of the gradient with $\alpha>1/3$ are sufficient for the convergence of the Langevin Monte Carlo algorithm with appropriate control of the parameters. We also propose Langevin-type algorithms with spherical smoothing for potentials without convexity or continuous differentiability.
    
[^14]: 可证收敛的即插即用拟牛顿方法

    Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.07271](http://arxiv.org/abs/2303.07271)

    本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。

    

    即插即用（PnP）方法是一类高效的迭代算法，旨在利用经典优化算法（如ISTA或ADMM），将数据保真度项和深度去噪器相结合。现有的可证明的PnP方法对去噪器或保真度函数施加了严格的限制，如非扩张性或严格凸性。本文提出了一种可证明的PnP方法，该方法基于近端去噪器施加相对较轻的条件，并引入了拟牛顿步骤以大大加速收敛。通过将深度去噪器特别参数化为梯度步骤，我们进一步将拟牛顿PnP算法的固定点表征为可能非凸函数的临界点。

    Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
    
[^15]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^16]: 条件平均处理效果的近因因果学习

    Proximal Causal Learning of Conditional Average Treatment Effects. (arXiv:2301.10913v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.10913](http://arxiv.org/abs/2301.10913)

    提出了P-学习器，一种用于学习处理效果异质性的定制两阶段损失函数，能够依靠代理变量进行因果推断，具有较高的灵活性和效率。

    

    高效灵活地估计处理效果的异质性是从医学到市场等各种领域中的一个重要任务，在此过程中有许多有前途的条件平均处理效果估计器。然而，这些估计器通常依赖于测量到的协变量足以证明条件交换性的假设。我们提出了P-学习器，受到R-学习器和DR-学习器的启发，这是一种专门为在观察到的协变量给定可交换性是不合理的情况下，学习异质性处理效果的定制两阶段损失函数，并希望依靠代理变量进行因果推断。我们提出的估计器可以通过现成的损失最小化机器学习方法实现，在核回归的情况下，只要妨碍组件得到合理的估计，就能满足估计误差的oracle限制。

    Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing, and there are a considerable number of promising conditional average treatment effect estimators currently available. These, however, typically rely on the assumption that the measured covariates are enough to justify conditional exchangeability. We propose the P-learner, motivated by the R- and DR-learner, a tailored two-stage loss function for learning heterogeneous treatment effects in settings where exchangeability given observed covariates is an implausible assumption, and we wish to rely on proxy variables for causal inference. Our proposed estimator can be implemented by off-the-shelf loss-minimizing machine learning methods, which in the case of kernel regression satisfies an oracle bound on the estimated error as long as the nuisance components are estimated reasonably well.
    
[^17]: 基于Wi-Fi信道数据的人与人互动识别的前瞻性方法——使用具有GUI应用程序实现的注意力双向门控循环神经网络(arXiv:2202.08146v4 [cs.LG] UPDATED)

    A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08146](http://arxiv.org/abs/2202.08146)

    该论文提出了一种基于Wi-Fi信道数据的人与人互动识别的新方法，使用注意力双向门控循环神经网络实现高精度的实时处理，准确率达到98.22%，并提供了一个GUI应用程序方便实时应用。

    

    由于最近的技术进步、人工智能算法、智能城市和社会经济变革的需要，人类活动识别(HAR)研究已经获得了重要的动力。然而，现有的基于计算机视觉和传感器的HAR解决方案存在隐私问题、存储和功率消耗问题以及佩戴传感器的不适感，这促使研究人员观察到HAR研究的范式转变。作为回应，基于WiFi的HAR因其更粗粒度的信道状态信息的可用性而越来越受欢迎。然而，现有的基于WiFi的HAR方法仅限于对在相等时间内执行的独立和非并发人类活动进行分类。与最近的研究不同的是，我们的研究利用了多输入多输出通信链路，其中发射器是WiFi路由器，接收器是配备了Intel 5300 NIC的智能手机。我们提出了一种基于注意力双向门控循环神经网络(ABiGRNN)的WiFi信道数据的人与人互动(HHI)识别的新方法，该方法允许高精度的实时处理。我们使用HHI活动数据集评估我们的方法，实现了98.22%的准确率，优于现有的最先进方法。此外，我们提供了一个图形用户界面(GUI)应用程序，可以在实时场景中轻松实现我们的方法来识别HHI。

    Human Activity Recognition (HAR) research has gained significant momentum due to recent technological advancements, artificial intelligence algorithms, the need for smart cities, and socioeconomic transformation. However, existing computer vision and sensor-based HAR solutions have limitations such as privacy issues, memory and power consumption, and discomfort in wearing sensors for which researchers are observing a paradigm shift in HAR research. In response, WiFi-based HAR is gaining popularity due to the availability of more coarse-grained Channel State Information. However, existing WiFi-based HAR approaches are limited to classifying independent and non-concurrent human activities performed within equal time duration. Recent research commonly utilizes a Single Input Multiple Output communication link with a WiFi signal of 5 GHz channel frequency, using two WiFi routers or two Intel 5300 NICs as transmitter-receiver. Our study, on the other hand, utilizes a Multiple Input Multiple
    
[^18]: 联邦学习中的比例公平性

    Proportional Fairness in Federated Learning. (arXiv:2202.01666v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01666](http://arxiv.org/abs/2202.01666)

    本文提出了用于保证联邦学习中公平性的比例公平性 (PF) 概念，并提出了一种新颖易于实现的算法 PropFair，能够在所有客户端平均性能和最差 10% 客户端平均性能之间达到良好平衡。

    

    随着联邦学习系统在现实世界中越来越广泛地部署，保证公平性变得至关重要但也面临着挑战，即需要为众多不同的客户端提供合理满意的表现。在本文中，我们引入并研究了联邦学习中一种新的公平性概念，即比例公平性 (PF)，它基于每个客户端性能的相对变化。通过与交易博弈的联系，我们提出了 PropFair，一种新颖且易于实现的算法，用于在联邦学习中寻找比例公平解，并研究了其收敛性质。通过对视觉和语言数据集的广泛实验，我们证明 PropFair 能够大致找到 PF 解，并在所有客户端的平均性能和最差 10% 客户端的平均性能之间实现良好的平衡。我们的代码可在 \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL} 找到。

    With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients. Our code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.
    
[^19]: 利用集成机器学习从常规血液检查中开发无风险的COVID-19筛查算法

    Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning. (arXiv:2108.05660v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.05660](http://arxiv.org/abs/2108.05660)

    本研究提出了一种风险低且高精度的堆叠集成机器学习模型，可以从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。

    

    反转录聚合酶链反应（RT-PCR）是辨别COVID-19感染的银弹诊断检测。快速抗原检测是一种筛查检测，可在15分钟内识别COVID-19阳性患者，但其灵敏度低于PCR检测。本研究通过利用COVID-19患者免疫和血液学资料的参数偏差，提出了一种风险低且高精度的堆叠集成机器学习模型，从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。

    The Reverse Transcription Polymerase Chain Reaction (RTPCR)} test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected and either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological and hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall and
    
[^20]: 高效的基于模型的多智能体均场强化学习算法

    Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.04050](http://arxiv.org/abs/2107.04050)

    本文针对均场控制问题提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在未知系统动力学的情况下，该算法可平衡探索和利用，并实现了可证明的问题求解，具有较好的表现。

    

    多智能体系统的学习充满挑战，包括智能体相互作用所引入的非稳态和状态和动作空间的组合性质等多个因素。本文关注的是均场控制问题，其假设存在无限数量的相同智能体，旨在共同最大化收益。针对未知系统动力学的情况，本文提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在策略学习期间平衡探索和利用，并实现了这一问题的可证明求解。该算法在多个基准问题上的表现优于多种最先进的方法。

    Learning in multi-agent systems is highly challenging due to several factors including the non-stationarity introduced by agents' interactions and the combinatorial nature of their state and action spaces. In particular, we consider the Mean-Field Control (MFC) problem which assumes an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. In many cases, solutions of an MFC problem are good approximations for large systems, hence, efficient learning for MFC is valuable for the analogous discrete agent setting with many agents. Specifically, we focus on the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm, $M^3-UCRL$, that runs in episodes, balances between exploration and exploitation during policy learning, and provably solves this problem. Our main theoretical contributions are the first
    
[^21]: 一种用于比较潜变量模型的核Stein检验

    A Kernel Stein Test for Comparing Latent Variable Models. (arXiv:1907.00586v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1907.00586](http://arxiv.org/abs/1907.00586)

    本研究提出了一种核Stein检验方法，可用于比较具有潜变量的模型，相比于当前方法，性能更好。

    

    我们提出了一种基于核的非参数拟合优度检验，旨在比较两个模型，这两个模型都可能具有未观测的潜变量，且观测变量的边缘分布不可观测。所提出的检验将最近提出的核Stein距离(KSD)检验(Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018)推广到潜变量模型的情况下，是一个比之前处理的全观测模型更加通用的类别。新的检验通过适当的校准阈值，得到良好控制的一类错误。对于某些具有低维潜在结构和高维观测的模型，在我们的测试中，显著优于基于模型样本和不利用潜在结构的相对最大平均距离检测。

    We propose a kernel-based nonparametric test of relative goodness of fit, where the goal is to compare two models, both of which may have unobserved latent variables, such that the marginal distribution of the observed variables is intractable. The proposed test generalizes the recently proposed kernel Stein discrepancy (KSD) tests (Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018) to the case of latent variable models, a much more general class than the fully observed models treated previously. The new test, with a properly calibrated threshold, has a well-controlled type-I error. In the case of certain models with low-dimensional latent structure and high-dimensional observations, our test significantly outperforms the relative Maximum Mean Discrepancy test, which is based on samples from the models and does not exploit the latent structure.
    

