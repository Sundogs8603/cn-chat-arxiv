# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Parallelized Acquisition for Active Learning using Monte Carlo Sampling.](http://arxiv.org/abs/2305.19267) | 本文介绍了一种基于高斯过程回归和主动采样的后验模拟器的方法，通过在均值预测上进行分级采样，在保持准确性的前提下实现了蒙特卡洛采样的并行处理，有效降低了成本。 |
| [^2] | [Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders.](http://arxiv.org/abs/2305.19259) | 本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。 |
| [^3] | [Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks.](http://arxiv.org/abs/2305.19243) | 通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。 |
| [^4] | [dotears: Scalable, consistent DAG estimation using observational and interventional data.](http://arxiv.org/abs/2305.19215) | dotears是一个可扩展的DAG结构学习框架，使用观测和干预数据来推断单个因果结构。它直接估计外生误差结构，避免了循环估计问题。 |
| [^5] | [Fast global convergence of gradient descent for low-rank matrix approximation.](http://arxiv.org/abs/2305.19206) | 本文证明了对于低秩矩阵逼近问题，使用小随机初始化的梯度下降方法具有非常快的全局收敛性，尤其是当最大特征值相同时。退化自由算法比Riemannian梯度下降方法更有效，在运行时间上可以减少29％。 |
| [^6] | [Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models.](http://arxiv.org/abs/2305.19187) | 本研究提出应对大型语言模型可信度问题的方法，研究黑盒模型中置信度与不确定性量化，并将其应用于选择性自然语言生成。 |
| [^7] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^8] | [Cooperative Thresholded Lasso for Sparse Linear Bandit.](http://arxiv.org/abs/2305.19161) | 本论文提出了一种基于合作阈值 Lasso 的方法，解决多智能体稀疏上下文线性 Bandit 问题，该方法可以降维与信息共享，以降低通信成本，同时保证每个智能体的最小累计遗憾。 |
| [^9] | [ELSA: Efficient Label Shift Adaptation through the Lens of Semiparametric Models.](http://arxiv.org/abs/2305.19123) | 本文基于影响函数几何提出了一个矩匹配框架来适应标签偏移，提出了一个名为“高效标签偏移适应”的适应方法，具有一致性和渐进正态性，可在不需要后预测校准的情况下实现最先进的估计性能。 |
| [^10] | [Embedding Inequalities for Barron-type Spaces.](http://arxiv.org/abs/2305.19082) | 本文测量了Barron空间和谱Barron空间之间的关系，并提供了嵌入不等式。 |
| [^11] | [Rank-adaptive spectral pruning of convolutional layers during training.](http://arxiv.org/abs/2305.19059) | 本论文提出了一种新的低参数训练方法，该方法将卷积分解为张量Tucker格式，并在训练过程中自适应地修剪卷积核的Tucker秩，可以有效地降低训练成本。 |
| [^12] | [A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction.](http://arxiv.org/abs/2305.19043) | 本论文提出了一种基于热扩散视角的地形保持降维方法，建立了热扩散与流形距离之间的理论联系。该方法在保持流形距离和簇结构方面优于最先进的方法。 |
| [^13] | [Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff.](http://arxiv.org/abs/2305.19008) | 本研究揭示了深度学习神经网路学习输入低维度表示和最小化特征映射中的复杂性/不规则性之间的权衡，控制了规律性，并利用理论工具证明了瓶颈结构的存在。 |
| [^14] | [Sharp high-probability sample complexities for policy evaluation with linear function approximation.](http://arxiv.org/abs/2305.19001) | 本文研究线性函数逼近下的策略评估问题，提出了两个广泛使用的算法所需的样本复杂度，具有高概率收敛保证且与容差水平的关联性最佳。 |
| [^15] | [Generalized Autoregressive Score Trees and Forests.](http://arxiv.org/abs/2305.18991) | 该论文提出了一种方法，通过使用决策树和随机森林来定位广义自回归得分模型的参数，改善其预测，并揭示了股票回报波动率和密度预测中的杠杆效应和方差风险溢价效应，以及股票-债券依赖性中的流向质量效应和高频交易持续时间中的成交量-波动性效应。 |
| [^16] | [Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers.](http://arxiv.org/abs/2305.18974) | 该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。 |
| [^17] | [Quantum Convolutional Neural Networks for Multi-Channel Supervised Learning.](http://arxiv.org/abs/2305.18961) | 本文介绍了多通道监督学习的量子卷积神经网络，通过硬件适应性的量子电路ansatzes用作卷积核，能够有效学习通道间信息，优于现有的QCNNs。 |
| [^18] | [Clip21: Error Feedback for Gradient Clipping.](http://arxiv.org/abs/2305.18929) | 本论文报道了Clip21，这是第一个适用于梯度裁剪的分布式训练的有效误差反馈方法，能够解决严重的收敛问题，证明了其收敛速度与分布式方法相同。 |
| [^19] | [Learning Perturbations to Explain Time Series Predictions.](http://arxiv.org/abs/2305.18840) | 本文提出了一种基于学习扰动的方法来解释预测，相对于传统的基于扰动的显著性方法，该方法可以显着提高对于多元时间序列数据的解释质量。 |
| [^20] | [PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time Series.](http://arxiv.org/abs/2305.18811) | PyPOTS是一个Python工具箱，用于对部分观测的时间序列数据进行数据挖掘和分析，包括插值、分类、聚类和预测等四个任务，算法种类繁多，适用于学术研究和工业应用。 |
| [^21] | [Prediction Error-based Classification for Class-Incremental Learning.](http://arxiv.org/abs/2305.18806) | 本论文提出了一种新的增量学习分类方法——基于预测误差的分类方法（PEC）。对PEC的评估表明，在各种基准测试中，PEC可以与最先进的增量学习方法相竞争，并具有许多实际优势，例如样本效率高、易于调整。 |
| [^22] | [Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits.](http://arxiv.org/abs/2305.18784) | 本研究研究了一个新的合作多智能体老虎机设置，并发展了去中心化算法以减少代理之间的集体遗憾，在数学分析中证明了该算法实现了近乎最优性能。 |
| [^23] | [It begins with a boundary: A geometric view on probabilistically robust learning.](http://arxiv.org/abs/2305.18779) | 本文探讨了深度神经网络对于对抗生成的示例缺乏鲁棒性的问题，并提出了一种从几何角度出发的新颖视角，介绍一族概率非局部周长函数来优化概率鲁棒学习（PRL）的原始表述，以提高其鲁棒性。 |
| [^24] | [SFCNeXt: a simple fully convolutional network for effective brain age estimation with small sample size.](http://arxiv.org/abs/2305.18771) | SFCNeXt是一个简单的全卷积网络，用于小型队列中进行脑龄估计，通过SPEC和HRL算法，以轻量化的方式充分探索每个批次的MRI、年龄和排名特征，避免对大量MRI的依赖和复杂模型结构。 |
| [^25] | [When Does Optimizing a Proper Loss Yield Calibration?.](http://arxiv.org/abs/2305.18764) | 研究优化适当的损失函数是否能在受限的预测器族中得到校准的模型，使用局部最优条件取代全局最优性条件并在此基础上进行了严格的证明。 |
| [^26] | [Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization.](http://arxiv.org/abs/2305.18730) | 本文提出了两种基于方差约简的优化算法，以实现对多块双层优化问题的高效求解，同时匹配单块标准 BO 问题的最优复杂度、实现并行化加速，以及避免计算高维度的 Hessian 矩阵的逆估计。 |
| [^27] | [Plug-in Performative Optimization.](http://arxiv.org/abs/2305.18728) | 研究了一种可能“规范不正确”模型的通用协议，“插件式表现优化”。 |
| [^28] | [Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs.](http://arxiv.org/abs/2305.18702) | 本研究提出了一种新的深度生成模型来调整训练集中的随机样本，以使PDE解的残余在最小化时能保持平滑的轮廓，并通过引入对抗性损失项优化PINN模型，从而使神经网络学习稳定的解。同时本文还展示了该方法可以扩展到纳入最优传输约束，从而形成了将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。 |
| [^29] | [Predicting Rare Events by Shrinking Towards Proportional Odds.](http://arxiv.org/abs/2305.18700) | 本文提出了一种序数回归比例减少模型的松弛方法PRESTO，通过对相邻权向量中相同特征之间的差异施加L1惩罚，利用先前更丰富的数据来改善罕见事件的概率估计。 |
| [^30] | [Efficient median of means estimator.](http://arxiv.org/abs/2305.18681) | 本文介绍了一种修改版的均值中位数估计器，可以在对底层分布的最小假设下实现亚高斯偏差界限和近乎最优常数。 |
| [^31] | [Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification.](http://arxiv.org/abs/2305.18671) | 本文提出了扰动辅助样本合成（PASS）方法，可从复杂数据中绘制可靠结论，并通过估计数据生成分布和蒙特卡罗实验证明任何统计数据的估计分布。进一步推出扰动辅助推理（PAI）框架，可以提供有效性的统计保证。 |
| [^32] | [Parity Calibration.](http://arxiv.org/abs/2305.18655) | 本文介绍一种新的校准预测目标——parity校准，其考虑时间序列中未来观测值的增加或减少。我们使用在线二进制校准方法实现了parity校准，并在流行病学、天气预报和核聚变控制等领域中表明该方法的有效性。 |
| [^33] | [Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees.](http://arxiv.org/abs/2305.18627) | Global-QSGD是一种新颖的全局缩放量化机制，可以提高分布式学习的效率，并且不需要昂贵的误差反馈，并提供了高达$O(\ sqrt{n})$的额外压缩比。 |
| [^34] | [Quick Adaptive Ternary Segmentation: An Efficient Decoding Procedure For Hidden Markov Models.](http://arxiv.org/abs/2305.18578) | 提出了一种名为QATS的新方法，用于高效解码隐藏马尔可夫模型序列。它的计算复杂性为多对数和立方，特别适用于具有相对较少状态的大型HMM。 |
| [^35] | [Towards Constituting Mathematical Structures for Learning to Optimize.](http://arxiv.org/abs/2305.18577) | 本文提出了一种结构受到数学启发的L2O模型，其具有广泛的适用性和良好的推广性能，并基于成功的更新规则通常满足的基本数学条件进行了推导。 |
| [^36] | [Robust Lipschitz Bandits to Adversarial Corruptions.](http://arxiv.org/abs/2305.18543) | 本文提出的强健Lipschitz赌徒算法，能够在对抗性攻击的情况下实现次线性遗憾，并在强敌手情况下最优。 |
| [^37] | [On the Variance, Admissibility, and Stability of Empirical Risk Minimization.](http://arxiv.org/abs/2305.18508) | 本文指出，对于使用平方损失函数的经验风险最小化(ERM)，其次优性必须归因于大的偏差而非方差，并且在ERM的平方误差的偏差-方差分解中，方差项必然具有极小的失误率。作者还提供了Chatterjee的不可允许性定理的简单证明，并表示他们的估计表明ERM的稳定性。 |
| [^38] | [Generalization Ability of Wide Residual Networks.](http://arxiv.org/abs/2305.18506) | 本文研究了在$\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力，表明当宽度$m\rightarrow\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)，并且早停策略的宽残差网络可以达到极小化速率，但在训练过度拟合数据时无法很好地推广。 |
| [^39] | [How to Query Human Feedback Efficiently in RL?.](http://arxiv.org/abs/2305.18505) | 该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。 |
| [^40] | [Escaping mediocrity: how two-layer networks learn hard single-index models with SGD.](http://arxiv.org/abs/2305.18502) | 研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。 |
| [^41] | [Generalized equivalences between subsampling and ridge regularization.](http://arxiv.org/abs/2305.18496) | 此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。 |
| [^42] | [A Bayesian sparse factor model with adaptive posterior concentration.](http://arxiv.org/abs/2305.18488) | 本文提出了一种自适应后验集中的贝叶斯稀疏因子模型，可以推断因子维数和加载矩阵的稀疏结构，同时保持计算可行性，并获得了优越的性能表现。 |
| [^43] | [Neural Fourier Transform: A General Approach to Equivariant Representation Learning.](http://arxiv.org/abs/2305.18484) | 神经傅里叶变换是一种通用的等变表示学习方法，它可以在不需要显式知识的情况下学习组的潜在线性作用，实现对数据隐藏结构的提取。 |
| [^44] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^45] | [On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences.](http://arxiv.org/abs/2305.18423) | 本文研究了添加噪声的多层Sigmoid循环神经网络在学习序列分类问题上的样本复杂度问题，发现带噪声情况下样本复杂度可以用$\log(T/\sigma)$来界定，不存在噪声时下界为$wT$，两者存在指数级别的差距。 |
| [^46] | [Sample Complexity of Variance-reduced Distributionally Robust Q-learning.](http://arxiv.org/abs/2305.18420) | 本文提出了两种新颖的无模型算法，为动态决策面对分布变化问题提供了鲁棒的解决方案，并通过将Q-learning与方差减少技术相结合，实现了样本复杂度的有效控制。 |
| [^47] | [Geometric Algebra Transformers.](http://arxiv.org/abs/2305.18415) | 本文介绍了一种通用架构几何代数变换器（GATr），用于解决几何数据问题。GATr使用投影几何代数表示输入输出和状态，具有可缩放性、表达性、多功能性。在n体建模和机器人规划的实验中，GATr相对于非几何基线表现出强大的改进。 |
| [^48] | [Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms.](http://arxiv.org/abs/2305.18409) | 本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。 |
| [^49] | [Conformal Prediction with Large Language Models for Multi-Choice Question Answering.](http://arxiv.org/abs/2305.18404) | 本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。 |
| [^50] | [On the impact of activation and normalization in obtaining isometric embeddings at initialization.](http://arxiv.org/abs/2305.18399) | 本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。 |
| [^51] | [The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation.](http://arxiv.org/abs/2305.18388) | 本文研究了强化学习中的时间差分策略评估问题，分析了量化时间差分学习算法在任务中的应用。结果表明，即使从实践者没有超过平均回报之外的回报分布的兴趣之处，在表格设置中，QTD也可以提供比传统TD学习等方法更好的性能。 |
| [^52] | [A Three-regime Model of Network Pruning.](http://arxiv.org/abs/2305.18383) | 该论文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响，通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型，揭示了修剪的优化过程以及对神经网络损失景观的变化规律。 |
| [^53] | [Constrained Optimization via Exact Augmented Lagrangian and Randomized Iterative Sketching.](http://arxiv.org/abs/2305.18379) | 本文提出了一种自适应的不精确牛顿法来求解等式约束的非线性、非凸优化问题，通过随机迭代草图求解增广拉格朗日牛顿系统，并通过在精确增广拉格朗日优势函数上执行线搜索来选择合适的步长。该方法具有高效、鲁棒性好的特点。 |
| [^54] | [Disentanglement via Latent Quantization.](http://arxiv.org/abs/2305.18378) | 本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。 |
| [^55] | [Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling.](http://arxiv.org/abs/2305.18375) | 本文探讨了如何使用学习跳跃方法来生成建模各种类型的数据，特别是对于计数和非负连续数据等高稀疏度、倾斜度、重尾度或过度分散度的数据，使用学习跳跃相比于学习去噪有更好的效果。 |
| [^56] | [Structured model selection via $\ell_1-\ell_2$ optimization.](http://arxiv.org/abs/2305.17467) | 通过稀疏最小二乘拟合一大组候选函数，使用 $\ell_1-\ell_2$ 稀疏优化方法进行结构模型选择，实现从不充分且嘈杂的时空数据中识别结构化动态系统；该方法在合成数据集上得到了验证，并证明具有理论保证和高效性。 |
| [^57] | [No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions.](http://arxiv.org/abs/2305.17380) | 本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。 |
| [^58] | [Most Neural Networks Are Almost Learnable.](http://arxiv.org/abs/2305.16508) | 本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。 |
| [^59] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | 介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。 |
| [^60] | [The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning.](http://arxiv.org/abs/2305.15703) | 通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。 |
| [^61] | [Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent.](http://arxiv.org/abs/2305.14076) | 本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。 |
| [^62] | [Few-Shot Continual Learning for Conditional Generative Adversarial Networks.](http://arxiv.org/abs/2305.11400) | 本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。 |
| [^63] | [New metrics and search algorithms for weighted causal DAGs.](http://arxiv.org/abs/2305.04445) | 本研究提供了针对加权因果 DAGs的新度量和搜索算法，发现了用于自适应干预的因果图，提供了一个新的基准来捕捉搜索算法的最坏干预成本，并提供自适应搜索算法实现对数逼近。 |
| [^64] | [Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation.](http://arxiv.org/abs/2304.07048) | 本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。 |
| [^65] | [Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling.](http://arxiv.org/abs/2304.05365) | 本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。 |
| [^66] | [Universal Approximation Property of Hamiltonian Deep Neural Networks.](http://arxiv.org/abs/2303.12147) | 本文研究了离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络的通用逼近能力，证明了其中的一部分流可以逐渐逼近紧致域上的任何连续函数，为实际使用提供了理论基础。 |
| [^67] | [Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions.](http://arxiv.org/abs/2302.10886) | 本文深入研究和描述神经网络实现的函数的Lipschitz行为，在多种设置下进行实证研究，并揭示了神经网络函数Lipschitz连续性的基本和有趣的特性，其中最引人注目的是在Lipschitz常数的上限和下限中识别出了明显的双下降趋势。 |
| [^68] | [The out-of-sample $R^2$: estimation and inference.](http://arxiv.org/abs/2302.05131) | 本文提出了用于比较两个预测模型的样本外$r^2$的无偏估计器，并利用最近关于数据不确定性的理论进展。 |
| [^69] | [Information Theoretical Importance Sampling Clustering.](http://arxiv.org/abs/2302.04421) | 本文提出了一种信息理论重要性采样聚类方法，该方法最小化在分布偏差约束下期望失真的最坏情况。 |
| [^70] | [PAC-Bayesian Soft Actor-Critic Learning.](http://arxiv.org/abs/2301.12776) | 本文提出了一种使用PAC-Bayesian bound作为Soft Actor-Critic (SAC)算法评论家训练目标的方法，以解决训练不稳定的问题，并通过评论家引导的随机搜索探索多个未来来提高在线学习性能。在多个经典控制和运动任务中，该算法具有样本效率和遗憾最小化方面的明显优势。 |
| [^71] | [ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts.](http://arxiv.org/abs/2301.12171) | 这篇论文提出了一种通过最优传输的方法，利用多个文本提示来实现零样本分割，达到了最先进的性能水平。 |
| [^72] | [Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates.](http://arxiv.org/abs/2301.11294) | 本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。 |
| [^73] | [On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures.](http://arxiv.org/abs/2301.10932) | 本论文研究了具有期望条件风险度量的风险厌恶策略梯度方法，提出了策略梯度更新，证明了其在约束和无约束情况下的全局收敛性和迭代复杂度，并测试了REINFORCE和actor-critic算法的风险厌恶变体来展示方法的实用价值和风险控制的重要性。 |
| [^74] | [Learning Deformation Trajectories of Boltzmann Densities.](http://arxiv.org/abs/2301.07388) | 本文介绍了一种学习Boltzmann密度变形轨迹的方法，其中通过插值能量函数等实现Boltzmann密度的变形，然后找到一个时间依赖向量场，将样本从一个分布转移到另一个分布，其表现在高斯混合和量子力学粒子的Boltzmann密度上比KL-反散度更具优势。 |
| [^75] | [Sequential Predictive Conformal Inference for Time Series.](http://arxiv.org/abs/2212.03463) | 提出了一种新的适用于时序数据的自适应重新估计条件分位数的置信预测算法SPCI，相较于其他现有方法，SPCI在所需经验覆盖下的区间宽度显著减小。 |
| [^76] | [Backtracking Counterfactuals.](http://arxiv.org/abs/2211.00472) | 本文介绍了一种基于回溯式反事实推理的形式化方法，该方法基于图形模型，提供了一种更加自然和直观的推理过去时反事实情景的方法。 |
| [^77] | [Adaptive Selection of the Optimal Strategy to Improve Precision and Power in Randomized Trials.](http://arxiv.org/abs/2210.17453) | 本研究提供了一种自适应预设方法，以选择在随机试验中调整哪些变量，以及以何种形式进行调整，从而最大化精度，同时保持Ⅰ类错误控制。 |
| [^78] | [Consistent and Truthful Interpretation with Fourier Analysis.](http://arxiv.org/abs/2210.17426) | 该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。 |
| [^79] | [Training Neural Networks for Sequential Change-point Detection.](http://arxiv.org/abs/2210.17312) | 本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。 |
| [^80] | [Dual control variate for faster black-box variational inference.](http://arxiv.org/abs/2210.07290) | 本论文提出了双控制变量方法，能够同时减少数据子抽样和蒙特卡罗抽样带来的梯度估计方差，提高黑盒变分推断的准确性和效率。 |
| [^81] | [InfoOT: Information Maximizing Optimal Transport.](http://arxiv.org/abs/2210.03164) | 提出了InfoOT，它是一种信息论扩展的最优输运方法，能够解决最优输运忽略了数据中相干结构的问题，同时能够处理离群值和集成新数据点，可以提高域自适应、跨域检索和单细胞对齐等任务的对齐质量。 |
| [^82] | [Training Normalizing Flows from Dependent Data.](http://arxiv.org/abs/2209.14933) | 该论文提出了一种考虑数据点依赖关系的归一化流似然目标和学习算法，在合成和真实数据上实现更好的经验结果和更高的统计功效。 |
| [^83] | [Superiority of GNN over NN in generalizing bandlimited functions.](http://arxiv.org/abs/2206.05904) | 本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。 |
| [^84] | [Computational Doob's h-transforms for Online Filtering of Discretely Observed Diffusions.](http://arxiv.org/abs/2206.03369) | 本文提出了一种计算Doob h变换的计算框架，用于离散观测非线性扩散过程的在线滤波。实验证明，该方法在高度信息化、观测值在模型下极端或状态维数较大时比最先进的粒子滤波器高几个数量级的效率。 |
| [^85] | [Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking.](http://arxiv.org/abs/2203.13151) | 本文提出了一种多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。并设计了基于高斯过程的Thompson抽样（GP-TS）算法，加速Pre-training过程并降低MLM损失。 |
| [^86] | [An Accelerated Stochastic Algorithm for Solving the Optimal Transport Problem.](http://arxiv.org/abs/2203.00813) | 本文提出了一种能够用较低的计算复杂度解决最优输运问题的加速随机算法。 |
| [^87] | [Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start.](http://arxiv.org/abs/2202.03397) | 本文针对一类双层问题，提出了无需warm-start也可实现最优样本复杂度的方法。 |
| [^88] | [Mixed Membership Distribution-Free Model.](http://arxiv.org/abs/2112.04389) | 本文提出了一种混合成员无分布模型，用于重叠加权网络的社群检测，支持节点所属多个社群和有限实数权值。提出的模型可以推广到之前的模型，包括混合成员随机块模型，并支持具有潜在社群结构的重叠符号网络的生成。我们使用高效谱算法估计模型下的社群成员资格，并提出了模糊加权模块度来评估重叠加权网络的社群检测质量并确定加权网络社群数量。 |
| [^89] | [Group equivariant neural posterior estimation.](http://arxiv.org/abs/2111.13139) | 本文提出了一种群等变神经后验估计（GNPE）算法，能够在参数和数据联合变换下整合等变性，用于从引力波观测中对双黑洞系统进行摊销推断。 |
| [^90] | [The Fragility of Optimized Bandit Algorithms.](http://arxiv.org/abs/2109.13595) | 本文表明，使用优化设计的赌博算法遗憾分布具有非常重的尾部，对于$p>1$，遗憾分布的$p$'th矩增长要比多对数级别快得多，当问题略微错误时，优化UCB赌博设计的遗憾可以比传统理论建议的增长得更快。 |
| [^91] | [Community detection for weighted bipartite networks.](http://arxiv.org/abs/2109.10319) | 本文提出了一种名为Bipartite Distribution-Free的模型，可用于建模和探测加权二分网络的社区结构，该模型考虑了节点度数的变化以及期望的块结构。同时，我们提出了谱算法用于识别社区。 |
| [^92] | [Which Invariance Should We Transfer? A Causal Minimax Learning Approach.](http://arxiv.org/abs/2107.01876) | 该论文从因果的角度提出了一种全面的极小化分析，旨在回答机器学习模型在转移稳定信息时应该转移哪个子集从而达到最佳的泛化能力这一问题 |
| [^93] | [Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses.](http://arxiv.org/abs/2106.09779) | 本文研究了无需信任服务器或其他数据源的跨 silo 联邦学习，考虑了跨 silo 记录级差分隐私 ISRL-DP。该算法可以确保来自每个人的数据都不会被泄漏。 |
| [^94] | [On the Tightness of the Moment Accountant for DP-SGD.](http://arxiv.org/abs/2102.09030) | 通过改进Moment Accountant方法，DP-SGD具有可关闭形式的$(\epsilon，\delta)$-DP保证，并且其保证接近是紧密的，具有最小的计算成本。 |
| [^95] | [How Powerful are Shallow Neural Networks with Bandlimited Random Weights?.](http://arxiv.org/abs/2008.08427) | 本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。 |

# 详细

[^1]: 蒙特卡罗采样下基于高斯过程回归的主动学习中并行获取样本的研究

    Parallelized Acquisition for Active Learning using Monte Carlo Sampling. (arXiv:2305.19267v1 [stat.ML])

    [http://arxiv.org/abs/2305.19267](http://arxiv.org/abs/2305.19267)

    本文介绍了一种基于高斯过程回归和主动采样的后验模拟器的方法，通过在均值预测上进行分级采样，在保持准确性的前提下实现了蒙特卡洛采样的并行处理，有效降低了成本。

    

    贝叶斯推断仍然是任何科学家最重要的工具之一，但是随着越来越复杂的实验需要越来越昂贵的似然函数，生成后验概率的蒙特卡罗样本的成本也在增加。最近，基于高斯过程回归的后验近似模拟器和主动采样的结合引起了人们的关注，以实现在更少的似然函数评估的情况下获得可比较的精度。该方法的关键在于批量获取建议，以便可以并行评估真正的后验概率。这通常通过连续最大化高度多峰的获取函数来实现。不幸的是，这种方法的并行处理效果不好，容易陷入局部最大值。我们的方法通过在高斯过程回归的平均预测上进行几乎尴尬地并行分层采样来解决这个问题。因此产生的几乎排序的蒙特卡罗样本可以并行评估，从而在保持近似后验的准确性的同时实现显著加速。

    Bayesian inference remains one of the most important tool-kits for any scientist, but increasingly expensive likelihood functions are required for ever-more complex experiments, raising the cost of generating a Monte Carlo sample of the posterior. Recent attention has been directed towards the use of emulators of the posterior based on Gaussian Process (GP) regression combined with active sampling to achieve comparable precision with far fewer costly likelihood evaluations. Key to this approach is the batched acquisition of proposals, so that the true posterior can be evaluated in parallel. This is usually achieved via sequential maximization of the highly multimodal acquisition function. Unfortunately, this approach parallelizes poorly and is prone to getting stuck in local maxima. Our approach addresses this issue by generating nearly-optimal batches of candidates using an almost-embarrassingly parallel Nested Sampler on the mean prediction of the GP. The resulting nearly-sorted Mont
    
[^2]: Shuffle SGD总是比SGD更好：对具有任意数据顺序的SGD进行改进分析

    Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])

    [http://arxiv.org/abs/2305.19259](http://arxiv.org/abs/2305.19259)

    本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。

    

    随机梯度下降（SGD）算法被广泛用于优化神经网络，随机重排（RR）和单次洗牌（SS）是通过循环遍历训练数据的随机或单个排列的常见选择，然而这些算法在非凸情况下的收敛性质尚未完全理解。现有结果表明，在实际的训练场景中，当时代的数量小于训练集大小时，RR可能表现不如SGD。本文分析了一种允许任意数据排序的普通SGD算法，并展示了在非凸函数情况下的改进收敛速度。具体而言，我们的分析表明，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。总的来说，我们的研究凸显了使用随机/单次洗牌的SGD的好处，并为其非凸收敛性质提供了新的见解。

    Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
    
[^3]: Auto-tune: 神经网络的先验与后验PAC-Bayes优化

    Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])

    [http://arxiv.org/abs/2305.19243](http://arxiv.org/abs/2305.19243)

    通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。

    

    通过精心设计训练过程，可以显著提高神经网络的泛化能力。目前最先进的训练方法涉及使用随机梯度下降或Adam优化算法，以及额外的正则化技术，如权重衰减、Dropout或噪声注入。通过网格搜索调整数量众多的超参数才能达到最优泛化，这可能耗时，并需要额外的验证数据集。为解决这个问题，我们引入了一个切实可行的PAC-Bayes训练框架，几乎是无需调整，也不需要额外的正则化，而在完成网格搜索和加入额外正则化后，达到了与SGD/Adam可比较的测试性能。我们提出的算法展示了PAC训练在深度神经网络上实现最先进性能的显著潜力。

    It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit
    
[^4]: dotears: 使用观测和干预数据进行可扩展和一致的DAG估计

    dotears: Scalable, consistent DAG estimation using observational and interventional data. (arXiv:2305.19215v1 [stat.ML])

    [http://arxiv.org/abs/2305.19215](http://arxiv.org/abs/2305.19215)

    dotears是一个可扩展的DAG结构学习框架，使用观测和干预数据来推断单个因果结构。它直接估计外生误差结构，避免了循环估计问题。

    

    从数据中学习因果有向无环图 (DAG)面临着可辨识性缺失和解决方案组合空间的复杂性。最近的研究提高了观测数据中基于得分的DAG结构学习的可操作性，但对外生误差方差的结构敏感。同时，从观测数据学习外生方差结构需要结构的先验知识。针对新的生物技术，将高度并行的基因干预与高维观测数据联系起来，我们提出了一个可扩展的结构学习框架dotears，通过连续优化利用观测和干预数据来推断单个因果结构。dotears利用干预的可预测的结构后果直接估计外生误差结构，从而避免了循环估计问题。我们扩展了先前的工作，从经验和分析方面进行了展示。

    Learning causal directed acyclic graphs (DAGs) from data is complicated by a lack of identifiability and the combinatorial space of solutions. Recent work has improved tractability of score-based structure learning of DAGs in observational data, but is sensitive to the structure of the exogenous error variances. On the other hand, learning exogenous variance structure from observational data requires prior knowledge of structure. Motivated by new biological technologies that link highly parallel gene interventions to a high-dimensional observation, we present $\texttt{dotears}$ [doo-tairs], a scalable structure learning framework which leverages observational and interventional data to infer a single causal structure through continuous optimization. $\texttt{dotears}$ exploits predictable structural consequences of interventions to directly estimate the exogenous error structure, bypassing the circular estimation problem. We extend previous work to show, both empirically and analytical
    
[^5]: 低秩矩阵逼近的梯度下降全局快速收敛性

    Fast global convergence of gradient descent for low-rank matrix approximation. (arXiv:2305.19206v1 [math.OC])

    [http://arxiv.org/abs/2305.19206](http://arxiv.org/abs/2305.19206)

    本文证明了对于低秩矩阵逼近问题，使用小随机初始化的梯度下降方法具有非常快的全局收敛性，尤其是当最大特征值相同时。退化自由算法比Riemannian梯度下降方法更有效，在运行时间上可以减少29％。

    

    本文研究了使用梯度下降求解低秩矩阵逼近问题。我们首先证明了对于对称矩阵逼近，梯度下降的局部线性收敛性。在此基础上，我们证明了梯度下降在使用随机小值初始化时具有非常快的全局收敛性。值得注意的是，我们表明，即使是在中等大小的随机初始化情况下，包括使用小随机初始化的特殊情况在内，当最大特征值相同时，梯度下降在快速收敛方面也具有很好的效果。此外，我们扩展了我们的分析以解决非对称矩阵逼近问题，并调查了一种不依赖投影的特征空间计算方法的有效性。数值实验强烈支持我们的理论。特别地，退化自由算法优于对应的Riemannian梯度下降方法，导致运行时间显着减少了29％。

    This paper investigates gradient descent for solving low-rank matrix approximation problems. We begin by establishing the local linear convergence of gradient descent for symmetric matrix approximation. Building on this result, we prove the rapid global convergence of gradient descent, particularly when initialized with small random values. Remarkably, we show that even with moderate random initialization, which includes small random initialization as a special case, gradient descent achieves fast global convergence in scenarios where the top eigenvalues are identical. Furthermore, we extend our analysis to address asymmetric matrix approximation problems and investigate the effectiveness of a retraction-free eigenspace computation method. Numerical experiments strongly support our theory. In particular, the retraction-free algorithm outperforms the corresponding Riemannian gradient descent method, resulting in a significant 29\% reduction in runtime.
    
[^6]: 生成可信的文本：大型语言模型的不确定性量化

    Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])

    [http://arxiv.org/abs/2305.19187](http://arxiv.org/abs/2305.19187)

    本研究提出应对大型语言模型可信度问题的方法，研究黑盒模型中置信度与不确定性量化，并将其应用于选择性自然语言生成。

    

    近期，专门用于自然语言生成的大型语言模型(LLMs)在各个领域表现出了很好的能力，但是评估LLMs生成的结果的可信度仍然是一个挑战，关于自然语言生成的不确定性量化的研究也较少。此外，现有的文献通常假定对语言模型的白盒访问，这要么是由于最新的LLMs的封闭源代码的性质，要么是由于计算限制。本文研究了黑盒LLMs的不确定性量化问题。我们首先区分了两种密切相关的概念: 只与输入有关的“不确定性”和还与生成的回复有关的“置信度”。然后我们提出并比较了几个置信度/不确定度指标，将它们应用于“选择性自然语言生成”，其中不可靠的结果可以被忽略或者移交给进一步的分析。

    Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
    
[^7]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^8]: 合作阈值 Lasso 处理稀疏线性 Bandit 问题

    Cooperative Thresholded Lasso for Sparse Linear Bandit. (arXiv:2305.19161v1 [cs.LG])

    [http://arxiv.org/abs/2305.19161](http://arxiv.org/abs/2305.19161)

    本论文提出了一种基于合作阈值 Lasso 的方法，解决多智能体稀疏上下文线性 Bandit 问题，该方法可以降维与信息共享，以降低通信成本，同时保证每个智能体的最小累计遗憾。

    

    我们提出了一种新的方法来解决多智能体稀疏上下文线性 Bandit 问题，其中特征向量具有高维度 $d$，而奖励函数仅依赖于一组有限的特征，精确地为 $s_0 \ll d$。此外，学习在信息共享约束下进行。所提出的方法采用 Lasso 回归进行降维，允许每个智能体独立估计一个近似的主维度集合，并根据网络结构与其他智能体共享该信息。然后通过特定的过程聚合信息并与所有智能体共享。然后，每个智能体只关注提取的维度，通过岭回归解决问题。我们提出了针对星形网络和点对点网络的算法。这些方法在确保每个智能体的最小累计遗憾的同时有效地降低了通信成本。理论上，我们证明了我们提出的方法具有 $O(s_0 \sqrt{T \log d})$ 和 $O(\sqrt{s_0} \sqrt{T \log d})$ 的遗憾界，其中 $T$ 表示所有智能体玩游戏的回合数。

    We present a novel approach to address the multi-agent sparse contextual linear bandit problem, in which the feature vectors have a high dimension $d$ whereas the reward function depends on only a limited set of features precisely $s_0 \ll d$. Furthermore, the learning follows under information-sharing constraints. The proposed method employs Lasso regression for dimension reduction, allowing each agent to independently estimate an approximate set of main dimensions and share that information with others depending on the network's structure. The information is then aggregated through a specific process and shared with all agents. Each agent then resolves the problem with ridge regression focusing solely on the extracted dimensions. We represent algorithms for both a star-shaped network and a peer-to-peer network. The approaches effectively reduce communication costs while ensuring minimal cumulative regret per agent. Theoretically, we show that our proposed methods have a regret boun
    
[^9]: ELSA: 基于半参数模型的高效标签偏移适应研究

    ELSA: Efficient Label Shift Adaptation through the Lens of Semiparametric Models. (arXiv:2305.19123v1 [stat.ML])

    [http://arxiv.org/abs/2305.19123](http://arxiv.org/abs/2305.19123)

    本文基于影响函数几何提出了一个矩匹配框架来适应标签偏移，提出了一个名为“高效标签偏移适应”的适应方法，具有一致性和渐进正态性，可在不需要后预测校准的情况下实现最先进的估计性能。

    

    本文研究了标签偏移下的域适应问题。在标签偏移的情况下，标签的边际分布在训练集和测试集之间不同，而给定标签的特征的条件分布相同。传统的标签偏移适应方法要么存在估计误差较大的问题，要么需要繁琐的后预测校准。为了解决这些问题，我们首先提出了一个基于影响函数几何的矩匹配框架来适应标签偏移。在此框架下，我们提出了一种名为“高效标签偏移适应”的新方法(ELSA)，其中适应权重可以通过解线性系统来估计。从理论上讲，ELSA估计器是$\sqrt{n}$-一致的(n是源数据的样本大小)，而且是渐进正态的。实际上，我们展示了ELSA在合成和真实数据集上都可以在不需要后预测校准的情况下实现最先进的估计性能。

    We study the domain adaptation problem with label shift in this work. Under the label shift context, the marginal distribution of the label varies across the training and testing datasets, while the conditional distribution of features given the label is the same. Traditional label shift adaptation methods either suffer from large estimation errors or require cumbersome post-prediction calibrations. To address these issues, we first propose a moment-matching framework for adapting the label shift based on the geometry of the influence function. Under such a framework, we propose a novel method named \underline{E}fficient \underline{L}abel \underline{S}hift \underline{A}daptation (ELSA), in which the adaptation weights can be estimated by solving linear systems. Theoretically, the ELSA estimator is $\sqrt{n}$-consistent ($n$ is the sample size of the source data) and asymptotically normal. Empirically, we show that ELSA can achieve state-of-the-art estimation performances without post-p
    
[^10]: Barron型空间的嵌入不等式

    Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v1 [stat.ML])

    [http://arxiv.org/abs/2305.19082](http://arxiv.org/abs/2305.19082)

    本文测量了Barron空间和谱Barron空间之间的关系，并提供了嵌入不等式。

    

    深度学习理论中的一个基本问题是理解高维条件下两层神经网络的逼近和泛化性质。为了解决这个问题，研究人员引入了Barron空间$\mathcal{B}_s(\Omega)$和谱Barron空间$\mathcal{F}_s(\Omega)$，其中指数$s$表征了这些空间中函数的平滑性，$\Omega\subset\mathbb{R}^d$表示输入域。然而，两种类型的Barron空间之间的关系仍不清楚。本文通过以下不等式建立了这些空间之间的连续嵌入：对于任意$\delta\in(0,1),s\in\mathbb{N}^{+}$和$f:\Omega \mapsto \mathbb{R}$，都有\[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \]其中$\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$，$\lesssim_s$表示仅与平滑参数$s$有关的常数。

    One of the fundamental problems in deep learning theory is understanding the approximation and generalization properties of two-layer neural networks in high dimensions. In order to tackle this issue, researchers have introduced the Barron space $\mathcal{B}_s(\Omega)$ and the spectral Barron space $\mathcal{F}_s(\Omega)$, where the index $s$ characterizes the smoothness of functions within these spaces and $\Omega\subset\mathbb{R}^d$ represents the input domain. However, it is still not clear what is the relationship between the two types of Barron spaces. In this paper, we establish continuous embeddings between these spaces as implied by the following inequality: for any $\delta\in (0,1), s\in \mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it holds that \[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \] where $\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$ and notab
    
[^11]: 训练期间的自适应秩谱剪枝卷积层

    Rank-adaptive spectral pruning of convolutional layers during training. (arXiv:2305.19059v1 [cs.LG])

    [http://arxiv.org/abs/2305.19059](http://arxiv.org/abs/2305.19059)

    本论文提出了一种新的低参数训练方法，该方法将卷积分解为张量Tucker格式，并在训练过程中自适应地修剪卷积核的Tucker秩，可以有效地降低训练成本。

    

    深度学习模型在计算成本和内存需求方面增长迅速，因此已经发展了各种剪枝技术以减少模型参数。大多数技术侧重于通过在完整训练后对网络进行修剪以减少推理成本。少量的方法解决了减少训练成本的问题，主要是通过低秩层分解来压缩网络。尽管这些方法对于线性层是有效的，但是它们无法有效处理卷积滤波器。在这项工作中，我们提出了一种低参数训练方法，将卷积分解为张量Tucker格式，并在训练过程中自适应地修剪卷积核的Tucker秩。利用微分方程在张量流形上的几何积分理论的基本结果，我们获得了一个鲁棒的训练算法，证明能够逼近完整的基线性能并保证损失下降。

    The computing cost and memory demand of deep learning pipelines have grown fast in recent years and thus a variety of pruning techniques have been developed to reduce model parameters. The majority of these techniques focus on reducing inference costs by pruning the network after a pass of full training. A smaller number of methods address the reduction of training costs, mostly based on compressing the network via low-rank layer factorizations. Despite their efficiency for linear layers, these methods fail to effectively handle convolutional filters. In this work, we propose a low-parametric training method that factorizes the convolutions into tensor Tucker format and adaptively prunes the Tucker ranks of the convolutional kernel during training. Leveraging fundamental results from geometric integration theory of differential equations on tensor manifolds, we obtain a robust training algorithm that provably approximates the full baseline performance and guarantees loss descent. A var
    
[^12]: 基于热扩散视角的地形保持降维方法

    A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction. (arXiv:2305.19043v1 [cs.LG])

    [http://arxiv.org/abs/2305.19043](http://arxiv.org/abs/2305.19043)

    本论文提出了一种基于热扩散视角的地形保持降维方法，建立了热扩散与流形距离之间的理论联系。该方法在保持流形距离和簇结构方面优于最先进的方法。

    

    基于扩散的流形学习方法在处理现代高维、高通量、噪声数据集的表示学习和降维中已经证明非常有用，特别是在生物学和物理学等领域。虽然人们认为这些方法通过学习测地线距离的代理来保持数据的底层流形结构，但没有确切的理论联系。在本文中，我们通过黎曼流形上的结果建立了这样的联系，明确了热扩散与流形距离之间的关系。在此过程中，我们还提出了一种更普遍的基于热核的流形嵌入方法，称为热测地线嵌入。这种新颖的视角使流形学习和去噪中的选择更加清晰。结果表明，我们的方法在保持基准流形距离和保持玩具数据集中的簇结构方面优于现有的最先进方法。我们还展示了单细胞RNA测序数据上的应用。

    Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call heat geodesic embeddings. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state of the art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-s
    
[^13]: 学习特征中的瓶颈结构：低维度与规律性的权衡

    Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff. (arXiv:2305.19008v1 [cs.LG])

    [http://arxiv.org/abs/2305.19008](http://arxiv.org/abs/2305.19008)

    本研究揭示了深度学习神经网路学习输入低维度表示和最小化特征映射中的复杂性/不规则性之间的权衡，控制了规律性，并利用理论工具证明了瓶颈结构的存在。

    

    先前研究表明，具有大深度$L$和$L_{2}$正则化的DNN偏向于学习输入的低维表示，可以解释为最小化学习函数$f$的秩$R^{(0)}(f)$的概念，其被推测为瓶颈秩。我们计算了这个结果的有限深度修正，揭示了一个度量$R^{(1)}$的规律性，它控制了雅可比矩阵$\left|Jf(x)\right|_{+}$的伪行列式并在组合和加法下是次可加的。这使得网络可以在学习低维表示和最小化特征映射中的复杂性/不规则性之间保持平衡，从而学习“正确”的内部尺寸。我们还展示了大学习速率如何控制学习函数的规律性。最后，我们使用这些理论工具证明了瓶颈结构在$L\to\infty$时在学习特征中的猜想：对于大深度，几乎所有的隐藏表示都集中在...

    Previous work has shown that DNNs with large depth $L$ and $L_{2}$-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank $R^{(0)}(f)$ of the learned function $f$, conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure $R^{(1)}$ of regularity which bounds the pseudo-determinant of the Jacobian $\left|Jf(x)\right|_{+}$ and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the `right' inner dimension. We also show how large learning rates also control the regularity of the learned function. Finally, we use these theoretical tools to prove the conjectured bottleneck structure in the learned features as $L\to\infty$: for large depths, almost all hidden representations concentrates aroun
    
[^14]: 线性函数逼近下的策略评估的高概率样本复杂度

    Sharp high-probability sample complexities for policy evaluation with linear function approximation. (arXiv:2305.19001v1 [stat.ML])

    [http://arxiv.org/abs/2305.19001](http://arxiv.org/abs/2305.19001)

    本文研究线性函数逼近下的策略评估问题，提出了两个广泛使用的算法所需的样本复杂度，具有高概率收敛保证且与容差水平的关联性最佳。

    

    本文涉及使用线性函数逼近在无限时间马尔可夫决策过程中进行策略评估的问题。我们研究了两种广泛使用的策略评估算法（时间差分学习算法和带有梯度校正的两个时间尺度线性时间差分算法）所需的样本复杂度，以保证最佳线性系数的预定义估计误差。在策略设置和离线设置中，我们建立了第一个具有高概率收敛保证的样本复杂度界限，达到了与容差水平的最佳关联性。我们还展示了与问题相关量明确的关系，并在策略设置中展示了我们的上限界限与关键问题参数上的极小极大下限界限相匹配。

    This paper is concerned with the problem of policy evaluation with linear function approximation in discounted infinite horizon Markov decision processes. We investigate the sample complexities required to guarantee a predefined estimation error of the best linear coefficients for two widely-used policy evaluation algorithms: the temporal difference (TD) learning algorithm and the two-timescale linear TD with gradient correction (TDC) algorithm. In both the on-policy setting, where observations are generated from the target policy, and the off-policy setting, where samples are drawn from a behavior policy potentially different from the target policy, we establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level. We also exhihit an explicit dependence on problem-related quantities, and show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters, in
    
[^15]: 广义自回归得分树与森林

    Generalized Autoregressive Score Trees and Forests. (arXiv:2305.18991v1 [econ.EM])

    [http://arxiv.org/abs/2305.18991](http://arxiv.org/abs/2305.18991)

    该论文提出了一种方法，通过使用决策树和随机森林来定位广义自回归得分模型的参数，改善其预测，并揭示了股票回报波动率和密度预测中的杠杆效应和方差风险溢价效应，以及股票-债券依赖性中的流向质量效应和高频交易持续时间中的成交量-波动性效应。

    

    我们提出了一种方法，通过使用决策树和随机森林来定位广义自回归得分（GAS）模型（Creal et. al, 2013; Harvey, 2013）的参数，从而改善其预测。这些方法避免了基于核函数的方法所面临的维度灾难，并允许同时利用多个状态变量的信息。我们将新模型应用于四种不同的实证分析中，并且在所有应用中，新的方法都显著优于基线GAS模型。在我们应用于股票回报波动率和密度预测的实验中，最优的GAS树模型揭示了杠杆效应和方差风险溢价效应。我们在股票-债券依赖性的研究中发现了优化的GAS森林预测中的流向质量效应，我们对高频交易持续时间的分析揭示了成交量-波动性效应。

    We propose methods to improve the forecasts from generalized autoregressive score (GAS) models (Creal et. al, 2013; Harvey, 2013) by localizing their parameters using decision trees and random forests. These methods avoid the curse of dimensionality faced by kernel-based approaches, and allow one to draw on information from multiple state variables simultaneously. We apply the new models to four distinct empirical analyses, and in all applications the proposed new methods significantly outperform the baseline GAS model. In our applications to stock return volatility and density prediction, the optimal GAS tree model reveals a leverage effect and a variance risk premium effect. Our study of stock-bond dependence finds evidence of a flight-to-quality effect in the optimal GAS forest forecasts, while our analysis of high-frequency trade durations uncovers a volume-volatility effect.
    
[^16]: 异常点存在时健壮经验风险最小化性能的渐进特性研究

    Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers. (arXiv:2305.18974v1 [stat.ML])

    [http://arxiv.org/abs/2305.18974](http://arxiv.org/abs/2305.18974)

    该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。

    

    我们研究了高维健壮线性回归问题，当维度$d$和数据点数量$n$以固定比率$\alpha=n/d$发散，并研究了包括异常点在内的数据模型。我们对使用$\ell_2$ -正则化$\ell_2$，$\ell_1$，和 Huber 损失的经验风险最小化（ERM）性能提供了精确的渐近特性，这是解决这类问题的标准方法。我们关注性能的两个指标：具有异常点的相似数据集的泛化误差和原始无污染函数的估计误差。我们将结果与信息论贝叶斯最优估计界进行了比较。对于泛化误差，我们发现如果进行简单的校准并计算收敛速率，则最优正则化ERM在大样本复杂度限制下是渐近一致的。然而，对于估计误差，由于范数校准不匹配，我们表明估计器的一致性需要一个较强的收敛假设，这对问题的解决还需要进一步的研究。

    We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\ell_2$-regularised $\ell_2$, $\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an
    
[^17]: 多通道监督学习的量子卷积神经网络

    Quantum Convolutional Neural Networks for Multi-Channel Supervised Learning. (arXiv:2305.18961v1 [quant-ph])

    [http://arxiv.org/abs/2305.18961](http://arxiv.org/abs/2305.18961)

    本文介绍了多通道监督学习的量子卷积神经网络，通过硬件适应性的量子电路ansatzes用作卷积核，能够有效学习通道间信息，优于现有的QCNNs。

    

    随着机器学习领域的快速发展，制造出非常有用的工具和模型，量子计算为机器学习算法提供加速的潜力正在日益受到重视。特别是，研究用于基于图像检测任务的量子电路取代经典卷积滤波器以利用量子优势的尝试，称为量子卷积神经网络（QCNNs）。然而，这些尝试缺乏处理具有多个通道的数据的能力，因此只适用于相对简单的输入。在这项工作中，我们提出了多种硬件适应性的量子电路ansatzes用作卷积核，并证明我们报告的量子神经网络在涉及多通道数据的分类任务中优于现有的QCNNs。我们预计，这些实现有效学习通道间信息的能力将允许量子机器学习在处理现实任务时获得重大突破。

    As the rapidly evolving field of machine learning continues to produce incredibly useful tools and models, the potential for quantum computing to provide speed up for machine learning algorithms is becoming increasingly desirable. In particular, quantum circuits in place of classical convolutional filters for image detection-based tasks are being investigated for the ability to exploit quantum advantage. However, these attempts, referred to as quantum convolutional neural networks (QCNNs), lack the ability to efficiently process data with multiple channels and therefore are limited to relatively simple inputs. In this work, we present a variety of hardware-adaptable quantum circuit ansatzes for use as convolutional kernels, and demonstrate that the quantum neural networks we report outperform existing QCNNs on classification tasks involving multi-channel data. We envision that the ability of these implementations to effectively learn inter-channel information will allow quantum machine
    
[^18]: Clip21：梯度裁剪的误差反馈

    Clip21: Error Feedback for Gradient Clipping. (arXiv:2305.18929v1 [cs.LG])

    [http://arxiv.org/abs/2305.18929](http://arxiv.org/abs/2305.18929)

    本论文报道了Clip21，这是第一个适用于梯度裁剪的分布式训练的有效误差反馈方法，能够解决严重的收敛问题，证明了其收敛速度与分布式方法相同。

    

    受到差分隐私（DP）约束下大规模训练的日益普及和重要性的推动，本研究研究了梯度裁剪的分布式梯度方法，即在节点处计算的梯度应用裁剪。虽然梯度裁剪是将正式DP保证注入梯度为基础的方法的重要工具，但它也会引入偏差，从而引起分布式环境下严重的收敛问题。启发于最近在误差反馈文献中取得的进展，该文献集中于驯服通信压缩运算符（例如Top - k）引入的偏差/误差，以及裁剪运算符与收缩压缩运算符之间的数学相似性，我们设计了 Clip21，这是分布式梯度裁剪的第一个能够被证明有效且实用的误差反馈机制。我们证明了我们的方法与分布式收敛速度相同。

    Motivated by the increasing popularity and importance of large-scale training under differential privacy (DP) constraints, we study distributed gradient methods with gradient clipping, i.e., clipping applied to the gradients computed from local information at the nodes. While gradient clipping is an essential tool for injecting formal DP guarantees into gradient-based methods [1], it also induces bias which causes serious convergence issues specific to the distributed setting. Inspired by recent progress in the error-feedback literature which is focused on taming the bias/error introduced by communication compression operators such as Top-$k$ [2], and mathematical similarities between the clipping operator and contractive compression operators, we design Clip21 -- the first provably effective and practically useful error feedback mechanism for distributed methods with gradient clipping. We prove that our method converges at the same $\mathcal{O}\left(\frac{1}{K}\right)$ rate as distrib
    
[^19]: 学习扰动来解释时间序列预测

    Learning Perturbations to Explain Time Series Predictions. (arXiv:2305.18840v1 [cs.LG])

    [http://arxiv.org/abs/2305.18840](http://arxiv.org/abs/2305.18840)

    本文提出了一种基于学习扰动的方法来解释预测，相对于传统的基于扰动的显著性方法，该方法可以显着提高对于多元时间序列数据的解释质量。

    

    解释基于多元时间序列数据的预测具有额外的困难，需要处理多个特征以及时间依赖关系。先前的工作使用基于扰动的显著性方法来解决这个问题，通过使用可训练的掩码扰动输入，发现哪些特征在哪个时刻驱动了预测。然而，这些方法引入了固定的扰动，受到静态数据类似方法的启发，而似乎在时间上没有什么动机这样做。在这项工作中，我们旨在通过学习关联扰动，而不仅仅是掩码，来解释预测。我们从实证上证明，学习这些扰动显着提高了时间序列数据的解释质量。

    Explaining predictions based on multivariate time series data carries the additional difficulty of handling not only multiple features, but also time dependencies. It matters not only what happened, but also when, and the same feature could have a very different impact on a prediction depending on this time information. Previous work has used perturbation-based saliency methods to tackle this issue, perturbing an input using a trainable mask to discover which features at which times are driving the predictions. However these methods introduce fixed perturbations, inspired from similar methods on static data, while there seems to be little motivation to do so on temporal data. In this work, we aim to explain predictions by learning not only masks, but also associated perturbations. We empirically show that learning these perturbations significantly improves the quality of these explanations on time series data.
    
[^20]: PyPOTS：用于部分观测时间序列数据挖掘的Python工具箱

    PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time Series. (arXiv:2305.18811v1 [cs.LG])

    [http://arxiv.org/abs/2305.18811](http://arxiv.org/abs/2305.18811)

    PyPOTS是一个Python工具箱，用于对部分观测的时间序列数据进行数据挖掘和分析，包括插值、分类、聚类和预测等四个任务，算法种类繁多，适用于学术研究和工业应用。

    

    PyPOTS是一个开源的Python库，致力于在多元部分观测时间序列数据上进行数据挖掘和分析，即针对存在缺失值的不完整时间序列，也称为不规则采样时间序列。特别地，它提供了对四个任务分类的不同算法的易用性支持：插值、分类、聚类和预测。它包含了概率方法和神经网络方法，提供了设计良好、完整文档的编程接口，供学术研究人员和工业专业人员使用。该工具包的设计理念是鲁棒性和可伸缩性，开发过程中遵循了软件构建的最佳实践，例如单元测试、持续集成（CI）和持续交付（CD）、代码覆盖率、可维护性评估、交互式教程和并行化等原则。该工具箱可在Python包索引（PyPI）和Anaconda上使用。

    PyPOTS is an open-source Python library dedicated to data mining and analysis on multivariate partially-observed time series, i.e. incomplete time series with missing values, A.K.A. irregularlysampled time series. Particularly, it provides easy access to diverse algorithms categorized into four tasks: imputation, classification, clustering, and forecasting. The included models contain probabilistic approaches as well as neural-network methods, with a well-designed and fully-documented programming interface for both academic researchers and industrial professionals to use. With robustness and scalability in its design philosophy, best practices of software construction, for example, unit testing, continuous integration (CI) and continuous delivery (CD), code coverage, maintainability evaluation, interactive tutorials, and parallelization, are carried out as principles during the development of PyPOTS. The toolkit is available on both Python Package Index (PyPI) and Anaconda. PyPOTS is o
    
[^21]: 基于预测误差的增量学习分类方法

    Prediction Error-based Classification for Class-Incremental Learning. (arXiv:2305.18806v1 [cs.LG])

    [http://arxiv.org/abs/2305.18806](http://arxiv.org/abs/2305.18806)

    本论文提出了一种新的增量学习分类方法——基于预测误差的分类方法（PEC）。对PEC的评估表明，在各种基准测试中，PEC可以与最先进的增量学习方法相竞争，并具有许多实际优势，例如样本效率高、易于调整。

    

    增量学习分类是连续学习中的一个挑战性问题，目标是学习来区分所有类别。现有的方法在处理大量分类时容易出现过度遗忘和分数不均衡。本研究提出了一种新方法，名为预测误差分类（PEC），它与传统的判别和生成分类范式有所不同。PEC通过测量模型在从该类别中学习的数据上复制随机神经网络输出的预测误差来计算类别得分。该方法可以解释为基于高斯过程后验方差的分类规则的近似。PEC具有几个实际优势，包括样本效率高、易于调整以及即使在逐个呈现数据时也很有效。本文的实证结果表明PEC在广泛的基准测试中表现出色，可以与最先进的增量学习方法相竞争。

    Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to classes that have not been seen together during training. In this study, we introduce a novel approach, Prediction Error-based Classification (PEC), which differs from traditional discriminative and generative classification paradigms. PEC computes a class score by measuring the prediction error of a model trained to replicate the outputs of a frozen random neural network on data from that class. The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance. PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time. Our empirical results show that PEC pe
    
[^22]: 合作多智能体异构多臂老虎机翻译论文

    Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits. (arXiv:2305.18784v1 [cs.LG])

    [http://arxiv.org/abs/2305.18784](http://arxiv.org/abs/2305.18784)

    本研究研究了一个新的合作多智能体老虎机设置，并发展了去中心化算法以减少代理之间的集体遗憾，在数学分析中证明了该算法实现了近乎最优性能。

    

    最近合作多智能体老虎机的研究吸引了很多关注。因此，我们开始研究一个新的合作设置，其中$N$个智能体中的每个智能体正在学习$M$个具有随机性的多臂老虎机，以减少他们的集体累计遗憾。我们开发了去中心化算法，促进了代理之间的合作，并针对两种情况进行了性能表征。通过推导每个代理的累积遗憾和集体遗憾的上限，我们对这些算法的性能进行了表征。我们还证明了这种情况下集体遗憾的下限，证明了所提出算法的近乎最优性能。

    The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms.
    
[^23]: 从几何角度看待概率鲁棒学习中的边界问题

    It begins with a boundary: A geometric view on probabilistically robust learning. (arXiv:2305.18779v1 [cs.LG])

    [http://arxiv.org/abs/2305.18779](http://arxiv.org/abs/2305.18779)

    本文探讨了深度神经网络对于对抗生成的示例缺乏鲁棒性的问题，并提出了一种从几何角度出发的新颖视角，介绍一族概率非局部周长函数来优化概率鲁棒学习（PRL）的原始表述，以提高其鲁棒性。

    

    尽管深度神经网络在许多分类任务上已经实现了超人类的表现，但它们往往对于对抗生成的示例缺乏鲁棒性，因此需要将经验风险最小化（ERM）重构为对抗性鲁棒的框架。最近，关注点已经转向了介于对抗性训练提供的鲁棒性和ERM提供的更高干净准确性和更快训练时间之间的方法。本文从几何角度出发，对一种这样的方法——概率鲁棒学习（PRL）（Robey等人，ICML，2022）进行了新颖的几何视角的探讨。我们提出了一个几何框架来理解PRL，这使我们能够确定其原始表述中的微妙缺陷，并介绍了一族概率非局部周长函数来解决这一问题。我们使用新颖的松弛方法证明了解的存在，并研究了引入的非局部周长函数的特性以及局部极限。

    Although deep neural networks have achieved super-human performance on many classification tasks, they often exhibit a worrying lack of robustness towards adversarially generated examples. Thus, considerable effort has been invested into reformulating Empirical Risk Minimization (ERM) into an adversarially robust framework. Recently, attention has shifted towards approaches which interpolate between the robustness offered by adversarial training and the higher clean accuracy and faster training times of ERM. In this paper, we take a fresh and geometric view on one such method -- Probabilistically Robust Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. We prove existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced per
    
[^24]: SFCNeXt：一种用于小样本下有效的脑龄估计的简单全卷积网络

    SFCNeXt: a simple fully convolutional network for effective brain age estimation with small sample size. (arXiv:2305.18771v1 [cs.CV])

    [http://arxiv.org/abs/2305.18771](http://arxiv.org/abs/2305.18771)

    SFCNeXt是一个简单的全卷积网络，用于小型队列中进行脑龄估计，通过SPEC和HRL算法，以轻量化的方式充分探索每个批次的MRI、年龄和排名特征，避免对大量MRI的依赖和复杂模型结构。

    

    深度神经网络被设计出来从T1加权磁共振图像中预测健康大脑的年龄，预测的脑龄可以作为早期检测发展相关或衰老相关疾病的有价值的生物标志物。然而，近期用于脑龄预测的深度神经网络通常过于依赖于大样本量和多阶段特征优化的复杂网络结构。在临床应用场景中，研究人员通常不能在每个数据中心获得成千上万的MRI以充分训练这些复杂的模型。本文提出了一种用于具有偏倚年龄分布的小型队列中进行脑龄估计的简单全卷积网络（SFCNeXt）。 SFCNeXt由Single Pathway Encoded ConvNeXt（SPEC）和Hybrid Ranking Loss（HRL）组成，旨在以轻量化的方式估计脑龄，并充分探索每批扫描的MRI、年龄和排名特征。

    Deep neural networks (DNN) have been designed to predict the chronological age of a healthy brain from T1-weighted magnetic resonance images (T1 MRIs), and the predicted brain age could serve as a valuable biomarker for the early detection of development-related or aging-related disorders. Recent DNN models for brain age estimations usually rely too much on large sample sizes and complex network structures for multi-stage feature refinement. However, in clinical application scenarios, researchers usually cannot obtain thousands or tens of thousands of MRIs in each data center for thorough training of these complex models. This paper proposes a simple fully convolutional network (SFCNeXt) for brain age estimation in small-sized cohorts with biased age distributions. The SFCNeXt consists of Single Pathway Encoded ConvNeXt (SPEC) and Hybrid Ranking Loss (HRL), aiming to estimate brain ages in a lightweight way with a sufficient exploration of MRI, age, and ranking features of each batch o
    
[^25]: 优化适当的损失函数是否能得到校准的预测器？

    When Does Optimizing a Proper Loss Yield Calibration?. (arXiv:2305.18764v1 [cs.LG])

    [http://arxiv.org/abs/2305.18764](http://arxiv.org/abs/2305.18764)

    研究优化适当的损失函数是否能在受限的预测器族中得到校准的模型，使用局部最优条件取代全局最优性条件并在此基础上进行了严格的证明。

    

    优化适当的损失函数被广泛认为会得到具有良好校准特性的预测器，这是因为对于这样的损失，全局最优解是预测真实概率，这确实是校准的。但是，典型的机器学习模型是训练来近似地最小化在受限制的预测器族中的损失，这些预测器族不太可能包含真实的概率。在什么情况下，优化受限制的预测器族中适当的损失可以得到校准的模型？它提供了什么精确的校准保证？在这项工作中，我们提供了这些问题的严格答案。我们用局部最优条件替换全局最优性条件，该条件规定了预测器（适当的）损失不能通过使用一定族群的Lipschitz函数后处理其预测而降低太多。我们证明了具有这种局部最优性质的任何预测器都满足Kakade-Foster(2008)、Błasiok等人(2023)中定义的平稳校准。

    Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade-Foster (2008), B{\l}asiok et al. (2023). L
    
[^26]: 多块双层优化的分块随机方差约简方法及并行加速

    Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization. (arXiv:2305.18730v1 [math.OC])

    [http://arxiv.org/abs/2305.18730](http://arxiv.org/abs/2305.18730)

    本文提出了两种基于方差约简的优化算法，以实现对多块双层优化问题的高效求解，同时匹配单块标准 BO 问题的最优复杂度、实现并行化加速，以及避免计算高维度的 Hessian 矩阵的逆估计。

    

    本文考虑非凸的多块双层优化问题，并提出了两种基于分块方差约简的优化算法。为了达到算法的三个期望：（a）能匹配单块标准 BO 问题的最优复杂度；（b）实现并行化加速，每个迭代中采样 $I$ 块并对每个采样块采样 $B$ 个样本；（c）避免计算高维度的 Hessian 矩阵的逆估计。本文旨在解决这些问题，并探讨了现有算法的关联性以及不足之处。

    In this paper, we consider non-convex multi-block bilevel optimization (MBBO) problems, which involve $m\gg 1$ lower level problems and have important applications in machine learning. Designing a stochastic gradient and controlling its variance is more intricate due to the hierarchical sampling of blocks and data and the unique challenge of estimating hyper-gradient. We aim to achieve three nice properties for our algorithm: (a) matching the state-of-the-art complexity of standard BO problems with a single block; (b) achieving parallel speedup by sampling $I$ blocks and sampling $B$ samples for each sampled block per-iteration; (c) avoiding the computation of the inverse of a high-dimensional Hessian matrix estimator. However, it is non-trivial to achieve all of these by observing that existing works only achieve one or two of these properties. To address the involved challenges for achieving (a, b, c), we propose two stochastic algorithms by using advanced blockwise variance-reductio
    
[^27]: 插件化表现优化

    Plug-in Performative Optimization. (arXiv:2305.18728v1 [cs.LG])

    [http://arxiv.org/abs/2305.18728](http://arxiv.org/abs/2305.18728)

    研究了一种可能“规范不正确”模型的通用协议，“插件式表现优化”。

    

    当预测具有表现性时，选择哪个预测器部署将影响未来观测的分布。在表现性学习中，总体目标是找到具有低“表现性风险”的预测器，即在其引导的分布上表现良好。最优化表现性风险的一系列解决方案，包括赌徒算法和其他无导数方法，在表现性反馈中不知道任何结构，导致收敛速度极慢。补充的一系列解决方案利用反馈中的显式“模型”，例如战略分类中的最佳响应模型，可以实现更快的速率。然而，这些速率关键依赖于反馈模型的规范。在本研究中，我们启动了对在表现性预测中使用可能的“规范不正确”模型的研究。我们研究了一种使用模型的通用协议，称为“插件式表现优化”。

    When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \emph{plug-in performative optimization}, a
    
[^28]: 对抗式自适应采样：将PINN和最优传输统一用于PDE近似

    Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs. (arXiv:2305.18702v1 [stat.ML])

    [http://arxiv.org/abs/2305.18702](http://arxiv.org/abs/2305.18702)

    本研究提出了一种新的深度生成模型来调整训练集中的随机样本，以使PDE解的残余在最小化时能保持平滑的轮廓，并通过引入对抗性损失项优化PINN模型，从而使神经网络学习稳定的解。同时本文还展示了该方法可以扩展到纳入最优传输约束，从而形成了将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。

    

    求解偏微分方程（PDE）是科学计算的一个核心任务。近年来，使用神经网络逼近PDE引起了越来越多的关注，具有无网格离散的灵活性和解决高维问题的潜力。一个基本的计算困难是训练集中的随机样本引入了统计错误，可能成为最终逼近中占主导的误差，从而掩盖了神经网络的建模能力。本文提出了一种新的minmax公式，同时优化近似的解和由深度生成模型提供的训练集中的随机样本。关键思想是使用深度生成模型调整训练集中的随机样本，使近似PDE解引起的残余在最小化时能保持平滑的轮廓。这种想法是通过在PINN优化过程中引入对抗性损失项来实现的，该损失项鼓励神经网络学习稳定的解，即使训练集中的样本有限或输入含噪声。我们进一步展示，所提出的方法可以自然地扩展到纳入最优传输约束，从而形成将PINN和最优传输的优点结合起来的统一框架，用于PDE近似。

    Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust random samples in the training set such that the residual induced by the approximate PDE solution can maintain a smooth profile when it is being minimized. Such an idea is ach
    
[^29]: 通过向比例减少预测罕见事件

    Predicting Rare Events by Shrinking Towards Proportional Odds. (arXiv:2305.18700v1 [stat.ME])

    [http://arxiv.org/abs/2305.18700](http://arxiv.org/abs/2305.18700)

    本文提出了一种序数回归比例减少模型的松弛方法PRESTO，通过对相邻权向量中相同特征之间的差异施加L1惩罚，利用先前更丰富的数据来改善罕见事件的概率估计。

    

    训练分类器在严重的类别不平衡下是困难的，但许多罕见事件是由许多常见的中间结果序列组成的。例如，在在线营销中，用户先看到广告，然后可能点击它，最后可能购买；由于它们的罕见性，估计购买的概率是困难的。我们通过理论和数据实验展示了早期步骤中更丰富的数据可能被利用来改善罕见事件的概率估计。我们提出了PRESTO，一种序数回归比例减少模型的松弛方法。我们不是为一个分离超平面估计权重，并通过每个估计Bayes决策边界之间的分类反应对应的单独拦截进行平移。我们估计每次这些转换的单独权重。我们对相邻权向量中相同特征之间的差异施加L1惩罚，以缩小这些权重的差距。

    Training classifiers is difficult with severe class imbalance, but many rare events are the culmination of a sequence with much more common intermediate outcomes. For example, in online marketing a user first sees an ad, then may click on it, and finally may make a purchase; estimating the probability of purchases is difficult because of their rarity. We show both theoretically and through data experiments that the more abundant data in earlier steps may be leveraged to improve estimation of probabilities of rare events. We present PRESTO, a relaxation of the proportional odds model for ordinal regression. Instead of estimating weights for one separating hyperplane that is shifted by separate intercepts for each of the estimated Bayes decision boundaries between adjacent pairs of categorical responses, we estimate separate weights for each of these transitions. We impose an L1 penalty on the differences between weights for the same feature in adjacent weight vectors in order to shrink 
    
[^30]: 高效的均值中位数估计器

    Efficient median of means estimator. (arXiv:2305.18681v1 [math.ST])

    [http://arxiv.org/abs/2305.18681](http://arxiv.org/abs/2305.18681)

    本文介绍了一种修改版的均值中位数估计器，可以在对底层分布的最小假设下实现亚高斯偏差界限和近乎最优常数。

    

    本文的目标是介绍一种流行的均值中位数估计器的修改版，它在对底层分布的最小假设下实现亚高斯偏差界限和近乎最优常数。我们基于作者最近关于该主题的相关工作，并证明所需的保证可以在较弱的条件下获得。

    The goal of this note is to present a modification of the popular median of means estimator that achieves sub-Gaussian deviation bounds with nearly optimal constants under minimal assumptions on the underlying distribution. We build on a recent work on the topic by the author, and prove that desired guarantees can be attained under weaker requirements.
    
[^31]: 扰动辅助样本合成：一种新的不确定性量化方法

    Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification. (arXiv:2305.18671v1 [stat.ML])

    [http://arxiv.org/abs/2305.18671](http://arxiv.org/abs/2305.18671)

    本文提出了扰动辅助样本合成（PASS）方法，可从复杂数据中绘制可靠结论，并通过估计数据生成分布和蒙特卡罗实验证明任何统计数据的估计分布。进一步推出扰动辅助推理（PAI）框架，可以提供有效性的统计保证。

    

    本文介绍了一种名为“扰动辅助样本合成（PASS）”的新型生成器，旨在从复杂数据中绘制可靠的结论，特别是在使用深度神经网络等高级建模技术时。 PASS利用扰动生成靠近原始数据分布的合成数据，包括数字和非结构化数据类型，如基因表达、图像和文本。通过估计数据生成分布并利用大型预训练生成模型，PASS提高了估计精度，并通过蒙特卡罗实验证明了任何统计数据的估计分布。基于PASS，我们提出了一种生成推理框架称为“扰动辅助推理（PAI）”，它提供了有效性的统计保证。在关键推理中，PAI使得在不知道引导分布（如模拟中）的情况下能够得出准确的结论，即使只有有限的数据。在非关键情况下，我们训练PASS使用中间变量插补策略来提高准确性。实验结果表明，在各种情况下，包括时间序列预测、图像分类和文本生成，PASS和PAI都优于现有最先进的替代品。

    This paper introduces a novel generator called Perturbation-Assisted Sample Synthesis (PASS), designed for drawing reliable conclusions from complex data, especially when using advanced modeling techniques like deep neural networks. PASS utilizes perturbation to generate synthetic data that closely mirrors the distribution of raw data, encompassing numerical and unstructured data types such as gene expression, images, and text. By estimating the data-generating distribution and leveraging large pre-trained generative models, PASS enhances estimation accuracy, providing an estimated distribution of any statistic through Monte Carlo experiments. Building on PASS, we propose a generative inference framework called Perturbation-Assisted Inference (PAI), which offers a statistical guarantee of validity. In pivotal inference, PAI enables accurate conclusions without knowing a pivotal's distribution as in simulations, even with limited data. In non-pivotal situations, we train PASS using an i
    
[^32]: Parity校准

    Parity Calibration. (arXiv:2305.18655v1 [cs.LG])

    [http://arxiv.org/abs/2305.18655](http://arxiv.org/abs/2305.18655)

    本文介绍一种新的校准预测目标——parity校准，其考虑时间序列中未来观测值的增加或减少。我们使用在线二进制校准方法实现了parity校准，并在流行病学、天气预报和核聚变控制等领域中表明该方法的有效性。

    

    在序列回归设置中，决策者可能更关注未来观测值是否比当前值增加或减少，而不是未来观测值的实际值。在此背景下，我们引入了平等校准的概念，它捕捉了时间序列增减事件的校准预测目标。平等概率可以从输出的预测分布中提取，但我们显示这种策略导致理论上的不可预测性和差劲的实际性能。然后我们发现，虽然原任务是回归，但平等校准可以被表达为二进制校准。基于这种联系，我们使用在线二进制校准方法实现了平等校准。我们通过流行病学、天气预报和基于模型的核聚变控制的实际案例展示了我们方法的有效性。

    In a sequential regression setting, a decision-maker may be primarily concerned with whether the future observation will increase or decrease compared to the current one, rather than the actual value of the future observation. In this context, we introduce the notion of parity calibration, which captures the goal of calibrated forecasting for the increase-decrease (or "parity") event in a timeseries. Parity probabilities can be extracted from a forecasted distribution for the output, but we show that such a strategy leads to theoretical unpredictability and poor practical performance. We then observe that although the original task was regression, parity calibration can be expressed as binary calibration. Drawing on this connection, we use an online binary calibration method to achieve parity calibration. We demonstrate the effectiveness of our approach on real-world case studies in epidemiology, weather forecasting, and model-based control in nuclear fusion.
    
[^33]: 全局缩放量化：具有理论保证的分布式学习实用的无浮点量化

    Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees. (arXiv:2305.18627v1 [cs.LG])

    [http://arxiv.org/abs/2305.18627](http://arxiv.org/abs/2305.18627)

    Global-QSGD是一种新颖的全局缩放量化机制，可以提高分布式学习的效率，并且不需要昂贵的误差反馈，并提供了高达$O(\ sqrt{n})$的额外压缩比。

    

    高效的分布式训练是推动深度学习近期进展的主要驱动力。然而，通信常常是系统的主要瓶颈并具有高昂的代价。因此，需要设计高效的通信机制，既能在经验上提高吞吐量，又能提供理论保证。在这项工作中，我们介绍了全局-QSGD，一种新颖的量化运算符，通过全局缩放设计来加速基于分布式学习。我们证明Global-QSGD是第一个理论上严格的Allreduce兼容压缩机制，通过在压缩误差和通信节省之间取得平衡来实现可证明的加速。重要的是，由于其固有的无偏性，Global-QSGD不依赖昂贵的误差反馈，并且相对于流行的QSGD量化能提供高达$O(\sqrt{n})$ 的额外压缩比（其中$n$表示工作者的数量）。为了获得理论保证，我们采用了信息论和凸分析技术。

    Efficient distributed training is a principal driver of recent advances in deep learning. However, communication often proves costly and becomes the primary bottleneck in these systems. As a result, there is a demand for the design of efficient communication mechanisms that can empirically boost throughput while providing theoretical guarantees. In this work, we introduce Global-QSGD, a novel family of quantization operators, engineered to accelerate distributed training based on global scaling. We demonstrate that Global-QSGD is the first theoretically rigorous Allreduce-compatible compression mechanism that achieves a provable speed-up by striking a balance between compression error and communication savings. Importantly, Global-QSGD does not rely on costly error feedback due to its inherent unbiasedness and offers up to $O(\sqrt{n})$ additional compression ratio compared to the popular QSGD quantization ($n$ represents the number of workers). To obtain theoretical guarantees, we gen
    
[^34]: 快速自适应三元分割：隐马尔可夫模型的有效解码程序。

    Quick Adaptive Ternary Segmentation: An Efficient Decoding Procedure For Hidden Markov Models. (arXiv:2305.18578v1 [stat.ME])

    [http://arxiv.org/abs/2305.18578](http://arxiv.org/abs/2305.18578)

    提出了一种名为QATS的新方法，用于高效解码隐藏马尔可夫模型序列。它的计算复杂性为多对数和立方，特别适用于具有相对较少状态的大型HMM。

    

    隐马尔可夫模型（HMM）以不可观察的（隐藏的）马尔可夫链和可观测的过程为特征，后者是隐藏链的噪声版本。从嘈杂的观测中解码原始信号（即隐藏链）是几乎所有基于HMM的数据分析的主要目标。现有的解码算法，如维特比算法，在观测序列长度最多线性的情况下具有计算复杂度，并且在马尔可夫链状态空间的大小中具有次二次计算复杂度。我们提出了快速自适应三元分割（QATS），这是一种分而治之的过程，可在序列长度的多对数计算复杂度和马尔可夫链状态空间的三次计算复杂度下解码隐藏的序列，因此特别适用于具有相对较少状态的大规模HMM。该程序还建议一种有效的数据存储方式，即特定的累积总和。实质上，估计的状态序列按顺序最大化局部似然。

    Hidden Markov models (HMMs) are characterized by an unobservable (hidden) Markov chain and an observable process, which is a noisy version of the hidden chain. Decoding the original signal (i.e., hidden chain) from the noisy observations is one of the main goals in nearly all HMM based data analyses. Existing decoding algorithms such as the Viterbi algorithm have computational complexity at best linear in the length of the observed sequence, and sub-quadratic in the size of the state space of the Markov chain. We present Quick Adaptive Ternary Segmentation (QATS), a divide-and-conquer procedure which decodes the hidden sequence in polylogarithmic computational complexity in the length of the sequence, and cubic in the size of the state space, hence particularly suited for large scale HMMs with relatively few states. The procedure also suggests an effective way of data storage as specific cumulative sums. In essence, the estimated sequence of states sequentially maximizes local likeliho
    
[^35]: 为学习优化构建数学结构

    Towards Constituting Mathematical Structures for Learning to Optimize. (arXiv:2305.18577v1 [cs.LG])

    [http://arxiv.org/abs/2305.18577](http://arxiv.org/abs/2305.18577)

    本文提出了一种结构受到数学启发的L2O模型，其具有广泛的适用性和良好的推广性能，并基于成功的更新规则通常满足的基本数学条件进行了推导。

    

    近年来，利用机器学习从数据中自动学习优化算法的学习优化(L2O)技术引起了人们的广泛关注。一种通用的L2O方法参数化了迭代更新规则，并将更新方向作为黑盒网络进行学习。虽然通用方法具有广泛适用性，但学习的模型可能过拟合，无法很好地推广到分布不同的测试集中。本文推导了成功的更新规则通常满足的基本数学条件。因此，我们提出了一种新的L2O模型，其结构受到数学启发，并且具有广泛的适用性和良好的推广性能。数值模拟验证了我们的理论发现，并展示了所提出的L2O模型的良好实验性能。

    Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approach parameterizes the iterative update rule and learns the update direction as a black-box network. While the generic approach is widely applicable, the learned model can overfit and may not generalize well to out-of-distribution test sets. In this paper, we derive the basic mathematical conditions that successful update rules commonly satisfy. Consequently, we propose a novel L2O model with a mathematics-inspired structure that is broadly applicable and generalized well to out-of-distribution problems. Numerical simulations validate our theoretical findings and demonstrate the superior empirical performance of the proposed L2O model.
    
[^36]: 针对对抗性攻击的强健Lipschitz赌徒算法

    Robust Lipschitz Bandits to Adversarial Corruptions. (arXiv:2305.18543v1 [cs.LG])

    [http://arxiv.org/abs/2305.18543](http://arxiv.org/abs/2305.18543)

    本文提出的强健Lipschitz赌徒算法，能够在对抗性攻击的情况下实现次线性遗憾，并在强敌手情况下最优。

    

    Lipschitz赌徒算法是一种处理定义在度量空间上的连续臂集的随机赌徒算法的变体，其中奖励函数受到Lipschitz约束。本文介绍了一种新的Lipschitz赌徒问题，即在对抗性破坏存在的情况下，自适应敌手将随机奖励损坏到总预算 $C$。 预算通过时间跨度 $T$ 中的破坏水平之和来衡量。 我们考虑弱和强敌手，其中弱敌手在攻击之前不知道当前的行动，而强敌手可以观察行动。我们的工作提出了第一行强健Lipschitz赌徒算法，在两种类型的敌手下，甚至在损坏总预算 $C$ 未向代理披露的情况下，均能实现次线性遗憾。我们在每种类型的敌手下提供下限，并证明了我们的算法在强类型下是最优的。最后，我们进行实验以说明该算法的有效性。

    Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effecti
    
[^37]: 论经验风险最小化的方差、可允许性和稳定性

    On the Variance, Admissibility, and Stability of Empirical Risk Minimization. (arXiv:2305.18508v1 [math.ST])

    [http://arxiv.org/abs/2305.18508](http://arxiv.org/abs/2305.18508)

    本文指出，对于使用平方损失函数的经验风险最小化(ERM)，其次优性必须归因于大的偏差而非方差，并且在ERM的平方误差的偏差-方差分解中，方差项必然具有极小的失误率。作者还提供了Chatterjee的不可允许性定理的简单证明，并表示他们的估计表明ERM的稳定性。

    

    众所周知，使用平方损失的经验风险最小化可能会达到极小的最大失误率。本文的关键信息是，在温和的假设下，ERM的次优性必须归因于大的偏差而非方差。在ERM的平方误差的偏差-方差分解中，方差项必然具有极小的失误率。我们为固定设计提供了一个简单的、使用概率方法证明这一事实的证明。然后，我们在随机设计设置下为各种模型证明了这一结果。此外，我们提供了 Chatterjee 不可允许性定理 (Chatterjee, 2014, Theorem 1.4) 的简单证明，该定理指出，在固定设计设置中，ERM不能被排除为一种最优方法，并将该结果扩展到随机设计设置。我们还表明，我们的估计表明ERM的稳定性，为Caponnetto和Rakhlin(2006)的非Donsker类的主要结果提供了补充。

    It is well known that Empirical Risk Minimization (ERM) with squared loss may attain minimax suboptimal error rates (Birg\'e and Massart, 1993). The key message of this paper is that, under mild assumptions, the suboptimality of ERM must be due to large bias rather than variance. More precisely, in the bias-variance decomposition of the squared error of the ERM, the variance term necessarily enjoys the minimax rate. In the case of fixed design, we provide an elementary proof of this fact using the probabilistic method. Then, we prove this result for various models in the random design setting. In addition, we provide a simple proof of Chatterjee's admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that ERM cannot be ruled out as an optimal method, in the fixed design setting, and extend this result to the random design setting. We also show that our estimates imply stability of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes.
    
[^38]: 宽残差网络的泛化能力

    Generalization Ability of Wide Residual Networks. (arXiv:2305.18506v1 [stat.ML])

    [http://arxiv.org/abs/2305.18506](http://arxiv.org/abs/2305.18506)

    本文研究了在$\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力，表明当宽度$m\rightarrow\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)，并且早停策略的宽残差网络可以达到极小化速率，但在训练过度拟合数据时无法很好地推广。

    

    本文研究了在$\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力。我们首先表明，当宽度$m\rightarrow\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)。这种统一收敛进一步保证了残差网络的泛化误差收敛于相对于RNTK的核回归的误差。作为直接推论，我们指出：$i$)如果目标回归函数落在与RNTK相关联的再生核希尔伯特空间(RKHS)中，采用早停策略的宽残差网络可以达到极小化速率；$ii$)如果训练到过度拟合数据，则无法很好地推广宽残差网络。最后，我们介绍一些实验来调和我们的理论结果与广泛观察到的“良性过拟合现象”之间的矛盾。

    In this paper, we study the generalization ability of the wide residual network on $\mathbb{S}^{d-1}$ with the ReLU activation function. We first show that as the width $m\rightarrow\infty$, the residual network kernel (RNK) uniformly converges to the residual neural tangent kernel (RNTK). This uniform convergence further guarantees that the generalization error of the residual network converges to that of the kernel regression with respect to the RNTK. As direct corollaries, we then show $i)$ the wide residual network with the early stopping strategy can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space (RKHS) associated with the RNTK; $ii)$ the wide residual network can not generalize well if it is trained till overfitting the data. We finally illustrate some experiments to reconcile the contradiction between our theoretical result and the widely observed ``benign overfitting phenomenon''
    
[^39]: 如何有效地在强化学习中进行人类反馈查询？

    How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])

    [http://arxiv.org/abs/2305.18505](http://arxiv.org/abs/2305.18505)

    该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。

    

    人类反馈强化学习（RLHF）是一种范例，在此范例下，RL代理学习使用对轨迹的成对优先级反馈来最优化任务，而不是使用明确的奖励信号。尽管RLHF在微调语言模型方面已经取得了实用成功，但现有的实证研究并未解决如何高效采样轨迹对以查询人类反馈的挑战。在本研究中，我们提出了一种有效的采样方法，用于获取探索性轨迹，在收集任何人类反馈之前，使学习隐藏的奖励函数更加准确。理论分析表明，与现有文献相比，我们的算法在线性参数化和未知过渡的基于偏好模型下学习最优策略所需的人类反馈更少。具体而言，我们的框架可以纳入线性和低秩MDPs。此外，我们研究了使用基于行动比较的反馈的RLHF，并介绍了一种高效的采样方法，以在优化具有有限反馈的任务时获得探索性轨迹。

    Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
    
[^40]: 逃离平庸：双层神经网络如何在 SGD 下学习困难的单指标模型

    Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])

    [http://arxiv.org/abs/2305.18502](http://arxiv.org/abs/2305.18502)

    研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。

    

    本研究探讨了在随机梯度下降（SGD）下双层神经网络学习单指数目标函数的样本复杂度问题，重点关注在初始化时存在许多平坦方向的挑战性情况。已经有研究表明，这种情况下通常需要 $n=O(d\log{d})$ 个样本。但是，我们提供了在高维度和不同宽度情况下的前置因子的精确结果。值得注意的是，我们的发现表明，在这个问题类中，过参数化只会增加一定因子的收敛性。这些见解基于 SGD 动态的低维度随机过程模型，其中逃离平庸等同于计算出站出时间。然而，我们证明这个过程的确定性近似足以代表逃逸时间，这意味着在这种情况下随机性的作用可能很小。

    This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.
    
[^41]: 子采样与岭回归的广义等价性研究

    Generalized equivalences between subsampling and ridge regularization. (arXiv:2305.18496v1 [math.ST])

    [http://arxiv.org/abs/2305.18496](http://arxiv.org/abs/2305.18496)

    此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。

    

    我们针对举集岭估计器，建立了子采样和岭回归之间的精确结构和风险等价性。具体而言，我们证明了，当用不同的岭正则化水平$\lambda$和子采样比例$\psi$拟合子样岭估计器的线性和二次泛函，在$(\lambda,\psi)$-平面上沿着特定路径渐近等价（其中$\psi$是特征维度与子采样大小的比率）。我们的结果仅要求特征和响应分布具有有界矩，并允许任意联合分布。此外，我们提供了一种数据相关的方法来确定$(\lambda,\psi)$的等价路径。我们结果的间接含义是，在数据方面比例中，调优的岭回归呈现出单调预测风险。这解决了Nakkiran等人提出的一个近期未解决的开放性问题，在一般数据分布和温和的正则条件下。

    We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda, \psi )$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a datadependent method to determine the equivalent paths of $(\lambda, \psi )$. An indirect implication of our equivalences is that optimally-tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. under general data distributions and a mild regularity condition that
    
[^42]: 一种自适应后验集中的贝叶斯稀疏因子模型

    A Bayesian sparse factor model with adaptive posterior concentration. (arXiv:2305.18488v1 [stat.ML])

    [http://arxiv.org/abs/2305.18488](http://arxiv.org/abs/2305.18488)

    本文提出了一种自适应后验集中的贝叶斯稀疏因子模型，可以推断因子维数和加载矩阵的稀疏结构，同时保持计算可行性，并获得了优越的性能表现。

    

    本文提出了一种新的贝叶斯推断方法，用于高维稀疏因子模型的推断，既可以推断因子维数，又可以推断加载矩阵的稀疏结构。其创新之处在于引入了一定的依赖关系，使得稀疏水平和因子维数进行自适应后验集中，同时保持计算可行性。我们证明后验分布在渐近意义下集中于真实因子维度，更重要的是，这种后验一致性会随着真实加载矩阵的稀疏水平和噪声方差而自适应。此外，我们还证明了这种方法在更一般的情况下达到了因子维数的最优检测率。同时，我们还获得了近乎最优的协方差矩阵后验集中速率。我们进行了数值实验，并展示了与现有方法的比较结果。

    In this paper, we propose a new Bayesian inference method for a high-dimensional sparse factor model that allows both the factor dimensionality and the sparse structure of the loading matrix to be inferred. The novelty is to introduce a certain dependence between the sparsity level and the factor dimensionality, which leads to adaptive posterior concentration while keeping computational tractability. We show that the posterior distribution asymptotically concentrates on the true factor dimensionality, and more importantly, this posterior consistency is adaptive to the sparsity level of the true loading matrix and the noise variance. We also prove that the proposed Bayesian model attains the optimal detection rate of the factor dimensionality in a more general situation than those found in the literature. Moreover, we obtain a near-optimal posterior concentration rate of the covariance matrix. Numerical studies are conducted and show the superiority of the proposed method compared with 
    
[^43]: 神经傅里叶变换：等变表示学习的通用方法

    Neural Fourier Transform: A General Approach to Equivariant Representation Learning. (arXiv:2305.18484v1 [stat.ML])

    [http://arxiv.org/abs/2305.18484](http://arxiv.org/abs/2305.18484)

    神经傅里叶变换是一种通用的等变表示学习方法，它可以在不需要显式知识的情况下学习组的潜在线性作用，实现对数据隐藏结构的提取。

    

    对称学习已被证明是提取数据隐藏结构的有效方法，其中等变关系概念起着中心作用。然而，大多数当前研究都建立在建筑理论和对数据形式的相应假设之上。我们提出了神经傅里叶变换（NFT），这是一种学习组的潜在线性作用的通用框架，而无需假设关于组如何作用于数据的显式知识。我们展示了NFT的理论基础，并表明等变特征的存在，即在等变性学习中普遍假定的，等价于数据空间中存在一组不变核。我们还提供实验结果，演示了在具有不同程度的关于操作组的知识的典型场景中应用NFT的应用。

    Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group.
    
[^44]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^45]: 噪音在学习循环神经网络的样本复杂度中的作用：长序列的指数差距

    On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences. (arXiv:2305.18423v1 [stat.ML])

    [http://arxiv.org/abs/2305.18423](http://arxiv.org/abs/2305.18423)

    本文研究了添加噪声的多层Sigmoid循环神经网络在学习序列分类问题上的样本复杂度问题，发现带噪声情况下样本复杂度可以用$\log(T/\sigma)$来界定，不存在噪声时下界为$wT$，两者存在指数级别的差距。

    

    我们考虑添加独立噪音的多层Sigmoid循环神经网络来分类长度为T的序列。我们的主要结果表明，这个类的PAC学习的样本复杂度可以被界定为$O (w\log(T/\sigma))$。对于相同类的非噪声版本（即$\sigma=0$），我们证明样本复杂度的下界为$\Omega (wT)$。我们的结果显示出在噪声和非噪声网络的样本复杂度对T的依赖性中存在指数差距。此外，考虑到上限对$1/\sigma$的对数依赖度很小，即使针对数值上可以忽略的$\sigma$，这个差距仍然存在。

    We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\mathcal{N}(0,\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\log(T/\sigma))$. For the non-noisy version of the same class (i.e., $\sigma=0$), we prove a lower bound of $\Omega (wT)$ for the sample complexity. Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\sigma$, this gap still holds even for numerically negligible values of $\sigma$.
    
[^46]: 方差减少的分布式鲁棒Q-learning的样本复杂度

    Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])

    [http://arxiv.org/abs/2305.18420](http://arxiv.org/abs/2305.18420)

    本文提出了两种新颖的无模型算法，为动态决策面对分布变化问题提供了鲁棒的解决方案，并通过将Q-learning与方差减少技术相结合，实现了样本复杂度的有效控制。

    

    在强化学习的理论和应用中，面对分布转移的动态决策是基本问题，因为数据收集所基于的环境分布可能会不同于模型部署所基于的分布。本文提出了两种新颖的无模型算法，即分布式鲁棒Q-learning和它的方差减少对应算法，能够高效地学习鲁棒策略，尽管会面对分布变化。这些算法旨在将带有Kullback-Leibler不确定性集的无限时域$\gamma$-折扣鲁棒马尔科夫决策过程的$q$-函数以元素$\epsilon$-精度有效逼近。进一步地，方差减少的分布式鲁棒Q-learning将同步Q-learning与方差减少技术相结合，以增强其性能，并且我们建立了它达到$ \tilde O(|S||A|(1-\gamma)^{-4}\epsilon^{-4}$的最小最大样本复杂度上界。

    Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\tilde O(|S||A|(1-\gamma)^{-4}\e
    
[^47]: 几何代数变换器

    Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])

    [http://arxiv.org/abs/2305.18415](http://arxiv.org/abs/2305.18415)

    本文介绍了一种通用架构几何代数变换器（GATr），用于解决几何数据问题。GATr使用投影几何代数表示输入输出和状态，具有可缩放性、表达性、多功能性。在n体建模和机器人规划的实验中，GATr相对于非几何基线表现出强大的改进。

    

    几何数据问题涉及计算机视觉、机器人、化学和物理领域。这些数据可以采用许多形式，例如点、方向向量、平面或变换，但迄今为止还没有一种单一的架构，可以应用于如此多种几何类型, 同时尊重它们的对称性。在本文中，我们介绍了几何代数变换器（GATr），一种用于几何数据的通用架构。GATr使用投影几何代数来表示输入、输出和隐藏状态，其提供常见几何对象的高效16维向量空间表示以及作用于它们的运算符。GATr是相对于E(3)（3D欧几里得空间的对称群）等变的。作为变换器，GATr可扩展、表达丰富且多功能。在n体建模和机器人规划的实验中，GATr相对于非几何基线均表现出强大的改进。

    Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines.
    
[^48]: 面向方向的多目标学习：简单且可证明的随机算法

    Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])

    [http://arxiv.org/abs/2305.18409](http://arxiv.org/abs/2305.18409)

    本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。

    

    多目标优化（MOO）已成为许多与多个目标相关的机器学习问题（如多标准学习和多任务学习（MTL））中一个有影响力的框架。本文提出了一种新的面向方向的多目标问题，通过在一个方向的邻域内限制公共下降方向来规范线性组合目标的最优方向，例如MTL中的平均损失。 这个公式包括GD和MGDA作为特殊情况，享受像CAGrad中的面向方向的好处，以及有利于随机算法的设计。为了解决这个问题，我们提出了随机方向导向多目标梯度下降（SDMGrad），它使用简单的SGD类型的更新算法，以及在目标数量较多的情况下，使用高效的目标采样的SDMGrad-OS算法。 对于恒定的正则化参数λ，我们证明SDMGrad和SDMGrad-OS确实收敛到帕累托稳定点。

    Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
    
[^49]: 基于大语言模型的多项选择题答案确认预测研究

    Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])

    [http://arxiv.org/abs/2305.18404](http://arxiv.org/abs/2305.18404)

    本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。

    

    随着大型语言模型的广泛开发，对它们进行健壮的不确定性量化技术将成为它们在高风险场景下安全部署的关键。本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。这种观察对于下游应用，如选择性分类和过滤低质量预测，可能会有用。我们还研究了符合性预测对于超出主题的问题的交换性假设，这可能是许多实际应用的更为现实的场景。本研究为在需要可靠保证错误率的安全关键情况下更加值得信赖和可靠地使用大型语言模型做出了贡献。

    As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
    
[^50]: 关于激活函数和规范化对初始化等距嵌入的影响

    On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])

    [http://arxiv.org/abs/2305.18399](http://arxiv.org/abs/2305.18399)

    本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。

    

    本文探讨了深度神经网络中倒数第二个 Gram 矩阵的结构，该矩阵包含与一批输入对应的输出之间的成对内积。在几种架构中，观察到在初始化时该 Gram 矩阵会随着深度变得退化，从而严重减缓训练速度。规范化层如批处理规范化或层规范化，在防止秩崩溃问题方面起着关键作用。然而现有的理论结果无法全面覆盖广泛用于 transformer 中的层规范化和有限深度下规范化的量化偏差。为了解决这个问题，我们证明了在初始化时，结合激活函数层使用的层规范化可以使多层感知机的 Gram 矩阵偏向指数级深度等距，并使用激活函数的 Hermite 展开来量化这个速度，从而填补了现有理论的空白。

    In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
    
[^51]: 量化时间差分学习在价值估计中的统计优势

    The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation. (arXiv:2305.18388v1 [cs.LG])

    [http://arxiv.org/abs/2305.18388](http://arxiv.org/abs/2305.18388)

    本文研究了强化学习中的时间差分策略评估问题，分析了量化时间差分学习算法在任务中的应用。结果表明，即使从实践者没有超过平均回报之外的回报分布的兴趣之处，在表格设置中，QTD也可以提供比传统TD学习等方法更好的性能。

    

    本文研究强化学习中基于时间差分的策略评估问题，特别是分析了一种分布式强化学习算法——量化时间差分学习（QTD）在这个任务中的应用。我们得出了一个惊人的结论：即使从实践者没有超过平均回报之外的回报分布的兴趣之处，在表格设置中，QTD（学习关于全部回报分布的预测）也可以提供比诸如传统TD学习（仅预测平均回报）等方法更好的性能。

    We study the problem of temporal-difference-based policy evaluation in reinforcement learning. In particular, we analyse the use of a distributional reinforcement learning algorithm, quantile temporal-difference learning (QTD), for this task. We reach the surprising conclusion that even if a practitioner has no interest in the return distribution beyond the mean, QTD (which learns predictions about the full distribution of returns) may offer performance superior to approaches such as classical TD learning, which predict only the mean return, even in the tabular setting.
    
[^52]: 神经网络修剪的三重模型

    A Three-regime Model of Network Pruning. (arXiv:2305.18383v1 [stat.ML])

    [http://arxiv.org/abs/2305.18383](http://arxiv.org/abs/2305.18383)

    该论文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响，通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型，揭示了修剪的优化过程以及对神经网络损失景观的变化规律。

    

    最近的研究强调了训练超参数（例如训练轮数）对机器学习模型修剪的影响，然而如何精确预测调整某一特定超参数对修剪的影响仍具有挑战性。为了解决这个问题，本文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响。我们发现了一个关键的实证结果：根据修剪后的模型中的一种负载类参数的值，当增加修剪前模型中一种类似温度的参数的值时，修剪性能可能会得到优化或损害。基于这种转变，我们通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型。该模型揭示了修剪的优化过程以及与修剪相关的神经网络损失景观的变化规律。

    Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals tha
    
[^53]: 精确增广拉格朗日和随机迭代草图算法求解约束优化问题

    Constrained Optimization via Exact Augmented Lagrangian and Randomized Iterative Sketching. (arXiv:2305.18379v1 [math.OC])

    [http://arxiv.org/abs/2305.18379](http://arxiv.org/abs/2305.18379)

    本文提出了一种自适应的不精确牛顿法来求解等式约束的非线性、非凸优化问题，通过随机迭代草图求解增广拉格朗日牛顿系统，并通过在精确增广拉格朗日优势函数上执行线搜索来选择合适的步长。该方法具有高效、鲁棒性好的特点。

    

    本文考虑解决等式约束的非线性、非凸优化问题。这类问题在机器学习和工程领域的各种应用中广泛出现，包括受约束的深度神经网络、最优控制和PDE约束优化。我们针对这类问题开发了一种自适应的不精确牛顿法。在每次迭代中，我们通过随机迭代草图求解增广拉格朗日牛顿系统，并通过在精确增广拉格朗日优势函数上执行线搜索来选择合适的步长。当配备适当的草图矩阵时，随机求解器相对于确定性线性系统求解器具有明显优势，可以显著减少每次迭代的浮点运算复杂度和存储成本。我们的方法自适应地控制随机求解器的精度和增广拉格朗日的惩罚参数，以确保不精确的牛顿方向是精确增广拉格朗日函数的下降方向。理论分析和数值实验均证明了所提出方法的效率和鲁棒性。

    We consider solving equality-constrained nonlinear, nonconvex optimization problems. This class of problems appears widely in a variety of applications in machine learning and engineering, ranging from constrained deep neural networks, to optimal control, to PDE-constrained optimization. We develop an adaptive inexact Newton method for this problem class. In each iteration, we solve the Lagrangian Newton system inexactly via a randomized iterative sketching solver, and select a suitable stepsize by performing line search on an exact augmented Lagrangian merit function. The randomized solvers have advantages over deterministic linear system solvers by significantly reducing per-iteration flops complexity and storage cost, when equipped with suitable sketching matrices. Our method adaptively controls the accuracy of the randomized solver and the penalty parameters of the exact augmented Lagrangian, to ensure that the inexact Newton direction is a descent direction of the exact augmented 
    
[^54]: 通过潜在量化进行解缠

    Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])

    [http://arxiv.org/abs/2305.18378](http://arxiv.org/abs/2305.18378)

    本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。

    

    在解缠表示学习中，模型需要将数据集的基础变化因素分开并独立地表示出来，而模型并没有提供有关这些因素的真实信息，归纳偏见在实现解缠方面发挥着重要作用。在本文中，我们通过施加严格的交流瓶颈和强大的模型规范化，构建了一种朝着组合编码和解码数据的归纳偏见。具体来说，我们对潜在维度进行可学习的离散编码，并为每个维度应用一个单独的标量码书。潜在量化迫使编码器在许多数据点上使用少量潜在值，从而使解码器能够为每个值分配一致的含义。规范化有助于将模型引向这种简明策略。我们在多个基准数据集上展示了该方法的广泛应用性，并且展示了我们的方法显著提高了一系列标准VAE模型学习的表征的可解释性。

    In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
    
[^55]: 学习跳跃: 薄化和加厚潜在计数用于生成建模

    Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling. (arXiv:2305.18375v1 [cs.LG])

    [http://arxiv.org/abs/2305.18375](http://arxiv.org/abs/2305.18375)

    本文探讨了如何使用学习跳跃方法来生成建模各种类型的数据，特别是对于计数和非负连续数据等高稀疏度、倾斜度、重尾度或过度分散度的数据，使用学习跳跃相比于学习去噪有更好的效果。

    

    学习去噪已成为设计最先进的深度生成模型（如扩散模型）的重要范式，用于建模连续的实值数据和分类数据已经有很好的研究。然而，本文发现学习去噪在建模某些其他类型的数据（如计数和非负连续数据）时能力有限，这些数据经常是高度稀疏、倾斜、重尾或过度分散的。为此，我们提出了学习跳跃作为各种类型数据的生成建模的通用方法。使用正向计数稀化方法构建学习目标，训练深度神经网络，通过逆向计数加厚过程迭代地改进其生成结果。我们演示了什么情况下学习跳跃与学习去噪表现相当，并且什么情况下学习跳跃表现更好。例如，建议在建模计数和非负连续数据时使用学习跳跃，这些数据往往具有稀疏性、倾斜性、重尾性或过度分散性。

    Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network. We demonstrate when learning to jump is expected to perform comparably to learning to denoise, and when it is expected to perform better. For example, learning to jump is recom
    
[^56]: 通过 $\ell_1-\ell_2$ 优化进行结构模型选择

    Structured model selection via $\ell_1-\ell_2$ optimization. (arXiv:2305.17467v1 [stat.ML])

    [http://arxiv.org/abs/2305.17467](http://arxiv.org/abs/2305.17467)

    通过稀疏最小二乘拟合一大组候选函数，使用 $\ell_1-\ell_2$ 稀疏优化方法进行结构模型选择，实现从不充分且嘈杂的时空数据中识别结构化动态系统；该方法在合成数据集上得到了验证，并证明具有理论保证和高效性。

    

    自动化模型选择在科学和工程中具有重要应用。本文提出了一种学习方法，通过稀疏最小二乘拟合一大组候选函数，用一种非凸 $\ell_1-\ell_2$ 稀疏优化方法求解，通过交替方向乘法的方法进行。我们证明，如果候选函数集合形成边界正交系统的结构随机采样矩阵，就可以通过伯恩斯坦样式的不等式和一致性条件稳定恢复，并且误差有界。该学习方法在由粘性Burgers'方程和两个反应扩散方程产生的合成数据上进行了验证。计算结果证明了成功的理论保证和相对于环境维数和候选函数数量的效率。

    Automated model selection is an important application in science and engineering. In this work, we develop a learning approach for identifying structured dynamical systems from undersampled and noisy spatiotemporal data. The learning is performed by a sparse least-squares fitting over a large set of candidate functions via a nonconvex $\ell_1-\ell_2$ sparse optimization solved by the alternating direction method of multipliers. Using a Bernstein-like inequality with a coherence condition, we show that if the set of candidate functions forms a structured random sampling matrix of a bounded orthogonal system, the recovery is stable and the error is bounded. The learning approach is validated on synthetic data generated by the viscous Burgers' equation and two reaction-diffusion equations. The computational results demonstrate the theoretical guarantees of success and the efficiency with respect to the ambient dimension and the number of candidate functions.
    
[^57]: 具有对抗性损失和转换的无遗憾在线强化学习

    No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])

    [http://arxiv.org/abs/2305.17380](http://arxiv.org/abs/2305.17380)

    本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。

    

    现有的对抗性马尔可夫决策过程的在线学习算法可以在与对手的$ T $轮交互之后实现${ O}(\sqrt{T})$的后悔，即使损失函数是由对手任意选择的，但前提是转移函数必须固定。这是因为已经有研究表明，对抗性转移函数使无悔学习变得不可能。尽管存在这种不可能性结果，我们开发了可以处理对抗性损失和对抗性转换的算法，后悔逐渐增加与对手的恶意程度成比例。更具体地说，我们首先提出了一种算法，它的后悔为$\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$，其中$C^{\textsf{P}}$表示转换函数的对抗性，最多可以为${O}(T)$。虽然此算法本身需要$C^{\textsf{P}}$的知识，但我们还开发了一种黑盒缩减方法来消除此要求。此外，我们还展示了一种进一步的方法，使得算法能够处理任意长度的锚定期。

    Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth
    
[^58]: 大部分神经网络几乎是可学习的

    Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])

    [http://arxiv.org/abs/2305.16508](http://arxiv.org/abs/2305.16508)

    本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。

    

    我们提出了一个PTAS来学习随机常数深度网络。我们证明了对于任何固定的$\epsilon>0$和深度$i$，存在一个多项式时间算法，对于$\sqrt{d} \cdot \mathbb{S}^{d-1}$上的任何分布，学习随机Xavier网络的深度$i$，误差为$\epsilon$。该算法的时间和样本复杂度为$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$，其中$\bar d$是网络的大小。对于某些类似于Sigmoid和ReLU的激活函数，可以将误差界限改进为$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$，从而得到一种几乎多项式时间算法来学习常数深度随机网络。

    We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
    
[^59]: 带扰动生成树的可微聚类方法

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。

    

    我们介绍了一种基于最小权重生成树的可微聚类方法，它是生成树的一种变体，具有多个连通分量。我们的方法依赖于线性规划解的随机扰动，以实现平滑和高效的梯度计算。这使我们能够在端到端可训练的流水线中包含聚类。我们证明了我们的方法即使在嘈杂的数据集和具有挑战性的几何环境下也能良好地工作。我们还利用这种方法制定了一个特别的损失，以有效地从部分聚类数据学习。我们在几个现实世界的数据集上展示了它在监督和半监督任务中的表现。

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^60]: 分布式强化学习的好处：小损失边界

    The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])

    [http://arxiv.org/abs/2305.15703](http://arxiv.org/abs/2305.15703)

    通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。

    

    虽然分布式强化学习已经取得了实证成果，但其何时何地有益的问题尚未得到回答。在这项工作中，通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，我们的边界会比非分布式方法更强。作为热身，我们展示了学习成本分布会在情境展开（CB）中导致小损失后悔边界，我们发现分布式CB在三个具有挑战性的任务上比最先进的技术在实证上表现更好。对于在线RL，我们提出了一个分布式版本空间算法，该算法使用最大似然估计构建置信区间，并证明了它在表格MDP中实现了小损失后悔，同时在潜变量模型中享有小损失PAC边界。以类似的见解为基础，我们提出了一个分布式离线RL算法

    While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
    
[^61]: 关于高斯-斯坦变分梯度下降动态性的探究

    Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])

    [http://arxiv.org/abs/2305.14076](http://arxiv.org/abs/2305.14076)

    本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。

    

    Stein Variational Gradient Descent (SVGD)是一种非参数基于粒子的确定性采样算法。尽管其被广泛使用，但理解SVGD的理论属性一直是一个具有挑战性的问题。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。受此事实的启发，我们通过双线性核将SVGD投影到高斯分布族中，即高斯变分推断 (GVI) 与 SVGD。我们通过考虑均场 PDE 和离散粒子系统，提供了一个完整的图像。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。我们的分析基于一个新的代数恒等式，该等式将目标高斯分布的费希尔信息矩阵与粒子均匀分布的费希尔信息矩阵相关联。这个等式为我们提供了透视 GVI with SVGD 在均场和粒子设置中的动态性的统一视角。

    Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
    
[^62]: 面向有条件生成对抗网络的少样本连续学习

    Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])

    [http://arxiv.org/abs/2305.11400](http://arxiv.org/abs/2305.11400)

    本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。

    

    在生成模型的少样本连续学习中，必须学习目标模式，并在不影响先前学习到的模式的情况下仅使用有限的样本。本文针对条件生成对抗网络提出了一种新的连续学习方法，基于一种新的用于生成建模的模式亲和力量度。我们的度量完全基于cGAN的判别器，可以识别最接近目标的现有模式。随后，我们通过包含基于最接近模式的加权标签来扩展连续学习模型。为了预防灾难性遗忘，我们首先使用cGAN的生成器生成带标签的数据样本，然后通过回放生成的数据来训练目标模式的cGAN模型。我们的实验结果证明了我们的方法在提高生成性能方面的有效性，超越了各种标准和最先进的方法。

    In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
    
[^63]: 用于加权因果 DAG 的新度量和搜索算法

    New metrics and search algorithms for weighted causal DAGs. (arXiv:2305.04445v1 [cs.LG])

    [http://arxiv.org/abs/2305.04445](http://arxiv.org/abs/2305.04445)

    本研究提供了针对加权因果 DAGs的新度量和搜索算法，发现了用于自适应干预的因果图，提供了一个新的基准来捕捉搜索算法的最坏干预成本，并提供自适应搜索算法实现对数逼近。

    

    从数据中恢复因果关系是一个重要的问题。在使用观测数据时，只能恢复到一个马尔科夫等价类的因果图，并且需要额外的假设或干预数据来完成恢复。本文在一些标准假设下，通过节点相关干预成本的自适应干预，研究因果图发现。对于这种情况，我们证明没有算法能够比验证次数的顺序更好地实现渐近保证，验证次数是自适应搜索算法的一个成熟基准。在这个负面结果的基础上，我们定义了一个捕捉任何搜索算法最坏干预成本的新基准。此外，针对这个新基准，我们提供了自适应搜索算法，在各种设置下都能实现对数逼近：原子、有界大小的干预和广义成本。

    Recovering causal relationships from data is an important problem. Using observational data, one can typically only recover causal graphs up to a Markov equivalence class and additional assumptions or interventional data are needed for complete recovery. In this work, under some standard assumptions, we study causal graph discovery via adaptive interventions with node-dependent interventional costs. For this setting, we show that no algorithm can achieve an approximation guarantee that is asymptotically better than linear in the number of vertices with respect to the verification number; a well-established benchmark for adaptive search algorithms. Motivated by this negative result, we define a new benchmark that captures the worst-case interventional cost for any search algorithm. Furthermore, with respect to this new benchmark, we provide adaptive search algorithms that achieve logarithmic approximations under various settings: atomic, bounded size interventions and generalized cost o
    
[^64]: Wasserstein PAC-Bayes 学习：泛化与优化之间的桥梁。

    Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])

    [http://arxiv.org/abs/2304.07048](http://arxiv.org/abs/2304.07048)

    本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。

    

    PAC-Bayes 学习是一种已建立的框架，用于在训练阶段评估学习算法的泛化能力。然而，在训练之前，弄清楚为什么知名算法的输出具有良好的泛化特性而 PAC-Bayes 是否有用仍然具有挑战性。我们通过扩展简要介绍在文献 \cite{amit2022ipm} 中提出的 \emph{Wasserstein PAC-Bayes} 框架来积极回答这个问题。我们提供了新的泛化界限，利用损失函数上的几何假设。使用我们的框架，我们在任何训练之前就证明了 \cite{lambert2022variational} 中算法的输出具有强大的渐近泛化能力。更具体地说，我们展示了如何在泛化框架中将优化结果结合起来，构建了 PAC-Bayes 和优化算法之间的桥梁。

    PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \emph{Wasserstein PAC-Bayes} framework, briefly introduced in \cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.
    
[^65]: 我们实现了个性化治疗吗？使用重复采样的在线强化学习算法进行个性化评估

    Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])

    [http://arxiv.org/abs/2304.05365](http://arxiv.org/abs/2304.05365)

    本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。

    

    在数字健康中，使用强化学习（RL）个性化治疗序列以支持用户采取更健康的行为越来越受到关注。这种连续决策问题涉及到基于用户的上下文（例如，先前的活动水平、位置等）在何时治疗以及如何治疗的决定。在线RL算法是这个问题的一个有前途的数据驱动方法，因为它基于每个用户的历史反馈进行学习，并利用这些知识个性化这些决策。然而，要决定是否应在实际部署的“优化”干预中包含RL算法，我们必须评估数据证据，表明RL算法实际上正在将治疗个性化适应其用户。由于RL算法中的随机性，人们可能会对其在某些状态下的学习并使用此学习来提供特定治疗的能力产生误解。我们使用工作定义的个性化，并介绍了一种重复采样政策评估方法来评估在线RL算法实现的个性化水平。我们使用模拟评估了我们提出的方法，并展示了我们的方法可以准确地识别个性化的策略。我们提出的方法在优化数字健康的个性化干预方面具有潜在应用。

    There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
    
[^66]: Hamiltonian深度神经网络的万能逼近性质研究

    Universal Approximation Property of Hamiltonian Deep Neural Networks. (arXiv:2303.12147v1 [cs.LG])

    [http://arxiv.org/abs/2303.12147](http://arxiv.org/abs/2303.12147)

    本文研究了离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络的通用逼近能力，证明了其中的一部分流可以逐渐逼近紧致域上的任何连续函数，为实际使用提供了理论基础。

    

    本文研究了由离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络（HDNN）的通用逼近能力。最近的研究表明，HDNN因设计而具有非消失梯度，在训练过程中提供数值稳定性。然而，尽管在几个应用中HDNN已经展示了最先进的性能，但缺少量化其表现力的全面研究。因此，我们提供了一个HDNN的通用逼近定理，并证明了HDNN的一部分流可以逐渐逼近紧致域上的任何连续函数。此结果为实际使用HDNN提供了牢固的理论基础。

    This paper investigates the universal approximation capabilities of Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of Hamiltonian Neural Ordinary Differential Equations. Recently, it has been shown that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical stability during training. However, although HDNNs have demonstrated state-of-the-art performance in several applications, a comprehensive study to quantify their expressivity is missing. In this regard, we provide a universal approximation theorem for HDNNs and prove that a portion of the flow of HDNNs can approximate arbitrary well any continuous function over a compact domain. This result provides a solid theoretical foundation for the practical use of HDNNs.
    
[^67]: 神经网络函数的Lipschitz连续性的一些基本方面

    Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10886](http://arxiv.org/abs/2302.10886)

    本文深入研究和描述神经网络实现的函数的Lipschitz行为，在多种设置下进行实证研究，并揭示了神经网络函数Lipschitz连续性的基本和有趣的特性，其中最引人注目的是在Lipschitz常数的上限和下限中识别出了明显的双下降趋势。

    

    Lipschitz连续性是任何预测模型的一个简单但关键的功能性质，它处于模型的稳健性、泛化性和对抗性脆弱性的核心。本文旨在深入研究和描述神经网络实现的函数的Lipschitz行为。因此，我们通过耗尽最简单和最一般的下限和上限的极限，在各种不同设置下进行实证研究（即，体系结构、损失、优化器、标签噪音等），虽然这一选择主要是受计算难度结果的驱动，但它也非常丰富，并揭示了神经网络函数Lipschitz连续性的几个基本和有趣的特性，我们还补充了适当的理论论证。

    Lipschitz continuity is a simple yet crucial functional property of any predictive model for it lies at the core of the model's robustness, generalisation, as well as adversarial vulnerability. Our aim is to thoroughly investigate and characterise the Lipschitz behaviour of the functions realised by neural networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, losses, optimisers, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. Although motivated primarily by computational hardness results, this choice nevertheless turns out to be rather resourceful and sheds light on several fundamental and intriguing traits of the Lipschitz continuity of neural network functions, which we also supplement with suitable theoretical arguments. As a highlight of this investigation, we identify a striking double descent trend in both upper and lower bounds to the Lipschitz constant with in
    
[^68]: 样本外$R^2$：估计和推断

    The out-of-sample $R^2$: estimation and inference. (arXiv:2302.05131v1 [stat.ME] CROSS LISTED)

    [http://arxiv.org/abs/2302.05131](http://arxiv.org/abs/2302.05131)

    本文提出了用于比较两个预测模型的样本外$r^2$的无偏估计器，并利用最近关于数据不确定性的理论进展。

    

    样本外预测是预测模型的一项重要测试，然而独立的测试数据集通常不可用于评估预测误差。因此，通常使用数据拆分算法（如交叉验证或自助法）来估计样本外表现。对于定量结果，可解释的方差与总方差的比率可以通过确定系数或样本内$R^2$来总结，这易于解释并可在不同结果变量之间进行比较。与样本内$R^2$相反，样本外$R^2$没有很好地定义，样本外$\hat{R}^2$的变异性被大量忽视。通常仅报告其点估计值，这阻碍了不同结果变量可预测性的正式比较。在本文中，我们明确将样本外$R^2$定义为两个预测模型的比较，并提供无偏估计器，利用最近关于数据不确定性的理论进展。

    Out-of-sample prediction is the acid test of predictive models, yet an independent test dataset is often not available for assessment of the prediction error. For this reason, out-of-sample performance is commonly estimated using data splitting algorithms such as cross-validation or the bootstrap. For quantitative outcomes, the ratio of variance explained to total variance can be summarized by the coefficient of determination or in-sample $R^2$, which is easy to interpret and to compare across different outcome variables. As opposed to the in-sample $R^2$, the out-of-sample $R^2$ has not been well defined and the variability on the out-of-sample $\hat{R}^2$ has been largely ignored. Usually only its point estimate is reported, hampering formal comparison of predictability of different outcome variables. Here we explicitly define the out-of-sample $R^2$ as a comparison of two predictive models, provide an unbiased estimator and exploit recent theoretical advances on uncertainty of data 
    
[^69]: 信息理论重要性采样聚类

    Information Theoretical Importance Sampling Clustering. (arXiv:2302.04421v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04421](http://arxiv.org/abs/2302.04421)

    本文提出了一种信息理论重要性采样聚类方法，该方法最小化在分布偏差约束下期望失真的最坏情况。

    

    大多数聚类方法假设训练数据和未来数据来自相同的分布，但在大多数真实场景下，这种假设可能不成立。本文提出了一种基于信息理论重要性采样的聚类方法（ITISC），其在分布偏差的约束下最小化期望失真的最坏情况。分布偏差约束可以转换为以基于重要性采样的均匀分布为中心的一组权重分布的约束。所提出方法的目标是在最大降级下最小化损失，因此得到的问题是一个带有约束的最小最大化优化问题，可以使用拉格朗日方法将其重新表达为无约束问题。该优化问题可以通过交替优化算法或使用商业可用软件的通用优化例程来解决。

    A current assumption of most clustering methods is that the training data and future data are taken from the same distribution. However, this assumption may not hold in most real-world scenarios. In this paper, we propose an information theoretical importance sampling based approach for clustering problems (ITISC) which minimizes the worst case of expected distortions under the constraint of distribution deviation. The distribution deviation constraint can be converted to the constraint over a set of weight distributions centered on the uniform distribution derived from importance sampling. The objective of the proposed approach is to minimize the loss under maximum degradation hence the resulting problem is a constrained minimax optimization problem which can be reformulated to an unconstrained problem using the Lagrange method. The optimization problem can be solved by both an alternative optimization algorithm or a general optimization routine by commercially available software. Exp
    
[^70]: PAC-Bayesian软演员-评论家学习

    PAC-Bayesian Soft Actor-Critic Learning. (arXiv:2301.12776v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12776](http://arxiv.org/abs/2301.12776)

    本文提出了一种使用PAC-Bayesian bound作为Soft Actor-Critic (SAC)算法评论家训练目标的方法，以解决训练不稳定的问题，并通过评论家引导的随机搜索探索多个未来来提高在线学习性能。在多个经典控制和运动任务中，该算法具有样本效率和遗憾最小化方面的明显优势。

    

    演员-评论家算法通过两个分别作策略评估和改进的功能逼近器来解决增强学习(RL)的双重目标。此方法的实用性是以训练不稳定为代价的，主要原因是评论家逼近误差对演员的破坏性影响。我们通过首次采用一个现有的可能近似正确(PAC)Bayesian界限作为Soft Actor-Critic (SAC)算法的评论家训练目标来解决这个瓶颈。此外，我们进一步证明了当随机演员通过评论家引导的随机搜索探索多个未来时，在线学习性能显著提高。我们观察到我们得到的算法在多个经典控制和运动任务中，在样本效率和遗憾最小化方面与现有技术相比具有明显优势。

    Actor-critic algorithms address the dual goals of reinforcement learning (RL), policy evaluation and improvement, via two separate function approximators. The practicality of this approach comes at the expense of training instability, caused mainly by the destructive effect of the approximation errors of the critic on the actor. We tackle this bottleneck by employing an existing Probably Approximately Correct (PAC) Bayesian bound for the first time as the critic training objective of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning performance improves significantly when a stochastic actor explores multiple futures by critic-guided random search. We observe our resulting algorithm to compare favorably to the state of the art on multiple classical control and locomotion tasks in terms of both sample efficiency and regret minimization.
    
[^71]: ZegOT:使用文本提示的最优传输实现零样本分割。

    ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts. (arXiv:2301.12171v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12171](http://arxiv.org/abs/2301.12171)

    这篇论文提出了一种通过最优传输的方法，利用多个文本提示来实现零样本分割，达到了最先进的性能水平。

    

    通过将图像和文本对齐的方法，利用大规模对比性语言-图像预训练（CLIP）的成功为零样本语义分割带来了很大的希望，然而现有的方法通常需要额外的图像编码器或对CLIP模块进行重新训练或微调。本论文提出了一种新的ZegOT方法，通过最优传输将多个文本提示与冻结的图像嵌入匹配，从而实现零样本分割。特别是，通过引入一种新的多提示最优传输求解器（MPOT），该方法为每个文本提示与冻结的图像编码器隐藏层的视觉特征映射之间学习了一种最优映射。这种独特的映射方法有效地使每个文本提示关注不同的视觉语义属性。通过在基准数据集上进行广泛的实验，我们展示了我们的方法优于现有方法，达到了最先进的性能水平。

    Recent success of large-scale Contrastive Language-Image Pre-training (CLIP) has led to great promise in zero-shot semantic segmentation by transferring image-text aligned knowledge to pixel-level classification. However, existing methods usually require an additional image encoder or retraining/tuning the CLIP module. Here, we propose a novel Zero-shot segmentation with Optimal Transport (ZegOT) method that matches multiple text prompts with frozen image embeddings through optimal transport. In particular, we introduce a novel Multiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an optimal mapping between multiple text prompts and visual feature maps of the frozen image encoder hidden layers. This unique mapping method facilitates each of the multiple text prompts to effectively focus on distinct visual semantic attributes. Through extensive experiments on benchmark datasets, we show that our method achieves the state-of-the-art (SOTA) performance over existing 
    
[^72]: 基于硬币采样的无需学习速率的基于梯度的贝叶斯推断方法

    Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates. (arXiv:2301.11294v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11294](http://arxiv.org/abs/2301.11294)

    本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。

    

    近年来，基于粒子的变分推断（ParVI）方法如Stein变分梯度下降（SVGD）因可扩展性在贝叶斯推理中越来越受欢迎。然而，这些方法的性质不可避免地取决于超参数（如学习速率），必须由从业者仔细调整，以确保以合适的速率收敛到目标测度。在本文中，我们引入了一组新的基于硬币投注的可扩展贝叶斯推断方法，这些方法完全不需要学习速率。我们在一系列数值例子中演示了我们方法的性能，包括几个高维模型和数据集，证明了与其他ParVI算法相当的性能，而无需调整学习速率。

    In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.
    
[^73]: 关于具有期望条件风险度量的风险厌恶策略梯度方法的全局收敛性

    On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures. (arXiv:2301.10932v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10932](http://arxiv.org/abs/2301.10932)

    本论文研究了具有期望条件风险度量的风险厌恶策略梯度方法，提出了策略梯度更新，证明了其在约束和无约束情况下的全局收敛性和迭代复杂度，并测试了REINFORCE和actor-critic算法的风险厌恶变体来展示方法的实用价值和风险控制的重要性。

    

    风险敏感的强化学习已经成为控制不确定结果和确保各种顺序决策问题的可靠性能的流行工具。虽然针对风险敏感的强化学习已经开发出了策略梯度方法，但这些方法是否具有与风险中性情况下相同的全局收敛保证还不清楚。本文考虑了一类动态时间一致风险度量，称为期望条件风险度量（ECRM），并为基于ECRM的目标函数推导出策略梯度更新。在约束直接参数化和无约束softmax参数化下，我们提供了相应的风险厌恶策略梯度算法的全局收敛性和迭代复杂度。我们进一步测试了REINFORCE和actor-critic算法的风险厌恶变体，以展示我们的方法的有效性和风险控制的重要性。

    Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms. We further test risk-averse variants of REINFORCE and actor-critic algorithms to demonstrate the efficacy of our method and the importance of risk control.
    
[^74]: 学习Boltzmann密度的变形轨迹

    Learning Deformation Trajectories of Boltzmann Densities. (arXiv:2301.07388v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07388](http://arxiv.org/abs/2301.07388)

    本文介绍了一种学习Boltzmann密度变形轨迹的方法，其中通过插值能量函数等实现Boltzmann密度的变形，然后找到一个时间依赖向量场，将样本从一个分布转移到另一个分布，其表现在高斯混合和量子力学粒子的Boltzmann密度上比KL-反散度更具优势。

    

    我们提出了一种连续标准化流的训练方法，可以在没有样本但存在能量函数的情况下使用。我们的方法依赖于能量函数$f_1$和广义高斯函数$f_0$之间的预定或学习插值$f_t$。能量函数的插值引起Boltzmann密度$p_t\propto e^{-f_t}$的插值，我们旨在找到一个沿着族$p_t$的时间依赖向量场$V_t$，将样本从一个分布转移到另一个分布。将样本沿着族$p_t$从一个分布转移到另一个分布的条件可以转化为$V_t$和$f_t$之间的PDE，我们优化$V_t$和$f_t$以满足此PDE。我们在高斯混合和双井势的量子力学粒子的Boltzmann密度上实验比较了所提出的训练目标与KL-反散度的差异。

    We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ can be translated to a PDE between $V_t$ and $f_t$ and we optimize $V_t$ and $f_t$ to satisfy this PDE. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.
    
[^75]: 时序数据的序列预测置信推断算法

    Sequential Predictive Conformal Inference for Time Series. (arXiv:2212.03463v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.03463](http://arxiv.org/abs/2212.03463)

    提出了一种新的适用于时序数据的自适应重新估计条件分位数的置信预测算法SPCI，相较于其他现有方法，SPCI在所需经验覆盖下的区间宽度显著减小。

    

    我们提出了一种新的分布自由的序列数据（例如时间序列）置信预测算法，称为“序列预测置信推断”（SPCI）。我们特别考虑到时间序列数据是不可交换的性质，因此许多现有的置信预测算法不适用。主要思想是在利用它们之间的时间依赖性时，自适应重新估计非一致性分数（例如预测残差）的条件分位数。更具体地，我们将置信预测区间的问题视为预测未来残差的分位数，给定用户指定的点预测算法。从理论上讲，在扩展分位数回归的一致性分析的基础上，我们建立了渐近有效的条件覆盖。通过模拟和真实数据实验，我们证明了SPCI相对于其他现有方法在所需经验覆盖下的区间宽度显著减小。

    We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the \textit{sequential predictive conformal inference} (\texttt{SPCI}). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of \texttt{SPCI} compared to other existing methods under the desired empirical cover
    
[^76]: 回溯式反事实推理

    Backtracking Counterfactuals. (arXiv:2211.00472v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.00472](http://arxiv.org/abs/2211.00472)

    本文介绍了一种基于回溯式反事实推理的形式化方法，该方法基于图形模型，提供了一种更加自然和直观的推理过去时反事实情景的方法。

    

    反事实推理是人类思维中广泛存在的一种推理方式——设想一些假设场景或可能存在的情况，这些情况与实际情况不同。传统上，反事实情景被视为局部违反自然规律的“小奇迹”，但它们具有相同的初始条件。而在Pearl的结构因果模型(SCM)框架中，这通过修改因果定律的干预而使得外生变量的值共享得到了数学上的严格化。但近年来，哲学家和心理学家对这种单纯的干预主义反事实观点提出了越来越多的质疑。相反，他们提出了一种回溯式反事实观点，即在反事实世界中因果定律保持不变，将与实际情况的差异“回溯”到改变的初始条件(外生变量)。在本文中，我们提出了一种基于简单但灵活的图形模型的回溯式反事实推理的形式化方法。我们认为，在某些情况下，特别是在推理过去时，我们的模型提供了一种更自然、更直观的反事实推理方法。

    Counterfactual reasoning -- envisioning hypothetical scenarios, or possible worlds, where some circumstances are different from what (f)actually occurred (counter-to-fact) -- is ubiquitous in human cognition. Conventionally, counterfactually-altered circumstances have been treated as "small miracles" that locally violate the laws of nature while sharing the same initial conditions. In Pearl's structural causal model (SCM) framework this is made mathematically rigorous via interventions that modify the causal laws while the values of exogenous variables are shared. In recent years, however, this purely interventionist account of counterfactuals has increasingly come under scrutiny from both philosophers and psychologists. Instead, they suggest a backtracking account of counterfactuals, according to which the causal laws remain unchanged in the counterfactual world; differences to the factual world are instead "backtracked" to altered initial conditions (exogenous variables). In the pres
    
[^77]: 自适应选择最优策略以提高随机试验的精度和功效

    Adaptive Selection of the Optimal Strategy to Improve Precision and Power in Randomized Trials. (arXiv:2210.17453v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.17453](http://arxiv.org/abs/2210.17453)

    本研究提供了一种自适应预设方法，以选择在随机试验中调整哪些变量，以及以何种形式进行调整，从而最大化精度，同时保持Ⅰ类错误控制。

    

    Benkeser等人展示了如何在随机试验中调整基线协变量，从而有意义地提高各种结果类型的精度。他们的研究建立在很长时间的历史基础上，始于1932年的R.A. Fisher，包括美国食品和药物管理局以及欧洲药品管理局最近的认可。本文着重探讨了一个重要的实际问题：如何选择调整方法，即哪些变量以及以何种形式，以最大化精度，同时保持Ⅰ类错误控制。Balzer等人以前提出了在TMLE中的自适应预设法，以灵活自动地从预先规定的集合中选择在小型试验（N < 40）中最大化经验效率的方法。为了避免在少数随机单位中过度拟合，之前的选择仅限于工作广义线性模型，调整单个协变量。现在，我们将自适应预设法针对具有许多随机单元的试验进行了调整。使用V-fold

    Benkeser et al. demonstrate how adjustment for baseline covariates in randomized trials can meaningfully improve precision for a variety of outcome types. Their findings build on a long history, starting in 1932 with R.A. Fisher and including more recent endorsements by the U.S. Food and Drug Administration and the European Medicines Agency. Here, we address an important practical consideration: *how* to select the adjustment approach -- which variables and in which form -- to maximize precision, while maintaining Type-I error control. Balzer et al. previously proposed *Adaptive Prespecification* within TMLE to flexibly and automatically select, from a prespecified set, the approach that maximizes empirical efficiency in small trials (N$<$40). To avoid overfitting with few randomized units, selection was previously limited to working generalized linear models, adjusting for a single covariate. Now, we tailor Adaptive Prespecification to trials with many randomized units. Using $V$-fold
    
[^78]: 傅立叶分析实现一致且真实的模型解释

    Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.17426](http://arxiv.org/abs/2210.17426)

    该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。

    

    对于许多跨学科领域，机器学习的解释需要与当前案例相关的假设情景一致，即如果一个因素改变，模型会如何反应？尽管归因方法由优雅的公理系统支持，但它们主要关注单个输入，并且通常不一致。为支持假设情景，我们引入了一个称为真实解释的新概念，并应用布尔函数的傅立叶分析来获得严格的保证。实验结果表明，对于各种半径的邻域，我们的方法与其他方法相比，可以实现2倍至50倍更低的解释误差。

    For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
    
[^79]: 训练神经网络用于时序变点检测

    Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17312](http://arxiv.org/abs/2210.17312)

    本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。

    

    检测数据流中的突变分布转换，即所谓的变点检测，是统计学和机器学习中的一个基本问题。我们引入了一种新颖的方法，使用神经网络进行在线变点检测。具体而言，我们的方法是训练神经网络来逐步计算检测统计量的累积和，当发生变点时，该量会显著变化。我们使用合成和真实世界数据证明了所提出的方法在检测变点方面的优越性和潜力。

    Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
    
[^80]: 双控制变量加速黑盒变分推断

    Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07290](http://arxiv.org/abs/2210.07290)

    本论文提出了双控制变量方法，能够同时减少数据子抽样和蒙特卡罗抽样带来的梯度估计方差，提高黑盒变分推断的准确性和效率。

    

    黑盒变分推断是一种广泛使用的贝叶斯后验推断框架，但在某些情况下，梯度估计中的高方差会损害准确性和效率。这种方差来自两个随机源：数据子抽样和蒙特卡罗抽样。现有的控制变量仅解决蒙特卡罗噪声，而增量梯度方法通常仅解决数据子抽样，我们提出了一种新的“双”控制变量，能够同时减少两种噪声源的方差。我们确认这导致了减少方差和在多个现实世界应用中提高优化效果。

    Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
    
[^81]: InfoOT: 信息最大化的最优输运

    InfoOT: Information Maximizing Optimal Transport. (arXiv:2210.03164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03164](http://arxiv.org/abs/2210.03164)

    提出了InfoOT，它是一种信息论扩展的最优输运方法，能够解决最优输运忽略了数据中相干结构的问题，同时能够处理离群值和集成新数据点，可以提高域自适应、跨域检索和单细胞对齐等任务的对齐质量。

    

    最优输运通过最小化它们之间的运输成本（例如几何距离）对分布中的样本进行对齐。然而，它忽略了数据中的相干结构，例如簇，不能很好地处理离群值，也不能集成新数据点。为解决这些问题，我们提出了InfoOT，它是最优输运的信息论扩展，可以在最小化几何距离的同时最大化域之间的互信息。最终的目标仍然可以被公式化为（广义的）最优输运问题，并可以通过投影梯度下降有效地求解。这种公式化产生了一种新的投影方法，它对离群值具有鲁棒性，并且可以推广到未见过的样本。实证结果表明，在域自适应、跨域检索和单细胞对齐等基准中，InfoOT可以提高对齐的质量。

    Optimal transport aligns samples across distributions by minimizing the transportation cost between them, e.g., the geometric distances. Yet, it ignores coherence structure in the data such as clusters, does not handle outliers well, and cannot integrate new data points. To address these drawbacks, we propose InfoOT, an information-theoretic extension of optimal transport that maximizes the mutual information between domains while minimizing geometric distances. The resulting objective can still be formulated as a (generalized) optimal transport problem, and can be efficiently solved by projected gradient descent. This formulation yields a new projection method that is robust to outliers and generalizes to unseen samples. Empirically, InfoOT improves the quality of alignments across benchmarks in domain adaptation, cross-domain retrieval, and single-cell alignment.
    
[^82]: 从相关数据中训练归一化流

    Training Normalizing Flows from Dependent Data. (arXiv:2209.14933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14933](http://arxiv.org/abs/2209.14933)

    该论文提出了一种考虑数据点依赖关系的归一化流似然目标和学习算法，在合成和真实数据上实现更好的经验结果和更高的统计功效。

    

    归一化流是一种功能强大的非参数统计模型，它是密度估计器和生成模型之间的混合体。目前的归一化流学习算法假定数据点是独立采样的，这一假设在实践中经常被违反，可能导致密度估计和数据生成的错误。我们提出了一种考虑数据点之间依赖关系的归一化流似然目标，并为此推导了一种适用于不同依赖结构的灵活高效的学习算法。我们表明，尊重观察值之间的依赖关系可以改善合成和实际数据的经验结果，并在对全基因组关联研究的下游应用中导致更高的统计功效。

    Normalizing flows are powerful non-parametric statistical models that function as a hybrid between density estimators and generative models. Current learning algorithms for normalizing flows assume that data points are sampled independently, an assumption that is frequently violated in practice, which may lead to erroneous density estimation and data generation. We propose a likelihood objective of normalizing flows incorporating dependencies between the data points, for which we derive a flexible and efficient learning algorithm suitable for different dependency structures. We show that respecting dependencies between observations can improve empirical results on both synthetic and real-world data, and leads to higher statistical power in a downstream application to genome-wide association studies.
    
[^83]: GNN在推广带限函数方面的优越性比NN更加明显

    Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05904](http://arxiv.org/abs/2206.05904)

    本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。

    

    图神经网络（GNN）以其整合图形信息的能力被广泛用于数据分析。然而，GNN的表达能力仅针对图级任务进行了研究，而不是针对节点级任务，例如节点分类，其中试图从观察到的节点标签中插值出缺失的标签信息。本文研究了GNN在所述分类任务中的表达能力，它实质上是一个函数插值问题。具体而言，我们导出了GNN插值$\mathbb{R}^d$中带限函数所需的权重和层数。我们的结果显示，使用GNN架构以$\epsilon$-近似离散带限信号仅需要$O((\log \epsilon^{-1})^{d})$个权重，这比使用完全连接的神经网络（NN）得到的最佳结果的所需权重少得多 - 特别地，使用使用$O((\log \epsilon^{-1})^{d})$个样本来训练GNN以$\epsilon$-逼近带限函数。

    Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
    
[^84]: 离散观测扩散过程的计算Doob h变换在线滤波

    Computational Doob's h-transforms for Online Filtering of Discretely Observed Diffusions. (arXiv:2206.03369v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03369](http://arxiv.org/abs/2206.03369)

    本文提出了一种计算Doob h变换的计算框架，用于离散观测非线性扩散过程的在线滤波。实验证明，该方法在高度信息化、观测值在模型下极端或状态维数较大时比最先进的粒子滤波器高几个数量级的效率。

    

    本文关注的是离散观测非线性扩散过程的在线滤波。我们的方法基于完全适应的辅助粒子滤波器，其中包括通常难以处理的Doob h变换。我们提出了一个计算框架，通过使用非线性Feynman-Kac公式和神经网络求解潜在的反向Kolmogorov方程来近似这些h变换。该方法允许在数据同化过程之前训练局部最优的粒子滤波器。数值实验表明，当观测值高度信息化，观测值在模型下极端，或状态维数较大时，所提出的方法比最先进的粒子滤波器高几个数量级的效率。

    This paper is concerned with online filtering of discretely observed nonlinear diffusion processes. Our approach is based on the fully adapted auxiliary particle filter, which involves Doob's $h$-transforms that are typically intractable. We propose a computational framework to approximate these $h$-transforms by solving the underlying backward Kolmogorov equations using nonlinear Feynman-Kac formulas and neural networks. The methodology allows one to train a locally optimal particle filter prior to the data-assimilation procedure. Numerical experiments illustrate that the proposed approach can be orders of magnitude more efficient than state-of-the-art particle filters in the regime of highly informative observations, when the observations are extreme under the model, or if the state dimension is large.
    
[^85]: 多臂老虎机用于语言模型预训练的资源高效、在线优化：动态遮盖的使用案例

    Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.13151](http://arxiv.org/abs/2203.13151)

    本文提出了一种多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。并设计了基于高斯过程的Thompson抽样（GP-TS）算法，加速Pre-training过程并降低MLM损失。

    

    我们设计并评估了一种贝叶斯优化框架，以资源高效的方式预训练基于Transformer的语言模型（TLM）。 TLM预训练需要高计算资源，并引入许多未解决的设计选择，例如选择其预训练超参数。 我们提出了一个多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。 我们设计了一个Thompson抽样算法，用于其顺序最小化的带有掩码语言模型（MLM）预训练目标的代理高斯过程奖励模型。 提出的基于高斯过程的Thompson抽样（GP-TS）不是使用固定掩码概率进行MLM预训练，而是通过顺序选择改善性能的掩码超参数来加速预训练。 我们通过实验证明了GP-TS如何高效进行语言模型的预训练，即在少量迭代中实现更低的MLM损失。

    We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters. We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fe
    
[^86]: 一种用于解决最优输运问题的加速随机算法

    An Accelerated Stochastic Algorithm for Solving the Optimal Transport Problem. (arXiv:2203.00813v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.00813](http://arxiv.org/abs/2203.00813)

    本文提出了一种能够用较低的计算复杂度解决最优输运问题的加速随机算法。

    

    本文提出了一种原始-对偶加速随机梯度下降与方差约减算法(PDASGD)，用于解决线性约束优化问题。 PDASGD可应用于解决离散最优输运（OT）问题，并具有已知的最佳计算复杂度-$\widetilde{\mathcal{O}}(n^2/\epsilon)$，其中$n$是原子数，$\epsilon> 0$是精度。 本文还讨论了使得PDASGD具有较低的计算复杂度的条件，来解决线性约束优化问题。 数值实验表明，该算法在解决OT问题时可以将复杂度的速率提高了$\widetilde{\mathcal{O}}(\sqrt{n})$。

    A primal-dual accelerated stochastic gradient descent with variance reduction algorithm (PDASGD) is proposed to solve linear-constrained optimization problems. PDASGD could be applied to solve the discrete optimal transport (OT) problem and enjoys the best-known computational complexity -$\widetilde{\mathcal{O}}(n^2/\epsilon)$, where $n$ is the number of atoms, and $\epsilon>0$ is the accuracy. In the literature, some primal-dual accelerated first-order algorithms, e.g., APDAGD, have been proposed and have the order of $\widetilde{\mathcal{O}}(n^{2.5}/\epsilon)$ for solving the OT problem. To understand why our proposed algorithm could improve the rate by a factor of $\widetilde{\mathcal{O}}(\sqrt{n})$, the conditions under which our stochastic algorithm has a lower order of computational complexity for solving linear-constrained optimization problems are discussed. It is demonstrated that the OT problem could satisfy the aforementioned conditions. Numerical experiments demonstrate s
    
[^87]: 有下层压缩的双层优化: 无warm-start情况下最优样本复杂度分析

    Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start. (arXiv:2202.03397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.03397](http://arxiv.org/abs/2202.03397)

    本文针对一类双层问题，提出了无需warm-start也可实现最优样本复杂度的方法。

    

    本文分析了一类一般的双层问题，其中上层问题是将一光滑目标函数最小化，下层问题是寻找一光滑收缩映射的不动点。这类问题包括元学习、均衡模型、超参数优化和数据污染对抗攻击的实例。我们展示了，即使没有warm-start，在某些情况下，如元学习和均衡模型，仍然可以实现顺序最优的样本复杂度。

    We analyse a general class of bilevel problems, in which the upper-level problem consists in the minimization of a smooth objective function and the lower-level problem is to find the fixed point of a smooth contraction map. This type of problems include instances of meta-learning, equilibrium models, hyperparameter optimization and data poisoning adversarial attacks. Several recent works have proposed algorithms which warm-start the lower-level problem, i.e. they use the previous lower-level approximate solution as a staring point for the lower-level solver. This warm-start procedure allows one to improve the sample complexity in both the stochastic and deterministic settings, achieving in some cases the order-wise optimal sample complexity. However, there are situations, e.g., meta learning and equilibrium models, in which the warm-start procedure is not well-suited or ineffective. In this work we show that without warm-start, it is still possible to achieve order-wise (near) optimal
    
[^88]: 混合成员无分布模型

    Mixed Membership Distribution-Free Model. (arXiv:2112.04389v4 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2112.04389](http://arxiv.org/abs/2112.04389)

    本文提出了一种混合成员无分布模型，用于重叠加权网络的社群检测，支持节点所属多个社群和有限实数权值。提出的模型可以推广到之前的模型，包括混合成员随机块模型，并支持具有潜在社群结构的重叠符号网络的生成。我们使用高效谱算法估计模型下的社群成员资格，并提出了模糊加权模块度来评估重叠加权网络的社群检测质量并确定加权网络社群数量。

    

    本文考虑在具有重叠加权网络中进行社群检测的问题，其中节点可以属于多个社群，边权可以是有限实数。为了对这样的复杂网络进行建模，我们提出了一个通用框架——混合成员无分布（MMDF）模型。MMDF没有对边权的分布约束，可以被视为一些先前模型的推广，包括著名的混合成员随机块模型。特别地，具有潜在社群结构的重叠符号网络也可以从我们的模型中生成。我们使用具有收敛率理论保证的高效谱算法来估计模型下的社群成员资格。我们还提出了模糊加权模块度来评估具有正负边权的重叠加权网络的社群检测质量。然后，我们提供了一种利用我们的fuzzy weighted modularity来确定加权网络社群数量的方法。

    We consider the problem of community detection in overlapping weighted networks, where nodes can belong to multiple communities and edge weights can be finite real numbers. To model such complex networks, we propose a general framework - the mixed membership distribution-free (MMDF) model. MMDF has no distribution constraints of edge weights and can be viewed as generalizations of some previous models, including the well-known mixed membership stochastic blockmodels. Especially, overlapping signed networks with latent community structures can also be generated from our model. We use an efficient spectral algorithm with a theoretical guarantee of convergence rate to estimate community memberships under the model. We also propose fuzzy weighted modularity to evaluate the quality of community detection for overlapping weighted networks with positive and negative edge weights. We then provide a method to determine the number of communities for weighted networks by taking advantage of our f
    
[^89]: 群等变神经后验估计

    Group equivariant neural posterior estimation. (arXiv:2111.13139v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.13139](http://arxiv.org/abs/2111.13139)

    本文提出了一种群等变神经后验估计（GNPE）算法，能够在参数和数据联合变换下整合等变性，用于从引力波观测中对双黑洞系统进行摊销推断。

    

    基于条件神经密度估计的仿真推理是解决科学领域反问题的强大方法。然而，这些方法通常将基础正向模型视为一个黑盒子，不会利用等变性等几何性质。等变性在科学模型中很常见，但将其直接整合到表达式推理网络（如归一化流）中并不简单。本文介绍了一种替代方法，可以在参数和数据的联合变换下整合等变性。我们的方法——称为群等变神经后验估计（GNPE）——基于自洽地标准化数据的“姿态”，同时估计参数后验。它是独立于体系结构的，并适用于精确和近似等变性。作为现实世界的应用，我们利用GNPE从引力波观测中对双黑洞系统进行了摊销推断。

    Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method -- called group equivariant neural posterior estimation (GNPE) -- is based on self-consistently standardizing the "pose" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitational-wave observations. W
    
[^90]: 优化赌博算法的脆弱性

    The Fragility of Optimized Bandit Algorithms. (arXiv:2109.13595v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.13595](http://arxiv.org/abs/2109.13595)

    本文表明，使用优化设计的赌博算法遗憾分布具有非常重的尾部，对于$p>1$，遗憾分布的$p$'th矩增长要比多对数级别快得多，当问题略微错误时，优化UCB赌博设计的遗憾可以比传统理论建议的增长得更快。

    

    大部分关于赌博算法最优设计的文献都是基于期望遗憾的最小化。已知对于某些指数族，最优设计在拉依-罗宾斯下界指导下，可实现期望遗憾以对数级别增长。本文表明，当使用这种最优设计时，关联算法的遗憾分布必然具有一个非常重的尾部，具体来说是截断柯西分布。此外，对于$p>1$，遗憾分布的$p$'th矩增长要比多对数级别快得多，特别是作为衣袖数的幂函数。我们表明，优化UCB赌博设计在另一个方面也很脆弱，即当问题略微错误时，遗憾可以比传统理论建议的增长得更快。我们的论点基于标准的措施改变思想，并表明最优设计可能导致一些不太可能发生的情况，因此应该以谨慎的方式对待。

    Much of the literature on optimal design of bandit algorithms is based on minimization of expected regret. It is well known that designs that are optimal over certain exponential families can achieve expected regret that grows logarithmically in the number of arm plays, at a rate governed by the Lai-Robbins lower bound. In this paper, we show that when one uses such optimized designs, the regret distribution of the associated algorithms necessarily has a very heavy tail, specifically, that of a truncated Cauchy distribution. Furthermore, for $p>1$, the $p$'th moment of the regret distribution grows much faster than poly-logarithmically, in particular as a power of the total number of arm plays. We show that optimized UCB bandit designs are also fragile in an additional sense, namely when the problem is even slightly mis-specified, the regret can grow much faster than the conventional theory suggests. Our arguments are based on standard change-of-measure ideas, and indicate that the mos
    
[^91]: 加权二分网络的社区发现

    Community detection for weighted bipartite networks. (arXiv:2109.10319v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.10319](http://arxiv.org/abs/2109.10319)

    本文提出了一种名为Bipartite Distribution-Free的模型，可用于建模和探测加权二分网络的社区结构，该模型考虑了节点度数的变化以及期望的块结构。同时，我们提出了谱算法用于识别社区。

    

    二分网络出现在多个领域，例如生物学、社会学、生理学和计算机科学中。过去的研究提出了随机共同块模型（ScBM）来检测二分图数据的社区结构，但是ScBM完全忽略边权并且无法解释加权二分网络的块结构。为了解决这个问题，我们通过放宽ScBM的分布约束，提出了一个名为Bipartite Distribution-Free的模型来建模加权二分网络，并考虑节点度数的变化来构建模型扩展。我们的模型不需要在邻接矩阵的生成元上指定特定的分布，而只需要在期望邻接矩阵上指定块结构。本文中，我们提出了具有理论保证的谱算法，来识别社区。通过模拟和实证例子来展示我们所提出的模型和算法。

    The bipartite network appears in various areas, such as biology, sociology, physiology, and computer science. \cite{rohe2016co} proposed Stochastic co-Blockmodel (ScBM) as a tool for detecting community structure of binary bipartite graph data in network studies. However, ScBM completely ignores edge weight and is unable to explain the block structure of a weighted bipartite network. Here, to model a weighted bipartite network, we introduce a Bipartite Distribution-Free model by releasing ScBM's distribution restriction. We also build an extension of the proposed model by considering the variation of node degree. Our models do not require a specific distribution on generating elements of the adjacency matrix but only a block structure on the expected adjacency matrix. Spectral algorithms with theoretical guarantees on the consistent estimation of node labels are presented to identify communities. Our proposed methods are illustrated by simulated and empirical examples.
    
[^92]: 我们应该转移哪种不变性？一种因果极小化学习方法

    Which Invariance Should We Transfer? A Causal Minimax Learning Approach. (arXiv:2107.01876v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.01876](http://arxiv.org/abs/2107.01876)

    该论文从因果的角度提出了一种全面的极小化分析，旨在回答机器学习模型在转移稳定信息时应该转移哪个子集从而达到最佳的泛化能力这一问题

    

    当前机器学习模型无法可靠应对数据集变化，因此大多数现有研究试图将稳定信息转移到看不见的环境中。特别地，基于独立因果机制的方法通过do-operator消除可变的因果机制。与之前的方法相比，所得到的稳定预测因为能够更有效地识别稳定信息而更加有效。然而，一个关键问题仍然存在：为了达到最佳的泛化能力，应该转移这整个稳定信息中的哪个子集？为了回答这个问题，我们从因果的角度提出了一种全面的极小化分析。具体来说，我们首先提供了一个用于判断整个稳定集是否最优的图形条件。当这个条件失败时，我们惊讶地发现，通过一个例子，这个整个稳定集虽然能够充分利用稳定信息，但并不是最优的转移集。为了确定最优集，我们提出了因果最小含义的方法，并给出了仿真和实际数据中的实验结果。

    A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Particularly, independent causal mechanisms-based methods proposed to remove mutable causal mechanisms via the do-operator. Compared to previous methods, the obtained stable predictors are more effective in identifying stable information. However, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the o
    
[^93]: 无需信任的私有联邦学习：凸损失函数的最优算法

    Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses. (arXiv:2106.09779v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.09779](http://arxiv.org/abs/2106.09779)

    本文研究了无需信任服务器或其他数据源的跨 silo 联邦学习，考虑了跨 silo 记录级差分隐私 ISRL-DP。该算法可以确保来自每个人的数据都不会被泄漏。

    

    本文探讨了联邦学习（FL）的研究，特别是跨数据源（跨 silo）FL，这些数据源的数据主人都不信任服务器或其他 silos。在这种情况下，每个数据源（例如医院）都有来自不同人（例如患者）的数据，并且必须维护每个人（例如医疗记录）数据的隐私，即使服务器或其他数据源是恶意监听者。这种要求促进了对跨 silo 记录级差分隐私（ISRL-DP）的研究，它要求 silo i 的通信满足记录 / 项目级差分隐私 (DP)。ISRL-DP 确保 silo i 中每个人（例如患者）的数据都不会泄漏。ISRL-DP 不同于各种已有的隐私概念。中心和用户级差分隐私假定人们信任服务器/其他数据源。在极端情况下，本地DP 假定人们根本不信任任何人（甚至是他们自己的数据源）。ISRL-DP 处于中心和本地DP 之间，使得在跨 silo 的真实情况下具有现实意义。

    This paper studies federated learning (FL)--especially cross-silo FL--with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person's data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo i's communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo i (e.g. hospital i) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-sil
    
[^94]: 论DP-SGD的Moment Accountant方法的紧密性

    On the Tightness of the Moment Accountant for DP-SGD. (arXiv:2102.09030v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.09030](http://arxiv.org/abs/2102.09030)

    通过改进Moment Accountant方法，DP-SGD具有可关闭形式的$(\epsilon，\delta)$-DP保证，并且其保证接近是紧密的，具有最小的计算成本。

    

    为了提供差分隐私，在差分隐私SGD（DP-SGD）中，在执行剪切操作后，向本地SGD更新添加标准差为$ \sigma $的高斯噪声。通过非平凡地改进Moment Accountant方法，我们证明了一个封闭形式的$(\epsilon，\delta)$-DP保证：如果$ \sigma=\sqrt{ 2(\epsilon+\ln(1/\delta))/\epsilon} $，则DP-SGD是$ (\epsilon \leq 1/2，\delta = 1 / N) $-DP，其中$T$至少为$ \approx 2k^2/\epsilon$， $(2/e)^2k^2-1/2\geq \ln(N)$，其中$T$是回合的总数，$ K = kN $是梯度计算的总数，其中$ k $用数据集的大小$N$的时代数量来衡量。我们证明我们的表达式接近紧，在$T$小于约为$ 8 $倍于下界$ \approx 2k^2/\epsilon$的常数因子时，$(\epsilon，\delta)$-DP保证将被违反。选择最小可能值的$T \approx 2k^2/\epsilon$不仅会导致接近密集的DP保证，而且还会最小化计算成本。

    In order to provide differential privacy, Gaussian noise with standard deviation $\sigma$ is added to local SGD updates after performing a clipping operation in Differential Private SGD (DP-SGD). By non-trivially improving the moment account method we prove a closed form $(\epsilon,\delta)$-DP guarantee: DP-SGD is $(\epsilon\leq 1/2,\delta=1/N)$-DP if $\sigma=\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx 2k^2/\epsilon$ and $(2/e)^2k^2-1/2\geq \ln(N)$, where $T$ is the total number of rounds, and $K=kN$ is the total number of gradient computations where $k$ measures $K$ in number of epochs of size $N$ of the local data set. We prove that our expression is close to tight in that if $T$ is more than a constant factor $\approx 8$ smaller than the lower bound $\approx 2k^2/\epsilon$, then the $(\epsilon,\delta)$-DP guarantee is violated. Choosing the smallest possible value $T\approx 2k^2/\epsilon$ not only leads to a close to tight DP guarantee, but also minimizes 
    
[^95]: 浅层神经网络带限制的随机权重有多大的能力？

    How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.08427](http://arxiv.org/abs/2008.08427)

    本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。

    

    本文探讨了深度为2的带限制随机神经网络的表达能力。随机网络是指隐藏层参数被冻结并赋予随机分配的神经网络，只有输出层参数通过损失最小化进行训练。使用随机权重的隐藏层是避免标准梯度下降学习中的非凸优化的有效方法，并已被近期深度学习理论所采用。尽管神经网络是普适逼近器的众所周知的事实，在这项研究中，我们数学上证明了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。我们特别导出了一个新的非平凡逼近误差下界。证明利用了Ridgelet分析技术，这是一种为神经网络设计的谐波分析方法。这种方法受到了经典信号处理中的基本原理的启发，特别是信号在某种限制下的采样。

    We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit
    

