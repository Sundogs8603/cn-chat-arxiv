# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Wasserstein perspective of Vanilla GANs](https://arxiv.org/abs/2403.15312) | 将普通GANs与水斯坦距离联系起来，扩展现有水斯坦GANs结果到普通GANs，获得了普通GANs的神谕不等式。 |
| [^2] | [Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models](https://arxiv.org/abs/2403.15263) | 该论文研究了联邦贝叶斯深度学习的方法，旨在解决在现代深度学习模型中传达认识不确定性的挑战。 |
| [^3] | [Double Cross-fit Doubly Robust Estimators: Beyond Series Regression](https://arxiv.org/abs/2403.15175) | 双交叉固定双稳健估计器针对因果推断中的预期条件协方差进行了研究，通过拆分训练数据并在独立样本上下调nuisance函数估计器，结构无关的错误分析以及更强假设的结果，提出了更精确的DCDR估计器。 |
| [^4] | [Quantification using Permutation-Invariant Networks based on Histograms](https://arxiv.org/abs/2403.15123) | 本文研究了深度神经网络在量化任务中的应用，提出了基于直方图的置换不变网络HistNetQ，在量化比赛中表现优异。 |
| [^5] | [Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks](https://arxiv.org/abs/2403.15108) | 该论文提出了一种基于Wasserstein距离和GroupSort神经网络的主动学习方法，该方法在回归问题中实现了更精确的估计并比其他模型更快提高准确性。 |
| [^6] | [Estimation of multiple mean vectors in high dimension](https://arxiv.org/abs/2403.15038) | 通过凸组合的方法估计高维空间中不同概率分布的多维均值，引入了两种权重确定策略：一种通过测试程序识别低方差的相邻均值，提出了封闭形式插补公式；另一种通过最小化二次风险的上置信界确定权重，通过理论分析得出方法对经验均值的二次风险改进，在维度渐近的角度上渐近地接近 Oracle（Minimax）改进。 |
| [^7] | [Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model](https://arxiv.org/abs/2403.15025) | 通过引入物理信息指导的结构因果模型，我们提出了一种在分布转移下稳健的符合预测方法。 |
| [^8] | [Empirical investigation of multi-source cross-validation in clinical machine learning](https://arxiv.org/abs/2403.15012) | 本研究在多源环境中系统地评估了标准K折交叉验证和留出源交叉验证方法，为实现更全面和真实的精确度评估提供了新的机会 |
| [^9] | [Contrastive Learning on Multimodal Analysis of Electronic Health Records](https://arxiv.org/abs/2403.14926) | 该论文研究了电子健康记录的多模态分析，强调了结构化和非结构化数据之间的协同作用，并尝试将多模态对比学习方法应用于提高患者医疗历史的完整性。 |
| [^10] | [Statistical Inference For Noisy Matrix Completion Incorporating Auxiliary Information](https://arxiv.org/abs/2403.14899) | 本文研究了在半监督模型中利用辅助信息进行有噪声矩阵补全的统计推断，提出了一种迭代最小二乘估计方法，展示了该方法的高效性和精确性。 |
| [^11] | [Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures](https://arxiv.org/abs/2403.14830) | 本文解决了深度聚类方法在评估聚类质量时面临的挑战，提出了一种系统方法来应用聚类有效性指标。 |
| [^12] | [Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection](https://arxiv.org/abs/2403.14829) | 通过使用P\'olya-Gamma随机变量制定VGPMIL，该方法产生与原始VGPMIL相同的变分后验近似，这是双曲正割分布所承认的两种表示的结果。 |
| [^13] | [Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets](https://arxiv.org/abs/2403.14822) | 提出了使用Sinkhorn不确定性集解决非凸鲁棒假设检验问题的新框架，并引入了确切的混合整数指数锥重构方法，证明了优于目前文献中最先进方法的凸逼近。 |
| [^14] | [Curvature Augmented Manifold Embedding and Learning](https://arxiv.org/abs/2403.14813) | 将降维问题建模为机械/物理模型，引入曲率增强力的曲率增强流形嵌入与学习（CAMEL）方法提供了一种新的方法来捕捉数据集的n维流形表示。 |
| [^15] | [Auditing Fairness under Unobserved Confounding](https://arxiv.org/abs/2403.14713) | 在未观测混杂因素的情况下，本文展示了即使在放宽或甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以给出对高风险个体分配率的信息丰富的界限。 |
| [^16] | [Statistical Agnostic Regression: a machine learning method to validate regression models](https://arxiv.org/abs/2402.15213) | 本文提出了一种新的方法，统计无关地评估了线性回归模型，并评估了ML估计在检测方面的表现。 |
| [^17] | [Mathematical Opportunities in Digital Twins (MATH-DT)](https://arxiv.org/abs/2402.10326) | 数字孪生中的数学机遇需要基础数学进展，与传统方法不同，数字孪生从特定现实出发，需要多尺度建模和耦合，通过传感器将数据输入，帮助人类做出决策。 |
| [^18] | [Learning to Embed Time Series Patches Independently](https://arxiv.org/abs/2312.16427) | 学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。 |
| [^19] | [Soft Contrastive Learning for Time Series](https://arxiv.org/abs/2312.16424) | 提出了一种名为SoftCLT的方法，通过引入实例级和时间级软对比损失，解决了在时间序列中忽略固有相关性所导致的学习表示质量下降的问题。 |
| [^20] | [Symmetry Breaking and Equivariant Neural Networks](https://arxiv.org/abs/2312.09016) | 提出了一种新颖的“放松等变性”的概念，用于解决等变函数无法在单个数据样本层面打破对称的限制，并展示了如何将其应用于等变多层感知机（E-MLP）中。 |
| [^21] | [Multiscale Hodge Scattering Networks for Data Analysis](https://arxiv.org/abs/2311.10270) | 提出了多尺度霍奇散射网络（MHSNs），利用多尺度基础词典和卷积结构，生成对节点排列不变的特征。 |
| [^22] | [Learning to Importance Sample in Primary Sample Space](https://arxiv.org/abs/1808.07840) | 提出了一种利用神经网络在主采样空间中学习重要性采样的新方法，用于渲染算法中的方差缩减。 |
| [^23] | [From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity.](http://arxiv.org/abs/2309.16512) | 本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。 |
| [^24] | [CARE: Large Precision Matrix Estimation for Compositional Data.](http://arxiv.org/abs/2309.06985) | CARE方法通过精确指定组成数据的精确矩阵，并利用其与基础矩阵之间的联系，提出了一种估计稀疏基础矩阵的组成数据估计方法。通过理论分析，我们发现在足够高的维度下，CARE估计器实现了极小化风险的速率。 |
| [^25] | [An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression.](http://arxiv.org/abs/2306.13185) | 本文研究了核岭回归中过拟合成本，采用“不可知”的观点，以分析样本量和任务特征结构对成本的影响。通过分析提供了更细致的过度拟合表征。 |
| [^26] | [EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search.](http://arxiv.org/abs/2210.06015) | 提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。 |

# 详细

[^1]: 水斯坦视角下的普通 GANs

    A Wasserstein perspective of Vanilla GANs

    [https://arxiv.org/abs/2403.15312](https://arxiv.org/abs/2403.15312)

    将普通GANs与水斯坦距离联系起来，扩展现有水斯坦GANs结果到普通GANs，获得了普通GANs的神谕不等式。

    

    生成对抗网络(GANs)的实证成功引起了对理论研究日益增长的兴趣。统计文献主要集中在水斯坦GANs及其扩展上，特别是允许具有良好的降维特性。对于普通GANs，即原始优化问题，统计结果仍然相当有限，需要假设平滑激活函数和潜空间与周围空间的维度相等。为了弥合这一差距，我们将普通GANs与水斯坦距离联系起来。通过这样做，现有的水斯坦GANs结果可以扩展到普通GANs。特别是，在水斯坦距离中获得了普通GANs的神谕不等式。这个神谕不等式的假设旨在由实践中常用的网络架构满足，如前馈ReLU网络。

    arXiv:2403.15312v1 Announce Type: cross  Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative resu
    
[^2]: 联邦贝叶斯深度学习：统计聚合方法应用于贝叶斯模型

    Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models

    [https://arxiv.org/abs/2403.15263](https://arxiv.org/abs/2403.15263)

    该论文研究了联邦贝叶斯深度学习的方法，旨在解决在现代深度学习模型中传达认识不确定性的挑战。

    

    联邦学习(FL)是一种训练机器学习模型的方法，利用多个分布式数据集，同时保持数据隐私和减少与共享本地数据集相关的通信成本。已经开发了聚合策略，用于整合或融合分布式确定性模型的权重和偏差；然而，现代确定性深度学习（DL）模型通常校准不佳，缺乏在预测中传达一种认识不确定性的能力，这对遥感平台和安全关键应用是理想的。相反，贝叶斯DL模型通常校准良好，能够量化和传达一种认识不确定性的能力以及具有竞争力的预测准确性。不幸的是，因为贝叶斯DL模型中的权重和偏差由概率分布定义，所以简单应用聚合方法是困难的。

    arXiv:2403.15263v1 Announce Type: new  Abstract: Federated learning (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associa
    
[^3]: 双交叉固定双稳健估计器：超越串行回归

    Double Cross-fit Doubly Robust Estimators: Beyond Series Regression

    [https://arxiv.org/abs/2403.15175](https://arxiv.org/abs/2403.15175)

    双交叉固定双稳健估计器针对因果推断中的预期条件协方差进行了研究，通过拆分训练数据并在独立样本上下调nuisance函数估计器，结构无关的错误分析以及更强假设的结果，提出了更精确的DCDR估计器。

    

    具有跨拟合交叉的双稳健估计器因其良好的结构无关错误保证而在因果推断中备受青睐。然而，当存在额外结构，例如H\"{o}lder平滑时，可以通过在独立样本上对训练数据进行拆分和下调nuisance函数估计器来构建更精确的“双交叉固定双稳健”（DCDR）估计器。我们研究了预期条件协方差的DCDR估计器，在因果推断和条件独立性检验中是一个感兴趣的函数，并得出了一系列逐渐更强假设的结果。首先，我们对DCDR估计器提供无需对nuisance函数或它们的估计器做出假设的结构无关错误分析。然后，假设nuisance函数是H\"{o}lder平滑，但不假设知晓真实平滑级别或协变量密度。

    arXiv:2403.15175v1 Announce Type: cross  Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate densit
    
[^4]: 基于直方图的置换不变网络的量化方法

    Quantification using Permutation-Invariant Networks based on Histograms

    [https://arxiv.org/abs/2403.15123](https://arxiv.org/abs/2403.15123)

    本文研究了深度神经网络在量化任务中的应用，提出了基于直方图的置换不变网络HistNetQ，在量化比赛中表现优异。

    

    量化，也称为类别普遍性估计，是监督学习任务，模型被训练用来预测给定样本集中每个类的普遍性。本文研究了深度神经网络在量化任务中的应用，特别关注可以应用对称监督方法的情况，从而消除了分类作为中间步骤的需求，直接解决量化问题。此外，本文讨论了为集合处理设计的现有置换不变层，并评估了它们对量化的适用性。在我们的分析基础上，我们提出了HistNetQ，一种新颖的神经架构，它依赖于基于直方图的置换不变表示，特别适用于量化问题。我们在迄今为止唯一的量化竞赛中进行的实验表明，HistNetQ胜过其他深度神经网络架构。

    arXiv:2403.15123v1 Announce Type: new  Abstract: Quantification, also known as class prevalence estimation, is the supervised learning task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric supervised approach that eliminates the need for classification as an intermediary step, directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems. Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural arch
    
[^5]: 基于Wasserstein距离和GroupSort神经网络的回归主动学习

    Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks

    [https://arxiv.org/abs/2403.15108](https://arxiv.org/abs/2403.15108)

    该论文提出了一种基于Wasserstein距离和GroupSort神经网络的主动学习方法，该方法在回归问题中实现了更精确的估计并比其他模型更快提高准确性。

    

    本文介绍了一种新的用于回归问题的主动学习策略。所提出的Wasserstein主动回归模型基于分布匹配原则，用于衡量标记数据集的代表性。使用GroupSort神经网络计算Wasserstein距离。这些网络的使用提供了理论基础，可以量化错误并明确其大小和深度。此解决方案结合了另一种基于不确定性的方法，对异常值更具容忍性，以完成查询策略。最后，该方法与其他经典和最近的解决方案进行了比较。研究实验性地展示了这种代表性-不确定性方法的相关性，该方法在整个查询过程中提供了良好的估计。此外，Wasserstein主动回归通常能够实现更精确的估计，并且往往比其他模型更快提高准确性。

    arXiv:2403.15108v1 Announce Type: new  Abstract: This paper addresses a new active learning strategy for regression problems. The presented Wasserstein active regression model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein active regression often achieves more precise estimations and tends to improve accuracy faster than other models.
    
[^6]: 高维情况下多个均值向量的估计

    Estimation of multiple mean vectors in high dimension

    [https://arxiv.org/abs/2403.15038](https://arxiv.org/abs/2403.15038)

    通过凸组合的方法估计高维空间中不同概率分布的多维均值，引入了两种权重确定策略：一种通过测试程序识别低方差的相邻均值，提出了封闭形式插补公式；另一种通过最小化二次风险的上置信界确定权重，通过理论分析得出方法对经验均值的二次风险改进，在维度渐近的角度上渐近地接近 Oracle（Minimax）改进。

    

    我们致力于基于独立样本在一个共同空间中估计来自不同概率分布的多维均值。我们的方法是通过对这些样本导出的经验均值进行凸组合来形成估计量。我们引入了两种策略来找到适当的依赖于数据的凸组合权重：第一种利用测试程序来识别具有低方差的相邻均值，从而产生了一个关于权重的封闭形式插补公式；第二种通过最小化二次风险的上置信区间来确定权重。通过理论分析，我们评估了我们的方法相对于经验均值提供的二次风险改进。我们的分析集中在维度渐近的角度上，显示我们的方法在数据的有效维度增加时渐近地接近于一个 Oracle（Minimax）改进。我们展示了通过提出的方法在均值估计中的应用。

    arXiv:2403.15038v1 Announce Type: cross  Abstract: We endeavour to estimate numerous multi-dimensional means of various probability distributions on a common space based on independent samples. Our approach involves forming estimators through convex combinations of empirical means derived from these samples. We introduce two strategies to find appropriate data-dependent convex combination weights: a first one employing a testing procedure to identify neighbouring means with low variance, which results in a closed-form plug-in formula for the weights, and a second one determining weights via minimization of an upper confidence bound on the quadratic risk.Through theoretical analysis, we evaluate the improvement in quadratic risk offered by our methods compared to the empirical means. Our analysis focuses on a dimensional asymptotics perspective, showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases.We demonstrat
    
[^7]: 通过物理信息结构因果模型在分布转移下的稳健符合预测

    Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model

    [https://arxiv.org/abs/2403.15025](https://arxiv.org/abs/2403.15025)

    通过引入物理信息指导的结构因果模型，我们提出了一种在分布转移下稳健的符合预测方法。

    

    不确定性对于利用机器学习进行可靠决策至关重要。 符合预测（CP）通过预测测试输入的一个集合来处理不确定性，希望该集合以至少$(1-\alpha)$的置信度覆盖真实标签。 即使在校准和测试数据集之间的边缘分布 $P_X$ 不同的情况下，这种覆盖也可以在测试数据上得到保证。 但是，实践中很常见的情况是，当校准和测试数据上的条件分布 $P_{Y|X}$ 不同时，覆盖就无法保证，在所有可能的置信水平下测量和最小化分布转移下的覆盖损失是至关重要的。 为了解决这些问题，我们使用校准和测试符合分数的累积密度函数以及Wasserstein距离在各个水平上上界覆盖差异。 受物理学在数据分布方面的不变性启发，我们提出了一种受物理信息指导的结构因果模型。

    arXiv:2403.15025v1 Announce Type: new  Abstract: Uncertainty is critical to reliable decision-making with machine learning. Conformal prediction (CP) handles uncertainty by predicting a set on a test input, hoping the set to cover the true label with at least $(1-\alpha)$ confidence. This coverage can be guaranteed on test data even if the marginal distributions $P_X$ differ between calibration and test datasets. However, as it is common in practice, when the conditional distribution $P_{Y|X}$ is different on calibration and test data, the coverage is not guaranteed and it is essential to measure and minimize the coverage loss under distributional shift at \textit{all} possible confidence levels. To address these issues, we upper bound the coverage difference at all levels using the cumulative density functions of calibration and test conformal scores and Wasserstein distance. Inspired by the invariance of physics across data distributions, we propose a physics-informed structural caus
    
[^8]: 临床机器学习中多源交叉验证的实证研究

    Empirical investigation of multi-source cross-validation in clinical machine learning

    [https://arxiv.org/abs/2403.15012](https://arxiv.org/abs/2403.15012)

    本研究在多源环境中系统地评估了标准K折交叉验证和留出源交叉验证方法，为实现更全面和真实的精确度评估提供了新的机会

    

    传统上，基于机器学习的临床预测模型是在来自单一来源（如医院）的患者数据上进行训练和评估的。交叉验证方法可通过重复随机拆分数据来估计这些模型在来自同一来源的新患者上的精确度。然而，与部署模型到数据集中未代表的源头（如新医院）获得的精确度相比，这些估计往往过于乐观。多源医疗数据集的不断增加为通过基于源头的交叉验证设计获得更全面和真实的预期精确度评估提供了新机会。

    arXiv:2403.15012v1 Announce Type: new  Abstract: Traditionally, machine learning-based clinical prediction models have been trained and evaluated on patient data from a single source, such as a hospital. Cross-validation methods can be used to estimate the accuracy of such models on new patients originating from the same source, by repeated random splitting of the data. However, such estimates tend to be highly overoptimistic when compared to accuracy obtained from deploying models to sources not represented in the dataset, such as a new hospital. The increasing availability of multi-source medical datasets provides new opportunities for obtaining more comprehensive and realistic evaluations of expected accuracy through source-level cross-validation designs.   In this study, we present a systematic empirical evaluation of standard K-fold cross-validation and leave-source-out cross-validation methods in a multi-source setting. We consider the task of electrocardiogram based cardiovascul
    
[^9]: 电子健康记录的多模态分析上的对比学习

    Contrastive Learning on Multimodal Analysis of Electronic Health Records

    [https://arxiv.org/abs/2403.14926](https://arxiv.org/abs/2403.14926)

    该论文研究了电子健康记录的多模态分析，强调了结构化和非结构化数据之间的协同作用，并尝试将多模态对比学习方法应用于提高患者医疗历史的完整性。

    

    电子健康记录（EHR）系统包含大量的多模态临床数据，包括结构化数据如临床编码和非结构化数据如临床笔记。然而，许多现有的针对EHR的研究传统上要么集中于个别模态，要么以一种相当粗糙的方式合并不同的模态。这种方法通常会导致将结构化和非结构化数据视为单独实体，忽略它们之间固有的协同作用。具体来说，这两个重要的模态包含临床相关、密切相关和互补的健康信息。通过联合分析这两种数据模态可以捕捉到患者医疗历史的更完整画面。尽管多模态对比学习在视觉语言领域取得了巨大成功，但在多模态EHR领域，尤其是在理论理解方面，其潜力仍未充分挖掘。

    arXiv:2403.14926v1 Announce Type: cross  Abstract: Electronic health record (EHR) systems contain a wealth of multimodal clinical data including structured data like clinical codes and unstructured data such as clinical notes. However, many existing EHR-focused studies has traditionally either concentrated on an individual modality or merged different modalities in a rather rudimentary fashion. This approach often results in the perception of structured and unstructured data as separate entities, neglecting the inherent synergy between them. Specifically, the two important modalities contain clinically relevant, inextricably linked and complementary health information. A more complete picture of a patient's medical history is captured by the joint analysis of the two modalities of data. Despite the great success of multimodal contrastive learning on vision-language, its potential remains under-explored in the realm of multimodal EHR, particularly in terms of its theoretical understandi
    
[^10]: 带有辅助信息的有噪声矩阵补全的统计推断

    Statistical Inference For Noisy Matrix Completion Incorporating Auxiliary Information

    [https://arxiv.org/abs/2403.14899](https://arxiv.org/abs/2403.14899)

    本文研究了在半监督模型中利用辅助信息进行有噪声矩阵补全的统计推断，提出了一种迭代最小二乘估计方法，展示了该方法的高效性和精确性。

    

    本文研究了在半监督模型中使用辅助协变量进行有噪声矩阵补全的统计推断。该模型包括两个部分。一个部分是由未观察到的潜在因素诱导的低秩矩阵；另一个部分通过由高维列向量组成的系数矩阵来模拟观察到的协变量的影响。我们通过逻辑回归对响应的观测模式进行建模，并允许其概率随着样本量的增加而趋于零。我们在考虑的背景下应用了迭代最小二乘（LS）估计方法。一般来说，迭代LS方法具有低计算成本，但推导出所得估计量的统计性质是一项具有挑战性的任务。我们展示了我们的方法仅需要少量迭代，以及得到的逐个条目的低秩矩阵估计量和系数矩阵

    arXiv:2403.14899v1 Announce Type: cross  Abstract: This paper investigates statistical inference for noisy matrix completion in a semi-supervised model when auxiliary covariates are available. The model consists of two parts. One part is a low-rank matrix induced by unobserved latent factors; the other part models the effects of the observed covariates through a coefficient matrix which is composed of high-dimensional column vectors. We model the observational pattern of the responses through a logistic regression of the covariates, and allow its probability to go to zero as the sample size increases. We apply an iterative least squares (LS) estimation approach in our considered context. The iterative LS methods in general enjoy a low computational cost, but deriving the statistical properties of the resulting estimators is a challenging task. We show that our method only needs a few iterations, and the resulting entry-wise estimators of the low-rank matrix and the coefficient matrix a
    
[^11]: 深度聚类评估：如何验证内部聚类有效性测量方法

    Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures

    [https://arxiv.org/abs/2403.14830](https://arxiv.org/abs/2403.14830)

    本文解决了深度聚类方法在评估聚类质量时面临的挑战，提出了一种系统方法来应用聚类有效性指标。

    

    arXiv:2403.14830v1 通告类型：跨领域 摘要：深度聚类是一种使用深度神经网络对复杂、高维数据进行划分的方法，它面临着独特的评估挑战。传统的聚类验证方法，设计用于低维空间，对于涉及将数据投影到较低维嵌入空间后再进行划分的深度聚类来说是有问题的。论文确定了两个关键问题：1）在将这些方法应用于原始数据时的维度灾难，2）由于不同聚类模型的训练过程和参数设置的变化而导致不同嵌入空间中的聚类结果无法可靠比较。本文解决了在深度学习中评估聚类质量所面临的挑战。我们提出了一个理论框架来强调在原始数据和嵌入数据上使用内部验证方法可能出现的无效性，并提出了一种系统方法来应用深度聚类有效性指标。

    arXiv:2403.14830v1 Announce Type: cross  Abstract: Deep clustering, a method for partitioning complex, high-dimensional data using deep neural networks, presents unique evaluation challenges. Traditional clustering validation measures, designed for low-dimensional spaces, are problematic for deep clustering, which involves projecting data into lower-dimensional embeddings before partitioning. Two key issues are identified: 1) the curse of dimensionality when applying these measures to raw data, and 2) the unreliable comparison of clustering results across different embedding spaces stemming from variations in training procedures and parameter settings in different clustering models. This paper addresses these challenges in evaluating clustering quality in deep learning. We present a theoretical framework to highlight ineffectiveness arising from using internal validation measures on raw and embedded data and propose a systematic approach to applying clustering validity indices in deep 
    
[^12]: 逻辑函数的双曲正割表示：在CT颅内出血检测中的概率多示例学习应用

    Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection

    [https://arxiv.org/abs/2403.14829](https://arxiv.org/abs/2403.14829)

    通过使用P\'olya-Gamma随机变量制定VGPMIL，该方法产生与原始VGPMIL相同的变分后验近似，这是双曲正割分布所承认的两种表示的结果。

    

    多示例学习（MIL）是一种成功应用于许多不同科学领域的弱监督范式，特别适用于医学成像。概率MIL方法，尤其是高斯过程（GPs），由于其高表达性和不确定性量化能力已经取得了出色的结果。最成功的基于GP的MIL方法之一，VGPMIL，使用变分边界处理逻辑函数的不可解性。在这里，我们使用P\'olya-Gamma随机变量来制定VGPMIL。这种方法产生与原始VGPMIL相同的变分后验近似，这是双曲正割分布所承认的两种表示的结果。这导致我们提出了一种通用的基于GP的MIL方法，通过简单利用除双曲正割以外的分布，可以采取不同形式。使用Gamma分布

    arXiv:2403.14829v1 Announce Type: new  Abstract: Multiple Instance Learning (MIL) is a weakly supervised paradigm that has been successfully applied to many different scientific areas and is particularly well suited to medical imaging. Probabilistic MIL methods, and more specifically Gaussian Processes (GPs), have achieved excellent results due to their high expressiveness and uncertainty quantification capabilities. One of the most successful GP-based MIL methods, VGPMIL, resorts to a variational bound to handle the intractability of the logistic function. Here, we formulate VGPMIL using P\'olya-Gamma random variables. This approach yields the same variational posterior approximations as the original VGPMIL, which is a consequence of the two representations that the Hyperbolic Secant distribution admits. This leads us to propose a general GP-based MIL method that takes different forms by simply leveraging distributions other than the Hyperbolic Secant one. Using the Gamma distribution
    
[^13]: 使用Sinkhorn不确定性集的非凸鲁棒假设检验

    Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets

    [https://arxiv.org/abs/2403.14822](https://arxiv.org/abs/2403.14822)

    提出了使用Sinkhorn不确定性集解决非凸鲁棒假设检验问题的新框架，并引入了确切的混合整数指数锥重构方法，证明了优于目前文献中最先进方法的凸逼近。

    

    我们提出了一个新框架来解决非凸鲁棒假设检验问题，其中目标是寻找最优探测器，以最小化最坏情况下的类型I和类型II风险函数的最大值。分布不确定性集围绕基于Sinkhorn距离的样本得出的经验分布构建。由于目标涉及非凸、非平滑的概率函数，通常难以优化，现有方法往往采用近似而非精确解决方案。为了解决这一挑战，我们引入了一个确切的混合整数指数锥重构问题的方法，可以在适量的输入数据下达到全局最优解。随后，我们提出了一个凸逼近，展示了其优于当前文献中最先进方法的方法。此外，我们建立了鲁棒假设检验与...

    arXiv:2403.14822v1 Announce Type: cross  Abstract: We present a new framework to address the non-convex robust hypothesis testing problem, wherein the goal is to seek the optimal detector that minimizes the maximum of worst-case type-I and type-II risk functions. The distributional uncertainty sets are constructed to center around the empirical distribution derived from samples based on Sinkhorn discrepancy. Given that the objective involves non-convex, non-smooth probabilistic functions that are often intractable to optimize, existing methods resort to approximations rather than exact solutions. To tackle the challenge, we introduce an exact mixed-integer exponential conic reformulation of the problem, which can be solved into a global optimum with a moderate amount of input data. Subsequently, we propose a convex approximation, demonstrating its superiority over current state-of-the-art methodologies in literature. Furthermore, we establish connections between robust hypothesis testi
    
[^14]: 曲率增强流形嵌入与学习

    Curvature Augmented Manifold Embedding and Learning

    [https://arxiv.org/abs/2403.14813](https://arxiv.org/abs/2403.14813)

    将降维问题建模为机械/物理模型，引入曲率增强力的曲率增强流形嵌入与学习（CAMEL）方法提供了一种新的方法来捕捉数据集的n维流形表示。

    

    发表于arXiv:2403.14813v1，其中提出了一种新的降维（DR）和数据可视化方法，称为曲率增强流形嵌入与学习（CAMEL）。其关键创新贡献在于将降维问题建模为一个机械/物理模型，其中节点（数据点）之间的力场被用来找到数据集的n维流形表示。与许多现有的吸引-排斥力方法相比，本方法的一个独特贡献是包含了一个非成对力。引入和讨论了一种新的力场模型，灵感来自于晶格粒子物理学中的多体势和拓扑学中的黎曼曲率。CAMEL中包含了一种曲率增强力。其次，提供了CAMEL用于无监督学习、监督学习、半监督学习/度量学习和逆向学习的公式。然后，通过与现有模型的比较，将CAMEL应用于许多基准数据集。

    arXiv:2403.14813v1 Announce Type: cross  Abstract: A new dimensional reduction (DR) and data visualization method, Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The key novel contribution is to formulate the DR problem as a mechanistic/physics model, where the force field among nodes (data points) is used to find an n-dimensional manifold representation of the data sets. Compared with many existing attractive-repulsive force-based methods, one unique contribution of the proposed method is to include a non-pairwise force. A new force field model is introduced and discussed, inspired by the multi-body potential in lattice-particle physics and Riemann curvature in topology. A curvature-augmented force is included in CAMEL. Following this, CAMEL formulation for unsupervised learning, supervised learning, semi-supervised learning/metric learning, and inverse learning are provided. Next, CAMEL is applied to many benchmark datasets by comparing existing models, suc
    
[^15]: 在未观测混杂因素下审计公平性

    Auditing Fairness under Unobserved Confounding

    [https://arxiv.org/abs/2403.14713](https://arxiv.org/abs/2403.14713)

    在未观测混杂因素的情况下，本文展示了即使在放宽或甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以给出对高风险个体分配率的信息丰富的界限。

    

    决策系统中的一个基本问题是跨越人口统计线存在不公平性。然而，不公平性可能难以量化，特别是如果我们对公平性的理解依赖于难以衡量的风险等观念（例如，对于那些没有其治疗就会死亡的人平等获得治疗）。审计这种不公平性需要准确测量个体风险，而在未观测混杂的现实环境中，难以估计。在这些未观测到的因素“解释”明显差异的情况下，我们可能低估或高估不公平性。在本文中，我们展示了即使在放宽或（令人惊讶地）甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以对高风险个体的分配率给出信息丰富的界限。我们利用了在许多实际环境中（例如引入新型治疗）我们拥有在任何分配之前的数据的事实。

    arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
    
[^16]: 统计无偏回归：一种用于验证回归模型的机器学习方法

    Statistical Agnostic Regression: a machine learning method to validate regression models

    [https://arxiv.org/abs/2402.15213](https://arxiv.org/abs/2402.15213)

    本文提出了一种新的方法，统计无关地评估了线性回归模型，并评估了ML估计在检测方面的表现。

    

    回归分析是统计建模中的一个核心主题，旨在估计因变量（通常称为响应变量）与一个或多个自变量（即解释变量）之间的关系。线性回归是迄今为止在预测、预测或因果推断等多个研究领域执行此任务的最流行方法。除了解决线性回归问题的各种传统方法外，如普通最小二乘法、岭回归或套索回归——这些方法往往是更高级机器学习（ML）技术的基础——后者已成功地应用在这种场景中，但没有对统计显著性进行正式定义。最多，基于经验测量（如残差或准确度）进行置换或基于经典分析，以反映ML估计对检测的更高能力。本文介绍了一种新的方法，该方法统计无关地评估了线性回归模型，并对ML估计在检测方面的表现进行了评估。

    arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
    
[^17]: 数字孪生中的数学机遇（MATH-DT）

    Mathematical Opportunities in Digital Twins (MATH-DT)

    [https://arxiv.org/abs/2402.10326](https://arxiv.org/abs/2402.10326)

    数字孪生中的数学机遇需要基础数学进展，与传统方法不同，数字孪生从特定现实出发，需要多尺度建模和耦合，通过传感器将数据输入，帮助人类做出决策。

    

    这份报告描述了2023年12月11日至13日在乔治梅森大学举办的“数字孪生中的数学机遇”（MATH-DT）研讨会的讨论。报告指出，数字孪生（DT）需要与传统方法不同的基础数学进展。传统模型在生物学、物理学、工程学或医学中起始于通用物理定律（例如方程），通常是对现实的简化。数字孪生则从特定的生态系统、物体或个人（例如个性化医疗）作为现实出发，需要多尺度、多物理建模和耦合。因此，这些过程在模拟和建模流程的两端开始，需要不同的可靠性标准和不确定性评估。此外，与现有方法不同，数字孪生帮助人类为物理系统做出决策，其通过传感器将数据传输到数字孪生中。

    arXiv:2402.10326v1 Announce Type: cross  Abstract: The report describes the discussions from the Workshop on Mathematical Opportunities in Digital Twins (MATH-DT) from December 11-13, 2023, George Mason University.   It illustrates that foundational Mathematical advances are required for Digital Twins (DTs) that are different from traditional approaches. A traditional model, in biology, physics, engineering or medicine, starts with a generic physical law (e.g., equations) and is often a simplification of reality. A DT starts with a specific ecosystem, object or person (e.g., personalized care) representing reality, requiring multi -scale, -physics modeling and coupling. Thus, these processes begin at opposite ends of the simulation and modeling pipeline, requiring different reliability criteria and uncertainty assessments. Additionally, unlike existing approaches, a DT assists humans to make decisions for the physical system, which (via sensors) in turn feeds data into the DT, and oper
    
[^18]: 独立学习将时间序列片段嵌入

    Learning to Embed Time Series Patches Independently

    [https://arxiv.org/abs/2312.16427](https://arxiv.org/abs/2312.16427)

    学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。

    

    最近，掩码时间序列建模作为一种自监督表示学习策略引起了广泛关注。受计算机视觉中的掩码图像建模启发，最近的研究首先将时间序列进行分块处理并部分掩盖，然后训练Transformer模型通过从未掩盖的块预测被掩盖块来捕捉块之间的依赖关系。然而，我们认为捕捉这种块之间的依赖关系可能不是时间序列表示学习的最佳策略；相反，独立学习嵌入片段会产生更好的时间序列表示。具体而言，我们建议使用1）简单的块重构任务，自动将每个块进行编码而不查看其他块，以及2）独自嵌入每个块的简单块式MLP。此外，我们引入互补对比学习来有效地分层捕获相邻时间序列信息。

    arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
    
[^19]: 时间序列的软对比学习

    Soft Contrastive Learning for Time Series

    [https://arxiv.org/abs/2312.16424](https://arxiv.org/abs/2312.16424)

    提出了一种名为SoftCLT的方法，通过引入实例级和时间级软对比损失，解决了在时间序列中忽略固有相关性所导致的学习表示质量下降的问题。

    

    对比学习已经被证明在自监督学习中对于从时间序列中学习表示是有效的。然而，将时间序列中相似的实例或相邻时间戳的值进行对比会忽略它们固有的相关性，从而导致学习表示的质量下降。为了解决这个问题，我们提出了SoftCLT，一种简单而有效的时间序列软对比学习策略。这是通过引入从零到一的软赋值的实例级和时间级对比损失来实现的。具体来说，我们为1)基于数据空间上的时间序列之间的距离定义了实例级对比损失的软赋值，并为2)基于时间戳之间的差异定义了时间级对比损失。SoftCLT是一种即插即用的时间序列对比学习方法，可以提高学习表示的质量，没有过多复杂的设计。

    arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
    
[^20]: 对称破缺和等变神经网络

    Symmetry Breaking and Equivariant Neural Networks

    [https://arxiv.org/abs/2312.09016](https://arxiv.org/abs/2312.09016)

    提出了一种新颖的“放松等变性”的概念，用于解决等变函数无法在单个数据样本层面打破对称的限制，并展示了如何将其应用于等变多层感知机（E-MLP）中。

    

    在深度学习中使用对称作为归纳偏差已被证明是一种有效的方法，可以设计出高效的模型。然而，神经网络中对称和等变性的关系并不总是显而易见。本文分析了等变函数中出现的一个关键限制：它们不能在单个数据样本的层面打破对称。为此，我们引入了一个新颖的“放松等变性”的概念来规避这个限制。我们进一步展示了如何将这种放松引入等变多层感知机（E-MLP），提供了一种替代注入噪声的方法。接着讨论了对称破缺在物理、图表示学习、组合优化和等变解码等各种应用领域的相关性。

    arXiv:2312.09016v2 Announce Type: replace  Abstract: Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.
    
[^21]: 用于数据分析的多尺度霍奇散射网络

    Multiscale Hodge Scattering Networks for Data Analysis

    [https://arxiv.org/abs/2311.10270](https://arxiv.org/abs/2311.10270)

    提出了多尺度霍奇散射网络（MHSNs），利用多尺度基础词典和卷积结构，生成对节点排列不变的特征。

    

    我们提出了一种新的散射网络，用于在单纯复合仿射上测量的信号，称为\emph{多尺度霍奇散射网络}（MHSNs）。我们的构造基于单纯复合仿射上的多尺度基础词典，即$\kappa$-GHWT和$\kappa$-HGLET，我们最近为给定单纯复合仿射中的维度$\kappa \in \mathbb{N}$推广了基于节点的广义哈-沃什变换（GHWT）和分层图拉普拉斯特征变换（HGLET）。$\kappa$-GHWT和$\kappa$-HGLET都形成冗余集合（即词典）的多尺度基础向量和给定信号的相应扩展系数。我们的MHSNs使用类似于卷积神经网络（CNN）的分层结构来级联词典系数模的矩。所得特征对单纯复合仿射的重新排序不变（即节点排列的置换

    arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
    
[^22]: 在主采样空间中学习重要性采样

    Learning to Importance Sample in Primary Sample Space

    [https://arxiv.org/abs/1808.07840](https://arxiv.org/abs/1808.07840)

    提出了一种利用神经网络在主采样空间中学习重要性采样的新方法，用于渲染算法中的方差缩减。

    

    重要性采样是蒙特卡洛渲染中最广泛使用的方差缩减策略之一。本文提出了一种新颖的重要性采样技术，该技术利用神经网络学习如何从一组样本表示的目标密度中进行采样。我们的方法将现有的蒙特卡洛渲染算法视为黑匣子。在场景相关的训练阶段，我们通过最大似然估计学习在渲染算法的主采样空间中生成具有目标密度的样本。我们利用最近设计用于在高维空间中表示实值非体积保持（'Real NVP'）变换的神经网络架构。我们使用Real NVP来非线性扭曲主采样空间并获得期望的密度。此外，Real NVP有效地计算了扭曲的雅可比行列式，这是实现积分变换所需的。

    arXiv:1808.07840v2 Announce Type: replace  Abstract: Importance sampling is one of the most widely used variance reduction strategies in Monte Carlo rendering. In this paper, we propose a novel importance sampling technique that uses a neural network to learn how to sample from a desired density represented by a set of samples. Our approach considers an existing Monte Carlo rendering algorithm as a black box. During a scene-dependent training phase, we learn to generate samples with a desired density in the primary sample space of the rendering algorithm using maximum likelihood estimation. We leverage a recent neural network architecture that was designed to represent real-valued non-volume preserving ('Real NVP') transformations in high dimensional spaces. We use Real NVP to non-linearly warp primary sample space and obtain desired densities. In addition, Real NVP efficiently computes the determinant of the Jacobian of the warp, which is required to implement the change of integratio
    
[^23]: 从复杂到清晰：通过Clifford的几何代数和凸优化的分析表达深度神经网络的权重

    From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])

    [http://arxiv.org/abs/2309.16512](http://arxiv.org/abs/2309.16512)

    本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。

    

    本文介绍了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们展示了当使用标准正则化损失进行训练时，深度ReLU神经网络的最优权重由训练样本的楔积给出。此外，训练问题可简化为对楔积特征进行凸优化，在其中编码训练数据集的几何结构。该结构以数据向量生成的三角形和平行体的有符号体积表示。凸问题通过$\ell_1$正则化找到样本的一个小子集，以发现仅相关的楔积特征。我们的分析提供了对深度神经网络内部工作机制的新视角，并揭示了隐藏层的作用。

    In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
    
[^24]: CARE: 大规模精确矩阵估计用于组成数据

    CARE: Large Precision Matrix Estimation for Compositional Data. (arXiv:2309.06985v1 [stat.ME])

    [http://arxiv.org/abs/2309.06985](http://arxiv.org/abs/2309.06985)

    CARE方法通过精确指定组成数据的精确矩阵，并利用其与基础矩阵之间的联系，提出了一种估计稀疏基础矩阵的组成数据估计方法。通过理论分析，我们发现在足够高的维度下，CARE估计器实现了极小化风险的速率。

    

    高维组成数据在许多应用中很常见。简单形式的约束对于推断组成数据中的条件依赖关系，即大规模精确矩阵所编码的组分之间的关系，带来了固有的挑战。我们引入了组成精确矩阵的精确定义，并将其与其基础对应物联系起来，在适当的稀疏性假设下得到渐近可辨认性。通过利用这种联系，我们提出了一种适合估计稀疏基础精确矩阵的组成适应正则化估计（CARE）方法。我们推导了估计器的收敛速率，并提供了关于支持恢复和数据驱动参数调整的理论保证。我们的理论揭示了鉴定和估计之间的有趣权衡，从而突显了维度在组成数据分析中的优势。特别地，在足够高的维度下，CARE估计器实现了极小化风险的速率。

    High-dimensional compositional data are prevalent in many applications. The simplex constraint poses intrinsic challenges to inferring the conditional dependence relationships among the components forming a composition, as encoded by a large precision matrix. We introduce a precise specification of the compositional precision matrix and relate it to its basis counterpart, which is shown to be asymptotically identifiable under suitable sparsity assumptions. By exploiting this connection, we propose a composition adaptive regularized estimation (CARE) method for estimating the sparse basis precision matrix. We derive rates of convergence for the estimator and provide theoretical guarantees on support recovery and data-driven parameter tuning. Our theory reveals an intriguing trade-off between identification and estimation, thereby highlighting the blessing of dimensionality in compositional data analysis. In particular, in sufficiently high dimensions, the CARE estimator achieves minimax
    
[^25]: (核) 岭回归中过度拟合成本的不可知观察

    An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression. (arXiv:2306.13185v1 [stat.ML])

    [http://arxiv.org/abs/2306.13185](http://arxiv.org/abs/2306.13185)

    本文研究了核岭回归中过拟合成本，采用“不可知”的观点，以分析样本量和任务特征结构对成本的影响。通过分析提供了更细致的过度拟合表征。

    

    本研究研究了有噪声的核岭回归 (KRR) 中过拟合的成本，我们将其定义为插值无岭模型的测试误差与最优调节模型的测试误差之比。我们采用“不可知”的观点，即对于任何目标函数，即使样本量不足以达到一致性或目标函数不在 RKHS 中，我们也将成本看作样本量的函数。使用最近推导出的（非严格的）风险评估，以任务特征结构为基础，利用高斯普适性假设分析过度拟合成本。我们的分析提供了良性、缓和和灾难性过度拟合（参见 Mallinar 等人 2022）的更精细的表征。

    We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an "agnostic" view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).
    
[^26]: EC-NAS: 面向神经架构搜索的能耗感知表格基准

    EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06015](http://arxiv.org/abs/2210.06015)

    提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。

    

    近年来，选择、训练和部署深度学习模型所需的能量消耗不断增加。本文旨在支持设计能效高、训练资源消耗较低、适用于实际边缘/移动计算环境并具有环境可持续性的深度学习模型。我们提出将能效作为神经架构搜索 (NAS) 的一项额外性能指标，并通过添加不同架构的能耗和碳足迹信息，提供更新的表格基准 EC-NAS 以在较低计算成本下评估 NAS 策略。EC-NAS 还包括用于预测能耗的代理模型，并有助于降低总能耗。

    Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
    

