# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [How Will It Drape Like? Capturing Fabric Mechanics from Depth Images.](http://arxiv.org/abs/2304.06704) | 该论文提出了一种使用深度相机进行随意捕捉的方法来预测织物的自由落体效应，并创新性地提出了一种模拟到真实的策略来训练学习框架，该框架可以输出完整的力学参数集。 |
| [^2] | [OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems.](http://arxiv.org/abs/2304.06686) | 本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。 |
| [^3] | [Do deep neural networks have an inbuilt Occam's razor?.](http://arxiv.org/abs/2304.06670) | 该研究利用基于函数先验的贝叶斯视角来研究深度神经网络（DNNs）的表现来源，结果表明DNNs之所以成功，是因为它对于具有结构的数据，具备一种内在的奥卡姆剃刀式的归纳偏差，足以抵消函数数量及复杂度的指数级增长。 |
| [^4] | [Convergence rate of Tsallis entropic regularized optimal transport.](http://arxiv.org/abs/2304.06616) | 本论文研究了Tsallis熵正则化最优输运问题，在讨论正则化参数接近0时的收敛速率时，利用量化和影子方法确定了收敛速率，并与KL散度进行了比较，证明KL散度在Tsallis相对熵意义下具有最快的收敛速率。 |
| [^5] | [Bayesian Inference for Jump-Diffusion Approximations of Biochemical Reaction Networks.](http://arxiv.org/abs/2304.06592) | 本文提出了一种基于贝叶斯推断算法的跳跃扩散模型，该模型可以很好地模拟生化反应网络的动态，尤其是在参数和种群状态方面难以表征的情况下。 |
| [^6] | [Bayes classifier cannot be learned from noisy responses with unknown noise rates.](http://arxiv.org/abs/2304.06574) | 本文研究了在有噪声标签的情况下训练分类器的问题，发现贝叶斯决策规则通常无法识别，并提出了一种简单算法来学习贝叶斯决策规则，不需要知道噪声分布。 |
| [^7] | [counterfactuals: An R Package for Counterfactual Explanation Methods.](http://arxiv.org/abs/2304.06569) | 该论文介绍了一个统一且模块化的 R6 接口，用于具体实现反事实解释方法。通过实现三种方法并推广到不同的情境中，结合真实用例，此方法能够快速准确地得出有关如何更改单个观测值的特征值以获得所需预测的信息。 |
| [^8] | [Non-asymptotic convergence bounds for Sinkhorn iterates and their gradients: a coupling approach.](http://arxiv.org/abs/2304.06549) | 本文提出了一种新的耦合方法分析了在$d$维环面$\mathbb{T}_L^d$上定义的在Haar测度相对于密度可允许的概率度量的Sinkhorn算法的收敛性，证明了Sinkhorn迭代及其梯度的点常指数收敛性，同时导出了其非渐近误差界限，这些界限与环境维数$d$和离散元素代价矩阵的尺寸无关。 |
| [^9] | [Signal identification without signal formulation.](http://arxiv.org/abs/2304.06522) | 该研究提出了一种无需信号建模即可识别信号的方法，该方法基于样本和其邻居之间相对距离，可以在小样本和高维数据中识别“类似于信号”的变量。 |
| [^10] | [Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective.](http://arxiv.org/abs/2304.06412) | 本论文提出了一种综合的、多阶段的机器学习方法，利用分位数回归森林生成区间预测来解决预测流程监控中的问题，同时使用SHapley可加解释来解释模型不确定性的来源。 |
| [^11] | [Understanding Overfitting in Adversarial Training in Kernel Regression.](http://arxiv.org/abs/2304.06326) | 本文研究了核回归的对抗训练和带噪声的数据增强，发现如果没有适当的正则化，这两种方法可能会导致过拟合现象，但适当的正则化可以缓解这种现象，提高性能。 |
| [^12] | [Fair Grading Algorithms for Randomized Exams.](http://arxiv.org/abs/2304.06254) | 本文对随机考试评分算法进行了研究，提出了最大似然评分算法Maximal-margin algorithm，利用Bradley-Terry-Luce模型进行评分。在实验和模拟中，相较于简单平均方法，该算法在预测精准度和公平性方面表现更优。 |
| [^13] | [Importance is Important: A Guide to Informed Importance Tempering Methods.](http://arxiv.org/abs/2304.06251) | 本论文详细介绍了一种易于实施的MCMC算法IIT及其在许多情况下的应用。该算法始终接受有信息的提议，可与其他MCMC技术相结合，并带来新的优化抽样器的机会。 |
| [^14] | [Post-selection Inference for Conformal Prediction: Trading off Coverage for Precision.](http://arxiv.org/abs/2304.06158) | 本论文提出一种使用无分布信赖带的 uniform conformal inference 算法，实现任意数据相关误覆盖水平的有限样本预测保证的统一一致性推理。 |
| [^15] | [Energy-guided Entropic Neural Optimal Transport.](http://arxiv.org/abs/2304.06094) | 本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。 |
| [^16] | [Reflected Diffusion Models.](http://arxiv.org/abs/2304.04740) | 该论文提出了反射扩散模型，通过学习反射随机微分方程的扰动评分函数，将数据约束原则性地整合到生成过程中，以取代之前采用的导致不自然样本的阈值处理方案。 |
| [^17] | [PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling.](http://arxiv.org/abs/2304.04307) | PriorCVAE 提出了一种处理高斯过程先验 MCMC 参数推断的贝叶斯深度生成建模新方法，可通过将 VAE 建模条件化于随机过程超参数处理超参数推断与学习先验之间的信息流断裂问题。 |
| [^18] | [Towards Inferential Reproducibility of Machine Learning Research.](http://arxiv.org/abs/2302.04054) | 本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。 |
| [^19] | [KL-divergence Based Deep Learning for Discrete Time Model.](http://arxiv.org/abs/2208.05100) | 该论文提出了一种基于KL散度的深度学习程序，用于解决生存分析中短数据问题，通过将外部生存预测模型与新收集的时间至事件数据相结合来获得更好的性能和鲁棒性。 |
| [^20] | [Generative Modelling With Inverse Heat Dissipation.](http://arxiv.org/abs/2206.13397) | 该论文提出了一种新的类似扩散的模型来生成图像，它通过随机反转热方程在2D平面上运行来生成图像，并展示了与标准扩散模型不同的新颖定性性质，包括图像中整体颜色和形状的解缠绕现象。 |
| [^21] | [Variable importance without impossible data.](http://arxiv.org/abs/2205.15750) | 评估黑箱预测模型中变量重要性的流行方法不可信，因为使用了不可能的数据。本文提出一种名为Cohort Shapley的方法，它基于经济博弈理论，仅使用实际观测到的数据来量化变量重要性，可以解决算法公平性问题。 |
| [^22] | [A method to integrate and classify normal distributions.](http://arxiv.org/abs/2012.14331) | 本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。 |
| [^23] | [The Role of Mutual Information in Variational Classifiers.](http://arxiv.org/abs/2010.11642) | 本文研究了基于随机编码的分类器的泛化误差，并得出了泛化误差受输入特征和潜空间表示之间互信息限制的结论。 |
| [^24] | [Semi-supervised Hypergraph Node Classification on Hypergraph Line Expansion.](http://arxiv.org/abs/2005.04843) | 本文提出了一种新的超图学习方法，即“线扩展(LE)”，该方法通过将顶点-超边对作为“线节点”，在超图中引出同构结构，适用于各种超图扩展并达到了显著优于SOTA的效果。 |

# 详细

[^1]: 如何预测织物的自由落体效应? 通过深度图像捕捉织物力学特性

    How Will It Drape Like? Capturing Fabric Mechanics from Depth Images. (arXiv:2304.06704v1 [cs.CV])

    [http://arxiv.org/abs/2304.06704](http://arxiv.org/abs/2304.06704)

    该论文提出了一种使用深度相机进行随意捕捉的方法来预测织物的自由落体效应，并创新性地提出了一种模拟到真实的策略来训练学习框架，该框架可以输出完整的力学参数集。

    

    我们提出了一种使用深度相机进行随意捕捉的方法来估计织物的力学参数。我们的方法可以创建真实世界纺织材料的机械正确数字表示，这是许多交互式设计和工程应用的基本步骤。与现有的捕捉方法相比，我们的解决方案可以在规模上进行捕捉，与纺织品的光学外观无关，并且易于由非专业操作者进行织物排列。为此，我们提出了一种模拟到真实的策略，以训练一个基于学习的框架，该框架可以将一个或多个图像作为输入并输出完整的力学参数集。由于经过精心设计的数据增强和转移学习协议，我们的解决方案可以推广到真实图像，尽管只是在合成数据上进行训练，因此成功地关闭了模拟到真实的循环。

    We propose a method to estimate the mechanical parameters of fabrics using a casual capture setup with a depth camera. Our approach enables to create mechanically-correct digital representations of real-world textile materials, which is a fundamental step for many interactive design and engineering applications. As opposed to existing capture methods, which typically require expensive setups, video sequences, or manual intervention, our solution can capture at scale, is agnostic to the optical appearance of the textile, and facilitates fabric arrangement by non-expert operators. To this end, we propose a sim-to-real strategy to train a learning-based framework that can take as input one or multiple images and outputs a full set of mechanical parameters. Thanks to carefully designed data augmentation and transfer learning protocols, our solution generalizes to real images despite being trained only on synthetic data, hence successfully closing the sim-to-real loop.Key in our work is to 
    
[^2]: OKRidge: 用于学习动态系统的可扩展 k 稀疏岭回归

    OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])

    [http://arxiv.org/abs/2304.06686](http://arxiv.org/abs/2304.06686)

    本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。

    

    本文解决了科学发现中的一个重要问题，即，确定非线性动态系统的稀疏控制方程，通过求解稀疏岭回归问题可以证明最优性，以确定驱动基础动态的项。我们提出了一种称为 OKRidge 的快速算法，用一种新颖的下界计算方法，涉及鞍点公式，然后使用线性系统或基于 ADMM 的方法来解决，其中可以通过解决另一个线性系统和单调回归问题来有效地计算近端算子。我们还提出了一种启动我们求解器的方法，利用了波束搜索。在实验中，我们的方法达到可证明的最优性，并且运行时间比商业求解器 Gurobi 解决的现有 MIP公式运行时间快几个数量级。

    We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
    
[^3]: 深度神经网络是否具备内置的奥卡姆剃刀？

    Do deep neural networks have an inbuilt Occam's razor?. (arXiv:2304.06670v1 [cs.LG])

    [http://arxiv.org/abs/2304.06670](http://arxiv.org/abs/2304.06670)

    该研究利用基于函数先验的贝叶斯视角来研究深度神经网络（DNNs）的表现来源，结果表明DNNs之所以成功，是因为它对于具有结构的数据，具备一种内在的奥卡姆剃刀式的归纳偏差，足以抵消函数数量及复杂度的指数级增长。

    

    超参数化深度神经网络（DNNs）的卓越性能必须源自于网络架构、训练算法和数据结构之间的相互作用。为了区分这三个部分，我们应用了基于DNN所表达的函数的贝叶斯视角来进行监督学习。经过网络确定的函数先验通过利用有序和混沌状态之间的转变而变化。对于布尔函数分类，我们利用函数的误差谱在数据上进行可能性的近似。当与先验相结合时，它可以精确地预测使用随机梯度下降训练的DNN的后验概率。该分析揭示了结构化数据，以及内在的奥卡姆剃刀式归纳偏差，即足以抵消复杂度随函数数量呈指数增长而产生的影响，是DNNs成功的关键。

    The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs.
    
[^4]: Tsallis熵正则化最优输运的收敛速率

    Convergence rate of Tsallis entropic regularized optimal transport. (arXiv:2304.06616v1 [math.OC])

    [http://arxiv.org/abs/2304.06616](http://arxiv.org/abs/2304.06616)

    本论文研究了Tsallis熵正则化最优输运问题，在讨论正则化参数接近0时的收敛速率时，利用量化和影子方法确定了收敛速率，并与KL散度进行了比较，证明KL散度在Tsallis相对熵意义下具有最快的收敛速率。

    

    本文考虑了Tsallis熵正则化最优输运问题，并且讨论了当正则化参数$\varepsilon$趋近于0时的收敛速率。特别地，我们利用Eckstein-Nutz提出的量化和影子方法来确定Tsallis熵正则化最优输运的收敛速率。我们将其与使用Kullback-Leibler（KL）散度的熵正则化最优输运的收敛速率进行了比较，并证明了在Tsallis相对熵意义下KL具有最快的收敛速率。

    In this paper, we consider Tsallis entropic regularized optimal transport and discuss the convergence rate as the regularization parameter $\varepsilon$ goes to $0$. In particular, we establish the convergence rate of the Tsallis entropic regularized optimal transport using the quantization and shadow arguments developed by Eckstein--Nutz. We compare this to the convergence rate of the entropic regularized optimal transport with Kullback--Leibler (KL) divergence and show that KL is the fastest convergence rate in terms of Tsallis relative entropy.
    
[^5]: 生化反应网络的跳跃扩散逼近的贝叶斯推断

    Bayesian Inference for Jump-Diffusion Approximations of Biochemical Reaction Networks. (arXiv:2304.06592v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.06592](http://arxiv.org/abs/2304.06592)

    本文提出了一种基于贝叶斯推断算法的跳跃扩散模型，该模型可以很好地模拟生化反应网络的动态，尤其是在参数和种群状态方面难以表征的情况下。

    

    生化反应网络是由多个不同物种之间相互作用的反应组成的。通常情况下，由于反应速率和物种丰度的高度变化，这些网络会表现出多尺度行为。所谓的跳跃扩散逼近是这种系统建模的有价值工具。该逼近通过将反应网络分为快反应和慢反应的快和慢子组来构建。这使得可以使用 Langevin 方程模拟快组的动态，同时为慢组的动态保留马尔可夫跳跃过程模型。由于大多数生化过程在参数和种群状态方面都很难表征，因此估计隐藏变量的方法非常重要。在本文中，我们基于马尔可夫链蒙特卡罗方法开发了一种易于处理的贝叶斯推断算法。所提出的阻塞 Gibbs 粒子

    Biochemical reaction networks are an amalgamation of reactions where each reaction represents the interaction of different species. Generally, these networks exhibit a multi-scale behavior caused by the high variability in reaction rates and abundances of species. The so-called jump-diffusion approximation is a valuable tool in the modeling of such systems. The approximation is constructed by partitioning the reaction network into a fast and slow subgroup of fast and slow reactions, respectively. This enables the modeling of the dynamics using a Langevin equation for the fast group, while a Markov jump process model is kept for the dynamics of the slow group. Most often biochemical processes are poorly characterized in terms of parameters and population states. As a result of this, methods for estimating hidden quantities are of significant interest. In this paper, we develop a tractable Bayesian inference algorithm based on Markov chain Monte Carlo. The presented blocked Gibbs particl
    
[^6]: 贝叶斯分类器无法从具有未知噪声率的嘈杂响应中学习。(arXiv:2304.06574v1 [stat.ML])

    Bayes classifier cannot be learned from noisy responses with unknown noise rates. (arXiv:2304.06574v1 [stat.ML])

    [http://arxiv.org/abs/2304.06574](http://arxiv.org/abs/2304.06574)

    本文研究了在有噪声标签的情况下训练分类器的问题，发现贝叶斯决策规则通常无法识别，并提出了一种简单算法来学习贝叶斯决策规则，不需要知道噪声分布。

    

    用嘈杂的标签训练分类器通常需要学习者指定标签噪音的分布，但在实际中往往是未知的。尽管最近有一些尝试放宽该要求，但我们表明，在大多数嘈杂标签分类问题中，贝叶斯决策规则是无法识别的。这表明一般情况下无法绕过或放宽该要求。在贝叶斯决策规则被识别的特殊情况下，我们开发了一种简单的算法来学习贝叶斯决策规则，不需要知道噪声分布。

    Training a classifier with noisy labels typically requires the learner to specify the distribution of label noise, which is often unknown in practice. Although there have been some recent attempts to relax that requirement, we show that the Bayes decision rule is unidentified in most classification problems with noisy labels. This suggests it is generally not possible to bypass/relax the requirement. In the special cases in which the Bayes decision rule is identified, we develop a simple algorithm to learn the Bayes decision rule, that does not require knowledge of the noise distribution.
    
[^7]: counterfactuals: 用于反事实解释方法的 R 包

    counterfactuals: An R Package for Counterfactual Explanation Methods. (arXiv:2304.06569v1 [stat.ML])

    [http://arxiv.org/abs/2304.06569](http://arxiv.org/abs/2304.06569)

    该论文介绍了一个统一且模块化的 R6 接口，用于具体实现反事实解释方法。通过实现三种方法并推广到不同的情境中，结合真实用例，此方法能够快速准确地得出有关如何更改单个观测值的特征值以获得所需预测的信息。

    

    反事实解释方法提供有关如何更改单个观测值的特征值以获得所需预测的信息。尽管研究中提出了越来越多的方法，但只有少数具有广泛变化的接口和要求的实现存在。在本文中，我们介绍 counterfactuals R 包，它提供了一个基于 R6 的模块化和统一的接口，用于反事实解释方法。我们已经实现了三种现有的反事实解释方法，并提出了一些可选的方法学扩展，以将这些方法推广到不同的场景并使其更具可比性。我们使用真实用例解释了包的结构和工作流程，并展示了如何将其他反事实解释方法集成到包中。此外，我们针对各种模型和数据集比较了实施的方法，以评估其反事实解释的质量和运行时行为。

    Counterfactual explanation methods provide information on how feature values of individual observations must be changed to obtain a desired prediction. Despite the increasing amount of proposed methods in research, only a few implementations exist whose interfaces and requirements vary widely. In this work, we introduce the counterfactuals R package, which provides a modular and unified R6-based interface for counterfactual explanation methods. We implemented three existing counterfactual explanation methods and propose some optional methodological extensions to generalize these methods to different scenarios and to make them more comparable. We explain the structure and workflow of the package using real use cases and show how to integrate additional counterfactual explanation methods into the package. In addition, we compared the implemented methods for a variety of models and datasets with regard to the quality of their counterfactual explanations and their runtime behavior.
    
[^8]: 针对Sinkhorn迭代及其梯度的非渐进收敛界限: 一种耦合方法

    Non-asymptotic convergence bounds for Sinkhorn iterates and their gradients: a coupling approach. (arXiv:2304.06549v1 [math.PR])

    [http://arxiv.org/abs/2304.06549](http://arxiv.org/abs/2304.06549)

    本文提出了一种新的耦合方法分析了在$d$维环面$\mathbb{T}_L^d$上定义的在Haar测度相对于密度可允许的概率度量的Sinkhorn算法的收敛性，证明了Sinkhorn迭代及其梯度的点常指数收敛性，同时导出了其非渐近误差界限，这些界限与环境维数$d$和离散元素代价矩阵的尺寸无关。

    

    最近，计算最优传输（OT）作为一个在各个领域应用的强有力的框架出现了。本文着重讨论对原始OT问题的一种放松形式，即熵OT问题，它可以实现高效的实际算法解决方案，即使在高维情况下也是如此。这个公式也被称为Schr\"odinger桥问题，显然与随机最优控制（SOC）相关，并且可以用流行的Sinkhorn算法来解决。对于离散状态空间的情况，已知该算法具有指数收敛性；然而，在更一般的情况下实现类似的收敛速率仍然是一个积极的研究领域。在本文中，我们分析了在$d$维环面$\mathbb{T}_L^d$上定义的在Haar测度相对于密度可允许的概率度量的Sinkhorn算法的收敛性。特别的，在选择一定范围内的熵参数时，我们证明了Sinkhorn迭代及其梯度的点常指数收敛性。我们的证明技术依赖于一种耦合方法，受到[C. E. Brubaker, M. Fathi和G. Peyré, NeurIPS 2020]的最新工作的启发。我们还推导了Sinkhorn算法的新收敛速率，从而导出了Sinkhorn迭代和耦合的非渐近误差界限，这些界限独立于环境维数$d$和离散元素代价矩阵的尺寸。

    Computational optimal transport (OT) has recently emerged as a powerful framework with applications in various fields. In this paper we focus on a relaxation of the original OT problem, the entropic OT problem, which allows to implement efficient and practical algorithmic solutions, even in high dimensional settings. This formulation, also known as the Schr\"odinger Bridge problem, notably connects with Stochastic Optimal Control (SOC) and can be solved with the popular Sinkhorn algorithm. In the case of discrete-state spaces, this algorithm is known to have exponential convergence; however, achieving a similar rate of convergence in a more general setting is still an active area of research. In this work, we analyze the convergence of the Sinkhorn algorithm for probability measures defined on the $d$-dimensional torus $\mathbb{T}_L^d$, that admit densities with respect to the Haar measure of $\mathbb{T}_L^d$. In particular, we prove pointwise exponential convergence of Sinkhorn iterat
    
[^9]: 无需信号建模的信号识别方法

    Signal identification without signal formulation. (arXiv:2304.06522v1 [physics.data-an])

    [http://arxiv.org/abs/2304.06522](http://arxiv.org/abs/2304.06522)

    该研究提出了一种无需信号建模即可识别信号的方法，该方法基于样本和其邻居之间相对距离，可以在小样本和高维数据中识别“类似于信号”的变量。

    

    当信号和噪声混合时，物理学家通常通过信号建模来识别信号，而统计学家则相反，他们试图对噪声进行建模来识别信号。在本研究中，我们应用了统计学家的信号检测概念，对具有小样本和高维数据的物理数据进行了处理，而不对信号进行建模。自然界中的大部分数据，无论是噪声还是信号，都被假定为是由动态系统生成的；因此，在这些生成过程之间基本上没有区别。我们提出了动态系统的相关长度和样本数对于在这样的系统中生成的信号变量中区分噪声变量的实际定义至关重要。由于具有短期相关性的变量随着样本数的减少会更快地达到正态分布，因此它们被认为是“类似于噪声”的变量，而具有相反特性的变量则是“类似于信号”的变量。正态性检验不适用于小样本和高维数据，因此我们提出了一种基于样本和其邻居之间相对距离的新方法来识别“类似于噪声”的变量。实验证明，所提出的方法可以在不进行任何信号建模的情况下识别“类似于信号”的变量。

    When there are signals and noises, physicists try to identify signals by modeling them, whereas statisticians oppositely try to model noise to identify signals. In this study, we applied the statisticians' concept of signal detection of physics data with small-size samples and high dimensions without modeling the signals. Most of the data in nature, whether noises or signals, are assumed to be generated by dynamical systems; thus, there is essentially no distinction between these generating processes. We propose that the correlation length of a dynamical system and the number of samples are crucial for the practical definition of noise variables among the signal variables generated by such a system. Since variables with short-term correlations reach normal distributions faster as the number of samples decreases, they are regarded to be ``noise-like'' variables, whereas variables with opposite properties are ``signal-like'' variables. Normality tests are not effective for data of small-
    
[^10]: 量化和解释预测流程监控中的机器学习不确定性：运筹学视角

    Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective. (arXiv:2304.06412v1 [cs.LG])

    [http://arxiv.org/abs/2304.06412](http://arxiv.org/abs/2304.06412)

    本论文提出了一种综合的、多阶段的机器学习方法，利用分位数回归森林生成区间预测来解决预测流程监控中的问题，同时使用SHapley可加解释来解释模型不确定性的来源。

    

    本文介绍了一种综合的、多阶段的机器学习方法，有效地将信息系统和人工智能融合，以增强运筹学领域内的决策过程。所提出的框架巧妙地解决了现有解决方案普遍存在的问题，比如忽略关键生产参数的数据驱动估计、仅生成点预测而不考虑模型不确定性以及缺乏关于这种不确定性来源的解释。我们的方法利用分位数回归森林生成区间预测，同时使用本地和全局变体的SHapley可加解释来解决研究的预测性过程监控问题。所提出的方法在一个真实的生产计划案例中得到了实际应用，并强调了规定性分析在精细化决策过程中的潜力。本文强调

    This paper introduces a comprehensive, multi-stage machine learning methodology that effectively integrates information systems and artificial intelligence to enhance decision-making processes within the domain of operations research. The proposed framework adeptly addresses common limitations of existing solutions, such as the neglect of data-driven estimation for vital production parameters, exclusive generation of point forecasts without considering model uncertainty, and lacking explanations regarding the sources of such uncertainty. Our approach employs Quantile Regression Forests for generating interval predictions, alongside both local and global variants of SHapley Additive Explanations for the examined predictive process monitoring problem. The practical applicability of the proposed methodology is substantiated through a real-world production planning case study, emphasizing the potential of prescriptive analytics in refining decision-making procedures. This paper accentuates
    
[^11]: 理解核回归对抗训练中的过拟合现象

    Understanding Overfitting in Adversarial Training in Kernel Regression. (arXiv:2304.06326v1 [stat.ML])

    [http://arxiv.org/abs/2304.06326](http://arxiv.org/abs/2304.06326)

    本文研究了核回归的对抗训练和带噪声的数据增强，发现如果没有适当的正则化，这两种方法可能会导致过拟合现象，但适当的正则化可以缓解这种现象，提高性能。

    

    对抗训练和带噪声的数据增强是提高神经网络性能的常见方法。本文研究了在再生希尔伯特空间（RKHS）中正则化回归的对抗训练和带噪声的数据增强。当攻击和噪声大小以及正则化参数趋向于零时，建立了这些技术的极限公式。根据该极限公式，分析了特定情况并证明了，如果没有适当的正则化，这两种方法可能具有大于标准核回归的广义误差和Lipschitz常数。然而，通过选择适当的正则化参数，这两种方法可以优于标准核回归，达到更小的广义误差和Lipschitz常数。这些发现支持对抗训练可能导致过拟合的经验观察，以及适当的正则化方法能够缓解这种过拟合现象。

    Adversarial training and data augmentation with noise are widely adopted techniques to enhance the performance of neural networks. This paper investigates adversarial training and data augmentation with noise in the context of regularized regression in a reproducing kernel Hilbert space (RKHS). We establish the limiting formula for these techniques as the attack and noise size, as well as the regularization parameter, tend to zero. Based on this limiting formula, we analyze specific scenarios and demonstrate that, without appropriate regularization, these two methods may have larger generalization error and Lipschitz constant than standard kernel regression. However, by selecting the appropriate regularization parameter, these two methods can outperform standard kernel regression and achieve smaller generalization error and Lipschitz constant. These findings support the empirical observations that adversarial training can lead to overfitting, and appropriate regularization methods, suc
    
[^12]: 面向随机考试的公平评分算法研究

    Fair Grading Algorithms for Randomized Exams. (arXiv:2304.06254v1 [stat.ML])

    [http://arxiv.org/abs/2304.06254](http://arxiv.org/abs/2304.06254)

    本文对随机考试评分算法进行了研究，提出了最大似然评分算法Maximal-margin algorithm，利用Bradley-Terry-Luce模型进行评分。在实验和模拟中，相较于简单平均方法，该算法在预测精准度和公平性方面表现更优。

    

    本文研究了随机考试的评分算法。在随机考试中，每个学生需回答来自较大题库的随机题目，评分规则通常采用简单平均的方式，即计算每个学生回答的题目得分的平均值。虽然这种方式在随机选择题目时是公平的，但在具体答题情境中并不公平。公平评分问题是要评估每个学生在全题库上的平均成绩。本文使用的Maximal-margin algorithm可用于传统的Bradley-Terry-Luce模型。经过实证研究和模拟验证，在小班级和考试的精度和公平性方面，我们的评分算法明显优于简单平均方法。

    This paper studies grading algorithms for randomized exams. In a randomized exam, each student is asked a small number of random questions from a large question bank. The predominant grading rule is simple averaging, i.e., calculating grades by averaging scores on the questions each student is asked, which is fair ex-ante, over the randomized questions, but not fair ex-post, on the realized questions. The fair grading problem is to estimate the average grade of each student on the full question bank. The maximum-likelihood estimator for the Bradley-Terry-Luce model on the bipartite student-question graph is shown to be consistent with high probability when the number of questions asked to each student is at least the cubed-logarithm of the number of students. In an empirical study on exam data and in simulations, our algorithm based on the maximum-likelihood estimator significantly outperforms simple averaging in prediction accuracy and ex-post fairness even with a small class and exam
    
[^13]: 实用指南：关于知情重要性调节方法的详细介绍

    Importance is Important: A Guide to Informed Importance Tempering Methods. (arXiv:2304.06251v1 [stat.CO])

    [http://arxiv.org/abs/2304.06251](http://arxiv.org/abs/2304.06251)

    本论文详细介绍了一种易于实施的MCMC算法IIT及其在许多情况下的应用。该算法始终接受有信息的提议，可与其他MCMC技术相结合，并带来新的优化抽样器的机会。

    

    知情重要性调节 (IIT) 是一种易于实施的MCMC算法，可视为通常的Metropolis-Hastings算法的扩展，具有始终接受有信息的提议的特殊功能，在Zhou和Smith（2022年）的研究中表明在一些常见情况下收敛更快。本文开发了一个新的、全面的指南，介绍了IIT在许多情况下的应用。首先，我们提出了两种IIT方案，这些方案在离散空间上的运行速度比现有的知情MCMC方法更快，因为它们不需要计算所有相邻状态的后验概率。其次，我们将IIT与其他MCMC技术（包括模拟回火、伪边缘和多重尝试方法，在一般状态空间上实施为Metropolis-Hastings方案，可能遭受低接受率的问题）进行了整合。使用IIT使我们能够始终接受提议，并带来了优化抽样器的新机会，这是在Metropolis-Hastings算法下不可能的。最后，我们提供了一个实用的指南，以选择IIT方案和调整算法参数。对各种模型的实验结果证明了我们所提出的方法的有效性。

    Informed importance tempering (IIT) is an easy-to-implement MCMC algorithm that can be seen as an extension of the familiar Metropolis-Hastings algorithm with the special feature that informed proposals are always accepted, and which was shown in Zhou and Smith (2022) to converge much more quickly in some common circumstances. This work develops a new, comprehensive guide to the use of IIT in many situations. First, we propose two IIT schemes that run faster than existing informed MCMC methods on discrete spaces by not requiring the posterior evaluation of all neighboring states. Second, we integrate IIT with other MCMC techniques, including simulated tempering, pseudo-marginal and multiple-try methods (on general state spaces), which have been conventionally implemented as Metropolis-Hastings schemes and can suffer from low acceptance rates. The use of IIT allows us to always accept proposals and brings about new opportunities for optimizing the sampler which are not possible under th
    
[^14]: 为一致性预测的后选推理：权衡精度和覆盖范围

    Post-selection Inference for Conformal Prediction: Trading off Coverage for Precision. (arXiv:2304.06158v1 [stat.ME])

    [http://arxiv.org/abs/2304.06158](http://arxiv.org/abs/2304.06158)

    本论文提出一种使用无分布信赖带的 uniform conformal inference 算法，实现任意数据相关误覆盖水平的有限样本预测保证的统一一致性推理。

    

    一致性推理在为具有有限样本保证的黑盒机器学习预测算法提供不确定性量化上发挥了重要作用。传统上，一致性预测推理需要独立于数据的错误覆盖水平规范。在实际应用中，人们可能会在计算出预测集之后更新错误覆盖水平。例如，在二元分类的情况下，分析人员可能会从一个95％的预测集开始，并发现大多数预测集包含所有输出类别。如果两个类别都不可取，分析人员可能会考虑80％的预测集。具有数据相关的误覆盖水平和保证覆盖范围的预测集的构建可以被认为是一个后选推理问题。在这项工作中，我们使用无分布信赖带，开发了具有任意数据相关误覆盖水平的有限样本预测保证的统一一致性推理。

    Conformal inference has played a pivotal role in providing uncertainty quantification for black-box ML prediction algorithms with finite sample guarantees. Traditionally, conformal prediction inference requires a data-independent specification of miscoverage level. In practical applications, one might want to update the miscoverage level after computing the prediction set. For example, in the context of binary classification, the analyst might start with a $95\%$ prediction sets and see that most prediction sets contain all outcome classes. Prediction sets with both classes being undesirable, the analyst might desire to consider, say $80\%$ prediction set. Construction of prediction sets that guarantee coverage with data-dependent miscoverage level can be considered as a post-selection inference problem. In this work, we develop uniform conformal inference with finite sample prediction guarantee with arbitrary data-dependent miscoverage levels using distribution-free confidence bands f
    
[^15]: 能量引导的熵神经最优输运

    Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])

    [http://arxiv.org/abs/2304.06094](http://arxiv.org/abs/2304.06094)

    本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。

    

    能量基础模型（EBMs）在机器学习社区已经有数十年的历史。自两千年代起，一直有很多高效的方法通过能量势（非归一化的似然函数）来解决生成建模问题。相比之下，最优输运（OT）领域，尤其是神经OT求解器，受到的探索要少得多，仅有一些近期的研究（不包括利用OT作为损失函数来解决问题的WGAN方法）。在本研究中，我们弥合了EBMs和熵正则化OT之间的差距，提出了一种新的方法，允许利用前者的最新发展和技术改进来丰富后者。我们在2D情景和标准的图像到图像翻译问题中验证了我们方法的适用性。为简单起见，我们选择了简短和长跑的EBMs。

    Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
    
[^16]: 反射扩散模型

    Reflected Diffusion Models. (arXiv:2304.04740v1 [stat.ML])

    [http://arxiv.org/abs/2304.04740](http://arxiv.org/abs/2304.04740)

    该论文提出了反射扩散模型，通过学习反射随机微分方程的扰动评分函数，将数据约束原则性地整合到生成过程中，以取代之前采用的导致不自然样本的阈值处理方案。

    

    基于分数的扩散模型学习将数据映射到噪声的随机微分方程的反向。然而，对于复杂任务，数值误差可以累积并导致高度不自然的样本。以前的工作通过阈值处理来缓解漂移，每次扩散步骤后投影到自然数据域（例如图像的像素空间），但这导致训练和生成过程之间存在不匹配。为了以一种原则性的方式合并数据约束，我们提出了反射扩散模型，该模型反向演化在数据支持的反射随机微分方程上。我们的方法通过一般化的分数匹配损失函数学习扰动评分函数，并扩展了标准扩散模型的关键组件，包括扩散引导、基于似然的训练和ODE采样。我们还弥合了阈值处理的理论差距:这样的方案只是反射SDE的离散化。在标准图像基准测试中，我们的

    Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalize score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and ODE sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected SDEs. On standard image benchmarks, our 
    
[^17]: PriorCVAE: 基于贝叶斯深度生成建模的可扩展 MCMC 参数推断

    PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v1 [stat.ML])

    [http://arxiv.org/abs/2304.04307](http://arxiv.org/abs/2304.04307)

    PriorCVAE 提出了一种处理高斯过程先验 MCMC 参数推断的贝叶斯深度生成建模新方法，可通过将 VAE 建模条件化于随机过程超参数处理超参数推断与学习先验之间的信息流断裂问题。

    

    在应用场景中，推理速度和模型灵活性至关重要，贝叶斯推断在具有随机过程先验的模型中（如高斯过程）被广泛应用。最近的研究表明，使用变分自动编码器（VAE）等深度生成模型可以编码由 GP 先验或其有限实现引起的计算瓶颈，并且所学生成器可以代替 MCMC 推断中的原始先验。虽然此方法实现了快速而高效的推理，但它丢失了关于随机过程超参数的信息，导致超参数推断不可能和学到的先验模糊不清。我们建议解决上述问题，通过将 VAE 建模条件化于随机过程超参数，以便超参数与 GP 实现一起进行编码。

    In applied fields where the speed of inference and model flexibility are crucial, the use of Bayesian inference for models with a stochastic process as their prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature has demonstrated that the computational bottleneck caused by GP priors or their finite realizations can be encoded using deep generative models such as variational autoencoders (VAEs), and the learned generators can then be used instead of the original priors during Markov chain Monte Carlo (MCMC) inference in a drop-in manner. While this approach enables fast and highly efficient inference, it loses information about the stochastic process hyperparameters, and, as a consequence, makes inference over hyperparameters impossible and the learned priors indistinct. We propose to resolve the aforementioned issue and disentangle the learned priors by conditioning the VAE on stochastic process hyperparameters. This way, the hyperparameters are encoded alongside GP real
    
[^18]: 追求机器学习研究的推理复现性

    Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04054](http://arxiv.org/abs/2302.04054)

    本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。

    

    机器学习评估的可靠性——即在复制的模型训练运行中观察到的评估分数的一致性——受到几种非确定性来源的影响，可以被视为测量噪声。目前的趋势是去除噪声，以强制研究结果的可复制性，忽略了实现层面固有的非确定性以及算法噪声因素和数据特性之间的关键相互作用效应。这限制了从这些实验中可以得出的结论范围。我们提出的方法是将几个方差来源，包括它们与数据特性的相互作用，纳入机器学习评估的显著性和可靠性分析中，以期从训练模型的特定实例得出推理结论, 而非去除噪声。我们展示如何使用线性混合效应模型（LMEM）来分析性能评估分数，并用广义似然比检验进行统计推断。我们的方法提供了一种系统的方式来考虑算法和数据相关的噪声来源，并使我们能够量化各个方差来源对机器学习实验的可靠性和可复制性的影响。我们在一系列合成和真实数据集上演示了我们方法的实用性，并说明了我们的方法如何促进对机器学习算法行为的更全面理解。

    Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
    
[^19]: 基于KL散度的离散时间模型深度学习

    KL-divergence Based Deep Learning for Discrete Time Model. (arXiv:2208.05100v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.05100](http://arxiv.org/abs/2208.05100)

    该论文提出了一种基于KL散度的深度学习程序，用于解决生存分析中短数据问题，通过将外部生存预测模型与新收集的时间至事件数据相结合来获得更好的性能和鲁棒性。

    

    神经网络（深度学习）是人工智能中的现代模型，并已被利用于生存分析中。尽管之前的研究已经展示了一些改进，但训练出一个优秀的深度学习模型需要大量的数据，在实践中可能并不存在。为了解决这个挑战，我们开发了一种基于Kullback-Leibler（KL）的深度学习程序，将外部生存预测模型与新收集的时间至事件数据相结合。利用时间相关的KL区分信息来衡量外部数据和内部数据之间的差异。这是第一个考虑使用先前信息来处理深度学习中的短数据问题的工作。模拟和实际数据结果表明，与之前的工作相比，所提出的模型具有更好的性能和更高的鲁棒性。

    Neural Network (Deep Learning) is a modern model in Artificial Intelligence and it has been exploited in Survival Analysis. Although several improvements have been shown by previous works, training an excellent deep learning model requires a huge amount of data, which may not hold in practice. To address this challenge, we develop a Kullback-Leibler-based (KL) deep learning procedure to integrate external survival prediction models with newly collected time-to-event data. Time-dependent KL discrimination information is utilized to measure the discrepancy between the external and internal data. This is the first work considering using prior information to deal with short data problem in Survival Analysis for deep learning. Simulation and real data results show that the proposed model achieves better performance and higher robustness compared with previous works.
    
[^20]: 带有逆热传导的生成建模

    Generative Modelling With Inverse Heat Dissipation. (arXiv:2206.13397v7 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.13397](http://arxiv.org/abs/2206.13397)

    该论文提出了一种新的类似扩散的模型来生成图像，它通过随机反转热方程在2D平面上运行来生成图像，并展示了与标准扩散模型不同的新颖定性性质，包括图像中整体颜色和形状的解缠绕现象。

    

    虽然扩散模型在图像生成方面取得了巨大成功，但它们的噪声反演生成过程并没有明确考虑图像的结构，例如其固有的多尺度性质。受到扩散模型和粗到细建模的实证成功的启发，我们提出了一种新的类似扩散的模型，通过随机地反转热方程在2D平面上运行来生成图像，当其运行时地局部抹去了图像的细尺度信息。我们将具有恒定加性噪声的正向热方程的解释为扩散潜在变量模型中的变分近似。我们的新模型显示出并不在标准扩散模型中看到的新颖的定性性质，例如图像中整体颜色和形状的解缠绕现象。自然图像的谱分析突出了与扩散模型的联系并揭示了其中一个隐含的粗到细归纳偏差。

    While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.
    
[^21]: 无需不可能数据的变量重要性分析方法

    Variable importance without impossible data. (arXiv:2205.15750v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15750](http://arxiv.org/abs/2205.15750)

    评估黑箱预测模型中变量重要性的流行方法不可信，因为使用了不可能的数据。本文提出一种名为Cohort Shapley的方法，它基于经济博弈理论，仅使用实际观测到的数据来量化变量重要性，可以解决算法公平性问题。

    

    目前，评估黑箱预测模型中变量重要性的最流行方法是使用人工合成的输入数据，这些数据结合了多个参与者的预测变量，这些输入数据可能是不可能的、物理上不可能的，甚至是逻辑上不可能的，由此得出的预测结果可能与黑箱训练数据有很大不同。因此，当解释决策时使用这些值时，用户不能信任预测算法的解释。我们提倡一种名为Cohort Shapley的方法，它基于经济博弈理论，与大多数其他博弈论方法不同，仅使用实际观测到的数据来量化变量重要性。Cohort Shapley通过缩小与目标对象在一个或多个特征上相似的对象组来实现。我们将其应用于一个算法公平性问题，其中必须将重要性归因于模型未经训练的受保护变量。

    The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.
    
[^22]: 一种整合和分类正态分布的方法

    A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14331](http://arxiv.org/abs/2012.14331)

    本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。

    

    单变量和多变量正态概率分布在模拟不确定性决策中被广泛使用。计算这些模型的性能需要在特定区域内对这些分布进行积分，这在不同的模型中可以有很大的差异。除了一些特殊情况，目前不存在通用的分析表达式、标准数值方法或软件来计算这些积分。本文提供了数学结果和开源软件，可以提供以下内容：（i）任意参数维度下任意域内法向的概率，（ii）法向向量函数的概率密度、累积分布和逆累积分布，（iii）任意数量正态分布之间的分类误差、贝叶斯最优辨别指数以及其与工作特征曲线的关系，（iv）此类问题的维度降低和可视化，以及（v）对于给定数据这些方法的可靠性测试。我们通过几个具体的例子，包括金融、生物和心理学来演示这些功能。

    Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
    
[^23]: 互信息在变分分类器中的作用

    The Role of Mutual Information in Variational Classifiers. (arXiv:2010.11642v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.11642](http://arxiv.org/abs/2010.11642)

    本文研究了基于随机编码的分类器的泛化误差，并得出了泛化误差受输入特征和潜空间表示之间互信息限制的结论。

    

    过拟合是一种众所周知的现象，与生成过度拟合特定数据实例的模型有关，因此可能无法可靠地预测未来观察结果。在实践中，通过各种，有时是启发式的正则化技术控制此行为，这些技术的动机是以开发上限来泛化误差。在这项工作中，我们研究了基于在交叉熵损失上训练的随机编码的分类器的泛化误差，这在深度学习中经常用于分类问题。我们推导了泛化误差的边界，表明存在一种区域，其中泛化误差由输入特征和与编码分布随机生成的潜空间中的相应表示之间的互信息所限制。我们的边界提供了对所谓的变异级别的泛化的信息论理解。

    Overfitting data is a well-known phenomenon related with the generation of a model that mimics too closely (or exactly) a particular instance of data, and may therefore fail to predict future observations reliably. In practice, this behaviour is controlled by various--sometimes heuristics--regularization techniques, which are motivated by developing upper bounds to the generalization error. In this work, we study the generalization error of classifiers relying on stochastic encodings trained on the cross-entropy loss, which is often used in deep learning for classification problems. We derive bounds to the generalization error showing that there exists a regime where the generalization error is bounded by the mutual information between input features and the corresponding representations in the latent space, which are randomly generated according to the encoding distribution. Our bounds provide an information-theoretic understanding of generalization in the so-called class of variation
    
[^24]: 基于超图线扩展的半监督超图节点分类

    Semi-supervised Hypergraph Node Classification on Hypergraph Line Expansion. (arXiv:2005.04843v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.04843](http://arxiv.org/abs/2005.04843)

    本文提出了一种新的超图学习方法，即“线扩展(LE)”，该方法通过将顶点-超边对作为“线节点”，在超图中引出同构结构，适用于各种超图扩展并达到了显著优于SOTA的效果。

    

    先前的超图扩展仅在顶点级别或超边级别上进行，因此缺乏数据共现的对称性，导致信息损失。为了解决这个问题，本文平等对待顶点和超边，提出了一种名为\emph{线扩展(LE)}的新型超图学习方法。该方法通过将顶点-超边对作为“线节点”，在超图中双射引出一种同构结构。通过将超图减少为简单图，提出的\emph{线扩展}使得已有的图学习算法适用于高阶结构，并已被证明是各种超图扩展的统一框架。我们在五个超图数据集上评估了所提出的线扩展，结果表明我们的方法显著优于SOTA基线。

    Previous hypergraph expansions are solely carried out on either vertex level or hyperedge level, thereby missing the symmetric nature of data co-occurrence, and resulting in information loss. To address the problem, this paper treats vertices and hyperedges equally and proposes a new hypergraph formulation named the \emph{line expansion (LE)} for hypergraphs learning. The new expansion bijectively induces a homogeneous structure from the hypergraph by treating vertex-hyperedge pairs as "line nodes". By reducing the hypergraph to a simple graph, the proposed \emph{line expansion} makes existing graph learning algorithms compatible with the higher-order structure and has been proven as a unifying framework for various hypergraph expansions. We evaluate the proposed line expansion on five hypergraph datasets, the results show that our method beats SOTA baselines by a significant margin.
    

