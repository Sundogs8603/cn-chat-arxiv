<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantitative knowledge retrieval from large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#29983;&#25104;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#30340;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#20316;&#20026;&#23450;&#37327;&#20449;&#24687;&#26816;&#32034;&#30340;&#23454;&#29992;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;LLMs&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#24110;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#22914;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#24341;&#23548;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#22635;&#34917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#35270;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#65292;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#39046;&#22495;&#20013;&#27604;&#36739;&#21709;&#24212;&#19982;&#26356;&#25104;&#29087;&#30340;&#26041;&#27861;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood. In this paper we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data. We present a prompt engineering framework, treating an LLM as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches. Implications and challenges of using LLMs as 'experts' are discussed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#24212;&#29992;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#65288;LSR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#22270;&#20687;&#20869;&#23481;&#19982;&#20854;&#26631;&#39064;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;LSR&#26816;&#32034;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.07736</link><description>&lt;p&gt;
&#22270;&#20687;&#24314;&#35758;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learned Sparse Retrieval for Image Suggestion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#24212;&#29992;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#65288;LSR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#22270;&#20687;&#20869;&#23481;&#19982;&#20854;&#26631;&#39064;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;LSR&#26816;&#32034;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#65288;LSR&#65289;&#26159;&#19968;&#32452;&#35774;&#35745;&#29992;&#20110;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#32534;&#30721;&#25104;&#31232;&#30095;&#35789;&#27719;&#21521;&#37327;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#20498;&#25490;&#32034;&#24341;&#36827;&#34892;&#39640;&#25928;&#22320;&#32034;&#24341;&#21644;&#26816;&#32034;&#12290;&#34429;&#28982;LSR&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;LSR&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#21363;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#65288;MLSR&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;MLSR&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#35780;&#20272;&#22270;&#20687;&#24314;&#35758;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20165;&#22522;&#20110;&#22270;&#20687;&#20869;&#23481;&#35299;&#20915;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20016;&#23500;&#22270;&#20687;&#20869;&#23481;&#19982;&#20854;&#26631;&#39064;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#22270;&#20687;&#26631;&#39064;&#25552;&#20379;&#20102;&#22270;&#20687;&#30340;&#31934;&#32454;&#27010;&#24565;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#35757;&#32451;LSR&#26816;&#32034;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned Sparse Retrieval (LSR) is a group of neural methods designed to encode queries and documents into sparse lexical vectors. These vectors can be efficiently indexed and retrieved using an inverted index. While LSR has shown promise in text retrieval, its potential in multi-modal retrieval remains largely unexplored. Motivated by this, in this work, we explore the application of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse Retrieval (MLSR). We conduct experiments using several MLSR model configurations and evaluate the performance on the image suggestion task. We find that solving the task solely based on the image content is challenging. Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images. Our approach presents a practical and effective solution for training LSR retrieval models in multi-modal settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#37096;&#20998;&#39034;&#24207;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#35299;&#20915;&#21333;&#19968;&#22270;&#24418;&#20013;&#22810;&#20010;&#34892;&#20026;&#30340;&#21327;&#21516;&#36807;&#28388;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#20041;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#30340;&#37096;&#20998;&#39034;&#24207;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#36793;&#21512;&#24182;&#34892;&#20026;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20027;&#35201;&#20219;&#21153;&#21644;&#36741;&#21161;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#33391;&#22909;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.07659</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#39034;&#24207;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#22810;&#34892;&#20026;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#37096;&#20998;&#39034;&#24207;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#35299;&#20915;&#21333;&#19968;&#22270;&#24418;&#20013;&#22810;&#20010;&#34892;&#20026;&#30340;&#21327;&#21516;&#36807;&#28388;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#20041;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#30340;&#37096;&#20998;&#39034;&#24207;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#36793;&#21512;&#24182;&#34892;&#20026;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20027;&#35201;&#20219;&#21153;&#21644;&#36741;&#21161;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#33391;&#22909;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#19968;&#22270;&#24418;&#21327;&#20316;&#36807;&#28388;&#65288;CF&#65289;&#21521;&#37327;&#20013;&#34920;&#31034;&#22810;&#20010;&#34892;&#20026;&#30340;&#20449;&#24687;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;&#19981;&#21516;&#30340;&#34892;&#20026;&#33258;&#28982;&#24418;&#25104;&#21333;&#29420;&#30340;&#34892;&#20026;&#22270;&#65292;&#24182;&#23398;&#20064;&#21333;&#29420;&#30340;CF&#23884;&#20837;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#36807;&#25351;&#23450;&#26576;&#20123;&#34892;&#20026;&#30340;CF&#23884;&#20837;&#20316;&#20026;&#20027;&#35201;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#20854;&#20182;&#36741;&#21161;&#24037;&#20855;&#26469;&#22686;&#24378;&#20027;&#35201;&#23884;&#20837;&#26469;&#21512;&#24182;&#36825;&#20123;&#21333;&#29420;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#22312;&#20027;&#35201;&#20219;&#21153;&#19978;&#32852;&#21512;&#23884;&#20837;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#21333;&#29420;&#30340;&#34892;&#20026;&#22270;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37096;&#20998;&#39034;&#24207;&#22270;&#65288;POG&#65289;&#30340;&#27010;&#24565;&#12290;POG&#23450;&#20041;&#20102;&#22810;&#20010;&#34892;&#20026;&#30340;&#37096;&#20998;&#39034;&#24207;&#20851;&#31995;&#65292;&#24182;&#23558;&#34892;&#20026;&#32452;&#21512;&#24314;&#27169;&#20026;&#24102;&#26377;&#26435;&#37325;&#30340;&#36793;&#65292;&#20197;&#23558;&#21333;&#29420;&#30340;&#34892;&#20026;&#22270;&#21512;&#24182;&#25104;&#19968;&#20010;&#32852;&#21512;&#30340;POG&#12290;&#29702;&#35770;&#35777;&#26126;&#20102;POG&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#32473;&#23450;&#30340;&#22810;&#20010;&#34892;&#20026;&#38598;&#12290;&#22522;&#20110;POG&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#21046;&#30340;&#37096;&#20998;&#39034;&#24207;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing the information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior graphs, we propose the concept of Partial Order Graphs (POG). POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored Partial Order 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GRILLBot&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#31995;&#32479;&#26159;&#29992;&#20110;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#24182;&#22788;&#29702;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#27169;&#22411;&#26469;&#20445;&#35777;&#20219;&#21153;&#23548;&#21521;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#23454;&#26102;&#20219;&#21153;&#35843;&#25972;&#30340;&#24615;&#33021;&#21644;&#20302;&#24310;&#36831;&#65292;&#20197;&#21450;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#30340;&#23545;&#35805;&#29366;&#24577;&#31649;&#29702;&#12290;&#35813;&#35770;&#25991;&#23545;&#20110;&#26500;&#24314;&#36866;&#24212;&#24615;&#20250;&#35805;&#20219;&#21153;&#21161;&#25163;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.07647</link><description>&lt;p&gt;
GRILLBot&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65306;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#24314;&#31435;&#36866;&#24212;&#24615;&#20250;&#35805;&#20219;&#21153;&#21161;&#25163;&#30340;&#32463;&#39564;&#19982;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GRILLBot&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#31995;&#32479;&#26159;&#29992;&#20110;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#24182;&#22788;&#29702;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#27169;&#22411;&#26469;&#20445;&#35777;&#20219;&#21153;&#23548;&#21521;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#23454;&#26102;&#20219;&#21153;&#35843;&#25972;&#30340;&#24615;&#33021;&#21644;&#20302;&#24310;&#36831;&#65292;&#20197;&#21450;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#30340;&#23545;&#35805;&#29366;&#24577;&#31649;&#29702;&#12290;&#35813;&#35770;&#25991;&#23545;&#20110;&#26500;&#24314;&#36866;&#24212;&#24615;&#20250;&#35805;&#20219;&#21153;&#21161;&#25163;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#26500;&#24314;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#23454;&#38469;&#22810;&#27169;&#24577;&#21161;&#25163;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;GRILLBot&#30340;&#23454;&#36341;&#24615;&#21644;&#25361;&#25112;&#24615;&#65292;&#35813;&#31995;&#32479;&#26159;Alexa Prize TaskBot&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21644;&#31532;&#20108;&#21517;&#30340;&#31995;&#32479;&#65288;&#20998;&#21035;&#22312;2022&#24180;&#21644;2023&#24180;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#24320;&#25918;&#21161;&#25163;&#24037;&#20855;&#21253;&#65288;OAT&#65289;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20026;&#38656;&#35201;&#38750;&#24120;&#20302;&#24310;&#36831;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#35843;&#20248;&#30340;&#19987;&#38376;&#27169;&#22411;&#12290;OAT&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#32467;&#26500;&#21270;&#19988;&#21487;&#37096;&#32626;&#30340;&#26041;&#24335;&#23450;&#20041;&#20309;&#26102;&#12289;&#22914;&#20309;&#20197;&#21450;&#20351;&#29992;&#21738;&#20123;LLMs&#12290;&#23545;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#23454;&#26102;&#20219;&#21153;&#35843;&#25972;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#20219;&#21153;&#32972;&#26223;&#21644;&#19990;&#30028;&#30693;&#35782;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#36229;&#36807;&#24310;&#36831;&#38382;&#39064;&#12290;&#23545;&#20110;&#23545;&#35805;&#29366;&#24577;&#31649;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#20855;&#26377;84&#65285;&#30340;&#26377;&#25928;&#24615;&#21644;100&#20493;&#30340;&#20302;&#24310;&#36831;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27934;&#35265;&#65292;&#24182;&#35752;&#35770;&#20102;&#26435;&#34913;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#39640;&#25928;&#22320;&#32034;&#24341;&#21644;&#20998;&#31867;&#35270;&#39057;&#20869;&#23481;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#25552;&#20379;&#30456;&#20851;&#30340;&#20449;&#24687;&#21644;&#25512;&#33616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#21560;&#24341;&#21147;&#30340;&#25506;&#32034;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07466</link><description>&lt;p&gt;
VCR: &#25991;&#26412;&#26816;&#32034;&#19979;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VCR: Video representation for Contextual Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#39640;&#25928;&#22320;&#32034;&#24341;&#21644;&#20998;&#31867;&#35270;&#39057;&#20869;&#23481;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#25552;&#20379;&#30456;&#20851;&#30340;&#20449;&#24687;&#21644;&#25512;&#33616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#21560;&#24341;&#21147;&#30340;&#25506;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23186;&#20307;&#26723;&#26696;&#20013;&#31616;&#21270;&#20869;&#23481;&#21457;&#29616;&#38656;&#35201;&#25972;&#21512;&#20808;&#36827;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#26377;&#25928;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20197;&#28165;&#26224;&#22320;&#21521;&#29992;&#25143;&#20256;&#36798;&#35270;&#39057;&#20027;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#23548;&#33322;&#22823;&#22411;&#35270;&#39057;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#20934;&#30830;&#22320;&#32034;&#24341;&#21644;&#20998;&#31867;&#35270;&#39057;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#36824;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#30456;&#20851;&#30340;&#20449;&#24687;&#21644;&#25512;&#33616;&#65292;&#20174;&#32780;&#22312;&#20351;&#29992;OpenAI GPT-4&#30340;&#20027;&#39064;&#26412;&#20307;&#22270;&#19978;&#33719;&#24471;&#30452;&#35266;&#32780;&#26377;&#21560;&#24341;&#21147;&#30340;&#25506;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streamlining content discovery within media archives requires integrating advanced data representations and effective visualization techniques for clear communication of video topics to users. The proposed system addresses the challenge of efficiently navigating large video collections by exploiting a fusion of visual, audio, and textual features to accurately index and categorize video content through a text-based method. Additionally, semantic embeddings are employed to provide contextually relevant information and recommendations to users, resulting in an intuitive and engaging exploratory experience over our topics ontology map using OpenAI GPT-4.
&lt;/p&gt;</description></item><item><title>AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07448</link><description>&lt;p&gt;
AraSpider&#65306;&#23454;&#29616;&#38463;&#25289;&#20271;&#35821;&#21040;SQL&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
AraSpider: Democratizing Arabic-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07448
&lt;/p&gt;
&lt;p&gt;
AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;AraSpider&#65292;&#36825;&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#35813;&#30740;&#31350;&#27979;&#35797;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#22312;&#23558;&#33521;&#25991;&#32763;&#35793;&#25104;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#22806;&#65292;&#36824;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#22312;&#20174;&#38463;&#25289;&#20271;&#25991;&#26412;&#29983;&#25104;SQL&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22238;&#35793;&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;Spider&#25968;&#25454;&#38598;&#19978;&#34987;&#35748;&#20026;&#26159;&#26368;&#20339;&#34920;&#29616;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT 3.5&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#65292;&#32780;SQLCoder&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#19978;&#19979;&#25991;&#27169;&#24335;&#21644;&#37319;&#29992;&#22238;&#35793;&#31574;&#30053;&#32435;&#20837;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#26041;&#27861;&#35770;&#20197;&#23454;&#29616;&#32467;&#26524;&#22797;&#29616;&#24182;&#23558;&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#31361;&#26174;&#20102;&#30740;&#31350;&#20419;&#36827;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07440</link><description>&lt;p&gt;
&#20351;&#29992;LoCo&#21644;M2-BERT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#26500;&#24314;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#31649;&#36947;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25991;&#26723;&#24456;&#38271;&#65288;&#20363;&#22914;10K&#20010;&#26631;&#35760;&#25110;&#26356;&#22810;&#65289;&#19988;&#38656;&#35201;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#21512;&#25104;&#20449;&#24687;&#26469;&#30830;&#23450;&#30456;&#20851;&#25991;&#26723;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#36866;&#29992;&#20110;&#36825;&#20123;&#39046;&#22495;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#32534;&#30721;&#22120;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#65292;&#65288;2&#65289;&#22914;&#20309;&#39044;&#35757;&#32451;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20197;&#34920;&#31034;&#30701;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#26597;&#35810;&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#25991;&#26723;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#26681;&#25454;GPU&#20869;&#23384;&#38480;&#21046;&#19979;&#30340;&#25209;&#37327;&#22823;&#23567;&#38480;&#21046;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LoCoV1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;12&#20010;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27979;&#37327;&#22312;&#19981;&#21487;&#20998;&#22359;&#25110;&#19981;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;80M&#21442;&#25968;&#29366;&#24577;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;Monarch Mixer&#26550;&#26500;&#26500;&#24314;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20010;&#20154;&#27969;&#34892;&#24230;&#26469;&#28040;&#38500;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#27809;&#26377;&#27880;&#24847;&#21040;&#20840;&#23616;&#27969;&#34892;&#24230;&#30340;&#26681;&#26412;&#38382;&#39064;&#65292;&#32780;&#20010;&#20154;&#27969;&#34892;&#24230;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#20010;&#20307;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#20010;&#20154;&#27969;&#34892;&#24230;&#24863;&#30693;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#65292;&#23558;&#20010;&#20154;&#27969;&#34892;&#24230;&#34701;&#20837;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.07425</link><description>&lt;p&gt;
&#29992;&#20010;&#20154;&#27969;&#34892;&#24230;&#28040;&#38500;&#25512;&#33616;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Debiasing Recommendation with Personal Popularity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20010;&#20154;&#27969;&#34892;&#24230;&#26469;&#28040;&#38500;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#27809;&#26377;&#27880;&#24847;&#21040;&#20840;&#23616;&#27969;&#34892;&#24230;&#30340;&#26681;&#26412;&#38382;&#39064;&#65292;&#32780;&#20010;&#20154;&#27969;&#34892;&#24230;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#20010;&#20307;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#20010;&#20154;&#27969;&#34892;&#24230;&#24863;&#30693;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#65292;&#23558;&#20010;&#20154;&#27969;&#34892;&#24230;&#34701;&#20837;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#27969;&#34892;&#24230;&#65288;GP&#65289;&#20559;&#35265;&#26159;&#25351;&#25512;&#33616;&#31995;&#32479;&#22312;&#25512;&#33616;&#29289;&#21697;&#26102;&#26222;&#36941;&#20559;&#21521;&#28909;&#38376;&#29289;&#21697;&#65292;&#36825;&#36829;&#32972;&#20102;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30446;&#26631;&#65292;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#36896;&#25104;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#20943;&#23569;GP&#20559;&#35265;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#27880;&#24847;&#21040;GP&#30340;&#26681;&#26412;&#38382;&#39064;&#65292;&#21363;&#23427;&#20174;&#20840;&#23616;&#30340;&#35282;&#24230;&#32771;&#34385;&#28909;&#38376;&#24230;&#65292;&#20351;&#29992;&#19968;&#32452;&#28909;&#38376;&#29289;&#21697;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#21040;&#20010;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#22522;&#30784;&#30340;&#29289;&#21697;&#27969;&#34892;&#24230;&#31216;&#20026;&#20010;&#20154;&#27969;&#34892;&#24230;&#65288;PP&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20998;&#20139;&#30456;&#20284;&#20852;&#36259;&#30340;&#29992;&#25143;&#26469;&#20026;&#27599;&#20010;&#29992;&#25143;&#35782;&#21035;&#19981;&#21516;&#30340;&#28909;&#38376;&#29289;&#21697;&#12290;&#30001;&#20110;PP&#27169;&#22411;&#21270;&#20102;&#20010;&#20307;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#23427;&#33258;&#28982;&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#24182;&#20943;&#23569;GP&#20559;&#35265;&#12290;&#20026;&#20102;&#23558;PP&#34701;&#20837;&#25512;&#33616;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#20154;&#27969;&#34892;&#24230;&#24863;&#30693;&#21453;&#20107;&#23454;&#65288;PPAC&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a \textit{global} perspective of \textit{all users} and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named \textit{personal popularity} (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias. To integrate PP into recommendation, we design a general \textit{personal popularity aware counterfactual} (PPAC) frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.07179</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#25552;&#31034;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#40065;&#26834;&#24615;&#22312;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#36805;&#36895;&#22686;&#38271;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#35270;&#20026;&#25552;&#39640;&#20174;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;RAG-based LLMs&#30340;&#36755;&#20986;&#22914;&#20309;&#21463;&#21040;&#31245;&#26377;&#19981;&#21516;&#30340;&#36755;&#20837;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#25552;&#31034;&#20013;&#25554;&#20837;&#19968;&#20010;&#24456;&#30701;&#30340;&#21069;&#32512;&#20063;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20107;&#23454;&#27491;&#30830;&#31572;&#26696;&#30456;&#21435;&#29978;&#36828;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#36825;&#31867;&#21069;&#32512;&#23545;RAG&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Gradient Guided Prompt Perturbation&#65288;GGPP&#65289;&#30340;&#26032;&#22411;&#20248;&#21270;&#25216;&#26415;&#12290;GGPP&#22312;&#23558;RAG-based LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#23545;&#25552;&#31034;&#20013;&#35831;&#27714;&#24573;&#30053;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;LLMs&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;GGPP&#25200;&#21160;&#30340;&#25552;&#31034;&#20043;&#38388;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#24046;&#24322;&#26469;&#25552;&#20379;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.07092</link><description>&lt;p&gt;
&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#21033;&#29992;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#26469;&#26816;&#32034;&#30456;&#20851;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#23558;&#23545;&#35805;&#35270;&#20026;&#19968;&#31995;&#21015;&#22266;&#23450;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24573;&#35270;&#20102;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064; - &#20063;&#23601;&#26159;&#35828;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#65292;&#32780;&#36825;&#20123;&#22791;&#36873;&#23545;&#35805;&#26159;&#26410;&#35760;&#24405;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#30340;&#26694;&#26550;(ConvAug)&#12290;ConvAug&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#20197;&#25429;&#25417;&#23545;&#35805;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#30340;&#27491;&#20363;&#12289;&#36127;&#20363;&#21644;&#24187;&#35273;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#22797;&#26434;&#23545;&#35805;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMA&#26694;&#26550;&#26469;&#25913;&#21892;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#30340;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#29305;&#24449;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07076</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#25913;&#21892;&#22810;&#39046;&#22495;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07076
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMA&#26694;&#26550;&#26469;&#25913;&#21892;B2B&#20113;&#35299;&#20915;&#26041;&#26696;&#30340;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#29305;&#24449;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35299;&#20915;&#26041;&#26696;&#22312;&#25216;&#26415;&#34892;&#19994;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#21644;&#24037;&#20855;&#26469;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#21830;&#30340;&#38144;&#21806;&#22242;&#38431;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#22797;&#26434;&#30340;&#19994;&#21153;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#36866;&#21512;&#29305;&#23450;&#30446;&#26631;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#30340;&#20844;&#21496;&#23458;&#25143;&#65292;&#29616;&#26377;&#30340;&#21305;&#37197;&#31995;&#32479;&#23578;&#26410;&#33021;&#22815;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;B2B&#35299;&#20915;&#26041;&#26696;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#22330;&#26223;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;(1) &#22797;&#26434;&#22810;&#39046;&#22495;&#29305;&#24449;&#30340;&#24314;&#27169;&#21644;(2) &#26377;&#38480;&#12289;&#19981;&#23436;&#25972;&#21644;&#31232;&#30095;&#30340;&#20132;&#26131;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;CAMA&#65292;&#23427;&#20197;&#20998;&#23618;&#22810;&#39046;&#22495;&#21305;&#37197;&#32467;&#26500;&#20026;&#20027;&#24178;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#24357;&#34917;&#21487;&#29992;&#25968;&#25454;&#30340;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06871</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Generative Models for Reranking Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#36890;&#36807;&#24314;&#27169;&#39033;&#30446;&#20043;&#38388;&#30340;&#20869;&#37096;&#30456;&#20851;&#24615;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37325;&#26032;&#25490;&#24207;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#25490;&#21015;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#25506;&#32034;&#26368;&#20339;&#24207;&#21015;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#29983;&#25104;&#22120;-&#35780;&#20272;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#29983;&#25104;&#22120;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#24207;&#21015;&#65292;&#35780;&#20272;&#22120;&#22522;&#20110;&#20272;&#35745;&#30340;&#21015;&#34920;&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#24207;&#21015;&#12290;&#29983;&#25104;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#29983;&#25104;&#22120;&#20989;&#25968;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#33258;&#22238;&#24402;&#31574;&#30053;&#36827;&#34892;&#24207;&#21015;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24037;&#19994;&#31995;&#32479;&#20013;&#37096;&#32626;&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65288;NAR4Rec&#65289;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;m
&lt;/p&gt;
&lt;p&gt;
In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
&lt;/p&gt;</description></item><item><title>LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06859</link><description>&lt;p&gt;
LiRank: &#39046;&#33521;&#30340;&#24037;&#19994;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiRank: Industrial Large Scale Ranking Models at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06859
&lt;/p&gt;
&lt;p&gt;
LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LiRank&#65292;&#36825;&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#23558;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24314;&#27169;&#25913;&#36827;&#65292;&#21253;&#25324;Residual DCN&#65292;&#23427;&#22312;&#33879;&#21517;&#30340;DCNv2&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#23558;SOTA&#26550;&#26500;&#32452;&#21512;&#21644;&#35843;&#20248;&#20197;&#21019;&#24314;&#32479;&#19968;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;Dense Gating&#12289;Transformers&#21644;Residual DCN&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#26657;&#20934;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25506;&#32034;/&#21033;&#29992;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;&#30340;&#26377;&#25928;&#12289;&#29983;&#20135;&#32423;&#26381;&#21153;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#21270;&#21644;&#35789;&#27719;&#21387;&#32553;&#35757;&#32451;&#21644;&#21387;&#32553;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Feed&#25490;&#21517;&#12289;&#32844;&#20301;&#25512;&#33616;&#21644;&#24191;&#21578;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#31561;&#22823;&#35268;&#27169;&#20351;&#29992;&#26696;&#20363;&#30340;&#37096;&#32626;&#35774;&#32622;&#32454;&#33410;&#12290;&#36890;&#36807;&#38416;&#26126;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#31181;A/B&#27979;&#35797;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#21160;&#24577;&#22270;&#20840;&#38754;&#25429;&#25417;&#32929;&#31080;&#20043;&#38388;&#30340;&#22810;&#26041;&#38754;&#20851;&#31995;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#65292;&#24182;&#21033;&#29992;Transformer&#32467;&#26500;&#30340;&#33021;&#21147;&#23545;&#22810;&#37325;&#20851;&#31995;&#30340;&#26102;&#38388;&#28436;&#21464;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#32929;&#31080;&#21450;&#20854;&#20851;&#32852;&#23454;&#20307;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#23436;&#25972;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.06633</link><description>&lt;p&gt;
MDGNN&#65306;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20840;&#38754;&#21644;&#21160;&#24577;&#30340;&#32929;&#31080;&#25237;&#36164;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive and Dynamic Stock Investment Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06633
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#21160;&#24577;&#22270;&#20840;&#38754;&#25429;&#25417;&#32929;&#31080;&#20043;&#38388;&#30340;&#22810;&#26041;&#38754;&#20851;&#31995;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#65292;&#24182;&#21033;&#29992;Transformer&#32467;&#26500;&#30340;&#33021;&#21147;&#23545;&#22810;&#37325;&#20851;&#31995;&#30340;&#26102;&#38388;&#28436;&#21464;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#32929;&#31080;&#21450;&#20854;&#20851;&#32852;&#23454;&#20307;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#23436;&#25972;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#24066;&#22330;&#26159;&#37329;&#34701;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#30001;&#20110;&#32463;&#27982;&#25351;&#26631;&#12289;&#36130;&#21153;&#25253;&#21578;&#12289;&#20840;&#29699;&#26032;&#38395;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#31561;&#22810;&#26041;&#38754;&#30340;&#21160;&#24577;&#21644;&#22797;&#26434;&#20851;&#31995;&#65292;&#39044;&#27979;&#32929;&#20215;&#30340;&#21464;&#21160;&#26159;&#22256;&#38590;&#30340;&#12290;&#20256;&#32479;&#30340;&#24207;&#21015;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#24050;&#32463;&#24212;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#25417;&#32929;&#20215;&#21464;&#21160;&#20013;&#30340;&#22810;&#26041;&#38754;&#21644;&#26102;&#38388;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNN&#65289;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#31163;&#25955;&#21160;&#24577;&#22270;&#20840;&#38754;&#25429;&#25417;&#32929;&#31080;&#20043;&#38388;&#30340;&#22810;&#26041;&#38754;&#20851;&#31995;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#30001;&#22270;&#29983;&#25104;&#30340;&#34920;&#31034;&#25552;&#20379;&#20102;&#32929;&#31080;&#21450;&#20854;&#20851;&#32852;&#23454;&#20307;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#23436;&#25972;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#36824;&#21033;&#29992;Transformer&#32467;&#26500;&#30340;&#33021;&#21147;&#23545;&#22810;&#37325;&#20851;&#31995;&#30340;&#26102;&#38388;&#28436;&#21464;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stock market is a crucial component of the financial system, but predicting the movement of stock prices is challenging due to the dynamic and intricate relations arising from various aspects such as economic indicators, financial reports, global news, and investor sentiment. Traditional sequential methods and graph-based models have been applied in stock movement prediction, but they have limitations in capturing the multifaceted and temporal influences in stock price movements. To address these challenges, the Multi-relational Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a discrete dynamic graph to comprehensively capture multifaceted relations among stocks and their evolution over time. The representation generated from the graph offers a complete perspective on the interrelationships among stocks and associated entities. Additionally, the power of the Transformer structure is leveraged to encode the temporal evolution of multiplex relations, provid
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;</title><link>https://arxiv.org/abs/2402.05116</link><description>&lt;p&gt;
&#37327;&#21270;&#30456;&#20284;&#24615;&#65306;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#20851;&#32852;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#20869;&#23481;&#33021;&#21147;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35780;&#20272;&#36890;&#36807;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#26377;&#29992;&#24615;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30340;&#24179;&#22343;&#20540;&#65292;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#20869;&#23481;&#19982;&#31185;&#23398;&#23478;&#20135;&#29983;&#30340;&#30495;&#23454;&#25991;&#29486;&#30340;&#30456;&#20284;&#24615;&#21644;&#25509;&#36817;&#31243;&#24230;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#20010;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#65292;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#29983;&#25104;ChatGPT&#21644;Google Bard&#30340;&#20020;&#24202;&#20869;&#23481;&#65292;&#20197;&#20415;&#19982;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#65288;2&#65289;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#25152;&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#30340;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#27604;&#36739;&#25991;&#26723;&#21644;&#30456;&#20851;&#30340;&#20108;&#20803;&#32452;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#26469;&#35780;&#20272;&#26415;&#35821;&#30340;&#20013;&#24515;&#24615;&#12290;&#32467;&#26524;&#65306;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Google Bard&#65288;38%&#23545;34%&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#20256;&#36755;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#21644;&#39640;&#32500;&#21442;&#25968;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.14968</link><description>&lt;p&gt;
&#38544;&#34255;&#24744;&#30340;&#27169;&#22411;&#65306;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#20256;&#36755;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hide Your Model: A Parameter Transmission-free Federated Recommender System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14968
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#20256;&#36755;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#21644;&#39640;&#32500;&#21442;&#25968;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#65292;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRec&#65289;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#36817;&#26469;&#21463;&#21040;&#37325;&#35270;&#12290;&#29616;&#26377;&#30340;FedRec&#36890;&#24120;&#36981;&#24490;&#19968;&#31181;&#23398;&#20064;&#21327;&#35758;&#65292;&#21363;&#20013;&#22830;&#26381;&#21153;&#22120;&#19982;&#23458;&#25143;&#31471;&#20849;&#20139;&#20840;&#23616;&#25512;&#33616;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#39057;&#32321;&#36890;&#20449;&#27169;&#22411;&#30340;&#20844;&#20849;&#21442;&#25968;&#26469;&#23454;&#29616;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#26694;&#26550;&#26377;&#20004;&#20010;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#21487;&#29992;&#24615;&#65306;&#65288;1&#65289;&#23427;&#38656;&#35201;&#19968;&#20010;&#20840;&#23616;&#20849;&#20139;&#30340;&#25512;&#33616;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#19982;&#25512;&#33616;&#27169;&#22411;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#20854;&#31639;&#27861;&#21644;&#21442;&#25968;&#65292;&#26500;&#25104;&#20102;&#24179;&#21488;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#22240;&#27492;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#19981;&#22826;&#21487;&#33021;&#20027;&#21160;&#21457;&#24067;&#27492;&#31867;&#20449;&#24687;&#12290;&#65288;2&#65289;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#30340;&#36890;&#20449;&#25104;&#26412;&#26114;&#36149;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#36890;&#24120;&#26159;&#39640;&#32500;&#30697;&#38453;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36890;&#20449;&#25104;&#26412;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing concerns regarding user data privacy, Federated Recommender System (FedRec) has garnered significant attention recently due to its privacy-preserving capabilities. Existing FedRecs generally adhere to a learning protocol in which a central server shares a global recommendation model with clients, and participants achieve collaborative learning by frequently communicating the model's public parameters. Nevertheless, this learning framework has two drawbacks that limit its practical usability: (1) It necessitates a global-sharing recommendation model; however, in real-world scenarios, information related to the recommender model, including its algorithm and parameters, constitutes the platforms' intellectual property. Hence, service providers are unlikely to release such information actively. (2) The communication costs of model parameter transmission are expensive since the model parameters are usually high-dimensional matrices. With the model size increasing, the commu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05116</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#35770;&#35777;&#21644;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20449;&#24687;&#21644;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#32858;&#21512;&#65288;CCA&#65289;&#21644;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25991;&#26723;&#32423;EAE&#12290;CCA&#27169;&#22359;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;RLIG&#27169;&#22359;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#35282;&#33394;&#34920;&#31034;&#25552;&#20379;&#23453;&#36149;&#30340;&#20449;&#24687;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CCA&#21644;RLIG&#27169;&#22359;&#32039;&#20945;&#12289;&#21487;&#31227;&#26893;&#19988;&#39640;&#25928;&#65292;&#24341;&#20837;&#30340;&#26032;&#21442;&#25968;&#19981;&#36229;&#36807;1%&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>RETEXO&#26159;&#31532;&#19968;&#20010;&#28040;&#38500;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#36890;&#20449;&#29942;&#39048;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#25042;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#21892;&#32593;&#32476;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2302.13053</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#22270;&#19978;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Training over Distributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13053
&lt;/p&gt;
&lt;p&gt;
RETEXO&#26159;&#31532;&#19968;&#20010;&#28040;&#38500;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#36890;&#20449;&#29942;&#39048;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#25042;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#21892;&#32593;&#32476;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28041;&#21450;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#12290;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#24448;&#24448;&#38656;&#35201;&#20998;&#24067;&#24335;&#23384;&#20648;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#19978;&#65292;&#21407;&#22240;&#19981;&#20165;&#26159;&#22240;&#20026;&#23481;&#37327;&#38480;&#21046;&#65292;&#36824;&#26377;&#25968;&#25454;&#25152;&#22312;&#22320;&#25110;&#38544;&#31169;&#27861;&#24459;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#32593;&#32476;&#36890;&#20449;&#25104;&#26412;&#24456;&#39640;&#65292;&#25104;&#20026;&#35757;&#32451;GNN&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#20248;&#21270;&#20027;&#35201;&#38024;&#23545;&#25968;&#25454;&#32423;&#21035;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;&#32531;&#23384;&#12289;&#32593;&#32476;&#24863;&#30693;&#21010;&#20998;&#21644;&#23376;&#37319;&#26679;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#25968;&#25454;&#20013;&#24515;&#31867;&#20284;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#22270;&#25968;&#25454;&#23545;&#21333;&#20010;&#23454;&#20307;&#21487;&#35775;&#38382;&#19988;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#34987;&#24573;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RETEXO&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#20005;&#37325;&#36890;&#20449;&#29942;&#39048;&#30340;&#39318;&#20010;&#26694;&#26550;&#65292;&#21516;&#26102;&#23562;&#37325;&#20219;&#20309;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#21306;&#37197;&#32622;&#12290;&#20851;&#38190;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#25042;&#28040;&#24687;&#20256;&#36882;&#65292;&#37325;&#26032;&#25490;&#24207;&#20102;&#28040;&#24687;&#20256;&#36882;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#35268;&#21017;&#30340;&#38750;&#35268;&#21017;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#30452;&#25509;&#32435;&#20837;&#24213;&#23618;&#35268;&#21017;&#35821;&#35328;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#30053;&#24494;&#36739;&#24369;&#34920;&#36798;&#33021;&#21147;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#29255;&#27573;&#12290;</title><link>https://arxiv.org/abs/2010.13442</link><description>&lt;p&gt;
&#19968;&#31181;&#32431;&#31929;&#35268;&#21017;&#30340;&#38750;&#35268;&#21017;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Purely Regular Approach to Non-Regular Core Spanners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.13442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#35268;&#21017;&#30340;&#38750;&#35268;&#21017;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#30452;&#25509;&#32435;&#20837;&#24213;&#23618;&#35268;&#21017;&#35821;&#35328;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#30053;&#24494;&#36739;&#24369;&#34920;&#36798;&#33021;&#21147;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21017;&#36328;&#24230;&#29983;&#25104;&#22120;&#26159;&#36890;&#36807;vset-&#33258;&#21160;&#26426;&#29305;&#24449;&#21270;&#30340;&#65292;&#23427;&#20204;&#23545;&#24182;&#38598;&#12289;&#36830;&#25509;&#21644;&#25237;&#24433;&#31561;&#20195;&#25968;&#25805;&#20316;&#23553;&#38381;&#65292;&#24182;&#20855;&#26377;&#29702;&#24819;&#30340;&#31639;&#27861;&#23646;&#24615;&#12290;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#20316;&#20026;IBM SystemT&#20013;&#26597;&#35810;&#35821;&#35328;AQL&#30340;&#26680;&#24515;&#21151;&#33021;&#30340;&#24418;&#24335;&#21270;&#24341;&#20837;&#65292;&#38500;&#20102;&#38656;&#35201;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#22806;&#65292;&#36824;&#34987;&#35777;&#26126;&#20250;&#23548;&#33268;&#38745;&#24577;&#20998;&#26512;&#21644;&#26597;&#35810;&#35780;&#20272;&#20013;&#20856;&#22411;&#38382;&#39064;&#30340;&#39640;&#22797;&#26434;&#24615;&#29978;&#33267;&#19981;&#21487;&#21028;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;&#65306;&#23558;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#30452;&#25509;&#32435;&#20837;&#34920;&#31034;&#24213;&#23618;&#35268;&#21017;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#35268;&#21017;&#35821;&#35328;&#20013;&#65288;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#22312;&#35268;&#21017;&#36328;&#24230;&#29983;&#25104;&#22120;&#25552;&#21462;&#30340;&#34920;&#19978;&#30340;&#20195;&#25968;&#25805;&#20316;&#65289;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#30053;&#24494;&#36739;&#24369;&#34920;&#36798;&#33021;&#21147;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The regular spanners (characterised by vset-automata) are closed under the algebraic operations of union, join and projection, and have desirable algorithmic properties. The core spanners (introduced by Fagin, Kimelfeld, Reiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core functionality of the query language AQL used in IBM's SystemT) additionally need string-equality selections and it has been shown by Freydenberger and Holldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high complexity and even undecidability of the typical problems in static analysis and query evaluation. We propose an alternative approach to core spanners: by incorporating the string-equality selections directly into the regular language that represents the underlying regular spanner (instead of treating it as an algebraic operation on the table extracted by the regular spanner), we obtain a fragment of core spanners that, while having slightly weaker expressive power t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20302;&#21516;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Social Heterophily-alleviating Rewiring (SHaRe)&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#25429;&#25417;&#39640;&#21516;&#36136;&#30340;&#31038;&#20132;&#20851;&#31995;&#24182;&#21098;&#20999;&#20302;&#21516;&#36136;&#20851;&#31995;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#25552;&#21462;&#20102;&#20559;&#22909;&#24863;&#30693;&#21516;&#36136;&#24615;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14606</link><description>&lt;p&gt;
&#25361;&#25112;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20302;&#21516;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Challenging Low Homophily in Social Recommendation. (arXiv:2401.14606v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20302;&#21516;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Social Heterophily-alleviating Rewiring (SHaRe)&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#25429;&#25417;&#39640;&#21516;&#36136;&#30340;&#31038;&#20132;&#20851;&#31995;&#24182;&#21098;&#20999;&#20302;&#21516;&#36136;&#20851;&#31995;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#25552;&#21462;&#20102;&#20559;&#22909;&#24863;&#30693;&#21516;&#36136;&#24615;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#25512;&#33616;&#20013;&#65292;&#21033;&#29992;&#31038;&#20132;&#20851;&#31995;&#26469;&#35299;&#20915;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#22522;&#20110;&#31038;&#20132;&#21516;&#36136;&#24615;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#25512;&#33616;&#33539;&#24335;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#21516;&#36136;&#24615;&#12290;&#34429;&#28982;&#31038;&#20132;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#25928;&#26524;&#65292;&#20294;&#23427;&#19982;&#29992;&#25143;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#24182;&#19981;&#20445;&#35777;&#65292;&#20174;&#32780;&#21487;&#33021;&#24341;&#20837;&#20449;&#24687;&#20887;&#20313;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#22270;&#23637;&#29616;&#20986;&#20302;&#20559;&#22909;&#24863;&#30693;&#30340;&#21516;&#36136;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#20840;&#38754;&#25552;&#21462;&#31038;&#20132;&#22270;&#20013;&#28508;&#22312;&#30340;&#20559;&#22909;&#24863;&#30693;&#21516;&#36136;&#24615;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Social Heterophily-alleviating Rewiring (SHaRe)&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#22270;&#37325;&#36830;&#25216;&#26415;&#26469;&#25429;&#25417;&#21644;&#28155;&#21152;&#39640;&#24230;&#21516;&#36136;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#24182;&#21098;&#20999;&#20302;&#21516;&#36136;&#65288;&#25110;&#24322;&#36136;&#65289;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20248;&#21270;&#20174;&#31038;&#20132;&#22270;&#20013;&#25552;&#21462;&#30340;&#25512;&#33616;&#27169;&#24335;&#30340;&#21051;&#30011;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#22522;&#20110;&#22270;&#30340;&#29992;&#25143;&#20559;&#22909;&#20998;&#24067;&#20462;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the u
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item></channel></rss>