<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#35780;&#20215;&#26469;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#37197;&#32622;&#25991;&#20214;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19981;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36879;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#33616;&#65292;&#32780;&#19988;&#24615;&#33021;&#27700;&#24179;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.05810</link><description>&lt;p&gt;
&#36879;&#26126;&#19982;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
Natural Language User Profiles for Transparent and Scrutable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#35780;&#20215;&#26469;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#37197;&#32622;&#25991;&#20214;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19981;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36879;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#33616;&#65292;&#32780;&#19988;&#24615;&#33021;&#27700;&#24179;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#38544;&#24335;&#25110;&#26174;&#24335;&#21453;&#39304;&#26469;&#24314;&#35758;&#26032;&#39033;&#30446;&#12290;&#34429;&#28982;&#22312;&#25512;&#33616;&#26032;&#36873;&#39033;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#20256;&#32479;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#12290;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#19981;&#20165;&#38480;&#21046;&#20102;&#29992;&#25143;&#23545;&#20026;&#20160;&#20040;&#24314;&#35758;&#26576;&#20123;&#39033;&#30446;&#30340;&#29702;&#35299;&#65292;&#36824;&#20943;&#23569;&#20102;&#29992;&#25143;&#36731;&#26494;&#23457;&#26597;&#21644;&#32534;&#36753;&#20854;&#20559;&#22909;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#20215;&#26469;&#21019;&#24314;&#25551;&#36848;&#29992;&#25143;&#20559;&#22909;&#30340;&#20010;&#24615;&#21270;&#33258;&#28982;&#35821;&#35328;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36825;&#20123;&#25551;&#36848;&#24615;&#37197;&#32622;&#25991;&#20214;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#25552;&#20379;&#36879;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#22312;&#19982;&#24050;&#24314;&#31435;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art recommender systems predominantly rely on either implicit or explicit feedback from users to suggest new items. While effective in recommending novel options, these conventional systems often use uninterpretable embeddings. This lack of transparency not only limits user understanding of why certain items are suggested but also reduces the user's ability to easily scrutinize and edit their preferences. For example, if a user has a change in interests, they would need to make significant changes to their interaction history to adjust the model's recommendations. To address these limitations, we introduce a novel method that utilizes user reviews to craft personalized, natural language profiles describing users' preferences. Through these descriptive profiles, our system provides transparent recommendations in natural language. Our evaluations show that this novel approach maintains a performance level on par with established recommender systems, but with the adde
&lt;/p&gt;</description></item><item><title>CounterCLR&#26159;&#19968;&#31181;&#22788;&#29702;&#25512;&#33616;&#31995;&#32479;&#20013;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#21453;&#20107;&#23454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#37319;&#29992;&#28145;&#24230;&#34920;&#31034;&#32593;&#32476;CauNet&#26469;&#25512;&#26029;&#32570;&#22833;&#25968;&#25454;&#24182;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.05740</link><description>&lt;p&gt;
CounterCLR: &#25512;&#33616;&#20013;&#20855;&#26377;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#21453;&#20107;&#23454;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05740
&lt;/p&gt;
&lt;p&gt;
CounterCLR&#26159;&#19968;&#31181;&#22788;&#29702;&#25512;&#33616;&#31995;&#32479;&#20013;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#21453;&#20107;&#23454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#37319;&#29992;&#28145;&#24230;&#34920;&#31034;&#32593;&#32476;CauNet&#26469;&#25512;&#26029;&#32570;&#22833;&#25968;&#25454;&#24182;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#35266;&#27979;&#21453;&#39304;&#36890;&#24120;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#31561;&#38382;&#39064;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#20934;&#30830;&#24615;&#21644;&#25490;&#21517;&#31561;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;CounterCLR&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CounterCLR&#37319;&#29992;&#20102;&#19968;&#20010;&#31216;&#20026;CauNet&#30340;&#28145;&#24230;&#34920;&#31034;&#32593;&#32476;&#65292;&#25512;&#26029;&#25512;&#33616;&#20013;&#30340;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are designed to learn user preferences from observed feedback and comprise many fundamental tasks, such as rating prediction and post-click conversion rate (pCVR) prediction. However, the observed feedback usually suffer from two issues: selection bias and data sparsity, where biased and insufficient feedback seriously degrade the performance of recommender systems in terms of accuracy and ranking. Existing solutions for handling the issues, such as data imputation and inverse propensity score, are highly susceptible to additional trained imputation or propensity models. In this work, we propose a novel counterfactual contrastive learning framework for recommendation, named CounterCLR, to tackle the problem of non-random missing data by exploiting the advances in contrast learning. Specifically, the proposed CounterCLR employs a deep representation network, called CauNet, to infer non-random missing data in recommendations and perform user preference modeling by fur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#20197;&#25351;&#20196;&#20026;&#23548;&#21521;&#30340;&#23884;&#20837;&#27169;&#22411;&#12290;&#27169;&#22411;&#22312;&#25512;&#29702;&#25928;&#29575;&#21644;&#23884;&#20837;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#24615;&#33021;&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#30340;&#20165;&#33521;&#25991;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.05672</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#65306;&#19968;&#39033;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Multilingual E5 Text Embeddings: A Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05672
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#20197;&#25351;&#20196;&#20026;&#23548;&#21521;&#30340;&#23884;&#20837;&#27169;&#22411;&#12290;&#27169;&#22411;&#22312;&#25512;&#29702;&#25928;&#29575;&#21644;&#23884;&#20837;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#24615;&#33021;&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#30340;&#20165;&#33521;&#25991;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;E5&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20110;2023&#24180;&#20013;&#26399;&#21457;&#24067;&#12290;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#65288;&#23567;/&#22522;&#30784;/&#22823;&#65289;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#25512;&#29702;&#25928;&#29575;&#21644;&#23884;&#20837;&#36136;&#37327;&#12290;&#35757;&#32451;&#36807;&#31243;&#36981;&#24490;&#33521;&#25991;E5&#27169;&#22411;&#30340;&#37197;&#26041;&#65292;&#28041;&#21450;10&#20159;&#20010;&#22810;&#35821;&#35328;&#25991;&#26412;&#23545;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19968;&#31995;&#21015;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20197;&#25351;&#20196;&#20026;&#23548;&#21521;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#24615;&#33021;&#19982;&#30456;&#20284;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#30340;&#20165;&#33521;&#25991;&#27169;&#22411;&#30456;&#24403;&#12290;&#26377;&#20851;&#27169;&#22411;&#21457;&#24067;&#30340;&#20449;&#24687;&#21487;&#20197;&#22312;https://github.com/microsoft/unilm/tree/master/e5&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#65292;LLMs&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.05318</link><description>&lt;p&gt;
&#33322;&#34892;&#30693;&#35782;&#20043;&#28023;&#65306;&#21033;&#29992;LLMs&#36827;&#34892;&#34892;&#26143;&#32423;&#31572;&#26696;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#65292;LLMs&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#20854;&#29305;&#28857;&#26159;&#20174;&#22522;&#26412;&#30340;&#36229;&#38142;&#25509;&#23548;&#33322;&#21040;&#22797;&#26434;&#30340;&#31639;&#27861;&#39537;&#21160;&#25628;&#32034;&#24341;&#25806;&#30340;&#19981;&#26029;&#25913;&#36827;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#19982;&#26032;&#20852;&#31572;&#26696;&#26816;&#32034;&#33539;&#24335;&#20043;&#38388;&#30340;&#26725;&#26753;&#20316;&#29992;&#12290;LLMs&#22312;&#21709;&#24212;&#26816;&#32034;&#21644;&#32034;&#24341;&#39046;&#22495;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#29992;&#25143;&#19982;&#20449;&#24687;&#31995;&#32479;&#20132;&#20114;&#26041;&#24335;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#26159;&#30001;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#39537;&#21160;&#30340;&#65292;&#23427;&#20204;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#20379;&#26356;&#30452;&#25509;&#21644;&#24773;&#22659;&#30456;&#20851;&#30340;&#31572;&#26696;&#32473;&#29992;&#25143;&#26597;&#35810;&#12290;&#36890;&#36807;&#36825;&#31181;&#25506;&#32034;&#65292;&#25105;&#20204;&#35797;&#22270;&#38416;&#26126;&#25216;&#26415;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones 
&lt;/p&gt;</description></item><item><title>CADReN&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;&#26426;&#21046;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#12290;</title><link>https://arxiv.org/abs/2402.05135</link><description>&lt;p&gt;
CADReN: &#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#29992;&#20110;&#21487;&#25511;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05135
&lt;/p&gt;
&lt;p&gt;
CADReN&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;&#26426;&#21046;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;(NIE)&#23545;&#20110;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23558;&#22806;&#37096;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#38745;&#24577;&#30340;&#21333;&#19968;&#22270;&#29305;&#24449;&#65292;&#22312;&#26032;&#22270;&#21644;&#29992;&#25143;&#29305;&#23450;&#35201;&#27714;&#26041;&#38754;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CADReN&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;(CA)&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#30456;&#23545;&#20110;CA&#35780;&#20272;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;(KGs)&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CADReN&#22312;&#36328;&#22270;NIE&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;CADReN&#36824;&#34987;&#35777;&#26126;&#22312;&#21333;&#19968;&#22270;NIE&#20219;&#21153;&#19978;&#19982;&#20197;&#21069;&#30340;&#27169;&#22411;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#65292;&#19987;&#38376;&#29992;&#20110;&#36328;&#22270;NIE&#30740;&#31350;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;</title><link>https://arxiv.org/abs/2402.05116</link><description>&lt;p&gt;
&#37327;&#21270;&#30456;&#20284;&#24615;&#65306;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#20851;&#32852;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#20869;&#23481;&#33021;&#21147;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35780;&#20272;&#36890;&#36807;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#26377;&#29992;&#24615;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30340;&#24179;&#22343;&#20540;&#65292;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#20869;&#23481;&#19982;&#31185;&#23398;&#23478;&#20135;&#29983;&#30340;&#30495;&#23454;&#25991;&#29486;&#30340;&#30456;&#20284;&#24615;&#21644;&#25509;&#36817;&#31243;&#24230;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#20010;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#65292;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#29983;&#25104;ChatGPT&#21644;Google Bard&#30340;&#20020;&#24202;&#20869;&#23481;&#65292;&#20197;&#20415;&#19982;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#65288;2&#65289;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#25152;&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#30340;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#27604;&#36739;&#25991;&#26723;&#21644;&#30456;&#20851;&#30340;&#20108;&#20803;&#32452;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#26469;&#35780;&#20272;&#26415;&#35821;&#30340;&#20013;&#24515;&#24615;&#12290;&#32467;&#26524;&#65306;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Google Bard&#65288;38%&#23545;34%&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#29702;&#35299;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#23558;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#65292;&#25105;&#20204;&#30340;LeRuD&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#35875;&#35328;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#26356;&#26377;&#28508;&#21147;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Detect Rumors on Social Media?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#29702;&#35299;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#23558;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#65292;&#25105;&#20204;&#30340;LeRuD&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#35875;&#35328;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#26356;&#26377;&#28508;&#21147;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25512;&#29702;&#25972;&#20010;&#20256;&#25773;&#20449;&#24687;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35813;&#20449;&#24687;&#21253;&#21547;&#26032;&#38395;&#20869;&#23481;&#21644;&#22823;&#37327;&#35780;&#35770;&#65292;LLMs&#21487;&#33021;&#26080;&#27861;&#38598;&#20013;&#20851;&#27880;&#22797;&#26434;&#20256;&#25773;&#20449;&#24687;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#22823;&#37327;&#21644;&#20887;&#20313;&#20449;&#24687;&#26102;&#38590;&#20197;&#36827;&#34892;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#22686;&#24378;&#30340;&#35875;&#35328;&#26816;&#27979;&#65288;LeRuD&#65289;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#20851;&#27880;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#24182;&#23558;&#25972;&#20010;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#20197;&#20943;&#36731;LLMs&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#22312;Twitter&#21644;&#24494;&#21338;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LeRuD&#30340;&#24615;&#33021;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;2.4&#65285;&#33267;7.6&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24212;&#29992;LLMs&#65292;LeRuD&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#26356;&#20855;&#26377;&#28508;&#21147;&#30340;&#35875;&#35328;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02152</link><description>&lt;p&gt;
&#35770;&#25991;&#39064;&#30446;&#65306;&#20026;&#20160;&#20040;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#20027;&#23548;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#65307;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#22788;&#20110;&#19968;&#31181;&#22855;&#29305;&#30340;&#22659;&#22320;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;A/B&#27979;&#35797;&#26469;&#34913;&#37327;&#24615;&#33021;&#26041;&#38754;&#26377;&#19968;&#20010;&#38750;&#24120;&#20005;&#26684;&#30340;&#21327;&#35758;&#65292;&#20294;&#25214;&#21040;&#35201;&#27979;&#35797;&#30340;&#8220;B&#8221;&#30340;&#26368;&#20339;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#38024;&#23545;&#24615;&#33021;&#65292;&#32780;&#26159;&#38024;&#23545;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;A/B&#27979;&#35797;&#30340;&#25104;&#21151;&#25110;&#22833;&#36133;&#23436;&#20840;&#21462;&#20915;&#20110;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#19982;&#24615;&#33021;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27809;&#26377;&#21407;&#21017;&#21487;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#30830;&#23450;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#26356;&#22909;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#20204;&#25720;&#19981;&#30528;&#22836;&#33041;&#12290;&#26412;&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#36136;&#30097;&#36825;&#31181;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#65292;&#24182;&#20027;&#24352;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#23454;&#38469;&#19978;&#26377;&#28508;&#21147;&#35299;&#38145;&#20248;&#21270;&#22870;&#21169;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
&lt;/p&gt;</description></item><item><title>ParlayANN&#26159;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#21644;&#24182;&#34892;&#24615;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;&#24211;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#24320;&#21457;&#36825;&#31867;&#31639;&#27861;&#30340;&#19968;&#22871;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.04359</link><description>&lt;p&gt;
ParlayANN: &#21487;&#25193;&#23637;&#21644;&#30830;&#23450;&#24615;&#30340;&#24182;&#34892;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.04359
&lt;/p&gt;
&lt;p&gt;
ParlayANN&#26159;&#19968;&#20010;&#20855;&#26377;&#30830;&#23450;&#24615;&#21644;&#24182;&#34892;&#24615;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;&#24211;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#24320;&#21457;&#36825;&#31867;&#31639;&#27861;&#30340;&#19968;&#22871;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#31639;&#27861;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#65288;&#21363;&#23884;&#20837;&#65289;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#22312;&#21508;&#31181;ANNS&#31639;&#27861;&#20013;&#65292;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#34987;&#35748;&#20026;&#22312;&#21534;&#21520;&#37327;&#21644;&#21484;&#22238;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;&#23613;&#31649;&#29616;&#20195;ANNS&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#22823;&#35268;&#27169;&#65292;&#20294;&#29616;&#26377;&#30340;&#24182;&#34892;&#22522;&#20110;&#22270;&#30340;&#23454;&#29616;&#30001;&#20110;&#22823;&#37327;&#20351;&#29992;&#38145;&#21644;&#20854;&#20182;&#39034;&#24207;&#29942;&#39048;&#32780;&#23384;&#22312;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20004;&#28857;&#38459;&#30861;&#20102;&#23427;&#20204;&#26377;&#25928;&#25193;&#23637;&#21040;&#22823;&#37327;&#22788;&#29702;&#22120;&#65292;&#24182;&#23548;&#33268;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#38750;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate nearest-neighbor search (ANNS) algorithms are a key part of the modern deep learning stack due to enabling efficient similarity search over high-dimensional vector space representations (i.e., embeddings) of data. Among various ANNS algorithms, graph-based algorithms are known to achieve the best throughput-recall tradeoffs. Despite the large scale of modern ANNS datasets, existing parallel graph based implementations suffer from significant challenges to scale to large datasets due to heavy use of locks and other sequential bottlenecks, which 1) prevents them from efficiently scaling to a large number of processors, and 2) results in nondeterminism that is undesirable in certain applications.   In this paper, we introduce ParlayANN, a library of deterministic and parallel graph-based approximate nearest neighbor search algorithms, along with a set of useful tools for developing such algorithms. In this library, we develop novel parallel implementations for four state-of-th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;AI&#25945;&#36741;Kwame&#65292;&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;Kwame for Science&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#21644;&#38382;&#39064;&#25552;&#38382;&#37327;&#12290;</title><link>https://arxiv.org/abs/2302.10786</link><description>&lt;p&gt;
&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#30340;AI&#25945;&#36741;Kwame&#30340;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;AI&#25945;&#36741;Kwame&#65292;&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;Kwame for Science&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#21644;&#38382;&#39064;&#25552;&#38382;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#27604;&#20363;&#39640;&#65292;&#36825;&#38480;&#21046;&#20102;&#23398;&#29983;&#20204;&#33719;&#21462;&#25945;&#32946;&#38382;&#39064;&#35299;&#31572;&#31561;&#23398;&#20064;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#23558;&#38754;&#21521;&#32534;&#30721;&#25945;&#32946;&#30340;AI&#25945;&#36741;Kwame&#25193;&#23637;&#20026;&#38754;&#21521;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#37096;&#32626;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#12290;Kwame for Science&#36890;&#36807;&#25552;&#20379;&#26469;&#33258;&#31934;&#36873;&#30693;&#35782;&#26469;&#28304;&#30340;&#27573;&#33853;&#20197;&#21450;&#22522;&#20110;&#35199;&#38750;&#39640;&#32423;&#20013;&#23398;&#35777;&#20070;&#32771;&#35797;&#65288;WASSCE&#65289;&#30340;&#32508;&#21512;&#31185;&#23398;&#31185;&#30446;&#30340;&#30456;&#20851;&#36807;&#21435;&#30340;&#22269;&#23478;&#32771;&#35797;&#38382;&#39064;&#30340;&#31572;&#26696;&#26469;&#22238;&#31572;&#23398;&#29983;&#20204;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23398;&#29983;&#20204;&#36824;&#21487;&#20197;&#26597;&#30475;&#36807;&#21435;&#30340;&#22269;&#23478;&#32771;&#35797;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#24320;&#21457;&#30340;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#25353;&#24180;&#20221;&#12289;&#38382;&#39064;&#31867;&#22411;&#21644;&#20027;&#39064;&#30340;&#33258;&#21160;&#20998;&#31867;&#65288;91%&#38750;&#21152;&#26435;&#24179;&#22343;&#21484;&#22238;&#29575;&#65289;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;&#20102;Kwame for Science&#36229;&#36807;8&#20010;&#26376;&#65292;&#26377;&#26469;&#33258;32&#20010;&#22269;&#23478;&#65288;&#20854;&#20013;15&#20010;&#22312;&#38750;&#27954;&#65289;&#30340;750&#20010;&#29992;&#25143;&#65292;&#20849;&#25552;&#20986;&#20102;1.5K&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;87.2%&#30340;&#21069;&#19977;&#21517;&#38382;&#39064;&#20934;&#30830;&#29575;&#65288;n=109&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109
&lt;/p&gt;</description></item><item><title>MMRec&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#31616;&#21270;&#21644;&#35268;&#33539;&#22810;&#27169;&#24335;&#25512;&#33616;&#27169;&#22411;&#30340;&#23454;&#26045;&#21644;&#27604;&#36739;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#24179;&#21488;&#26469;&#34701;&#21512;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2302.03497</link><description>&lt;p&gt;
MMRec&#65306;&#31616;&#21270;&#22810;&#27169;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
MMRec: Simplifying Multimodal Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03497
&lt;/p&gt;
&lt;p&gt;
MMRec&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#31616;&#21270;&#21644;&#35268;&#33539;&#22810;&#27169;&#24335;&#25512;&#33616;&#27169;&#22411;&#30340;&#23454;&#26045;&#21644;&#27604;&#36739;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#24179;&#21488;&#26469;&#34701;&#21512;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24335;&#25512;&#33616;&#30340;&#24320;&#28304;&#24037;&#20855;&#31665;MMRec&#12290;MMRec&#31616;&#21270;&#21644;&#35268;&#33539;&#20102;&#23454;&#26045;&#21644;&#27604;&#36739;&#22810;&#27169;&#24335;&#25512;&#33616;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;MMRec&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#19988;&#21487;&#37197;&#32622;&#30340;&#24179;&#21488;&#65292;&#20197;&#26368;&#23567;&#21270;&#23454;&#26045;&#21644;&#27979;&#35797;&#22810;&#27169;&#24335;&#25512;&#33616;&#27169;&#22411;&#30340;&#24037;&#20316;&#37327;&#12290;&#23427;&#20351;&#24471;&#20256;&#32479;&#30340;&#30697;&#38453;&#20998;&#35299;&#21040;&#29616;&#20195;&#30340;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#31561;&#22810;&#27169;&#24335;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25991;&#26723;&#12289;&#31034;&#20363;&#21644;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/enoche/MMRec}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an open-source toolbox, MMRec for multimodal recommendation. MMRec simplifies and canonicalizes the process of implementing and comparing multimodal recommendation models. The objective of MMRec is to provide a unified and configurable arena that can minimize the effort in implementing and testing multimodal recommendation models. It enables multimodal models, ranging from traditional matrix factorization to modern graph-based algorithms, capable of fusing information from multiple modalities simultaneously. Our documentation, examples, and source code are available at \url{https://github.com/enoche/MMRec}.
&lt;/p&gt;</description></item><item><title>&#24102;&#21453;&#21521;&#24341;&#29992;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#21442;&#25968;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20869;&#23384;&#30830;&#23450;&#24615;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#20854;&#20182;&#30456;&#20851;&#27010;&#24565;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#38590;&#20197;&#26816;&#27979;&#30340;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/1903.05896</link><description>&lt;p&gt;
&#24102;&#21453;&#21521;&#24341;&#29992;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#65306;&#22810;&#39033;&#24335;&#26102;&#38388;&#21305;&#37197;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Regular Expressions with Backreferences: Polynomial-Time Matching Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1903.05896
&lt;/p&gt;
&lt;p&gt;
&#24102;&#21453;&#21521;&#24341;&#29992;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#21442;&#25968;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20869;&#23384;&#30830;&#23450;&#24615;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#20854;&#20182;&#30456;&#20851;&#27010;&#24565;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#38590;&#20197;&#26816;&#27979;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21453;&#21521;&#24341;&#29992;&#65288;regex&#65289;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#22312;&#22823;&#22810;&#25968;&#29616;&#20195;&#27491;&#21017;&#34920;&#36798;&#24335;&#21305;&#37197;&#24211;&#20013;&#24471;&#21040;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20855;&#26377;&#19968;&#20010;NP&#23436;&#20840;&#30340;&#21305;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22797;&#26434;&#24230;&#21442;&#25968;&#65292;&#31216;&#20026;&#27963;&#21160;&#21464;&#37327;&#24230;&#65292;&#22312;&#27492;&#21442;&#25968;&#21463;&#21040;&#24120;&#25968;&#38480;&#21046;&#26102;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21305;&#37197;&#27491;&#21017;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30830;&#23450;&#24615;&#31867;&#22411;&#65292;&#29992;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#65288;&#22312;&#33258;&#21160;&#26426;&#29702;&#35770;&#23618;&#38754;&#19978;&#65289;&#65292;&#36825;&#20135;&#29983;&#20102;&#21487;&#20197;&#22312;O(|w|p(|r|))&#26102;&#38388;&#20869;&#21305;&#37197;&#30340;&#20869;&#23384;&#30830;&#23450;&#24615;&#27491;&#21017;&#34920;&#36798;&#24335;&#12290;&#36825;&#37324;&#65292;r&#26159;&#27491;&#21017;&#34920;&#36798;&#24335;&#65292;w&#26159;&#23383;&#31526;&#20018;&#12290;&#36825;&#20123;&#27010;&#24565;&#30340;&#33258;&#28982;&#25193;&#23637;&#23548;&#33268;&#20102;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#19968;&#20123;&#38590;&#20197;&#26816;&#27979;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular expressions with backreferences (regex, for short), as supported by most modern libraries for regular expression matching, have an NP-complete matching problem. We define a complexity parameter of regex, called active variable degree, such that regex with this parameter bounded by a constant can be matched in polynomial-time. Moreover, we formulate a novel type of determinism for regex (on an automaton-theoretic level), which yields the class of memory-deterministic regex that can be matched in time O(|w|p(|r|)) for a polynomial p (where r is the regex and w the word). Natural extensions of these concepts lead to properties of regex that are intractable to check.
&lt;/p&gt;</description></item><item><title>REFORM&#26159;&#19968;&#20010;CTR&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#27969;&#24335;&#21472;&#21152;&#30340;&#24490;&#29615;&#32467;&#26500;&#21033;&#29992;&#20102;&#22810;&#32423;&#39640;&#38454;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#28040;&#38500;&#20102;&#35823;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2309.14891</link><description>&lt;p&gt;
REFORM: &#31227;&#38500;CTR&#39044;&#27979;&#20013;&#30340;&#35823;&#20851;&#32852;&#30340;&#22810;&#32423;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
REFORM: Removing False Correlation in Multi-level Interaction for CTR Prediction. (arXiv:2309.14891v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14891
&lt;/p&gt;
&lt;p&gt;
REFORM&#26159;&#19968;&#20010;CTR&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#27969;&#24335;&#21472;&#21152;&#30340;&#24490;&#29615;&#32467;&#26500;&#21033;&#29992;&#20102;&#22810;&#32423;&#39640;&#38454;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#28040;&#38500;&#20102;&#35823;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#29992;&#25143;&#23450;&#20301;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#21069;&#27839;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22797;&#26434;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#29305;&#24449;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#30001;&#28151;&#28102;&#22240;&#23376;&#25110;&#36873;&#25321;&#20559;&#24046;&#24341;&#36215;&#30340;&#35823;&#20851;&#32852;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#36825;&#20123;&#20132;&#20114;&#30340;&#22797;&#26434;&#24615;&#21644;&#20887;&#20313;&#24615;&#19979;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CTR&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;REFORM&#65292;&#22312;&#22810;&#32423;&#29305;&#24449;&#20132;&#20114;&#20013;&#31227;&#38500;&#20102;&#35823;&#20851;&#32852;&#12290;&#25152;&#25552;&#20986;&#30340;REFORM&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#27969;&#24335;&#21472;&#21152;&#30340;&#24490;&#29615;&#32467;&#26500;&#21033;&#29992;&#20102;&#22823;&#37327;&#30340;&#22810;&#32423;&#39640;&#38454;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#28040;&#38500;&#20102;&#35823;&#20851;&#32852;&#12290;&#35813;&#26694;&#26550;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;I. &#22810;&#32423;&#21472;&#21152;&#24490;&#29615;&#65288;MSR&#65289;&#32467;&#26500;&#20351;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#21040;&#26469;&#33258;&#29305;&#24449;&#31354;&#38388;&#30340;&#22810;&#26679;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is a critical task in online advertising and recommendation systems, as accurate predictions are essential for user targeting and personalized recommendations. Most recent cutting-edge methods primarily focus on investigating complex implicit and explicit feature interactions. However, these methods neglect the issue of false correlations caused by confounding factors or selection bias. This problem is further magnified by the complexity and redundancy of these interactions. We propose a CTR prediction framework that removes false correlation in multi-level feature interaction, termed REFORM. The proposed REFORM framework exploits a wide range of multi-level high-order feature representations via a two-stream stacked recurrent structure while eliminating false correlations. The framework has two key components: I. The multi-level stacked recurrent (MSR) structure enables the model to efficiently capture diverse nonlinear interactions from feature spa
&lt;/p&gt;</description></item></channel></rss>