<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#28151;&#21512;&#22120;&#21644;&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#20998;&#31163;&#65292;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#24314;&#31435;&#21644;&#26816;&#32034;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13820</link><description>&lt;p&gt;
&#35270;&#39057;&#21644;&#38899;&#39057;&#26159;&#22270;&#20687;&#65306;&#29992;&#20110;&#35270;&#39057;&#38899;&#39057;&#26816;&#32034;&#21407;&#22987;&#25968;&#25454;&#30340;&#36328;&#27169;&#24577;&#28151;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Video and Audio are Images: A Cross-Modal Mixer for Original Data on Video-Audio Retrieval. (arXiv:2308.13820v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#28151;&#21512;&#22120;&#21644;&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#20998;&#31163;&#65292;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#24314;&#31435;&#21644;&#26816;&#32034;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#22312;&#36817;&#24180;&#26469;&#21464;&#24471;&#27969;&#34892;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22810;&#23186;&#20307;&#30340;&#20852;&#36215;&#12290;&#36890;&#24120;&#65292;&#27599;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#34920;&#31034;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#29305;&#24449;&#24448;&#24448;&#22788;&#20110;&#32534;&#30721;&#20026;&#21452;&#22612;&#32467;&#26500;&#30340;&#20998;&#31163;&#28508;&#31354;&#38388;&#20013;&#65292;&#24182;&#19988;&#38590;&#20197;&#24314;&#31435;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23548;&#33268;&#26816;&#32034;&#24615;&#33021;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#36328;&#27169;&#24577;&#28151;&#21512;&#22120;&#12289;&#19968;&#20010;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#36328;&#27169;&#24577;&#28151;&#21512;&#22120;&#21644;&#25513;&#27169;&#24314;&#27169;&#26469;&#34701;&#21512;&#21407;&#22987;&#27169;&#24577;&#24182;&#28040;&#38500;&#20887;&#20313;&#12290;&#28982;&#21518;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24212;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23454;&#29616;&#34701;&#21512;&#21518;&#20998;&#31163;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25513;&#27169;&#34701;&#21512;&#34920;&#31034;&#39304;&#36865;&#21040;&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#29992;&#35299;&#30721;&#22120;&#36827;&#34892;&#37325;&#26500;&#65292;&#26368;&#32456;&#20998;&#31163;&#20986;&#20004;&#20010;&#27169;&#24577;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval has become popular in recent years, particularly with the rise of multimedia. Generally, the information from each modality exhibits distinct representations and semantic information, which makes feature tends to be in separate latent spaces encoded with dual-tower architecture and makes it difficult to establish semantic relationships between modalities, resulting in poor retrieval performance. To address this issue, we propose a novel framework for cross-modal retrieval which consists of a cross-modal mixer, a masked autoencoder for pre-training, and a cross-modal retriever for downstream tasks.In specific, we first adopt cross-modal mixer and mask modeling to fuse the original modality and eliminate redundancy. Then, an encoder-decoder architecture is applied to achieve a fuse-then-separate task in the pre-training phase.We feed masked fused representations into the encoder and reconstruct them with the decoder, ultimately separating the original data of two mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20013;&#24515;&#30456;&#20284;&#24615;&#22810;&#35270;&#22270;&#21704;&#24076;&#26041;&#27861;&#65288;CSMVH&#65289;&#65292;&#38024;&#23545;&#22810;&#23186;&#20307;&#26816;&#32034;&#20013;&#30340;&#23616;&#37096;&#30456;&#20284;&#24615;&#21644;&#22810;&#35270;&#22270;&#29305;&#24449;&#34701;&#21512;&#38382;&#39064;&#65292;&#21033;&#29992;&#20013;&#24515;&#30456;&#20284;&#24615;&#23398;&#20064;&#20840;&#23616;&#30456;&#20284;&#24615;&#24182;&#36816;&#29992;&#38376;&#25511;&#34701;&#21512;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13774</link><description>&lt;p&gt;
&#22810;&#23186;&#20307;&#26816;&#32034;&#30340;&#20013;&#24515;&#30456;&#20284;&#24615;&#22810;&#35270;&#22270;&#21704;&#24076;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Central Similarity Multi-View Hashing for Multimedia Retrieval. (arXiv:2308.13774v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20013;&#24515;&#30456;&#20284;&#24615;&#22810;&#35270;&#22270;&#21704;&#24076;&#26041;&#27861;&#65288;CSMVH&#65289;&#65292;&#38024;&#23545;&#22810;&#23186;&#20307;&#26816;&#32034;&#20013;&#30340;&#23616;&#37096;&#30456;&#20284;&#24615;&#21644;&#22810;&#35270;&#22270;&#29305;&#24449;&#34701;&#21512;&#38382;&#39064;&#65292;&#21033;&#29992;&#20013;&#24515;&#30456;&#20284;&#24615;&#23398;&#20064;&#20840;&#23616;&#30456;&#20284;&#24615;&#24182;&#36816;&#29992;&#38376;&#25511;&#34701;&#21512;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#24322;&#26500;&#25968;&#25454;&#30340;&#21704;&#24076;&#34920;&#31034;&#23398;&#20064;&#26159;&#25552;&#39640;&#22810;&#23186;&#20307;&#26816;&#32034;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#21033;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65292;&#26410;&#33021;&#28145;&#24230;&#34701;&#21512;&#22810;&#35270;&#22270;&#29305;&#24449;&#65292;&#23548;&#33268;&#26816;&#32034;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24573;&#35270;&#20102;&#20840;&#23616;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#21152;&#26435;&#21644;&#25110;&#20018;&#32852;&#26469;&#34701;&#21512;&#22810;&#35270;&#22270;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#34701;&#21512;&#26041;&#27861;&#19981;&#36275;&#20197;&#25429;&#25417;&#21508;&#31181;&#35270;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#24515;&#30456;&#20284;&#24615;&#22810;&#35270;&#22270;&#21704;&#24076;&#26041;&#27861;&#65288;CSMVH&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20013;&#24515;&#30456;&#20284;&#24615;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#23616;&#37096;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#21487;&#20197;&#21033;&#29992;&#21704;&#24076;&#20013;&#24515;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#22522;&#20110;&#38376;&#25511;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#22312;MS COCO&#21644;NUS-WIDE&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;CSMVH&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hash representation learning of multi-view heterogeneous data is the key to improving the accuracy of multimedia retrieval. However, existing methods utilize local similarity and fall short of deeply fusing the multi-view features, resulting in poor retrieval accuracy. Current methods only use local similarity to train their model. These methods ignore global similarity. Furthermore, most recent works fuse the multi-view features via a weighted sum or concatenation. We contend that these fusion methods are insufficient for capturing the interaction between various views. We present a novel Central Similarity Multi-View Hashing (CSMVH) method to address the mentioned problems. Central similarity learning is used for solving the local similarity problem, which can utilize the global similarity between the hash center and samples. We present copious empirical data demonstrating the superiority of gate-based fusion over conventional approaches. On the MS COCO and NUS-WIDE, the proposed CSM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27573;&#33853;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26032;&#39062;&#26041;&#27861;PCAS&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PCAS&#19981;&#20165;&#22312;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#32780;&#19988;&#22312;&#30830;&#23450;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#23558;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2308.13760</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#24110;&#21161;&#65311;&#25506;&#32034;&#27573;&#33853;&#21644;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#30340;&#32852;&#21512;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
How Can Context Help? Exploring Joint Retrieval of Passage and Personalized Context. (arXiv:2308.13760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27573;&#33853;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26032;&#39062;&#26041;&#27861;PCAS&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PCAS&#19981;&#20165;&#22312;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#32780;&#19988;&#22312;&#30830;&#23450;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#23558;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22806;&#37096;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;&#21040;&#20197;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21830;&#19994;&#20215;&#20540;&#65292;&#20294;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#21463;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27573;&#33853;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22810;&#20010;&#22522;&#20934;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#25628;&#32034;(Personalized Context-Aware Search&#65292;PCAS)&#65292;&#23427;&#22312;&#27573;&#33853;&#26816;&#32034;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#27969;&#34892;&#30340;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#32780;&#19988;&#22312;&#30830;&#23450;&#25152;&#26377;&#21487;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#25104;&#20026;&#28608;&#21169;&#26410;&#26469;&#30740;&#31350;&#30340;&#20652;&#21270;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of external personalized context information into document-grounded conversational systems has significant potential business value, but has not been well-studied. Motivated by the concept of personalized context-aware document-grounded conversational systems, we introduce the task of context-aware passage retrieval. We also construct a dataset specifically curated for this purpose. We describe multiple baseline systems to address this task, and propose a novel approach, Personalized Context-Aware Search (PCAS), that effectively harnesses contextual information during passage retrieval. Experimental evaluations conducted on multiple popular dense retrieval systems demonstrate that our proposed approach not only outperforms the baselines in retrieving the most relevant passage but also excels at identifying the pertinent context among all the available contexts. We envision that our contributions will serve as a catalyst for inspiring future research endeavors in this pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#65292;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13754</link><description>&lt;p&gt;
ZC3: &#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ZC3: Zero-Shot Cross-Language Code Clone Detection. (arXiv:2308.13754v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#65292;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#24341;&#20837;&#20195;&#30721;&#20811;&#38534;&#20197;&#25552;&#39640;&#32534;&#31243;&#25928;&#29575;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#22312;&#21333;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#32534;&#20889;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#31243;&#24207;&#65292;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#24179;&#21488;&#65292;&#24182;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#39033;&#30446;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#12290;&#32771;&#34385;&#21040;&#25910;&#38598;&#36328;&#35821;&#35328;&#24182;&#34892;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65289;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#65292;&#35774;&#35745;&#19968;&#31181;&#19981;&#20381;&#36182;&#20219;&#20309;&#24182;&#34892;&#25968;&#25454;&#30340;&#26377;&#25928;&#36328;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#12290;ZC3&#36890;&#36807;&#35774;&#35745;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#26469;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#12290;&#22522;&#20110;&#27492;&#65292;ZC3&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#20197;&#29983;&#25104;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC3 for Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC3 exploits domain-aware learning and cycle consistency learning to further constrain the model to generate represen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;LSTM&#27169;&#22411;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;Net&#21697;&#29260;&#22768;&#35465;&#31639;&#27861;&#35780;&#20272;&#24494;&#26381;&#21153;&#30340;&#22768;&#35465;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#32452;&#19982;Amazon Web&#24494;&#26381;&#21153;&#30456;&#20851;&#30340;&#36229;&#36807;10,000&#26465;&#35780;&#35770;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.13590</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;Web&#24494;&#26381;&#21153;&#21475;&#30865;&#35780;&#20998;&#30340;QoE&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LSTM-based QoE Evaluation for Web Microservices' Reputation Scoring. (arXiv:2308.13590v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;LSTM&#27169;&#22411;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;Net&#21697;&#29260;&#22768;&#35465;&#31639;&#27861;&#35780;&#20272;&#24494;&#26381;&#21153;&#30340;&#22768;&#35465;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#32452;&#19982;Amazon Web&#24494;&#26381;&#21153;&#30456;&#20851;&#30340;&#36229;&#36807;10,000&#26465;&#35780;&#35770;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#25366;&#25496;&#20316;&#32773;&#23545;&#29305;&#23450;&#23454;&#20307;&#30340;&#24847;&#35265;&#30340;&#20219;&#21153;&#12290;&#23427;&#20801;&#35768;&#32452;&#32455;&#23454;&#26102;&#30417;&#25511;&#19981;&#21516;&#30340;&#26381;&#21153;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#34892;&#21160;&#12290;&#22768;&#35465;&#26159;&#20154;&#20204;&#25110;&#20107;&#29289;&#36890;&#24120;&#35828;&#21040;&#25110;&#30456;&#20449;&#30340;&#19996;&#35199;&#12290;&#38750;&#27491;&#24335;&#22320;&#35828;&#65292;&#22768;&#35465;&#32508;&#21512;&#20102;&#20174;&#29992;&#25143;&#37027;&#37324;&#24471;&#21040;&#30340;&#21453;&#39304;&#12289;&#35780;&#35770;&#21644;&#35780;&#32423;&#25152;&#21453;&#26144;&#30340;&#21487;&#38752;&#24230;&#24230;&#37327;&#65292;&#36825;&#20123;&#21453;&#26144;&#20102;&#20182;&#20204;&#30340;&#20307;&#39564;&#36136;&#37327;(QoE)&#65292;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#25439;&#23475;&#25152;&#25552;&#20379;&#26381;&#21153;&#30340;&#22768;&#35465;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23545;Web&#24494;&#26381;&#21153;&#35780;&#20215;&#25351;&#26631;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#21033;&#29992;&#25552;&#20379;&#30340;&#20449;&#24687;&#26469;&#35780;&#20272;&#21644;&#35780;&#20998;&#24494;&#26381;&#21153;&#30340;&#22768;&#35465;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#21697;&#29260;&#22768;&#35465;(NBR)&#31639;&#27861;&#35780;&#20272;&#24494;&#26381;&#21153;&#30340;&#22768;&#35465;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#22312;&#19982;15&#20010;Amazon Web&#24494;&#26381;&#21153;&#30456;&#20851;&#30340;10000&#22810;&#26465;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is the task of mining the authors' opinions about specific entities. It allows organizations to monitor different services in real time and act accordingly. Reputation is what is generally said or believed about people or things. Informally, reputation combines the measure of reliability derived from feedback, reviews, and ratings gathered from users, which reflect their quality of experience (QoE) and can either increase or harm the reputation of the provided services. In this study, we propose to perform sentiment analysis on web microservices reviews to exploit the provided information to assess and score the microservices' reputation. Our proposed approach uses the Long Short-Term Memory (LSTM) model to perform sentiment analysis and the Net Brand Reputation (NBR) algorithm to assess reputation scores for microservices. This approach is tested on a set of more than 10,000 reviews related to 15 Amazon Web microservices, and the experimental results have shown that
&lt;/p&gt;</description></item><item><title>&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;(ChatGPT, BARD&#21644;GPT4)&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25552;&#21462;&#20107;&#25925;&#30456;&#20851;&#20449;&#24687;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.13563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;ChatGPT&#12289;BARD&#21644;GPT-4&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4. (arXiv:2308.13563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13563
&lt;/p&gt;
&lt;p&gt;
&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;(ChatGPT, BARD&#21644;GPT4)&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25552;&#21462;&#20107;&#25925;&#30456;&#20851;&#20449;&#24687;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#23433;&#20840;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#20998;&#26512;&#20174;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20102;&#35299;&#27969;&#34892;&#30340;LLM&#25509;&#21475;&#22312;&#20998;&#31867;&#25110;&#20174;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25509;&#21475;&#8212;&#8212;ChatGPT&#12289;BARD&#21644;GPT4&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#25552;&#21462;&#20449;&#24687;&#21644;&#22238;&#31572;&#19982;&#20107;&#25925;&#26377;&#20851;&#30340;&#26597;&#35810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#20174;&#29233;&#33655;&#21326;&#24030;&#21644;&#22570;&#33832;&#26031;&#24030;&#30340;100&#20010;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#23545;&#26597;&#35810;&#30340;&#21709;&#24212;&#12290;&#20116;&#20010;&#19982;&#21465;&#36848;&#30456;&#20851;&#30340;&#38382;&#39064;&#34987;&#25552;&#20986;&#65306;1&#65289;&#35841;&#26159;&#36131;&#20219;&#26041;&#65311;2&#65289;&#30896;&#25758;&#26041;&#24335;&#26159;&#20160;&#20040;&#65311;3&#65289;&#20107;&#25925;&#21457;&#29983;&#22312;&#24037;&#20316;&#21306;&#21527;&#65311;4&#65289;&#20107;&#25925;&#28041;&#21450;&#34892;&#20154;&#21527;&#65311;5&#65289;&#20107;&#25925;&#20013;&#26377;&#23475;&#20107;&#20214;&#30340;&#39034;&#24207;&#26159;&#20160;&#20040;&#65311;&#23545;&#20110;&#31532;1&#21040;&#31532;4&#20010;&#38382;&#39064;&#65292;&#19977;&#20010;LLM&#25509;&#21475;&#30340;&#22238;&#31572;&#37117;&#32463;&#36807;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traffic safety research, extracting information from crash narratives using text analysis is a common practice. With recent advancements of large language models (LLM), it would be useful to know how the popular LLM interfaces perform in classifying or extracting information from crash narratives. To explore this, our study has used the three most popular publicly available LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from Iowa and Kansas. During the investigation, their capabilities and limitations were assessed and their responses to the queries were compared. Five questions were asked related to the narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has the crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5) What are the sequence of harmful events in the crash? For questions 1 through 4, the o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#12289;&#27880;&#37322;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.13545</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23391;&#21152;&#25289;&#25991;&#26412;&#20998;&#31867;&#30340;&#29305;&#24449;&#25552;&#21462;&#65288;arXiv:2308.13545v1 [cs.IR]&#65289;
&lt;/p&gt;
&lt;p&gt;
Feature Extraction Using Deep Generative Models for Bangla Text Classification on a New Comprehensive Dataset. (arXiv:2308.13545v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#12289;&#27880;&#37322;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26159;&#25991;&#26412;&#25366;&#25496;&#21644;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#31532;&#20845;&#22823;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#23427;&#19968;&#30452;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#12289;&#27880;&#37322;&#21644;&#20934;&#22791;&#20102;&#19968;&#20010;&#21253;&#21547;212,184&#20010;&#23391;&#21152;&#25289;&#25991;&#26723;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65306;LSTM&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;LSTM VAE&#65289;&#12289;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;AC-GAN&#65289;&#21644;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#26469;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#24212;&#29992;&#26368;&#21021;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29616;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#36825;&#19977;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#20102;&#24471;&#21040;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The selection of features for text classification is a fundamental task in text mining and information retrieval. Despite being the sixth most widely spoken language in the world, Bangla has received little attention due to the scarcity of text datasets. In this research, we collected, annotated, and prepared a comprehensive dataset of 212,184 Bangla documents in seven different categories and made it publicly accessible. We implemented three deep learning generative models: LSTM variational autoencoder (LSTM VAE), auxiliary classifier generative adversarial network (AC-GAN), and adversarial autoencoder (AAE) to extract text features, although their applications are initially found in the field of computer vision. We utilized our dataset to train these three models and used the feature space obtained in the document classification task. We evaluated the performance of the classifiers and found that the adversarial autoencoder model produced the best feature space.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20813;&#36153;&#30340;&#23545;&#25239;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35268;&#33539;&#29992;&#25143;/&#39033;&#30446;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;SharpCF&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.13541</link><description>&lt;p&gt;
&#38754;&#21521;&#20813;&#36153;&#30340;&#23545;&#25239;&#24615;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Adversarial Collaborative Filtering for Free. (arXiv:2308.13541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20813;&#36153;&#30340;&#23545;&#25239;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35268;&#33539;&#29992;&#25143;/&#39033;&#30446;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;SharpCF&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#24050;&#25104;&#21151;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CF&#26041;&#27861;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#23545;&#25512;&#33616;&#30340;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35268;&#33539;&#29992;&#25143;/&#39033;&#30446;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26694;&#26550;&#19979;&#23398;&#20064;&#23545;&#25239;&#25200;&#21160;&#21644;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;1&#65289;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#28155;&#21152;&#25200;&#21160;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65307;2&#65289;&#35299;&#20915;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#38656;&#35201;&#32791;&#26102;&#12290;&#38500;&#20102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#27599;&#27425;&#36845;&#20195;&#36824;&#38656;&#35201;&#39069;&#22806;&#35745;&#31639;&#26469;&#26356;&#26032;&#25200;&#21160;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35268;&#27169;&#22823;&#30340;&#24037;&#19994;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Sharpness-aware Collaborative Filtering&#65288;SharpCF&#65289;&#26041;&#27861;&#65292;&#23427;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) has been successfully used to help users discover the items of interest. Nevertheless, existing CF methods suffer from noisy data issue, which negatively impacts the quality of recommendation. To tackle this problem, many prior studies leverage adversarial learning to regularize the representations of users/items, which improves both generalizability and robustness. Those methods often learn adversarial perturbations and model parameters under min-max optimization framework. However, there still have two major drawbacks: 1) Existing methods lack theoretical guarantees of why adding perturbations improve the model generalizability and robustness; 2) Solving min-max optimization is time-consuming. In addition to updating the model parameters, each iteration requires additional computations to update the perturbations, making them not scalable for industry-scale datasets.  In this paper, we present Sharpness-aware Collaborative Filtering (SharpCF), a simple ye
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28216;&#25103;&#29305;&#24449;&#24314;&#35758;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#21462;&#20027;&#39064;&#30456;&#20284;&#30340;&#28216;&#25103;&#29305;&#24449;&#24182;&#29983;&#25104;&#26032;&#29305;&#24449;&#12290;&#32463;&#36807;&#29992;&#25143;&#30740;&#31350;&#27604;&#36739;&#65292;&#35813;&#31995;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20154;&#24037;&#24314;&#35758;&#12290;&#35813;&#31995;&#32479;&#26159;&#19968;&#20010;&#19982;&#29992;&#25143;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#36827;&#34892;&#21327;&#20316;&#30340;&#28216;&#25103;&#35774;&#35745;&#21161;&#25163;&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.13538</link><description>&lt;p&gt;
&#19968;&#20010;&#27010;&#24565;&#28216;&#25103;&#29305;&#24449;&#29983;&#25104;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Study on a Conceptual Game Feature Generation and Recommendation System. (arXiv:2308.13538v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28216;&#25103;&#29305;&#24449;&#24314;&#35758;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#21462;&#20027;&#39064;&#30456;&#20284;&#30340;&#28216;&#25103;&#29305;&#24449;&#24182;&#29983;&#25104;&#26032;&#29305;&#24449;&#12290;&#32463;&#36807;&#29992;&#25143;&#30740;&#31350;&#27604;&#36739;&#65292;&#35813;&#31995;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20154;&#24037;&#24314;&#35758;&#12290;&#35813;&#31995;&#32479;&#26159;&#19968;&#20010;&#19982;&#29992;&#25143;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#36827;&#34892;&#21327;&#20316;&#30340;&#28216;&#25103;&#35774;&#35745;&#21161;&#25163;&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#28216;&#25103;&#29305;&#24449;&#24314;&#35758;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#30340;GLoVe&#27169;&#22411;&#30340;&#35789;&#23884;&#20837;&#26469;&#25552;&#21462;&#20027;&#39064;&#30456;&#20284;&#30340;&#28216;&#25103;&#20013;&#30340;&#29305;&#24449;&#21644;&#23454;&#20307;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#20256;&#36882;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#25143;&#25552;&#31034;&#30340;&#26032;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30701;&#26399;&#29992;&#25143;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#26469;&#33258;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-2&#27169;&#22411;&#12289;&#20351;&#29992;ConceptNet&#30340;&#27169;&#22411;&#20197;&#21450;&#20154;&#24037;&#32534;&#20889;&#30340;&#28216;&#25103;&#29305;&#24449;&#29983;&#25104;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;&#20154;&#24037;&#24314;&#35758;&#33719;&#24471;&#20102;&#32477;&#22823;&#22810;&#25968;&#30340;&#25237;&#31080;&#65292;&#20294;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#65292;GPT-2&#27169;&#22411;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20154;&#24037;&#24314;&#35758;&#12290;&#35813;&#31995;&#32479;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#28216;&#25103;&#35774;&#35745;&#21161;&#25163;&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#65292;&#33021;&#22815;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#19982;&#29992;&#25143;&#36827;&#34892;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a system used to generate game feature suggestions based on a text prompt. Trained on the game descriptions of almost 60k games, it uses the word embeddings of a small GLoVe model to extract features and entities found in thematically similar games which are then passed through a generator model to generate new features for a user's prompt. We perform a short user study comparing the features generated from a fine-tuned GPT-2 model, a model using the ConceptNet, and human-authored game features. Although human suggestions won the overall majority of votes, the GPT-2 model outperformed the human suggestions in certain games. This system is part of a larger game design assistant tool that is able to collaborate with users at a conceptual level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STEM&#30340;&#26032;&#33539;&#20363;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#36127;&#20256;&#36882;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;STEM&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#20013;&#27491;&#21453;&#39304;&#25968;&#37327;&#30340;&#30456;&#23545;&#27604;&#20363;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13537</link><description>&lt;p&gt;
STEM:&#37322;&#25918;Embedding&#22312;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
STEM: Unleashing the Power of Embeddings for Multi-task Recommendation. (arXiv:2308.13537v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STEM&#30340;&#26032;&#33539;&#20363;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#36127;&#20256;&#36882;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;STEM&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#20013;&#27491;&#21453;&#39304;&#25968;&#37327;&#30340;&#30456;&#23545;&#27604;&#20363;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#12290;MTL&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36127;&#20256;&#36882;&#30340;&#21457;&#29983;&#65292;&#21363;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#20914;&#31361;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#19968;&#20010;&#25972;&#20307;&#26469;&#25506;&#32034;&#36127;&#20256;&#36882;&#65292;&#24573;&#35270;&#20102;&#20854;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26681;&#25454;&#20219;&#21153;&#20043;&#38388;&#27491;&#21453;&#39304;&#30340;&#30456;&#23545;&#25968;&#37327;&#23558;&#26679;&#26412;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#29616;&#26377;MTL&#26041;&#27861;&#22312;&#25910;&#21040;&#21508;&#20219;&#21153;&#31867;&#20284;&#21453;&#39304;&#30340;&#26679;&#26412;&#19978;&#20173;&#28982;&#23384;&#22312;&#36127;&#20256;&#36882;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20849;&#20139;&#23884;&#20837;&#30340;&#33539;&#20363;&#65292;&#24182;&#19988;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#30340;&#22833;&#36133;&#21487;&#20197;&#24402;&#22240;&#20110;&#20351;&#29992;&#36825;&#31181;&#36890;&#29992;&#23884;&#20837;&#26469;&#24314;&#27169;&#19981;&#21516;&#29992;&#25143;&#20559;&#22909;&#30340;&#26377;&#38480;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has gained significant popularity in recommendation systems as it enables the simultaneous optimization of multiple objectives. A key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks. Existing research has explored negative transfer by treating all samples as a whole, overlooking the inherent complexities within them. To this end, we delve into the intricacies of samples by splitting them based on the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. It is worth noting that existing methods commonly employ a shared-embedding paradigm, and we hypothesize that their failure can be attributed to the limited capacity of modeling diverse user preferences across tasks using such universal embeddings.  In this paper, we introduce a novel paradigm called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#24335;&#30340;ZCA&#30333;&#21270;&#20316;&#29992;&#65292;&#20197;&#21450;&#20854;&#23545;&#20302;&#32500;&#29289;&#21697;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13536</link><description>&lt;p&gt;
&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38544;&#24335;ZCA&#30333;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implicit ZCA Whitening Effects of Linear Autoencoders for Recommendation. (arXiv:2308.13536v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#24335;&#30340;ZCA&#30333;&#21270;&#20316;&#29992;&#65292;&#20197;&#21450;&#20854;&#23545;&#20302;&#32500;&#29289;&#21697;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#32447;&#24615;&#22238;&#24402;&#65288;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#27169;&#22411;&#34987;&#30740;&#31350;&#20316;&#20026;&#23398;&#20064;&#29289;&#21697;&#30456;&#20284;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#19982;&#25512;&#33616;&#25968;&#25454;&#30340;ZCA&#30333;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#23545;&#20598;&#24418;&#24335;&#35299;&#22312;&#29289;&#21697;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#23454;&#38469;&#19978;&#20855;&#26377;ZCA&#30333;&#21270;&#25928;&#26524;&#65292;&#32780;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;/&#22238;&#24402;&#27169;&#22411;&#30340;&#21407;&#22987;&#38382;&#39064;&#20013;&#65292;&#29289;&#21697;&#34987;&#35270;&#20026;&#36755;&#20837;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23558;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#20351;&#29992;Item2vec&#31561;&#23884;&#20837;&#26041;&#27861;&#33719;&#24471;&#30340;&#20302;&#32500;&#29289;&#21697;&#21521;&#37327;&#26469;&#20272;&#35745;&#29289;&#21697;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#30333;&#21270;&#20302;&#32500;&#29289;&#21697;&#23884;&#20837;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, in the field of recommendation systems, linear regression (autoencoder) models have been investigated as a way to learn item similarity. In this paper, we show a connection between a linear autoencoder model and ZCA whitening for recommendation data. In particular, we show that the dual form solution of a linear autoencoder model actually has ZCA whitening effects on feature vectors of items, while items are considered as input features in the primal problem of the autoencoder/regression model. We also show the correctness of applying a linear autoencoder to low-dimensional item vectors obtained using embedding methods such as Item2vec to estimate item-item similarities. Our experiments provide preliminary results indicating the effectiveness of whitening low-dimensional item embeddings.
&lt;/p&gt;</description></item><item><title>MUSE&#26159;&#19968;&#31181;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#22686;&#24378;&#38543;&#26426;&#25773;&#25918;&#25512;&#33616;&#23454;&#29616;&#20102;&#23545;&#38899;&#20048;&#39046;&#22495;&#29420;&#29305;&#25361;&#25112;&#30340;&#35299;&#20915;&#12290;&#23427;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#21019;&#26032;&#30340;&#20250;&#35805;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#25972;&#20307;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.09649</link><description>&lt;p&gt;
MUSE: &#22312;&#38543;&#26426;&#25773;&#25918;&#25512;&#33616;&#22686;&#24378;&#26041;&#38754;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MUSE: Music Recommender System with Shuffle Play Recommendation Enhancement. (arXiv:2308.09649v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09649
&lt;/p&gt;
&lt;p&gt;
MUSE&#26159;&#19968;&#31181;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#22686;&#24378;&#38543;&#26426;&#25773;&#25918;&#25512;&#33616;&#23454;&#29616;&#20102;&#23545;&#38899;&#20048;&#39046;&#22495;&#29420;&#29305;&#25361;&#25112;&#30340;&#35299;&#20915;&#12290;&#23427;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#21019;&#26032;&#30340;&#20250;&#35805;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#25972;&#20307;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#25773;&#25918;&#21015;&#34920;&#21644;&#20419;&#36827;&#20598;&#28982;&#21457;&#29616;&#26032;&#38899;&#20048;&#26469;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#24573;&#35270;&#20102;&#38899;&#20048;&#39046;&#22495;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#25773;&#25918;&#20250;&#20197;&#38543;&#26426;&#24207;&#21015;&#25552;&#20379;&#36830;&#32493;&#30340;&#26354;&#30446;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#38543;&#26426;&#25773;&#25918;&#20250;&#35805;&#20027;&#35201;&#30001;&#20110;&#39640;&#29420;&#29305;&#36716;&#25442;&#29575;&#38459;&#30861;&#20102;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#30340;&#25972;&#20307;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22686;&#24378;&#20102;&#38543;&#26426;&#25773;&#25918;&#25512;&#33616;&#12290; MUSE&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#26354;&#30446;&#30340;&#36716;&#25442;&#26469;&#26368;&#22823;&#21270;&#21407;&#22987;&#25773;&#25918;&#20250;&#35805;&#19982;&#22686;&#24378;&#20250;&#35805;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#31181;&#22686;&#24378;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#20250;&#35805;&#22686;&#24378;&#26041;&#27861;&#65288;&#31216;&#20026;&#22522;&#20110;&#36716;&#25442;&#30340;&#22686;&#24378;&#65289;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#20004;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#32454;&#31890;&#24230;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become indispensable in music streaming services, enhancing user experiences by personalizing playlists and facilitating the serendipitous discovery of new music. However, the existing recommender systems overlook the unique challenges inherent in the music domain, specifically shuffle play, which provides subsequent tracks in a random sequence. Based on our observation that the shuffle play sessions hinder the overall training process of music recommender systems mainly due to the high unique transition rates of shuffle play sessions, we propose a Music Recommender System with Shuffle Play Recommendation Enhancement (MUSE). MUSE employs the self-supervised learning framework that maximizes the agreement between the original session and the augmented session, which is augmented by our novel session augmentation method, called transition-based augmentation. To further facilitate the alignment of the representations between the two views, we devise two fine-grain
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35780;&#20272;&#20302;&#25104;&#26412;&#35009;&#20915;&#26041;&#27861;&#22312;&#20445;&#25345;&#23436;&#25972;&#38598;&#21512;&#20013;&#31995;&#32479;&#20043;&#38388;&#25104;&#23545;&#26174;&#33879;&#24046;&#24322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09340</link><description>&lt;p&gt;
&#20320;&#30340;Qrels&#26377;&#22810;&#20855;&#26377;&#21306;&#20998;&#24615;&#65311;&#22914;&#20309;&#30740;&#31350;&#25991;&#26723;&#35009;&#20915;&#26041;&#27861;&#30340;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Discriminative Are Your Qrels? How To Study the Statistical Significance of Document Adjudication Methods. (arXiv:2308.09340v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35780;&#20272;&#20302;&#25104;&#26412;&#35009;&#20915;&#26041;&#27861;&#22312;&#20445;&#25345;&#23436;&#25972;&#38598;&#21512;&#20013;&#31995;&#32479;&#20043;&#38388;&#25104;&#23545;&#26174;&#33879;&#24046;&#24322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#31163;&#32447;&#26816;&#32034;&#35780;&#20272;&#30340;&#27979;&#35797;&#38598;&#38656;&#35201;&#20154;&#24037;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#20943;&#23569;&#35780;&#20272;&#25104;&#26412;&#26041;&#38754;&#65292;&#35009;&#20915;&#26041;&#27861;&#31215;&#26497;&#20915;&#23450;&#19987;&#23478;&#23457;&#26597;&#25991;&#26723;&#30340;&#39034;&#24207;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35780;&#20272;&#39044;&#31639;&#25110;&#38477;&#20302;&#20854;&#25104;&#26412;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#27979;&#37327;&#23436;&#25972;&#38598;&#21512;&#19979;&#24050;&#30693;&#31995;&#32479;&#30340;&#37329;&#26631;&#25490;&#21517;&#19982;&#20302;&#25104;&#26412;&#38598;&#21512;&#19979;&#31995;&#32479;&#30340;&#35266;&#23519;&#25490;&#21517;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#20256;&#32479;&#20998;&#26512;&#24573;&#30053;&#20102;&#20302;&#25104;&#26412;&#35009;&#20915;&#23545;&#20110;&#23436;&#25972;&#38598;&#21512;&#20013;&#31995;&#32479;&#20043;&#38388;&#30340;&#32479;&#35745;&#26174;&#33879;&#24046;&#24322;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#20302;&#25104;&#26412;&#35009;&#20915;&#26041;&#27861;&#22312;&#20445;&#25345;&#23436;&#25972;&#38598;&#21512;&#20013;&#31995;&#32479;&#20043;&#38388;&#25104;&#23545;&#26174;&#33879;&#24046;&#24322;&#26041;&#38754;&#30340;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating test collections for offline retrieval evaluation requires human effort to judge documents' relevance. This expensive activity motivated much work in developing methods for constructing benchmarks with fewer assessment costs. In this respect, adjudication methods actively decide both which documents and the order in which experts review them, in order to better exploit the assessment budget or to lower it. Researchers evaluate the quality of those methods by measuring the correlation between the known gold ranking of systems under the full collection and the observed ranking of systems under the lower-cost one. This traditional analysis ignores whether and how the low-cost judgements impact on the statistically significant differences among systems with respect to the full collection. We fill this void by proposing a novel methodology to evaluate how the low-cost adjudication methods preserve the pairwise significant differences between systems as the full collection. In other
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Meituan&#25628;&#32034;&#20013;&#36827;&#34892;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.07711</link><description>&lt;p&gt;
SPM: Meituan&#25628;&#32034;&#20013;&#29992;&#20110;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#32467;&#26500;&#21270;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search. (arXiv:2308.07711v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Meituan&#25628;&#32034;&#20013;&#36827;&#34892;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21830;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#28385;&#36275;&#29992;&#25143;&#20307;&#39564;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#19982;&#20256;&#32479;&#30340;&#30005;&#21830;&#24179;&#21488;&#19981;&#21516;&#65292;&#29992;&#25143;&#22312;&#32654;&#22242;&#31561;&#29983;&#27963;&#26381;&#21153;&#24179;&#21488;&#19978;&#36827;&#34892;&#25628;&#32034;&#20027;&#35201;&#26159;&#20026;&#20102;&#20135;&#21697;&#20379;&#24212;&#21830;&#65292;&#36825;&#20123;&#20379;&#24212;&#21830;&#36890;&#24120;&#25317;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#21517;&#31216;&#12289;&#22320;&#22336;&#12289;&#31867;&#21035;&#12289;&#25104;&#21315;&#19978;&#19975;&#30340;&#20135;&#21697;&#12290;&#20351;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20869;&#23481;&#36827;&#34892;&#25628;&#32034;&#30456;&#20851;&#24615;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21516;&#23383;&#27573;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#23384;&#22312;&#35821;&#35328;&#20998;&#24067;&#24046;&#24322;&#65292;&#26080;&#27861;&#30452;&#25509;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65288;&#22914;BERT&#65289;&#12290;&#65288;2&#65289;&#19981;&#21516;&#23383;&#27573;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#19988;&#38271;&#24230;&#24046;&#24322;&#24456;&#22823;&#65292;&#24456;&#38590;&#25552;&#21462;&#23545;&#30456;&#20851;&#24615;&#21305;&#37197;&#26377;&#24110;&#21161;&#30340;&#25991;&#26723;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#65292;&#29992;&#20110;&#20016;&#23500;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching.  To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structure
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26631;&#35760;&#26694;&#26550;&#65292;&#21033;&#29992;&#35266;&#30475;&#26102;&#38388;&#21453;&#39304;&#36827;&#34892;&#30701;&#35270;&#39057;&#25512;&#33616;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#35821;&#20041;&#30340;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#20998;&#20301;&#25968;&#26469;&#25552;&#21462;&#35266;&#30475;&#26102;&#38388;&#30340;&#20449;&#24687;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#21152;&#23481;&#26131;&#65292;&#21516;&#26102;&#20943;&#23569;&#20559;&#35265;&#23545;&#25512;&#33616;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17426</link><description>&lt;p&gt;
&#21033;&#29992;&#35266;&#30475;&#26102;&#38388;&#21453;&#39304;&#36827;&#34892;&#30701;&#35270;&#39057;&#25512;&#33616;&#65306;&#19968;&#31181;&#22240;&#26524;&#26631;&#35760;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Leveraging Watch-time Feedback for Short-Video Recommendations: A Causal Labeling Framework. (arXiv:2306.17426v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26631;&#35760;&#26694;&#26550;&#65292;&#21033;&#29992;&#35266;&#30475;&#26102;&#38388;&#21453;&#39304;&#36827;&#34892;&#30701;&#35270;&#39057;&#25512;&#33616;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#35821;&#20041;&#30340;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#20998;&#20301;&#25968;&#26469;&#25552;&#21462;&#35266;&#30475;&#26102;&#38388;&#30340;&#20449;&#24687;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#21152;&#23481;&#26131;&#65292;&#21516;&#26102;&#20943;&#23569;&#20559;&#35265;&#23545;&#25512;&#33616;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30701;&#35270;&#39057;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#30701;&#35270;&#39057;&#25512;&#33616;&#30340;&#37325;&#35201;&#24615;&#22823;&#22823;&#22686;&#21152;&#12290;&#19982;&#20854;&#20182;&#25512;&#33616;&#22330;&#26223;&#19981;&#21516;&#65292;&#30701;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#22823;&#37327;&#20381;&#36182;&#20110;&#35266;&#30475;&#26102;&#38388;&#30340;&#21453;&#39304;&#12290;&#29616;&#26377;&#26041;&#27861;&#31616;&#21333;&#22320;&#23558;&#35266;&#30475;&#26102;&#38388;&#35270;&#20026;&#30452;&#25509;&#26631;&#31614;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#24191;&#27867;&#30340;&#35821;&#20041;&#24182;&#24341;&#20837;&#20559;&#35265;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22522;&#20110;&#35266;&#30475;&#26102;&#38388;&#24314;&#27169;&#29992;&#25143;&#20852;&#36259;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21435;&#20559;&#22810;&#35821;&#20041;&#25552;&#21462;&#26631;&#35760;&#65288;DML&#65289;&#30340;&#26694;&#26550;&#12290;DML&#21033;&#29992;&#35266;&#30475;&#26102;&#38388;&#20998;&#24067;&#24471;&#20986;&#30340;&#20998;&#20301;&#25968;&#26500;&#24314;&#21253;&#21547;&#21508;&#31181;&#35821;&#20041;&#30340;&#26631;&#31614;&#65292;&#20248;&#20808;&#32771;&#34385;&#30456;&#23545;&#39034;&#24207;&#32780;&#19981;&#26159;&#32477;&#23545;&#26631;&#31614;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#20415;&#20110;&#27169;&#22411;&#23398;&#20064;&#65292;&#21516;&#26102;&#31526;&#21512;&#25512;&#33616;&#30340;&#25490;&#24207;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#22240;&#26524;&#35843;&#25972;&#21551;&#21457;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#26631;&#31614;&#23450;&#20041;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#23545;&#25512;&#33616;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of short video applications, the significance of short video recommendations has vastly increased. Unlike other recommendation scenarios, short video recommendation systems heavily rely on feedback from watch time. Existing approaches simply treat watch time as a direct label, failing to effectively harness its extensive semantics and introduce bias, thereby limiting the potential for modeling user interests based on watch time. To overcome this challenge, we propose a framework named Debiasied Multiple-semantics-extracting Labeling (DML). DML constructs labels that encompass various semantics by utilizing quantiles derived from the distribution of watch time, prioritizing relative order rather than absolute label values. This approach facilitates easier model learning while aligning with the ranking objective of recommendations. Furthermore, we introduce a method inspired by causal adjustment to refine label definitions, thereby reducing the impact of bias on th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20915;&#31574;Transformer&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25506;&#32034;&#29992;&#25143;&#34892;&#20026;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25351;&#23548;&#20195;&#29702;&#25429;&#25417;&#21160;&#24577;&#20852;&#36259;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#24773;&#22659;&#19979;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07920</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22240;&#26524;&#20915;&#31574;Transformer&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning. (arXiv:2304.07920v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20915;&#31574;Transformer&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25506;&#32034;&#29992;&#25143;&#34892;&#20026;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25351;&#23548;&#20195;&#29702;&#25429;&#25417;&#21160;&#24577;&#20852;&#36259;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#24773;&#22659;&#19979;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#25512;&#33616;&#31574;&#30053;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#35774;&#35745;&#24448;&#24448;&#24182;&#19981;&#31616;&#21333;&#12290;&#25506;&#32034;&#29992;&#25143;&#34892;&#20026;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#25351;&#23548;&#20195;&#29702;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20852;&#36259;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20223;&#30495;&#29615;&#22659;&#30340;&#20856;&#22411;&#38480;&#21046;&#65288;&#22914;&#25968;&#25454;&#25928;&#29575;&#65289;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#26080;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#24773;&#22659;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#20223;&#30495;&#22120;&#65292;&#20294;&#25968;&#25454;&#25928;&#29575;&#20351;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#26356;&#21152;&#32531;&#24930;&#12290;&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#23398;&#20064;&#65292;&#23427;&#26080;&#27861;&#22312;&#21333;&#27425;&#20132;&#20114;&#36807;&#31243;&#20013;&#25910;&#38598;&#36275;&#22815;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#20687;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37027;&#26679;&#20855;&#22791;&#30452;&#25509;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#30340;&#29282;&#22266;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-based recommender systems have recently gained popularity. However, the design of the reward function, on which the agent relies to optimize its recommendation policy, is often not straightforward. Exploring the causality underlying users' behavior can take the place of the reward function in guiding the agent to capture the dynamic interests of users. Moreover, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in large-scale situations. Although some works attempt to convert the offline dataset into a simulator, data inefficiency makes the learning process even slower. Because of the nature of reinforcement learning (i.e., learning by interaction), it cannot collect enough data to train during a single interaction. Furthermore, traditional reinforcement learning algorithms do not have a solid capability like supervised learning methods to learn from offline datasets directly. In this p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32654;&#22269;&#22269;&#20250;&#26696;&#20363;&#30740;&#31350;&#20013;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#31526;&#21495;&#32593;&#32476;&#26497;&#21270;&#21644;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#29305;&#24615;&#65292;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#12289;&#23545;&#20219;&#24847;&#23567;&#30340;&#23376;&#22270;&#36827;&#34892;&#25968;&#37327;&#21270;&#21644;&#35270;&#35273;&#35780;&#20272;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2209.00676</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#20855;&#26377;&#31526;&#21495;&#32593;&#32476;&#30340;&#26497;&#21270;&#21644;&#24179;&#34913;&#65306;&#32654;&#22269;&#22269;&#20250;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analyzing and visualizing polarization and balance with signed networks: the U.S. Congress case study. (arXiv:2209.00676v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32654;&#22269;&#22269;&#20250;&#26696;&#20363;&#30740;&#31350;&#20013;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#31526;&#21495;&#32593;&#32476;&#26497;&#21270;&#21644;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#29305;&#24615;&#65292;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#12289;&#23545;&#20219;&#24847;&#23567;&#30340;&#23376;&#22270;&#36827;&#34892;&#25968;&#37327;&#21270;&#21644;&#35270;&#35273;&#35780;&#20272;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#32593;&#32476;&#21644;&#24179;&#34913;&#29702;&#35770;&#20026;&#23637;&#31034;&#26497;&#21270;&#21160;&#24577;&#12289;&#27491;&#36127;&#20851;&#31995;&#21644;&#25919;&#27835;&#20826;&#27966;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#29615;&#22659;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#30740;&#31350;&#20102;&#32654;&#22269;&#22269;&#20250;&#20004;&#20010;&#35758;&#38498;&#25237;&#31080;&#30340;&#26085;&#30410;&#26497;&#21270;&#65292;&#20174;&#20108;&#25112;&#20197;&#26469;&#12290;&#20026;&#20102;&#23545;&#36825;&#20010;&#29305;&#23450;&#26696;&#20363;&#30740;&#31350;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#30340;&#24212;&#29992;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#31526;&#21495;&#22270;&#30340;&#37197;&#32622;&#65292;&#22522;&#20110;&#23545;&#24212;&#30340;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#35889;&#29305;&#24615;&#30340;&#21033;&#29992;&#12290;&#25972;&#20307;&#26041;&#27861;&#19982;&#22522;&#20110;&#25387;&#36133;&#25351;&#25968;&#30340;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#65292;&#20294;&#33267;&#23569;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#38656;&#35201;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65307;&#20854;&#27425;&#65292;&#23427;&#20801;&#35768;&#23545;&#20219;&#24847;&#23567;&#30340;&#23376;&#22270;&#65288;&#29978;&#33267;&#21333;&#20010;&#33410;&#28857;&#65289;&#23545;&#32593;&#32476;&#30340;&#25972;&#20307;&#24179;&#34913;&#65288;&#25110;&#19981;&#24179;&#34913;&#65289;&#20135;&#29983;&#25968;&#37327;&#21270;&#21644;&#35270;&#35273;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#20801;&#35768;&#23545;&#26497;&#21270;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed networks and balance theory provide a natural setting for real-world scenarios that show polarization dynamics, positive/negative relationships, and political partisanship. For example, they have been proven effective in studying the increasing polarization of the votes in the two chambers of the U.S. Congress from World War II on.  To provide further insights into this particular case study, we propose the application of a pipeline to analyze and visualize a signed graph's configuration based on the exploitation of the corresponding Laplacian matrix' spectral properties. The overall methodology is comparable with others based on the frustration index, but it has at least two main advantages: first, it requires a much lower computational cost; second, it allows for a quantitative and visual assessment of how arbitrarily small subgraphs (even single nodes) contribute to the overall balance (or unbalance) of the network.  The proposed pipeline allows the exploration of polarizatio
&lt;/p&gt;</description></item></channel></rss>