<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Ducho&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#30028;&#38754;&#26469;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.17125</link><description>&lt;p&gt;
Ducho: &#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ducho: A Unified Framework for the Extraction of Multimodal Features in Recommendation. (arXiv:2306.17125v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17125
&lt;/p&gt;
&lt;p&gt;
Ducho&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#30028;&#38754;&#26469;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26377;&#24847;&#20041;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#25552;&#21462;&#26159;&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#22522;&#30784;&#12290;&#36890;&#24120;&#65292;&#27599;&#20010;&#25512;&#33616;&#26694;&#26550;&#37117;&#20250;&#20351;&#29992;&#29305;&#23450;&#30340;&#31574;&#30053;&#21644;&#24037;&#20855;&#26469;&#23454;&#29616;&#20854;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#25552;&#21462;&#36807;&#31243;&#12290;&#36825;&#31181;&#38480;&#21046;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#65288;&#19968;&#65289;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#31574;&#30053;&#19981;&#21033;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#26694;&#26550;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#65292;&#22240;&#27492;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#65307;&#65288;&#20108;&#65289;&#30001;&#20110;&#19981;&#21516;&#30340;&#24320;&#28304;&#24037;&#20855;&#25552;&#20379;&#20102;&#22823;&#37327;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#27169;&#22411;&#35774;&#35745;&#32773;&#26080;&#27861;&#35775;&#38382;&#20849;&#20139;&#30028;&#38754;&#26469;&#25552;&#21462;&#29305;&#24449;&#12290;&#22312;&#19978;&#36848;&#38382;&#39064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ducho&#65292;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#36890;&#36807;&#38598;&#25104;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#21363;TensorFlow&#12289;PyTorch&#21644;Transformers&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#30028;&#38754;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#65292;&#27599;&#20010;&#21518;&#31471;&#37117;&#26377;&#33258;&#24049;&#30340;&#29305;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multimodal-aware recommendation, the extraction of meaningful multimodal features is at the basis of high-quality recommendations. Generally, each recommendation framework implements its multimodal extraction procedures with specific strategies and tools. This is limiting for two reasons: (i) different extraction strategies do not ease the interdependence among multimodal recommendation frameworks; thus, they cannot be efficiently and fairly compared; (ii) given the large plethora of pre-trained deep learning models made available by different open source tools, model designers do not have access to shared interfaces to extract features. Motivated by the outlined aspects, we propose Ducho, a unified framework for the extraction of multimodal features in recommendation. By integrating three widely-adopted deep learning libraries as backends, namely, TensorFlow, PyTorch, and Transformers, we provide a shared interface to extract and process features where each backend's specific metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26597;&#35810;&#25193;&#23637;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#37325;&#26032;&#25490;&#24207;&#20877;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#65292;&#20197;&#21450;&#24341;&#20837;&#26032;&#30340;&#25193;&#23637;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#32452;&#20214;&#26469;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;NDCG&#12289;MAP&#21644;R@1000&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17082</link><description>&lt;p&gt;
&#37325;&#26032;&#25490;&#24207;-&#25193;&#23637;-&#37325;&#22797;&#65306;&#20351;&#29992;&#21333;&#35789;&#21644;&#23454;&#20307;&#30340;&#33258;&#36866;&#24212;&#26597;&#35810;&#25193;&#23637;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Re-Rank - Expand - Repeat: Adaptive Query Expansion for Document Retrieval Using Words and Entities. (arXiv:2306.17082v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26597;&#35810;&#25193;&#23637;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#37325;&#26032;&#25490;&#24207;&#20877;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#65292;&#20197;&#21450;&#24341;&#20837;&#26032;&#30340;&#25193;&#23637;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#32452;&#20214;&#26469;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;NDCG&#12289;MAP&#21644;R@1000&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;(PRf)&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#26597;&#35810;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22312;&#31532;&#19968;&#27425;&#26816;&#32034;&#20013;&#30340;&#20302;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(NLMs)&#30340;&#36827;&#23637;&#21487;&#20197;&#23558;&#30456;&#20851;&#25991;&#26723;&#37325;&#26032;&#25490;&#21517;&#21040;&#21069;&#20960;&#21517;&#65292;&#21363;&#20351;&#37325;&#26032;&#25490;&#21517;&#27744;&#20013;&#21482;&#26377;&#24456;&#23569;&#30340;&#25991;&#26723;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;&#26597;&#35810;&#25193;&#23637;&#20043;&#21069;&#24212;&#29992;&#37325;&#26032;&#25490;&#24207;&#24182;&#37325;&#26032;&#25191;&#34892;&#26597;&#35810;&#26469;&#35299;&#20915;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20165;&#20973;&#36825;&#20010;&#25913;&#21464;&#23601;&#21487;&#20197;&#23558;&#31232;&#30095;&#21644;&#23494;&#38598;PRF&#26041;&#27861;&#30340;&#26816;&#32034;&#25928;&#26524;&#25552;&#21319;5-8%&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#23637;&#27169;&#22411;&#65292;&#28508;&#22312;&#23454;&#20307;&#25193;&#23637;(LEE)&#65292;&#23427;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#35789;&#21644;&#23454;&#20307;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#21253;&#25324;&#23616;&#37096;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#33258;&#36866;&#24212;&#8221;&#32452;&#20214;&#65292;&#23427;&#22312;&#35780;&#20998;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#23637;&#27169;&#22411;&#36845;&#20195;&#22320;&#35843;&#25972;&#37325;&#26032;&#25490;&#21517;&#27744;&#65292;&#21363;&#8220;&#37325;&#26032;&#25490;&#24207;-&#25193;&#23637;-&#37325;&#22797;&#8221;&#12290;&#20351;&#29992;LEE&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;(&#25454;&#25105;&#20204;&#25152;&#30693;)&#26368;&#22909;&#30340;NDCG&#12289;MAP&#21644;R@1000&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse and dense pseudo-relevance feedback (PRF) approaches perform poorly on challenging queries due to low precision in first-pass retrieval. However, recent advances in neural language models (NLMs) can re-rank relevant documents to top ranks, even when few are in the re-ranking pool. This paper first addresses the problem of poor pseudo-relevance feedback by simply applying re-ranking prior to query expansion and re-executing this query. We find that this change alone can improve the retrieval effectiveness of sparse and dense PRF approaches by 5-8%. Going further, we propose a new expansion model, Latent Entity Expansion (LEE), a fine-grained word and entity-based relevance modelling incorporating localized features. Finally, we include an "adaptive" component to the retrieval process, which iteratively refines the re-ranking pool during scoring using the expansion model, i.e. we "re-rank - expand repeat". Using LEE, we achieve (to our knowledge) the best NDCG, MAP and R@1000 re
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#39640;&#36798;97%&#12290;&#36825;&#34920;&#26126;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#37325;&#35201;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#36825;&#19968;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16891</link><description>&lt;p&gt;
&#21033;&#29992;Hugging Face Transformers&#39044;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Hugging Face Transformers for Predicting Mental Health Disorders in Social Networks. (arXiv:2306.16891v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#39640;&#36798;97%&#12290;&#36825;&#34920;&#26126;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#37325;&#35201;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#31934;&#31070;&#38556;&#30861;&#24182;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#20419;&#36827;&#39044;&#38450;&#20005;&#37325;&#20260;&#23475;&#21644;&#25913;&#21892;&#27835;&#30103;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#22914;&#20309;&#29992;&#20110;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;Hugging Face&#30340;&#22235;&#31181;&#19981;&#21516;BERT&#27169;&#22411;&#21644;&#36817;&#26399;&#25991;&#29486;&#20013;&#29992;&#20110;&#33258;&#21160;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;97%&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#34917;&#20805;&#20808;&#21069;&#30340;&#21457;&#29616;&#65292;&#23545;&#32467;&#26524;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#24494;&#23567;&#30340;&#25968;&#25454;&#37327;&#65288;&#22914;&#29992;&#25143;&#30340;&#20010;&#20154;&#31616;&#20171;&#25551;&#36848;&#65289;&#20063;&#26377;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#26497;&#22909;&#30340;&#26469;&#28304;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#33258;&#21160;&#21270;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of mental disorders and intervention can facilitate the prevention of severe injuries and the improvement of treatment results. Using social media and pre-trained language models, this study explores how user-generated data can be used to predict mental disorder symptoms. Our study compares four different BERT models of Hugging Face with standard machine learning techniques used in automatic depression diagnosis in recent literature. The results show that new models outperform the previous approach with an accuracy rate of up to 97%. Analyzing the results while complementing past findings, we find that even tiny amounts of data (like users' bio descriptions) have the potential to predict mental disorders. We conclude that social media data is an excellent source of mental health screening, and pre-trained models can effectively automate this critical task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#22312;&#35821;&#27861;&#21387;&#32553;&#25991;&#26412;&#20013;&#35745;&#31639;&#20840;&#23545;&#20840;&#26368;&#22823;&#31934;&#30830;&#21305;&#37197;&#65288;MEM&#65289;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23436;&#20840;&#24179;&#34913;&#30340;&#35821;&#27861;&#65292;&#28385;&#36275;&#29305;&#23450;&#23646;&#24615;&#65292;&#24182;&#20462;&#25913;&#35821;&#27861;&#35268;&#21017;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;MEMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.16815</link><description>&lt;p&gt;
&#22312;&#35821;&#27861;&#21387;&#32553;&#25991;&#26412;&#20013;&#35745;&#31639;&#20840;&#23545;&#20840;&#26368;&#22823;&#31934;&#30830;&#21305;&#37197;&#65288;MEM&#65289;
&lt;/p&gt;
&lt;p&gt;
Computing all-vs-all MEMs in grammar-compressed text. (arXiv:2306.16815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#22312;&#35821;&#27861;&#21387;&#32553;&#25991;&#26412;&#20013;&#35745;&#31639;&#20840;&#23545;&#20840;&#26368;&#22823;&#31934;&#30830;&#21305;&#37197;&#65288;MEM&#65289;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23436;&#20840;&#24179;&#34913;&#30340;&#35821;&#27861;&#65292;&#28385;&#36275;&#29305;&#23450;&#23646;&#24615;&#65292;&#24182;&#20462;&#25913;&#35821;&#27861;&#35268;&#21017;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;MEMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#37325;&#22797;&#38598;&#21512;$\mathcal{T}$&#20013;&#23383;&#31526;&#20018;&#20043;&#38388;&#30340;&#20840;&#23545;&#20840;&#26368;&#22823;&#31934;&#30830;&#21305;&#37197;&#65288;MEM&#65289;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#20174;$\mathcal{T}$&#26500;&#24314;&#19968;&#20010;&#23436;&#20840;&#24179;&#34913;&#30340;&#35821;&#27861;$\mathcal{G}$&#65292;&#28385;&#36275;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;fix-free&#8221;&#30340;&#23646;&#24615;&#65306;&#35299;&#26512;&#26641;&#20013;&#20855;&#26377;&#30456;&#21516;&#39640;&#24230;&#30340;&#38750;&#32456;&#32467;&#31526;&#30340;&#25193;&#23637;&#24418;&#25104;&#20102;&#19968;&#20010;fix-free&#38598;&#21512;&#65288;&#21363;&#21069;&#32512;&#33258;&#30001;&#21644;&#21518;&#32512;&#33258;&#30001;&#65289;&#12290;fix-free&#23646;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#21518;&#32512;&#26641;&#30340;MEM&#31639;&#27861;&#22312;$\mathcal{G}$&#19978;&#36882;&#22686;&#22320;&#35745;&#31639;$\mathcal{T}$&#30340;MEM&#65292;&#35813;&#31639;&#27861;&#19968;&#27425;&#22312;&#19968;&#32452;&#35821;&#27861;&#35268;&#21017;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#19981;&#35299;&#21387;&#32553;&#38750;&#32456;&#32467;&#31526;&#12290;&#36890;&#36807;&#20462;&#25913;Christiansen&#31561;&#20154;2020&#24180;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#35821;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#26500;&#24314;$\mathcal{G}$&#20174;$\mathcal{T}$&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MEM&#31639;&#27861;&#22312;$\mathcal{G}$&#19978;&#20197;$O(G+occ)$&#26102;&#38388;&#36816;&#34892;&#65292;&#24182;&#19988;&#20351;&#29992;$O(\log G(G+occ))$&#20301;&#65292;&#20854;&#20013;$G$&#26159;&#35821;&#27861;&#22823;&#23567;&#65292;&#32780;$occ$&#26159;MEM&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a compression-aware method to compute all-vs-all maximal exact matches (MEM) among strings of a repetitive collection $\mathcal{T}$. The key concept in our work is the construction of a fully-balanced grammar $\mathcal{G}$ from $\mathcal{T}$ that meets a property that we call \emph{fix-free}: the expansions of the nonterminals that have the same height in the parse tree form a fix-free set (i.e., prefix-free and suffix-free). The fix-free property allows us to compute the MEMs of $\mathcal{T}$ incrementally over $\mathcal{G}$ using a standard suffix-tree-based MEM algorithm, which runs on a subset of grammar rules at a time and does not decompress nonterminals. By modifying the locally-consistent grammar of Christiansen et al 2020., we show how we can build $\mathcal{G}$ from $\mathcal{T}$ in linear time and space. We also demonstrate that our MEM algorithm runs on top of $\mathcal{G}$ in $O(G +occ)$ time and uses $O(\log G(G+occ))$ bits, where $G$ is the grammar size, and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#20915;&#40479;&#40483;&#20998;&#31867;&#27604;&#36187;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#40479;&#31867;&#29289;&#31181;&#20998;&#31867;&#26041;&#38754;&#26377;&#25928;&#65292;&#24182;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16760</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#40479;&#40483;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification. (arXiv:2306.16760v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16760
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#20915;&#40479;&#40483;&#20998;&#31867;&#27604;&#36187;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#40479;&#31867;&#29289;&#31181;&#20998;&#31867;&#26041;&#38754;&#26377;&#25928;&#65292;&#24182;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20351;&#29992;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#36827;&#34892;&#40479;&#40483;&#20998;&#31867;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#24037;&#20316;&#31508;&#35760;&#65292;&#37325;&#28857;&#26159;&#22312;&#35760;&#24405;&#30340;&#22768;&#26223;&#20013;&#35782;&#21035;&#38750;&#27954;&#40479;&#31867;&#29289;&#31181;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#29616;&#25104;&#27169;&#22411;BirdNET&#21644;MixIT&#26469;&#35299;&#20915;&#27604;&#36187;&#20013;&#30340;&#34920;&#31034;&#21644;&#26631;&#27880;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;BirdNET&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#21508;&#31181;&#27169;&#22411;&#21644;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#20197;&#22312;&#27604;&#36187;&#25490;&#34892;&#27036;&#19978;&#36798;&#21040;&#26368;&#20339;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40479;&#31867;&#29289;&#31181;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#27880;&#37322;&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition, focused on identifying African bird species in recorded soundscapes. Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition. We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning. Our experiments involve various models and feature engineering approaches to maximize performance on the competition leaderboard. The results demonstrate the effectiveness of our approach in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.
&lt;/p&gt;</description></item><item><title>&#22810;&#24773;&#26223;&#23398;&#20064;&#22312;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#19994;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#25628;&#32034;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.16732</link><description>&lt;p&gt;
&#22810;&#24773;&#26223;&#25490;&#24207;&#19982;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Scenario Ranking with Adaptive Feature Learning. (arXiv:2306.16732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16732
&lt;/p&gt;
&lt;p&gt;
&#22810;&#24773;&#26223;&#23398;&#20064;&#22312;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#19994;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#25628;&#32034;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#24773;&#26223;&#23398;&#20064;&#65288;MSL&#65289;&#22312;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#19994;&#20013;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#20174;&#19981;&#21516;&#24773;&#26223;&#20013;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20943;&#36731;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#24182;&#38477;&#20302;&#20102;&#32500;&#25252;&#25104;&#26412;&#12290;&#36825;&#20123;&#21162;&#21147;&#36890;&#36807;&#25628;&#32034;&#26356;&#20248;&#32593;&#32476;&#32467;&#26500;&#65288;&#22914;&#36741;&#21161;&#32593;&#32476;&#12289;&#19987;&#23478;&#32593;&#32476;&#21644;&#22810;&#22612;&#32593;&#32476;&#65289;&#20135;&#29983;&#19981;&#21516;&#30340;MSL&#33539;&#24335;&#12290;&#19981;&#21516;&#30340;&#24773;&#26223;&#21487;&#20197;&#25345;&#26377;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#28608;&#27963;&#29992;&#25143;&#30340;&#24847;&#22270;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#24773;&#26223;&#19979;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#36741;&#21161;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20063;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#20197;&#24773;&#26223;&#24863;&#30693;&#30340;&#26041;&#24335;&#20248;&#21270;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#33719;&#24471;&#26356;&#22909;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#22320;&#25628;&#32034;&#26368;&#20248;&#32593;&#32476;&#32467;&#26500;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#20027;&#35201;&#34987;&#24573;&#35270;&#65292;&#20294;&#21364;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20063;&#39564;&#35777;&#20102;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Multi-Scenario Learning (MSL) is widely used in recommendation and retrieval systems in the industry because it facilitates transfer learning from different scenarios, mitigating data sparsity and reducing maintenance cost. These efforts produce different MSL paradigms by searching more optimal network structure, such as Auxiliary Network, Expert Network, and Multi-Tower Network. It is intuitive that different scenarios could hold their specific characteristics, activating the user's intents quite differently. In other words, different kinds of auxiliary features would bear varying importance under different scenarios. With more discriminative feature representations refined in a scenario-aware manner, better ranking performance could be easily obtained without expensive search for the optimal network structure. Unfortunately, this simple idea is mainly overlooked but much desired in real-world systems.Further analysis also validates the rationality of adaptive feature learni
&lt;/p&gt;</description></item><item><title>SPLADE&#27169;&#22411;&#26159;&#19968;&#31181;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#20445;&#25345;&#20498;&#25490;&#21015;&#34920;&#30340;&#31232;&#30095;&#24615;&#26469;&#25552;&#39640;&#35789;&#27719;&#21305;&#37197;&#21644;&#25490;&#21517;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#19981;&#24120;&#35265;&#30340;&#35789;&#27719;&#65292;&#22914;&#20572;&#29992;&#35789;&#25110;&#38543;&#26426;&#35789;&#65292;SPLADE&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#25490;&#21517;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2306.16680</link><description>&lt;p&gt;
&#25506;&#32034; SPLADE &#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Representation Power of SPLADE Models. (arXiv:2306.16680v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16680
&lt;/p&gt;
&lt;p&gt;
SPLADE&#27169;&#22411;&#26159;&#19968;&#31181;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#20445;&#25345;&#20498;&#25490;&#21015;&#34920;&#30340;&#31232;&#30095;&#24615;&#26469;&#25552;&#39640;&#35789;&#27719;&#21305;&#37197;&#21644;&#25490;&#21517;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#19981;&#24120;&#35265;&#30340;&#35789;&#27719;&#65292;&#22914;&#20572;&#29992;&#35789;&#25110;&#38543;&#26426;&#35789;&#65292;SPLADE&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#25490;&#21517;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SPLADE (SParse Lexical AnD Expansion) &#27169;&#22411;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#65292;&#20854;&#20013;&#25991;&#26723;&#30001;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#35789;&#39033;&#24433;&#21709;&#20998;&#25968;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;SPLADE &#24212;&#29992;&#27491;&#21017;&#21270;&#26469;&#30830;&#20445;&#20498;&#25490;&#21015;&#34920;&#20445;&#25345;&#31232;&#30095; -- &#30446;&#30340;&#26159;&#27169;&#25311;&#33258;&#28982;&#35789;&#39033;&#20998;&#24067;&#30340;&#29305;&#24615; -- &#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#35789;&#27719;&#21305;&#37197;&#21644;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20551;&#35774; SPLADE &#21487;&#20197;&#36890;&#36807;&#24120;&#35265;&#30340;&#20498;&#25490;&#21015;&#34920;&#36827;&#19968;&#27493;&#32534;&#30721;&#38468;&#21152;&#30340;&#20449;&#21495;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25511;&#21046;&#35789;&#27719;&#37325;&#26032;&#35757;&#32451; SPLADE &#24182;&#34913;&#37327;&#20854;&#22312;&#25490;&#21517;&#27573;&#33853;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#35789;&#27719;&#34987;&#38480;&#21046;&#22312;&#20256;&#32479;&#19978;&#29992;&#20110;&#25490;&#21517;&#30340;&#26080;&#29992;&#35789;&#39033;&#65288;&#22914;&#20572;&#29992;&#35789;&#25110;&#38543;&#26426;&#35789;&#65289;&#65292;SPLADE &#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#25991;&#26723;&#20013;&#32534;&#30721;&#26377;&#25928;&#30340;&#25490;&#21517;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SPLADE (SParse Lexical AnD Expansion) model is a highly effective approach to learned sparse retrieval, where documents are represented by term impact scores derived from large language models. During training, SPLADE applies regularization to ensure postings lists are kept sparse -- with the aim of mimicking the properties of natural term distributions -- allowing efficient and effective lexical matching and ranking. However, we hypothesize that SPLADE may encode additional signals into common postings lists to further improve effectiveness. To explore this idea, we perform a number of empirical analyses where we re-train SPLADE with different, controlled vocabularies and measure how effective it is at ranking passages. Our findings suggest that SPLADE can effectively encode useful ranking signals in documents even when the vocabulary is constrained to terms that are not traditionally useful for ranking, such as stopwords or even random words.
&lt;/p&gt;</description></item><item><title>&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#23545;&#20110;&#25628;&#32034;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#31070;&#32463;&#27169;&#22411;&#23548;&#33268;&#20102;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#28304;&#28040;&#32791;&#22686;&#21152;&#65292;&#20294;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30456;&#27604;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2306.16668</link><description>&lt;p&gt;
&#36229;&#36234;CO2&#25490;&#25918;&#65306;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#27700;&#28040;&#32791;&#30340;&#34987;&#24573;&#35270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of Information Retrieval Models. (arXiv:2306.16668v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16668
&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#23545;&#20110;&#25628;&#32034;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#31070;&#32463;&#27169;&#22411;&#23548;&#33268;&#20102;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#28304;&#28040;&#32791;&#22686;&#21152;&#65292;&#20294;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30456;&#27604;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19968;&#26679;&#65292;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#23545;&#19982;&#31070;&#32463;&#27169;&#22411;&#30456;&#20851;&#30340;&#33021;&#37327;&#28040;&#32791;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#23545;&#25628;&#32034;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#31070;&#32463;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#28304;&#28040;&#32791;&#20063;&#38543;&#20043;&#22686;&#21152;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#32780;&#35328;&#20173;&#28982;&#36739;&#20302;&#65292;&#20294;&#36825;&#31181;&#20852;&#36259;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
As in other fields of artificial intelligence, the information retrieval community has grown interested in investigating the power consumption associated with neural models, particularly models of search. This interest has become particularly relevant as the energy consumption of information retrieval models has risen with new neural models based on large language models, leading to an associated increase of CO2 emissions, albeit relatively low compared to fields such as natural language processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#30340;&#25628;&#32034;&#24341;&#25806;&#23454;&#29616;&#26041;&#26696;&#65292;&#29992;&#20110;Iconclass&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#25991;&#26412;&#26597;&#35810;&#26469;&#26816;&#32034;&#21644;&#25506;&#32034;Iconclass&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2306.16529</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;Iconclass&#19978;&#36827;&#34892;&#22810;&#27169;&#24577;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multimodal Search on Iconclass using Vision-Language Pre-Trained Models. (arXiv:2306.16529v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#30340;&#25628;&#32034;&#24341;&#25806;&#23454;&#29616;&#26041;&#26696;&#65292;&#29992;&#20110;Iconclass&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#25991;&#26412;&#26597;&#35810;&#26469;&#26816;&#32034;&#21644;&#25506;&#32034;Iconclass&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#35821;&#26469;&#28304;&#65292;&#22914;&#21463;&#25511;&#35789;&#27719;&#12289;&#35789;&#34920;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#22312;&#25968;&#23383;&#21270;&#25991;&#21270;&#36951;&#20135;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20801;&#35768;&#26597;&#35810;&#21644;&#25506;&#32034;&#36825;&#20123;&#35789;&#27719;&#36164;&#28304;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#24448;&#24448;&#32570;&#20047;&#23545;&#29992;&#25143;&#25628;&#32034;&#32972;&#21518;&#35821;&#20041;&#30340;&#36866;&#24403;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#34920;&#36798;&#26041;&#24335;&#20256;&#36798;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#12289;&#20851;&#38190;&#35789;&#25110;&#25991;&#26412;&#25551;&#36848;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#24341;&#25806;&#23454;&#29616;&#26041;&#26696;&#65292;&#29992;&#20110;&#20854;&#20013;&#19968;&#31181;&#26368;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;Iconclass&#12290;&#35813;&#31995;&#32479;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273; - &#35821;&#35328;&#27169;&#22411;CLIP&#26469;&#26816;&#32034;&#21644;&#25506;&#32034;Iconclass&#27010;&#24565;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#35270;&#35273;&#26597;&#35810;&#36824;&#26159;&#25991;&#26412;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terminology sources, such as controlled vocabularies, thesauri and classification systems, play a key role in digitizing cultural heritage. However, Information Retrieval (IR) systems that allow to query and explore these lexical resources often lack an adequate representation of the semantics behind the user's search, which can be conveyed through multiple expression modalities (e.g., images, keywords or textual descriptions). This paper presents the implementation of a new search engine for one of the most widely used iconography classification system, Iconclass. The novelty of this system is the use of a pre-trained vision-language model, namely CLIP, to retrieve and explore Iconclass concepts using visual or textual queries.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20154;&#20204;&#30340;&#20581;&#24247;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20915;&#31574;&#26641;&#12289;k&#26368;&#36817;&#37051;&#23621;(kNN)&#12289;AdaBoost&#21644;Bagging&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.16528</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Food Recommender System in Academic Environments Based on Machine Learning Models. (arXiv:2306.16528v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20154;&#20204;&#30340;&#20581;&#24247;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20915;&#31574;&#26641;&#12289;k&#26368;&#36817;&#37051;&#23621;(kNN)&#12289;AdaBoost&#21644;Bagging&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20154;&#20204;&#30340;&#20581;&#24247;&#21462;&#20915;&#20110;&#36866;&#24403;&#30340;&#39278;&#39135;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#22914;&#20170;&#65292;&#38543;&#30528;&#20154;&#20204;&#29983;&#27963;&#30340;&#26426;&#26800;&#21270;&#22686;&#21152;&#65292;&#36866;&#24403;&#30340;&#39278;&#39135;&#20064;&#24815;&#21644;&#34892;&#20026;&#34987;&#24573;&#35270;&#20102;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#39135;&#29289;&#25512;&#33616;&#20063;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#26159;&#38543;&#30528;&#35199;&#26041;&#39278;&#39135;&#39118;&#26684;&#30340;&#24341;&#20837;&#21644;&#35199;&#26041;&#21270;&#23398;&#33647;&#29289;&#30340;&#36827;&#27493;&#65292;&#22312;&#30142;&#30149;&#27835;&#30103;&#21644;&#33829;&#20859;&#26041;&#38754;&#20986;&#29616;&#20102;&#35768;&#22810;&#38382;&#39064;&#12290;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23548;&#33268;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21019;&#24314;&#65292;&#20197;&#25913;&#21892;&#20154;&#20204;&#30340;&#20581;&#24247;&#12290;&#26041;&#27861;&#65306;&#37319;&#29992;&#28151;&#21512;&#25512;&#33616;&#31995;&#32479;&#65292;&#21253;&#25324;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#22312;2519&#21517;&#23398;&#29983;&#30340;&#33829;&#20859;&#31649;&#29702;&#31995;&#32479;&#20013;&#65292;&#30740;&#31350;&#20102;&#20915;&#31574;&#26641;&#12289;k&#26368;&#36817;&#37051;&#23621;(kNN)&#12289;AdaBoost&#21644;Bagging&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: People's health depends on the use of proper diet as an important factor. Today, with the increasing mechanization of people's lives, proper eating habits and behaviors are neglected. On the other hand, food recommendations in the field of health have also tried to deal with this issue. But with the introduction of the Western nutrition style and the advancement of Western chemical medicine, many issues have emerged in the field of disease treatment and nutrition. Recent advances in technology and the use of artificial intelligence methods in information systems have led to the creation of recommender systems in order to improve people's health. Methods: A hybrid recommender system including, collaborative filtering, content-based, and knowledge-based models was used. Machine learning models such as Decision Tree, k-Nearest Neighbors (kNN), AdaBoost, and Bagging were investigated in the field of food recommender systems on 2519 students in the nutrition management system of
&lt;/p&gt;</description></item><item><title>OBELISC&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;141&#20159;&#20010;&#32593;&#39029;&#65292;3.53&#20159;&#20010;&#22270;&#20687;&#21644;1150&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;80&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16527</link><description>&lt;p&gt;
OBELISC&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#22270;&#20687;&#25991;&#26723;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents. (arXiv:2306.16527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16527
&lt;/p&gt;
&lt;p&gt;
OBELISC&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;141&#20159;&#20010;&#32593;&#39029;&#65292;3.53&#20159;&#20010;&#22270;&#20687;&#21644;1150&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;80&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35201;&#27714;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#22270;&#29255;&#36827;&#34892;&#25512;&#29702;&#29983;&#25104;&#25991;&#26412;&#30340;&#21508;&#31181;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#33258;&#28982;&#25991;&#26723;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#38169;&#65289;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#23578;&#26410;&#21457;&#24067;&#65292;&#24182;&#19988;&#25910;&#38598;&#36807;&#31243;&#23578;&#26410;&#23436;&#20840;&#26126;&#30830;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;OBELISC&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#30001;141&#20159;&#20010;&#20174;Common Crawl&#25552;&#21462;&#30340;&#32593;&#39029;&#65292;3.53&#20159;&#20010;&#30456;&#20851;&#22270;&#20687;&#21644;1150&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#32452;&#25104;&#30340;&#24320;&#25918;&#24335;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#30340;&#22270;&#20687;&#25991;&#26412;&#20132;&#38169;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36807;&#28388;&#35268;&#21017;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#30340;&#20869;&#23481;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#23637;&#31034;OBELISC&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;80&#20159;&#21442;&#25968;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#29992;&#20110;&#37325;&#29616;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#20197;&#21450;&#25968;&#25454;&#38598;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELISC dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34394;&#20551;&#35780;&#35770;&#29983;&#25104;&#22120;&#23545;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#25805;&#32437;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#20122;&#39532;&#36874;&#19978;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;RBRS&#12290;</title><link>http://arxiv.org/abs/2306.16526</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#23545;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#25805;&#32437;&#30340;&#40657;&#30418;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Shilling Black-box Review-based Recommender Systems through Fake Review Generation. (arXiv:2306.16526v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34394;&#20551;&#35780;&#35770;&#29983;&#25104;&#22120;&#23545;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#25805;&#32437;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#20122;&#39532;&#36874;&#19978;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;RBRS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RBRS&#65289;&#30001;&#20110;&#33021;&#22815;&#32531;&#35299;&#20247;&#25152;&#21608;&#30693;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;RBRS&#21033;&#29992;&#35780;&#35770;&#26469;&#26500;&#24314;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#23545;&#35780;&#35770;&#30340;&#20381;&#36182;&#21487;&#33021;&#20250;&#20351;&#31995;&#32479;&#38754;&#20020;&#34987;&#25805;&#32437;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;RBRS&#36827;&#34892;&#25805;&#32437;&#25915;&#20987;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#19968;&#20010;&#34394;&#20551;&#35780;&#35770;&#29983;&#25104;&#22120;&#65292;&#23427;&#36890;&#36807;&#21521;&#31995;&#32479;&#28155;&#21152;&#29983;&#25104;&#30340;&#35780;&#35770;&#23548;&#33268;&#39044;&#27979;&#20559;&#31227;&#20174;&#32780;&#24694;&#24847;&#25512;&#24191;&#29289;&#21697;&#12290;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#22870;&#21169;&#65292;&#20511;&#21161;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#26041;&#38754;&#39044;&#27979;&#22120;&#26469;&#22686;&#21152;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#29983;&#25104;&#30340;&#35780;&#35770;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#25805;&#32437;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#20122;&#39532;&#36874;&#19978;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;RBRS&#12290;
&lt;/p&gt;
&lt;p&gt;
Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate well-known cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;Twitter&#25968;&#25454;&#27969;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.16495</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#27969;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Event Detection from Social Media Stream: Methods, Datasets and Opportunities. (arXiv:2306.16495v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;Twitter&#25968;&#25454;&#27969;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#27969;&#21253;&#21547;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#65292;&#20174;&#26085;&#24120;&#29983;&#27963;&#25925;&#20107;&#21040;&#26368;&#26032;&#30340;&#20840;&#29699;&#21644;&#24403;&#22320;&#20107;&#20214;&#21644;&#26032;&#38395;&#12290;&#29305;&#21035;&#26159;Twitter&#65292;&#20801;&#35768;&#24555;&#36895;&#20256;&#25773;&#23454;&#26102;&#21457;&#29983;&#30340;&#20107;&#20214;&#65292;&#24182;&#20351;&#20010;&#20154;&#21644;&#32452;&#32455;&#33021;&#22815;&#21450;&#26102;&#20102;&#35299;&#24403;&#21069;&#20107;&#20214;&#12290;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#26816;&#27979;&#20107;&#20214;&#19982;&#20256;&#32479;&#25991;&#26412;&#38754;&#20020;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;Twitter&#25968;&#25454;&#27969;&#30340;&#24191;&#27867;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24110;&#21161;&#35835;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media streams contain large and diverse amount of information, ranging from daily-life stories to the latest global and local events and news. Twitter, especially, allows a fast spread of events happening real time, and enables individuals and organizations to stay informed of the events happening now. Event detection from social media data poses different challenges from traditional text and is a research area that has attracted much attention in recent years. In this paper, we survey a wide range of event detection methods for Twitter data stream, helping readers understand the recent development in this area. We present the datasets available to the public. Furthermore, a few research opportunities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;Precision@5&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16478</link><description>&lt;p&gt;
&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#31264;&#23494;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering. (arXiv:2306.16478v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;Precision@5&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#20854;&#20013;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#23545;&#20110;&#22238;&#31572;&#38382;&#39064;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20010;&#31867;&#21035;&#34987;&#31216;&#20026;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#65288;OK-VQA&#65289;&#12290;&#24320;&#21457;OK-VQA&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#20026;&#32473;&#23450;&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#30446;&#21069;&#27492;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#30340;&#38750;&#23545;&#31216;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26597;&#35810;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#21333;&#27169;&#24577;&#25991;&#26723;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#35757;&#32451;OK-VQA&#20219;&#21153;&#30340;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38750;&#23545;&#31216;&#26550;&#26500;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;Precision@5&#25552;&#21319;&#20102;26.9%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a category of visual question answering tasks, in which accessing external knowledge is necessary for answering the questions. This category is called outside-knowledge visual question answering (OK-VQA). A major step in developing OK-VQA systems is to retrieve relevant documents for the given multi-modal query. Current state-of-the-art asymmetric dense retrieval model for this task uses an architecture with a multi-modal query encoder and a uni-modal document encoder. Such an architecture requires a large amount of training data for effective performance. We propose an automatic data generation pipeline for pre-training passage retrieval models for OK-VQA tasks. The proposed approach leads to 26.9% Precision@5 improvements compared to the current state-of-the-art asymmetric architecture. Additionally, the proposed pre-training approach exhibits a good ability in zero-shot retrieval scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;CCTL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#39046;&#22495;CTR&#39044;&#27979;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.16425</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#25512;&#33616;&#30340;&#21327;&#20316;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Collaborative Transfer Learning Framework for Cross-domain Recommendation. (arXiv:2306.16425v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;CCTL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#39046;&#22495;CTR&#39044;&#27979;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26377;&#22810;&#20010;&#19981;&#21516;&#30340;&#19994;&#21153;&#39046;&#22495;&#26469;&#28385;&#36275;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20852;&#36259;&#21644;&#38656;&#27714;&#65292;&#19981;&#21516;&#39046;&#22495;&#30340;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;&#19981;&#21516;&#19994;&#21153;&#39046;&#22495;&#36827;&#34892;CTR&#39044;&#27979;&#24314;&#27169;&#12290;&#34892;&#19994;&#35299;&#20915;&#26041;&#26696;&#26159;&#23545;&#27599;&#20010;&#39046;&#22495;&#20351;&#29992;&#29305;&#23450;&#30340;&#27169;&#22411;&#25110;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#21069;&#32773;&#30340;&#32570;&#28857;&#26159;&#21333;&#19968;&#39046;&#22495;&#27169;&#22411;&#27809;&#26377;&#21033;&#29992;&#20854;&#20182;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#32780;&#21518;&#32773;&#21017;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#25152;&#26377;&#25968;&#25454;&#65292;&#20294;&#36801;&#31227;&#23398;&#20064;&#30340;&#24494;&#35843;&#27169;&#22411;&#21487;&#33021;&#20351;&#27169;&#22411;&#38519;&#20837;&#28304;&#39046;&#22495;&#30340;&#23616;&#37096;&#26368;&#20248;&#65292;&#38590;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;&#21516;&#26102;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#29305;&#24449;&#27169;&#24335;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#21363;&#39046;&#22495;&#20559;&#31227;&#65292;&#22312;&#36801;&#31227;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#36127;&#38754;&#36801;&#31227;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#20316;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;CCTL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recommendation systems, there are multiple business domains to meet the diverse interests and needs of users, and the click-through rate(CTR) of each domain can be quite different, which leads to the demand for CTR prediction modeling for different business domains. The industry solution is to use domain-specific models or transfer learning techniques for each domain. The disadvantage of the former is that the data from other domains is not utilized by a single domain model, while the latter leverage all the data from different domains, but the fine-tuned model of transfer learning may trap the model in a local optimum of the source domain, making it difficult to fit the target domain. Meanwhile, significant differences in data quantity and feature schemas between different domains, known as domain shift, may lead to negative transfer in the process of transferring. To overcome these challenges, we propose the Collaborative Cross-Domain Transfer Learning Framework (CCTL). CCTL e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03838</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#21040;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#26080;&#20851;&#28151;&#28102;&#22240;&#32032;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30452;&#25509;&#35757;&#32451;&#20110;&#20154;&#33080;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#25935;&#24863;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20154;&#30340;&#36523;&#20221;&#12290;&#35768;&#22810;&#19982;&#20154;&#33080;&#30456;&#20851;&#30340;&#20219;&#21153;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#26159;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#65292;&#24182;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#34920;&#29616;&#19968;&#33268;&#65288;&#21363;&#20844;&#24179;&#65289;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#22343;&#21248;&#24615;&#26159;&#24230;&#37327;&#21644;&#23454;&#26045;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#30340;&#25104;&#26412;&#65292;&#36825;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#65292;&#22823;&#22810;&#25968;&#20154;&#33080;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20219;&#21153;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26080;&#38656;&#27492;&#31867;&#27880;&#37322;&#21363;&#21487;&#25552;&#39640;&#36523;&#20221;&#30456;&#20851;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
&lt;/p&gt;</description></item></channel></rss>