<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#22312;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;&#20013;&#20351;&#29992;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#25928;&#26524;&#65292;&#20197;&#31361;&#20986;&#22312;&#19981;&#21516;&#21152;&#26435;&#25968;&#20540;&#19979;&#20102;&#35299;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06213</link><description>&lt;p&gt;
&#27979;&#35797;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Testing different Log Bases For Vector Model Weighting Technique. (arXiv:2307.06213v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#22312;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;&#20013;&#20351;&#29992;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#25928;&#26524;&#65292;&#20197;&#31361;&#20986;&#22312;&#19981;&#21516;&#21152;&#26435;&#25968;&#20540;&#19979;&#20102;&#35299;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#26681;&#25454;&#29992;&#25143;&#25552;&#20132;&#30340;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#25991;&#26723;&#39318;&#20808;&#34987;&#32034;&#24341;&#65292;&#25991;&#26723;&#20013;&#30340;&#35789;&#35821;&#20351;&#29992;&#31216;&#20026;TFIDF&#30340;&#21152;&#26435;&#25216;&#26415;&#34987;&#36171;&#20104;&#26435;&#37325;&#65292;TFIDF&#26159;&#35789;&#39057;&#65288;TF&#65289;&#21644;&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;IDF&#65289;&#30340;&#20056;&#31215;&#12290;TF&#20195;&#34920;&#35789;&#39033;&#22312;&#25991;&#26723;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#12290;IDF&#34913;&#37327;&#35789;&#39033;&#22312;&#25152;&#26377;&#25991;&#26723;&#20013;&#30340;&#26222;&#36941;&#31243;&#24230;&#12290;&#23427;&#36890;&#36807;&#23558;&#31995;&#32479;&#20013;&#30340;&#24635;&#25991;&#26723;&#25968;&#38500;&#20197;&#21253;&#21547;&#35813;&#35789;&#39033;&#30340;&#25991;&#26723;&#25968;&#65292;&#28982;&#21518;&#35745;&#31639;&#21830;&#30340;&#23545;&#25968;&#26469;&#35745;&#31639;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;10&#20026;&#24213;&#30340;&#23545;&#25968;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#20174;0.1&#21040;100.0&#30340;&#19968;&#31995;&#21015;&#23545;&#25968;&#24213;&#25968;&#26469;&#35745;&#31639;IDF&#65292;&#20197;&#27979;&#35797;&#36825;&#31181;&#21152;&#26435;&#25216;&#26415;&#12290;&#27979;&#35797;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;&#30340;&#30446;&#30340;&#26159;&#31361;&#20986;&#22312;&#19981;&#21516;&#21152;&#26435;&#25968;&#20540;&#19979;&#20102;&#35299;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;MED&#30340;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval systems retrieves relevant documents based on a query submitted by the user. The documents are initially indexed and the words in the documents are assigned weights using a weighting technique called TFIDF which is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF represents the number of occurrences of a term in a document. IDF measures whether the term is common or rare across all documents. It is computed by dividing the total number of documents in the system by the number of documents containing the term and then computing the logarithm of the quotient. By default, we use base 10 to calculate the logarithm. In this paper, we are going to test this weighting technique by using a range of log bases from 0.1 to 100.0 to calculate the IDF. Testing different log bases for vector model weighting technique is to highlight the importance of understanding the performance of the system at different weighting values. We use the documents of MED
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDNAS&#30340;&#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#26550;&#26500;&#34920;&#31034;&#21644;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;DDNAS&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06005</link><description>&lt;p&gt;
DDNAS: &#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDNAS&#30340;&#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#26550;&#26500;&#34920;&#31034;&#21644;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;DDNAS&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#22312;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;NAS&#26082;&#26410;&#23545;&#26550;&#26500;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#34701;&#21512;&#20197;&#20248;&#21270;&#65292;&#20063;&#26410;&#23545;&#25991;&#26412;&#36755;&#20837;&#32972;&#21518;&#30340;&#28508;&#22312;&#23618;&#32423;&#20998;&#31867;&#36827;&#34892;&#32534;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21363;Discretized Differentiable Neural Architecture Search (DDNAS)&#65292;&#29992;&#20110;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#26550;&#26500;&#34920;&#31034;&#30340;&#36830;&#32493;&#26494;&#24347;&#65292;DDNAS&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26469;&#36827;&#34892;&#25628;&#32034;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#23558;&#20854;&#26045;&#21152;&#20110;&#27599;&#20010;&#25628;&#32034;&#33410;&#28857;&#19978;&#65292;&#20197;&#23545;&#25991;&#26412;&#34920;&#31034;&#20013;&#30340;&#28508;&#22312;&#23618;&#32423;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DDNAS&#22987;&#32456;&#33021;&#22815;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;NAS&#26041;&#27861;&#12290;&#23613;&#31649;DDNAS&#20165;&#20381;&#36182;&#20110;&#21367;&#31215;&#65292;&#27744;&#21270;&#21644;&#26080;&#25805;&#20316;&#36825;&#19977;&#20010;&#22522;&#26412;&#25805;&#20316;&#65292;&#20316;&#20026;&#20505;&#36873;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#26694;&#26550;(CL4CVR)&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#36716;&#21270;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05974</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36716;&#21270;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Conversion Rate Prediction. (arXiv:2307.05974v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05974
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#26694;&#26550;(CL4CVR)&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#36716;&#21270;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#22312;&#24191;&#21578;&#31995;&#32479;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#22312;CVR&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#36739;&#39640;&#12290;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#65292;&#34429;&#28982;&#23384;&#22312;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#24191;&#21578;&#65292;&#20294;&#29992;&#25143;&#24448;&#24448;&#21482;&#28857;&#20987;&#20854;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#22312;&#20854;&#20013;&#30340;&#26356;&#23567;&#37096;&#20998;&#36827;&#34892;&#36716;&#21270;&#12290;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#38480;&#21046;&#20102;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;CVR&#39044;&#27979;&#65288;CL4CVR&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;&#30417;&#30563;CVR&#39044;&#27979;&#20219;&#21153;&#19982;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20851;&#32852;&#36215;&#26469;&#65292;&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;CVR&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#20110;CVR&#39044;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#20837;&#24335;&#25513;&#30721;&#65288;EM&#65289;&#65292;&#32780;&#19981;&#26159;&#29305;&#24449;&#25513;&#30721;&#65292;&#26469;&#21019;&#24314;&#20004;&#20010;&#22686;&#24378;&#26679;&#26412;&#35270;&#22270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#38452;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction plays an important role in advertising systems. Recently, supervised deep neural network-based models have shown promising performance in CVR prediction. However, they are data hungry and require an enormous amount of training data. In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the power of these deep models. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples. We also propose a false negativ
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.05827</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#22312;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#19978;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05827
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#22823;&#37096;&#20998;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20174;&#33258;&#30001;&#26684;&#24335;&#30340;&#36830;&#32493;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#27604;&#22914;&#34920;&#26684;&#12290;&#25105;&#20204;&#20174;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22788;&#29702;&#34920;&#26684;&#21270;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#32593;&#32476;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#21644;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38169;&#35823;&#20998;&#26512;&#21644;&#21093;&#31163;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#36129;&#29486;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05722</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#34892;&#20026;&#22270;&#30340;&#29702;&#35299;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34892;&#20026;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#26469;&#25552;&#21319;&#22312;&#32447;&#25307;&#32856;&#20013;&#30340;&#25512;&#33616;&#65292;&#21253;&#25324;&#20419;&#36827;&#38750;&#20998;&#24067;&#24335;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#24182;&#25581;&#31034;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#36335;&#24452;&#25552;&#31034;&#26500;&#36896;&#22120;&#65292;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#39318;&#27425;&#29702;&#35299;&#34892;&#20026;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#36335;&#24452;&#22686;&#24378;&#27169;&#22359;&#26469;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#24207;&#21015;&#36755;&#20837;&#24341;&#20837;&#30340;&#25552;&#31034;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23558;LM&#30340;&#29305;&#28857;&#24341;&#20837;&#21040;&#34892;&#20026;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#24191;&#21578;&#21830;&#22312;&#38750;&#31283;&#24577;&#37319;&#36141;&#29615;&#22659;&#19979;&#21160;&#24577;&#20248;&#21270;&#24191;&#21578;&#24179;&#21488;&#21442;&#25968;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.05698</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#33258;&#21160;&#25237;&#26631;&#19990;&#30028;&#20013;&#30340;&#22312;&#32447;&#24191;&#21578;&#37319;&#36141;
&lt;/p&gt;
&lt;p&gt;
Online Ad Procurement in Non-stationary Autobidding Worlds. (arXiv:2307.05698v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#24191;&#21578;&#21830;&#22312;&#38750;&#31283;&#24577;&#37319;&#36141;&#29615;&#22659;&#19979;&#21160;&#24577;&#20248;&#21270;&#24191;&#21578;&#24179;&#21488;&#21442;&#25968;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#22312;&#32447;&#24191;&#21578;&#21830;&#36890;&#36807;&#19982;&#33258;&#21160;&#25237;&#26631;&#24179;&#21488;&#36827;&#34892;&#20132;&#20114;&#26469;&#37319;&#36141;&#25968;&#23383;&#24191;&#21578;&#23637;&#31034;&#65306;&#24191;&#21578;&#21830;&#36890;&#36807;&#35774;&#32622;&#39044;&#31639;&#12289;&#30446;&#26631;&#25237;&#36164;&#22238;&#25253;&#29575;&#12289;&#27599;&#27425;&#28857;&#20987;&#30340;&#26368;&#22823;&#25104;&#26412;&#31561;&#21442;&#25968;&#26469;&#20256;&#36798;&#39640;&#32423;&#37319;&#36141;&#30446;&#26631;&#12290;&#28982;&#21518;&#24191;&#21578;&#24179;&#21488;&#20195;&#34920;&#24191;&#21578;&#21830;&#37319;&#36141;&#23637;&#31034;&#65292;&#24182;&#21521;&#24191;&#21578;&#21830;&#25253;&#21578;&#26368;&#32456;&#37319;&#36141;&#36716;&#21270;&#32467;&#26524;&#65288;&#20363;&#22914;&#28857;&#20987;&#37327;&#65289;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24191;&#21578;&#21830;&#21487;&#33021;&#21482;&#20250;&#25509;&#25910;&#21040;&#24179;&#21488;&#37319;&#36141;&#32454;&#33410;&#30340;&#26368;&#23569;&#20449;&#24687;&#65292;&#24182;&#19988;&#37319;&#36141;&#32467;&#26524;&#21463;&#21040;&#23395;&#33410;&#24615;&#27169;&#24335;&#12289;&#20598;&#21457;&#24615;&#31995;&#32479;&#25925;&#38556;&#21644;&#24066;&#22330;&#36235;&#21183;&#31561;&#38750;&#31283;&#24577;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#24191;&#21578;&#21830;&#38590;&#20197;&#26377;&#25928;&#20248;&#21270;&#21442;&#25968;&#20915;&#31574;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#24191;&#21578;&#21830;&#22312;&#20855;&#26377;&#38750;&#31283;&#24577;&#37319;&#36141;&#32467;&#26524;&#30340;&#29616;&#23454;&#22810;&#33218;&#36172;&#21338;&#29615;&#22659;&#19979;&#65292;&#22312;&#21463;&#36890;&#29992;&#38271;&#26399;&#32422;&#26463;&#26465;&#20214;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#20248;&#21270;&#24191;&#21578;&#24179;&#21488;&#30340;&#21442;&#25968;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21407;&#22987;&#30340;-d
&lt;/p&gt;
&lt;p&gt;
Today's online advertisers procure digital ad impressions through interacting with autobidding platforms: advertisers convey high level procurement goals via setting levers such as budget, target return-on-investment, max cost per click, etc.. Then ads platforms subsequently procure impressions on advertisers' behalf, and report final procurement conversions (e.g. click) to advertisers. In practice, advertisers may receive minimal information on platforms' procurement details, and procurement outcomes are subject to non-stationary factors like seasonal patterns, occasional system corruptions, and market trends which make it difficult for advertisers to optimize lever decisions effectively. Motivated by this, we present an online learning framework that helps advertisers dynamically optimize ad platform lever decisions while subject to general long-term constraints in a realistic bandit feedback environment with non-stationary procurement outcomes. In particular, we introduce a primal-d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GoTogether&#65292;&#19968;&#20010;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#20026;&#25340;&#36710;&#26381;&#21153;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#21382;&#21490;&#36873;&#25321;&#65292;GoTogether&#33021;&#22815;&#39044;&#27979;&#20010;&#20154;&#20849;&#20056;&#30340;&#24895;&#26395;&#65292;&#24182;&#25552;&#20379;&#39640;&#25104;&#21151;&#29575;&#30340;&#25340;&#36710;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.05697</link><description>&lt;p&gt;
&#19968;&#20010;&#21160;&#24577;&#20010;&#24615;&#21270;&#25340;&#36710;&#26381;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services. (arXiv:2307.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GoTogether&#65292;&#19968;&#20010;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#20026;&#25340;&#36710;&#26381;&#21153;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#21382;&#21490;&#36873;&#25321;&#65292;GoTogether&#33021;&#22815;&#39044;&#27979;&#20010;&#20154;&#20849;&#20056;&#30340;&#24895;&#26395;&#65292;&#24182;&#25552;&#20379;&#39640;&#25104;&#21151;&#29575;&#30340;&#25340;&#36710;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#21496;&#26426;&#19982;&#20855;&#26377;&#30456;&#20284;&#34892;&#31243;&#21644;&#26102;&#38388;&#23433;&#25490;&#30340;&#26053;&#23458;&#20849;&#20139;&#27773;&#36710;&#65292;&#25340;&#36710;&#34987;&#26399;&#26395;&#22312;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#27745;&#26579;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#22312;&#19968;&#32452;&#21496;&#26426;&#21644;&#28508;&#22312;&#20056;&#23458;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#25104;&#21151;&#30340;&#25340;&#36710;&#21305;&#37197;&#65292;&#35774;&#35745;&#20102;&#35768;&#22810;&#25340;&#36710;&#21305;&#37197;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#22312;&#24050;&#32463;&#35748;&#35782;&#21040;&#38500;&#20102;&#31616;&#21333;&#30340;&#20986;&#34892;&#38656;&#27714;&#22806;&#65292;&#35768;&#22810;&#38750;&#36135;&#24065;&#26041;&#38754;&#21644;&#31038;&#20250;&#22240;&#32032;&#21487;&#33021;&#24433;&#21709;&#20010;&#20154;&#24895;&#24847;&#20849;&#20056;&#30340;&#24847;&#24895;&#65292;&#36825;&#20123;&#22240;&#32032;&#24456;&#38590;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GoTogether&#65292;&#36825;&#26159;&#19968;&#20010;&#25340;&#36710;&#26381;&#21153;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#20174;&#29992;&#25143;&#30340;&#36873;&#25321;&#21382;&#21490;&#65288;&#21363;&#25509;&#21463;&#25110;&#25298;&#32477;&#20849;&#20139;&#20056;&#36710;&#30340;&#31867;&#22411;&#65289;&#20013;&#33258;&#21160;&#25512;&#23548;&#20986;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;GoTogether&#26500;&#24314;&#25512;&#33616;&#20056;&#36710;&#21015;&#34920;&#20197;&#26368;&#22823;&#21270;&#21305;&#37197;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car pooling is expected to significantly help in reducing traffic congestion and pollution in cities by enabling drivers to share their cars with travellers with similar itineraries and time schedules. A number of car pooling matching services have been designed in order to efficiently find successful ride matches in a given pool of drivers and potential passengers. However, it is now recognised that many non-monetary aspects and social considerations, besides simple mobility needs, may influence the individual willingness of sharing a ride, which are difficult to predict. To address this problem, in this study we propose GoTogether, a recommender system for car pooling services that leverages on learning-to-rank techniques to automatically derive the personalised ranking model of each user from the history of her choices (i.e., the type of accepted or rejected shared rides). Then, GoTogether builds the list of recommended rides in order to maximise the success rate of the offered matc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32972;&#26223;&#19979;&#65292;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05696</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65306;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32972;&#26223;&#19979;&#65292;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#38656;&#35201;&#24037;&#20855;&#26469;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#20256;&#32479;&#30340;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#20010;&#20154;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#19988;&#32570;&#20047;&#39640;&#25928;&#20449;&#24687;&#22788;&#29702;&#30340;&#32467;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#26723;&#32508;&#21512;&#25104;&#31616;&#27905;&#30340;&#23618;&#32423;&#27010;&#24565;&#22270;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21644;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#26469;&#31215;&#26497;&#21442;&#19982;&#29992;&#25143;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#29305;&#23450;&#20027;&#39064;&#30340;&#26410;&#35265;&#25991;&#26723;&#29983;&#25104;&#20010;&#24615;&#21270;&#25688;&#35201;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23548;&#33322;&#65292;&#24182;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#29420;&#29305;&#30340;&#38656;&#27714;&#20174;&#22823;&#37327;&#25991;&#26723;&#38598;&#21512;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of textual data has created a crucial need for tools that assist users in extracting meaningful insights. Traditional document summarization approaches often fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
&lt;/p&gt;</description></item><item><title>&#12298;&#31185;&#25216;&#25991;&#26723;&#20013;&#30340;&#22270;&#24418;&#20998;&#31867;&#25216;&#26415;&#32508;&#36848;&#12299;&#23545;&#22270;&#24418;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26803;&#29702;&#65292;&#21253;&#25324;&#34920;&#26684;&#12289;&#29031;&#29255;&#12289;&#22270;&#34920;&#12289;&#22320;&#22270;&#21644;&#32472;&#22270;&#20116;&#31867;&#65292;&#24182;&#25209;&#21028;&#24615;&#22320;&#35780;&#36848;&#20102;&#29616;&#26377;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.05694</link><description>&lt;p&gt;
&#12298;&#31185;&#25216;&#25991;&#26723;&#20013;&#30340;&#22270;&#24418;&#20998;&#31867;&#25216;&#26415;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Figure Classification Techniques in Scientific Documents. (arXiv:2307.05694v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05694
&lt;/p&gt;
&lt;p&gt;
&#12298;&#31185;&#25216;&#25991;&#26723;&#20013;&#30340;&#22270;&#24418;&#20998;&#31867;&#25216;&#26415;&#32508;&#36848;&#12299;&#23545;&#22270;&#24418;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26803;&#29702;&#65292;&#21253;&#25324;&#34920;&#26684;&#12289;&#29031;&#29255;&#12289;&#22270;&#34920;&#12289;&#22320;&#22270;&#21644;&#32472;&#22270;&#20116;&#31867;&#65292;&#24182;&#25209;&#21028;&#24615;&#22320;&#35780;&#36848;&#20102;&#29616;&#26377;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#20110;&#20256;&#36798;&#31185;&#23398;&#20107;&#23454;&#21644;&#20449;&#24687;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#22270;&#24418;&#20013;&#25552;&#21462;&#25968;&#25454;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#23558;&#22270;&#24418;&#20998;&#20026;&#34920;&#26684;&#12289;&#29031;&#29255;&#12289;&#22270;&#34920;&#12289;&#22320;&#22270;&#21644;&#32472;&#22270;&#20116;&#31867;&#65292;&#24182;&#23545;&#35299;&#20915;&#22270;&#24418;&#20998;&#31867;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#25209;&#21028;&#24615;&#32508;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22270;&#24418;&#20998;&#31867;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LogitMat&#30340;&#38646;&#27169;&#22411;&#36801;&#31227;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#23556;&#20987;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05680</link><description>&lt;p&gt;
LogitMat&#65306;&#38646;&#27169;&#22411;&#36801;&#31227;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#23556;&#20987;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LogitMat : Zeroshot Learning Algorithm for Recommender Systems without Transfer Learning or Pretrained Models. (arXiv:2307.05680v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LogitMat&#30340;&#38646;&#27169;&#22411;&#36801;&#31227;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#23556;&#20987;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#34987;&#20114;&#32852;&#32593;&#34892;&#19994;&#35748;&#20026;&#26159;&#26368;&#26377;&#21033;&#21487;&#22270;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#19982;&#37329;&#34701;&#31185;&#25216;&#34892;&#19994;&#30340;&#27450;&#35784;&#26816;&#27979;&#31561;&#20854;&#20182;&#39046;&#22495;&#19981;&#21516;&#65292;&#25512;&#33616;&#31995;&#32479;&#26082;&#28145;&#21448;&#24191;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;/&#20803;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#22768;&#31216;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#27599;&#20010;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#26469;&#33258;&#20854;&#20182;&#26469;&#28304;&#30340;&#39069;&#22806;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;2021&#24180;&#21644;2022&#24180;&#65292;&#35832;&#22914;ZeroMat&#12289;DotMat&#12289;PoissonMat&#21644;PowerMat&#31561;&#20960;&#20010;&#38646;&#27169;&#22411;&#36801;&#31227;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#23556;&#20987;&#23398;&#20064;&#31639;&#27861;&#34987;&#21457;&#26126;&#20986;&#26469;&#12290;&#23427;&#20204;&#26159;&#31532;&#19968;&#25209;&#26080;&#38656;&#27169;&#22411;&#36801;&#31227;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27839;&#29992;&#27492;&#24605;&#36335;&#65292;&#24182;&#21457;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#23556;&#20987;&#23398;&#20064;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;LogitMat&#12290;&#25105;&#20204;&#21033;&#29992;Zipf&#20998;&#24067;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Recommender system is adored in the internet industry as one of the most profitable technologies. Unlike other sectors such as fraud detection in the Fintech industry, recommender system is both deep and broad. In recent years, many researchers start to focus on the cold-start problem of recommender systems. In spite of the large volume of research literature, the majority of the research utilizes transfer learning / meta learning and pretrained model to solve the problem. Although the researchers claim the effectiveness of the approaches, everyone of them does rely on extra input data from other sources. In 2021 and 2022, several zeroshot learning algorithm for recommender system such as ZeroMat, DotMat, PoissonMat and PowerMat were invented. They are the first batch of the algorithms that rely on no transfer learning or pretrained models to tackle the problem. In this paper, we follow this line and invent a new zeroshot learning algorithm named LogitMat. We take advantage of the Zipf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;&#25968;&#23383;&#21270;&#30340;&#37051;&#36817;&#24230;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#25628;&#32034;&#38750;&#25968;&#23383;&#21270;&#23454;&#20307;&#20869;&#23481;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18683</link><description>&lt;p&gt;
&#20854;&#25152;&#22312;&#30340;&#20844;&#21496;&#65306;&#22522;&#20110;&#37051;&#36817;&#24230;&#30340;&#26723;&#26696;&#24211;&#23454;&#20307;&#20869;&#23481;&#32034;&#24341;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Known by the Company it Keeps: Proximity-Based Indexing for Physical Content in Archival Repositories. (arXiv:2305.18683v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;&#25968;&#23383;&#21270;&#30340;&#37051;&#36817;&#24230;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#25628;&#32034;&#38750;&#25968;&#23383;&#21270;&#23454;&#20307;&#20869;&#23481;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#20869;&#23481;&#65292;&#20294;&#37325;&#35201;&#30340;&#23454;&#20307;&#20869;&#23481;&#23384;&#20648;&#22312;&#32440;&#36136;&#25110;&#24494;&#32553;&#33180;&#31561;&#29289;&#29702;&#20171;&#36136;&#20013;&#12290;&#20256;&#32479;&#30340;&#38750;&#25968;&#23383;&#21270;&#20869;&#23481;&#32034;&#24341;&#26041;&#27861;&#26159;&#20351;&#29992;&#25163;&#21160;&#21019;&#24314;&#30340;&#20803;&#25968;&#25454;&#26469;&#25551;&#36848;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;&#25968;&#23383;&#21270;&#30340;&#23567;&#37096;&#20998;&#20869;&#23481;&#20316;&#20026;&#37051;&#36817;&#24230;&#32034;&#24341;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#20197;&#23558;&#29992;&#25143;&#26356;&#25509;&#36817;&#20182;&#20204;&#27491;&#22312;&#23547;&#25214;&#30340;&#20855;&#20307;&#20869;&#23481;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#26041;&#27861;&#26500;&#24314;&#30340;&#30418;&#32423;&#32034;&#24341;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#25628;&#32034;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of born-digital content, vast troves of important content remain accessible only on physical media such as paper or microfilm. The traditional approach to indexing undigitized content is using manually created metadata that describes content at some level of aggregation (e.g., folder, box, or collection). Searchers led in this way to some subset of the content often must then manually examine substantial quantities of physical media to find what they are looking for. This paper proposes a complementary approach, in which selective digitization of a small portion of the content is used as a basis for proximity-based indexing as a way of bringing the user closer to the specific content for which they are looking. Experiments with 35 boxes of partially digitized US State Department records indicate that box-level indexes built in this way can provide a useful basis for search.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>