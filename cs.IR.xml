<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Bravo MaRDI&#26159;&#19968;&#20010;&#22522;&#20110;Wikibase&#30340;&#25968;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#26088;&#22312;&#22635;&#34917;&#24403;&#20195;&#25968;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#31574;&#21010;&#30693;&#35782;&#22270;&#35889;&#30340;&#31354;&#30333;&#12290;&#35813;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#25968;&#23398;&#30740;&#31350;&#25968;&#25454;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#36719;&#20214;&#12289;&#20986;&#29256;&#29289;&#20197;&#21450;&#25968;&#23398;&#20844;&#24335;&#21644;&#20551;&#35774;&#31561;&#12290;</title><link>http://arxiv.org/abs/2309.11484</link><description>&lt;p&gt;
Bravo MaRDI: &#19968;&#20010;&#20197;Wikibase&#20026;&#25903;&#25345;&#30340;&#25968;&#23398;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Bravo MaRDI: A Wikibase Powered Knowledge Graph on Mathematics. (arXiv:2309.11484v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11484
&lt;/p&gt;
&lt;p&gt;
Bravo MaRDI&#26159;&#19968;&#20010;&#22522;&#20110;Wikibase&#30340;&#25968;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#26088;&#22312;&#22635;&#34917;&#24403;&#20195;&#25968;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#31574;&#21010;&#30693;&#35782;&#22270;&#35889;&#30340;&#31354;&#30333;&#12290;&#35813;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#25968;&#23398;&#30740;&#31350;&#25968;&#25454;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#36719;&#20214;&#12289;&#20986;&#29256;&#29289;&#20197;&#21450;&#25968;&#23398;&#20844;&#24335;&#21644;&#20551;&#35774;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#19990;&#30028;&#30693;&#35782;&#26159;Wikidata&#30340;&#19968;&#20010;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19987;&#38376;&#20851;&#27880;&#24403;&#20195;&#25968;&#23398;&#30340;&#19987;&#23478;&#31574;&#21010;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25968;&#23398;&#30740;&#31350;&#25968;&#25454;&#20513;&#35758;&#65288;MaRDI&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#23558;&#25968;&#23398;&#20013;&#30340;&#22810;&#27169;&#24335;&#30740;&#31350;&#25968;&#25454;&#30456;&#20114;&#38142;&#25509;&#12290;&#36825;&#21253;&#25324;&#20256;&#32479;&#30340;&#30740;&#31350;&#25968;&#25454;&#39033;&#65292;&#22914;&#25968;&#25454;&#38598;&#12289;&#36719;&#20214;&#21644;&#20986;&#29256;&#29289;&#65292;&#20197;&#21450;&#21253;&#21547;&#25968;&#23398;&#20844;&#24335;&#21644;&#20551;&#35774;&#31561;&#35821;&#20041;&#39640;&#32423;&#23545;&#35937;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#22522;&#20110;Wikibase&#30340;MaRDI&#30693;&#35782;&#22270;&#35889;&#65292;&#23427;&#23558;&#22312;&#20854;&#39318;&#27425;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36827;&#34892;&#12290;&#21457;&#24067;&#20195;&#21495;&#20026;Bravo&#65292;&#21487;&#22312;https://portal.mardi4nfdi.de&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical world knowledge is a fundamental component of Wikidata. However, to date, no expertly curated knowledge graph has focused specifically on contemporary mathematics. Addressing this gap, the Mathematical Research Data Initiative (MaRDI) has developed a comprehensive knowledge graph that links multimodal research data in mathematics. This encompasses traditional research data items like datasets, software, and publications and includes semantically advanced objects such as mathematical formulas and hypotheses. This paper details the abilities of the MaRDI knowledge graph, which is based on Wikibase, leading up to its inaugural public release, codenamed Bravo, available on https://portal.mardi4nfdi.de.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#36947;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#29992;&#20110;&#33258;&#21160;&#39564;&#35777;&#24320;&#25918;&#22495;&#38382;&#31572;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#27491;&#30830;&#24615;&#12290;&#20351;&#29992;&#31232;&#30095;&#26816;&#32034;&#12289;&#23494;&#38598;&#26816;&#32034;&#21644;&#31070;&#32463;&#37325;&#25490;&#22120;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;+&#29983;&#25104;&#31572;&#26696;+&#26816;&#32034;&#31572;&#26696;&#30340;&#32452;&#21512;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.11392</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#38382;&#31572;&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Retrieving Supporting Evidence for Generative Question Answering. (arXiv:2309.11392v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#29992;&#20110;&#33258;&#21160;&#39564;&#35777;&#24320;&#25918;&#22495;&#38382;&#31572;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#27491;&#30830;&#24615;&#12290;&#20351;&#29992;&#31232;&#30095;&#26816;&#32034;&#12289;&#23494;&#38598;&#26816;&#32034;&#21644;&#31070;&#32463;&#37325;&#25490;&#22120;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;+&#29983;&#25104;&#31572;&#26696;+&#26816;&#32034;&#31572;&#26696;&#30340;&#32452;&#21512;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#24320;&#25918;&#22495;&#38382;&#31572;&#65289;&#19978;&#34920;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#20250;&#29983;&#21160;&#22320;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#65292;&#22240;&#27492;&#22312;&#25509;&#21463;&#38382;&#39064;&#31572;&#26696;&#20043;&#21069;&#24517;&#39035;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#23454;&#39564;&#26469;&#33258;&#21160;&#39564;&#35777;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#35821;&#26009;&#24211;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22522;&#20110;MS MARCO&#65288;V1&#65289;&#27979;&#35797;&#38598;&#20013;&#30340;&#38382;&#39064;&#21644;&#27573;&#33853;&#65292;&#20197;&#21450;&#30001;&#31232;&#30095;&#26816;&#32034;&#12289;&#23494;&#38598;&#26816;&#32034;&#21644;&#31070;&#32463;&#37325;&#25490;&#22120;&#32452;&#25104;&#30340;&#26816;&#32034;&#27969;&#31243;&#12290;&#22312;&#31532;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#25972;&#20010;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#39564;&#35777;&#12290;&#23558;&#38382;&#39064;&#25552;&#20379;&#32473;LLM&#65292;&#24182;&#25509;&#25910;&#29983;&#25104;&#30340;&#31572;&#26696;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38382;&#39064;+&#29983;&#25104;&#31572;&#26696;&#30340;&#32452;&#21512;&#22312;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;+&#29983;&#25104;&#31572;&#26696;+&#26816;&#32034;&#31572;&#26696;&#30340;&#32452;&#21512;&#20877;&#27425;&#25552;&#20379;&#32473;LLM&#65292;&#20419;&#20351;&#20854;&#25351;&#26126;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large language models (LLMs) can exhibit near-human levels of performance on many natural language-based tasks, including open-domain question answering. Unfortunately, at this time, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report two simple experiments to automatically validate generated answers against a corpus. We base our experiments on questions and passages from the MS MARCO (V1) test collection, and a retrieval pipeline consisting of sparse retrieval, dense retrieval and neural rerankers. In the first experiment, we validate the generated answer in its entirety. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LAGCL&#65289;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38271;&#23614;&#22686;&#24378;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#26469;&#35299;&#20915;&#22836;&#23614;&#33410;&#28857;&#20043;&#38388;&#30340;&#26174;&#33879;&#24230;&#24046;&#24322;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#24230;&#22343;&#34913;&#26426;&#21046;&#26469;&#24179;&#34913;&#25968;&#25454;&#22686;&#24378;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.11177</link><description>&lt;p&gt;
&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Long-tail Augmented Graph Contrastive Learning for Recommendation. (arXiv:2309.11177v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LAGCL&#65289;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38271;&#23614;&#22686;&#24378;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#26469;&#35299;&#20915;&#22836;&#23614;&#33410;&#28857;&#20043;&#38388;&#30340;&#26174;&#33879;&#24230;&#24046;&#24322;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#24230;&#22343;&#34913;&#26426;&#21046;&#26469;&#24179;&#34913;&#25968;&#25454;&#22686;&#24378;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#24050;&#32463;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#39640;&#38454;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#36890;&#24120;&#36935;&#21040;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;GCN&#30340;&#25512;&#33616;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24341;&#20837;&#33258;&#30417;&#30563;&#30340;&#20449;&#21495;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;&#22836;&#21644;&#23614;&#33410;&#28857;&#20043;&#38388;&#26174;&#33879;&#24230;&#24046;&#24322;&#30340;&#32771;&#34385;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#38750;&#22343;&#21248;&#30340;&#34920;&#31034;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25512;&#33616;&#30340;&#38271;&#23614;&#22686;&#24378;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LAGCL&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#38271;&#23614;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#34917;&#20805;&#39044;&#27979;&#30340;&#37051;&#23621;&#20449;&#24687;&#26469;&#22686;&#24378;&#23614;&#33410;&#28857;&#65292;&#24182;&#22522;&#20110;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#12290;&#20026;&#20102;&#20351;&#25968;&#25454;&#22686;&#24378;&#30340;&#31243;&#24230;&#24179;&#34913;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#22343;&#34913;&#26426;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LAGCL&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. However, these methods usually encounter data sparsity issue in real-world scenarios. To address this issue, GCN-based recommendation methods employ contrastive learning to introduce self-supervised signals. Despite their effectiveness, these methods lack consideration of the significant degree disparity between head and tail nodes. This can lead to non-uniform representation distribution, which is a crucial factor for the performance of contrastive learning methods. To tackle the above issue, we propose a novel Long-tail Augmented Graph Contrastive Learning (LAGCL) method for recommendation. Specifically, we introduce a learnable long-tail augmentation approach to enhance tail nodes by supplementing predicted neighbor information, and generate contrastive views based on the resulting augmented graph. To make the data augmentation sch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#30340;&#26234;&#33021;&#21161;&#25163;&#65288;AIIA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20808;&#36827;&#30340;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25552;&#20379;&#20132;&#20114;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#31616;&#21270;&#20449;&#24687;&#33719;&#21462;&#12289;&#20419;&#36827;&#30693;&#35782;&#35780;&#20272;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#26469;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#20854;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#22238;&#31572;&#23398;&#29983;&#38382;&#39064;&#12289;&#29983;&#25104;&#27979;&#39564;&#21644;&#38378;&#21345;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#21487;&#23545;&#39640;&#31561;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#34394;&#25311;&#21161;&#25945;&#65288;VTA&#65289;&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#26524;&#12289;&#21442;&#19982;&#24230;&#21644;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.10892</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#26234;&#33021;&#21161;&#25163;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education. (arXiv:2309.10892v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#30340;&#26234;&#33021;&#21161;&#25163;&#65288;AIIA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20808;&#36827;&#30340;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25552;&#20379;&#20132;&#20114;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#31616;&#21270;&#20449;&#24687;&#33719;&#21462;&#12289;&#20419;&#36827;&#30693;&#35782;&#35780;&#20272;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#26469;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#20854;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#22238;&#31572;&#23398;&#29983;&#38382;&#39064;&#12289;&#29983;&#25104;&#27979;&#39564;&#21644;&#38378;&#21345;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#21487;&#23545;&#39640;&#31561;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#34394;&#25311;&#21161;&#25945;&#65288;VTA&#65289;&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#26524;&#12289;&#21442;&#19982;&#24230;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#21161;&#21147;&#30340;&#26234;&#33021;&#21161;&#25163;(AIIA)&#65292;&#29992;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290; AIIA&#31995;&#32479;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23398;&#20064;&#24179;&#21488;&#12290;&#35813;&#24179;&#21488;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#30340;&#31616;&#21333;&#33719;&#21462;&#12289;&#20419;&#36827;&#30693;&#35782;&#35780;&#20272;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#26469;&#20943;&#23569;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20197;&#36866;&#24212;&#20010;&#20307;&#38656;&#27714;&#21644;&#23398;&#20064;&#39118;&#26684;&#12290; AIIA&#30340;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#22238;&#31572;&#23398;&#29983;&#30340;&#38382;&#39064;&#12289;&#29983;&#25104;&#27979;&#39564;&#21644;&#38378;&#21345;&#65292;&#20197;&#21450;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#26377;&#21487;&#33021;&#23545;&#39640;&#31561;&#25945;&#32946;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#25945;&#23398;&#21161;&#25163;(VTA)&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#20026;&#24320;&#21457;&#21019;&#26032;&#30340;&#25945;&#32946;&#24037;&#20855;&#25552;&#20379;&#25351;&#23548;&#65292;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#26524;&#12289;&#21442;&#19982;&#24230;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework, Artificial Intelligence-Enabled Intelligent Assistant (AIIA), for personalized and adaptive learning in higher education. The AIIA system leverages advanced AI and Natural Language Processing (NLP) techniques to create an interactive and engaging learning platform. This platform is engineered to reduce cognitive load on learners by providing easy access to information, facilitating knowledge assessment, and delivering personalized learning support tailored to individual needs and learning styles. The AIIA's capabilities include understanding and responding to student inquiries, generating quizzes and flashcards, and offering personalized learning pathways. The research findings have the potential to significantly impact the design, implementation, and evaluation of AI-enabled Virtual Teaching Assistants (VTAs) in higher education, informing the development of innovative educational tools that can enhance student learning outcomes, engagement, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#33258;&#21160;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#20013;&#30340;&#32452;&#32455;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10880</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#36827;&#34892;&#32452;&#32455;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Organizations for Food System Ontologies using Natural Language Processing. (arXiv:2309.10880v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#33258;&#21160;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#20013;&#30340;&#32452;&#32455;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#33258;&#21160;&#23545;&#23454;&#20307;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#36798;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#19982;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#30340;&#38598;&#25104;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#33021;&#22815;&#33258;&#21160;&#23558;&#32452;&#32455;&#26681;&#25454;&#19982;&#29615;&#22659;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#21035;&#20197;&#21450;&#32654;&#22269;&#25919;&#24220;&#29992;&#20110;&#25551;&#36848;&#21830;&#19994;&#27963;&#21160;&#30340;&#26631;&#20934;&#20135;&#19994;&#20998;&#31867;&#65288;SIC&#65289;&#20195;&#30721;&#36827;&#34892;&#20998;&#31867;&#30340;NLP&#27169;&#22411;&#12290;NLP&#27169;&#22411;&#30340;&#36755;&#20837;&#20026;&#27599;&#20010;&#32452;&#32455;&#36890;&#36807;Google&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#21040;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#35813;&#25991;&#26412;&#29255;&#27573;&#29992;&#20316;&#29992;&#20110;&#23398;&#20064;&#30340;&#32452;&#32455;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#30456;&#24403;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20854;&#20182;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NLP&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research explores the use of natural language processing (NLP) methods to automatically classify entities for the purpose of knowledge graph population and integration with food system ontologies. We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification (SIC) codes, which are used by the U.S. government to characterize business activities. As input, the NLP models are provided with text snippets retrieved by the Google search engine for each organization, which serves as a textual description of the organization that is used for learning. Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks, and they rely on a general framework that could be applied to many other classification problems as well. We believe that NLP models represent a promising approach for automatically harvesting informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MelodyGLM&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#30340;&#26059;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#26469;&#25429;&#25417;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10738</link><description>&lt;p&gt;
MelodyGLM: &#38899;&#20048;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation. (arXiv:2309.10738v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MelodyGLM&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#30340;&#26059;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#26469;&#25429;&#25417;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#38899;&#20048;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#25429;&#25417;&#38899;&#31526;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#32500;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#25991;&#26412;&#21644;&#38899;&#20048;&#20043;&#38388;&#39046;&#22495;&#30693;&#35782;&#24046;&#24322;&#30340;&#32536;&#25925;&#12290;&#27492;&#22806;&#65292;&#21487;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MelodyGLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#26059;&#24459;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#65292;&#20026;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#24314;&#31435;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#31354;&#30333;&#22635;&#20805;&#20219;&#21153;&#65292;&#20197;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38899;&#39640;n-gram&#12289;&#33410;&#22863;n-gram&#21450;&#20854;&#32452;&#21512;&#30340;n-gram&#32435;&#20837;&#38899;&#20048;n-gram&#31354;&#30333;&#22635;&#20805;&#20219;&#21153;&#20013;&#65292;&#20197;&#24314;&#27169;&#26059;&#24459;&#30340;&#22810;&#32500;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have achieved impressive results in various music understanding and generation tasks. However, existing pre-training methods for symbolic melody generation struggle to capture multi-scale, multi-dimensional structural information in note sequences, due to the domain knowledge discrepancy between text and music. Moreover, the lack of available large-scale symbolic melody datasets limits the pre-training improvement. In this paper, we propose MelodyGLM, a multi-task pre-training framework for generating melodies with long-term structure. We design the melodic n-gram and long span sampling strategies to create local and global blank infilling tasks for modeling the local and global structures in melodies. Specifically, we incorporate pitch n-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram blank infilling tasks for modeling the multi-dimensional structures in melodies. To this end, we have constructed a large-scale symbolic melody datas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ANT&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#39033;&#30446;&#20449;&#24687;&#65292;inc&#12290;</title><link>http://arxiv.org/abs/2309.10195</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30456;&#36935;&#20877;&#23398;&#20064;&#65306;&#20943;&#36731;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-modality Meets Re-learning: Mitigating Negative Transfer in Sequential Recommendation. (arXiv:2309.10195v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ANT&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#39033;&#30446;&#20449;&#24687;&#65292;inc&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31232;&#30095;&#29992;&#25143;&#20132;&#20114;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#25512;&#33616;&#27169;&#22411;&#26159;&#21457;&#23637;&#29616;&#20195;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#39044;&#35757;&#32451;&#20174;&#30456;&#20851;&#20219;&#21153;&#65288;&#21363;&#36741;&#21161;&#20219;&#21153;&#65289;&#20013;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#35813;&#30693;&#35782;&#35843;&#25972;&#21040;&#30446;&#26631;&#20219;&#21153;&#65288;&#21363;&#30446;&#26631;&#20219;&#21153;&#65289;&#20013;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#25152;&#28508;&#21147;&#65292;&#26412;&#25991;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#30340;&#24694;&#21517;&#26157;&#33879;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#20013;&#34920;&#29616;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;ANT&#30340;&#21487;&#36801;&#31227;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ANT&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#65306;1&#65289;&#32467;&#21512;&#22810;&#27169;&#24577;&#39033;&#30446;&#20449;&#24687;&#65292;inc
&lt;/p&gt;
&lt;p&gt;
Learning effective recommendation models from sparse user interactions represents a fundamental challenge in developing modern sequential recommendation methods. Recently, pre-training-based methods have been developed to tackle this challenge. The key idea behind these methods is to learn transferable knowledge from related tasks (i.e., auxiliary tasks) via pre-training and adapt the knowledge to the task of interest (i.e., target task) to mitigate its data sparsity, thereby enabling more accurate recommendations. Though promising, in this paper, we show that existing methods suffer from the notorious negative transfer issue, where the model adapted from the pre-trained model results in worse performance compared to the model learned from scratch in the target task. To address this issue, we develop a method, denoted as ANT, for transferable sequential recommendation. Compared to existing methods, ANT mitigates negative transfer by 1) incorporating multi-modality item information, inc
&lt;/p&gt;</description></item><item><title>NineRec&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#29289;&#21697;&#30001;&#25991;&#26412;&#25551;&#36848;&#21644;&#39640;&#20998;&#36776;&#29575;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.07705</link><description>&lt;p&gt;
NineRec: &#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation. (arXiv:2309.07705v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07705
&lt;/p&gt;
&lt;p&gt;
NineRec&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#29289;&#21697;&#30001;&#25991;&#26412;&#25551;&#36848;&#21644;&#39640;&#20998;&#36776;&#29575;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#29289;&#21697;&#30340;&#21407;&#22987;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#31561;&#65289;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#31216;&#20026;MoRec&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290; MoRec&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#36731;&#26494;&#21463;&#30410;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#29305;&#24449;&#33258;&#28982;&#25903;&#25345;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#31216;&#20026;&#21487;&#36801;&#31227;&#25512;&#33616;&#31995;&#32479;&#25110;TransRec&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#19982;NLP&#21644;CV&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#22522;&#30784;&#27169;&#22411;&#30456;&#27604;&#65292;TransRec&#21462;&#24471;&#20102;&#24456;&#23567;&#30340;&#36827;&#23637;&#12290;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NineRec&#65292;&#36825;&#26159;&#19968;&#20010;TransRec&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;NineRec&#20013;&#30340;&#27599;&#20010;&#29289;&#21697;&#37117;&#30001;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#21644;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;&#36890;&#36807;NineRec&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;Tran
&lt;/p&gt;
&lt;p&gt;
Learning a recommender system model from an item's raw modality features (such as image, text, audio, etc.), called MoRec, has attracted growing interest recently. One key advantage of MoRec is that it can easily benefit from advances in other fields, such as natural language processing (NLP) and computer vision (CV). Moreover, it naturally supports transfer learning across different systems through modality features, known as transferable recommender systems, or TransRec.  However, so far, TransRec has made little progress, compared to groundbreaking foundation models in the fields of NLP and CV. The lack of large-scale, high-quality recommendation datasets poses a major obstacle. To this end, we introduce NineRec, a TransRec dataset suite that includes a large-scale source domain recommendation dataset and nine diverse target domain recommendation datasets. Each item in NineRec is represented by a text description and a high-resolution cover image. With NineRec, we can implement Tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05809</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#20013;&#37319;&#29992;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#37319;&#29992;&#19968;&#31181;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#65292;&#21521;&#27599;&#20010;&#29992;&#25143;&#25552;&#20379;&#30456;&#21516;&#31243;&#24230;&#30340;&#35299;&#37322;&#65292;&#32780;&#19981;&#32771;&#34385;&#20182;&#20204;&#30340;&#20010;&#20307;&#38656;&#27714;&#21644;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#22823;&#22810;&#20197;&#38745;&#24577;&#21644;&#38750;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#22522;&#20110;&#20854;&#38656;&#27714;&#21644;&#20559;&#22909;&#36827;&#34892;&#20132;&#20114;&#12289;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65288;&#22522;&#26412;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#65289;&#65292;&#24182;&#22312;&#36879;&#26126;&#30340;&#25512;&#33616;&#21644;&#20852;&#36259;&#24314;&#27169;&#24212;&#29992;&#65288;RIMA&#65289;&#20013;&#23454;&#29616;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#65292;&#20197;&#35843;&#26597;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#23545;&#29992;&#25143;&#23545;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
&lt;/p&gt;</description></item></channel></rss>