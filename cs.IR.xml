<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>MarineVRS&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#28023;&#27915;&#39046;&#22495;&#35774;&#35745;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23545;&#35937;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22823;&#37327;&#25968;&#25454;&#12289;&#36974;&#25377;&#12289;&#27169;&#31946;&#12289;&#20302;&#29031;&#26126;&#31561;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#36824;&#26032;&#22686;&#20102;&#19968;&#20010;&#35299;&#37322;&#27169;&#22359;&#65292;&#36755;&#20986;&#26597;&#35810;&#25152;&#28041;&#21450;&#23545;&#35937;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04593</link><description>&lt;p&gt;
MarineVRS: &#20855;&#26377;&#35821;&#20041;&#29702;&#35299;&#35299;&#37322;&#24615;&#30340;&#28023;&#27915;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MarineVRS: Marine Video Retrieval System with Explainability via Semantic Understanding. (arXiv:2306.04593v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04593
&lt;/p&gt;
&lt;p&gt;
MarineVRS&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#28023;&#27915;&#39046;&#22495;&#35774;&#35745;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23545;&#35937;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#22823;&#37327;&#25968;&#25454;&#12289;&#36974;&#25377;&#12289;&#27169;&#31946;&#12289;&#20302;&#29031;&#26126;&#31561;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#36824;&#26032;&#22686;&#20102;&#19968;&#20010;&#35299;&#37322;&#27169;&#22359;&#65292;&#36755;&#20986;&#26597;&#35810;&#25152;&#28041;&#21450;&#23545;&#35937;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#19988;&#21487;&#38752;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#28023;&#27915;&#29615;&#22659;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#30340;&#23494;&#38598;&#21644;&#37325;&#22797;&#25968;&#25454;&#12289;&#36974;&#25377;&#12289;&#27169;&#31946;&#12289;&#20302;&#29031;&#26126;&#26465;&#20214;&#21644;&#25277;&#35937;&#26597;&#35810;&#31561;&#22810;&#31181;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MarineVRS&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#28023;&#27915;&#39046;&#22495;&#35774;&#35745;&#30340;&#26032;&#39062;&#19988;&#28789;&#27963;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#12290;MarineVRS&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23545;&#35937;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#22320;&#25628;&#32034;&#21644;&#20998;&#26512;&#22823;&#37327;&#30340;&#27700;&#19979;&#35270;&#39057;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#20801;&#35768;&#29992;&#25143;&#32034;&#24341;&#19968;&#31995;&#21015;&#22270;&#20687;&#25110;&#35270;&#39057;&#24182;&#20351;&#29992;&#33258;&#30001;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#31995;&#32479;&#36824;&#21253;&#25324;&#19968;&#20010;&#39069;&#22806;&#30340;&#35299;&#37322;&#27169;&#22359;&#65292;&#36755;&#20986;&#36755;&#20837;&#26597;&#35810;&#25152;&#28041;&#21450;&#23545;&#35937;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#35813;&#21151;&#33021;&#20801;&#35768;&#29992;&#25143;&#22312;&#26816;&#32034;&#30340;&#35270;&#39057;&#20013;&#20934;&#30830;&#35782;&#21035;&#21644;&#20998;&#31163;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MarineVRS&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#28023;&#27915;&#35270;&#39057;&#25968;&#25454;&#26816;&#32034;&#25552;&#20379;&#20102;&#21487;&#38752;&#21644;&#30452;&#35266;&#30340;&#25628;&#32034;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a video retrieval system that is robust and reliable, especially for the marine environment, is a challenging task due to several factors such as dealing with massive amounts of dense and repetitive data, occlusion, blurriness, low lighting conditions, and abstract queries. To address these challenges, we present MarineVRS, a novel and flexible video retrieval system designed explicitly for the marine domain. MarineVRS integrates state-of-the-art methods for visual and linguistic object representation to enable efficient and accurate search and analysis of vast volumes of underwater video data. In addition, unlike the conventional video retrieval system, which only permits users to index a collection of images or videos and search using a free-form natural language sentence, our retrieval system includes an additional Explainability module that outputs the segmentation masks of the objects that the input query referred to. This feature allows users to identify and isolate spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#26412;&#20307;&#30340;&#32422;&#26463;&#25512;&#33616;&#31995;&#32479;&#26469;&#21305;&#37197;&#21361;&#26426;&#31649;&#29702;&#27169;&#25311;&#20013;&#30340;&#39550;&#39542;&#21592;&#19982;&#36710;&#36742;&#65292;&#20197;&#25903;&#25345;&#22312;&#20154;&#21475;&#30095;&#25955;&#20013;&#30340;&#24212;&#24613;/&#30095;&#25955;&#36710;&#36742;&#30340;&#30095;&#25955;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.04553</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#30340;&#21361;&#26426;&#31649;&#29702;&#27169;&#25311;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Constraint-based recommender system for crisis management simulations. (arXiv:2306.04553v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#26412;&#20307;&#30340;&#32422;&#26463;&#25512;&#33616;&#31995;&#32479;&#26469;&#21305;&#37197;&#21361;&#26426;&#31649;&#29702;&#27169;&#25311;&#20013;&#30340;&#39550;&#39542;&#21592;&#19982;&#36710;&#36742;&#65292;&#20197;&#25903;&#25345;&#22312;&#20154;&#21475;&#30095;&#25955;&#20013;&#30340;&#24212;&#24613;/&#30095;&#25955;&#36710;&#36742;&#30340;&#30095;&#25955;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#30095;&#25955;&#30340;&#24773;&#22659;&#19979;&#65292;&#26377;&#20123;&#24066;&#27665;/&#24535;&#24895;&#32773;&#21487;&#33021;&#24076;&#26395;&#24182;&#33021;&#22815;&#36890;&#36807;&#33258;&#24049;&#30340;&#36710;&#36742;&#26469;&#25588;&#21161;&#24212;&#24613;/&#30095;&#25955;&#36710;&#36742;&#30340;&#30095;&#25955;&#22256;&#38590;&#30340;&#20154;&#32676;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#21361;&#26426;&#31649;&#29702;&#27169;&#25311;&#31995;&#32479;&#20013;&#28155;&#21152;&#19968;&#20010;&#39550;&#39542;&#21592;/&#36710;&#36742;&#21305;&#37197;&#25512;&#33616;&#27169;&#22359;&#26469;&#25903;&#25345;&#36825;&#19968;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#27169;&#22411;&#21270;&#21644;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#30340;&#22522;&#20110;&#32422;&#26463;&#30340;&#21361;&#26426;&#31649;&#29702;&#27169;&#25311;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of the evacuation of populations, some citizens/volunteers may want and be able to participate in the evacuation of populations in difficulty by coming to lend a hand to emergency/evacuation vehicles with their own vehicles. One way of framing these impulses of solidarity would be to be able to list in real-time the citizens/volunteers available with their vehicles (land, sea, air, etc.), to be able to geolocate them according to the risk areas to be evacuated, and adding them to the evacuation/rescue vehicles. Because it is difficult to propose an effective real-time operational system on the field in a real crisis situation, in this work, we propose to add a module for recommending driver/vehicle pairs (with their specificities) to a system of crisis management simulation. To do that, we chose to model and develop an ontology-supported constraint-based recommender system for crisis management simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27169;&#31946;&#20559;&#22909;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;&#8221;&#65288;VPMCR&#65289;&#30340;&#26032;&#22330;&#26223;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#22312; CRS &#20013;&#30340;&#27169;&#31946;&#21644;&#27874;&#21160;&#30340;&#20559;&#22909;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#36719;&#20272;&#35745;&#26426;&#21046;&#36991;&#20813;&#36807;&#28388;&#36807;&#24230;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#31946;&#20559;&#22909;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#33719;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#33391;&#22909;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04487</link><description>&lt;p&gt;
&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;&#65306;&#33258;&#36866;&#24212;&#27169;&#31946;&#20559;&#22909;&#31574;&#30053;&#23398;&#20064;&#29992;&#20110;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Embracing Uncertainty: Adaptive Vague Preference Policy Learning for Multi-round Conversational Recommendation. (arXiv:2306.04487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27169;&#31946;&#20559;&#22909;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;&#8221;&#65288;VPMCR&#65289;&#30340;&#26032;&#22330;&#26223;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#22312; CRS &#20013;&#30340;&#27169;&#31946;&#21644;&#27874;&#21160;&#30340;&#20559;&#22909;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#36719;&#20272;&#35745;&#26426;&#21046;&#36991;&#20813;&#36807;&#28388;&#36807;&#24230;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#31946;&#20559;&#22909;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#33719;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#33391;&#22909;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479; (CRS) &#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#65292;&#21160;&#24577;&#24341;&#23548;&#29992;&#25143;&#34920;&#36798;&#20559;&#22909;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340; CRS &#22522;&#26412;&#19978;&#20551;&#35774;&#29992;&#25143;&#26377;&#26126;&#30830;&#30340;&#20559;&#22909;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#23558;&#23436;&#20840;&#20449;&#20219;&#29992;&#25143;&#21453;&#39304;&#65292;&#24182;&#23558;&#25509;&#21463;&#25110;&#25298;&#32477;&#20449;&#21495;&#35270;&#20026;&#36807;&#28388;&#39033;&#30446;&#21644;&#20943;&#23569;&#20505;&#36873;&#31354;&#38388;&#30340;&#24378;&#25351;&#26631;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#28388;&#36807;&#24230;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#24448;&#24448;&#26159;&#27169;&#31946;&#21644;&#27874;&#21160;&#30340;&#65292;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#20182;&#20204;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#24895;&#26395;&#21644;&#20915;&#31574;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#65292;&#31216;&#20026;&#8220;&#27169;&#31946;&#20559;&#22909;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;&#8221;&#65288;VPMCR&#65289;&#65292;&#23427;&#32771;&#34385;&#21040;&#29992;&#25143;&#22312; CRS &#20013;&#30340;&#27169;&#31946;&#21644;&#27874;&#21160;&#30340;&#20559;&#22909;&#12290;VPMCR &#37319;&#29992;&#36719;&#20272;&#35745;&#26426;&#21046;&#20026;&#25152;&#26377;&#20505;&#36873;&#39033;&#30446;&#20998;&#37197;&#38750;&#38646;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#33258;&#28982;&#22320;&#36991;&#20813;&#20102;&#36807;&#28388;&#36807;&#24230;&#30340;&#38382;&#39064;&#12290;&#22312; VPMCR &#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#31946;&#20559;&#22909;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20559;&#22909;&#24341;&#23548;&#26469;&#23398;&#20064; CRS &#20195;&#29702;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340; VPMCR &#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommendation systems (CRS) effectively address information asymmetry by dynamically eliciting user preferences through multi-turn interactions. Existing CRS widely assumes that users have clear preferences. Under this assumption, the agent will completely trust the user feedback and treat the accepted or rejected signals as strong indicators to filter items and reduce the candidate space, which may lead to the problem of over-filtering. However, in reality, users' preferences are often vague and volatile, with uncertainty about their desires and changing decisions during interactions.  To address this issue, we introduce a novel scenario called Vague Preference Multi-round Conversational Recommendation (VPMCR), which considers users' vague and volatile preferences in CRS.VPMCR employs a soft estimation mechanism to assign a non-zero confidence score for all candidate items to be displayed, naturally avoiding the over-filtering problem. In the VPMCR setting, we introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RD-Suite&#30340;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35299;&#20915;&#25490;&#21517;&#27169;&#22411;&#33976;&#39311;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04455</link><description>&lt;p&gt;
RD-Suite: &#19968;&#20010;&#29992;&#20110;&#25490;&#21517;&#33976;&#39311;&#22522;&#20934;&#27979;&#35797;&#30340;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
RD-Suite: A Benchmark for Ranking Distillation. (arXiv:2306.04455v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RD-Suite&#30340;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35299;&#20915;&#25490;&#21517;&#27169;&#22411;&#33976;&#39311;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#27169;&#22411;&#30340;&#33976;&#39311;&#24050;&#25104;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RD-Suite&#30340;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#26159;&#30001;4&#20010;&#22823;&#22411;&#23454;&#38469;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#20219;&#21153;&#22871;&#20214;&#65292;&#20197;&#35299;&#20915;&#27492;&#31867;&#27169;&#22411;&#30340;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;DPVP&#26041;&#27861;&#65292;&#29992;&#20110;&#22806;&#21334;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#23545;&#21830;&#24215;&#21644;&#39135;&#21697;&#30340;&#21452;&#37325;&#20559;&#22909;&#20197;&#21450;&#22312;&#19968;&#22825;&#20013;&#30340;&#19981;&#21516;&#26102;&#27573;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.04370</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21608;&#26399;&#21464;&#21270;&#20559;&#22909;&#30340;&#22806;&#21334;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Dual Period-Varying Preferences for Takeaway Recommendation. (arXiv:2306.04370v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;DPVP&#26041;&#27861;&#65292;&#29992;&#20110;&#22806;&#21334;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#23545;&#21830;&#24215;&#21644;&#39135;&#21697;&#30340;&#21452;&#37325;&#20559;&#22909;&#20197;&#21450;&#22312;&#19968;&#22825;&#20013;&#30340;&#19981;&#21516;&#26102;&#27573;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#21334;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#31934;&#20934;&#25552;&#20379;&#31526;&#21512;&#29992;&#25143;&#20852;&#36259;&#30340;&#21830;&#24215;&#21644;&#39135;&#29289;&#65292;&#24050;&#32463;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#30340;&#26085;&#24120;&#29983;&#27963;&#25552;&#20379;&#26381;&#21153;&#12290;&#19982;&#20256;&#32479;&#25512;&#33616;&#19981;&#21516;&#65292;&#22806;&#21334;&#25512;&#33616;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#21452;&#37325;&#20132;&#20114;&#24863;&#30693;&#20559;&#22909;&#24314;&#27169;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#36890;&#24120;&#20851;&#27880;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#21333;&#19968;&#20559;&#22909;&#65292;&#32780;&#22806;&#21334;&#25512;&#33616;&#38656;&#35201;&#20840;&#38754;&#32771;&#34385;&#29992;&#25143;&#23545;&#21830;&#24215;&#21644;&#39135;&#21697;&#30340;&#21452;&#37325;&#20559;&#22909;&#12290;(2) &#21608;&#26399;&#24615;&#21464;&#21270;&#20559;&#22909;&#24314;&#27169;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#36890;&#24120;&#20174;&#20250;&#35805;&#32423;&#21035;&#25110;&#26085;&#32423;&#21035;&#30340;&#35282;&#24230;&#26469;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#30340;&#36830;&#32493;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#30340;&#22806;&#21334;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22312;&#26089;&#26216;&#12289;&#20013;&#21320;&#12289;&#26202;&#19978;&#21644;&#28145;&#22812;&#31561;&#26102;&#27573;&#37117;&#20250;&#26377;&#26174;&#33879;&#30340;&#21464;&#21270;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#21608;&#26399;&#21464;&#21270;&#20559;&#22909;&#30340;&#22806;&#21334;&#25512;&#33616;&#24314;&#27169;&#65288;DPVP&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#37325;&#20132;&#20114;&#24863;&#30693;&#27169;&#22359;&#65292;&#26088;&#22312;
&lt;/p&gt;
&lt;p&gt;
Takeaway recommender systems, which aim to accurately provide stores that offer foods meeting users' interests, have served billions of users in our daily life. Different from traditional recommendation, takeaway recommendation faces two main challenges: (1) Dual Interaction-Aware Preference Modeling. Traditional recommendation commonly focuses on users' single preferences for items while takeaway recommendation needs to comprehensively consider users' dual preferences for stores and foods. (2) Period-Varying Preference Modeling. Conventional recommendation generally models continuous changes in users' preferences from a session-level or day-level perspective. However, in practical takeaway systems, users' preferences vary significantly during the morning, noon, night, and late night periods of the day. To address these challenges, we propose a Dual Period-Varying Preference modeling (DPVP) for takeaway recommendation. Specifically, we design a dual interaction-aware module, aiming to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;Egocentric Text-Video Retrieval&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#65292;&#38024;&#23545;"frame length bias"&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#32416;&#27491;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04345</link><description>&lt;p&gt;
"Egocentric Text-Video Retrieval&#30340;&#25361;&#25112;&#32508;&#36848;"
&lt;/p&gt;
&lt;p&gt;
An Overview of Challenges in Egocentric Text-Video Retrieval. (arXiv:2306.04345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;Egocentric Text-Video Retrieval&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#65292;&#38024;&#23545;"frame length bias"&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#32416;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#38754;&#20020;&#30528;&#26469;&#33258;&#22810;&#20010;&#26041;&#38754;&#30340;&#20559;&#24046;&#31561;&#25361;&#25112;&#65292;&#26412;&#25991;&#38024;&#23545;&#19968;&#20123;&#20559;&#24046;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#38416;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24103;&#38271;&#20559;&#24046;&#32416;&#27491;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Text-video retrieval contains various challenges, including biases coming from diverse sources. We highlight some of them supported by illustrations to open a discussion. Besides, we address one of the biases, frame length bias, with a simple method which brings a very incremental but promising increase. We conclude with future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#38382;&#31572;&#20013;&#20256;&#32479;&#26041;&#27861;&#30340;&#28431;&#27934;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04293</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#20250;&#35805;&#20381;&#36182;&#24314;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#38382;&#31572;&#30340;&#30701;&#35821;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Phrase Retrieval for Open-Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning. (arXiv:2306.04293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#38382;&#31572;&#20013;&#20256;&#32479;&#26041;&#27861;&#30340;&#28431;&#27934;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#38382;&#31572;(ODConvQA)&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19968;&#20010;&#36861;&#28335;-&#38405;&#35835;&#22120;&#27169;&#22411;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#20854;&#22312;ODConvQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#21040;&#24207;&#21015;&#25490;&#24207;&#30340;&#27010;&#24565;&#24863;&#30693;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#65288;SRC&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#24863;&#30693;&#32534;&#30721;&#22120;&#27169;&#22359;&#25429;&#33719;&#36755;&#20837;&#23398;&#20064;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#27169;&#22359;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#25353;&#39034;&#24207;&#29983;&#25104;&#25972;&#20010;&#23398;&#20064;&#36335;&#24452;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04234</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#21040;&#24207;&#21015;&#25490;&#24207;&#30340;&#27010;&#24565;&#24863;&#30693;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Set-to-Sequence Ranking-based Concept-aware Learning Path Recommendation. (arXiv:2306.04234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04234
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#21040;&#24207;&#21015;&#25490;&#24207;&#30340;&#27010;&#24565;&#24863;&#30693;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#65288;SRC&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#24863;&#30693;&#32534;&#30721;&#22120;&#27169;&#22359;&#25429;&#33719;&#36755;&#20837;&#23398;&#20064;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#27169;&#22359;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#25353;&#39034;&#24207;&#29983;&#25104;&#25972;&#20010;&#23398;&#20064;&#36335;&#24452;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#25945;&#32946;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#20010;&#24615;&#21270;&#25945;&#32946;&#25512;&#33616;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#36335;&#24452;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;&#29992;&#25143;&#22312;&#27599;&#20010;&#20250;&#35805;&#20013;&#29983;&#25104;&#21644;&#25512;&#33616;&#25972;&#20010;&#23398;&#20064;&#36335;&#24452;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#36335;&#24452;&#20013;&#27010;&#24565;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#38598;&#21512;&#21040;&#24207;&#21015;&#25490;&#24207;&#30340;&#27010;&#24565;&#24863;&#30693;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#65288;SRC&#65289;&#65292;&#35813;&#26694;&#26550;&#23558;&#25512;&#33616;&#20219;&#21153;&#22312;&#38598;&#21512;&#21040;&#24207;&#21015;&#30340;&#33539;&#24335;&#19979;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#24565;&#24863;&#30693;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21487;&#20197;&#25429;&#33719;&#36755;&#20837;&#23398;&#20064;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#36755;&#20986;&#39304;&#36865;&#21040;&#35299;&#30721;&#22120;&#27169;&#22359;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#22788;&#29702;&#23398;&#20064;&#21644;&#30446;&#26631;&#27010;&#24565;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#25353;&#39034;&#24207;&#29983;&#25104;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31574;&#30053;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#36741;&#21161;&#27169;&#22359;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36335;&#24452;&#30340;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;SRC&#26694;&#26550;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of the online education system, personalized education recommendation has played an essential role. In this paper, we focus on developing path recommendation systems that aim to generating and recommending an entire learning path to the given user in each session. Noticing that existing approaches fail to consider the correlations of concepts in the path, we propose a novel framework named Set-to-Sequence Ranking-based Concept-aware Learning Path Recommendation (SRC), which formulates the recommendation task under a set-to-sequence paradigm. Specifically, we first design a concept-aware encoder module which can capture the correlations among the input learning concepts. The outputs are then fed into a decoder module that sequentially generates a path through an attention mechanism that handles correlations between the learning and target concepts. Our recommendation policy is optimized by policy gradient. In addition, we also introduce an auxiliary module based on 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SANGEET&#30340;&#22522;&#20110;XML&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23384;&#20648;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20316;&#21697;&#30340;&#20840;&#38754;&#20449;&#24687;&#65292;&#24182;&#25903;&#25345;&#20174;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#36827;&#34892;&#38899;&#20048;&#20449;&#24687;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.04148</link><description>&lt;p&gt;
SANGEET: &#19968;&#31181;&#22522;&#20110;XML&#30340;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#30740;&#31350;&#24320;&#25918;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet. (arXiv:2306.04148v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SANGEET&#30340;&#22522;&#20110;XML&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23384;&#20648;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20316;&#21697;&#30340;&#20840;&#38754;&#20449;&#24687;&#65292;&#24182;&#25903;&#25345;&#20174;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#36827;&#34892;&#38899;&#20048;&#20449;&#24687;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#20016;&#23500;&#38899;&#20048;&#25968;&#25454;&#38598;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#19987;&#27880;&#20110;&#23384;&#20648;&#35821;&#38899;&#25110;&#20048;&#22120;&#24405;&#38899;&#25968;&#25454;&#65292;&#24182;&#24573;&#30053;&#20102;&#20854;&#35270;&#35273;&#34920;&#29616;&#21644;&#26816;&#32034;&#38656;&#27714;&#12290;&#26412;&#25991;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#21517;&#20026;SANGEET&#30340;&#22522;&#20110;XML&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#23384;&#20648;&#33879;&#21517;&#38899;&#20048;&#23398;&#23478;Pt. Vishnu Narayan Bhatkhande&#21019;&#20316;&#30340;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20316;&#21697;&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;SANGEET&#20197;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#20445;&#23384;&#20102;&#20219;&#20309;&#32473;&#23450;&#20316;&#21697;&#30340;&#25152;&#38656;&#20449;&#24687;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#12289;&#32467;&#26500;&#12289;&#31526;&#35760;&#12289;&#33410;&#22863;&#21644;&#26059;&#24459;&#20449;&#24687;&#65292;&#20197;&#20415;&#20110;&#38899;&#20048;&#20449;&#24687;&#30340;&#26131;&#20110;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#20026;&#38899;&#20048;&#20449;&#24687;&#30740;&#31350;&#20219;&#21153;&#25552;&#20379;&#30495;&#23454;&#22522;&#30784;&#20449;&#24687;&#65292;&#22240;&#27492;&#25903;&#25345;&#20174;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#36827;&#34892;&#22810;&#39033;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#28436;&#31034;&#20854;&#22312;&#26059;&#24459;&#39044;&#27979;&#20219;&#21153;&#21644;&#27468;&#26354;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is very important to access a rich music dataset that is useful in a wide variety of applications. Currently, available datasets are mostly focused on storing vocal or instrumental recording data and ignoring the requirement of its visual representation and retrieval. This paper attempts to build an XML-based public dataset, called SANGEET, that stores comprehensive information of Hindustani Sangeet (North Indian Classical Music) compositions written by famous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the required information of any given composition including metadata, structural, notational, rhythmic, and melodic information in a standardized way for easy and efficient storage and extraction of musical information. The dataset is intended to provide the ground truth information for music information research tasks, thereby supporting several data-driven analysis from a machine learning perspective. We present the usefulness of the dataset by demonstrating i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30418;&#29366;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#32452;&#21512;&#26597;&#35810;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25903;&#25345;&#21547;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2306.04133</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#35770;&#23884;&#20837;&#22238;&#31572;&#32452;&#21512;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Compositional Queries with Set-Theoretic Embeddings. (arXiv:2306.04133v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30418;&#29366;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#32452;&#21512;&#26597;&#35810;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25903;&#25345;&#21547;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#20998;&#38754;&#23548;&#33322;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#35768;&#22810;&#37325;&#35201;&#20219;&#21153;&#20013;&#65292;&#32039;&#20945;&#32780;&#31283;&#20581;&#22320;&#34920;&#31034;&#39033;&#30446;-&#23646;&#24615;&#20851;&#31995;&#30340;&#38656;&#27714;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#39033;&#20219;&#21153;&#30340;&#19968;&#31181;&#27969;&#34892;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#30340;&#39640;&#28857;&#31215;&#34920;&#31034;&#39033;&#30446;&#20855;&#26377;&#23646;&#24615;&#65292;&#36825;&#31181;&#34920;&#31034;&#19981;&#20165;&#26159;&#23494;&#38598;&#30340;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#20462;&#27491;&#26377;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#36890;&#36807;&#21333;&#20010;&#23646;&#24615;&#65288;&#20363;&#22914;&#8220;&#21916;&#21095;&#30005;&#24433;&#8221;&#65289;&#26816;&#32034;&#39033;&#30446;&#30340;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21521;&#37327;&#23884;&#20837;&#24182;&#19981;&#22826;&#20934;&#30830;&#25903;&#25345;&#32452;&#21512;&#26597;&#35810;&#65288;&#20363;&#22914;&#26082;&#26159;&#21916;&#21095;&#30005;&#24433;&#21448;&#26159;&#33521;&#22269;&#30340;&#65292;&#20294;&#19981;&#26159;&#28010;&#28459;&#30005;&#24433;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38598;&#21512;&#35770;&#32452;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#29992;&#30418;&#29366;&#23884;&#20837;&#26367;&#25442;&#21521;&#37327;&#65292;&#30418;&#29366;&#23884;&#20837;&#26159;&#19968;&#31181;&#21306;&#22495;&#20026;&#22522;&#30784;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#21487;&#23398;&#20064;&#30340;&#25991;&#27663;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#26597;&#35810;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#30418;&#29366;&#23884;&#20837;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to compactly and robustly represent item-attribute relations arises in many important tasks, such as faceted browsing and recommendation systems. A popular machine learning approach for this task denotes that an item has an attribute by a high dot-product between vectors for the item and attribute -- a representation that is not only dense, but also tends to correct noisy and incomplete data. While this method works well for queries retrieving items by a single attribute (such as \emph{movies that are comedies}), we find that vector embeddings do not so accurately support compositional queries (such as movies that are comedies and British but not romances). To address these set-theoretic compositions, this paper proposes to replace vectors with box embeddings, a region-based representation that can be thought of as learnable Venn diagrams. We introduce a new benchmark dataset for compositional queries, and present experiments and analysis providing insights into the behavior o
&lt;/p&gt;</description></item><item><title>PANE-GNN&#27169;&#22411;&#32479;&#19968;&#20102;&#29992;&#25143;&#30340;&#27491;&#21453;&#39304;&#21453;&#39304;&#20449;&#24687;&#65292;&#37319;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23884;&#20837;&#34920;&#36798;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04095</link><description>&lt;p&gt;
PANE-GNN&#65306;&#32479;&#19968;&#27491;&#21453;&#39304;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
PANE-GNN: Unifying Positive and Negative Edges in Graph Neural Networks for Recommendation. (arXiv:2306.04095v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04095
&lt;/p&gt;
&lt;p&gt;
PANE-GNN&#27169;&#22411;&#32479;&#19968;&#20102;&#29992;&#25143;&#30340;&#27491;&#21453;&#39304;&#21453;&#39304;&#20449;&#24687;&#65292;&#37319;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23884;&#20837;&#34920;&#36798;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#21521;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#26469;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#20511;&#37492;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#29992;&#25143;&#30340;&#31215;&#26497;&#21453;&#39304;&#32780;&#24573;&#35270;&#20102;&#28040;&#26497;&#21453;&#39304;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#27169;&#22411;PANE-GNN&#65292;&#29992;&#20110;&#32479;&#19968;&#27491;&#21453;&#39304;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#32467;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#21453;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;PANE-GNN&#23558;&#21407;&#22987;&#35780;&#20998;&#22270;&#20998;&#25104;&#22522;&#20110;&#27491;&#21453;&#39304;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#20108;&#20998;&#22270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23884;&#20837;&#65292;&#21363;&#20852;&#36259;&#23884;&#20837;&#21644;&#19981;&#20852;&#36259;&#23884;&#20837;&#65292;&#26469;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#26368;&#32456;&#30340;&#25512;&#33616;&#20998;&#25968;&#26159;&#22522;&#20110;&#36825;&#20004;&#20010;&#23884;&#20837;&#30340;&#32452;&#21512;&#35745;&#31639;&#24471;&#20986;&#30340;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PANE-GNN&#22312;&#25512;&#33616;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in addressing the issue of information overload by delivering personalized recommendations to users. In recent years, there has been a growing interest in leveraging graph neural networks (GNNs) for recommender systems, capitalizing on advancements in graph representation learning. These GNN-based models primarily focus on analyzing users' positive feedback while overlooking the valuable insights provided by their negative feedback. In this paper, we propose PANE-GNN, an innovative recommendation model that unifies Positive And Negative Edges in Graph Neural Networks for recommendation. By incorporating user preferences and dispreferences, our approach enhances the capability of recommender systems to offer personalized suggestions. PANE-GNN first partitions the raw rating graph into two distinct bipartite graphs based on positive and negative feedback. Subsequently, we employ two separate embeddings, the interest embedding and the disinterest em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#23545;&#25968;&#27169;&#22411;&#65288;MoL&#65289;&#30340;&#38750;&#28857;&#31215;&#26816;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#22522;&#26412;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#38271;&#23614;&#25968;&#25454;&#20013;&#12290;&#32467;&#21512;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#65288;h-indexer&#65289;&#65292;&#26412;&#25991;&#25104;&#21151;&#23558;MoL&#25193;&#23637;&#21040;&#21333;&#20010;GPU&#19978;&#30340;100M&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.04039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21152;&#36895;&#22120;&#19978;&#30340;&#31070;&#32463;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Neural Retrieval on Accelerators. (arXiv:2306.04039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#23545;&#25968;&#27169;&#22411;&#65288;MoL&#65289;&#30340;&#38750;&#28857;&#31215;&#26816;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#22522;&#26412;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#38271;&#23614;&#25968;&#25454;&#20013;&#12290;&#32467;&#21512;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#65288;h-indexer&#65289;&#65292;&#26412;&#25991;&#25104;&#21151;&#23558;MoL&#25193;&#23637;&#21040;&#21333;&#20010;GPU&#19978;&#30340;100M&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#38656;&#35201;&#20174;&#22823;&#37327;&#25991;&#26412;&#20013;&#25214;&#20986;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#20854;&#20013;&#65292;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#65288;item&#65289;&#30340;&#30456;&#20284;&#24230;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#20851;&#38190;&#12290;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#23558;&#29992;&#25143;&#19982;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#34920;&#31034;&#20026;&#20004;&#20010;&#23398;&#20064;&#23884;&#20837;&#30340;&#28857;&#31215;&#65292;&#21448;&#34987;&#31216;&#20026;&#26368;&#22823;&#20869;&#31215;&#26816;&#32034;&#65288;MIPS&#65289;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#25928;&#29575;&#65292;&#20294;&#26159;&#23427;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#36895;&#22120;&#30340;&#38750;&#28857;&#31215;&#26816;&#32034;&#26041;&#27861;&#65306;&#28151;&#21512;&#23545;&#25968;&#27169;&#22411;&#65288;Mixture of Logits&#65292;MoL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#22522;&#26412;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#38271;&#23614;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#36824;&#32467;&#21512;&#19968;&#31181;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#65288;h-indexer&#65289;&#23558;MoL&#25193;&#23637;&#21040;&#21333;&#20010;GPU&#19978;&#30340;100M&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency
&lt;/p&gt;</description></item><item><title>KADS&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#25351;&#21335;&#20013;&#24471;&#20986;&#30340;&#26126;&#30830;&#35828;&#26126;&#26469;&#26500;&#24314;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03959</link><description>&lt;p&gt;
&#21033;&#29992;&#26174;&#24335;&#36807;&#31243;&#35828;&#26126;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction. (arXiv:2306.03959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03959
&lt;/p&gt;
&lt;p&gt;
KADS&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#25351;&#21335;&#20013;&#24471;&#20986;&#30340;&#26126;&#30830;&#35828;&#26126;&#26469;&#26500;&#24314;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#36890;&#24120;&#35201;&#27714;&#20195;&#29702;&#25353;&#29031;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#31243;&#24207;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21463;&#38480;&#29615;&#22659;&#19979;&#33258;&#21160;&#21270;&#36825;&#20123;&#23545;&#35805;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#24191;&#27867;&#37096;&#32626;&#21463;&#21040;&#25152;&#38656;&#30340;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#26500;&#24314;&#23545;&#35805;&#31995;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#25351;&#21335;&#20013;&#24471;&#20986;&#30340;&#26126;&#30830;&#35828;&#26126;&#65292;&#20363;&#22914;&#20844;&#21496;&#25919;&#31574;&#25110;&#23458;&#25143;&#26381;&#21153;&#25163;&#20876;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30693;&#35782;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479; (KADS) &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#26816;&#32034;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#35813;&#27169;&#22359;&#20174;&#39044;&#23450;&#20041;&#30340;&#25919;&#31574;&#38598;&#21512;&#20013;&#26816;&#32034;&#27010;&#36848;&#30456;&#20851;&#31243;&#24207;&#30340;&#25991;&#26723;&#65292;&#24182;&#32473;&#20986;&#29992;&#25143;&#20195;&#29702;&#20132;&#20114;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#29992;&#23545;&#35805;&#25991;&#26723;&#21305;&#37197;&#21644;&#38754;&#21521;&#21160;&#20316;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#19982;&#37096;&#20998;&#21442;&#25968;&#20923;&#32467;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as company policies or customer service manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a large language model with a knowledge retrieval module that pulls documents outlining relevant procedures from a predefined set of policies, given a user-agent interaction. To train this system, we introduce a semi-supervised pre-training scheme that employs dialogue-document matching and action-oriented masked language modeling with partial parameter freezing. We evaluate the effectivenes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;bgGLUE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;NLU&#20219;&#21153;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.02349</link><description>&lt;p&gt;
bgGLUE&#65306;&#20445;&#21152;&#21033;&#20122;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark. (arXiv:2306.02349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;bgGLUE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;NLU&#20219;&#21153;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;bgGLUE&#65288;&#20445;&#21152;&#21033;&#20122;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#24207;&#21015;&#26631;&#35760;&#12289;&#25991;&#26723;&#32423;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#30340;NLU&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;&#20445;&#21152;&#21033;&#20122;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#36328;&#36275;&#20102;&#20061;&#20010;&#20219;&#21153;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#32467;&#26524;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;bgGLUE&#19982;&#24494;&#35843;&#21644;&#35780;&#20272;&#20195;&#30721;&#19968;&#36215;&#20844;&#24320;&#25552;&#20379;&#65292;&#20197;&#21450;&#22312;https://bgglue.github.io/&#19978;&#25552;&#20379;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#24076;&#26395;&#23427;&#33021;&#20419;&#36827;&#26356;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present bgGLUE(Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io/, and we hope that it will enable further advancements in developi
&lt;/p&gt;</description></item><item><title>DataFinder&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#31185;&#23398;&#23478;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16636</link><description>&lt;p&gt;
DataFinder: &#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25512;&#33616;&#31185;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16636
&lt;/p&gt;
&lt;p&gt;
DataFinder&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#31185;&#23398;&#23478;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#21644;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#12290;&#37492;&#20110;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20219;&#20309;&#30740;&#31350;&#38382;&#39064;&#23545;&#33021;&#22815;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#30340;&#35201;&#27714;&#37117;&#26377;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#24577;&#21644;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#19968;&#20010;&#30740;&#31350;&#24819;&#27861;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#25512;&#33616;&#23384;&#22312;&#29420;&#29305;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#25968;&#25454;&#38598;&#24456;&#38590;&#30452;&#25509;&#32034;&#24341;&#36827;&#34892;&#25628;&#32034;&#65292;&#20063;&#27809;&#26377;&#29616;&#25104;&#30340;&#35821;&#26009;&#24211;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;DataFinder&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#30340;&#36739;&#22823;&#35757;&#32451;&#38598;&#65288;17500&#20010;&#26597;&#35810;&#65289;&#21644;&#19968;&#20010;&#36739;&#23567;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#35780;&#20272;&#38598;&#65288;392&#20010;&#26597;&#35810;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We introduce a new task of recommending relevant datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To operationalize this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25345;&#32493;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#20419;&#36827;&#21644;&#32500;&#25345;&#38271;&#26399;&#21442;&#19982;&#30340;&#26368;&#20248;&#20241;&#24687;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#30450;&#30446;&#25512;&#21160;&#29992;&#25143;&#22686;&#21152;&#28040;&#36153;&#23548;&#33268;&#29123;&#23613;&#12289;&#27969;&#22833;&#21644;&#25104;&#30270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2211.13585</link><description>&lt;p&gt;
&#23398;&#20064;&#24314;&#35758;&#20241;&#24687;&#65306;&#21487;&#25345;&#32493;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement. (arXiv:2211.13585v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25345;&#32493;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#20419;&#36827;&#21644;&#32500;&#25345;&#38271;&#26399;&#21442;&#19982;&#30340;&#26368;&#20248;&#20241;&#24687;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#30450;&#30446;&#25512;&#21160;&#29992;&#25143;&#22686;&#21152;&#28040;&#36153;&#23548;&#33268;&#29123;&#23613;&#12289;&#27969;&#22833;&#21644;&#25104;&#30270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#65292;&#20294;&#26159;&#30450;&#30446;&#22320;&#25512;&#21160;&#29992;&#25143;&#22686;&#21152;&#28040;&#36153;&#20250;&#22686;&#21152;&#29123;&#23613;&#12289;&#27969;&#22833;&#29978;&#33267;&#25104;&#30270;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#20419;&#36827;&#25968;&#23383;&#31119;&#21033;&#65292;&#29616;&#22312;&#22823;&#22810;&#25968;&#24179;&#21488;&#37117;&#25552;&#20379;&#23450;&#26399;&#25552;&#31034;&#29992;&#25143;&#20241;&#24687;&#30340;&#26381;&#21153;&#65292;&#20294;&#36825;&#20123;&#26381;&#21153;&#24517;&#39035;&#25163;&#21160;&#35774;&#32622;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#29992;&#25143;&#21644;&#31995;&#32479;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20241;&#24687;&#22312;&#25512;&#33616;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20419;&#36827;&#21644;&#32500;&#25345;&#38271;&#26399;&#21442;&#19982;&#30340;&#26368;&#20248;&#20241;&#24687;&#31574;&#30053;&#12290;&#22522;&#20110;&#25512;&#33616;&#21160;&#24577;&#26131;&#21463;&#21040;&#31215;&#26497;&#21644;&#28040;&#26497;&#21453;&#39304;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#34920;&#31034;&#20026;&#19968;&#20010; Lotka-Volterra &#21160;&#24577;&#31995;&#32479;&#65292;&#20854;&#20013;&#20241;&#24687;&#20943;&#23569;&#20026;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that recommendation dynamics are susceptible to both positive and negative feedback, we cast recommendation as a Lotka-Volterra dynamical system, where breaking reduces to a problem of optimal control. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically demonstrate the utility of our approach on semi-synthetic data.
&lt;/p&gt;</description></item></channel></rss>