<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07946</link><description>&lt;p&gt;
&#30740;&#31350;&#65306;&#31038;&#20132;&#24863;&#30693;&#26102;&#38388;&#26494;&#25955;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
STUDY: Socially Aware Temporally Casual Decoder Recommender Systems. (arXiv:2306.07946v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#25968;&#37327;&#36807;&#20110;&#24222;&#22823;&#65292;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#24403;&#31038;&#20132;&#32593;&#32476;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#20570;&#20986;&#26356;&#22909;&#30340;&#25512;&#33616;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#36825;&#20123;&#32593;&#32476;&#35757;&#32451;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#12290;STUDY&#37319;&#29992;&#19968;&#20010;&#32463;&#36807;&#20462;&#25913;&#30340;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#21521;&#21069;&#20256;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23398;&#26657;&#35838;&#22530;&#32467;&#26500;&#23450;&#20041;&#31038;&#20132;&#32593;&#32476;&#30340;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#21333;&#19968;&#22343;&#21248;&#32593;&#32476;&#35774;&#35745;&#31616;&#21333;&#24615;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the overwhelming amount of data available both on and offline today, recommender systems have become much needed to help users find items tailored to their interests. When social network information exists there are methods that utilize this information to make better recommendations, however the methods are often clunky with complex architectures and training procedures. Furthermore many of the existing methods utilize graph neural networks which are notoriously difficult to train. To address this, we propose Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint inference over groups of users who are adjacent in the social network graph using a single forward pass of a modified transformer decoder network. We test our method in a school-based educational content setting, using classroom structure to define social networks. Our method outperforms both social and sequential methods while maintaining the design simplicity of a single homogeneous netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;&#35789;&#20856;&#31934;&#24230;&#25110;lexiprecision&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#39640;&#23545;&#26368;&#28385;&#24847;&#29992;&#25143;&#30340;&#25490;&#21517;&#36136;&#37327;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20498;&#25968;&#25490;&#21517;&#30340;&#28789;&#25935;&#24615;&#32570;&#20047;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07908</link><description>&lt;p&gt;
&#26368;&#20339;&#24773;&#20917;&#26816;&#32034;&#35780;&#20272;&#65306;&#29992;&#35789;&#20856;&#31934;&#24230;&#25552;&#39640;&#20498;&#25968;&#25490;&#21517;&#30340;&#28789;&#25935;&#24230;
&lt;/p&gt;
&lt;p&gt;
Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision. (arXiv:2306.07908v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;&#35789;&#20856;&#31934;&#24230;&#25110;lexiprecision&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#39640;&#23545;&#26368;&#28385;&#24847;&#29992;&#25143;&#30340;&#25490;&#21517;&#36136;&#37327;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20498;&#25968;&#25490;&#21517;&#30340;&#28789;&#25935;&#24615;&#32570;&#20047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#25490;&#21517;&#20219;&#21153;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20498;&#25968;&#25490;&#21517;&#26469;&#34913;&#37327;&#23545;&#20165;&#23545;&#19968;&#20010;&#30456;&#20851;&#39033;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#20498;&#25968;&#25490;&#21517;&#22312;&#21306;&#20998;&#31995;&#32479;&#26041;&#38754;&#26159;&#33030;&#24369;&#30340;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#22312;&#29616;&#20195;&#35780;&#20272;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#24378;&#21270;&#65292;&#20854;&#20013;&#24403;&#21069;&#30340;&#39640;&#31934;&#24230;&#31995;&#32479;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24182;&#23558;&#20854;&#19982;&#26368;&#20339;&#24773;&#20917;&#26816;&#32034;&#30340;&#27010;&#24565;&#30456;&#32852;&#31995;&#26469;&#35299;&#20915;&#20498;&#25968;&#25490;&#21517;&#30340;&#28789;&#25935;&#24615;&#32570;&#20047;&#38382;&#39064;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#23545;&#21487;&#33021;&#30340;&#21484;&#22238;&#35201;&#27714;&#19979;&#26368;&#28385;&#24847;&#30340;&#29992;&#25143;&#30340;&#25490;&#21517;&#36136;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20010;&#35266;&#28857;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#24191;&#20498;&#25968;&#25490;&#21517;&#24182;&#23450;&#20041;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20559;&#22909;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35789;&#20856;&#31934;&#24230;&#25110;lexiprecision&#12290;&#36890;&#36807;&#25968;&#23398;&#26500;&#36896;&#65292;&#25105;&#20204;&#30830;&#20445;lexiprecision&#20445;&#30041;&#20102;&#20498;&#25968;&#25490;&#21517;&#26816;&#27979;&#21040;&#30340;&#21306;&#21035;&#65292;&#21516;&#26102;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#25490;&#21517;&#19978;&#25552;&#39640;&#20102;&#28789;&#25935;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a br
&lt;/p&gt;</description></item><item><title>ReadProbe &#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#20197;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.07875</link><description>&lt;p&gt;
ReadProbe: &#19968;&#31181;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading. (arXiv:2306.07875v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07875
&lt;/p&gt;
&lt;p&gt;
ReadProbe &#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#20197;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#19981;&#23454;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#20256;&#25773;&#65292;&#20154;&#20204;&#38656;&#35201;&#24037;&#20855;&#26469;&#24110;&#21161;&#20182;&#20204;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#30340;&#21487;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#27178;&#21521;&#38405;&#35835;&#26159;&#19968;&#31181;&#36328;&#21442;&#32771;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#31574;&#30053;&#65292;&#21487;&#33021;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21517;&#20026; ReadProbe&#65292;&#23427;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#65292;&#30001; OpenAI &#30340;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24517;&#24212;&#25628;&#32034;&#24341;&#25806;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#33021;&#22815;&#20026;&#27178;&#21521;&#38405;&#35835;&#29983;&#25104;&#26377;&#29992;&#30340;&#38382;&#39064;&#65292;&#25628;&#23547;&#32593;&#32476;&#19978;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#20135;&#29983;&#33391;&#22909;&#24402;&#22240;&#30340;&#31572;&#26696;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;&#25105;&#20204;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#20110; Web &#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#28436;&#31034;&#20102; ReadProbe &#22914;&#20309;&#24110;&#21161;&#20943;&#23569;&#34987;&#34394;&#20551;&#20449;&#24687;&#35823;&#23548;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/DakeZhang1998/ReadProbe &#19978;&#33719;&#24471;&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#29256;&#26412;&#36194;&#24471;&#20102;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#34394;&#20551;&#20449;&#24687;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#19968;&#31561;&#22870;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth and spread of online misinformation, people need tools to help them evaluate the credibility and accuracy of online information. Lateral reading, a strategy that involves cross-referencing information with multiple sources, may be an effective approach to achieving this goal. In this paper, we present ReadProbe, a tool to support lateral reading, powered by generative large language models from OpenAI and the Bing search engine. Our tool is able to generate useful questions for lateral reading, scour the web for relevant documents, and generate well-attributed answers to help people better evaluate online information. We made a web-based application to demonstrate how ReadProbe can help reduce the risk of being misled by false information. The code is available at https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won the first prize in a national AI misinformation hackathon.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#22823;&#35268;&#27169;&#12289;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;KuaiSAR&#65292;&#35813;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;&#24555;&#25163;&#30701;&#35270;&#39057;&#24212;&#29992;&#31243;&#24207;&#20013;&#30495;&#23454;&#30340;&#38598;&#25104;&#25628;&#32034;&#21644;&#25512;&#33616;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.07705</link><description>&lt;p&gt;
KuaiSAR: &#19968;&#20221;&#32479;&#19968;&#30340;&#25628;&#32034;&#19982;&#25512;&#33616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KuaiSAR: A Unified Search And Recommendation Dataset. (arXiv:2306.07705v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07705
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#22823;&#35268;&#27169;&#12289;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;KuaiSAR&#65292;&#35813;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;&#24555;&#25163;&#30701;&#35270;&#39057;&#24212;&#29992;&#31243;&#24207;&#20013;&#30495;&#23454;&#30340;&#38598;&#25104;&#25628;&#32034;&#21644;&#25512;&#33616;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#21644;&#25512;&#33616;&#26381;&#21153;&#30340;&#34701;&#21512;&#26159;&#20687;&#24555;&#25163;&#21644;&#25238;&#38899;&#36825;&#26679;&#30340;&#22312;&#32447;&#20869;&#23481;&#24179;&#21488;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;S&amp;R&#24314;&#27169;&#30340;&#25972;&#21512;&#26159;&#19994;&#30028;&#23454;&#36341;&#32773;&#37319;&#29992;&#30340;&#39640;&#24230;&#30452;&#35266;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#23398;&#26415;&#30028;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#26126;&#26174;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#20135;&#19994;&#30028;&#20043;&#38388;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#30340;&#23454;&#36341;&#20043;&#38388;&#20986;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24555;&#25163;&#30340;&#19968;&#20010;&#39046;&#20808;&#30701;&#35270;&#39057;&#24212;&#29992;&#31243;&#24207;&#25910;&#38598;&#30340;&#38598;&#25104;&#25628;&#32034;&#19982;&#25512;&#33616;&#34892;&#20026;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;KuaiSAR&#12290;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;KuaiSAR&#35760;&#24405;&#20102;&#30495;&#23454;&#29992;&#25143;&#30340;&#34892;&#20026;&#65292;&#27599;&#20010;&#34892;&#20026;&#30340;&#21457;&#29983;&#26102;&#38388;&#37117;&#34987;&#31934;&#30830;&#35760;&#24405;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The confluence of Search and Recommendation services is a vital aspect of online content platforms like Kuaishou and TikTok. The integration of S&amp;R modeling is a highly intuitive approach adopted by industry practitioners. However, there is a noticeable lack of research conducted in this area within the academia, primarily due to the absence of publicly available datasets. Consequently, a substantial gap has emerged between academia and industry regarding research endeavors in this field. To bridge this gap, we introduce the first large-scale, real-world dataset KuaiSAR of integrated Search And Recommendation behaviors collected from Kuaishou, a leading short-video app in China with over 300 million daily active users. Previous research in this field has predominantly employed publicly available datasets that are semi-synthetic and simulated, with artificially fabricated search behaviors. Distinct from previous datasets, KuaiSAR records genuine user behaviors, the occurrence of each in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#31232;&#30095;&#25968;&#25454;&#20013;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;ANN&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#65292;&#29305;&#21035;&#26159;HNSW&#31639;&#27861;&#22312;&#25628;&#32034;&#31232;&#30095;&#23884;&#20837;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#30340;&#31232;&#30095;&#23884;&#20837;&#30340;SGN&#21464;&#20307;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.07607</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;ANN&#31639;&#27861;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65306;&#21345;&#26041;&#21452;&#22612;&#27169;&#22411;&#12289;HNSW&#12289;&#31526;&#21495;&#26607;&#35199;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Practice with Graph-based ANN Algorithms on Sparse Data: Chi-square Two-tower model, HNSW, Sign Cauchy Projections. (arXiv:2306.07607v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#31232;&#30095;&#25968;&#25454;&#20013;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;ANN&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#65292;&#29305;&#21035;&#26159;HNSW&#31639;&#27861;&#22312;&#25628;&#32034;&#31232;&#30095;&#23884;&#20837;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#30340;&#31232;&#30095;&#23884;&#20837;&#30340;SGN&#21464;&#20307;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#31232;&#30095;&#25968;&#25454;&#24456;&#24120;&#35265;&#12290;&#20256;&#32479;&#30340;&#8220;&#25163;&#24037;&#21046;&#20316;&#8221;&#30340;&#29305;&#24449;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#12290;&#36890;&#36807;&#35757;&#32451;&#24471;&#21040;&#30340;&#23884;&#20837;&#21521;&#37327;&#20063;&#21487;&#33021;&#38750;&#24120;&#31232;&#30095;&#65292;&#20363;&#22914;&#20351;&#29992;&#8220;ReLu&#8221;&#28608;&#27963;&#20989;&#25968;&#35757;&#32451;&#30340;&#23884;&#20837;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;ANN&#31639;&#27861;&#65288;&#20363;&#22914;&#65292;HNSW&#25110;&#20854;GPU&#29256;&#26412;SONG&#65289;&#36827;&#34892;&#31232;&#30095;&#25968;&#25454;&#20013;&#30340;&#39640;&#25928;&#25628;&#32034;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#24037;&#19994;&#23454;&#36341;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#20363;&#22914;&#25628;&#32034;&#21644;&#24191;&#21578;&#65288;&#24191;&#21578;&#25237;&#25918;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19987;&#21033;&#24191;&#21578;&#23450;&#21521;&#24212;&#29992;&#30340;&#23454;&#39564;&#65292;&#24182;&#27979;&#35797;&#20102;&#22522;&#20934;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#24191;&#21578;&#23450;&#21521;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#8220;&#20313;&#24358;&#21452;&#22612;&#8221;&#27169;&#22411;&#21644;&#24320;&#21457;&#30340;&#8220;&#21345;&#26041;&#21452;&#22612;&#8221;&#27169;&#22411;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#33021;&#22815;&#20135;&#29983;&#65288;&#38750;&#24120;&#65289;&#31232;&#30095;&#30340;&#23884;&#20837;&#65292;&#24403;&#23427;&#20204;&#19982;&#8220;ReLu&#8221;&#28608;&#27963;&#20989;&#25968;&#38598;&#25104;&#26102;&#12290;&#22312;&#23884;&#20837;&#24335;&#26816;&#32034;&#65288;EBR&#65289;&#24212;&#29992;&#20013;&#65292;&#22312;&#35757;&#32451;&#23436;&#23884;&#20837;&#21521;&#37327;&#21518;&#65292;&#19979;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#26159;&#29992;&#20110;&#26381;&#21153;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#65288;ANN&#65289;&#25628;&#32034;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;ANN&#31639;&#27861;&#21487;&#29992;&#65292;&#20294;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#24050;&#30693;&#26159;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;HNSW&#22312;&#25628;&#32034;&#31232;&#30095;&#23884;&#20837;&#65288;&#39640;&#36798;97&#65285;&#30340;&#31232;&#30095;&#24230;&#65289;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#30340;&#31232;&#30095;&#23884;&#20837;&#30340;SGN&#21464;&#20307;&#65288;&#31526;&#21495;&#26607;&#35199;&#25237;&#24433;&#65289;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#20197;&#21450;&#26469;&#33258;&#24191;&#21578;&#24212;&#29992;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse data are common. The traditional ``handcrafted'' features are often sparse. Embedding vectors from trained models can also be very sparse, for example, embeddings trained via the ``ReLu'' activation function. In this paper, we report our exploration of efficient search in sparse data with graph-based ANN algorithms (e.g., HNSW, or SONG which is the GPU version of HNSW), which are popular in industrial practice, e.g., search and ads (advertising).  We experiment with the proprietary ads targeting application, as well as benchmark public datasets. For ads targeting, we train embeddings with the standard ``cosine two-tower'' model and we also develop the ``chi-square two-tower'' model. Both models produce (highly) sparse embeddings when they are integrated with the ``ReLu'' activation function. In EBR (embedding-based retrieval) applications, after we the embeddings are trained, the next crucial task is the approximate near neighbor (ANN) search for serving. While there are many AN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07528</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65306;&#24378;&#21270;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26088;&#22312;&#36890;&#36807;&#24050;&#37096;&#32626;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#20248;&#21270;&#25490;&#21517;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32463;&#24120;&#23545;&#29992;&#25143;&#22914;&#20309;&#29983;&#25104;&#28857;&#20987;&#25968;&#25454;&#21363;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#65292;&#22240;&#27492;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#19987;&#38376;&#35843;&#25972;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#36807;&#31243;&#22312;&#19968;&#33324;&#38543;&#26426;&#28857;&#20987;&#27169;&#22411;&#19979;&#32479;&#19968;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;RL&#25216;&#26415;&#36827;&#34892;&#38750;&#21516;&#31574;&#30053;LTR&#65292;&#24182;&#25552;&#20986;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MDP&#30340;&#19987;&#38376;&#21046;&#23450;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#21435;&#20559;&#20506;&#25216;&#26415;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#30340;&#35299;&#37322;&#24615;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#25512;&#33616;&#36825;&#20123;&#25991;&#31456;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.07506</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#26032;&#38395;&#25512;&#33616;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topic-Centric Explanations for News Recommendation. (arXiv:2306.07506v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#30340;&#35299;&#37322;&#24615;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#25512;&#33616;&#36825;&#20123;&#25991;&#31456;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#32593;&#31449;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#30340;&#20852;&#36259;&#25214;&#21040;&#30456;&#20851;&#25991;&#31456;&#12290;&#28982;&#32780;&#65292;&#25512;&#33616;&#32570;&#20047;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#30340;&#19981;&#20449;&#20219;&#21644;&#25512;&#33616;&#30340;&#32570;&#20047;&#25509;&#21463;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#30340;&#26032;&#38395;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#39064;&#30340;&#35299;&#37322;&#24615;&#25512;&#33616;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#25512;&#33616;&#36825;&#20123;&#25991;&#31456;&#65292;&#21033;&#29992;&#30456;&#20851;&#20027;&#39064;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#20004;&#31181;&#29992;&#20110;&#35780;&#20272;&#20027;&#39064;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25552;&#20379;&#20102;&#36825;&#20123;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;MIND&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;NRS&#20248;&#20110;&#20854;&#20182;&#20960;&#20010;&#22522;&#32447;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommender systems (NRS) have been widely applied for online news websites to help users find relevant articles based on their interests. Recent methods have demonstrated considerable success in terms of recommendation performance. However, the lack of explanation for these recommendations can lead to mistrust among users and lack of acceptance of recommendations. To address this issue, we propose a new explainable news model to construct a topic-aware explainable recommendation approach that can both accurately identify relevant articles and explain why they have been recommended, using information from associated topics. Additionally, our model incorporates two coherence metrics applied to assess topic quality, providing measure of the interpretability of these explanations. The results of our experiments on the MIND dataset indicate that the proposed explainable NRS outperforms several other baseline systems, while it is also capable of producing interpretable topics compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.07479</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;TikTok&#21644;YouTube&#36825;&#26679;&#30340;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#24179;&#21488;&#30340;&#20915;&#31574;&#31639;&#27861;&#22609;&#36896;&#20102;&#20869;&#23481;&#29983;&#20135;&#32773;&#30340;&#28608;&#21169;&#65292;&#21253;&#25324;&#29983;&#20135;&#32773;&#22312;&#20869;&#23481;&#36136;&#37327;&#19978;&#25237;&#20837;&#22810;&#23569;&#21162;&#21147;&#12290;&#35768;&#22810;&#24179;&#21488;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#20250;&#20135;&#29983;&#36328;&#26102;&#38388;&#30340;&#28608;&#21169;&#65292;&#22240;&#20026;&#20170;&#22825;&#29983;&#20135;&#30340;&#20869;&#23481;&#20250;&#24433;&#21709;&#26410;&#26469;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20135;&#29983;&#30340;&#28608;&#21169;&#65292;&#20998;&#26512;&#20102;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#29983;&#20135;&#30340;&#20869;&#23481;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20687;Hedge&#21644;EXP3&#36825;&#26679;&#30340;&#32463;&#20856;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#29305;&#21035;&#22320;&#65292;&#20869;&#23481;&#36136;&#37327;&#22312;&#23398;&#20064;&#29575;&#26041;&#38754;&#26377;&#19978;&#38480;&#65292;&#24182;&#19988;&#38543;&#30528;&#20856;&#22411;&#23398;&#20064;&#29575;&#36827;&#23637;&#32780;&#36235;&#36817;&#20110;&#38646;&#12290;&#22312;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#24809;&#32602;&#21019;&#24314;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#29983;&#20135;&#32773;&#8212;&#8212;&#27491;&#30830;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#26032;&#39062;&#30340;&#31574;&#30053;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;BEIR&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#21442;&#32771;&#23454;&#29616;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#24615;&#33021;&#21644;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.07471</link><description>&lt;p&gt;
&#12298;&#29992;&#20110;&#32534;&#21046;BEIR&#30340;&#36164;&#28304;&#65306;&#21487;&#37325;&#22797;&#21442;&#32771;&#27169;&#22411;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#12299;
&lt;/p&gt;
&lt;p&gt;
Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. (arXiv:2306.07471v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07471
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;BEIR&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#21442;&#32771;&#23454;&#29616;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#24615;&#33021;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BEIR&#26159;&#19968;&#20010;&#36328;&#36234;18&#20010;&#19981;&#21516;&#39046;&#22495;/&#20219;&#21153;&#32452;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#30446;&#30585;&#20102;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;transformers&#22312;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19979;&#24314;&#31435;&#26816;&#32034;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#26085;&#30410;&#26222;&#21450;&#12290;&#20294;&#36825;&#33258;&#28982;&#20250;&#24341;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#22312;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#26102;&#26377;&#22810;&#26377;&#25928;&#65311;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;BEIR&#22312;&#23454;&#29616;&#20854;&#20840;&#37096;&#28508;&#21147;&#26041;&#38754;&#23384;&#22312;&#30340;&#20004;&#20010;&#32570;&#38519;&#12290;&#31532;&#19968;&#65292;&#29616;&#20195;&#31070;&#32463;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#24403;&#21069;&#30340;&#36719;&#20214;&#22522;&#30784;&#35774;&#26045;&#21019;&#24314;&#20102;&#23545;&#26032;&#25163;&#30340;&#36827;&#20837;&#38376;&#27099;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35206;&#30422;&#20004;&#20010;&#20027;&#35201;&#26816;&#32034;&#27169;&#22411;&#31867;&#30340;&#21487;&#37325;&#22797;&#21442;&#32771;&#23454;&#29616;&#12290;&#31532;&#20108;&#65292;&#34429;&#28982;BEIR&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#20294;&#27809;&#26377;&#23448;&#26041;&#25490;&#21517;&#27036;&#21487;&#36319;&#36394;&#27169;&#22411;&#24615;&#33021;&#21644;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#23448;&#26041;BEIR&#25490;&#34892;&#27036;&#65292;&#21487;&#25552;&#20132;&#32467;&#26524;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
BEIR is a benchmark dataset for zero-shot evaluation of information retrieval models across 18 different domain/task combinations. In recent years, we have witnessed the growing popularity of a representation learning approach to building retrieval models, typically using pretrained transformers in a supervised setting. This naturally begs the question: How effective are these models when presented with queries and documents that differ from the training data? Examples include searching in different domains (e.g., medical or legal text) and with different types of queries (e.g., keywords vs. well-formed questions). While BEIR was designed to answer these questions, our work addresses two shortcomings that prevent the benchmark from achieving its full potential: First, the sophistication of modern neural methods and the complexity of current software infrastructure create barriers to entry for newcomers. To this end, we provide reproducible reference implementations that cover the two m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20844;&#24179;LTR-RC&#65292;&#23427;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#65292;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.07188</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#39118;&#38505;&#25511;&#21046;&#30340;&#20844;&#24179;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Fair Learning to Rank with Distribution-free Risk Control. (arXiv:2306.07188v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20844;&#24179;LTR-RC&#65292;&#23427;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#65292;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32463;&#27982;&#20013;&#65292;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#25552;&#20379;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;LTR&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#25353;&#27604;&#20363;&#20998;&#37197;&#26333;&#20809;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20855;&#26377;&#30456;&#21516;&#30456;&#20851;&#24615;&#30340;&#39033;&#25509;&#25910;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#25968;&#26102;&#65292;&#30830;&#23450;&#24615;&#25490;&#21517;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#26333;&#20809;&#20998;&#37197;&#12290;&#38543;&#26426;LTR&#27169;&#22411;&#65292;&#21253;&#25324;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20294;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20445;&#35777;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;LTR-RC&#65292;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20844;&#24179;LTR-RC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35780;&#20998;&#20989;&#25968;&#21019;&#24314;&#38543;&#26426;LTR&#27169;&#22411;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#20844;&#24179;LTR-RC&#20351;&#29992;&#26080;&#20998;&#24067;&#24335;&#39118;&#38505;&#25511;&#21046;&#26694;&#26550;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#25552;&#20379;&#26377;&#38480;&#30340;&#26679;&#26412;&#20445;&#35777;&#12290;&#36890;&#36807;&#21478;&#22806;&#32467;&#21512;Thresholded PL&#65288;TPL&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;FairLTR-RC&#22312;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#24615;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank (LTR) methods are vital in online economies, affecting users and item providers. Fairness in LTR models is crucial to allocate exposure proportionally to item relevance. The deterministic ranking model can lead to unfair exposure distribution when items with the same relevance receive slightly different scores. Stochastic LTR models, incorporating the Plackett-Luce (PL) model, address fairness issues but have limitations in computational cost and performance guarantees. To overcome these limitations, we propose FairLTR-RC, a novel post-hoc model-agnostic method. FairLTR-RC leverages a pretrained scoring function to create a stochastic LTR model, eliminating the need for expensive training. Furthermore, FairLTR-RC provides finite-sample guarantees on a user-specified utility using distribution-free risk control framework. By additionally incorporating the Thresholded PL (TPL) model, we are able to achieve an effective trade-off between utility and fairness. Experimental
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CMR&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#65292;&#20351;&#24471;&#20559;&#22909;&#26435;&#37325;&#21487;&#20197;&#22312;&#32447;&#20248;&#21270;&#65292;&#32780;&#19981;&#29992;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05118</link><description>&lt;p&gt;
&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#30340;&#21487;&#25511;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Controllable Multi-Objective Re-ranking with Policy Hypernetworks. (arXiv:2306.05118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CMR&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#65292;&#20351;&#24471;&#20559;&#22909;&#26435;&#37325;&#21487;&#20197;&#22312;&#32447;&#20248;&#21270;&#65292;&#32780;&#19981;&#29992;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38454;&#27573;&#25490;&#21517;&#31649;&#36947;&#24050;&#25104;&#20026;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#32456;&#38454;&#27573;&#26088;&#22312;&#36820;&#22238;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#65292;&#20197;&#24179;&#34913;&#29992;&#25143;&#20559;&#22909;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#31561;&#22810;&#20010;&#35201;&#27714;&#12290;&#32447;&#24615;&#26631;&#37327;&#21270;&#26159;&#23558;&#22810;&#20010;&#35201;&#27714;&#21512;&#24182;&#20026;&#19968;&#20010;&#20248;&#21270;&#30446;&#26631;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#23450;&#30340;&#20559;&#22909;&#26435;&#37325;&#26469;&#24635;&#32467;&#36825;&#20123;&#35201;&#27714;&#12290;&#29616;&#26377;&#30340;&#26368;&#32456;&#38454;&#27573;&#25490;&#21517;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38745;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#20559;&#22909;&#26435;&#37325;&#22312;&#31163;&#32447;&#35757;&#32451;&#26399;&#38388;&#30830;&#23450;&#65292;&#24182;&#22312;&#22312;&#32447;&#26381;&#21153;&#26399;&#38388;&#20445;&#25345;&#19981;&#21464;&#12290;&#27599;&#24403;&#38656;&#35201;&#20462;&#25913;&#20559;&#22909;&#26435;&#37325;&#26102;&#65292;&#27169;&#22411;&#24517;&#39035;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#26102;&#38388;&#21644;&#36164;&#28304;&#19978;&#30340;&#28010;&#36153;&#12290;&#21516;&#26102;&#65292;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#25110;&#19981;&#21516;&#26102;&#38388;&#27573;&#65288;&#20363;&#22914;&#65292;&#22312;&#33410;&#26085;&#20419;&#38144;&#26399;&#38388;&#65289;&#30340;&#26368;&#21512;&#36866;&#26435;&#37325;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25511;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#65288;CMR&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#65292;&#20197;&#20351;&#20559;&#22909;&#26435;&#37325;&#22312;&#32447;&#20248;&#21270;&#65292;&#32780;&#19981;&#24517;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-stage ranking pipelines have become widely used strategies in modern recommender systems, where the final stage aims to return a ranked list of items that balances a number of requirements such as user preference, diversity, novelty etc. Linear scalarization is arguably the most widely used technique to merge multiple requirements into one optimization objective, by summing up the requirements with certain preference weights. Existing final-stage ranking methods often adopt a static model where the preference weights are determined during offline training and kept unchanged during online serving. Whenever a modification of the preference weights is needed, the model has to be re-trained, which is time and resources inefficient. Meanwhile, the most appropriate weights may vary greatly for different groups of targeting users or at different time periods (e.g., during holiday promotions). In this paper, we propose a framework called controllable multi-objective re-ranking (CMR) whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#25506;&#35752;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26032;&#25351;&#26631;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#27169;&#22411;&#21644;&#29992;&#25143;&#30028;&#38754;&#31561;&#65292;&#20197;&#25506;&#31350;&#23427;&#26159;&#21542;&#26159;IR&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#35813;&#30740;&#35752;&#20250;&#20851;&#27880;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22320;&#28857;&#29992;&#20110;&#25506;&#35752;&#21644;&#25506;&#32034;&#22914;&#20309;&#23558;&#29983;&#25104;&#24335;IR&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.02887</link><description>&lt;p&gt;
2023&#24180;SIGIR&#20250;&#35758;&#19978;&#30340;Gen-IR&#30740;&#35752;&#20250;&#65306;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#30340;&#39318;&#20010;&#30740;&#35752;&#20250;
&lt;/p&gt;
&lt;p&gt;
Gen-IR @ SIGIR 2023: The First Workshop on Generative Information Retrieval. (arXiv:2306.02887v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#25506;&#35752;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26032;&#25351;&#26631;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#27169;&#22411;&#21644;&#29992;&#25143;&#30028;&#38754;&#31561;&#65292;&#20197;&#25506;&#31350;&#23427;&#26159;&#21542;&#26159;IR&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#35813;&#30740;&#35752;&#20250;&#20851;&#27880;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22320;&#28857;&#29992;&#20110;&#25506;&#35752;&#21644;&#25506;&#32034;&#22914;&#20309;&#23558;&#29983;&#25104;&#24335;IR&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#22312;&#22810;&#20010;&#30740;&#31350;&#31038;&#21306;&#65288;&#20363;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65289;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#24182;&#22312;&#27969;&#34892;&#23186;&#20307;&#19978;&#22791;&#21463;&#20851;&#27880;&#12290;&#24050;&#21457;&#24067;&#20102;&#29702;&#35770;&#12289;&#23454;&#35777;&#21644;&#23454;&#38469;&#29992;&#25143;&#20135;&#21697;&#65292;&#36825;&#20123;&#20135;&#21697;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#25991;&#26723;&#65288;&#36890;&#36807;&#29983;&#25104;&#65289;&#25110;&#30452;&#25509;&#29983;&#25104;&#31572;&#26696;&#26469;&#26816;&#32034;&#25991;&#26723;&#25110;&#22238;&#31572;&#36755;&#20837;&#35831;&#27714;&#12290;&#25105;&#20204;&#24819;&#35843;&#26597;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#21482;&#26159;&#21478;&#19968;&#31181;&#36235;&#21183;&#65292;&#36824;&#26159;&#20687;&#26576;&#20123;&#20154;&#25152;&#22768;&#31216;&#30340;&#37027;&#26679;&#65292;&#26159;IR&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#38656;&#35201;&#26032;&#30340;&#25351;&#26631;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#27169;&#22411;&#12289;&#29992;&#25143;&#30028;&#38754;&#31561;&#12290;&#26412;&#27425;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#20851;&#27880;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#29983;&#25104;&#24335;IR&#25216;&#26415;&#65292;&#22914;&#25991;&#26723;&#26816;&#32034;&#21644;&#30452;&#25509;&#23454;&#29616;&#30340;&#22522;&#30784;&#31572;&#26696;&#29983;&#25104;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#19968;&#20010;&#22320;&#28857;&#29992;&#20110;&#25506;&#35752;&#21644;&#25506;&#32034;&#29983;&#25104;&#24335;IR&#22914;&#20309;&#24212;&#29992;&#20110;&#25512;&#33616;&#31561;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative information retrieval (IR) has experienced substantial growth across multiple research communities (e.g., information retrieval, computer vision, natural language processing, and machine learning), and has been highly visible in the popular press. Theoretical, empirical, and actual user-facing products have been released that retrieve documents (via generation) or directly generate answers given an input request. We would like to investigate whether end-to-end generative models are just another trend or, as some claim, a paradigm change for IR. This necessitates new metrics, theoretical grounding, evaluation methods, task definitions, models, user interfaces, etc. The goal of this workshop (https://coda.io/@sigir/gen-ir) is to focus on previously explored Generative IR techniques like document retrieval and direct Grounded Answer Generation, while also offering a venue for the discussion and exploration of how Generative IR can be applied to new domains like recommendation s
&lt;/p&gt;</description></item><item><title>TEIMMA &#26159;&#19968;&#27454;&#21487;&#20197;&#27880;&#37322;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#25968;&#23398;&#20844;&#24335;&#37325;&#29992;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#35760;&#24405;&#19981;&#21516;&#31867;&#22411;&#30340;&#37325;&#29992;&#24182;&#25903;&#25345;&#21487;&#35270;&#21270;&#26597;&#30475;&#20869;&#23481;&#37325;&#22797;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#26816;&#27979;&#25220;&#34989;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13193</link><description>&lt;p&gt;
TEIMMA&#65306;&#31532;&#19968;&#20010;&#25903;&#25345;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#25968;&#23398;&#20844;&#24335;&#20869;&#23481;&#37325;&#29992;&#27880;&#37322;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
TEIMMA: The First Content Reuse Annotator for Text, Images, and Math. (arXiv:2305.13193v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13193
&lt;/p&gt;
&lt;p&gt;
TEIMMA &#26159;&#19968;&#27454;&#21487;&#20197;&#27880;&#37322;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#25968;&#23398;&#20844;&#24335;&#37325;&#29992;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#35760;&#24405;&#19981;&#21516;&#31867;&#22411;&#30340;&#37325;&#29992;&#24182;&#25903;&#25345;&#21487;&#35270;&#21270;&#26597;&#30475;&#20869;&#23481;&#37325;&#22797;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#26816;&#27979;&#25220;&#34989;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102; TEIMMA &#24037;&#20855;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#25968;&#23398;&#20844;&#24335;&#22312;&#25991;&#26723;&#23545;&#20013;&#37325;&#29992;&#36827;&#34892;&#27880;&#37322;&#30340;&#24037;&#20855;&#12290;&#27880;&#37322;&#20869;&#23481;&#37325;&#29992;&#23545;&#20110;&#24320;&#21457;&#26816;&#27979;&#25220;&#34989;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#29616;&#23454;&#20013;&#30340;&#20869;&#23481;&#37325;&#29992;&#36890;&#24120;&#34987;&#28151;&#28102;&#65292;&#36825;&#20351;&#24471;&#35782;&#21035;&#27492;&#31867;&#24773;&#20917;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;TEIMMA &#20801;&#35768;&#36755;&#20837;&#28151;&#28102;&#31867;&#22411;&#65292;&#20197;&#20415;&#38024;&#23545;&#24050;&#30830;&#35748;&#30340;&#25220;&#34989;&#26696;&#20363;&#36827;&#34892;&#26032;&#30340;&#20998;&#31867;&#12290;&#23427;&#21487;&#20197;&#35760;&#24405; HTML &#20013;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#25968;&#23398;&#20844;&#24335;&#30340;&#19981;&#21516;&#37325;&#29992;&#31867;&#22411;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#25968;&#23398;&#30456;&#20284;&#24230;&#26816;&#27979;&#26041;&#27861;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#26597;&#30475;&#25991;&#26723;&#23545;&#20013;&#30340;&#20869;&#23481;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This demo paper presents the first tool to annotate the reuse of text, images, and mathematical formulae in a document pair -- TEIMMA. Annotating content reuse is particularly useful to develop plagiarism detection algorithms. Real-world content reuse is often obfuscated, which makes it challenging to identify such cases. TEIMMA allows entering the obfuscation type to enable novel classifications for confirmed cases of plagiarism. It enables recording different reuse types for text, images, and mathematical formulae in HTML and supports users by visualizing the content reuse in a document pair using similarity detection methods for text and math.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.04891</link><description>&lt;p&gt;
&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DELTA: Dynamic Embedding Learning with Truncated Conscious Attention for CTR Prediction. (arXiv:2305.04891v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#20135;&#21697;&#21644;&#20869;&#23481;&#25512;&#33616;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#29305;&#24449;&#23884;&#20837;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#23398;&#20064;&#22266;&#23450;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#32570;&#20047;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#34920;&#31034;&#30340;&#26426;&#21046;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#19968;&#20123;&#36817;&#26399;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#20301;&#26435;&#37325;&#25110;&#22686;&#24378;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#21463;&#21040;&#19978;&#19979;&#25991;&#20013;&#26080;&#20449;&#24687;&#25110;&#20887;&#20313;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24847;&#35782;&#21152;&#24037;&#20013;&#20840;&#23616;&#24037;&#20316;&#21306;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#21482;&#26377;&#29305;&#23450;&#30340;&#20135;&#21697;&#29305;&#24449;&#19982;&#28857;&#20987;&#34892;&#20026;&#30456;&#20851;&#65292;&#20854;&#20313;&#29305;&#24449;&#21487;&#33021;&#20250;&#22122;&#38899;&#24178;&#25200;&#65292;&#29978;&#33267;&#26377;&#23475;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;DELTA&#36827;&#34892;CTR&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction is a pivotal task in product and content recommendation, where learning effective feature embeddings is of great significance. However, traditional methods typically learn fixed feature representations without dynamically refining feature representations according to the context information, leading to suboptimal performance. Some recent approaches attempt to address this issue by learning bit-wise weights or augmented embeddings for feature representations, but suffer from uninformative or redundant features in the context. To tackle this problem, inspired by the Global Workspace Theory in conscious processing, which posits that only a specific subset of the product features are pertinent while the rest can be noisy and even detrimental to human-click behaviors, we propose a CTR model that enables Dynamic Embedding Learning with Truncated Conscious Attention for CTR prediction, termed DELTA. DELTA contains two key components: (I) conscious truncatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; KGE-SymCL&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#21306;&#20998;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.10738</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure. (arXiv:2211.10738v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; KGE-SymCL&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837; (KGE) &#26088;&#22312;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#20197;&#21463;&#30410;&#20110;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#21033;&#29992;&#20110;&#22270;&#23398;&#20064;&#65292;&#20316;&#20026;&#22686;&#24378;&#25152;&#23398;&#34920;&#31034;&#30340;&#21487;&#21306;&#20998;&#33021;&#21147;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;KG&#30340;&#22797;&#26434;&#32467;&#26500;&#20351;&#24471;&#26500;&#24314;&#36866;&#24403;&#30340;&#23545;&#27604;&#23545;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#25968;&#19981;&#22810;&#30340;&#20960;&#20010;&#23581;&#35797;&#23558;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#19982;KGE&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22823;&#22810;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Bert&#65289;&#36827;&#34892;&#23545;&#27604;&#23545;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#23436;&#20840;&#25366;&#25496;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#20869;&#30340;&#23454;&#20307;&#36890;&#24120;&#30456;&#20284;&#19988;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;KGE-SymCL&#65292;&#23427;&#22312;KG&#20013;&#25366;&#25496;&#23545;&#31216;&#32467;&#26500;&#20449;&#24687;&#20197;&#22686;&#24378;&#25152;&#23398;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models ( e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#26032;&#29305;&#24449;&#30340;&#22806;&#25512;&#65292;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.04514</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#24402;&#32435;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Feature Extrapolation: An Inductive Graph Learning Approach. (arXiv:2110.04514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#26032;&#29305;&#24449;&#30340;&#22806;&#25512;&#65292;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#32463;&#36807;&#25193;&#23637;&#65292;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#22788;&#29702;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#26032;&#29305;&#24449;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#39592;&#24178;&#32593;&#32476;&#20316;&#20026;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#23558;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65307;2&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36739;&#39640;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#29305;&#24449;-&#25968;&#25454;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23398;&#20064;&#22806;&#25512;&#26032;&#29305;&#24449;&#30340;&#23884;&#20837;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#24402;&#32435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36171;&#20104;&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We target open-world feature extrapolation problem where the feature space of input data goes through expansion and a model trained on partially observed features needs to handle new features in test data without further retraining. The problem is of much significance for dealing with features incrementally collected from different fields. To this end, we propose a new learning paradigm with graph representation and learning. Our framework contains two modules: 1) a backbone network (e.g., feedforward neural nets) as a lower model takes features as input and outputs predicted labels; 2) a graph neural network as an upper model learns to extrapolate embeddings for new features via message passing over a feature-data graph built from observed data. Based on our framework, we design two training strategies, a self-supervised approach and an inductive learning approach, to endow the model with extrapolation ability and alleviate feature-level over-fitting. We also provide theoretical analy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1808.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#23454;&#20307;&#30456;&#20851;&#24615;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.08316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#23454;&#20307;&#30456;&#20851;&#24615;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#38745;&#24577;&#35774;&#32622;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#19979;&#30740;&#31350;&#23454;&#20307;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#20307;&#24448;&#24448;&#28041;&#21450;&#35768;&#22810;&#19981;&#21516;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#23454;&#20307;&#20851;&#31995;&#38543;&#26102;&#38388;&#21464;&#24471;&#38750;&#24120;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#21147;&#20316;&#20026;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#32852;&#21512;&#26694;&#26550;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#33021;&#22815;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#23454;&#20307;&#26041;&#38754;&#65292;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/1803.07890</link><description>&lt;p&gt;
&#25512;&#33616;&#23454;&#20307;&#30340;&#26102;&#38388;&#22240;&#32032;&#30340;&#22810;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multiple Models for Recommending Temporal Aspects of Entities. (arXiv:1803.07890v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1803.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#33021;&#22815;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#23454;&#20307;&#26041;&#38754;&#65292;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#26041;&#38754;&#30340;&#25512;&#33616;&#26159;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#26032;&#20852;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#24039;&#21512;&#21644;&#31361;&#20986;&#20449;&#24687;&#65292;&#20854;&#20013;&#26174;&#30528;&#24615;&#65288;&#20363;&#22914;&#27969;&#34892;&#24230;&#65289;&#26159;&#20197;&#21069;&#24037;&#20316;&#20013;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#20294;&#26159;&#65292;&#23454;&#20307;&#26041;&#38754;&#26159;&#20855;&#26377;&#26102;&#38388;&#21160;&#24577;&#24615;&#30340;&#65292;&#32463;&#24120;&#21463;&#21040;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#22522;&#20110;&#26174;&#30528;&#24615;&#29305;&#24449;&#30340;&#26041;&#38754;&#24314;&#35758;&#21487;&#33021;&#20250;&#32473;&#20986;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#26174;&#30528;&#24615;&#36890;&#24120;&#22312;&#38271;&#26102;&#38388;&#27573;&#20869;&#32047;&#31215;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#26368;&#36817;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#19982;&#20107;&#20214;&#23454;&#20307;&#30456;&#20851;&#30340;&#35768;&#22810;&#26041;&#38754;&#24378;&#28872;&#20381;&#36182;&#20110;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#32473;&#23450;&#23454;&#20307;&#30340;&#26102;&#38388;&#26041;&#38754;&#25512;&#33616;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22810;&#20010;&#26102;&#38388;&#21644;&#31867;&#22411;&#20381;&#36182;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#65292;&#24182;&#21160;&#24577;&#26435;&#34913;&#26174;&#30528;&#24615;&#21644;&#26368;&#36817;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity aspect recommendation is an emerging task in semantic search that helps users discover serendipitous and prominent information with respect to an entity, of which salience (e.g., popularity) is the most important factor in previous work. However, entity aspects are temporally dynamic and often driven by events happening over time. For such cases, aspect suggestion based solely on salience features can give unsatisfactory results, for two reasons. First, salience is often accumulated over a long time period and does not account for recency. Second, many aspects related to an event entity are strongly time-dependent. In this paper, we study the task of temporal aspect recommendation for a given entity, which aims at recommending the most relevant aspects and takes into account time in order to improve search experience. We propose a novel event-centric ensemble ranking method that learns from multiple time and type-dependent models and dynamically trades off salience and recency c
&lt;/p&gt;</description></item></channel></rss>