<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340; K-&#21311;&#21517;&#32452;&#38431;&#31639;&#27861;-&#36830;&#32493;&#19968;&#33268;&#21152;&#26435;&#25277;&#26679;&#65288;CCWS&#65289;&#26469;&#26500;&#24314;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#29992;&#25143;&#32676;&#32452;&#65292;&#20197;&#26816;&#32034;&#20010;&#24615;&#21270;&#24191;&#21578;&#12290;</title><link>http://arxiv.org/abs/2304.13677</link><description>&lt;p&gt;
&#21033;&#29992;&#36830;&#32493;&#19968;&#33268;&#21152;&#26435;&#37319;&#26679;&#65288;CCWS&#65289;&#26500;&#24314; K-&#21311;&#21517;&#29992;&#25143;&#32452;&#38431;
&lt;/p&gt;
&lt;p&gt;
Building K-Anonymous User Cohorts with\\ Consecutive Consistent Weighted Sampling (CCWS). (arXiv:2304.13677v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340; K-&#21311;&#21517;&#32452;&#38431;&#31639;&#27861;-&#36830;&#32493;&#19968;&#33268;&#21152;&#26435;&#25277;&#26679;&#65288;CCWS&#65289;&#26469;&#26500;&#24314;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#29992;&#25143;&#32676;&#32452;&#65292;&#20197;&#26816;&#32034;&#20010;&#24615;&#21270;&#24191;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#26816;&#32034;&#20010;&#24615;&#21270;&#24191;&#21578;&#31995;&#21015;&#21644;&#21019;&#24847;&#65292;&#25968;&#23383;&#24191;&#21578;&#27491;&#22312;&#20174;&#22522;&#20110;&#25104;&#21592;&#30340;&#36523;&#20221;&#36716;&#21521;&#22522;&#20110;&#23567;&#32452;&#30340;&#36523;&#20221;&#12290;&#22312;&#36825;&#31181;&#36523;&#20221;&#21046;&#24230;&#19979;&#65292;&#38656;&#35201;&#19968;&#31181;&#20934;&#30830;&#39640;&#25928;&#30340;&#32452;&#38431;&#31639;&#27861;&#26469;&#23558;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#29992;&#25143;&#20998;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340; K-&#21311;&#21517;&#32452;&#38431;&#31639;&#27861;&#65292;&#31216;&#20026;&#36830;&#32493;&#19968;&#33268;&#21152;&#26435;&#25277;&#26679;&#65288;CCWS&#65289;&#12290;&#35813;&#26041;&#27861;&#23558;&#19968;&#33268;&#21152;&#26435;&#25277;&#26679;&#65288;p -powered&#65289;&#21644;&#20998;&#23618;&#32858;&#31867;&#30340;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#36890;&#36807;&#24378;&#21046;&#38480;&#21046;&#23567;&#32452;&#22823;&#23567;&#30340;&#19979;&#38480;&#26469;&#30830;&#20445; K-&#21311;&#21517;&#24615;&#12290;&#22312;&#30001; 70 &#22810;&#20010;&#30334;&#19975;&#29992;&#25143;&#21644;&#24191;&#21578;&#31995;&#21015;&#32452;&#25104;&#30340; LinkedIn &#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;CCWS &#30456;&#23545;&#20110;&#20960;&#31181;&#22522;&#20110;&#21704;&#24076;&#30340;&#26041;&#27861;&#65288;&#21253;&#25324; SignRP&#12289;MinHash &#20197;&#21450;&#22522;&#26412;&#30340; CWS&#65289;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
To retrieve personalized campaigns and creatives while protecting user privacy, digital advertising is shifting from member-based identity to cohort-based identity. Under such identity regime, an accurate and efficient cohort building algorithm is desired to group users with similar characteristics. In this paper, we propose a scalable $K$-anonymous cohort building algorithm called {\em consecutive consistent weighted sampling} (CCWS). The proposed method combines the spirit of the ($p$-powered) consistent weighted sampling and hierarchical clustering, so that the $K$-anonymity is ensured by enforcing a lower bound on the size of cohorts. Evaluations on a LinkedIn dataset consisting of $&gt;70$M users and ads campaigns demonstrate that CCWS achieves substantial improvements over several hashing-based methods including sign random projections (SignRP), minwise hashing (MinHash), as well as the vanilla CWS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#12300;\framework&#12301;&#30340;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#35775;&#38382;&#35831;&#27714;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#20851;&#27880;&#32593;&#32476;&#23558;&#29992;&#25143;&#30340;&#20559;&#22909;&#32435;&#20837;&#32771;&#34385;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21152;&#36148;&#24515;&#21644;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#35775;&#38382;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.13654</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;&#65306;&#32479;&#19968;&#20449;&#24687;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
A Personalized Dense Retrieval Framework for Unified Information Access. (arXiv:2304.13654v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#12300;\framework&#12301;&#30340;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#35775;&#38382;&#35831;&#27714;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#20851;&#27880;&#32593;&#32476;&#23558;&#29992;&#25143;&#30340;&#20559;&#22909;&#32435;&#20837;&#32771;&#34385;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21152;&#36148;&#24515;&#21644;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#35775;&#38382;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#21709;&#24212;&#20174;&#26816;&#32034;&#21040;&#25512;&#33616;&#12289;&#21040;&#22238;&#31572;&#38382;&#39064;&#31561;&#24191;&#27867;&#30340;&#20449;&#24687;&#35775;&#38382;&#35831;&#27714;&#65292;&#19968;&#30452;&#26159;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#26412;&#25991;&#35748;&#20026;&#36817;&#24180;&#26469;&#31264;&#23494;&#26816;&#32034;&#21644;&#36817;&#37051;&#25628;&#32034;&#30340;&#21457;&#23637;&#24102;&#26469;&#30340;&#28789;&#27963;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#24050;&#32463;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#12300;\framework&#12301;&#30340;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#65288;&#20010;&#24615;&#21270;&#30340;&#65289;&#20449;&#24687;&#35775;&#38382;&#35831;&#27714;&#65292;&#20363;&#22914;&#20851;&#38190;&#23383;&#25628;&#32034;&#12289;&#31034;&#20363;&#26597;&#35810;&#21644;&#34917;&#20805;&#29289;&#21697;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#20851;&#27880;&#32593;&#32476;&#65292;&#23558;&#29992;&#25143;&#29305;&#23450;&#30340;&#20559;&#22909;&#32435;&#20837;&#32771;&#34385;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#22312;&#21363;&#26102;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#21151;&#33021;&#65292;&#20351;&#20010;&#24615;&#21270;&#20449;&#24687;&#35775;&#38382;&#20307;&#39564;&#26356;&#21152;&#36148;&#24515;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a universal model that can efficiently and effectively respond to a wide range of information access requests -- from retrieval to recommendation to question answering -- has been a long-lasting goal in the information retrieval community. This paper argues that the flexibility, efficiency, and effectiveness brought by the recent development in dense retrieval and approximate nearest neighbor search have smoothed the path towards achieving this goal. We develop a generic and extensible dense retrieval framework, called \framework, that can handle a wide range of (personalized) information access requests, such as keyword search, query by example, and complementary item recommendation. Our proposed approach extends the capabilities of dense retrieval models for ad-hoc retrieval tasks by incorporating user-specific preferences through the development of a personalized attentive network. This allows for a more tailored and accurate personalized information access experience. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21452;&#32534;&#30721;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;DEDR&#65292;&#20197;&#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24046;&#36317;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;MM-FiD&#65292;&#19968;&#31181;&#22810;&#27169;&#24335;&#34701;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#30340;&#23545;&#31216;&#21452;&#32534;&#30721;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21452;&#32534;&#30721;&#31264;&#23494;&#26816;&#32034;&#26694;&#26550;DEDR&#65292;&#20197;&#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24046;&#36317;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;MM-FiD&#65292;&#19968;&#31181;&#22810;&#27169;&#24335;&#34701;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#65288;KI-VQA&#65289;&#26159;&#25351;&#22238;&#31572;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#20854;&#31572;&#26696;&#19981;&#22312;&#22270;&#20687;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KI-VQA&#20219;&#21153;&#27969;&#31243;&#65292;&#21253;&#25324;&#19968;&#20010;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#38405;&#35835;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEDR&#65292;&#23427;&#26159;&#19968;&#31181;&#23545;&#31216;&#21452;&#32534;&#30721;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#21333;&#27169;&#65288;&#25991;&#26412;&#65289;&#21644;&#22810;&#27169;&#32534;&#30721;&#22120;&#23558;&#25991;&#26723;&#21644;&#26597;&#35810;&#32534;&#30721;&#20026;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36845;&#20195;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#24357;&#21512;&#36825;&#20004;&#20010;&#32534;&#30721;&#22120;&#20013;&#30340;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23545;&#20004;&#20010;&#25104;&#29087;&#30340;KI-VQA&#25968;&#25454;&#38598;OK-VQA&#21644;FVQA&#36827;&#34892;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DEDR&#22312;OK-VQA&#21644;FVQA&#19978;&#30340;&#24615;&#33021;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;11.6&#65285;&#21644;30.9&#65285;&#12290;&#21033;&#29992;DEDR&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#65292;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;MM-FiD&#65292;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22810;&#27169;&#24335;&#34701;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20026;KI-VQA&#20219;&#21153;&#29983;&#25104;&#25991;&#26412;&#31572;&#26696;&#12290;MM-FiD&#23558;&#38382;&#39064;&#12289;&#22270;&#20687;&#21644;&#27599;&#20010;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#32534;&#30721;&#20026;&#21333;&#29420;&#30340;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#20174;&#23427;&#20204;&#30340;&#36830;&#25509;&#35299;&#30721;&#29983;&#25104;&#31572;&#26696;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MM-FiD&#22312;OK-VQA&#21644;FVQA&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retri
&lt;/p&gt;</description></item><item><title>&#37325;&#22609;CTR&#39044;&#27979;&#65306;&#23398;&#20064;&#31283;&#23450;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26410;&#26469;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13643</link><description>&lt;p&gt;
&#37325;&#26500;CTR&#39044;&#27979;&#65306;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21464;&#30340;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation. (arXiv:2304.13643v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13643
&lt;/p&gt;
&lt;p&gt;
&#37325;&#22609;CTR&#39044;&#27979;&#65306;&#23398;&#20064;&#31283;&#23450;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26410;&#26469;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#20316;&#20026;&#29992;&#25143;&#25490;&#21517;&#39033;&#30446;&#30340;&#26368;&#32456;&#36807;&#28388;&#22120;&#12290;&#35299;&#20915;CTR&#20219;&#21153;&#30340;&#20851;&#38190;&#26159;&#23398;&#20064;&#23545;&#39044;&#27979;&#26377;&#29992;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#27169;&#24335;&#26469;&#36866;&#37197;&#21382;&#21490;&#28857;&#20987;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#20195;&#34920;&#26041;&#27861;&#21253;&#25324;&#20998;&#35299;&#26426;&#21644;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23398;&#20064;&#21040;&#19981;&#31283;&#23450;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#21363;&#22312;&#21382;&#21490;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#20294;&#22312;&#26410;&#26469;&#20132;&#20184;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;CTR&#20219;&#21153; - &#19981;&#20877;&#20851;&#27880;&#21382;&#21490;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#32780;&#26159;&#23558;&#21382;&#21490;&#25968;&#25454;&#25353;&#26102;&#38388;&#39034;&#24207;&#20998;&#25104;&#20960;&#20010;&#26399;&#38388;&#65288;&#31216;&#20026;&#29615;&#22659;&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#31283;&#23450;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#24212;&#35813;&#33021;&#22815;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#39044;&#27979;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction plays a core role in recommender systems, serving as the final-stage filter to rank items for a user. The key to addressing the CTR task is learning feature interactions that are useful for prediction, which is typically achieved by fitting historical click data with the Empirical Risk Minimization (ERM) paradigm. Representative methods include Factorization Machines and Deep Interest Network, which have achieved wide success in industrial applications. However, such a manner inevitably learns unstable feature interactions, i.e., the ones that exhibit strong correlations in historical data but generalize poorly for future serving. In this work, we reformulate the CTR task -- instead of pursuing ERM on historical data, we split the historical data chronologically into several periods (a.k.a, environments), aiming to learn feature interactions that are stable across periods. Such feature interactions are supposed to generalize better to predict future 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#23398;&#21407;&#29702;&#30740;&#31350;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#37325;&#35201;&#30340;&#27010;&#29575;&#31639;&#27861;&#26159;&#25552;&#39640;&#31639;&#27861;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#20171;&#32461;&#20004;&#31181;&#19981;&#21516;&#25968;&#23398;&#36317;&#31163;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.13579</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#23398;&#21407;&#29702;&#30340;&#25512;&#33616;&#31995;&#32479;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improvements on Recommender System based on Mathematical Principles. (arXiv:2304.13579v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#23398;&#21407;&#29702;&#30740;&#31350;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#37325;&#35201;&#30340;&#27010;&#29575;&#31639;&#27861;&#26159;&#25552;&#39640;&#31639;&#27861;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#20171;&#32461;&#20004;&#31181;&#19981;&#21516;&#25968;&#23398;&#36317;&#31163;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#30740;&#31350;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29616;&#21407;&#29702;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#25968;&#23398;&#21407;&#29702;&#35299;&#37322;&#25512;&#33616;&#31639;&#27861;&#65292;&#24182;&#23547;&#25214;&#21487;&#34892;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;&#27010;&#29575;&#31639;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#25105;&#20204;&#23558;&#25551;&#36848;&#23427;&#20204;&#22914;&#20309;&#24110;&#21161;&#25552;&#39640;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;&#26412;&#25991;&#36824;&#23558;&#35814;&#32454;&#38416;&#36848;&#20004;&#31181;&#19981;&#21516;&#25968;&#23398;&#36317;&#31163;&#25551;&#36848;&#30456;&#20284;&#24230;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we will research the Recommender System's implementation about how it works and the algorithms used. We will explain the Recommender System's algorithms based on mathematical principles, and find feasible methods for improvements. The algorithms based on probability have its significance in Recommender System, we will describe how they help to increase the accuracy and speed of the algorithms. Both the weakness and the strength of two different mathematical distance used to describe the similarity will be detailed illustrated in this article.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25163;&#20889;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#65292;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#29305;&#24449;&#25552;&#21462;&#12289;&#25163;&#20889;&#35782;&#21035;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27493;&#39588;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#20998;&#21106;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.13530</link><description>&lt;p&gt;
&#20174;&#20840;&#25163;&#20889;&#39029;&#38754;&#20013;&#25552;&#21462;&#38190;&#20540;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13530
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25163;&#20889;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#65292;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#29305;&#24449;&#25552;&#21462;&#12289;&#25163;&#20889;&#35782;&#21035;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27493;&#39588;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#20998;&#21106;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#23383;&#21270;&#30340;&#25163;&#20889;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#30446;&#21069;&#30001;&#29420;&#31435;&#27169;&#22411;&#25191;&#34892;&#30340;&#19981;&#21516;&#27493;&#39588;&#65306;&#29305;&#24449;&#25552;&#21462;&#12289;&#25163;&#20889;&#35782;&#21035;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20043;&#21069;&#36827;&#34892;&#25163;&#20889;&#35782;&#21035;&#65292;&#24182;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#21576;&#29616;&#32467;&#26524;&#65306;&#34892;&#12289;&#27573;&#33853;&#21644;&#39029;&#38754;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;&#20110;&#25972;&#20010;&#39029;&#38754;&#26102;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#29305;&#21035;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#20808;&#20998;&#21106;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#20174;&#38190;&#20540;&#27880;&#37322;&#20013;&#36827;&#34892;&#23398;&#20064;&#65306;&#21363;&#37325;&#35201;&#21333;&#35789;&#21644;&#30456;&#24212;&#21629;&#21517;&#23454;&#20307;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#65288;IAM&#12289;ESPOSALLES&#21644;POPP&#65289;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20449;&#24687;&#22788;&#29702;&#27963;&#21160;&#29992;&#25143;&#30740;&#31350;&#20013;&#38750;&#21463;&#25511;&#21464;&#37327;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#20013;&#20219;&#21153;&#25345;&#32493;&#26102;&#38388;&#26159;&#20010;&#20307;&#24046;&#24322;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.13488</link><description>&lt;p&gt;
&#30740;&#31350;&#38750;&#21463;&#25511;&#21464;&#37327;&#23545;&#20449;&#24687;&#22788;&#29702;&#27963;&#21160;&#29992;&#25143;&#30740;&#31350;&#20013;&#29983;&#29702;&#20449;&#21495;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Impact of Uncontrolled Variables on Physiological Signals in User Studies for Information Processing Activities. (arXiv:2304.13488v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13488
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20449;&#24687;&#22788;&#29702;&#27963;&#21160;&#29992;&#25143;&#30740;&#31350;&#20013;&#38750;&#21463;&#25511;&#21464;&#37327;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#20013;&#20219;&#21153;&#25345;&#32493;&#26102;&#38388;&#26159;&#20010;&#20307;&#24046;&#24322;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29702;&#20449;&#21495;&#26377;&#28508;&#21147;&#20316;&#20026;&#23458;&#35266;&#27979;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#29702;&#35299;&#29992;&#25143;&#19982;&#20449;&#24687;&#35775;&#38382;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#21442;&#19982;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#21495;&#38750;&#24120;&#25935;&#24863;&#65292;&#22312;&#23454;&#39564;&#23460;&#29992;&#25143;&#30740;&#31350;&#20013;&#38656;&#35201;&#35768;&#22810;&#25511;&#21046;&#12290;&#20026;&#20102;&#30740;&#31350;&#20219;&#21153;&#39034;&#24207;&#25110;&#25345;&#32493;&#26102;&#38388;&#31561;&#21463;&#25511;&#25110;&#38750;&#21463;&#25511;&#65288;&#21363;&#28151;&#28102;&#65289;&#21464;&#37327;&#23545;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#24433;&#21709;&#31243;&#24230;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#20854;&#20013;&#27599;&#20010;&#21442;&#19982;&#32773;&#23436;&#25104;&#20102;&#22235;&#31181;&#20449;&#24687;&#22788;&#29702;&#27963;&#21160;&#65288;READ&#65292;LISTEN&#65292;SPEAK&#21644;WRITE&#65289;&#65292;&#21516;&#26102;&#25105;&#20204;&#25910;&#38598;&#20102;&#34880;&#23481;&#37327;&#33033;&#20914;&#12289;&#30382;&#32932;&#30005;&#27963;&#21160;&#21644;&#30643;&#23380;&#21453;&#24212;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#26597;&#29992;&#25143;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#21463;&#25511;&#21644;&#38750;&#21463;&#25511;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20219;&#21153;&#25345;&#32493;&#26102;&#38388;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#34920;&#26126;&#23427;&#20195;&#34920;&#30340;&#26159;&#20010;&#20307;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#26377;&#20851;&#30446;&#26631;&#21464;&#37327;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physiological signals can potentially be applied as objective measures to understand the behavior and engagement of users interacting with information access systems. However, the signals are highly sensitive, and many controls are required in laboratory user studies. To investigate the extent to which controlled or uncontrolled (i.e., confounding) variables such as task sequence or duration influence the observed signals, we conducted a pilot study where each participant completed four types of information-processing activities (READ, LISTEN, SPEAK, and WRITE). Meanwhile, we collected data on blood volume pulse, electrodermal activity, and pupil responses. We then used machine learning approaches as a mechanism to examine the influence of controlled and uncontrolled variables that commonly arise in user studies. Task duration was found to have a substantial effect on the model performance, suggesting it represents individual differences rather than giving insight into the target varia
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#37096;&#20998;&#20869;&#23481;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#22797;&#26434;&#27169;&#22411;&#30340;&#32553;&#25918;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;STIR&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#39030;&#37096;&#36755;&#20986;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20840;&#23616;/&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2304.13393</link><description>&lt;p&gt;
STIR&#65306;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#21518;&#22788;&#29702;&#30340;Siamese Transformer&#65288;arXiv&#65306;2304.13393v1 [cs.IR]&#65289;
&lt;/p&gt;
&lt;p&gt;
STIR: Siamese Transformer for Image Retrieval Postprocessing. (arXiv:2304.13393v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13393
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#37096;&#20998;&#20869;&#23481;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#22797;&#26434;&#27169;&#22411;&#30340;&#32553;&#25918;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;STIR&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#39030;&#37096;&#36755;&#20986;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20840;&#23616;/&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#22270;&#20687;&#26816;&#32034;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23398;&#20064;&#20855;&#26377;&#20449;&#24687;&#30340;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#26041;&#27861;&#22914;&#20313;&#24358;&#36317;&#31163;&#23558;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;&#22914;HypViT&#65289;&#36716;&#21521;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26356;&#38590;&#20197;&#25193;&#23637;&#21040;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#20855;&#26377;&#30828;&#36127;&#20363;&#25366;&#25496;&#65292;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#36825;&#20123;&#32570;&#28857;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#26816;&#32034;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#30340;Siamese Transformer&#65288;STIR&#65289;&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#39030;&#37096;&#36755;&#20986;&#12290;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#37325;&#25490;&#21464;&#21387;&#22120;&#19981;&#21516;&#65292;STIR&#19981;&#20381;&#36182;&#20110;&#20840;&#23616;/&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#26159;&#20511;&#21161;&#27880;&#24847;&#26426;&#21046;&#30452;&#25509;&#22312;&#20687;&#32032;&#32423;&#21035;&#27604;&#36739;&#26597;&#35810;&#22270;&#20687;&#21644;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#22270;&#20687;&#12290;&#30001;&#27492;&#24471;&#20986;&#30340;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#32456;&#36523;&#23398;&#20064;&#36328;&#27169;&#24577;&#21704;&#24076;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#22797;&#35757;&#32451;&#21704;&#24076;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#29983;&#21629;&#21608;&#26399;&#21704;&#24076;&#26816;&#32034;&#65292;&#32780;&#19988;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#22686;&#37327;&#25968;&#25454;&#26469;&#26356;&#26032;&#21704;&#24076;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#38750;&#36830;&#32493;&#21704;&#24076;&#26816;&#32034;&#26356;&#26032;&#30340;&#32791;&#26102;&#12290;</title><link>http://arxiv.org/abs/2304.13357</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#21629;&#21608;&#26399;&#36328;&#27169;&#24577;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Deep Lifelong Cross-modal Hashing. (arXiv:2304.13357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#32456;&#36523;&#23398;&#20064;&#36328;&#27169;&#24577;&#21704;&#24076;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#22797;&#35757;&#32451;&#21704;&#24076;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#29983;&#21629;&#21608;&#26399;&#21704;&#24076;&#26816;&#32034;&#65292;&#32780;&#19988;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#22686;&#37327;&#25968;&#25454;&#26469;&#26356;&#26032;&#21704;&#24076;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#38750;&#36830;&#32493;&#21704;&#24076;&#26816;&#32034;&#26356;&#26032;&#30340;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21704;&#24076;&#26041;&#27861;&#22312;&#20132;&#21449;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#26597;&#35810;&#36895;&#24230;&#21644;&#20302;&#23384;&#20648;&#25104;&#26412;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21704;&#24076;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#38750;&#32447;&#24615;&#24322;&#26500;&#29305;&#24449;&#25552;&#21462;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#19981;&#26029;&#21040;&#26469;&#26102;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#38750;&#36830;&#32493;&#21704;&#24076;&#26816;&#32034;&#26356;&#26032;&#30340;&#32791;&#26102;&#20173;&#28982;&#26159;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#29983;&#21629;&#21608;&#26399;&#36328;&#27169;&#24577;&#21704;&#24076;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#29983;&#21629;&#21608;&#26399;&#21704;&#24076;&#26816;&#32034;&#32780;&#19981;&#26159;&#37325;&#22797;&#35757;&#32451;&#21704;&#24076;&#20989;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32456;&#36523;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#22686;&#37327;&#25968;&#25454;&#26469;&#26356;&#26032;&#21704;&#24076;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#32047;&#35745;&#25968;&#25454;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#21704;&#24076;&#20989;&#25968;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#21629;&#21608;&#26399;&#21704;&#24076;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20351;&#21407;&#22987;&#21704;&#24076;&#30721;&#21487;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hashing methods have made significant progress in cross-modal retrieval tasks with fast query speed and low storage cost. Among them, deep learning-based hashing achieves better performance on large-scale data due to its excellent extraction and representation ability for nonlinear heterogeneous features. However, there are still two main challenges in catastrophic forgetting when data with new categories arrive continuously, and time-consuming for non-continuous hashing retrieval to retrain for updating. To this end, we, in this paper, propose a novel deep lifelong cross-modal hashing to achieve lifelong hashing retrieval instead of re-training hash function repeatedly when new data arrive. Specifically, we design lifelong learning strategy to update hash functions by directly training the incremental data instead of retraining new hash functions using all the accumulated data, which significantly reduce training time. Then, we propose lifelong hashing loss to enable original hash cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ConvRerank&#65292;&#19968;&#31181;&#37319;&#29992;&#35270;&#22270;&#38598;&#25104;&#30340;&#20250;&#35805;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20250;&#35805;&#25628;&#32034;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13290</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#22270;&#38598;&#25104;&#25552;&#39640;&#20250;&#35805;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improving Conversational Passage Re-ranking with View Ensemble. (arXiv:2304.13290v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ConvRerank&#65292;&#19968;&#31181;&#37319;&#29992;&#35270;&#22270;&#38598;&#25104;&#30340;&#20250;&#35805;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20250;&#35805;&#25628;&#32034;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConvRerank&#65292;&#19968;&#31181;&#37319;&#29992;&#26032;&#24320;&#21457;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#20250;&#35805;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35270;&#22270;&#38598;&#25104;&#26041;&#27861;&#22686;&#24378;&#20102;&#20266;&#26631;&#35760;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ConvRerank&#30340;&#24494;&#35843;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;ConvRerank&#19982;&#20250;&#35805;&#23494;&#38598;&#26816;&#32034;&#22120;&#32423;&#32852;&#20351;&#29992;&#21487;&#20197;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32423;&#32852;&#27969;&#27700;&#32447;&#34920;&#29616;&#20986;&#26356;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#39640;&#30340;&#25490;&#21517;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#28145;&#20837;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20250;&#35805;&#25628;&#32034;&#25928;&#26524;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ConvRerank, a conversational passage re-ranker that employs a newly developed pseudo-labeling approach. Our proposed view-ensemble method enhances the quality of pseudo-labeled data, thus improving the fine-tuning of ConvRerank. Our experimental evaluation on benchmark datasets shows that combining ConvRerank with a conversational dense retriever in a cascaded manner achieves a good balance between effectiveness and efficiency. Compared to baseline methods, our cascaded pipeline demonstrates lower latency and higher top-ranking effectiveness. Furthermore, the in-depth analysis confirms the potential of our approach to improving the effectiveness of conversational search.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#37319;&#29992;&#21452;&#22612;&#26816;&#32034;&#26550;&#26500;&#21644;&#27169;&#24577;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39033;&#30446;&#20919;&#21551;&#21160;&#21644;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13277</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multi-Modal Sequential Recommendation. (arXiv:2304.13277v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#37319;&#29992;&#21452;&#22612;&#26816;&#32034;&#26550;&#26500;&#21644;&#27169;&#24577;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39033;&#30446;&#20919;&#21551;&#21160;&#21644;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#22312;&#32447;&#26381;&#21153;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#25512;&#21160;&#19994;&#21153;&#25910;&#20837;&#30340;&#20851;&#38190;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26174;&#24335;&#29289;&#21697;ID&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#22312;&#22788;&#29702;&#39033;&#30446;&#20919;&#21551;&#21160;&#21644;&#22495;&#36716;&#31227;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#20351;&#29992;&#19982;&#29289;&#21697;&#20851;&#32852;&#30340;&#27169;&#24577;&#29305;&#24449;&#26367;&#20195;&#29289;&#21697;ID&#65292;&#20351;&#23398;&#20064;&#30340;&#30693;&#35782;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20256;&#36882;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#19982;&#29289;&#21697;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#21463;&#21040;&#39640;&#32423;&#29305;&#24449;&#21521;&#37327;&#21644;&#20302;&#32423;&#29305;&#24449;&#23884;&#20837;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#38459;&#30861;&#36827;&#19968;&#27493;&#30340;&#27169;&#22411;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#21452;&#22612;&#26816;&#32034;&#26550;&#26500;&#12290;&#22312;&#36825;&#20010;&#26550;&#26500;&#20013;&#65292;&#20174;&#29992;&#25143;&#32534;&#30721;&#22120;&#39044;&#27979;&#30340;&#23884;&#20837;&#34987;&#29992;&#20110;&#26816;&#32034;&#20174;&#29289;&#21697;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#19982;&#39069;&#22806;&#30340;&#27169;&#24577;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#25512;&#33616;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#23427;&#22312;&#22788;&#29702;&#39033;&#30446;&#20919;&#21551;&#21160;&#21644;&#22495;&#36716;&#31227;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing development of e-commerce and online services, personalized recommendation systems have become crucial for enhancing user satisfaction and driving business revenue. Traditional sequential recommendation methods that rely on explicit item IDs encounter challenges in handling item cold start and domain transfer problems. Recent approaches have attempted to use modal features associated with items as a replacement for item IDs, enabling the transfer of learned knowledge across different datasets. However, these methods typically calculate the correlation between the model's output and item embeddings, which may suffer from inconsistencies between high-level feature vectors and low-level feature embeddings, thereby hindering further model learning. To address this issue, we propose a dual-tower retrieval architecture for sequence recommendation. In this architecture, the predicted embedding from the user encoder is used to retrieve the generated embedding from the item 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;&#26041;&#27861;&#65288;GRF&#65289;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#26041;&#27861;&#65292;&#23427;&#20174;&#38271;&#24418;&#25991;&#26412;&#20013;&#26500;&#24314;&#27010;&#29575;&#21453;&#39304;&#27169;&#22411;&#65292;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#25991;&#29486;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GRF&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13157</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Generative Relevance Feedback with Large Language Models. (arXiv:2304.13157v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;&#26041;&#27861;&#65288;GRF&#65289;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#26041;&#27861;&#65292;&#23427;&#20174;&#38271;&#24418;&#25991;&#26412;&#20013;&#26500;&#24314;&#27010;&#29575;&#21453;&#39304;&#27169;&#22411;&#65292;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#25991;&#29486;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GRF&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26597;&#35810;&#25193;&#23637;&#27169;&#22411;&#20351;&#29992;&#20266;&#30456;&#20851;&#21453;&#39304;&#26469;&#25552;&#39640;&#31532;&#19968;&#36941;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;, &#20294;&#26159;&#24403;&#21021;&#22987;&#32467;&#26524;&#19981;&#30456;&#20851;&#26102;&#21017;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;&#65288;GRF&#65289;&#65292;&#35813;&#27169;&#22411;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#24418;&#25991;&#26412;&#20013;&#26500;&#24314;&#27010;&#29575;&#21453;&#39304;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#38646;&#26679;&#26412;&#29983;&#25104;&#23376;&#20219;&#21153;-&#26597;&#35810;&#65292;&#23454;&#20307;&#65292;&#20107;&#23454;&#65292;&#26032;&#38395;&#25991;&#31456;&#65292;&#25991;&#26723;&#21644;&#25991;&#31456;-&#26469;&#30740;&#31350;&#29983;&#25104;&#25991;&#26412;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#21508;&#31181;&#26597;&#35810;&#21644;&#25991;&#26723;&#38598;&#21512;&#30340;&#25991;&#26723;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;GRF&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;GRF&#26041;&#27861;&#27604;&#20808;&#21069;&#30340;PRF&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#30456;&#27604;RM3&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;5-19%&#30340;MAP&#21644;17-24%&#30340;NDCG@10&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;R @ 1k&#25928;&#26524;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#12289;&#23494;&#38598;&#21644;&#25193;&#23637;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve the best R@1k effectiveness on all datasets compared to state-of-the-art sparse, dense, and expansion models.
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#34394;&#25311;&#21161;&#25163;&#20013;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#24314;&#27169;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#20013;&#30340;&#26426;&#36935;&#65307;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26597;&#35810;&#39046;&#22495;&#20998;&#31867;&#12289;&#30693;&#35782;&#22270;&#35889;&#31561;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65307;&#31616;&#35201;&#27010;&#36848;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.13149</link><description>&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#20013;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#30340;&#24314;&#27169;&#65306;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities. (arXiv:2304.13149v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13149
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#34394;&#25311;&#21161;&#25163;&#20013;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#24314;&#27169;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#20013;&#30340;&#26426;&#36935;&#65307;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26597;&#35810;&#39046;&#22495;&#20998;&#31867;&#12289;&#30693;&#35782;&#22270;&#35889;&#31561;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65307;&#31616;&#35201;&#27010;&#36848;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#27491;&#22312;&#25104;&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#20449;&#24687;&#26816;&#32034;&#24179;&#21488;&#65292;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#34394;&#25311;&#21161;&#25163;&#21475;&#35821;&#20449;&#24687;&#26597;&#35810;&#24314;&#27169;&#30340;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21644;&#30740;&#31350;&#21487;&#20197;&#24212;&#29992;&#20110;&#25552;&#39640;&#34394;&#25311;&#21161;&#25163;&#35821;&#38899;&#35782;&#21035;&#36136;&#37327;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#26597;&#35810;&#39046;&#22495;&#20998;&#31867;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20197;&#21450;&#26597;&#35810;&#20010;&#24615;&#21270;&#26469;&#24110;&#21161;&#25913;&#21892;&#21475;&#35821;&#20449;&#24687;&#39046;&#22495;&#26597;&#35810;&#30340;&#20934;&#30830;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#31616;&#35201;&#27010;&#36848;&#20102;&#35821;&#38899;&#35782;&#21035;&#20013;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks.  We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition.  We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MBIB&#65292;&#19968;&#20010;&#23558;&#19981;&#21516;&#31867;&#22411;&#23186;&#20307;&#20559;&#35265;&#20998;&#20026;&#20849;&#21516;&#26694;&#26550;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#21333;&#19968;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#30740;&#31350;&#20852;&#36259;&#21644;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.13148</link><description>&lt;p&gt;
MBIB--&#39318;&#20010;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#38598;&#21512;&#30340;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection. (arXiv:2304.13148v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MBIB&#65292;&#19968;&#20010;&#23558;&#19981;&#21516;&#31867;&#22411;&#23186;&#20307;&#20559;&#35265;&#20998;&#20026;&#20849;&#21516;&#26694;&#26550;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#21333;&#19968;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#30740;&#31350;&#20852;&#36259;&#21644;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#26469;&#20998;&#32452;&#36825;&#20123;&#35780;&#20272;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#65288;MBIB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#23186;&#20307;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#35821;&#35328;&#12289;&#35748;&#30693;&#12289;&#25919;&#27835;&#65289;&#20998;&#20026;&#19968;&#20010;&#20849;&#21516;&#30340;&#26694;&#26550;&#65292;&#20197;&#27979;&#35797;&#39044;&#27979;&#26816;&#27979;&#25216;&#26415;&#30340;&#27010;&#25324;&#21270;&#31243;&#24230;&#12290;&#22312;&#35780;&#20272;&#20102;115&#20010;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;9&#20010;&#20219;&#21153;&#65292;&#20180;&#32454;&#25552;&#20986;&#20102;22&#20010;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;Transformer&#25216;&#26415;&#65288;&#20363;&#22914;T5&#12289;BART&#65289;&#35780;&#20272;MBIB&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20167;&#24680;&#35328;&#35770;&#12289;&#31181;&#26063;&#20559;&#35265;&#21644;&#24615;&#21035;&#20559;&#35265;&#26356;&#23481;&#26131;&#26816;&#27979;&#65292;&#20294;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#26576;&#20123;&#20559;&#35265;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#25216;&#26415;&#21487;&#20197;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#30740;&#31350;&#20852;&#36259;&#21644;&#36164;&#28304;&#20998;&#37197;&#22312;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#30340;&#20010;&#21035;&#20219;&#21153;&#19978;&#23384;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark (MBIB), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5, BART). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26597;&#35810;&#37325;&#26500;&#65292;&#21363;&#22312;&#26597;&#35810;&#20013;&#28155;&#21152;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#65292;&#20197;&#25506;&#32034;&#29992;&#25143;&#21644;&#25628;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.13129</link><description>&lt;p&gt;
&#26597;&#35810;&#37325;&#26500;&#20013;&#30340;&#24615;&#21035;&#19987;&#19994;&#21270;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Patterns of gender-specializing query reformulation. (arXiv:2304.13129v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26597;&#35810;&#37325;&#26500;&#65292;&#21363;&#22312;&#26597;&#35810;&#20013;&#28155;&#21152;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#65292;&#20197;&#25506;&#32034;&#29992;&#25143;&#21644;&#25628;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#31995;&#32479;&#30340;&#29992;&#25143;&#32463;&#24120;&#36890;&#36807;&#28155;&#21152;&#26597;&#35810;&#26415;&#35821;&#26469;&#21453;&#26144;&#20182;&#20204;&#26085;&#30410;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#25110;&#32773;&#24403;&#31995;&#32479;&#26080;&#27861;&#25552;&#20379;&#30456;&#20851;&#20869;&#23481;&#26102;&#26356;&#20934;&#30830;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#20998;&#26512;&#36825;&#20123;&#26597;&#35810;&#37325;&#26500;&#21487;&#20197;&#20026;&#25105;&#20204;&#25552;&#20379;&#26377;&#20851;&#31995;&#32479;&#21644;&#29992;&#25143;&#34892;&#20026;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#29305;&#27530;&#31867;&#21035;&#30340;&#26597;&#35810;&#37325;&#26500;&#65292;&#36825;&#31181;&#26597;&#35810;&#37325;&#26500;&#28041;&#21450;&#25351;&#23450;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#65292;&#27604;&#22914;&#24615;&#21035;&#65292;&#20316;&#20026;&#37325;&#26500;&#26597;&#35810;&#30340;&#19968;&#37096;&#20998;&#65288;&#20363;&#22914;&#65292;&#8220;&#22885;&#36816;&#20250;2021&#24180;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#8221;&#21464;&#25104;&#8220;&#22885;&#36816;&#20250;2021&#24180;&#22899;&#23376;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#8221;&#65289;&#12290;&#26597;&#35810;&#12289;&#25628;&#32034;&#32467;&#26524;&#21644;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#65289;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#20851;&#31995;&#65292;&#36825;&#23548;&#33268;&#25105;&#20204;&#20551;&#35774;&#19981;&#21516;&#30340;&#21407;&#22240;&#65292;&#22914;&#21407;&#22987;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#27424;&#20195;&#34920;&#24615;&#25110;&#26631;&#35760;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#39033;&#24615;&#21035;&#19987;&#19994;&#21270;&#26597;&#35810;&#37325;&#26500;&#30340;&#35266;&#23519;&#24615;&#30740;&#31350;&#8212;&#8212;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#21644;&#24433;&#21709;&#8212;&#8212;&#20316;&#20026;&#25506;&#32034;&#29992;&#25143;&#21644;&#25628;&#32034;&#31995;&#32479;&#20043;&#38388;&#20851;&#31995;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users of search systems often reformulate their queries by adding query terms to reflect their evolving information need or to more precisely express their information need when the system fails to surface relevant content. Analyzing these query reformulations can inform us about both system and user behavior. In this work, we study a special category of query reformulations that involve specifying demographic group attributes, such as gender, as part of the reformulated query (e.g., "olympic 2021 soccer results" to "olympic 2021 women's soccer results"). There are many ways a query, the search results, and a demographic attribute such as gender may relate, leading us to hypothesize different causes for these reformulation patterns, such as under-representation on the original result page or based on the linguistic theory of markedness. This paper reports on an observational study of gender-specializing query reformulations -- their contexts and effects -- as a lens on the relationship
&lt;/p&gt;</description></item><item><title>OFAR&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#30340;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#30452;&#25773;&#24179;&#21488;&#31435;&#21363;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#38750;&#27861;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.12608</link><description>&lt;p&gt;
OFAR: &#19968;&#31181;&#29992;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#30340;&#22810;&#27169;&#24335;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OFAR: A Multimodal Evidence Retrieval Framework for Illegal Live-streaming Identification. (arXiv:2304.12608v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12608
&lt;/p&gt;
&lt;p&gt;
OFAR&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#30340;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#30452;&#25773;&#24179;&#21488;&#31435;&#21363;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#38750;&#27861;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#26159;&#20026;&#20102;&#24110;&#21161;&#30452;&#25773;&#24179;&#21488;&#31435;&#21363;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#38750;&#27861;&#34892;&#20026;&#65292;&#20363;&#22914;&#21806;&#21334;&#29645;&#36149;&#21644;&#28626;&#21361;&#21160;&#29289;&#65292;&#23545;&#20928;&#21270;&#32593;&#32476;&#29615;&#22659;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OFAR&#30340;&#22810;&#27169;&#24335;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;&#65292;&#20197;&#21033;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#12290;OFAR&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#26597;&#35810;&#32534;&#30721;&#22120;&#12289;&#25991;&#26723;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;MaxSim&#30340;&#23545;&#27604;&#26202;&#20132;&#38598;&#12290;&#26597;&#35810;&#32534;&#30721;&#22120;&#21644;&#25991;&#26723;&#32534;&#30721;&#22120;&#37117;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;OFA&#12290;
&lt;/p&gt;
&lt;p&gt;
Illegal live-streaming identification, which aims to help live-streaming platforms immediately recognize the illegal behaviors in the live-streaming, such as selling precious and endangered animals, plays a crucial role in purifying the network environment. Traditionally, the live-streaming platform needs to employ some professionals to manually identify the potential illegal live-streaming. Specifically, the professional needs to search for related evidence from a large-scale knowledge database for evaluating whether a given live-streaming clip contains illegal behavior, which is time-consuming and laborious. To address this issue, in this work, we propose a multimodal evidence retrieval system, named OFAR, to facilitate the illegal live-streaming identification. OFAR consists of three modules: \textit{Query Encoder}, \textit{Document Encoder}, and \textit{MaxSim-based Contrastive Late Intersection}. Both query encoder and document encoder are implemented with the advanced \mbox{OFA} 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07763</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-optimized Contrastive Learning for Sequential Recommendation. (arXiv:2304.07763v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26159;&#35299;&#20915;&#31232;&#30095;&#19988;&#21547;&#22122;&#22768;&#25512;&#33616;&#25968;&#25454;&#30340;&#19968;&#20010;&#26032;&#20852;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#38024;&#23545;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#22686;&#24378;&#65292;&#35201;&#20040;&#21482;&#20351;&#29992;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#24456;&#38590;&#25512;&#24191;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard 
&lt;/p&gt;</description></item><item><title>RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.05239</link><description>&lt;p&gt;
RecD&#65306;&#20026;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05239
&lt;/p&gt;
&lt;p&gt;
RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; RecD&#65288;&#25512;&#33616;&#21435;&#37325;&#65289;&#65292;&#23427;&#26159;&#19968;&#32452;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411; (DLRM) &#35757;&#32451;&#27969;&#31243;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#12290;RecD&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#36825;&#26159;&#22823;&#35268;&#27169; DLRM &#35757;&#32451;&#25968;&#25454;&#38598;&#20869;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026; DLRM &#25968;&#25454;&#38598;&#26159;&#20174;&#20132;&#20114;&#20013;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; RecD &#22914;&#20309;&#21033;&#29992;&#27492;&#23646;&#24615;&#26469;&#20248;&#21270;&#29983;&#20135;&#25968;&#25454;&#30340;&#27969;&#31243;&#65292;&#20943;&#23569;&#25968;&#25454;&#38598;&#23384;&#20648;&#21644;&#39044;&#22788;&#29702;&#38656;&#27714;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#22312;&#35757;&#32451;&#25209;&#27425;&#20013;&#37325;&#22797;&#12290;RecD &#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs)&#65292;&#20197;&#22312;&#27599;&#20010;&#25209;&#27425;&#20013;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; DLRM &#27169;&#22411;&#26550;&#26500;&#22914;&#20309;&#21033;&#29992; IKJTs &#26469;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27861;&#24459;&#26465;&#27454;&#32771;&#34385;&#22312;&#20869;&#20197;&#25913;&#21892;&#27861;&#24459;&#26696;&#20214;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.11012</link><description>&lt;p&gt;
&#22686;&#24378;&#27861;&#24459;&#26696;&#20214;&#21305;&#37197;&#30340;&#27861;&#24459;&#26465;&#27454;&#65306;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Law Article-Enhanced Legal Case Matching: a Causal Learning Approach. (arXiv:2210.11012v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27861;&#24459;&#26465;&#27454;&#32771;&#34385;&#22312;&#20869;&#20197;&#25913;&#21892;&#27861;&#24459;&#26696;&#20214;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#26696;&#20363;&#21305;&#37197;&#22312;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#35821;&#20041;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#21482;&#32771;&#34385;&#26696;&#20214;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#65292;&#24573;&#30053;&#20102;&#27861;&#24459;&#26465;&#27454;&#22312;&#21305;&#37197;&#32467;&#26524;&#20013;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27861;&#24459;&#26465;&#27454;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#21892;&#21305;&#37197;&#32467;&#26524;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case matching, which automatically constructs a model to estimate the similarities between the source and target cases, has played an essential role in intelligent legal systems. Semantic text matching models have been applied to the task where the source and target legal cases are considered as long-form text documents. These general-purpose matching models make the predictions solely based on the texts in the legal cases, overlooking the essential role of the law articles in legal case matching. In the real world, the matching results (e.g., relevance labels) are dramatically affected by the law articles because the contents and the judgments of a legal case are radically formed on the basis of law. From the causal sense, a matching decision is affected by the mediation effect from the cited law articles by the legal cases, and the direct effect of the key circumstances (e.g., detailed fact descriptions) in the legal cases. In light of the observation, this paper proposes a mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECONET&#30340;&#26032;&#22411;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#30340;&#21387;&#32553;&#24863;&#30693;&#65292;&#33021;&#26377;&#25928;&#22320;&#37325;&#26500;&#21521;&#37327;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#23637;&#24320;&#32593;&#32476;&#65292;&#22312;&#20272;&#35745;&#20102;&#20854;&#27867;&#21270;&#35823;&#24046;&#30340;&#22522;&#30784;&#19978;&#24471;&#20986;&#30456;&#20851;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2205.07050</link><description>&lt;p&gt;
DECONET&#65306;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#21387;&#32553;&#24863;&#30693;&#30340;&#23637;&#24320;&#32593;&#32476;&#21450;&#20854;&#27867;&#21270;&#35823;&#24046;&#30028;
&lt;/p&gt;
&lt;p&gt;
DECONET: an Unfolding Network for Analysis-based Compressed Sensing with Generalization Error Bounds. (arXiv:2205.07050v6 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECONET&#30340;&#26032;&#22411;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#30340;&#21387;&#32553;&#24863;&#30693;&#65292;&#33021;&#26377;&#25928;&#22320;&#37325;&#26500;&#21521;&#37327;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#23637;&#24320;&#32593;&#32476;&#65292;&#22312;&#20272;&#35745;&#20102;&#20854;&#27867;&#21270;&#35823;&#24046;&#30340;&#22522;&#30784;&#19978;&#24471;&#20986;&#30456;&#20851;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;DECONET&#65292;&#29992;&#20110;&#22522;&#20110;&#20998;&#26512;&#31232;&#30095;&#24615;&#30340;&#21387;&#32553;&#24863;&#30693;&#12290;DECONET&#32852;&#21512;&#23398;&#20064;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#20174;&#19981;&#23436;&#25972;&#12289;&#22122;&#22768;&#27979;&#37327;&#20013;&#37325;&#26500;&#21521;&#37327;&#65292;&#20197;&#21450;&#19968;&#20010;&#20887;&#20313;&#30340;&#31232;&#30095;&#20998;&#26512;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#22312;DECONET&#30340;&#21508;&#20010;&#23618;&#20043;&#38388;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;DECONET&#30340;&#20551;&#35774;&#31867;&#24182;&#20272;&#35745;&#20854;&#30456;&#20851;&#30340;Rademacher&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#20272;&#35745;&#32467;&#26524;&#20026;DECONET&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19978;&#38480;&#65292;&#29992;&#20110;&#35780;&#20272;DECONET&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#23637;&#24320;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#19988;&#20854;&#34892;&#20026;&#31526;&#21512;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new deep unfolding network for analysis-sparsity-based Compressed Sensing. The proposed network coined Decoding Network (DECONET) jointly learns a decoder that reconstructs vectors from their incomplete, noisy measurements and a redundant sparsifying analysis operator, which is shared across the layers of DECONET. Moreover, we formulate the hypothesis class of DECONET and estimate its associated Rademacher complexity. Then, we use this estimate to deliver meaningful upper bounds for the generalization error of DECONET. Finally, the validity of our theoretical results is assessed and comparisons to state-of-the-art unfolding networks are made, on both synthetic and real-world datasets. Experimental results indicate that our proposed network outperforms the baselines, consistently for all datasets, and its behaviour complies with our theoretical findings.
&lt;/p&gt;</description></item></channel></rss>