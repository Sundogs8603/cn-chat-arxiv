<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36798;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#33021;&#21147;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;TDL&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;GNN4TDL&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#28436;&#21270;&#39046;&#22495;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.02143</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#24102;&#26377;&#20998;&#31867;&#21644;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions. (arXiv:2401.02143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36798;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#33021;&#21147;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;TDL&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;GNN4TDL&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#28436;&#21270;&#39046;&#22495;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#35813;&#32508;&#36848;&#31361;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;TDL&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65306;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36848;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#22825;&#28982;&#33021;&#21147;&#26469;&#27169;&#25311;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;TDL&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20852;&#36259;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#35774;&#35745;&#21644;&#23454;&#29616;GNN&#29992;&#20110;TDL&#65288;GNN4TDL&#65289;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#23427;&#21253;&#25324;&#23545;&#22522;&#30784;&#38382;&#39064;&#30340;&#35814;&#32454;&#30740;&#31350;&#21644;&#22522;&#20110;GNN&#30340;TDL&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#20026;&#20854;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#27880;&#26500;&#24314;&#22270;&#32467;&#26500;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#20840;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural Networks (GNNs), a domain where deep learning-based approaches have increasingly shown superior performance in both classification and regression tasks compared to traditional methods. The survey highlights a critical gap in deep neural TDL methods: the underrepresentation of latent correlations among data instances and feature values. GNNs, with their innate capability to model intricate relationships and interactions between diverse elements of tabular data, have garnered significant interest and application across various TDL domains. Our survey provides a systematic review of the methods involved in designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed investigation into the foundational aspects and an overview of GNN-based TDL methods, offering insights into their evolving landscape. We present a comprehensive taxonomy focused on constructing graph structures and representation learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SComGNN&#65289;&#29992;&#20110;&#27169;&#25311;&#21644;&#29702;&#35299;&#21830;&#21697;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#21830;&#21697;&#12290;</title><link>http://arxiv.org/abs/2401.02130</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20114;&#34917;&#21830;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Spectral-based Graph Neutral Networks for Complementary Item Recommendation. (arXiv:2401.02130v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SComGNN&#65289;&#29992;&#20110;&#27169;&#25311;&#21644;&#29702;&#35299;&#21830;&#21697;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#21830;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20114;&#34917;&#20851;&#31995;&#26497;&#22823;&#22320;&#24110;&#21161;&#25512;&#33616;&#31995;&#32479;&#22312;&#36141;&#20080;&#19968;&#20010;&#21830;&#21697;&#21518;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#30340;&#21830;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#30456;&#20284;&#20851;&#31995;&#19981;&#21516;&#65292;&#20855;&#26377;&#20114;&#34917;&#20851;&#31995;&#30340;&#21830;&#21697;&#21487;&#33021;&#20250;&#36830;&#32493;&#36141;&#20080;&#65288;&#20363;&#22914;iPhone&#21644;AirPods Pro&#65289;&#65292;&#23427;&#20204;&#19981;&#20165;&#20849;&#20139;&#30456;&#20851;&#24615;&#65292;&#36824;&#23637;&#29616;&#20986;&#19981;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#23646;&#24615;&#26159;&#30456;&#21453;&#30340;&#65292;&#24314;&#27169;&#20114;&#34917;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#23581;&#35797;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#25110;&#36807;&#24230;&#31616;&#21270;&#20102;&#19981;&#30456;&#20284;&#24615;&#23646;&#24615;&#65292;&#23548;&#33268;&#24314;&#27169;&#26080;&#25928;&#24182;&#19988;&#26080;&#27861;&#24179;&#34913;&#36825;&#20004;&#20010;&#23646;&#24615;&#12290;&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#22312;&#39057;&#35889;&#22495;&#20013;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#22522;&#20110;&#39057;&#35889;&#30340;GNNs&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24314;&#27169;&#20114;&#34917;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#39057;&#35889;&#30340;&#20114;&#34917;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SComGNN&#65289;&#65292;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#27604;&#36739;&#22909;&#22320;&#21033;&#29992;&#20114;&#34917;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling complementary relationships greatly helps recommender systems to accurately and promptly recommend the subsequent items when one item is purchased. Unlike traditional similar relationships, items with complementary relationships may be purchased successively (such as iPhone and Airpods Pro), and they not only share relevance but also exhibit dissimilarity. Since the two attributes are opposites, modeling complementary relationships is challenging. Previous attempts to exploit these relationships have either ignored or oversimplified the dissimilarity attribute, resulting in ineffective modeling and an inability to balance the two attributes. Since Graph Neural Networks (GNNs) can capture the relevance and dissimilarity between nodes in the spectral domain, we can leverage spectral-based GNNs to effectively understand and model complementary relationships. In this study, we present a novel approach called Spectral-based Complementary Graph Neural Networks (SComGNN) that utilize
&lt;/p&gt;</description></item><item><title>Starling&#26159;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#29255;&#27573;&#19978;&#36827;&#34892;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.02116</link><description>&lt;p&gt;
Starling: &#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#29255;&#27573;&#20013; (arXiv:2401.02116v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment. (arXiv:2401.02116v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02116
&lt;/p&gt;
&lt;p&gt;
Starling&#26159;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#29255;&#27573;&#19978;&#36827;&#34892;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;(HVSS)&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#27491;&#21463;&#21040;&#20851;&#27880;&#12290;&#38543;&#30528;&#21521;&#37327;&#25968;&#25454;&#30340;&#22686;&#38271;&#65292;&#20869;&#23384;&#32034;&#24341;&#21464;&#24471;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#25193;&#23637;&#20027;&#20869;&#23384;&#36164;&#28304;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30913;&#30424;&#30340;&#23454;&#29616;&#65292;&#23558;&#21521;&#37327;&#25968;&#25454;&#23384;&#20648;&#21644;&#25628;&#32034;&#22312;&#39640;&#24615;&#33021;&#35774;&#22791;(&#22914;NVMe SSD)&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#25454;&#29255;&#27573;&#30340;HVSS&#20173;&#28982;&#26159;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#26377;&#22810;&#20010;&#29255;&#27573;&#26469;&#23454;&#29616;&#31995;&#32479;&#21151;&#33021;&#65288;&#22914;&#25193;&#23637;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#29255;&#27573;&#30340;&#20869;&#23384;&#21644;&#30913;&#30424;&#31354;&#38388;&#26377;&#38480;&#65292;&#22240;&#27492;&#25968;&#25454;&#29255;&#27573;&#19978;&#30340;HVSS&#38656;&#35201;&#22312;&#20934;&#30830;&#24615;&#65292;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#21516;&#26102;&#32771;&#34385;&#21040;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Starling&#65292;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#23427;&#22312;&#29255;&#27573;&#20013;&#20248;&#21270;&#25968;&#25454;&#24067;&#23616;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional vector similarity search (HVSS) is receiving a spotlight as a powerful tool for various data science and AI applications. As vector data grows larger, in-memory indexes become extremely expensive because they necessitate substantial expansion of main memory resources. One possible solution is to use disk-based implementation, which stores and searches vector data in high-performance devices like NVMe SSDs. However, HVSS for data segments is still challenging in vector databases, where one machine has multiple segments for system features (like scaling) purposes. In this setting, each segment has limited memory and disk space, so HVSS on the data segment needs to balance accuracy, efficiency, and space cost. Existing disk-based methods are sub-optimal because they do not consider all these requirements together. In this paper, we present Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy in the segment. It has t
&lt;/p&gt;</description></item><item><title>Tailor&#26159;&#19968;&#20010;&#38024;&#23545;&#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#30340;&#23610;&#23544;&#24314;&#35758;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#38544;&#24335;&#21644;&#26174;&#24335;&#29992;&#25143;&#20449;&#21495;&#65292;&#37319;&#29992;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23610;&#23544;&#24314;&#35758;&#12290;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21152;&#36141;&#29289;&#36710;&#30340;&#20132;&#20114;&#22686;&#21152;&#20102;&#29992;&#25143;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.01978</link><description>&lt;p&gt;
Tailor: &#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#30340;&#23610;&#23544;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Tailor: Size Recommendations for High-End Fashion Marketplaces. (arXiv:2401.01978v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01978
&lt;/p&gt;
&lt;p&gt;
Tailor&#26159;&#19968;&#20010;&#38024;&#23545;&#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#30340;&#23610;&#23544;&#24314;&#35758;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#38544;&#24335;&#21644;&#26174;&#24335;&#29992;&#25143;&#20449;&#21495;&#65292;&#37319;&#29992;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23610;&#23544;&#24314;&#35758;&#12290;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21152;&#36141;&#29289;&#36710;&#30340;&#20132;&#20114;&#22686;&#21152;&#20102;&#29992;&#25143;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#21644;&#21160;&#24577;&#30340;&#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#20013;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#23610;&#23544;&#24314;&#35758;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#28385;&#36275;&#39038;&#23458;&#22312;&#36825;&#26041;&#38754;&#30340;&#26399;&#26395;&#19981;&#20165;&#23545;&#30830;&#20445;&#20182;&#20204;&#30340;&#28385;&#24847;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#20063;&#22312;&#20419;&#36827;&#39038;&#23458;&#20445;&#25345;&#65292;&#36825;&#26159;&#20219;&#20309;&#26102;&#23578;&#38646;&#21806;&#21830;&#25104;&#21151;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#38544;&#24335;&#65288;&#21152;&#36141;&#29289;&#36710;&#65289;&#21644;&#26174;&#24335;&#65288;&#36864;&#36135;&#21407;&#22240;&#65289;&#29992;&#25143;&#20449;&#21495;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65306;&#19968;&#20010;&#37319;&#29992;LSTM&#23545;&#29992;&#25143;&#20449;&#21495;&#36827;&#34892;&#32534;&#30721;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#27604;SFNet&#25552;&#39640;&#20102;45.7%&#12290;&#36890;&#36807;&#20351;&#29992;&#21152;&#36141;&#29289;&#36710;&#30340;&#20132;&#20114;&#65292;&#19982;&#20165;&#20351;&#29992;&#35746;&#21333;&#30456;&#27604;&#65292;&#25105;&#20204;&#23558;&#29992;&#25143;&#35206;&#30422;&#33539;&#22260;&#22686;&#21152;&#20102;24.5%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#20197;&#27979;&#37327;&#27169;&#22411;&#30340;&#24310;&#36831;&#24615;&#33021;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#23454;&#26102;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-changing and dynamic realm of high-end fashion marketplaces, providing accurate and personalized size recommendations has become a critical aspect. Meeting customer expectations in this regard is not only crucial for ensuring their satisfaction but also plays a pivotal role in driving customer retention, which is a key metric for the success of any fashion retailer. We propose a novel sequence classification approach to address this problem, integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our approach comprises two distinct models: one employs LSTMs to encode the user signals, while the other leverages an Attention mechanism. Our best model outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions we increase the user coverage by 24.5% when compared with only using Orders. Moreover, we evaluate the models' usability in real-time recommendation scenarios by conducting experiments to measure their latency performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.12728</link><description>&lt;p&gt;
Lookahead:&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#26080;&#25439;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12289;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#25903;&#20184;&#23453;&#36825;&#26679;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#25552;&#20379;&#37325;&#35201;&#37329;&#34701;&#20135;&#21697;&#30340;&#38656;&#35201;&#20934;&#30830;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25903;&#20184;&#23453;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#23558;LLMs&#19982;&#26368;&#20934;&#30830;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#30495;&#23454;&#20135;&#21697;&#26469;&#35828;&#65292;LLMs&#30340;&#25512;&#29702;&#36895;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#23454;&#39564;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;RAG&#31995;&#32479;&#30340;&#36895;&#24230;&#22823;&#24133;&#25552;&#21319;&#21644;&#25104;&#26412;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#26080;&#25439;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#22312;&#20256;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#20196;&#29260;&#37117;&#30001;LLMs&#25353;&#39034;&#24207;&#29983;&#25104;&#65292;&#23548;&#33268;&#30340;&#26102;&#38388;&#28040;&#32791;&#19982;&#29983;&#25104;&#30340;&#20196;&#29260;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33539;&#24335;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19251</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22240;&#26524;&#21435;&#20559;&#35265;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33539;&#24335;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#24314;&#31435;&#33539;&#24335;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#25551;&#36848;&#36890;&#29992;&#20219;&#21153;&#31354;&#38388;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#25104;&#21151;&#22320;&#36866;&#24212;&#35299;&#20915;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#65288;&#22914;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#65289;&#12290;&#21463;&#21040;&#36825;&#26679;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#30740;&#31350;&#20102;&#23558;&#36825;&#31181;&#33539;&#24335;&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35282;&#19979;&#36739;&#23569;&#34987;&#35843;&#26597;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30340;&#36890;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#21040;&#36890;&#29992;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#25552;&#21319;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#65288;&#25968;&#25454;&#26377;&#38480;&#65289;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).  However, unlike vision/language data which share strong conformity in the semant
&lt;/p&gt;</description></item></channel></rss>