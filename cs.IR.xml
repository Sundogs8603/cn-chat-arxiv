<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#26631;&#35760;&#26694;&#26550;&#65292;&#23454;&#29616;&#33258;&#21160;&#20174;&#27861;&#24459;&#25991;&#20214;&#20013;&#25552;&#21462;&#23646;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25552;&#21462;&#30340;&#23646;&#24615;&#22312;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12131</link><description>&lt;p&gt;
&#33258;&#21160;&#20174;&#27861;&#24459;&#31243;&#24207;&#20013;&#25552;&#21462;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Automated Attribute Extraction from Legal Proceedings. (arXiv:2310.12131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#26631;&#35760;&#26694;&#26550;&#65292;&#23454;&#29616;&#33258;&#21160;&#20174;&#27861;&#24459;&#25991;&#20214;&#20013;&#25552;&#21462;&#23646;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25552;&#21462;&#30340;&#23646;&#24615;&#22312;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25346;&#36215;&#26696;&#20214;&#25968;&#37327;&#19981;&#26029;&#19978;&#21319;&#24050;&#25104;&#20026;&#20840;&#29699;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#25968;&#23383;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#37319;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#27861;&#26469;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#24179;&#38754;&#25991;&#26412;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#22788;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#21009;&#20107;&#26696;&#20214;&#31243;&#24207;&#30340;&#22810;&#26679;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#26631;&#35760;&#26694;&#26550;&#26469;&#33258;&#21160;&#20174;&#27861;&#24459;&#25991;&#20214;&#20013;&#25552;&#21462;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20174;&#25552;&#21462;&#30340;&#23646;&#24615;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65288;&#21363;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating number of pending cases is a growing concern world-wide. Recent advancements in digitization have opened up possibilities for leveraging artificial intelligence (AI) tools in the processing of legal documents. Adopting a structured representation for legal documents, as opposed to a mere bag-of-words flat text representation, can significantly enhance processing capabilities. With the aim of achieving this objective, we put forward a set of diverse attributes for criminal case proceedings. We use a state-of-the-art sequence labeling framework to automatically extract attributes from the legal documents. Moreover, we demonstrate the efficacy of the extracted attributes in a downstream task, namely legal judgment prediction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#32593;&#39029;&#34920;&#26684;&#26816;&#32034;&#12290;&#36890;&#36807;&#32771;&#34385;&#25628;&#32034;&#39033;&#30446;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#65292;&#29305;&#21035;&#26159;&#34920;&#26684;&#30340;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;&#20316;&#32773;&#36824;&#22522;&#20110;Doc2Query&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#29992;&#25143;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#26597;&#35810;&#37325;&#26500;&#31574;&#30053;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#30495;&#23454;&#29992;&#25143;&#30740;&#31350;&#30340;&#19968;&#31181;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#30340;&#25104;&#26412;&#33539;&#24335;&#19979;&#32771;&#34385;&#29992;&#25143;&#25928;&#26524;&#23545;&#20110;&#35780;&#20272;&#20132;&#20114;&#24335;&#32593;&#39029;&#34920;&#26684;&#26816;&#32034;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.11931</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#32593;&#39029;&#34920;&#26684;&#26816;&#32034;&#20013;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Simulating Users in Interactive Web Table Retrieval. (arXiv:2310.11931v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#32593;&#39029;&#34920;&#26684;&#26816;&#32034;&#12290;&#36890;&#36807;&#32771;&#34385;&#25628;&#32034;&#39033;&#30446;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#65292;&#29305;&#21035;&#26159;&#34920;&#26684;&#30340;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;&#20316;&#32773;&#36824;&#22522;&#20110;Doc2Query&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#29992;&#25143;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#26597;&#35810;&#37325;&#26500;&#31574;&#30053;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#30495;&#23454;&#29992;&#25143;&#30740;&#31350;&#30340;&#19968;&#31181;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#30340;&#25104;&#26412;&#33539;&#24335;&#19979;&#32771;&#34385;&#29992;&#25143;&#25928;&#26524;&#23545;&#20110;&#35780;&#20272;&#20132;&#20114;&#24335;&#32593;&#39029;&#34920;&#26684;&#26816;&#32034;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#25628;&#32034;&#39033;&#30446;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#23545;&#20110;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#26159;&#26377;&#30410;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32593;&#39029;&#34920;&#26684;&#26816;&#32034;&#65288;WTR&#65289;&#23454;&#39564;&#20013;&#65292;&#32771;&#34385;&#21040;&#34920;&#26684;&#30340;&#22810;&#27169;&#24577;&#23646;&#24615;&#21487;&#20197;&#25552;&#21319;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21333;&#19968;&#27169;&#24577;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20998;&#26512;&#20102;&#22312;&#20020;&#26102;&#26816;&#32034;&#22522;&#20934;&#19979;&#30340;WTR&#24615;&#33021;&#65292;&#23427;&#24573;&#30053;&#20102;&#20132;&#20114;&#24335;&#25628;&#32034;&#34892;&#20026;&#65292;&#24182;&#19988;&#38480;&#21046;&#20102;&#23545;&#20110;&#30495;&#23454;&#29992;&#25143;&#29615;&#22659;&#21547;&#20041;&#30340;&#32467;&#35770;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#27169;&#25311;&#20132;&#20114;&#24335;WTR&#25628;&#32034;&#20250;&#35805;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#20316;&#20026;&#27604;&#30495;&#23454;&#29992;&#25143;&#30740;&#31350;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20316;&#20026;&#39318;&#27425;&#23581;&#35797;&#65292;&#25105;&#20204;&#22522;&#20110;Doc2Query&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#25311;&#29992;&#25143;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#26597;&#35810;&#37325;&#26500;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#25104;&#26412;&#33539;&#24335;&#32771;&#34385;&#29992;&#25143;&#25928;&#26524;&#65292;&#21363;&#25353;&#26597;&#35810;&#21644;&#25353;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering the multimodal signals of search items is beneficial for retrieval effectiveness. Especially in web table retrieval (WTR) experiments, accounting for multimodal properties of tables boosts effectiveness. However, it still remains an open question how the single modalities affect user experience in particular. Previous work analyzed WTR performance in ad-hoc retrieval benchmarks, which neglects interactive search behavior and limits the conclusion about the implications for real-world user environments.  To this end, this work presents an in-depth evaluation of simulated interactive WTR search sessions as a more cost-efficient and reproducible alternative to real user studies. As a first of its kind, we introduce interactive query reformulation strategies based on Doc2Query, incorporating cognitive states of simulated user knowledge. Our evaluations include two perspectives on user effectiveness by considering different cost paradigms, namely query-wise and time-oriented mea
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#31185;&#23398;&#38498;&#20449;&#24687;&#26816;&#32034;&#22242;&#38431;&#65288;CIR&#65289;&#22312;NTCIR-17 ULTRE-2&#20219;&#21153;&#20013;&#37319;&#29992;&#21452;&#21521;&#23398;&#20064;&#31639;&#27861;&#65288;DLA&#65289;&#22788;&#29702;&#20301;&#32622;&#20559;&#24046;&#65292;&#24182;&#35299;&#20915;&#20102;&#30334;&#24230;&#25628;&#32034;&#25968;&#25454;&#20013;&#20005;&#37325;&#30340;&#34394;&#20551;&#36127;&#38754;&#38382;&#39064;&#12290;&#20182;&#20204;&#36890;&#36807;&#32416;&#27491;&#26410;&#34987;&#28857;&#20987;&#39033;&#30446;&#30340;&#26631;&#31614;&#21644;&#24341;&#20837;&#38543;&#26426;&#25991;&#26723;&#21644;&#20855;&#26377;&#37096;&#20998;&#21305;&#37197;&#30340;&#25991;&#26723;&#20316;&#20026;&#36127;&#38754;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11852</link><description>&lt;p&gt;
CIR&#21442;&#19982;NTCIR-17 ULTRE-2&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
CIR at the NTCIR-17 ULTRE-2 Task. (arXiv:2310.11852v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11852
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#31185;&#23398;&#38498;&#20449;&#24687;&#26816;&#32034;&#22242;&#38431;&#65288;CIR&#65289;&#22312;NTCIR-17 ULTRE-2&#20219;&#21153;&#20013;&#37319;&#29992;&#21452;&#21521;&#23398;&#20064;&#31639;&#27861;&#65288;DLA&#65289;&#22788;&#29702;&#20301;&#32622;&#20559;&#24046;&#65292;&#24182;&#35299;&#20915;&#20102;&#30334;&#24230;&#25628;&#32034;&#25968;&#25454;&#20013;&#20005;&#37325;&#30340;&#34394;&#20551;&#36127;&#38754;&#38382;&#39064;&#12290;&#20182;&#20204;&#36890;&#36807;&#32416;&#27491;&#26410;&#34987;&#28857;&#20987;&#39033;&#30446;&#30340;&#26631;&#31614;&#21644;&#24341;&#20837;&#38543;&#26426;&#25991;&#26723;&#21644;&#20855;&#26377;&#37096;&#20998;&#21305;&#37197;&#30340;&#25991;&#26723;&#20316;&#20026;&#36127;&#38754;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#31185;&#23398;&#38498;&#20449;&#24687;&#26816;&#32034;&#22242;&#38431;&#65288;CIR&#65289;&#21442;&#21152;&#20102;NTCIR-17 ULTRE-2&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;ULTRE-2&#20219;&#21153;&#20013;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#22312;&#30334;&#24230;&#25628;&#32034;&#25968;&#25454;&#20013;&#65292;&#34394;&#20551;&#36127;&#38754;&#32467;&#26524;&#30340;&#38382;&#39064;&#38750;&#24120;&#20005;&#37325;&#65292;&#27604;&#20301;&#32622;&#20559;&#24046;&#26356;&#21152;&#20005;&#37325;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21452;&#21521;&#23398;&#20064;&#31639;&#27861;&#65288;DLA&#65289;&#26469;&#35299;&#20915;&#20301;&#32622;&#20559;&#24046;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#30740;&#31350;&#22914;&#20309;&#32531;&#35299;&#34394;&#20551;&#36127;&#38754;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;1&#65289;&#36890;&#36807;&#20174;DLA&#35757;&#32451;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#27169;&#22411;&#26469;&#32416;&#27491;&#26410;&#34987;&#28857;&#20987;&#39033;&#30446;&#30340;&#26631;&#31614;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#25490;&#24207;&#22120;&#65292;&#35813;&#25490;&#24207;&#22120;&#30001;DLA&#21021;&#22987;&#21270;&#65307;2&#65289;&#23558;&#38543;&#26426;&#25991;&#26723;&#20316;&#20026;&#30495;&#36127;&#38754;&#21644;&#20855;&#26377;&#37096;&#20998;&#21305;&#37197;&#30340;&#25991;&#26723;&#20316;&#20026;&#38590;&#36127;&#38754;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#22312;nDCG@10&#19978;&#36798;&#21040;&#20102;0.5355&#65292;&#27604;&#32452;&#32455;&#32773;&#30340;&#26368;&#20339;&#24471;&#20998;&#25552;&#39640;&#20102;2.66%&#12290;
&lt;/p&gt;
&lt;p&gt;
The Chinese academy of sciences Information Retrieval team (CIR) has participated in the NTCIR-17 ULTRE-2 task. This paper describes our approaches and reports our results on the ULTRE-2 task. We recognize the issue of false negatives in the Baidu search data in this competition is very severe, much more severe than position bias. Hence, we adopt the Dual Learning Algorithm (DLA) to address the position bias and use it as an auxiliary model to study how to alleviate the false negative issue. We approach the problem from two perspectives: 1) correcting the labels for non-clicked items by a relevance judgment model trained from DLA, and learn a new ranker that is initialized from DLA; 2) including random documents as true negatives and documents that have partial matching as hard negatives. Both methods can enhance the model performance and our best method has achieved nDCG@10 of 0.5355, which is 2.66% better than the best score from the organizer.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#28145;&#24230;&#20132;&#21449;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21442;&#25968;&#20849;&#20139;&#65292;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#30340;&#25104;&#21151;&#29575;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.11777</link><description>&lt;p&gt;
DCRNN: &#19968;&#31181;&#22522;&#20110;RNN&#30340;&#28145;&#24230;&#20132;&#21449;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing in Multi-task Learning. (arXiv:2310.11777v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#28145;&#24230;&#20132;&#21449;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21442;&#25968;&#20849;&#20139;&#65292;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#30340;&#25104;&#21151;&#29575;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;DL&#21457;&#23637;&#36805;&#36895;&#65292;&#20010;&#24615;&#21270;&#26381;&#21153;&#27491;&#25506;&#32034;&#20351;&#29992;DL&#31639;&#27861;&#26469;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#20010;&#24615;&#21270;&#26381;&#21153;&#26469;&#35828;&#65292;&#25104;&#21151;&#30340;&#25512;&#33616;&#21253;&#25324;&#21560;&#24341;&#29992;&#25143;&#28857;&#20987;&#39033;&#30446;&#21644;&#29992;&#25143;&#24895;&#24847;&#28040;&#36153;&#39033;&#30446;&#20004;&#20010;&#26041;&#38754;&#12290;&#22914;&#26524;&#21516;&#26102;&#38656;&#35201;&#39044;&#27979;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20250;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#32321;&#29712;&#19988;&#19981;&#33021;&#26377;&#25928;&#22320;&#24314;&#27169;&#8220;&#28857;&#20987;-&#28040;&#36153;&#8221;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#30340;&#25104;&#21151;&#29575;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#23581;&#35797;&#24314;&#27169;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#30828;&#21442;&#25968;&#20849;&#20139;&#25110;&#36719;&#21442;&#25968;&#20849;&#20139;&#26550;&#26500;&#65292;&#20294;&#36825;&#20004;&#31181;&#26550;&#26500;&#21508;&#33258;&#23384;&#22312;&#19968;&#23450;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#25512;&#33616;&#30340;&#26032;&#22411;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, DL has developed rapidly, and personalized services are exploring using DL algorithms to improve the performance of the recommendation system. For personalized services, a successful recommendation consists of two parts: attracting users to click the item and users being willing to consume the item. If both tasks need to be predicted at the same time, traditional recommendation systems generally train two independent models. This approach is cumbersome and does not effectively model the relationship between the two subtasks of "click-consumption". Therefore, in order to improve the success rate of recommendation and reduce computational costs, researchers are trying to model multi-task learning.  At present, existing multi-task learning models generally adopt hard parameter sharing or soft parameter sharing architecture, but these two architectures each have certain problems. Therefore, in this work, we propose a novel recommendation model based on real recommendation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22120;(FER)&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#26126;&#39564;&#35777;&#32773;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#20107;&#23454;&#39564;&#35777;&#20013;&#30340;&#35777;&#25454;&#26816;&#32034;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;FER&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11675</link><description>&lt;p&gt;
&#20174;&#30456;&#20851;&#24615;&#21040;&#23454;&#29992;&#24615;: &#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification. (arXiv:2310.11675v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22120;(FER)&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#26126;&#39564;&#35777;&#32773;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#20107;&#23454;&#39564;&#35777;&#20013;&#30340;&#35777;&#25454;&#26816;&#32034;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;FER&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#65292;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#24050;&#25104;&#20026;&#20027;&#35201;&#30340;&#26041;&#27861;&#20043;&#19968;&#65307;&#23427;&#38656;&#35201;&#23545;&#22810;&#20010;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#39564;&#35777;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#26816;&#32034;&#35777;&#25454;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#21407;&#21017;&#35774;&#35745;&#30340;&#29616;&#25104;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#30340;&#26159;&#22768;&#26126;&#39564;&#35777;&#32773;&#20174;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#20013;&#33719;&#24471;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#19981;&#26159;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22120;&#65288;FER&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#26126;&#39564;&#35777;&#32773;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#35777;&#25454;&#26816;&#32034;&#36807;&#31243;&#12290;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#25105;&#20204;&#20351;&#29992;&#39564;&#35777;&#32773;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21644;&#22522;&#20934;&#35777;&#25454;&#20043;&#38388;&#23454;&#29992;&#24615;&#30340;&#24046;&#24322;&#26469;&#20135;&#29983;&#26368;&#32456;&#30340;&#22768;&#26126;&#26631;&#31614;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;FER&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the probability ranking principle. We argue that, rather than relevance, for FV we need to focus on the utility that a claim verifier derives from the retrieved evidence. We introduce the feedback-based evidence retriever(FER) that optimizes the evidence retrieval process by incorporating feedback from the claim verifier. As a feedback signal we use the divergence in utility between how effectively the verifier utilizes the retrieved evidence and the ground-truth evidence to produce the final claim label. Empirical studies demonstrate the superiority of FER over prevailing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#21363;&#20174;&#35270;&#39057;&#25991;&#26412;&#20013;&#25552;&#21462;&#23618;&#27425;&#21270;&#20851;&#38190;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#25286;&#20998;&#20219;&#21153;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23454;&#29616;&#26041;&#26696;&#65292;&#21363;PipVKIE&#21644;UniVKIE&#12290;&#20004;&#31181;&#26041;&#26696;&#37117;&#21033;&#29992;&#20102;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22352;&#26631;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.11650</link><description>&lt;p&gt;
VKIE:&#24212;&#29992;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#20110;&#35270;&#39057;&#25991;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VKIE: The Application of Key Information Extraction on Video Text. (arXiv:2310.11650v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#21363;&#20174;&#35270;&#39057;&#25991;&#26412;&#20013;&#25552;&#21462;&#23618;&#27425;&#21270;&#20851;&#38190;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#25286;&#20998;&#20219;&#21153;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23454;&#29616;&#26041;&#26696;&#65292;&#21363;PipVKIE&#21644;UniVKIE&#12290;&#20004;&#31181;&#26041;&#26696;&#37117;&#21033;&#29992;&#20102;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22352;&#26631;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#23545;&#20110;&#34892;&#19994;&#20013;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#20174;&#35270;&#39057;&#25991;&#26412;&#20013;&#25552;&#21462;&#23618;&#27425;&#21270;&#20851;&#38190;&#20449;&#24687;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#20854;&#25286;&#20998;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23454;&#29616;&#26041;&#26696;&#65292;&#20998;&#21035;&#31216;&#20026;PipVKIE&#21644;UniVKIE&#12290;PipVKIE&#25353;&#29031;&#36830;&#32493;&#38454;&#27573;&#39034;&#24207;&#23436;&#25104;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#32780;UniVKIE&#36890;&#36807;&#23558;&#25152;&#26377;&#23376;&#20219;&#21153;&#32479;&#19968;&#21040;&#19968;&#20010;&#20027;&#24178;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;PipVKIE&#21644;UniVKIE&#37117;&#21033;&#29992;&#20102;&#26469;&#33258;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22352;&#26631;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouples it into four subtasks and introduce two implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially completes the four subtasks in continuous stages, while UniVKIE is improved by unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage multimodal information from vision, text, and coordinates for feature representation. Extensive experiments on one well-defined dataset demonstrate that our solutions can achieve remarkable performance and efficient inference speed. The code and dataset will be publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#65288;OIE&#65289;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#39046;&#22495;&#30340;&#20851;&#31995;&#26469;&#25913;&#21892;&#20851;&#31995;&#25552;&#21462;&#25216;&#26415;&#65292;&#36991;&#20813;&#25163;&#21160;&#26631;&#35760;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#65292;&#24182;&#27010;&#36848;&#20102;OIE&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.11644</link><description>&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#65306;&#22522;&#20934;&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction: A Review of Baseline Techniques, Approaches, and Applications. (arXiv:2310.11644v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#65288;OIE&#65289;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#39046;&#22495;&#30340;&#20851;&#31995;&#26469;&#25913;&#21892;&#20851;&#31995;&#25552;&#21462;&#25216;&#26415;&#65292;&#36991;&#20813;&#25163;&#21160;&#26631;&#35760;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#65292;&#24182;&#27010;&#36848;&#20102;OIE&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#37327;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#25991;&#26412;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#38656;&#35201;&#25552;&#21462;&#30701;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#29992;&#20960;&#20010;&#35789;&#27719;&#24635;&#27599;&#20010;&#25991;&#26723;&#30340;&#20027;&#35201;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#19968;&#30452;&#36827;&#34892;&#20102;&#35768;&#22810;&#20851;&#20110;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#65288;OIE&#65289;&#30340;&#30740;&#31350;&#12290;OIE&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#39046;&#22495;&#30340;&#20851;&#31995;&#26469;&#25913;&#36827;&#20851;&#31995;&#25552;&#21462;&#25216;&#26415;&#65292;&#24182;&#36991;&#20813;&#22312;&#21477;&#23376;&#20013;&#35201;&#27714;&#25163;&#21160;&#26631;&#35760;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;OIE&#30340;&#26368;&#26032;&#26041;&#27861;&#20197;&#21450;&#20854;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;OIE&#22522;&#30784;&#26041;&#27861;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#12290;&#23427;&#31616;&#35201;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#26041;&#27861;&#21644;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#23427;&#23545;OIE&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;OIE&#24212;&#29992;&#30340;&#25361;&#25112;&#12289;&#24320;&#25918;&#38382;&#39064;&#21644;&#26410;&#26469;&#24037;&#20316;&#26426;&#20250;&#36827;&#34892;&#20102;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the abundant amount of available online and offline text data, there arises a crucial need to extract the relation between phrases and summarize the main content of each document in a few words. For this purpose, there have been many studies recently in Open Information Extraction (OIE). OIE improves upon relation extraction techniques by analyzing relations across different domains and avoids requiring hand-labeling pre-specified relations in sentences. This paper surveys recent approaches of OIE and its applications on Knowledge Graph (KG), text summarization, and Question Answering (QA). Moreover, the paper describes OIE basis methods in relation extraction. It briefly discusses the main approaches and the pros and cons of each method. Finally, it gives an overview about challenges, open issues, and future work opportunities for OIE, relation extraction, and OIE applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#38024;&#23545;&#25512;&#33616;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#22270;&#25299;&#25169;&#29305;&#24449;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11270</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;: &#21487;&#22797;&#29616;&#24615;&#12289;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Recommendation: Reproducibility, Graph Topology, and Node Representation. (arXiv:2310.11270v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#38024;&#23545;&#25512;&#33616;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#22270;&#25299;&#25169;&#29305;&#24449;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#30697;&#38453;&#34920;&#31034;&#20026;&#19968;&#20010;&#20108;&#37096;&#22270;&#21644;&#26080;&#21521;&#22270;&#65292;GNN&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;-&#29289;&#21697;&#20043;&#38388;&#30340;&#36817;&#36317;&#31163;&#21644;&#36828;&#36317;&#31163;&#20132;&#20114;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#25512;&#33616;&#26041;&#27861;&#23398;&#20064;&#21040;&#26356;&#20934;&#30830;&#30340;&#20559;&#22909;&#27169;&#24335;&#12290;&#19982;&#20043;&#21069;&#30340;&#21516;&#31867;&#25945;&#31243;&#19981;&#21516;&#65292;&#26412;&#25945;&#31243;&#26088;&#22312;&#23637;&#31034;&#21644;&#25506;&#35752;&#25512;&#33616;&#20013;GNNs&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#65288;i&#65289;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#65288;ii&#65289;&#22270;&#25299;&#25169;&#29305;&#24449;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22312;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#29305;&#24449;&#25110;&#21033;&#29992;&#39044;&#35757;&#32451;&#23884;&#20837;&#20316;&#20026;&#39069;&#22806;&#29289;&#21697;&#20449;&#24687;&#65288;&#20363;&#22914;&#22810;&#27169;&#24577;&#29305;&#24449;&#65289;&#26102;&#65292;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#19977;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#35270;&#35282;&#65292;&#30446;&#21069;&#22312;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#20105;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained prominence in recommendation systems in recent years. By representing the user-item matrix as a bipartite and undirected graph, GNNs have demonstrated their potential to capture short- and long-distance user-item interactions, thereby learning more accurate preference patterns than traditional recommendation approaches. In contrast to previous tutorials on the same topic, this tutorial aims to present and examine three key aspects that characterize GNNs for recommendation: (i) the reproducibility of state-of-the-art approaches, (ii) the potential impact of graph topological characteristics on the performance of these models, and (iii) strategies for learning node representations when training features from scratch or utilizing pre-trained embeddings as additional item information (e.g., multimodal features). The goal is to provide three novel theoretical and practical perspectives on the field, currently subject to debate in graph learning but l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.09716</link><description>&lt;p&gt;
&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#30340;&#20449;&#24687;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09716
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#37325;&#20889;&#22312;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#29420;&#31435;&#24418;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20154;&#24037;&#37325;&#20889;&#30340;&#26597;&#35810;&#20316;&#20026;&#26631;&#31614;&#26469;&#35757;&#32451;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#37325;&#20889;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#20449;&#24687;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#37325;&#35201;&#29305;&#24615;&#26469;&#23450;&#20041;&#35268;&#33539;&#30340;&#37325;&#20889;&#65292;&#24182;&#23558;&#20854;&#20840;&#37096;&#32435;&#20837;&#25351;&#20196;&#20013;&#12290;&#27492;&#22806;&#65292;&#24403;&#21021;&#22987;&#26597;&#35810;&#37325;&#20889;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMs&#30340;&#37325;&#20889;&#32534;&#36753;&#22120;&#30340;&#35282;&#33394;&#65292;&#24418;&#25104;&#19968;&#20010;&#8220;&#37325;&#20889;-&#32534;&#36753;&#8221;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;LLMs&#30340;&#37325;&#20889;&#33021;&#21147;&#25552;&#28860;&#25104;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#37325;&#20889;&#24310;&#36831;&#12290;&#25105;&#20204;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can y
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05809</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#20013;&#37319;&#29992;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#37319;&#29992;&#19968;&#31181;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#65292;&#21521;&#27599;&#20010;&#29992;&#25143;&#25552;&#20379;&#30456;&#21516;&#31243;&#24230;&#30340;&#35299;&#37322;&#65292;&#32780;&#19981;&#32771;&#34385;&#20182;&#20204;&#30340;&#20010;&#20307;&#38656;&#27714;&#21644;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#22823;&#22810;&#20197;&#38745;&#24577;&#21644;&#38750;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#22522;&#20110;&#20854;&#38656;&#27714;&#21644;&#20559;&#22909;&#36827;&#34892;&#20132;&#20114;&#12289;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65288;&#22522;&#26412;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#65289;&#65292;&#24182;&#22312;&#36879;&#26126;&#30340;&#25512;&#33616;&#21644;&#20852;&#36259;&#24314;&#27169;&#24212;&#29992;&#65288;RIMA&#65289;&#20013;&#23454;&#29616;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#65292;&#20197;&#35843;&#26597;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#23545;&#29992;&#25143;&#23545;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
&lt;/p&gt;</description></item><item><title>CLaMP&#26159;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#23398;&#20064;&#31526;&#21495;&#38899;&#20048;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#65292;&#23427;&#23558;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#25104;&#38271;&#24230;&#19981;&#21040;10&#65285;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.11029</link><description>&lt;p&gt;
CLaMP&#65306;&#29992;&#20110;&#36328;&#27169;&#24577;&#31526;&#21495;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval. (arXiv:2304.11029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11029
&lt;/p&gt;
&lt;p&gt;
CLaMP&#26159;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#23398;&#20064;&#31526;&#21495;&#38899;&#20048;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#65292;&#23427;&#23558;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#25104;&#38271;&#24230;&#19981;&#21040;10&#65285;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLaMP&#65306;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#65292;&#23427;&#20351;&#29992;&#38899;&#20048;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#32852;&#21512;&#35757;&#32451;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#21644;&#31526;&#21495;&#38899;&#20048;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#20026;&#20102;&#39044;&#35757;&#32451;CLaMP&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;140&#19975;&#20010;&#38899;&#20048;-&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23427;&#20351;&#29992;&#20102;&#25991;&#26412;&#38543;&#26426;&#22833;&#27963;&#26469;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#38899;&#20048;&#25968;&#25454;&#65292;&#20174;&#32780;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#21040;&#19981;&#21040;10&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;CLaMP&#38598;&#25104;&#20102;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#25903;&#25345;&#35821;&#20041;&#25628;&#32034;&#21644;&#38899;&#20048;&#20998;&#31867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;WikiMusicText&#65288;WikiMT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1010&#20010;ABC&#31526;&#21495;&#35889;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#35889;&#37117;&#38468;&#24102;&#26377;&#26631;&#39064;&#12289;&#33402;&#26415;&#23478;&#12289;&#27969;&#27966;&#21644;&#25551;&#36848;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#26469;&#25913;&#36827;&#26816;&#32034;&#25928;&#26524;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09473</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#31232;&#30095;&#22810;&#31890;&#24230;&#23398;&#20064;&#36827;&#34892;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#26469;&#25913;&#36827;&#26816;&#32034;&#25928;&#26524;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65292;&#26368;&#36817;&#22312;&#25506;&#32034;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#23398;&#20064;&#35270;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#31232;&#30095;&#27010;&#24565;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#27599;&#20010;&#27010;&#24565;&#37117;&#23545;&#24212;&#19968;&#20123;&#35789;&#35821;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#25105;&#20204;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#21644;&#26356;&#26032;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#21644;&#23545;&#40784;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#22810;&#31890;&#24230;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#24103;&#34920;&#31034;&#26041;&#27861;&#32435;&#20837;&#27169;&#22411;&#65292;&#26356;&#22909;&#22320;&#23545;&#35270;&#39057;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#21644;&#35745;&#31639;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S3MA&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/yim&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at https://github.com/yim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#25968;&#25454;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35757;&#32451;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#25968;&#25454;&#21457;&#29616;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.03560</link><description>&lt;p&gt;
Solo: &#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#25968;&#25454;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach. (arXiv:2301.03560v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#25968;&#25454;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35757;&#32451;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#25968;&#25454;&#21457;&#29616;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24050;&#37096;&#32626;&#30340;&#25968;&#25454;&#21457;&#29616;&#31995;&#32479;&#65292;&#20363;&#22914;Google Datasets&#21644;&#24320;&#25918;&#25968;&#25454;&#38376;&#25143;&#65292;&#21482;&#25903;&#25345;&#20851;&#38190;&#35789;&#25628;&#32034;&#12290;&#20851;&#38190;&#35789;&#25628;&#32034;&#36866;&#29992;&#20110;&#26222;&#36890;&#29992;&#25143;&#65292;&#20294;&#38480;&#21046;&#20102;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#30340;&#26597;&#35810;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#20010;&#23398;&#20064;&#30340;&#25968;&#25454;&#21457;&#29616;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#32452;&#35013;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#30340;&#23398;&#20064;&#21457;&#29616;&#31995;&#32479;&#12290;&#23427;&#38656;&#35201;&#35299;&#20915;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#30340;&#35774;&#35745;&#20197;&#36827;&#34892;&#25968;&#25454;&#21457;&#29616;&#12289;&#29992;&#20110;&#36755;&#20837;&#27169;&#22411;&#30340;&#34920;&#26684;&#34920;&#31034;&#31574;&#30053;&#21644;&#33021;&#19982;&#21512;&#25104;&#29983;&#25104;&#30340;&#38382;&#39064;&#24456;&#22909;&#24037;&#20316;&#30340;&#30456;&#20851;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#19978;&#36848;&#36129;&#29486;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#21517;&#20026;Solo&#30340;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#25216;&#26415;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most deployed data discovery systems, such as Google Datasets, and open data portals only support keyword search. Keyword search is geared towards general audiences but limits the types of queries the systems can answer. We propose a new system that lets users write natural language questions directly. A major barrier to using this learned data discovery system is it needs expensive-to-collect training data, thus limiting its utility. In this paper, we introduce a self-supervised approach to assemble training datasets and train learned discovery systems without human intervention. It requires addressing several challenges, including the design of self-supervised strategies for data discovery, table representation strategies to feed to the models, and relevance models that work well with the synthetically generated questions. We combine all the above contributions into a system, Solo, that solves the problem end to end. The evaluation results demonstrate the new techniques outperform st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2103.03223</link><description>&lt;p&gt;
&#37327;&#21270;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comparative Evaluation of Quantification Methods. (arXiv:2103.03223v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#25351;&#22312;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#31867;&#21035;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#23427;&#20063;&#20195;&#34920;&#30528;&#19968;&#20010;&#22312;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20221;&#20840;&#38754;&#30340;&#23454;&#35777;&#27604;&#36739;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20197;&#25903;&#25345;&#31639;&#27861;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#30340;&#24443;&#24213;&#23454;&#35777;&#24615;&#24615;&#33021;&#27604;&#36739;&#65292;&#21253;&#25324;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#37327;&#21270;&#35774;&#32622;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27809;&#26377;&#21333;&#19968;&#31639;&#27861;&#33021;&#22815;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#22810;&#20998;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21478;&#19968;&#32452;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;Generaliz&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification represents the problem of predicting class distributions in a dataset. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on overall more than 40 data sets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the threshold selection-based Median Sweep and TSMax methods, the DyS framework, and Friedman's method that performs best in the binary setting. For the multiclass setting, we observe that a different group of algorithms yields good performance, including the Generaliz
&lt;/p&gt;</description></item></channel></rss>