<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12273</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#25913;&#36827;VTE&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12273
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#38745;&#33033;&#34880;&#26643;&#26643;&#22622;&#65288;VTE&#65289;&#65292;&#21253;&#25324;&#28145;&#38745;&#33033;&#34880;&#26643;&#65288;DVT&#65289;&#21644;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#65292;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#22312;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;VTE&#20107;&#20214;&#25110;&#24110;&#21161;&#20020;&#24202;&#19987;&#23478;&#35782;&#21035;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;VTE&#20107;&#20214;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#26377;&#38480;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#26377;&#25928;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#27169;&#22411;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DL&#26041;&#27861;&#30340;&#26032;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#12289;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;NLP&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#38750;&#32467;&#26500;&#21270;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;VTE&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
&lt;/p&gt;</description></item><item><title>SR-PredictAO&#26159;&#19968;&#31181;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.12218</link><description>&lt;p&gt;
SR-PredictAO: &#20855;&#26377;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#38468;&#21152;&#20214;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On. (arXiv:2309.12218v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12218
&lt;/p&gt;
&lt;p&gt;
SR-PredictAO&#26159;&#19968;&#31181;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20165;&#22522;&#20110;&#21333;&#20010;&#20250;&#35805;&#20013;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#39033;&#30446;&#28857;&#20987;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#26576;&#20123;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#38656;&#35201;&#19968;&#20010;&#39640;&#33021;&#21147;&#30340;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21160;&#20316;&#30340;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#29616;&#26377;&#27169;&#22411;&#36981;&#24490;&#32534;&#30721;&#22120;-&#39044;&#27979;&#22120;&#33539;&#24335;&#65292;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#24191;&#27867;&#20248;&#21270;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#22914;&#20309;&#20248;&#21270;&#39044;&#27979;&#22120;&#27169;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#23384;&#22312;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#31216;&#20026;\emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#33021;&#21147;&#30340;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#21487;&#20197;&#20943;&#36731;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation, aiming at making the prediction of the user's next item click based on the information in a single session only even in the presence of some random user's behavior, is a complex problem. This complex problem requires a high-capability model of predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm where all studies focus on how to optimize the encoder module extensively in the paradigm but they ignore how to optimize the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module which could alleviate the effect of random user's behavior for prediction. It is worth mentioning that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#30340;&#24403;&#21069;&#21457;&#23637;&#24773;&#20917;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.12158</link><description>&lt;p&gt;
&#36808;&#21521;&#40065;&#26834;&#21644;&#30495;&#27491;&#22823;&#35268;&#27169;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval. (arXiv:2309.12158v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#30340;&#24403;&#21069;&#21457;&#23637;&#24773;&#20917;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#19968;&#31995;&#21015;&#24212;&#29992;&#38598;&#20013;&#22312;&#35299;&#20915;&#23558;&#22823;&#37327;&#20048;&#35889;&#22270;&#20687;&#19982;&#30456;&#24212;&#30340;&#38899;&#39057;&#35760;&#24405;&#36830;&#25509;&#36215;&#26469;&#30340;&#38382;&#39064;&#65292;&#21363;&#35782;&#21035;&#24341;&#29992;&#30456;&#21516;&#38899;&#20048;&#20869;&#23481;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#25688;&#24405;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#20856;&#22411;&#26041;&#27861;&#26159;&#20351;&#29992;&#36328;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#23398;&#20064;&#23558;&#20004;&#31181;&#19981;&#21516;&#27169;&#24577;&#65288;&#38899;&#39057;&#21644;&#20048;&#35889;&#22270;&#20687;&#65289;&#36830;&#25509;&#36215;&#26469;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#26377;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#24403;&#21069;&#22312;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#26041;&#38754;&#30340;&#21457;&#23637;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23454;&#29616;&#40065;&#26834;&#21644;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#38899;&#20048;&#26816;&#32034;&#30340;&#19968;&#31995;&#21015;&#20027;&#35201;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#36804;&#20170;&#20026;&#27490;&#25105;&#20204;&#24050;&#32463;&#37319;&#21462;&#30340;&#19968;&#20123;&#27493;&#39588;&#26469;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
A range of applications of multi-modal music information retrieval is centred around the problem of connecting large collections of sheet music (images) to corresponding audio recordings, that is, identifying pairs of audio and score excerpts that refer to the same musical content. One of the typical and most recent approaches to this task employs cross-modal deep learning architectures to learn joint embedding spaces that link the two distinct modalities - audio and sheet music images. While there has been steady improvement on this front over the past years, a number of open problems still prevent large-scale employment of this methodology. In this article we attempt to provide an insightful examination of the current developments on audio-sheet music retrieval via deep learning methods. We first identify a set of main challenges on the road towards robust and large-scale cross-modal music retrieval in real scenarios. We then highlight the steps we have taken so far to address some o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#31283;&#20581;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#27604;&#20004;&#31181;&#27169;&#24577;&#30340;&#38543;&#26426;&#22686;&#24378;&#35270;&#22270;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#22312;&#25152;&#26377;&#22330;&#26223;&#20013;&#26356;&#31934;&#30830;&#22320;&#26816;&#32034;&#29255;&#27573;&#12290;</title><link>http://arxiv.org/abs/2309.12134</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#31283;&#20581;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems. (arXiv:2309.12134v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#31283;&#20581;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#27604;&#20004;&#31181;&#27169;&#24577;&#30340;&#38543;&#26426;&#22686;&#24378;&#35270;&#22270;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#22312;&#25152;&#26377;&#22330;&#26223;&#20013;&#26356;&#31934;&#30830;&#22320;&#26816;&#32034;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20048;&#35889;&#22270;&#20687;&#19982;&#38899;&#39057;&#24405;&#38899;&#36827;&#34892;&#38142;&#25509;&#20173;&#28982;&#26159;&#24320;&#21457;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#38899;&#20048;&#26816;&#32034;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#19968;&#31181;&#22522;&#26412;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#36830;&#25509;&#30701;&#38899;&#39057;&#29255;&#27573;&#21644;&#20048;&#35889;&#30340;&#36328;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#38899;&#20048;&#20869;&#23481;&#30340;&#24102;&#27880;&#37322;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#24433;&#21709;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#30495;&#23454;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#32531;&#35299;&#36825;&#20010;&#38480;&#21046;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#26292;&#38706;&#22312;&#22823;&#37327;&#30495;&#23454;&#38899;&#20048;&#25968;&#25454;&#20013;&#20316;&#20026;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#23545;&#27604;&#38543;&#26426;&#22686;&#24378;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#22270;&#20687;&#29255;&#27573;&#30340;&#35270;&#22270;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#38050;&#29748;&#25968;&#25454;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#22312;&#25152;&#26377;&#22330;&#26223;&#21644;&#39044;&#35757;&#32451;&#37197;&#32622;&#20013;&#26356;&#31934;&#30830;&#22320;&#26816;&#32034;&#29255;&#27573;&#12290;&#22312;&#36825;&#20123;&#32467;&#26524;&#30340;&#40723;&#33310;&#19979;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Linking sheet music images to audio recordings remains a key problem for the development of efficient cross-modal music retrieval systems. One of the fundamental approaches toward this task is to learn a cross-modal embedding space via deep neural networks that is able to connect short snippets of audio and sheet music. However, the scarcity of annotated data from real musical content affects the capability of such methods to generalize to real retrieval scenarios. In this work, we investigate whether we can mitigate this limitation with self-supervised contrastive learning, by exposing a network to a large amount of real music data as a pre-training step, by contrasting randomly augmented views of snippets of both modalities, namely audio and sheet images. Through a number of experiments on synthetic and real piano data, we show that pre-trained models are able to retrieve snippets with better precision in all scenarios and pre-training configurations. Encouraged by these results, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#24490;&#29615;&#27169;&#22411;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#27169;&#24577;&#24490;&#29615;&#32593;&#32476;&#29983;&#25104;&#21487;&#20197;&#25688;&#35201;&#23545;&#24212;&#38899;&#39057;&#21644;&#20048;&#35889;&#30340;&#26356;&#38271;&#27573;&#33853;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#24369;&#23545;&#40784;&#30340;&#38899;&#39057;-&#20048;&#35889;&#23545;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;&#21644;&#20048;&#35889;&#20043;&#38388;&#30340;&#33410;&#22863;&#21464;&#21270;&#23548;&#33268;&#30340;&#38750;&#32447;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12111</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#27169;&#22411;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#30340;&#27573;&#33853;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval. (arXiv:2309.12111v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#24490;&#29615;&#27169;&#22411;&#30340;&#38899;&#39057;-&#20048;&#35889;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#27169;&#24577;&#24490;&#29615;&#32593;&#32476;&#29983;&#25104;&#21487;&#20197;&#25688;&#35201;&#23545;&#24212;&#38899;&#39057;&#21644;&#20048;&#35889;&#30340;&#26356;&#38271;&#27573;&#33853;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#24369;&#23545;&#40784;&#30340;&#38899;&#39057;-&#20048;&#35889;&#23545;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;&#21644;&#20048;&#35889;&#20043;&#38388;&#30340;&#33410;&#22863;&#21464;&#21270;&#23548;&#33268;&#30340;&#38750;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#38899;&#20048;&#26816;&#32034;&#30340;&#35768;&#22810;&#24212;&#29992;&#19982;&#23558;&#20048;&#35889;&#22270;&#20687;&#19982;&#38899;&#39057;&#24405;&#38899;&#36830;&#25509;&#22312;&#19968;&#36215;&#26377;&#20851;&#12290;&#30446;&#21069;&#30340;&#20856;&#22411;&#26041;&#27861;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#20851;&#32852;&#30701;&#22266;&#23450;&#22823;&#23567;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#29255;&#27573;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#30456;&#20284;&#24615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#24102;&#26469;&#30340;&#20004;&#20010;&#25361;&#25112;&#26159;&#35757;&#32451;&#32593;&#32476;&#38656;&#35201;&#24378;&#23545;&#40784;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#30001;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#36895;&#24230;&#24046;&#24322;&#32780;&#36896;&#25104;&#30340;&#38899;&#39057;&#21644;&#20048;&#35889;&#29255;&#27573;&#20043;&#38388;&#30340;&#38899;&#20048;&#20869;&#23481;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36328;&#27169;&#24577;&#24490;&#29615;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#32570;&#28857;&#65292;&#35813;&#32593;&#32476;&#23398;&#20064;&#21487;&#20197;&#25688;&#35201;&#23545;&#24212;&#38899;&#39057;&#21644;&#20048;&#35889;&#30340;&#26356;&#38271;&#27573;&#33853;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#23427;&#21482;&#38656;&#35201;&#24369;&#23545;&#40784;&#30340;&#38899;&#39057;-&#20048;&#35889;&#23545;&#65292;&#20197;&#21450;&#24490;&#29615;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#38899;&#39057;&#21644;&#20048;&#35889;&#20043;&#38388;&#30340;&#33410;&#22863;&#21464;&#21270;&#23548;&#33268;&#30340;&#38750;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGPIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21098;&#26525;&#29992;&#25143;&#22270;&#21644;&#24847;&#22270;&#22270;&#35299;&#20915;&#31354;&#38388;&#24322;&#36136;&#24615;&#21644;&#31232;&#30095;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#36866;&#29992;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.11741</link><description>&lt;p&gt;
&#25581;&#31034;&#26368;&#20339;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#36335;&#24452;&#65306;&#21033;&#29992;&#22270;&#21098;&#26525;&#21644;&#24847;&#22270;&#22270;&#36827;&#34892;&#26377;&#25928;&#25512;&#33616;&#30340;&#21019;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations. (arXiv:2309.11741v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGPIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21098;&#26525;&#29992;&#25143;&#22270;&#21644;&#24847;&#22270;&#22270;&#35299;&#20915;&#31354;&#38388;&#24322;&#36136;&#24615;&#21644;&#31232;&#30095;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#36866;&#29992;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#22320;&#21306;&#20419;&#36827;&#29983;&#24577;&#12289;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#36164;&#28304;&#21487;&#25345;&#32493;&#24615;&#30340;&#37325;&#35201;&#25163;&#27573;&#26159;&#25512;&#33616;&#36866;&#24403;&#30340;&#21457;&#23637;&#36335;&#24452;&#65292;&#20063;&#31216;&#20026;&#29983;&#24577;&#25991;&#26126;&#27169;&#24335;&#65288;&#21363;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#20805;&#20998;&#35299;&#20915;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#31354;&#38388;&#24322;&#36136;&#24615;&#21644;&#21306;&#22495;&#21382;&#21490;&#20114;&#21160;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25512;&#33616;&#21487;&#25345;&#32493;&#21457;&#23637;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21098;&#26525;&#29992;&#25143;&#22270;&#21644;&#24847;&#22270;&#22270;&#65288;UGPIG&#65289;&#8221;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of appropriate development pathways, also known as ecological civilization patterns for achieving Sustainable Development Goals (namely, sustainable development patterns), are of utmost importance for promoting ecological, economic, social, and resource sustainability in a specific region. To achieve this, the recommendation process must carefully consider the region's natural, environmental, resource, and economic characteristics. However, current recommendation algorithms in the field of computer science fall short in adequately addressing the spatial heterogeneity related to environment and sparsity of regional historical interaction data, which limits their effectiveness in recommending sustainable development patterns. To overcome these challenges, this paper proposes a method called User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the high-density linking capability of the pruned User Graph to address the issue of spatial heterogeneity neg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20505;&#36873;&#38598;&#30340;&#36873;&#25321;&#31574;&#30053;&#19982;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#21457;&#29616;&#25277;&#26679;&#30340;&#20505;&#36873;&#38598;&#21487;&#20197;&#20943;&#23569;&#24230;&#37327;&#20272;&#35745;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.11723</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;Top-N&#25512;&#33616;&#30340;&#20505;&#36873;&#38598;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Candidate Set Sampling for Evaluating Top-N Recommendation. (arXiv:2309.11723v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20505;&#36873;&#38598;&#30340;&#36873;&#25321;&#31574;&#30053;&#19982;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#21457;&#29616;&#25277;&#26679;&#30340;&#20505;&#36873;&#38598;&#21487;&#20197;&#20943;&#23569;&#24230;&#37327;&#20272;&#35745;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#30340;Top-N&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#65292;&#36873;&#25321;&#20505;&#36873;&#38598;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#37325;&#35201;&#20915;&#31574;&#12290;&#20505;&#36873;&#38598;&#26159;&#30001;&#29992;&#25143;&#30340;&#27979;&#35797;&#29289;&#21697;&#21644;&#19968;&#23450;&#25968;&#37327;&#30340;&#38750;&#30456;&#20851;&#29289;&#21697;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#35825;&#39285;&#29289;&#21697;&#65289;&#30340;&#24182;&#38598;&#32452;&#25104;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#19981;&#21516;&#20505;&#36873;&#38598;&#22823;&#23567;&#21644;&#36873;&#25321;&#31574;&#30053;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20505;&#36873;&#38598;&#36873;&#25321;&#31574;&#30053;&#19982;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#29305;&#23450;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#25277;&#26679;&#30340;&#20505;&#36873;&#38598;&#26159;&#21542;&#23548;&#33268;&#24230;&#37327;&#20272;&#35745;&#23545;&#20110;&#36890;&#24120;&#22312;&#23454;&#39564;&#20013;&#19981;&#21487;&#29992;&#30340;&#23436;&#25972;&#25968;&#25454;&#26356;&#23569;&#22320;&#23384;&#22312;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strategy for selecting candidate sets -- the set of items that the recommendation system is expected to rank for each user -- is an important decision in carrying out an offline top-$N$ recommender system evaluation. The set of candidates is composed of the union of the user's test items and an arbitrary number of non-relevant items that we refer to as decoys. Previous studies have aimed to understand the effect of different candidate set sizes and selection strategies on evaluation. In this paper, we extend this knowledge by studying the specific interaction of candidate set selection strategies with popularity bias, and use simulation to assess whether sampled candidate sets result in metric estimates that are less biased with respect to the true metric values under complete data that is typically unavailable in ordinary experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SE-PEF&#65292;&#19968;&#20010;&#29992;&#20110;&#20010;&#24615;&#21270;&#19987;&#23478;&#26597;&#25214;&#30340;&#36164;&#28304;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#36229;&#36807;25&#19975;&#20010;&#26597;&#35810;&#21644;56.5&#19975;&#20010;&#31572;&#26696;&#65292;&#24182;&#20351;&#29992;&#19968;&#22871;&#20016;&#23500;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SE-PEF&#36866;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#26377;&#25928;&#30340;&#19987;&#23478;&#26597;&#25214;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.11686</link><description>&lt;p&gt;
SE-PEF:&#19968;&#20010;&#29992;&#20110;&#20010;&#24615;&#21270;&#19987;&#23478;&#26597;&#25214;&#30340;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
SE-PEF: a Resource for Personalized Expert Finding. (arXiv:2309.11686v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11686
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SE-PEF&#65292;&#19968;&#20010;&#29992;&#20110;&#20010;&#24615;&#21270;&#19987;&#23478;&#26597;&#25214;&#30340;&#36164;&#28304;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#36229;&#36807;25&#19975;&#20010;&#26597;&#35810;&#21644;56.5&#19975;&#20010;&#31572;&#26696;&#65292;&#24182;&#20351;&#29992;&#19968;&#22871;&#20016;&#23500;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SE-PEF&#36866;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#26377;&#25928;&#30340;&#19987;&#23478;&#26597;&#25214;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#20449;&#24687;&#26816;&#32034;&#30340;&#38382;&#39064;&#24050;&#32463;&#34987;&#30740;&#31350;&#20102;&#24456;&#38271;&#26102;&#38388;&#12290;&#19982;&#36825;&#39033;&#20219;&#21153;&#30456;&#20851;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#20197;&#25903;&#25345;&#20010;&#24615;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;&#20026;&#20102;&#22312;&#36825;&#26041;&#38754;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;SE-PEF&#65288;StackExchange-&#20010;&#24615;&#21270;&#19987;&#23478;&#26597;&#25214;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#19982;&#19987;&#23478;&#26597;&#25214;&#65288;EF&#65289;&#20219;&#21153;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#25152;&#36129;&#29486;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;3306&#20010;&#19987;&#23478;&#30340;&#36229;&#36807;25&#19975;&#20010;&#26597;&#35810;&#21644;56.5&#19975;&#20010;&#31572;&#26696;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#19968;&#22871;&#20016;&#23500;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#28909;&#38376;cQA&#24179;&#21488;&#19978;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#21021;&#27493;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SE-PEF&#36866;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#26377;&#25928;&#30340;EF&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of personalization in Information Retrieval has been under study for a long time. A well-known issue related to this task is the lack of publicly available datasets that can support a comparative evaluation of personalized search systems. To contribute in this respect, this paper introduces SE-PEF (StackExchange - Personalized Expert Finding), a resource useful for designing and evaluating personalized models related to the task of Expert Finding (EF). The contributed dataset includes more than 250k queries and 565k answers from 3 306 experts, which are annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. The results of the preliminary experiments conducted show the appropriateness of SE-PEF to evaluate and to train effective EF models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#23545;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#65292;&#26435;&#37325;&#30456;&#20851;&#30697;&#38453;&#20998;&#35299;&#21644;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#23545;&#20110;&#19981;&#22826;&#27969;&#34892;&#30340;&#33402;&#26415;&#23478;&#26469;&#35828;&#65292;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30456;&#23545;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.11671</link><description>&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#20013;&#30340;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Popularity Degradation Bias in Local Music Recommendation. (arXiv:2309.11671v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#23545;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#65292;&#26435;&#37325;&#30456;&#20851;&#30697;&#38453;&#20998;&#35299;&#21644;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#23545;&#20110;&#19981;&#22826;&#27969;&#34892;&#30340;&#33402;&#26415;&#23478;&#26469;&#35828;&#65292;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30456;&#23545;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#23545;&#26412;&#22320;&#38899;&#20048;&#25512;&#33616;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#20004;&#31181;&#26368;&#20339;&#25512;&#33616;&#31639;&#27861;&#65292;&#21363;&#26435;&#37325;&#30456;&#20851;&#30697;&#38453;&#20998;&#35299;&#65288;WRMF&#65289;&#21644;&#22810;&#39033;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;Mult-VAE&#65289;&#22312;&#25512;&#33616;&#33402;&#26415;&#23478;&#26102;&#30340;&#20934;&#30830;&#24615;&#19982;&#33402;&#26415;&#23478;&#27969;&#34892;&#24230;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#34920;&#29616;&#20986;&#27969;&#34892;&#24230;&#34928;&#20943;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#25512;&#33616;&#27969;&#34892;&#33402;&#26415;&#23478;&#26041;&#38754;&#30340;&#34920;&#29616;&#27700;&#24179;&#30456;&#20284;&#65292;&#20294;&#23545;&#20110;&#19981;&#22826;&#27969;&#34892;&#30340;&#33402;&#26415;&#23478;&#26469;&#35828;&#65292;Mult-VAE&#30456;&#23545;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;&#22312;&#26412;&#22320;&#65288;&#38271;&#23614;&#65289;&#38899;&#20048;&#33402;&#26415;&#23478;&#25512;&#33616;&#20013;&#65292;&#24212;&#35813;&#20248;&#20808;&#36873;&#25321;&#36825;&#20010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the effect of popularity degradation bias in the context of local music recommendations. Specifically, we examine how accurate two top-performing recommendation algorithms, Weight Relevance Matrix Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at recommending artists as a function of artist popularity. We find that both algorithms improve recommendation performance for more popular artists and, as such, exhibit popularity degradation bias. While both algorithms produce a similar level of performance for more popular artists, Mult-VAE shows better relative performance for less popular artists. This suggests that this algorithm should be preferred for local (long-tail) music artist recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36127;&#38754;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#39034;&#24207;&#38899;&#20048;&#25512;&#33616;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11623</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#36830;&#32493;&#38899;&#20048;&#25512;&#33616;&#36827;&#34892;&#36127;&#38754;&#20449;&#21495;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation. (arXiv:2309.11623v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36127;&#38754;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#39034;&#24207;&#38899;&#20048;&#25512;&#33616;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#20208;&#36182;&#20854;&#25512;&#33616;&#24341;&#25806;&#36830;&#32493;&#21521;&#29992;&#25143;&#25552;&#20379;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#39034;&#24207;&#25512;&#33616;&#24050;&#32463;&#24341;&#36215;&#20102;&#24403;&#21069;&#25991;&#29486;&#30340;&#30456;&#24403;&#20851;&#27880;&#65292;&#32780;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#38271;&#26399;&#21644;&#30701;&#26399;&#29992;&#25143;&#21382;&#21490;&#21644;&#39033;&#30446;&#29305;&#24449;&#65289;&#30340;&#33258;&#25105;&#20851;&#27880;&#27169;&#22411;&#19978;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#38271;&#26684;&#24335;&#20869;&#23481;&#39046;&#22495;&#65288;&#38646;&#21806;&#12289;&#30005;&#24433;&#31561;&#65289;&#32780;&#19981;&#26159;&#30701;&#26684;&#24335;&#65292;&#20363;&#22914;&#38899;&#20048;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#30740;&#31350;&#26410;&#25506;&#32034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#34701;&#20837;&#36127;&#38754;&#20250;&#35805;&#32423;&#21453;&#39304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;&#33258;&#25105;&#20851;&#27880;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23398;&#20064;&#29992;&#20110;&#39034;&#24207;&#38899;&#20048;&#25512;&#33616;&#30340;&#38544;&#24335;&#20250;&#35805;&#32423;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#34701;&#20837;&#36127;&#38754;&#21453;&#39304;&#65288;&#20363;&#22914;&#36339;&#36807;&#30340;&#26354;&#30446;&#65289;&#20197;&#20419;&#36827;&#27491;&#38754;&#21629;&#20013;&#24182;&#24809;&#32602;&#36127;&#38754;&#21629;&#20013;&#12290;&#36825;&#20010;&#20219;&#21153;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25439;&#22833;&#39033;&#65292;&#21487;&#20197;&#21152;&#20837;&#21040;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music streaming services heavily rely on their recommendation engines to continuously provide content to their consumers. Sequential recommendation consequently has seen considerable attention in current literature, where state of the art approaches focus on self-attentive models leveraging contextual information such as long and short-term user history and item features; however, most of these studies focus on long-form content domains (retail, movie, etc.) rather than short-form, such as music. Additionally, many do not explore incorporating negative session-level feedback during training. In this study, we investigate the use of transformer-based self-attentive architectures to learn implicit session-level information for sequential music recommendation. We additionally propose a contrastive learning task to incorporate negative feedback (e.g skipped tracks) to promote positive hits and penalize negative hits. This task is formulated as a simple loss term that can be incorporated in
&lt;/p&gt;</description></item><item><title>TrueLearn&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20102;&#24320;&#25918;&#23398;&#20064;&#32773;&#30340;&#27010;&#24565;&#21644;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11527</link><description>&lt;p&gt;
TrueLearn: &#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#20449;&#24687;&#25512;&#33616;&#30340;Python&#24211;&#65288;&#24102;&#26377;&#65288;&#38544;&#24335;&#65289;&#21453;&#39304;&#65289;
&lt;/p&gt;
&lt;p&gt;
TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11527
&lt;/p&gt;
&lt;p&gt;
TrueLearn&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20102;&#24320;&#25918;&#23398;&#20064;&#32773;&#30340;&#27010;&#24565;&#21644;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TrueLearn Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#25945;&#32946;&#65288;&#25110;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#20449;&#24687;&#65289;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#32452;&#27169;&#22411;&#26159;&#26681;&#25454;&#8220;&#24320;&#25918;&#23398;&#20064;&#32773;&#8221;&#30340;&#27010;&#24565;&#35774;&#35745;&#30340;&#65292;&#20351;&#29992;&#30452;&#35266;&#30340;&#29992;&#25143;&#34920;&#36798;&#12290;&#20026;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#35753;&#29992;&#25143;&#26377;&#25511;&#21046;&#24863;&#65292;TrueLearn&#24211;&#36824;&#21253;&#21547;&#19981;&#21516;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#24110;&#21161;&#26368;&#32456;&#29992;&#25143;&#21487;&#35270;&#21270;&#23398;&#20064;&#32773;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#23558;&#26469;&#29992;&#25143;&#19982;&#33258;&#24049;&#30340;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#19982;&#35813;&#24211;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#21069;&#20844;&#24320;&#21457;&#24067;&#30340;&#38544;&#24335;&#21453;&#39304;&#25945;&#32946;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#20351;&#35813;&#24211;&#23545;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21644;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#20174;&#19994;&#32773;&#37117;&#38750;&#24120;&#26131;&#20110;&#20351;&#29992;&#12290;&#35813;&#24211;&#21644;&#24102;&#26377;&#31034;&#20363;&#30340;&#25903;&#25345;&#25991;&#26723;&#21487;&#22312;https&#65306;//&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https:/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.11518</link><description>&lt;p&gt;
&#22312;&#20869;&#23481;&#24066;&#22330;&#20013;&#30340;&#31163;&#32447;&#23398;&#20064;&#19979;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Ad-load Balancing via Off-policy Learning in a Content Marketplace. (arXiv:2309.11518v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#26159;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#32972;&#26223;&#19979;&#65292;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;&#20256;&#32479;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#20381;&#36182;&#20110;&#38745;&#24577;&#20998;&#37197;&#31574;&#30053;&#65292;&#26080;&#27861;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#20381;&#25454;&#35760;&#24405;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#24191;&#21578;&#25910;&#20837;&#20043;&#38388;&#30340;&#20914;&#31361;&#30446;&#26631;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30001;&#20110;&#29992;&#25143;&#24322;&#36136;&#24615;&#21644;&#29992;&#25143;&#22312;&#20250;&#35805;&#20013;&#30340;&#20301;&#32622;&#30340;&#20381;&#36182;&#24615;&#32780;&#24341;&#36215;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22312;&#29305;&#23450;&#30340;&#20869;&#23481;&#33719;&#21462;&#20013;&#30830;&#23450;&#26368;&#20248;&#24191;&#21578;&#36127;&#36733;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ad-load balancing is a critical challenge in online advertising systems, particularly in the context of social media platforms, where the goal is to maximize user engagement and revenue while maintaining a satisfactory user experience. This requires the optimization of conflicting objectives, such as user satisfaction and ads revenue. Traditional approaches to ad-load balancing rely on static allocation policies, which fail to adapt to changing user preferences and contextual factors. In this paper, we present an approach that leverages off-policy learning and evaluation from logged bandit feedback. We start by presenting a motivating analysis of the ad-load balancing problem, highlighting the conflicting objectives between user satisfaction and ads revenue. We emphasize the nuances that arise due to user heterogeneity and the dependence on the user's position within a session. Based on this analysis, we define the problem as determining the optimal ad-load for a particular feed fetch.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#36827;&#34892;&#31169;&#26377;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#12289;&#26131;&#35843;&#25972;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11516</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#30340;&#31169;&#26377;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Private Matrix Factorization with Public Item Features. (arXiv:2309.11516v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11516
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#36827;&#34892;&#31169;&#26377;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#12289;&#26131;&#35843;&#25972;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#31169;&#26377;&#25512;&#33616;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#38556;&#65292;&#20294;&#20250;&#23548;&#33268;&#25512;&#33616;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#20844;&#20849;&#39033;&#30446;&#29305;&#24449;&#21487;&#20197;&#24110;&#21161;&#32531;&#35299;&#25512;&#33616;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20307;&#30697;&#38453;&#20998;&#35299;&#65288;CMF&#65289;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23545;&#20004;&#20010;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#65306;&#29992;&#25143;&#21453;&#39304;&#30697;&#38453;&#65288;&#20195;&#34920;&#25935;&#24863;&#25968;&#25454;&#65289;&#21644;&#19968;&#20010;&#32534;&#30721;&#20844;&#24320;&#21487;&#29992;&#65288;&#38750;&#25935;&#24863;&#65289;&#39033;&#30446;&#20449;&#24687;&#30340;&#39033;&#30446;&#29305;&#24449;&#30697;&#38453;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#26131;&#20110;&#35843;&#25972;&#65292;&#32780;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20844;&#20849;&#39033;&#30446;&#25968;&#25454;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#20998;&#31867;&#39033;&#30446;&#29305;&#24449;&#65307;&#65288;2&#65289;&#20174;&#20844;&#20849;&#26469;&#28304;&#23398;&#20064;&#30340;&#39033;&#30446;&#38388;&#30456;&#20284;&#24615;&#65307;&#20197;&#21450;&#65288;3&#65289;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24577;&#21487;&#20197;&#20849;&#21516;&#21033;&#29992;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of training private recommendation models with access to public item features. Training with Differential Privacy (DP) offers strong privacy guarantees, at the expense of loss in recommendation quality. We show that incorporating public item features during training can help mitigate this loss in quality. We propose a general approach based on collective matrix factorization (CMF), that works by simultaneously factorizing two matrices: the user feedback matrix (representing sensitive data) and an item feature matrix that encodes publicly available (non-sensitive) item information.  The method is conceptually simple, easy to tune, and highly scalable. It can be applied to different types of public item data, including: (1) categorical item features; (2) item-item similarities learned from public sources; and (3) publicly available user feedback. Furthermore, these data modalities can be collectively utilized to fully leverage public data.  Evaluating our method o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.11515</link><description>&lt;p&gt;
&#36816;&#29992;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11515
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#20013;&#39640;&#35843;&#30340;&#38544;&#31169;&#27844;&#38706;&#20107;&#20214;&#39057;&#32321;&#21457;&#29983;&#65292;&#29992;&#25143;&#23545;&#38544;&#31169;&#36234;&#26469;&#36234;&#20851;&#27880;&#12290;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#22312;&#32447;&#24179;&#21488;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20854;&#38544;&#31169;&#20445;&#25252;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#21482;&#32771;&#34385;&#38745;&#24577;&#21644;&#29420;&#31435;&#30340;&#29992;&#25143;&#20132;&#20114;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#20851;&#27880;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#21482;&#20445;&#25252;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#31216;&#20026;DIPSGNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#22312;&#32447;&#25968;&#25454;&#28304;&#30340;&#22270;&#29255;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#24314;&#27169;&#22312;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20256;&#32479;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.11510</link><description>&lt;p&gt;
&#20309;&#26102;&#31639;&#26159;&#22522;&#30784;&#27169;&#22411;&#65311;&#65288;arXiv:2309.11510v1 [cs.IR]&#65289;
&lt;/p&gt;
&lt;p&gt;
When is a Foundation Model a Foundation Model. (arXiv:2309.11510v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#22312;&#32447;&#25968;&#25454;&#28304;&#30340;&#22270;&#29255;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#24314;&#27169;&#22312;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20256;&#32479;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#20960;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;&#26469;&#33258;Twitter&#21644;PubMed&#31561;&#22312;&#32447;&#25968;&#25454;&#28304;&#30340;&#22270;&#29255;&#36827;&#34892;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#29992;&#20110;&#22270;&#20687;-&#25991;&#26412;&#24314;&#27169;&#12290;&#22522;&#37329;&#20250;&#27169;&#22411;&#26159;&#22823;&#22411;&#12289;&#28145;&#24230;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#24322;&#24120;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#23398;&#20064;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#19982;&#36739;&#23567;&#30340;&#20256;&#32479;&#28145;&#24230;&#32593;&#32476;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, several studies have reported on the fine-tuning of foundation models for image-text modeling in the field of medicine, utilizing images from online data sources such as Twitter and PubMed. Foundation models are large, deep artificial neural networks capable of learning the context of a specific domain through training on exceptionally extensive datasets. Through validation, we have observed that the representations generated by such models exhibit inferior performance in retrieval tasks within digital pathology when compared to those generated by significantly smaller, conventional deep networks.
&lt;/p&gt;</description></item><item><title>AdBooster&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#26550;&#26500;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#20316;&#30340;&#21019;&#24847;&#20248;&#21270;&#65292;&#20854;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2309.11507</link><description>&lt;p&gt;
AdBooster&#65306;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#24847;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting. (arXiv:2309.11507v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11507
&lt;/p&gt;
&lt;p&gt;
AdBooster&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#26550;&#26500;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#20316;&#30340;&#21019;&#24847;&#20248;&#21270;&#65292;&#20854;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#24191;&#21578;&#20013;&#65292;&#26368;&#20339;&#25512;&#33616;&#39033;&#30446;&#65288;&#25512;&#33616;&#39033;&#65289;&#30340;&#36873;&#25321;&#21644;&#26368;&#20339;&#21019;&#24847;&#23637;&#31034;&#65288;&#21019;&#24847;&#20248;&#21270;&#65289;&#20256;&#32479;&#19978;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#23398;&#31185;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#37117;&#26377;&#26174;&#33879;&#36129;&#29486;&#65292;&#36825;&#26159;&#22522;&#20110;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#21644;&#23637;&#31034;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#35270;&#35273;&#21019;&#24847;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#29983;&#25104;&#21019;&#24847;&#20248;&#21270;&#65288;GCO&#65289;&#8221;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21019;&#24847;&#29983;&#25104;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#20197;&#21450;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#22806;&#26223;&#26550;&#26500;&#30340;&#8220;AdBooster&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20010;&#24615;&#21270;&#24191;&#21578;&#21019;&#24847;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24494;&#35843;&#21644;&#29983;&#25104;&#26102;&#37117;&#21487;&#20197;&#29420;&#29305;&#22320;&#32467;&#21512;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;AdBooster&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AdBooster&#22312;&#29983;&#25104;&#26356;&#22810;&#31934;&#24425;&#21019;&#24847;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital advertising, the selection of the optimal item (recommendation) and its best creative presentation (creative optimization) have traditionally been considered separate disciplines. However, both contribute significantly to user satisfaction, underpinning our assumption that it relies on both an item's relevance and its presentation, particularly in the case of visual creatives. In response, we introduce the task of {\itshape Generative Creative Optimization (GCO)}, which proposes the use of generative models for creative generation that incorporate user interests, and {\itshape AdBooster}, a model for personalized ad creatives based on the Stable Diffusion outpainting architecture. This model uniquely incorporates user interests both during fine-tuning and at generation time. To further improve AdBooster's performance, we also introduce an automated data augmentation pipeline. Through our experiments on simulated data, we validate AdBooster's effectiveness in generating more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.11506</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Table Metadata with Business Glossaries Using Large Language Models. (arXiv:2309.11506v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36890;&#24120;&#25317;&#26377;&#22823;&#37327;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20197;&#22823;&#22411;&#25968;&#25454;&#24211;&#25110;&#20225;&#19994;&#25968;&#25454;&#28246;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#20803;&#25968;&#25454;&#21644;&#20005;&#26684;&#30340;&#35775;&#38382;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#23545;&#25968;&#25454;&#20869;&#23481;&#30340;&#35775;&#38382;&#65292;&#24182;&#22240;&#27492;&#38480;&#21046;&#20102;&#32463;&#20856;&#30340;&#26816;&#32034;&#21644;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#20803;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#21253;&#21547;&#25968;&#25454;&#26631;&#31614;&#21644;&#25551;&#36848;&#30340;&#19994;&#21153;&#35789;&#27719;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#21033;&#29992;&#21487;&#29992;&#25110;&#31574;&#21010;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35268;&#21017;&#25110;&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#21015;&#21517;&#21644;&#35789;&#27719;&#25551;&#36848;&#65288;&#25110;&#23427;&#20204;&#30340;&#21521;&#37327;&#23884;&#20837;&#65289;&#20043;&#38388;&#25214;&#21040;&#26368;&#21305;&#37197;&#30340;&#39033;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#25163;&#21160;&#26631;&#27880;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#35768;&#22810;&#19994;&#21153;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprises often own large collections of structured data in the form of large databases or an enterprise data lake. Such data collections come with limited metadata and strict access policies that could limit access to the data contents and, therefore, limit the application of classic retrieval and analysis solutions. As a result, there is a need for solutions that can effectively utilize the available metadata. In this paper, we study the problem of matching table metadata to a business glossary containing data labels and descriptions. The resulting matching enables the use of an available or curated business glossary for retrieval and analysis without or before requesting access to the data contents. One solution to this problem is to use manually-defined rules or similarity measures on column names and glossary descriptions (or their vector embeddings) to find the closest match. However, such approaches need to be tuned through manual labeling and cannot handle many business gloss
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#19982;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#22256;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35748;&#30693;&#25928;&#29992;&#32972;&#26223;&#19979;&#23454;&#29616;&#20844;&#27491;&#30340;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.11503</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#19982;&#20010;&#24615;&#21270;&#65306;&#36808;&#21521;&#35748;&#30693;&#25928;&#29992;&#30340;&#20844;&#27491;
&lt;/p&gt;
&lt;p&gt;
Fairness Vs. Personalization: Towards Equity in Epistemic Utility. (arXiv:2309.11503v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#19982;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#22256;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35748;&#30693;&#25928;&#29992;&#32972;&#26223;&#19979;&#23454;&#29616;&#20844;&#27491;&#30340;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#24212;&#29992;&#33539;&#22260;&#27491;&#22312;&#36805;&#36895;&#25193;&#23637;&#65292;&#28085;&#30422;&#31038;&#20132;&#23186;&#20307;&#12289;&#22312;&#32447;&#36141;&#29289;&#12289;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#31561;&#39046;&#22495;&#12290;&#36825;&#20123;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#26469;&#27983;&#35272;&#22823;&#37327;&#21487;&#29992;&#30340;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21457;&#23637;&#65292;&#20154;&#20204;&#23545;&#31639;&#27861;&#31995;&#32479;&#21487;&#33021;&#23384;&#22312;&#24182;&#24310;&#32493;&#20559;&#35265;&#30340;&#28508;&#21147;&#26377;&#20102;&#26356;&#22810;&#35748;&#35782;&#65292;&#23384;&#22312;&#20010;&#24615;&#21270;&#39046;&#22495;&#20013;&#30340;&#19981;&#20844;&#24179;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20010;&#24615;&#21270;&#19982;&#20256;&#32479;&#20844;&#24179;&#23454;&#29616;&#20043;&#38388;&#22266;&#26377;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#35748;&#30693;&#25928;&#29992;&#32972;&#26223;&#19979;&#23454;&#29616;&#20844;&#27491;&#30340;&#20844;&#24179;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30446;&#26631;&#21644;&#23454;&#38469;&#23454;&#29616;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#20197;&#22312;&#20010;&#24615;&#21270;&#31995;&#32479;&#20013;&#23454;&#29616;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The applications of personalized recommender systems are rapidly expanding: encompassing social media, online shopping, search engine results, and more. These systems offer a more efficient way to navigate the vast array of items available. However, alongside this growth, there has been increased recognition of the potential for algorithmic systems to exhibit and perpetuate biases, risking unfairness in personalized domains. In this work, we explicate the inherent tension between personalization and conventional implementations of fairness. As an alternative, we propose equity to achieve fairness in the context of epistemic utility. We provide a mapping between goals and practical implementations and detail policy recommendations across key stakeholders to forge a path towards achieving fairness in personalized systems.
&lt;/p&gt;</description></item><item><title>Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08617</link><description>&lt;p&gt;
Drifter: &#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#20197;&#25552;&#39640;&#25968;&#25454;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;
Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08617
&lt;/p&gt;
&lt;p&gt;
Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#22312;&#22823;&#35268;&#27169;&#12289;&#21160;&#24577;&#27969;&#20013;&#32500;&#25252;&#25968;&#25454;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Drifter&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#21644;&#39564;&#35777;&#30340;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#12290;Drifter&#36890;&#36807;&#25552;&#20379;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12289;&#28418;&#31227;&#26816;&#27979;&#20197;&#21450;&#23545;&#26377;&#38382;&#39064;&#30340;&#29983;&#20135;&#20107;&#20214;&#30340;&#27934;&#23519;&#12290;Drifter&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25968;&#25454;&#22312;&#32447;&#29305;&#24449;&#25490;&#21517;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#27599;&#20998;&#38047;&#22788;&#29702;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#20165;&#38656;&#35201;&#20004;&#20010;&#32447;&#31243;&#21644;&#23569;&#20110;&#19968;GB&#30340;RAM&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;Drifter&#22312;&#35686;&#25253;&#21644;&#32531;&#35299;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23454;&#26102;&#23454;&#20917;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09683</link><description>&lt;p&gt;
PubMed&#21450;&#20854;&#20182;&#65306;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search. (arXiv:2307.09683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20854;&#20013;&#24456;&#22810;&#21482;&#33021;&#36890;&#36807;&#25991;&#29486;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#26816;&#32034;&#26159;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#22312;&#20808;&#21069;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23558;&#21151;&#33021;&#25193;&#23637;&#21040;&#20102;&#36229;&#36234;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25628;&#32034;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#21487;&#33021;&#23545;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#36824;&#27604;&#36739;&#38476;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#29305;&#23450;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#22320;&#28385;&#36275;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;PubMed&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#25913;&#36827;&#21644;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20116;&#31181;&#29305;&#23450;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65306;1.&#20026;&#24490;&#35777;&#21307;&#23398;&#23547;&#25214;&#39640;&#36136;&#37327;&#20020;&#24202;&#30740;&#31350;&#12290;2.&#20026;&#31934;&#20934;&#21307;&#23398;&#21644;&#22522;&#22240;&#32452;&#23398;&#26816;&#32034;&#22522;&#22240;&#30456;&#20851;&#20449;&#24687;&#12290;3.&#26681;&#25454;&#24847;&#20041;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.11963</link><description>&lt;p&gt;
&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;:&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#12289;&#30693;&#35782;&#21040;&#26234;&#24935;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Multimodality Fusion for Smart Healthcare: a Journey from Data, Information, Knowledge to Wisdom. (arXiv:2306.11963v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#24050;&#25104;&#20026;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#19968;&#31181;&#38761;&#26032;&#24615;&#26041;&#27861;&#65292;&#33021;&#22815;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20026;&#26234;&#24935;&#21307;&#30103;&#24102;&#26469;&#30340;&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#21644;&#30693;&#35782;&#21040;&#26234;&#24935;&#65288;DIKW&#65289;&#20043;&#26053;&#12290;&#20840;&#38754;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#38598;&#25104;&#26041;&#24335;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#30528;&#37325;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#36848;&#30340;&#26694;&#26550;&#21644;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#26410;&#26469;&#19982;&#39044;&#27979;&#12289;&#39044;&#38450;&#12289;&#20010;&#24615;&#21270;&#21644;&#27835;&#30103;&#26377;&#20851;&#30340;&#21307;&#30103;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal medical data fusion has emerged as a transformative approach in smart healthcare, enabling a comprehensive understanding of patient health and personalized treatment plans. In this paper, a journey from data, information, and knowledge to wisdom (DIKW) is explored through multimodal fusion for smart healthcare. A comprehensive review of multimodal medical data fusion focuses on the integration of various data modalities are presented. It explores different approaches such as Feature selection, Rule-based systems, Machine learning, Deep learning, and Natural Language Processing for fusing and analyzing multimodal data. The paper also highlights the challenges associated with multimodal fusion in healthcare. By synthesizing the reviewed frameworks and insights, a generic framework for multimodal medical data fusion is proposed while aligning with the DIKW mechanism. Moreover, it discusses future directions aligned with the four pillars of healthcare: Predictive, Preventive, Pe
&lt;/p&gt;</description></item></channel></rss>