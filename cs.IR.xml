<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>WikiMT++&#26159;&#19968;&#20010;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#30340;WikiMusicText&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#23427;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21487;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;CLaMP&#26469;&#32416;&#27491;&#23646;&#24615;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13259</link><description>&lt;p&gt;
WikiMT++&#25968;&#25454;&#38598;&#21345;&#29255;
&lt;/p&gt;
&lt;p&gt;
WikiMT++ Dataset Card. (arXiv:2309.13259v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13259
&lt;/p&gt;
&lt;p&gt;
WikiMT++&#26159;&#19968;&#20010;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#30340;WikiMusicText&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#23427;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21487;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;CLaMP&#26469;&#32416;&#27491;&#23646;&#24615;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WikiMT++&#26159;WikiMusicText&#65288;WikiMT&#65289;&#30340;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#20026;&#20102;&#25193;&#23637;WikiMT&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#65288;&#19987;&#36753;&#12289;&#27468;&#35789;&#12289;&#35270;&#39057;&#65289;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65288;12&#20010;&#24773;&#24863;&#24418;&#23481;&#35789;&#65289;&#21644;&#24773;&#24863;4Q&#65288;Russell 4Q&#65289;&#65292;&#22686;&#24378;&#20102;&#20854;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#12289;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#12289;&#33258;&#21160;&#20316;&#26354;&#21644;&#24773;&#24863;&#20998;&#31867;&#31561;&#26041;&#38754;&#30340;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;CLaMP&#26469;&#32416;&#27491;&#20174;WikiMT&#32487;&#25215;&#30340;&#23646;&#24615;&#65292;&#20197;&#20943;&#23569;&#21407;&#22987;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38169;&#35823;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
WikiMT++ is an expanded and refined version of WikiMusicText (WikiMT), featuring 1010 curated lead sheets in ABC notation. To expand application scenarios of WikiMT, we add both objective (album, lyrics, video) and subjective emotion (12 emotion adjectives) and emo\_4q (Russell 4Q) attributes, enhancing its usability for music information retrieval, conditional music generation, automatic composition, and emotion classification, etc. Additionally, CLaMP is implemented to correct the attributes inherited from WikiMT to reduce errors introduced during original data collection and enhance the accuracy and completeness of our dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;LayoutLMv3&#21644;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#65292;&#29992;&#20110;&#35782;&#21035;&#20256;&#30495;&#36716;&#35786;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#36716;&#35786;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.13184</link><description>&lt;p&gt;
&#21307;&#30103;&#36716;&#35786;&#30340;&#25991;&#20214;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Document Understanding for Healthcare Referrals. (arXiv:2309.13184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;LayoutLMv3&#21644;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#65292;&#29992;&#20110;&#35782;&#21035;&#20256;&#30495;&#36716;&#35786;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#36716;&#35786;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#25195;&#25551;&#25991;&#26723;&#21644;&#20256;&#30495;&#36890;&#20449;&#30340;&#21307;&#30103;&#36716;&#35786;&#23548;&#33268;&#20102;&#39640;&#26114;&#30340;&#34892;&#25919;&#25104;&#26412;&#21644;&#21487;&#33021;&#24433;&#21709;&#30149;&#20154;&#25252;&#29702;&#30340;&#38169;&#35823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;LayoutLMv3&#21644;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#26469;&#35782;&#21035;&#20256;&#30495;&#36716;&#35786;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#30149;&#20154;&#12289;&#21307;&#29983;&#21644;&#26816;&#26597;&#30456;&#20851;&#23454;&#20307;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#24212;&#29992;&#20110;&#36716;&#35786;&#20013;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#36825;&#20123;&#36716;&#35786;&#30340;&#26684;&#24335;&#22240;&#21307;&#30103;&#23454;&#36341;&#32780;&#24322;&#65292;&#24182;&#20351;&#29992;MUC-5&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#23454;&#38469;&#29992;&#20363;&#30340;&#36866;&#24403;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#28155;&#21152;&#21040;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#31934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#65292;&#36825;&#34920;&#26126;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#36716;&#35786;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliance on scanned documents and fax communication for healthcare referrals leads to high administrative costs and errors that may affect patient care. In this work we propose a hybrid model leveraging LayoutLMv3 along with domain-specific rules to identify key patient, physician, and exam-related entities in faxed referral documents. We explore some of the challenges in applying a document understanding model to referrals, which have formats varying by medical practice, and evaluate model performance using MUC-5 metrics to obtain appropriate metrics for the practical use case. Our analysis shows the addition of domain-specific rules to the transformer model yields greatly increased precision and F1 scores, suggesting a hybrid model trained on a curated dataset can increase efficiency in referral management.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24222;&#22823;&#19988;&#26032;&#39062;&#30340;&#32654;&#22269;&#23478;&#24237;&#38431;&#21015;&#65288;AFC&#65289;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36164;&#28304;&#65292;&#21253;&#21547;&#32422;9000&#19975;&#27425;&#23601;&#35786;&#35760;&#24405;&#21644;750&#19975;&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.13175</link><description>&lt;p&gt;
&#32654;&#22269;&#23478;&#24237;&#38431;&#21015;&#30740;&#31350;&#36164;&#28304;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
American Family Cohort, a data resource description. (arXiv:2309.13175v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24222;&#22823;&#19988;&#26032;&#39062;&#30340;&#32654;&#22269;&#23478;&#24237;&#38431;&#21015;&#65288;AFC&#65289;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36164;&#28304;&#65292;&#21253;&#21547;&#32422;9000&#19975;&#27425;&#23601;&#35786;&#35760;&#24405;&#21644;750&#19975;&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#24222;&#22823;&#19988;&#26032;&#39062;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36164;&#28304;&#65292;&#21517;&#20026;&#32654;&#22269;&#23478;&#24237;&#38431;&#21015;&#65288;AFC&#65289;&#12290;AFC&#25968;&#25454;&#28304;&#33258;&#32654;&#22269;&#23478;&#24237;&#21307;&#23398;&#22996;&#21592;&#20250;&#65288;ABFM&#65289;&#30340;PRIME&#27880;&#20876;&#34920;&#65292;&#35813;&#27880;&#20876;&#34920;&#26159;&#26368;&#22823;&#30340;&#22269;&#23478;&#21512;&#26684;&#30340;&#20020;&#24202;&#25968;&#25454;&#27880;&#20876;&#34920;&#65288;QCDR&#65289;&#20043;&#19968;&#12290;&#25968;&#25454;&#24050;&#36716;&#25442;&#20026;&#36890;&#29992;&#30340;&#25968;&#25454;&#27169;&#22411;&#65292;&#21363;&#35266;&#23519;&#24615;&#20581;&#24247;&#25968;&#25454;&#31185;&#23398;&#19982;&#20449;&#24687;&#23398;&#65288;OHDSI&#65289;&#35266;&#23519;&#24615;&#21307;&#30103;&#32467;&#26524;&#20249;&#20276;&#20851;&#31995;&#65288;OMOP&#65289;&#30340;&#20849;&#21516;&#25968;&#25454;&#27169;&#22411;&#65288;CDM&#65289;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#32422;9000&#19975;&#27425;&#23601;&#35786;&#35760;&#24405;&#21644;750&#19975;&#21517;&#24739;&#32773;&#12290;100&#65285;&#30340;&#24739;&#32773;&#25552;&#20379;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#22320;&#22336;&#20449;&#24687;&#65292;73&#65285;&#25253;&#21578;&#31181;&#26063;&#20449;&#24687;&#12290;&#36817;93&#65285;&#30340;&#24739;&#32773;&#30340;&#23454;&#39564;&#23460;&#25968;&#25454;&#20351;&#29992;LOINC&#26631;&#20934;&#65292;86&#65285;&#30340;&#24739;&#32773;&#30340;&#33647;&#29289;&#25968;&#25454;&#20351;&#29992;RxNorm&#26631;&#20934;&#65292;93&#65285;&#30340;&#24739;&#32773;&#30340;&#35786;&#26029;&#25968;&#25454;&#20351;&#29992;SNOWMED&#21644;ICD&#65292;81&#65285;&#30340;&#24739;&#32773;&#30340;&#25805;&#20316;&#25968;&#25454;&#20351;&#29992;HCPCS&#25110;CPT&#65292;61&#65285;&#30340;&#24739;&#32773;&#26377;&#20445;&#38505;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript is a research resource description and presents a large and novel Electronic Health Records (EHR) data resource, American Family Cohort (AFC). The AFC data is derived from Centers for Medicare and Medicaid Services (CMS) certified American Board of Family Medicine (ABFM) PRIME registry. The PRIME registry is the largest national Qualified Clinical Data Registry (QCDR) for Primary Care. The data is converted to a popular common data model, the Observational Health Data Sciences and Informatics (OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM).  The resource presents approximately 90 million encounters for 7.5 million patients. All 100% of the patients present age, gender, and address information, and 73% report race. Nealy 93% of patients have lab data in LOINC, 86% have medication data in RxNorm, 93% have diagnosis in SNOWMED and ICD, 81% have procedures in HCPCS or CPT, and 61% have insurance information. The richness, breadth, and diver
&lt;/p&gt;</description></item><item><title>UNICON&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#28040;&#36153;&#32773;&#32454;&#20998;&#26694;&#26550;&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25193;&#22823;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#31181;&#23376;&#32454;&#20998;&#26469;&#33719;&#21462;&#31867;&#20284;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#20855;&#26377;&#30456;&#20284;&#24615;&#20542;&#21521;&#30340;&#38750;&#26126;&#26174;&#28040;&#36153;&#32773;&#32454;&#20998;&#26469;&#33719;&#21462;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13068</link><description>&lt;p&gt;
UNICON:&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#22522;&#20110;&#34892;&#20026;&#30340;&#28040;&#36153;&#32773;&#32454;&#20998;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UNICON: A unified framework for behavior-based consumer segmentation in e-commerce. (arXiv:2309.13068v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13068
&lt;/p&gt;
&lt;p&gt;
UNICON&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#28040;&#36153;&#32773;&#32454;&#20998;&#26694;&#26550;&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25193;&#22823;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#31181;&#23376;&#32454;&#20998;&#26469;&#33719;&#21462;&#31867;&#20284;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#20855;&#26377;&#30456;&#20284;&#24615;&#20542;&#21521;&#30340;&#38750;&#26126;&#26174;&#28040;&#36153;&#32773;&#32454;&#20998;&#26469;&#33719;&#21462;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#26159;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#30340;&#20851;&#38190;&#23454;&#36341;&#65292;&#25552;&#39640;&#20102;&#20225;&#19994;&#20026;&#28040;&#36153;&#32773;&#25552;&#20379;&#26356;&#30456;&#20851;&#20869;&#23481;&#30340;&#26041;&#24335;&#12290;&#32780;&#36229;&#32423;&#20010;&#24615;&#21270;&#20026;&#27599;&#20010;&#28040;&#36153;&#32773;&#25552;&#20379;&#39640;&#24230;&#20010;&#23450;&#21046;&#30340;&#20307;&#39564;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#20010;&#20154;&#25968;&#25454;&#26469;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#26053;&#31243;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#20010;&#24615;&#21270;&#25552;&#20379;&#20102;&#22522;&#20110;&#26356;&#24191;&#27867;&#30340;&#20849;&#21516;&#20559;&#22909;&#26500;&#24314;&#30340;&#20013;&#24230;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#20173;&#33021;&#23545;&#32467;&#26524;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UNICON&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#28040;&#36153;&#32773;&#32454;&#20998;&#26694;&#26550;&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#25968;&#25454;&#26469;&#23398;&#20064;&#38271;&#26399;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#25552;&#21462;&#20004;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#32454;&#20998;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20010;&#24615;&#21270;&#20351;&#29992;&#26696;&#20363;&#65306;&#31867;&#20284;&#30446;&#26631;&#65292;&#36890;&#36807;&#19982;&#34892;&#20026;&#30456;&#20284;&#30340;&#28040;&#36153;&#32773;&#25193;&#22823;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#31181;&#23376;&#32454;&#20998;&#65292;&#24182;&#19988;&#25968;&#25454;&#39537;&#21160;&#65292;&#25581;&#31034;&#20855;&#26377;&#30456;&#20284;&#24615;&#20542;&#21521;&#30340;&#38750;&#26126;&#26174;&#28040;&#36153;&#32773;&#32454;&#20998;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Data-driven personalization is a key practice in fashion e-commerce, improving the way businesses serve their consumers needs with more relevant content. While hyper-personalization offers highly targeted experiences to each consumer, it requires a significant amount of private data to create an individualized journey. To alleviate this, group-based personalization provides a moderate level of personalization built on broader common preferences of a consumer segment, while still being able to personalize the results. We introduce UNICON, a unified deep learning consumer segmentation framework that leverages rich consumer behavior data to learn long-term latent representations and utilizes them to extract two pivotal types of segmentation catering various personalization use-cases: lookalike, expanding a predefined target seed segment with consumers of similar behavior, and data-driven, revealing non-obvious consumer segments with similar affinities. We demonstrate through extensive exp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13063</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#24212;&#29992;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#21487;&#20197;&#25581;&#31034;&#29992;&#25143;&#19982;&#32593;&#32476;&#25628;&#32034;&#26381;&#21153;&#30340;&#20132;&#20114;&#26041;&#24335;&#12289;&#29992;&#25143;&#30340;&#38656;&#27714;&#20197;&#21450;&#28385;&#24847;&#31243;&#24230;&#31561;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#30340;&#32593;&#32476;&#25628;&#32034;&#24418;&#24335;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#12290;&#20026;&#20102;&#29702;&#35299;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#29992;&#26377;&#24847;&#20041;&#30340;&#20998;&#31867;&#26041;&#24335;&#26631;&#35760;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20854;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#35201;&#20040;&#20195;&#20215;&#39640;&#26114;&#35201;&#20040;&#19981;&#22815;&#28789;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20016;&#23500;&#19988;&#30456;&#20851;&#30340;&#27010;&#24565;&#12289;&#25551;&#36848;&#21644;&#31034;&#20363;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLM&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24535;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#20998;&#31867;&#24471;&#19981;&#21040;&#22806;&#37096;&#39564;&#35777;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#19981;&#33391;&#30340;&#21453;&#39304;&#22238;&#36335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#35780;&#20272;&#32773;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#24211;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#35299;&#30721;&#20102;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#31995;&#32479;&#20013;&#23398;&#20301;&#30340;&#19981;&#30830;&#23450;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23545;&#23398;&#29983;&#36861;&#36394;&#25253;&#21578;&#36827;&#34892;&#35299;&#37322;&#21644;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#20301;&#32423;&#21035;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#36825;&#31181;&#20998;&#31867;&#26377;&#21161;&#20110;&#30740;&#31350;&#23398;&#29983;&#25104;&#21151;&#21644;&#27969;&#21160;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.13050</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#65306;&#25968;&#25454;&#24211;&#21644;&#25991;&#26412;&#25366;&#25496;&#65292;&#35299;&#30721;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#20307;&#31995;&#20013;&#30340;&#23398;&#20301;&#30340;&#23383;&#27597;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining. (arXiv:2309.13050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#24211;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#35299;&#30721;&#20102;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#31995;&#32479;&#20013;&#23398;&#20301;&#30340;&#19981;&#30830;&#23450;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23545;&#23398;&#29983;&#36861;&#36394;&#25253;&#21578;&#36827;&#34892;&#35299;&#37322;&#21644;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#20301;&#32423;&#21035;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#36825;&#31181;&#20998;&#31867;&#26377;&#21161;&#20110;&#30740;&#31350;&#23398;&#29983;&#25104;&#21151;&#21644;&#27969;&#21160;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#22269;&#23478;&#23398;&#29983;&#28165;&#31639;&#20013;&#24515;&#65288;NSC&#65289;&#30340;&#23398;&#29983;&#36861;&#36394;&#25253;&#21578;&#20013;&#21547;&#31946;&#19981;&#28165;&#22320;&#34920;&#36798;&#30340;&#39640;&#31561;&#25945;&#32946;&#23398;&#20301;&#65288;&#20363;&#22914;&#23398;&#22763;&#12289;&#30805;&#22763;&#31561;&#65289;&#30340;&#32423;&#21035;&#12290;&#35813;&#27169;&#22411;&#26159;&#20004;&#20010;&#27169;&#22359;&#30340;&#28151;&#21512;&#20307;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#21442;&#32771;&#25105;&#20204;&#32534;&#21046;&#30340;&#36817;950&#20010;&#32654;&#22269;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#23398;&#20301;&#26631;&#39064;&#32553;&#20889;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#65292;&#35299;&#37322;NSC&#25253;&#21578;&#20013;&#23884;&#20837;&#30340;&#30456;&#20851;&#32553;&#20889;&#20803;&#32032;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#26159;CNN-BiLSTM&#27169;&#22411;&#30340;&#29305;&#24449;&#20998;&#31867;&#21644;&#25991;&#26412;&#25366;&#25496;&#30340;&#32452;&#21512;&#65292;&#21069;&#38754;&#26377;&#25968;&#20010;&#32321;&#37325;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#26368;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#36820;&#22238;&#20102;97.83&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;&#23545;&#23398;&#20301;&#32423;&#21035;&#30340;&#24443;&#24213;&#20998;&#31867;&#23558;&#20026;&#30740;&#31350;&#23398;&#29983;&#25104;&#21151;&#21644;&#27969;&#21160;&#30340;&#27169;&#24335;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model to predict the levels (e.g., Bachelor, Master, etc.) of postsecondary degree awards that have been ambiguously expressed in the student tracking reports of the National Student Clearinghouse (NSC). The model will be the hybrid of two modules. The first module interprets the relevant abbreviatory elements embedded in NSC reports by referring to a comprehensive database that we have made of nearly 950 abbreviations for degree titles used by American postsecondary educators. The second module is a combination of feature classification and text mining modeled with CNN-BiLSTM, which is preceded by several steps of heavy pre-processing. The model proposed in this paper was trained with four multi-label datasets of different grades of resolution and returned 97.83\% accuracy with the most sophisticated dataset. Such a thorough classification of degree levels will provide insights into the modeling patterns of student success and mobility. To date, such a classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#26469;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#20197;&#38450;&#27490;&#38544;&#31169;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#20010;&#20154;&#36164;&#26009;&#25968;&#25454;&#24211;&#30340;&#38656;&#27714;&#65292;&#24182;&#33021;&#26377;&#25928;&#39564;&#35777;&#29992;&#25143;&#30340;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2309.13046</link><description>&lt;p&gt;
&#38024;&#23545;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy Preserving Machine Learning for Behavioral Authentication Systems. (arXiv:2309.13046v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#26469;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#20197;&#38450;&#27490;&#38544;&#31169;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#20010;&#20154;&#36164;&#26009;&#25968;&#25454;&#24211;&#30340;&#38656;&#27714;&#65292;&#24182;&#33021;&#26377;&#25928;&#39564;&#35777;&#29992;&#25143;&#30340;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#20351;&#29992;&#29992;&#25143;&#30340;&#34892;&#20026;&#29305;&#24449;&#26469;&#39564;&#35777;&#20854;&#36523;&#20221;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#34892;&#20026;&#35748;&#35777;&#39564;&#35777;&#31639;&#27861;&#12290;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#21576;&#29616;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#26524;&#20998;&#31867;&#32467;&#26524;&#19982;&#22768;&#26126;&#30340;&#36523;&#20221;&#21305;&#37197;&#65292;&#21017;&#25509;&#21463;&#35813;&#22768;&#26126;&#12290;&#36825;&#31181;&#22522;&#20110;&#20998;&#31867;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#32500;&#25252;&#20010;&#20154;&#36164;&#26009;&#25968;&#25454;&#24211;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#34892;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#20026;&#20102;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24191;&#27867;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#19968;&#31181;&#38750;&#21152;&#23494;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#26469;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#38543;&#26426;&#25237;&#24433;&#26159;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#30340;&#36317;&#31163;&#20445;&#25345;&#36716;&#25442;&#12290;&#22312;&#19982;&#39564;&#35777;&#32773;&#20849;&#20139;&#20010;&#20154;&#36164;&#26009;&#20043;&#21069;&#65292;&#29992;&#25143;&#23558;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#23545;&#20854;&#20010;&#20154;&#36164;&#26009;&#36827;&#34892;&#36716;&#25442;&#65292;&#24182;&#20445;&#25345;&#20854;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A behavioral authentication (BA) system uses the behavioral characteristics of users to verify their identity claims. A BA verification algorithm can be constructed by training a neural network (NN) classifier on users' profiles. The trained NN model classifies the presented verification data, and if the classification matches the claimed identity, the verification algorithm accepts the claim. This classification-based approach removes the need to maintain a profile database. However, similar to other NN architectures, the NN classifier of the BA system is vulnerable to privacy attacks. To protect the privacy of training and test data used in an NN different techniques are widely used. In this paper, our focus is on a non-crypto-based approach, and we used random projection (RP) to ensure data privacy in an NN model. RP is a distance-preserving transformation based on a random matrix. Before sharing the profiles with the verifier, users will transform their profiles by RP and keep thei
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.10563</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21450;&#20854;&#35299;&#37322;&#24120;&#24120;&#38754;&#20020;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#32479;&#19968;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#19978;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23558;&#36825;&#19968;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#31232;&#32570;&#26631;&#27880;&#27861;&#24459;&#25991;&#20214;&#8221;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;MESc&#65288;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#26694;&#26550;&#26469;&#25506;&#32034;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#21644;&#38271;&#25991;&#26723;&#30340;&#29305;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#20010;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21478;&#19968;&#32452;Transformer&#32534;&#30721;&#22120;&#23618;&#23398;&#20064;&#37096;&#20998;&#20043;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.06219</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#38142;&#25509;&#39044;&#27979;&#22312;&#29983;&#27963;&#26041;&#24335;vlog&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#21363;&#30830;&#23450;&#20004;&#20010;&#20154;&#31867;&#21160;&#20316;&#26159;&#21542;&#21487;&#20197;&#22312;&#21516;&#19968;&#26102;&#38388;&#38388;&#38548;&#20869;&#20849;&#29616;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;ACE&#65288;Action Co-occurrencE&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#32422;12k&#20010;&#20849;&#29616;&#30340;&#35270;&#35273;&#21160;&#20316;&#23545;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#32452;&#25104;&#30340;&#22823;&#22411;&#22270;&#24418;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#33258;&#21160;&#25512;&#26029;&#20004;&#20010;&#21160;&#20316;&#26159;&#21542;&#20849;&#29616;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#24418;&#29305;&#21035;&#36866;&#21512;&#25429;&#25417;&#20154;&#31867;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#22270;&#24418;&#34920;&#31034;&#23545;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#22495;&#20013;&#25429;&#25417;&#21040;&#26032;&#39062;&#32780;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;ACE&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/MichiganNLP/vlog_action_co-occurrence&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20851;&#31995;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;RCL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#21333;&#19968;&#34892;&#20026;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;RCL&#27169;&#22411;&#36890;&#36807;&#22810;&#20851;&#31995;&#22270;&#32534;&#30721;&#22120;&#25429;&#25417;&#30701;&#26399;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#36328;&#20851;&#31995;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#38271;&#26399;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.01103</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Contrastive Learning for Recommendation. (arXiv:2309.01103v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20851;&#31995;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;RCL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#21333;&#19968;&#34892;&#20026;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;RCL&#27169;&#22411;&#36890;&#36807;&#22810;&#20851;&#31995;&#22270;&#32534;&#30721;&#22120;&#25429;&#25417;&#30701;&#26399;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#36328;&#20851;&#31995;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#38271;&#26399;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#25429;&#25417;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20559;&#22909;&#12289;&#25552;&#20379;&#20934;&#30830;&#26377;&#25928;&#30340;&#25512;&#33616;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25512;&#33616;&#27169;&#22411;&#21482;&#20381;&#36182;&#20110;&#19968;&#31181;&#34892;&#20026;&#23398;&#20064;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#20197;&#22810;&#31181;&#26041;&#24335;&#19982;&#29289;&#21697;&#20114;&#21160;&#65292;&#21253;&#25324;&#28857;&#20987;&#12289;&#26631;&#35760;&#20026;&#21916;&#29233;&#12289;&#35780;&#35770;&#21644;&#36141;&#20080;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;RCL&#65289;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;RCL&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#22810;&#20851;&#31995;&#22270;&#32534;&#30721;&#22120;&#65292;&#25429;&#25417;&#30701;&#26399;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#31867;&#22411;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#19987;&#29992;&#20851;&#31995;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#36328;&#20851;&#31995;&#35760;&#24518;&#32593;&#32476;&#65292;&#20351;RCL&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#30340;&#38271;&#26399;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommender systems play a crucial role in capturing users' evolving preferences over time to provide accurate and effective recommendations on various online platforms. However, many recommendation models rely on a single type of behavior learning, which limits their ability to represent the complex relationships between users and items in real-life scenarios. In such situations, users interact with items in multiple ways, including clicking, tagging as favorite, reviewing, and purchasing. To address this issue, we propose the Relation-aware Contrastive Learning (RCL) framework, which effectively models dynamic interaction heterogeneity. The RCL model incorporates a multi-relational graph encoder that captures short-term preference heterogeneity while preserving the dedicated relation semantics for different types of user-item interactions. Moreover, we design a dynamic cross-relational memory network that enables the RCL model to capture users' long-term multi-behavior p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.11551</link><description>&lt;p&gt;
&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;VTR&#65289;&#26159;&#20114;&#32852;&#32593;&#19978;&#28023;&#37327;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#26102;&#20195;&#20013;&#19968;&#39033;&#20851;&#38190;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#20351;&#29992;&#21452;&#27969;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#35270;&#39057;&#25991;&#26412;&#23545;&#30340;&#32852;&#21512;&#34920;&#31034;&#25104;&#20026;VTR&#20219;&#21153;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20551;&#35774;&#35270;&#39057;&#25991;&#26412;&#23545;&#24212;&#26159;&#21452;&#23556;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#24573;&#35270;&#20102;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#35270;&#39057;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#22810;&#20010;&#20107;&#20214;&#65292;&#32780;&#29992;&#25143;&#26597;&#35810;&#25110;&#32593;&#39029;&#20803;&#25968;&#25454;&#31561;&#25991;&#26412;&#24448;&#24448;&#26159;&#20855;&#20307;&#30340;&#65292;&#24182;&#23545;&#24212;&#21333;&#20010;&#20107;&#20214;&#12290;&#36825;&#36896;&#25104;&#20102;&#20043;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#26089;&#26399;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#38024;&#23545;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#22330;&#26223;&#65292;&#20316;&#20026;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#19968;&#20010;&#21033;&#22522;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.05065</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20351;&#29992;&#22823;&#35268;&#27169;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35757;&#32451;&#21452;&#32534;&#30721;&#27169;&#22411;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#36873;&#25321;&#32473;&#23450;&#26597;&#35810;&#23884;&#20837;&#30340;&#39030;&#37096;&#20505;&#36873;&#39033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#33539;&#20363;&#65306;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#35299;&#30721;&#30446;&#26631;&#20505;&#36873;&#39033;&#30340;&#26631;&#35782;&#31526;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#20026;&#27599;&#20010;&#39033;&#30446;&#20998;&#37197;&#38543;&#26426;&#29983;&#25104;&#30340;&#21407;&#23376;ID&#65292;&#32780;&#26159;&#29983;&#25104;&#35821;&#20041;ID&#65306;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#32452;&#32534;&#30721;&#35789;&#65292;&#23427;&#20316;&#20026;&#20854;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;RQ-VAE&#30340;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;&#36825;&#20123;&#32534;&#30721;&#35789;&#12290;&#19968;&#26086;&#25105;&#20204;&#23545;&#25152;&#26377;&#39033;&#30446;&#37117;&#26377;&#20102;&#35821;&#20041;ID&#65292;&#23601;&#20250;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;ID&#12290;&#30001;&#20110;&#36825;&#20010;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#30452;&#25509;&#39044;&#27979;&#26631;&#35782;&#19979;&#19968;&#20010;&#39033;&#30340;&#32534;&#30721;&#35789;&#20803;&#32452;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
&lt;/p&gt;</description></item><item><title>TREC NeuCLIR&#36712;&#36947;&#30340;&#31532;&#19968;&#24180;&#65292;&#30740;&#31350;&#31070;&#32463;&#26041;&#27861;&#23545;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33521;&#35821;&#26597;&#35810;&#26469;ad hoc&#25490;&#21517;&#26816;&#32034;&#20013;&#25991;&#12289;&#27874;&#26031;&#35821;&#25110;&#20420;&#35821;&#26032;&#38395;&#25991;&#26723;&#65292;&#20849;&#26377;12&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;172&#27425;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.12367</link><description>&lt;p&gt;
TREC 2022 NeuCLIR&#36712;&#36947;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of the TREC 2022 NeuCLIR Track. (arXiv:2304.12367v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12367
&lt;/p&gt;
&lt;p&gt;
TREC NeuCLIR&#36712;&#36947;&#30340;&#31532;&#19968;&#24180;&#65292;&#30740;&#31350;&#31070;&#32463;&#26041;&#27861;&#23545;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33521;&#35821;&#26597;&#35810;&#26469;ad hoc&#25490;&#21517;&#26816;&#32034;&#20013;&#25991;&#12289;&#27874;&#26031;&#35821;&#25110;&#20420;&#35821;&#26032;&#38395;&#25991;&#26723;&#65292;&#20849;&#26377;12&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;172&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;TREC&#31070;&#32463;CLIR&#65288;NeuCLIR&#65289;&#36712;&#36947;&#30340;&#31532;&#19968;&#24180;&#65292;&#26088;&#22312;&#30740;&#31350;&#31070;&#32463;&#26041;&#27861;&#23545;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;&#20170;&#24180;&#36712;&#36947;&#30340;&#20027;&#35201;&#20219;&#21153;&#26159;&#20351;&#29992;&#29992;&#33521;&#35821;&#34920;&#36798;&#30340;&#26597;&#35810;&#65292;&#23545;&#20013;&#25991;&#12289;&#27874;&#26031;&#35821;&#25110;&#20420;&#35821;&#26032;&#38395;&#25991;&#26723;&#36827;&#34892;ad hoc&#25490;&#21517;&#26816;&#32034;&#12290;&#35805;&#39064;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;TREC&#27969;&#31243;&#24320;&#21457;&#30340;&#65292;&#38500;&#20102;&#22312;&#35780;&#20272;&#35813;&#35805;&#39064;&#30340;&#19981;&#21516;&#35821;&#35328;&#19978;&#35780;&#20272;&#19968;&#20010;&#27880;&#37322;&#22120;&#24320;&#21457;&#30340;&#35805;&#39064;&#26102;&#65292;&#30001;&#21478;&#19968;&#20010;&#27880;&#37322;&#22120;&#24320;&#21457;&#30340;&#35805;&#39064;&#12290;&#20849;&#26377;12&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;172&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first year of the TREC Neural CLIR (NeuCLIR) track, which aims to study the impact of neural approaches to cross-language information retrieval. The main task in this year's track was ad hoc ranked retrieval of Chinese, Persian, or Russian newswire documents using queries expressed in English. Topics were developed using standard TREC processes, except that topics developed by an annotator for one language were assessed by a different annotator when evaluating that topic on a different language. There were 172 total runs submitted by twelve teams.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25215;&#35834;PIR&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#32447;&#24615;&#26144;&#23556;&#25215;&#35834;&#21644;&#20219;&#24847;&#32447;&#24615;PIR&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;$k$-&#21487;&#39564;&#35777;&#30340;PIR&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.01733</link><description>&lt;p&gt;
&#25215;&#35834;&#31169;&#20154;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Committed Private Information Retrieval. (arXiv:2302.01733v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25215;&#35834;PIR&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#32447;&#24615;&#26144;&#23556;&#25215;&#35834;&#21644;&#20219;&#24847;&#32447;&#24615;PIR&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;$k$-&#21487;&#39564;&#35777;&#30340;PIR&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#20154;&#20449;&#24687;&#26816;&#32034;&#65288;PIR&#65289;&#26041;&#26696;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;$k$&#20010;&#26381;&#21153;&#22120;&#30340;$n$&#20010;&#39033;&#30446;$x_1,x_2,\ldots,x_n$&#20013;&#26816;&#32034;&#20986;&#19968;&#20010;&#25968;&#25454;&#39033;&#30446;$x_i$&#65292;&#21363;&#20351;&#24403;$t&lt;k$&#20010;&#26381;&#21153;&#22120;&#21512;&#35851;&#24182;&#35797;&#22270;&#23398;&#20064;$i$&#26102;&#20063;&#19981;&#20250;&#36879;&#38706;$i$&#26159;&#20160;&#20040;&#12290;&#36825;&#26679;&#30340;PIR&#26041;&#26696;&#34987;&#31216;&#20026;$t-$&#31169;&#23494;&#12290;&#22914;&#26524;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;$v\leq k$&#20010;&#26381;&#21153;&#22120;&#21512;&#35851;&#24182;&#35797;&#22270;&#36890;&#36807;&#21457;&#36865;&#31713;&#25913;&#30340;&#25968;&#25454;&#26469;&#24858;&#24324;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#39564;&#35777;&#26816;&#32034;&#21040;&#30340;$x_i$&#30340;&#27491;&#30830;&#24615;&#65292;&#21017;PIR&#26041;&#26696;&#20026;$v-$&#21487;&#39564;&#35777;&#12290;&#25991;&#29486;&#20013;&#30340;&#22823;&#22810;&#25968;&#20808;&#21069;&#30740;&#31350;&#20551;&#35774;$v&lt;k$&#65292;&#30041;&#19979;&#20102;&#26381;&#21153;&#22120;&#20840;&#37096;&#21512;&#35851;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26500;&#36896;&#65292;&#23558;&#32447;&#24615;&#26144;&#23556;&#25215;&#35834;&#65288;LMC&#65289;&#21644;&#20219;&#24847;&#32447;&#24615;PIR&#26041;&#26696;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20135;&#29983;&#19968;&#20010;$k-$&#21487;&#39564;&#35777;&#30340;PIR&#26041;&#26696;&#65292;&#31216;&#20026;&#25215;&#35834;PIR&#26041;&#26696;&#12290;&#21363;&#20351;&#22312;&#26368;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#25152;&#26377;&#26381;&#21153;&#22120;&#37117;&#22312;&#25915;&#20987;&#32773;&#30340;&#25511;&#21046;&#19979;&#65292;&#23613;&#31649;&#38544;&#31169;&#26080;&#27861;&#36991;&#20813;&#20002;&#22833;&#65292;&#23458;&#25143;&#31471;&#20063;&#19981;&#20250;&#34987;&#24858;&#24324;&#32780;&#25509;&#21463;&#19981;&#27491;&#30830;&#30340;$x_i$&#12290;
&lt;/p&gt;
&lt;p&gt;
A private information retrieval (PIR) scheme allows a client to retrieve a data item $x_i$ among $n$ items $x_1,x_2,\ldots,x_n$ from $k$ servers, without revealing what $i$ is even when $t &lt; k$ servers collude and try to learn $i$. Such a PIR scheme is said to be $t$-private. A PIR scheme is $v$-verifiable if the client can verify the correctness of the retrieved $x_i$ even when $v \leq k$ servers collude and try to fool the client by sending manipulated data. Most of the previous works in the literature on PIR assumed that $v &lt; k$, leaving the case of all-colluding servers open. We propose a generic construction that combines a linear map commitment (LMC) and an arbitrary linear PIR scheme to produce a $k$-verifiable PIR scheme, termed a committed PIR scheme. Such a scheme guarantees that even in the worst scenario, when all servers are under the control of an attacker, although the privacy is unavoidably lost, the client won't be fooled into accepting an incorrect $x_i$. We demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#22343;&#21248;&#24615;&#21644;&#23545;&#40784;&#24615;&#29305;&#24615;&#65292;&#22686;&#24378;&#20102;GNNs&#23545;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.11461</link><description>&lt;p&gt;
&#22686;&#24378;GNNs&#30340;&#26102;&#31354;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation. (arXiv:2209.11461v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#22343;&#21248;&#24615;&#21644;&#23545;&#40784;&#24615;&#29305;&#24615;&#65292;&#22686;&#24378;&#20102;GNNs&#23545;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#21033;&#29992;&#29992;&#25143;&#30340;&#30701;&#26399;&#34892;&#20026;&#24207;&#21015;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#65292;&#32780;&#19981;&#38656;&#35201;&#35814;&#32454;&#30340;&#29992;&#25143;&#36164;&#26009;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#23558;&#20250;&#35805;&#35270;&#20026;&#39033;&#30446;&#20043;&#38388;&#30340;&#36716;&#25442;&#22270;&#65292;&#24182;&#21033;&#29992;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23545;&#39033;&#30446;&#21450;&#20854;&#37051;&#23621;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20174;&#31354;&#38388;&#22270;&#32467;&#26500;&#30340;&#35270;&#35282;&#32858;&#21512;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#22312;&#20449;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#39033;&#30446;&#30340;&#37051;&#23621;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#38382;&#39064;&#12290;&#20854;&#20182;&#30340;&#24037;&#20316;&#36890;&#36807;&#25972;&#21512;&#39069;&#22806;&#30340;&#26102;&#38388;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#20043;&#38388;&#20805;&#20998;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#22343;&#21248;&#24615;&#21644;&#23545;&#40784;&#24615;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation (SBR) systems aim to utilize the user's short-term behavior sequence to predict the next item without the detailed user profile. Most recent works try to model the user preference by treating the sessions as between-item transition graphs and utilize various graph neural networks (GNNs) to encode the representations of pair-wise relations among items and their neighbors. Some of the existing GNN-based models mainly focus on aggregating information from the view of spatial graph structure, which ignores the temporal relations within neighbors of an item during message passing and the information loss results in a sub-optimal problem. Other works embrace this challenge by incorporating additional temporal information but lack sufficient interaction between the spatial and temporal patterns. To address this issue, inspired by the uniformity and alignment properties of contrastive learning techniques, we propose a novel framework called Session-based Recommenda
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#34987;&#35843;&#24230;&#21592;&#24573;&#35270;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#28909;&#28857;&#36235;&#21183;&#24448;&#24448;&#19982;&#25972;&#20307;&#20107;&#20214;&#23494;&#24230;&#30340;&#22686;&#21152;&#30456;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2202.04176</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#30340;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation. (arXiv:2202.04176v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#21644;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#29359;&#32618;&#28909;&#28857;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#34987;&#35843;&#24230;&#21592;&#24573;&#35270;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#28909;&#28857;&#36235;&#21183;&#24448;&#24448;&#19982;&#25972;&#20307;&#20107;&#20214;&#23494;&#24230;&#30340;&#22686;&#21152;&#30456;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29359;&#32618;&#35760;&#24405;&#21465;&#36848;&#30340;&#38598;&#21512;&#36827;&#34892;&#20027;&#39064;&#20998;&#24067;&#30340;&#35745;&#31639;&#65292;&#30830;&#23450;&#30456;&#20284;&#21628;&#21483;&#30340;&#20998;&#32452;&#20197;&#21450;&#23427;&#20204;&#30340;&#30456;&#23545;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#21465;&#36848;&#33719;&#21462;&#19968;&#20010;&#20027;&#39064;&#20998;&#24067;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#36817;&#37051;&#30456;&#23545;&#23494;&#24230;&#20272;&#35745;&#65288;kNN-RDE&#65289;&#26041;&#27861;&#26469;&#33719;&#24471;&#27599;&#20010;&#20027;&#39064;&#30340;&#31354;&#38388;&#30456;&#23545;&#23494;&#24230;&#12290;&#22312;&#20122;&#29305;&#20848;&#22823;&#35686;&#23519;&#23616;&#30340;&#22823;&#37327;&#21465;&#36848;&#25991;&#26723;&#65288;$n=475,019$&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#36890;&#24120;&#34987;&#21628;&#21483;&#35843;&#24230;&#21592;&#19968;&#24320;&#22987;&#27809;&#26377;&#23519;&#35273;&#21040;&#30340;&#22320;&#29702;&#28909;&#28857;&#36235;&#21183;&#65292;&#36825;&#20123;&#36235;&#21183;&#30001;&#20110;&#19982;&#19968;&#33324;&#20107;&#20214;&#23494;&#24230;&#30340;&#28151;&#28102;&#32780;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to capture groupings of similar calls and determine their relative spatial distribution from a collection of crime record narratives. We first obtain a topic distribution for each narrative, and then propose a nearest neighbors relative density estimation (kNN-RDE) approach to obtain spatial relative densities per topic. Experiments over a large corpus ($n=475,019$) of narrative documents from the Atlanta Police Department demonstrate the viability of our method in capturing geographic hot-spot trends which call dispatchers do not initially pick up on and which go unnoticed due to conflation with elevated event density in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2105.01331</link><description>&lt;p&gt;
BLM-17m: &#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26435;&#20445;&#25252;&#26159;&#19990;&#30028;&#19978;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#28085;&#30422;&#26368;&#36817;&#20960;&#20010;&#26376;&#20840;&#29699;&#24433;&#21709;&#28145;&#36828;&#30340;&#20154;&#26435;&#30683;&#30462;&#20043;&#19968;&#8212;&#8212;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;17&#30334;&#19975;&#25512;&#25991;&#30340;&#20027;&#39064;&#26816;&#27979;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#20174;2020&#24180;5&#26376;25&#26085;&#33267;2020&#24180;8&#26376;21&#26085;&#25910;&#38598;&#30340;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#20107;&#20214;&#24320;&#22987;&#21518;&#30340;89&#22825;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#27979;&#20840;&#29699;&#21644;&#26412;&#22320;&#25253;&#32440;&#30340;&#26368;&#28909;&#38376;&#26032;&#38395;&#20027;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;TF-IDF&#21644;LDA&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;k&#20540;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/MeysamAsgariC/BLMT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
&lt;/p&gt;</description></item></channel></rss>