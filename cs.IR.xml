<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;MS MARCO-passage&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20844;&#27491;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;&#21644;&#39046;&#22495;&#19981;&#21487;&#38752;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.12904</link><description>&lt;p&gt;
&#20004;&#20010;MS MARCO&#30340;&#25925;&#20107;--&#20182;&#20204;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
The tale of two MS MARCO -- and their unfair comparisons. (arXiv:2304.12904v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12904
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;MS MARCO-passage&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20844;&#27491;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;&#21644;&#39046;&#22495;&#19981;&#21487;&#38752;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MS MARCO-passage &#25968;&#25454;&#38598;&#26159;IR&#31038;&#21306;&#20027;&#35201;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22810;&#24180;&#26469;&#23427;&#25104;&#21151;&#22320;&#25512;&#21160;&#20102;&#26032;&#22411;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20294;&#26159;&#65292;&#20107;&#23454;&#35777;&#26126;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;MS MARCO&#35821;&#26009;&#24211;&#65292;&#19968;&#20010;&#26159;&#23448;&#26041;&#29256;&#26412;&#65292;&#19968;&#20010;&#26159;&#21152;&#20837;&#20102;&#26631;&#39064;&#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24341;&#20837;&#20102;Tevatron&#20195;&#30721;&#24211;&#12290;&#28982;&#32780;&#65292;&#28155;&#21152;&#26631;&#39064;&#23454;&#38469;&#19978;&#27844;&#28431;&#20102;&#30456;&#20851;&#20449;&#24687;&#65292;&#32780;&#19988;&#36829;&#21453;&#20102;MS MARCO-passage&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#25351;&#21335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#35780;&#20272;&#26032;&#26041;&#27861;&#26102;&#65292;&#23427;&#20204;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#35770;&#25991;&#27809;&#26377;&#24688;&#24403;&#22320;&#25253;&#21578;&#25152;&#20351;&#29992;&#30340;&#29256;&#26412;&#65292;&#37027;&#20040;&#23601;&#22522;&#26412;&#19978;&#19981;&#21487;&#33021;&#20844;&#24179;&#22320;&#22797;&#29616;&#20854;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#24403;&#21069;&#23457;&#26597;&#30340;&#29366;&#24577;&#65292;&#30417;&#27979;&#26368;&#26032;&#36827;&#23637;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#25317;&#26377;&#20004;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;MS MARCO&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#19981;&#21487;&#38752;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MS MARCO-passage dataset has been the main large-scale dataset open to the IR community and it has fostered successfully the development of novel neural retrieval models over the years. But, it turns out that two different corpora of MS MARCO are used in the literature, the official one and a second one where passages were augmented with titles, mostly due to the introduction of the Tevatron code base. However, the addition of titles actually leaks relevance information, while breaking the original guidelines of the MS MARCO-passage dataset. In this work, we investigate the differences between the two corpora and demonstrate empirically that they make a significant difference when evaluating a new method. In other words, we show that if a paper does not properly report which version is used, reproducing fairly its results is basically impossible. Furthermore, given the current status of reviewing, where monitoring state-of-the-art results is of great importance, having two differen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12833</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#20449;&#24687;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21171;&#24503;&#183;&#39321;&#20892;&#25552;&#20986;&#20102;&#29109;&#30340;&#27010;&#24565;&#26469;&#37327;&#21270;&#36890;&#20449;&#32534;&#30721;&#29702;&#35770;&#20013;&#38543;&#26426;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29109;&#30340;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#29305;&#24615;&#20063;&#38480;&#21046;&#20102;&#20854;&#22312;&#25968;&#23398;&#24314;&#27169;&#20013;&#30340;&#30452;&#25509;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; troenpy&#65292;&#20316;&#20026;&#29109;&#30340;&#35268;&#33539;&#23545;&#20598;&#65292;&#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#31532;&#19968;&#20010;&#26159;&#29992;&#20110;&#20256;&#32479;&#30340;&#25991;&#26723;&#20998;&#31867;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; troenpy &#26435;&#37325;&#26041;&#26696;&#26469;&#21033;&#29992;&#25991;&#26723;&#20998;&#31867;&#26631;&#31614;&#12290;&#31532;&#20108;&#20010;&#26159;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#25105; troenpy &#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#21253;&#21547;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#23454;&#29616;&#26174;&#33879;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#20316;&#20026; Von Neumann &#29109;&#30340;&#23545;&#20598;&#65292;&#20197;&#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21457;&#25496;&#20449;&#24687;&#29109;&#33258;&#28982;&#23545;&#20598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;troenpy&#65292;&#24182;&#24212;&#29992;&#20110;&#25552;&#20986;&#20102;&#22522;&#20110;troenpy&#30340;&#25991;&#26723;&#21152;&#26435;&#26041;&#26696;&#65292;&#21363;&#27491;&#31867;&#21035;&#39057;&#29575;&#65288;PCF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#20449;&#24687;&#20559;&#24046;&#29305;&#24449;ECIB&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#20114;&#20449;&#24687;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2304.12814</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;Shannon&#20449;&#24687;&#21450;&#21152;&#26435;&#26041;&#26696;&#30340;&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
A Novel Dual of Shannon Information and Weighting Scheme. (arXiv:2304.12814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21457;&#25496;&#20449;&#24687;&#29109;&#33258;&#28982;&#23545;&#20598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;troenpy&#65292;&#24182;&#24212;&#29992;&#20110;&#25552;&#20986;&#20102;&#22522;&#20110;troenpy&#30340;&#25991;&#26723;&#21152;&#26435;&#26041;&#26696;&#65292;&#21363;&#27491;&#31867;&#21035;&#39057;&#29575;&#65288;PCF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#20449;&#24687;&#20559;&#24046;&#29305;&#24449;ECIB&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#20114;&#20449;&#24687;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shannon&#20449;&#24687;&#29702;&#35770;&#19981;&#20165;&#22312;&#36890;&#20449;&#25216;&#26415;&#39046;&#22495;&#65292;&#20854;&#24212;&#29992;&#36824;&#25299;&#23637;&#33267;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#26412;&#25991;&#21457;&#25496;&#20449;&#24687;&#29109;&#23384;&#22312;&#33258;&#28982;&#23545;&#20598;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;troenpy&#65292;&#29992;&#20110;&#34913;&#37327;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#12289;&#26222;&#36941;&#24615;&#21644;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;troenpy&#30340;&#25991;&#26723;&#21152;&#26435;&#26041;&#26696;&#65292;&#21363;&#27491;&#31867;&#21035;&#39057;&#29575;&#65288;PCF&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#20449;&#24687;&#20559;&#24046;&#29305;&#24449;ECIB&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#20114;&#20449;&#24687;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shannon Information theory has achieved great success in not only communication technology where it was originally developed for but also many other science and engineering fields such as machine learning and artificial intelligence. Inspired by the famous weighting scheme TF-IDF, we discovered that information entropy has a natural dual. We complement the classical Shannon information theory by proposing a novel quantity, namely troenpy. Troenpy measures the certainty, commonness and similarity of the underlying distribution. To demonstrate its usefulness, we propose a troenpy based weighting scheme for document with class labels, namely positive class frequency (PCF). On a collection of public datasets we show the PCF based weighting scheme outperforms the classical TF-IDF and a popular Optimal Transportation based word moving distance algorithm in a kNN setting. We further developed a new odds-ratio type feature, namely Expected Class Information Bias(ECIB), which can be regarded as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#38745;&#24577;&#20462;&#21098;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#37319;&#29992;&#38754;&#21521;&#25991;&#26723;&#12289;&#38754;&#21521;&#26415;&#35821;&#21644;&#19981;&#21487;&#30693;&#19977;&#31181;&#31574;&#30053;&#30340;&#38745;&#24577;&#20462;&#21098;&#20173;&#28982;&#36866;&#29992;&#20110;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;2&#20493;&#36895;&#24230;&#25552;&#21319;&#65292;&#19988;&#25928;&#26524;&#25439;&#22833;&#26497;&#23567;&#65288;&#8804; 2%&#65289;&#12290;</title><link>http://arxiv.org/abs/2304.12702</link><description>&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#38745;&#24577;&#20462;&#21098;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Static Pruning Study on Sparse Neural Retrievers. (arXiv:2304.12702v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#38745;&#24577;&#20462;&#21098;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#37319;&#29992;&#38754;&#21521;&#25991;&#26723;&#12289;&#38754;&#21521;&#26415;&#35821;&#21644;&#19981;&#21487;&#30693;&#19977;&#31181;&#31574;&#30053;&#30340;&#38745;&#24577;&#20462;&#21098;&#20173;&#28982;&#36866;&#29992;&#20110;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;2&#20493;&#36895;&#24230;&#25552;&#21319;&#65292;&#19988;&#25928;&#26524;&#25439;&#22833;&#26497;&#23567;&#65288;&#8804; 2%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DeepImpact&#12289;uniCOIL&#21644;SPLADE&#30340;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#65292;&#23427;&#20204;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#36890;&#36807;&#20498;&#25490;&#32034;&#24341;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26415;&#35821;&#37325;&#35201;&#24615;&#21644;&#25991;&#26723;&#25193;&#23637;&#26469;&#25552;&#20379;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35789;&#34955;&#30340;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;&#20363;&#22914;BM25&#65289;&#26356;&#20026;&#26377;&#25928;&#30340;&#25991;&#26723;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#24050;&#34987;&#35777;&#26126;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#21644;&#26597;&#35810;&#22788;&#29702;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26088;&#22312;&#25552;&#39640;&#20498;&#25490;&#32034;&#24341;&#26597;&#35810;&#22788;&#29702;&#25928;&#29575;&#30340;&#33879;&#21517;&#25216;&#26415;&#23478;&#26063;&#65306;&#38745;&#24577;&#20462;&#21098;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#38745;&#24577;&#20462;&#21098;&#31574;&#30053;&#65292;&#21363;&#38754;&#21521;&#25991;&#26723;&#12289;&#38754;&#21521;&#26415;&#35821;&#21644;&#19981;&#21487;&#30693;&#20462;&#21098;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#25216;&#26415;&#20173;&#28982;&#36866;&#29992;&#20110;&#31232;&#30095;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#22120;&#12290;&#29305;&#21035;&#22320;&#65292;&#38745;&#24577;&#20462;&#21098;&#23454;&#29616;&#20102;2&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#19988;&#25928;&#26524;&#25439;&#22833;&#26497;&#23567;&#65288;&#8804; 2%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse neural retrievers, such as DeepImpact, uniCOIL and SPLADE, have been introduced recently as an efficient and effective way to perform retrieval with inverted indexes. They aim to learn term importance and, in some cases, document expansions, to provide a more effective document ranking compared to traditional bag-of-words retrieval models such as BM25. However, these sparse neural retrievers have been shown to increase the computational costs and latency of query processing compared to their classical counterparts. To mitigate this, we apply a well-known family of techniques for boosting the efficiency of query processing over inverted indexes: static pruning. We experiment with three static pruning strategies, namely document-centric, term-centric and agnostic pruning, and we assess, over diverse datasets, that these techniques still work with sparse neural retrievers. In particular, static pruning achieves $2\times$ speedup with negligible effectiveness loss ($\leq 2\%$ drop) 
&lt;/p&gt;</description></item><item><title>THUIR&#22312;WSDM Cup 2023&#8220;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#8221;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#20351;&#29992;&#20102;&#20256;&#32479;IR&#27169;&#22411;&#21644;Transformer-based cross-encoder architecture&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#29305;&#24449;&#26469;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12650</link><description>&lt;p&gt;
THUIR&#22312;WSDM Cup 2023&#20219;&#21153;1&#20013;&#30340;&#34920;&#29616;&#65306;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
THUIR at WSDM Cup 2023 Task 1: Unbiased Learning to Rank. (arXiv:2304.12650v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12650
&lt;/p&gt;
&lt;p&gt;
THUIR&#22312;WSDM Cup 2023&#8220;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#8221;&#20219;&#21153;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#20351;&#29992;&#20102;&#20256;&#32479;IR&#27169;&#22411;&#21644;Transformer-based cross-encoder architecture&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#29305;&#24449;&#26469;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;WSDM Cup 2023&#20219;&#21153;1&#65306;&#8220;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#8221;&#20013;&#25152;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20256;&#32479;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#21644;&#22522;&#20110;Transformer&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#30340;&#25490;&#24207;&#23398;&#20064;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#26368;&#32456;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the approaches we have used to participate in the WSDM Cup 2023 Task 1: Unbiased Learning to Rank. In brief, we have attempted a combination of both traditional IR models and transformer-based cross-encoder architectures. To further enhance the ranking performance, we also considered a series of features for learning to rank. As a result, we won 2nd place on the final leaderboard.
&lt;/p&gt;</description></item><item><title>MG-ShopDial&#26159;&#19968;&#20010;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;&#22810;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;17k&#20010;&#28041;&#21450;&#25628;&#32034;&#12289;&#25512;&#33616;&#21450;&#21830;&#21697;&#30456;&#20851;&#38382;&#39064;&#30340;&#23545;&#35805;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12636</link><description>&lt;p&gt;
MG-ShopDial&#65306;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;&#22810;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MG-ShopDial: A Multi-Goal Conversational Dataset for e-Commerce. (arXiv:2304.12636v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12636
&lt;/p&gt;
&lt;p&gt;
MG-ShopDial&#26159;&#19968;&#20010;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;&#22810;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;17k&#20010;&#28041;&#21450;&#25628;&#32034;&#12289;&#25512;&#33616;&#21450;&#21830;&#21697;&#30456;&#20851;&#38382;&#39064;&#30340;&#23545;&#35805;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#23547;&#25214;&#21512;&#36866;&#30340;&#20135;&#21697;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#25628;&#32034;&#21151;&#33021;&#12289;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#12289;&#25552;&#20379;&#25512;&#33616;&#21450;&#22238;&#31572;&#19982;&#21830;&#21697;&#26377;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#20805;&#20998;&#25903;&#25345;&#28151;&#21512;&#19981;&#21516;&#30340;&#23545;&#35805;&#30446;&#26631;&#65288;&#20363;&#22914;&#25628;&#32034;&#12289;&#25512;&#33616;&#21644;&#38382;&#31572;&#65289;&#65292;&#32780;&#26159;&#32858;&#28966;&#20110;&#21333;&#19968;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;MG-ShopDial&#65306;&#19968;&#31181;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#28151;&#21512;&#19981;&#21516;&#30446;&#26631;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational systems can be particularly effective in supporting complex information seeking scenarios with evolving information needs. Finding the right products on an e-commerce platform is one such scenario, where a conversational agent would need to be able to provide search capabilities over the item catalog, understand and make recommendations based on the user's preferences, and answer a range of questions related to items and their usage. Yet, existing conversational datasets do not fully support the idea of mixing different conversational goals (i.e., search, recommendation, and question answering) and instead focus on a single goal. To address this, we introduce MG-ShopDial: a dataset of conversations mixing different goals in the domain of e-commerce. Specifically, we make the following contributions. First, we develop a coached human-human data collection protocol where each dialogue participant is given a set of instructions, instead of a specific script or answers to ch
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12633</link><description>&lt;p&gt;
PUNR: &#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#30340;&#26032;&#38395;&#25512;&#33616;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#39044;&#27979;&#28857;&#20987;&#34892;&#20026;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24314;&#27169;&#29992;&#25143;&#34920;&#31034;&#26159;&#25512;&#33616;&#39318;&#36873;&#26032;&#38395;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#30340;&#25913;&#36827;&#19978;&#12290;&#28982;&#32780;&#65292;&#36824;&#32570;&#20047;&#38024;&#23545;&#29992;&#25143;&#34920;&#31034;&#20248;&#21270;&#30340;&#22522;&#20110;PLM&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#33539;&#20363;&#65292;&#21363;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#21644;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#65292;&#22343;&#33268;&#21147;&#20110;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#24674;&#22797;&#22522;&#20110;&#19978;&#19979;&#25991;&#34892;&#20026;&#30340;&#25513;&#34109;&#29992;&#25143;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26356;&#24378;&#22823;&#12289;&#26356;&#20840;&#38754;&#30340;&#29992;&#25143;&#26032;&#38395;&#38405;&#35835;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#22686;&#24378;&#20174;&#29992;&#25143;&#32534;&#30721;&#22120;&#27966;&#29983;&#20986;&#30340;&#29992;&#25143;&#34920;&#31034;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#36848;&#39044;&#35757;&#32451;&#30340;&#29992;&#25143;&#24314;&#27169;&#26469;&#36827;&#34892;&#26032;&#38395;&#25512;&#33616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of PLM-based unsupervised pre-training methods optimized for user representations. In this work, we propose an unsupervised pre-training paradigm with two tasks, i.e. user behavior masking and user behavior generation, both towards effective user behavior modeling. Firstly, we introduce the user behavior masking pre-training task to recover the masked user behaviors based on their contextual behaviors. In this way, the model could capture a much stronger and more comprehensive user news reading pattern. Besides, we incorporate a novel auxiliary user behavior generation pre-training task to enhance the user representation vector derived from the user encoder. We use the above pre-trained user modeling en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;NRM&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#31561;&#20215;&#26597;&#35810;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;NRM&#32467;&#26524;&#19982;&#20855;&#26377;&#31561;&#20215;&#26597;&#35810;&#30340;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#30340;&#32467;&#26524;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#31561;&#20215;&#26597;&#35810;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#26816;&#32034;&#25928;&#26524;&#21644;&#27599;&#31181;&#26041;&#27861;&#29983;&#25104;&#30340;&#35789;&#39033;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2304.12631</link><description>&lt;p&gt;
BM25&#31616;&#21333;&#26131;&#25026;&#65306;&#29992;&#31232;&#30095;&#36924;&#36817;&#35299;&#37322;&#23494;&#38598;&#27169;&#22411;&#30340;&#25490;&#21517;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
Explain like I am BM25: Interpreting a Dense Model's Ranked-List with a Sparse Approximation. (arXiv:2304.12631v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;NRM&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#31561;&#20215;&#26597;&#35810;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;NRM&#32467;&#26524;&#19982;&#20855;&#26377;&#31561;&#20215;&#26597;&#35810;&#30340;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#30340;&#32467;&#26524;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#31561;&#20215;&#26597;&#35810;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#26816;&#32034;&#25928;&#26524;&#21644;&#27599;&#31181;&#26041;&#27861;&#29983;&#25104;&#30340;&#35789;&#39033;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;(NRMs)&#22240;&#20026;&#21487;&#20197;&#36890;&#36807;&#31264;&#23494;&#25991;&#26723;&#34920;&#31034;&#26469;&#25429;&#25417;&#35821;&#20041;&#24847;&#20041;&#32780;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#20381;&#36182;&#20110;&#26126;&#30830;&#30340;&#35789;&#39033;&#21305;&#37197;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21487;&#35835;&#24615;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#26597;&#35810;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#31561;&#20215;&#26597;&#35810;&#8221;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;NRM&#32467;&#26524;&#19982;&#20855;&#26377;&#31561;&#20215;&#26597;&#35810;&#30340;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#30340;&#32467;&#26524;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#31561;&#20215;&#26597;&#35810;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;(&#22914;&#22522;&#20110;RM3&#30340;&#26597;&#35810;&#25193;&#23637;)&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23545;&#26816;&#32034;&#25928;&#26524;&#21644;&#27599;&#31181;&#26041;&#27861;&#29983;&#25104;&#30340;&#35789;&#39033;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural retrieval models (NRMs) have been shown to outperform their statistical counterparts owing to their ability to capture semantic meaning via dense document representations. These models, however, suffer from poor interpretability as they do not rely on explicit term matching. As a form of local per-query explanations, we introduce the notion of equivalent queries that are generated by maximizing the similarity between the NRM's results and the result set of a sparse retrieval system with the equivalent query. We then compare this approach with existing methods such as RM3-based query expansion and contrast differences in retrieval effectiveness and in the terms generated by each approach.
&lt;/p&gt;</description></item><item><title>OFAR&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#30340;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#30452;&#25773;&#24179;&#21488;&#31435;&#21363;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#38750;&#27861;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.12608</link><description>&lt;p&gt;
OFAR: &#19968;&#31181;&#29992;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#30340;&#22810;&#27169;&#24335;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OFAR: A Multimodal Evidence Retrieval Framework for Illegal Live-streaming Identification. (arXiv:2304.12608v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12608
&lt;/p&gt;
&lt;p&gt;
OFAR&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#30340;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#30452;&#25773;&#24179;&#21488;&#31435;&#21363;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#38750;&#27861;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#26159;&#20026;&#20102;&#24110;&#21161;&#30452;&#25773;&#24179;&#21488;&#31435;&#21363;&#35782;&#21035;&#30452;&#25773;&#20013;&#30340;&#38750;&#27861;&#34892;&#20026;&#65292;&#20363;&#22914;&#21806;&#21334;&#29645;&#36149;&#21644;&#28626;&#21361;&#21160;&#29289;&#65292;&#23545;&#20928;&#21270;&#32593;&#32476;&#29615;&#22659;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OFAR&#30340;&#22810;&#27169;&#24335;&#35777;&#25454;&#26816;&#32034;&#26694;&#26550;&#65292;&#20197;&#21033;&#20110;&#38750;&#27861;&#30452;&#25773;&#35782;&#21035;&#12290;OFAR&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#26597;&#35810;&#32534;&#30721;&#22120;&#12289;&#25991;&#26723;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;MaxSim&#30340;&#23545;&#27604;&#26202;&#20132;&#38598;&#12290;&#26597;&#35810;&#32534;&#30721;&#22120;&#21644;&#25991;&#26723;&#32534;&#30721;&#22120;&#37117;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;OFA&#12290;
&lt;/p&gt;
&lt;p&gt;
Illegal live-streaming identification, which aims to help live-streaming platforms immediately recognize the illegal behaviors in the live-streaming, such as selling precious and endangered animals, plays a crucial role in purifying the network environment. Traditionally, the live-streaming platform needs to employ some professionals to manually identify the potential illegal live-streaming. Specifically, the professional needs to search for related evidence from a large-scale knowledge database for evaluating whether a given live-streaming clip contains illegal behavior, which is time-consuming and laborious. To address this issue, in this work, we propose a multimodal evidence retrieval system, named OFAR, to facilitate the illegal live-streaming identification. OFAR consists of three modules: \textit{Query Encoder}, \textit{Document Encoder}, and \textit{MaxSim-based Contrastive Late Intersection}. Both query encoder and document encoder are implemented with the advanced \mbox{OFA} 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#25903;&#26609;&#30340;&#22270;&#25991;&#26816;&#32034;&#37325;&#25490;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#31561;&#32423;&#26368;&#39640;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#37051;&#23621;&#20316;&#20026;&#25903;&#26609;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#37325;&#26500;&#25968;&#25454;&#26679;&#26412;&#65292;&#23558;&#27599;&#20010;&#26679;&#26412;&#26144;&#23556;&#21040;&#22810;&#27169;&#24577;&#25903;&#26609;&#31354;&#38388;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.12570</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#25903;&#26609;&#30340;&#22270;&#25991;&#26816;&#32034;&#37325;&#25490;
&lt;/p&gt;
&lt;p&gt;
Learnable Pillar-based Re-ranking for Image-Text Retrieval. (arXiv:2304.12570v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#25903;&#26609;&#30340;&#22270;&#25991;&#26816;&#32034;&#37325;&#25490;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#31561;&#32423;&#26368;&#39640;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#37051;&#23621;&#20316;&#20026;&#25903;&#26609;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#37325;&#26500;&#25968;&#25454;&#26679;&#26412;&#65292;&#23558;&#27599;&#20010;&#26679;&#26412;&#26144;&#23556;&#21040;&#22810;&#27169;&#24577;&#25903;&#26609;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25991;&#26816;&#32034;&#26088;&#22312;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#36328;&#27169;&#24577;&#26816;&#32034;&#20869;&#23481;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#20108;&#20803;&#20851;&#31995;&#65288;&#21363;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#21305;&#37197;&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#39640;&#38454;&#37051;&#23621;&#20851;&#31995;&#65288;&#21363;&#22810;&#20010;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#21305;&#37197;&#32467;&#26500;&#65289;&#12290;&#37325;&#25490;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21518;&#22788;&#29702;&#23454;&#36341;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#21333;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#25429;&#25417;&#37051;&#23621;&#20851;&#31995;&#30340;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#30340;&#37325;&#25490;&#31639;&#27861;&#30452;&#25509;&#25193;&#23637;&#21040;&#22270;&#25991;&#26816;&#32034;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#27492;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#33324;&#21270;&#12289;&#28789;&#27963;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#19981;&#23545;&#31216;&#24615;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#25903;&#26609;&#30340;&#22270;&#25991;&#26816;&#32034;&#37325;&#25490;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#31561;&#32423;&#26368;&#39640;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#37051;&#23621;&#20316;&#20026;&#25903;&#26609;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#21644;&#25903;&#26609;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#37325;&#26500;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27599;&#20010;&#26679;&#26412;&#21487;&#20197;&#26144;&#23556;&#21040;&#22810;&#27169;&#24577;&#25903;&#26609;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text retrieval aims to bridge the modality gap and retrieve cross-modal content based on semantic similarities. Prior work usually focuses on the pairwise relations (i.e., whether a data sample matches another) but ignores the higher-order neighbor relations (i.e., a matching structure among multiple data samples). Re-ranking, a popular post-processing practice, has revealed the superiority of capturing neighbor relations in single-modality retrieval tasks. However, it is ineffective to directly extend existing re-ranking algorithms to image-text retrieval. In this paper, we analyze the reason from four perspectives, i.e., generalization, flexibility, sparsity, and asymmetry, and propose a novel learnable pillar-based re-ranking paradigm. Concretely, we first select top-ranked intra- and inter-modal neighbors as pillars, and then reconstruct data samples with the neighbor relations between them and the pillars. In this way, each sample can be mapped into a multimodal pillar space
&lt;/p&gt;</description></item><item><title>COUPA&#26159;&#19968;&#20010;&#38754;&#21521;O2O&#26381;&#21153;&#24179;&#21488;&#30340;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#20559;&#22909;&#21644;&#20301;&#32622;&#24863;&#30693;&#20559;&#22909;&#26469;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12549</link><description>&lt;p&gt;
COUPA: &#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#21040;&#32447;&#19979;&#26381;&#21153;&#24179;&#21488;&#30340;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
COUPA: An Industrial Recommender System for Online to Offline Service Platforms. (arXiv:2304.12549v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12549
&lt;/p&gt;
&lt;p&gt;
COUPA&#26159;&#19968;&#20010;&#38754;&#21521;O2O&#26381;&#21153;&#24179;&#21488;&#30340;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#20559;&#22909;&#21644;&#20301;&#32622;&#24863;&#30693;&#20559;&#22909;&#26469;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#21040;&#32447;&#19979;&#65288;O2O&#65289;&#26381;&#21153;&#24179;&#21488;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#22312;&#26412;&#22320;&#21457;&#29616;&#38646;&#21806;&#26381;&#21153;&#65288;&#20363;&#22914;&#23089;&#20048;&#21644;&#39184;&#39278;&#65289;&#65292;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36825;&#26497;&#22823;&#22320;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#22522;&#20110;&#25903;&#20184;&#23453;&#24179;&#21488;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#38024;&#23545;O2O&#26381;&#21153;&#30340;&#29305;&#27530;&#24773;&#26223;&#21457;&#29616;&#20102;&#36882;&#24402;&#22522;&#30784;&#19978;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20301;&#32622;&#20559;&#32622;&#26222;&#36941;&#23384;&#22312;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#25512;&#33616;&#25928;&#26524;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COUPA&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#19994;&#32423;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#26469;&#34920;&#24449;&#29992;&#25143;&#20559;&#22909;&#65306;(1)&#26102;&#38388;&#24863;&#30693;&#20559;&#22909;&#65306;&#25105;&#20204;&#37319;&#29992;&#20102;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#36830;&#32493;&#26102;&#38388;&#24863;&#30693;&#28857;&#36807;&#31243;&#65292;&#20197;&#23436;&#20840;&#25429;&#25417;&#25512;&#33616;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;(2)&#20301;&#32622;&#24863;&#30693;&#20559;&#22909;&#65306;&#19968;&#20010;&#37197;&#22791;&#20102;&#20301;&#32622;&#20010;&#24615;&#21270;&#27169;&#22359;&#30340;&#20301;&#32622;&#36873;&#25321;&#22120;&#32452;&#20214;&#34987;&#31934;&#24515;&#35774;&#35745;&#65292;&#20197;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20180;&#32454;&#23454;&#29616;&#24182;&#37096;&#32626;COUPA&#22312;&#25903;&#20184;&#23453;&#24179;&#21488;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at helping users locally discovery retail services (e.g., entertainment and dinning), Online to Offline (O2O) service platforms have become popular in recent years, which greatly challenge current recommender systems. With the real data in Alipay, a feeds-like scenario for O2O services, we find that recurrence based temporal patterns and position biases commonly exist in our scenarios, which seriously threaten the recommendation effectiveness. To this end, we propose COUPA, an industrial system targeting for characterizing user preference with following two considerations: (1) Time aware preference: we employ the continuous time aware point process equipped with an attention mechanism to fully capture temporal patterns for recommendation. (2) Position aware preference: a position selector component equipped with a position personalization module is elaborately designed to mitigate position bias in a personalized manner. Finally, we carefully implement and deploy COUPA on Alipay 
&lt;/p&gt;</description></item><item><title>GARCIA&#21033;&#29992;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#65292;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.12537</link><description>&lt;p&gt;
GARCIA&#65306;&#21033;&#29992;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GARCIA: Powering Representations of Long-tail Query with Multi-granularity Contrastive Learning. (arXiv:2304.12537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12537
&lt;/p&gt;
&lt;p&gt;
GARCIA&#21033;&#29992;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#65292;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26381;&#21153;&#24179;&#21488;&#30340;&#21457;&#23637;&#20026;&#29992;&#25143;&#21644;&#21830;&#23478;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#20415;&#21033;&#65292;&#26381;&#21153;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#25991;&#26412;&#26597;&#35810;&#22312;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#19981;&#21487;&#25511;&#21046;&#30340;&#25628;&#32034;&#20064;&#24815;&#36890;&#24120;&#24102;&#26469;&#22823;&#37327;&#30340;&#38271;&#23614;&#26597;&#35810;&#65292;&#36825;&#20005;&#37325;&#23041;&#32961;&#21040;&#25628;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;GARCIA&#65292;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the growth of service platforms brings great convenience to both users and merchants, where the service search engine plays a vital role in improving the user experience by quickly obtaining desirable results via textual queries. Unfortunately, users' uncontrollable search customs usually bring vast amounts of long-tail queries, which severely threaten the capability of search models. Inspired by recently emerging graph neural networks (GNNs) and contrastive learning (CL), several efforts have been made in alleviating the long-tail issue and achieve considerable performance. Nevertheless, they still face a few major weaknesses. Most importantly, they do not explicitly utilize the contextual structure between heads and tails for effective knowledge transfer, and intention-level information is commonly ignored for more generalized representations.  To this end, we develop a novel framework GARCIA, which exploits the graph based knowledge transfer and intention based representat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Rank Flow Embedding&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#30340;&#27969;&#24418;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.12448</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#27969;&#24418;&#23398;&#20064;&#30340;Rank Flow Embedding
&lt;/p&gt;
&lt;p&gt;
Rank Flow Embedding for Unsupervised and Semi-Supervised Manifold Learning. (arXiv:2304.12448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Rank Flow Embedding&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#30340;&#27969;&#24418;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#33719;&#21462;&#21644;&#20849;&#20139;&#25216;&#26415;&#20351;&#24471;&#22810;&#23186;&#20307;&#38598;&#21512;&#21450;&#20854;&#24212;&#29992;&#30340;&#22686;&#38271;&#20960;&#20046;&#26080;&#38480;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#24448;&#24448;&#26114;&#36149;&#32780;&#36153;&#26102;&#65292;&#22240;&#27492;&#26377;&#30417;&#30563;&#30340;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#27880;&#25968;&#25454;&#21487;&#29992;&#24615;&#30456;&#21453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rank Flow Embedding&#65288;RFE&#65289;&#30340;&#26032;&#22411;&#27969;&#24418;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#26368;&#36817;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#36229;&#22270;&#12289;&#31515;&#21345;&#23572;&#31215;&#21644;&#36830;&#36890;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Impressive advances in acquisition and sharing technologies have made the growth of multimedia collections and their applications almost unlimited. However, the opposite is true for the availability of labeled data, which is needed for supervised training, since such data is often expensive and time-consuming to obtain. While there is a pressing need for the development of effective retrieval and classification methods, the difficulties faced by supervised approaches highlight the relevance of methods capable of operating with few or no labeled data. In this work, we propose a novel manifold learning algorithm named Rank Flow Embedding (RFE) for unsupervised and semi-supervised scenarios. The proposed method is based on ideas recently exploited by manifold learning approaches, which include hypergraphs, Cartesian products, and connected components. The algorithm computes context-sensitive embeddings, which are refined following a rank-based processing flow, while complementary contextu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12395</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#30340;&#26497;&#38480;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#26377;&#29992;&#27493;&#39588;&#12290; SMART&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#21069;k&#20010;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#31867;&#22411;&#12290;&#30001;&#20110;KG&#20013;&#23384;&#22312;&#22823;&#37327;&#31867;&#22411;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20855;&#20307;&#22320;&#25913;&#21892;&#20102;XBERT&#27969;&#31243;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;KG&#20013;&#27966;&#29983;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;SMART&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
&lt;/p&gt;</description></item><item><title>TREC NeuCLIR&#36712;&#36947;&#30340;&#31532;&#19968;&#24180;&#65292;&#30740;&#31350;&#31070;&#32463;&#26041;&#27861;&#23545;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33521;&#35821;&#26597;&#35810;&#26469;ad hoc&#25490;&#21517;&#26816;&#32034;&#20013;&#25991;&#12289;&#27874;&#26031;&#35821;&#25110;&#20420;&#35821;&#26032;&#38395;&#25991;&#26723;&#65292;&#20849;&#26377;12&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;172&#27425;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.12367</link><description>&lt;p&gt;
TREC 2022 NeuCLIR&#36712;&#36947;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of the TREC 2022 NeuCLIR Track. (arXiv:2304.12367v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12367
&lt;/p&gt;
&lt;p&gt;
TREC NeuCLIR&#36712;&#36947;&#30340;&#31532;&#19968;&#24180;&#65292;&#30740;&#31350;&#31070;&#32463;&#26041;&#27861;&#23545;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33521;&#35821;&#26597;&#35810;&#26469;ad hoc&#25490;&#21517;&#26816;&#32034;&#20013;&#25991;&#12289;&#27874;&#26031;&#35821;&#25110;&#20420;&#35821;&#26032;&#38395;&#25991;&#26723;&#65292;&#20849;&#26377;12&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;172&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;TREC&#31070;&#32463;CLIR&#65288;NeuCLIR&#65289;&#36712;&#36947;&#30340;&#31532;&#19968;&#24180;&#65292;&#26088;&#22312;&#30740;&#31350;&#31070;&#32463;&#26041;&#27861;&#23545;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;&#20170;&#24180;&#36712;&#36947;&#30340;&#20027;&#35201;&#20219;&#21153;&#26159;&#20351;&#29992;&#29992;&#33521;&#35821;&#34920;&#36798;&#30340;&#26597;&#35810;&#65292;&#23545;&#20013;&#25991;&#12289;&#27874;&#26031;&#35821;&#25110;&#20420;&#35821;&#26032;&#38395;&#25991;&#26723;&#36827;&#34892;ad hoc&#25490;&#21517;&#26816;&#32034;&#12290;&#35805;&#39064;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;TREC&#27969;&#31243;&#24320;&#21457;&#30340;&#65292;&#38500;&#20102;&#22312;&#35780;&#20272;&#35813;&#35805;&#39064;&#30340;&#19981;&#21516;&#35821;&#35328;&#19978;&#35780;&#20272;&#19968;&#20010;&#27880;&#37322;&#22120;&#24320;&#21457;&#30340;&#35805;&#39064;&#26102;&#65292;&#30001;&#21478;&#19968;&#20010;&#27880;&#37322;&#22120;&#24320;&#21457;&#30340;&#35805;&#39064;&#12290;&#20849;&#26377;12&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;172&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first year of the TREC Neural CLIR (NeuCLIR) track, which aims to study the impact of neural approaches to cross-language information retrieval. The main task in this year's track was ad hoc ranked retrieval of Chinese, Persian, or Russian newswire documents using queries expressed in English. Topics were developed using standard TREC processes, except that topics developed by an annotator for one language were assessed by a different annotator when evaluating that topic on a different language. There were 172 total runs submitted by twelve teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;USTEP&#65292;&#35813;&#26041;&#27861;&#22312;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.12331</link><description>&lt;p&gt;
USTEP: &#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
USTEP: Structuration des logs en flux gr{\^a}ce {\`a} un arbre de recherche {\'e}volutif. (arXiv:2304.12331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;USTEP&#65292;&#35813;&#26041;&#27861;&#22312;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#34892;&#26102;&#26085;&#24535;&#35760;&#24405;&#20102;&#26377;&#20215;&#20540;&#30340;&#31995;&#32479;&#20449;&#24687;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24320;&#21457;&#21644;&#30417;&#25511;&#30446;&#30340;&#12290;&#35299;&#26512;&#26085;&#24535;&#28040;&#24687;&#20197;&#32467;&#26500;&#21270;&#20854;&#26684;&#24335;&#26159;&#26085;&#24535;&#25366;&#25496;&#20219;&#21153;&#30340;&#32463;&#20856;&#39044;&#22791;&#27493;&#39588;&#12290;&#30001;&#20110;&#23427;&#20204;&#20986;&#29616;&#22312;&#19978;&#28216;&#65292;&#35299;&#26512;&#25805;&#20316;&#21487;&#33021;&#25104;&#20026;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#22788;&#29702;&#26102;&#38388;&#29942;&#39048;&#12290;&#35299;&#26512;&#36136;&#37327;&#20063;&#30452;&#25509;&#24433;&#21709;&#20854;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;USTEP&#12290;&#23545;&#26469;&#33258;&#19981;&#21516;&#23454;&#38469;&#31995;&#32479;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22312;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;USTEP&#22312;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs record valuable system information at runtime. They are widely used by data-driven approaches for development and monitoring purposes. Parsing log messages to structure their format is a classic preliminary step for log-mining tasks. As they appear upstream, parsing operations can become a processing time bottleneck for downstream applications. The quality of parsing also has a direct influence on their efficiency. Here, we propose USTEP, an online log parsing method based on an evolving tree structure. Evaluation results on a wide panel of datasets coming from different real-world systems demonstrate USTEP superiority in terms of both effectiveness and robustness when compared to other online methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CONTINUOUS&#65292;&#21487;&#20197;&#23545;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#65292;&#23427;&#36890;&#36807;&#23558;&#23884;&#20837;&#22823;&#23567;&#36873;&#25321;&#24314;&#27169;&#20026;&#36830;&#32493;&#21464;&#37327;&#35299;&#20915;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03501</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#36830;&#32493;&#36755;&#20837;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continuous Input Embedding Size Search For Recommender Systems. (arXiv:2304.03501v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CONTINUOUS&#65292;&#21487;&#20197;&#23545;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#65292;&#23427;&#36890;&#36807;&#23558;&#23884;&#20837;&#22823;&#23567;&#36873;&#25321;&#24314;&#27169;&#20026;&#36830;&#32493;&#21464;&#37327;&#35299;&#20915;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#26159;&#29616;&#20170;&#25512;&#33616;&#31995;&#32479;&#26368;&#27969;&#34892;&#30340;&#22522;&#30784;&#65292;&#20854;&#24615;&#33021;&#21331;&#36234;&#12290;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36890;&#36807;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#36827;&#34892;&#34920;&#31034;&#65292;&#29992;&#20110;&#23545;&#25104;&#23545;&#30456;&#20284;&#24230;&#30340;&#35745;&#31639;&#12290;&#25152;&#26377;&#23884;&#20837;&#21521;&#37327;&#20256;&#32479;&#19978;&#37117;&#34987;&#38480;&#21046;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#22823;&#30340;&#32479;&#19968;&#22823;&#23567;&#65288;&#20363;&#22914;256&#32500;&#65289;&#12290;&#38543;&#30528;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#20013;&#29992;&#25143;&#21644;&#39033;&#30446;&#30446;&#24405;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36825;&#31181;&#35774;&#35745;&#26174;&#28982;&#21464;&#24471;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#20419;&#36827;&#36731;&#37327;&#32423;&#25512;&#33616;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26368;&#36817;&#24320;&#36767;&#20102;&#19968;&#20123;&#26426;&#20250;&#65292;&#29992;&#20110;&#35782;&#21035;&#19981;&#21516;&#29992;&#25143;/&#39033;&#30446;&#30340;&#19981;&#21516;&#23884;&#20837;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#25628;&#32034;&#25928;&#29575;&#21644;&#23398;&#20064;&#26368;&#20248;RL&#31574;&#30053;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#34987;&#38480;&#21046;&#20026;&#39640;&#24230;&#31163;&#25955;&#30340;&#39044;&#23450;&#20041;&#23884;&#20837;&#22823;&#23567;&#36873;&#39033;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#34987;&#24191;&#27867;&#24573;&#35270;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#24341;&#20837;&#26356;&#32454;&#30340;&#31890;&#24230;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;CONTINUOUS&#65292;&#21487;&#20197;&#23545;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#12290;CONTINUOUS&#36890;&#36807;&#23558;&#23884;&#20837;&#22823;&#23567;&#36873;&#25321;&#24314;&#27169;&#20026;&#36830;&#32493;&#21464;&#37327;&#21644;&#21046;&#23450;&#21487;&#24494;&#20248;&#21270;&#38382;&#39064;&#30340;&#24418;&#24335;&#26469;&#35299;&#20915;&#20043;&#21069;&#24037;&#20316;&#30340;&#25361;&#25112;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;CONTINUOUS&#20248;&#20110;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#21160;&#24577;&#20248;&#21270;&#23884;&#20837;&#22823;&#23567;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent factor models are the most popular backbones for today's recommender systems owing to their prominent performance. Latent factor models represent users and items as real-valued embedding vectors for pairwise similarity computation, and all embeddings are traditionally restricted to a uniform size that is relatively large (e.g., 256-dimensional). With the exponentially expanding user base and item catalog in contemporary e-commerce, this design is admittedly becoming memory-inefficient. To facilitate lightweight recommendation, reinforcement learning (RL) has recently opened up opportunities for identifying varying embedding sizes for different users/items. However, challenged by search efficiency and learning an optimal RL policy, existing RL-based methods are restricted to highly discrete, predefined embedding size choices. This leads to a largely overlooked potential of introducing finer granularity into embedding sizes to obtain better recommendation effectiveness under a giv
&lt;/p&gt;</description></item><item><title>CODER &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#20195;&#30721;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#24494;&#35266;&#29992;&#25143;-&#20195;&#30721;&#20132;&#20114;&#21644;&#23439;&#35266;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65292;&#39044;&#27979;&#24320;&#21457;&#32773;&#26410;&#26469;&#30340;&#36129;&#29486;&#34892;&#20026;&#65292;&#20197;&#32553;&#30701;&#24320;&#21457;&#26102;&#38388;&#24182;&#25552;&#39640;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.08332</link><description>&lt;p&gt;
&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Code Recommendation for Open Source Software Developers. (arXiv:2210.08332v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08332
&lt;/p&gt;
&lt;p&gt;
CODER &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#20195;&#30721;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#24494;&#35266;&#29992;&#25143;-&#20195;&#30721;&#20132;&#20114;&#21644;&#23439;&#35266;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65292;&#39044;&#27979;&#24320;&#21457;&#32773;&#26410;&#26469;&#30340;&#36129;&#29486;&#34892;&#20026;&#65292;&#20197;&#32553;&#30701;&#24320;&#21457;&#26102;&#38388;&#24182;&#25552;&#39640;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#36719;&#20214;&#26159;&#25216;&#26415;&#22522;&#30784;&#35774;&#26045;&#30340;&#25903;&#26609;&#65292;&#21560;&#24341;&#25968;&#30334;&#19975;&#20154;&#25165;&#20570;&#20986;&#36129;&#29486;&#12290;&#32771;&#34385;&#21040;&#24320;&#21457;&#20154;&#21592;&#30340;&#20852;&#36259;&#21644;&#39033;&#30446;&#20195;&#30721;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#21521;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#32773;&#25512;&#33616;&#36866;&#24403;&#30340;&#24320;&#21457;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20195;&#30721;&#25512;&#33616;&#38382;&#39064;&#65292;&#20854;&#30446;&#30340;&#26159;&#26681;&#25454;&#24320;&#21457;&#32773;&#30340;&#20132;&#20114;&#21382;&#21490;&#12289;&#28304;&#20195;&#30721;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#39033;&#30446;&#30340;&#20998;&#23618;&#25991;&#20214;&#32467;&#26500;&#26469;&#39044;&#27979;&#24320;&#21457;&#32773;&#26410;&#26469;&#30340;&#36129;&#29486;&#34892;&#20026;&#12290;&#32771;&#34385;&#21040;&#31995;&#32479;&#20013;&#22810;&#26041;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CODER&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#32773;&#20195;&#30721;&#25512;&#33616;&#26694;&#26550;&#12290;CODER&#36890;&#36807;&#24322;&#26500;&#22270;&#20849;&#21516;&#24314;&#27169;&#24494;&#35266;&#29992;&#25143;-&#20195;&#30721;&#20132;&#20114;&#21644;&#23439;&#35266;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#22312;&#21453;&#26144;&#25991;&#20214;&#32467;&#26500;&#30340;&#25991;&#20214;&#32467;&#26500;&#22270;&#19978;&#30340;&#32858;&#21512;&#36827;&#19968;&#27493;&#36830;&#25509;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Source Software (OSS) is forming the spines of technology infrastructures, attracting millions of talents to contribute. Notably, it is challenging and critical to consider both the developers' interests and the semantic features of the project code to recommend appropriate development tasks to OSS developers. In this paper, we formulate the novel problem of code recommendation, whose purpose is to predict the future contribution behaviors of developers given their interaction history, the semantic features of source code, and the hierarchical file structures of projects. Considering the complex interactions among multiple parties within the system, we propose CODER, a novel graph-based code recommendation framework for open source software developers. CODER jointly models microscopic user-code interactions and macroscopic user-project interactions via a heterogeneous graph and further bridges the two levels of information through aggregation on file-structure graphs that reflect 
&lt;/p&gt;</description></item></channel></rss>