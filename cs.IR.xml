<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#26041;&#27861;&#65288;RVD&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#20043;&#21069;&#21033;&#29992;&#37051;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17278</link><description>&lt;p&gt;
&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#26041;&#27861;&#22312;&#20581;&#22766;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Recommendation via Real-time Vicinal Defense. (arXiv:2309.17278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#26041;&#27861;&#65288;RVD&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#20043;&#21069;&#21033;&#29992;&#37051;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24694;&#24847;&#25968;&#25454;&#25554;&#20837;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#25552;&#20379;&#26377;&#20559;&#35265;&#30340;&#25512;&#33616;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20581;&#22766;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#25110;&#29305;&#23450;&#20110;&#25915;&#20987;&#30340;&#65292;&#32570;&#20047;&#24191;&#27867;&#24615;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65289;&#20391;&#37325;&#20110;&#36867;&#36920;&#25915;&#20987;&#65292;&#22312;&#27602;&#21270;&#25915;&#20987;&#19978;&#26377;&#24456;&#24369;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#8212;&#8212;&#23454;&#26102;&#37051;&#36817;&#38450;&#24481;&#65288;RVD&#65289;&#65292;&#23427;&#21033;&#29992;&#37051;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#22312;&#20026;&#27599;&#20010;&#29992;&#25143;&#36827;&#34892;&#25512;&#33616;&#20043;&#21069;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;RVD&#22312;&#25512;&#26029;&#38454;&#27573;&#24037;&#20316;&#65292;&#20197;&#30830;&#20445;&#23454;&#26102;&#24615;&#26679;&#26412;&#30340;&#20581;&#22766;&#24615;&#65292;&#22240;&#27492;&#26080;&#38656;&#26356;&#25913;&#27169;&#22411;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#26356;&#21152;&#23454;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RVD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have been shown to be vulnerable to poisoning attacks, where malicious data is injected into the dataset to cause the recommender system to provide biased recommendations. To defend against such attacks, various robust learning methods have been proposed. However, most methods are model-specific or attack-specific, making them lack generality, while other methods, such as adversarial training, are oriented towards evasion attacks and thus have a weak defense strength in poisoning attacks.  In this paper, we propose a general method, Real-time Vicinal Defense (RVD), which leverages neighboring training data to fine-tune the model before making a recommendation for each user. RVD works in the inference phase to ensure the robustness of the specific sample in real-time, so there is no need to change the model structure and training process, making it more practical. Extensive experimental results demonstrate that RVD effectively mitigates targeted poisoning attacks acr
&lt;/p&gt;</description></item><item><title>SAppKG&#26159;&#19968;&#20010;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#31227;&#21160;&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#27169;&#22411;&#21644;&#20391;&#38754;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17115</link><description>&lt;p&gt;
SAppKG&#65306;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20391;&#38754;&#20449;&#24687;&#30340;&#31227;&#21160;&#24212;&#29992;&#25512;&#33616;-&#19968;&#20010;&#23433;&#20840;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SAppKG: Mobile App Recommendation Using Knowledge Graph and Side Information-A Secure Framework. (arXiv:2309.17115v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17115
&lt;/p&gt;
&lt;p&gt;
SAppKG&#26159;&#19968;&#20010;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#31227;&#21160;&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#27169;&#22411;&#21644;&#20391;&#38754;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26234;&#33021;&#25163;&#26426;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#31227;&#21160;&#24212;&#29992;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25214;&#21040;&#19968;&#32452;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#21644;&#20559;&#22909;&#30340;&#21512;&#36866;&#24212;&#29992;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#31227;&#21160;&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#23384;&#22312;&#19968;&#20010;&#32570;&#28857;&#65292;&#21363;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#35775;&#38382;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#23433;&#20840;&#36829;&#35268;&#12290;&#34429;&#28982;&#29992;&#25143;&#23547;&#27714;&#20934;&#30830;&#30340;&#24847;&#35265;&#65292;&#20294;&#19981;&#24076;&#26395;&#22312;&#27492;&#36807;&#31243;&#20013;&#25439;&#23475;&#33258;&#24049;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;SAppKG&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30693;&#35782;&#22270;&#35889;&#26550;&#26500;&#65292;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#27169;&#22411;&#65292;&#22914;SAppKG-S&#21644;SAppKG-D&#65292;&#21033;&#29992;&#24212;&#29992;&#20132;&#20114;&#25968;&#25454;&#21644;&#20391;&#38754;&#20449;&#24687;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#25512;&#33616;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#35895;&#27468;Play&#24212;&#29992;&#21830;&#24215;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;&#24179;&#22343;&#32477;&#23545;&#31934;&#30830;&#24230;&#21644;&#12290;&#22343;&#26041;&#26681;&#35823;&#24046;&#31561;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid development of technology and the widespread usage of smartphones, the number of mobile applications is exponentially growing. Finding a suitable collection of apps that aligns with users needs and preferences can be challenging. However, mobile app recommender systems have emerged as a helpful tool in simplifying this process. But there is a drawback to employing app recommender systems. These systems need access to user data, which is a serious security violation. While users seek accurate opinions, they do not want to compromise their privacy in the process. We address this issue by developing SAppKG, an end-to-end user privacy-preserving knowledge graph architecture for mobile app recommendation based on knowledge graph models such as SAppKG-S and SAppKG-D, that utilized the interaction data and side information of app attributes. We tested the proposed model on real-world data from the Google Play app store, using precision, recall, mean absolute precision, and me
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#29305;&#23450;&#24615;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#24615;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.17078</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21453;&#39304;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20449;&#24687;&#26816;&#32034;&#19978;&#19979;&#25991;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback. (arXiv:2309.17078v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#29305;&#23450;&#24615;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#24615;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;(IR)&#26159;&#23547;&#25214;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#30340;&#36807;&#31243;&#65292;&#22312;&#29616;&#20195;&#20154;&#30340;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26480;&#20986;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;IR&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#32463;&#24120;&#38754;&#20020;&#29983;&#25104;&#32570;&#20047;&#29305;&#23450;&#24615;&#22238;&#22797;&#30340;&#38382;&#39064;&#12290;&#36825;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#38480;&#21046;&#20102;LLMs&#22312;IR&#20013;&#30340;&#25972;&#20307;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;&#23545;&#27604;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;(RLCF)&#65292;&#23427;&#36171;&#20104;LLMs&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#36136;&#37327;&#21448;&#19982;IR&#20219;&#21153;&#38656;&#27714;&#30456;&#31526;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#20010;&#25991;&#26723;&#19982;&#20854;&#30456;&#20284;&#25991;&#26723;&#36827;&#34892;&#27604;&#36739;&#26500;&#24314;&#23545;&#27604;&#21453;&#39304;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Batched-MRR&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25945;&#23548;LLMs&#29983;&#25104;&#33021;&#22815;&#25429;&#25417;&#21306;&#20998;&#25991;&#26723;&#19982;&#20854;&#30456;&#20284;&#25991;&#26723;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Retrieval (IR), the process of finding information to satisfy user's information needs, plays an essential role in modern people's lives. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, some of which are important for IR. Nonetheless, LLMs frequently confront the issue of generating responses that lack specificity. This has limited the overall effectiveness of LLMs for IR in many cases. To address these issues, we present an unsupervised alignment framework called Reinforcement Learning from Contrastive Feedback (RLCF), which empowers LLMs to generate both high-quality and context-specific responses that suit the needs of IR tasks. Specifically, we construct contrastive feedback by comparing each document with its similar documents, and then propose a reward function named Batched-MRR to teach LLMs to generate responses that captures the fine-grained information that distinguish documents from their similar ones. To dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#20174;&#24322;&#26500;&#25551;&#36848;&#24615;&#20449;&#24687;&#20013;&#25552;&#21462;&#30456;&#20851;&#35821;&#20041;&#12289;&#32508;&#21512;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25512;&#26029;&#29992;&#25143;&#20852;&#36259;&#20197;&#21450;&#22788;&#29702;&#25968;&#20540;&#20449;&#24687;&#27010;&#29575;&#24433;&#21709;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17037</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#29616;: &#22810;&#27169;&#24577;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Beyond Co-occurrence: Multi-modal Session-based Recommendation. (arXiv:2309.17037v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#20174;&#24322;&#26500;&#25551;&#36848;&#24615;&#20449;&#24687;&#20013;&#25552;&#21462;&#30456;&#20851;&#35821;&#20041;&#12289;&#32508;&#21512;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25512;&#26029;&#29992;&#25143;&#20852;&#36259;&#20197;&#21450;&#22788;&#29702;&#25968;&#20540;&#20449;&#24687;&#27010;&#29575;&#24433;&#21709;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26088;&#22312;&#26681;&#25454;&#30701;&#20250;&#35805;&#26469;&#25581;&#31034;&#21311;&#21517;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22312;&#20250;&#35805;&#20013;&#36890;&#36807;&#29289;&#21697;ID&#23637;&#31034;&#30340;&#26377;&#38480;&#29289;&#21697;&#20849;&#29616;&#27169;&#24335;&#30340;&#25366;&#25496;&#65292;&#32780;&#24573;&#35270;&#20102;&#29992;&#25143;&#23545;&#29305;&#23450;&#29289;&#21697;&#30340;&#21560;&#24341;&#21147;&#26159;&#22810;&#27169;&#24577;&#39029;&#38754;&#19978;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#22810;&#27169;&#24577;&#20449;&#24687;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;: &#25551;&#36848;&#24615;&#20449;&#24687;(&#20363;&#22914;&#29289;&#21697;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;)&#21644;&#25968;&#20540;&#20449;&#24687;(&#20363;&#22914;&#20215;&#26684;)&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25972;&#20307;&#24314;&#27169;&#19978;&#36848;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#25913;&#36827;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20174;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25581;&#31034;&#29992;&#25143;&#24847;&#22270;&#20027;&#35201;&#23384;&#22312;&#19977;&#20010;&#38382;&#39064;: (1) &#22914;&#20309;&#20174;&#20855;&#26377;&#19981;&#21516;&#22122;&#22768;&#30340;&#24322;&#26500;&#25551;&#36848;&#24615;&#20449;&#24687;&#20013;&#25552;&#21462;&#30456;&#20851;&#35821;&#20041;? (2) &#22914;&#20309;&#32508;&#21512;&#21033;&#29992;&#36825;&#20123;&#24322;&#26500;&#25551;&#36848;&#24615;&#20449;&#24687;&#20840;&#38754;&#25512;&#26029;&#29992;&#25143;&#20852;&#36259;? (3) &#22914;&#20309;&#22788;&#29702;&#25968;&#20540;&#20449;&#24687;&#30340;&#27010;&#29575;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation is devoted to characterizing preferences of anonymous users based on short sessions. Existing methods mostly focus on mining limited item co-occurrence patterns exposed by item ID within sessions, while ignoring what attracts users to engage with certain items is rich multi-modal information displayed on pages. Generally, the multi-modal information can be classified into two categories: descriptive information (e.g., item images and description text) and numerical information (e.g., price). In this paper, we aim to improve session-based recommendation by modeling the above multi-modal information holistically. There are mainly three issues to reveal user intent from multi-modal information: (1) How to extract relevant semantics from heterogeneous descriptive information with different noise? (2) How to fuse these heterogeneous descriptive information to comprehensively infer user interests? (3) How to handle probabilistic influence of numerical information
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#65292;&#36890;&#36807;&#22312;Longformer Encoder-Decoder&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16781</link><description>&lt;p&gt;
&#38271;&#36755;&#20837;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Hallucination Reduction in Long Input Text Summarization. (arXiv:2309.16781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#65292;&#36890;&#36807;&#22312;Longformer Encoder-Decoder&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#27169;&#22411;&#29983;&#25104;&#19981;&#34987;&#36755;&#20837;&#28304;&#25991;&#26723;&#25903;&#25345;&#30340;&#20449;&#24687;&#30340;&#29616;&#35937;&#12290;&#24187;&#35273;&#32473;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#38271;&#31687;&#31185;&#23398;&#30740;&#31350;&#25991;&#26723;&#21450;&#20854;&#25688;&#35201;&#30340;PubMed&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;Longformer Encoder-Decoder (LED)&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#21152;&#20837;&#20102;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#65288;JAENS&#65289;&#25216;&#26415;&#65292;&#20197;&#26368;&#23567;&#21270;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#19979;&#25351;&#26631;&#26469;&#34913;&#37327;&#23454;&#20307;&#32423;&#21035;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65306;&#28304;&#31934;&#30830;&#24230;&#21644;&#30446;&#26631;F1&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LED&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#25968;&#25454;&#36807;&#28388;&#25216;&#26415;&#22522;&#20110;&#19968;&#20123;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination in text summarization refers to the phenomenon where the model generates information that is not supported by the input source document. Hallucination poses significant obstacles to the accuracy and reliability of the generated summaries. In this paper, we aim to reduce hallucinated outputs or hallucinations in summaries of long-form text documents. We have used the PubMed dataset, which contains long scientific research documents and their abstracts. We have incorporated the techniques of data filtering and joint entity and summary generation (JAENS) in the fine-tuning of the Longformer Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the quality of the generated summary. We have used the following metrics to measure factual consistency at the entity level: precision-source, and F1-target. Our experiments show that the fine-tuned LED model performs well in generating the paper abstract. Data filtering techniques based on some preprocessing steps
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.15730</link><description>&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Temporal graph models fail to capture global temporal dynamics. (arXiv:2309.15730v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15730
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#38142;&#25509;&#23646;&#24615;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#26102;&#38388;&#22270;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20004;&#20010;&#24230;&#37327;&#65292;&#21487;&#20197;&#37327;&#21270;&#25968;&#25454;&#38598;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20840;&#23616;&#21160;&#24577;&#30340;&#24378;&#24230;&#12290;&#36890;&#36807;&#20998;&#26512;&#25105;&#20204;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#22522;&#32447;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#65292;&#23548;&#33268;&#26080;&#27861;&#23545;&#26102;&#38388;&#22270;&#32593;&#32476;&#36827;&#34892;&#25490;&#24207;&#30340;&#39044;&#27979;&#23436;&#20840;&#39281;&#21644;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on all medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26032;&#39046;&#22495;&#20013;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#65292;&#20943;&#23569;&#25110;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#36741;&#21161;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01188</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#21487;&#36801;&#31227;&#30340;&#38646;&#26679;&#26412;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Neural Recommenders: A Transferable Zero-Shot Framework for Recommendation Systems. (arXiv:2309.01188v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26032;&#39046;&#22495;&#20013;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#65292;&#20943;&#23569;&#25110;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#36741;&#21161;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#25216;&#26415;&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20869;&#23481;&#20849;&#20139;&#24179;&#21488;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25216;&#26415;&#26377;&#25152;&#36827;&#27493;&#65292;&#20294;&#23545;&#20110;&#27599;&#20010;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#20173;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;NCF&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65292;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#24773;&#20917;&#65292;&#35201;&#20040;&#26159;&#26377;&#38480;&#30340;&#24494;&#35843;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#26032;&#39046;&#22495;&#20013;&#25903;&#25345;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#21482;&#38656;&#26368;&#23569;&#25110;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#36741;&#21161;&#29992;&#25143;&#25110;&#39033;&#30446;&#20449;&#24687;&#12290;&#38646;&#26679;&#26412;&#25512;&#33616;&#22312;&#27809;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24403;&#27809;&#26377;&#37325;&#21472;&#30340;&#29992;&#25143;&#25110;&#39033;&#30446;&#26102;&#65292;&#25105;&#20204;&#26080;&#27861;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#24314;&#31435;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#35265;&#35299;&#26159;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#30340;&#32479;&#35745;&#29305;&#24449;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26222;&#36941;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural collaborative filtering techniques are critical to the success of e-commerce, social media, and content-sharing platforms. However, despite technical advances -- for every new application domain, we need to train an NCF model from scratch. In contrast, pre-trained vision and language models are routinely applied to diverse applications directly (zero-shot) or with limited fine-tuning. Inspired by the impact of pre-trained models, we explore the possibility of pre-trained recommender models that support building recommender systems in new domains, with minimal or no retraining, without the use of any auxiliary user or item information. Zero-shot recommendation without auxiliary information is challenging because we cannot form associations between users and items across datasets when there are no overlapping users or items. Our fundamental insight is that the statistical characteristics of the user-item interaction matrix are universally available across different domains 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#28789;&#27963;&#30340;DNA&#23384;&#20648;&#26550;&#26500;&#65292;&#21487;&#20197;&#29420;&#31435;&#19988;&#39640;&#25928;&#22320;&#35775;&#38382;&#21644;&#26356;&#26032;&#23384;&#20648;&#31354;&#38388;&#20013;&#30340;&#22359;&#25968;&#25454;&#65292;&#21516;&#26102;&#20801;&#35768;&#23545;&#36830;&#32493;&#25968;&#25454;&#22359;&#36827;&#34892;&#39034;&#24207;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2212.13447</link><description>&lt;p&gt;
&#39640;&#25928;&#23454;&#29616;DNA&#23384;&#20648;&#20013;&#30340;&#22359;&#35821;&#20041;&#21644;&#25968;&#25454;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Efficiently Enabling Block Semantics and Data Updates in DNA Storage. (arXiv:2212.13447v2 [cs.ET] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#28789;&#27963;&#30340;DNA&#23384;&#20648;&#26550;&#26500;&#65292;&#21487;&#20197;&#29420;&#31435;&#19988;&#39640;&#25928;&#22320;&#35775;&#38382;&#21644;&#26356;&#26032;&#23384;&#20648;&#31354;&#38388;&#20013;&#30340;&#22359;&#25968;&#25454;&#65292;&#21516;&#26102;&#20801;&#35768;&#23545;&#36830;&#32493;&#25968;&#25454;&#22359;&#36827;&#34892;&#39034;&#24207;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#28789;&#27963;&#30340;DNA&#23384;&#20648;&#26550;&#26500;&#65292;&#23558;&#23384;&#20648;&#31354;&#38388;&#20998;&#21106;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#21333;&#20301;&#65288;&#22359;&#65289;&#65292;&#21487;&#20197;&#29420;&#31435;&#19988;&#39640;&#25928;&#22320;&#38543;&#26426;&#35775;&#38382;&#36827;&#34892;&#35835;&#20889;&#25805;&#20316;&#65292;&#24182;&#19988;&#36824;&#20801;&#35768;&#23545;&#36830;&#32493;&#25968;&#25454;&#22359;&#36827;&#34892;&#26377;&#25928;&#30340;&#39034;&#24207;&#35775;&#38382;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#65292;&#22312;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#65292;&#38271;&#24230;&#20026;20&#30340;&#38543;&#26426;&#35775;&#38382;PCR&#24341;&#29289;&#24182;&#19981;&#23450;&#20041;&#19968;&#20010;&#21333;&#29420;&#30340;&#23545;&#35937;&#65292;&#32780;&#26159;&#23450;&#20041;&#19968;&#20010;&#29420;&#31435;&#30340;&#23384;&#20648;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#22312;&#20869;&#37096;&#36827;&#34892;&#22359;&#21010;&#20998;&#21644;&#29420;&#31435;&#31649;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#20010;&#20998;&#21306;&#30340;&#20869;&#37096;&#22320;&#22336;&#31354;&#38388;&#30340;&#28789;&#27963;&#24615;&#21644;&#32422;&#26463;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#20379;&#20016;&#23500;&#21644;&#21151;&#33021;&#40784;&#20840;&#30340;&#23384;&#20648;&#35821;&#20041;&#65292;&#22914;&#22359;&#23384;&#20648;&#32452;&#32455;&#12289;&#39640;&#25928;&#30340;&#25968;&#25454;&#26356;&#26032;&#23454;&#29616;&#21644;&#39034;&#24207;&#35775;&#38382;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;PCR&#23547;&#22336;&#30340;&#21069;&#32512;&#29305;&#24615;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36716;&#25442;&#20998;&#21306;&#30340;&#20869;&#37096;&#23547;&#22336;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel and flexible DNA-storage architecture, which divides the storage space into fixed-size units (blocks) that can be independently and efficiently accessed at random for both read and write operations, and further allows efficient sequential access to consecutive data blocks. In contrast to prior work, in our architecture a pair of random-access PCR primers of length 20 does not define a single object, but an independent storage partition, which is internally blocked and managed independently of other partitions. We expose the flexibility and constraints with which the internal address space of each partition can be managed, and incorporate them into our design to provide rich and functional storage semantics, such as block-storage organization, efficient implementation of data updates, and sequential access. To leverage the full power of the prefix-based nature of PCR addressing, we define a methodology for transforming the internal addressing scheme of a partition int
&lt;/p&gt;</description></item></channel></rss>