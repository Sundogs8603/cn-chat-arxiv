<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#25628;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#29702;&#35299;&#25972;&#20010;&#31508;&#35760;&#26412;&#20869;&#23481;&#35821;&#20041;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13234</link><description>&lt;p&gt;
&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#35299;&#38145;&#27934;&#35265;&#65306;&#35821;&#20041;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unlocking Insights: Semantic Search in Jupyter Notebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#25628;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#29702;&#35299;&#25972;&#20010;&#31508;&#35760;&#26412;&#20869;&#23481;&#35821;&#20041;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25628;&#32034;&#26088;&#22312;&#36890;&#36807;&#29702;&#35299;&#25628;&#32034;&#32773;&#30340;&#24847;&#22270;&#21644;&#21487;&#25628;&#32034;&#25968;&#25454;&#31354;&#38388;&#20013;&#26415;&#35821;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#65292;&#25552;&#20379;&#39640;&#24230;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#25628;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;Jupyter&#31508;&#35760;&#26412;&#39046;&#22495;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#32034;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#22914;&#22270;&#34920;&#12289;&#20851;&#32852;&#20989;&#25968;&#21644;&#26041;&#27861;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21487;&#20197;&#20840;&#38754;&#29702;&#35299;&#25972;&#20010;&#31508;&#35760;&#26412;&#20869;&#23481;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65306;1&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;Jupyter&#31508;&#35760;&#26412;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#21333;&#20803;&#26684;&#65292;&#21253;&#25324;Markdown&#21644;&#20195;&#30721;&#21333;&#20803;&#26684;&#12290;2&#65289;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13234v1 Announce Type: cross  Abstract: Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval. In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information.   We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries. Key components of this framework include:   1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#24577;&#20272;&#35745;&#20013;&#21033;&#29992;&#37096;&#20998;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#32534;&#30721;&#23454;&#29616;&#26368;&#20248;&#20449;&#24687;&#33719;&#21462;&#65292;&#24320;&#21457;&#20102;&#31895;&#31961;&#30340;&#20805;&#20998;&#32479;&#35745;&#29992;&#20110;&#27169;&#24577;&#35782;&#21035;&#65292;&#24182;&#23558;&#36172;&#21338;&#31639;&#27861;&#35843;&#25972;&#20026;&#26032;&#35774;&#32622;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2402.13079</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21453;&#39304;&#30340;&#27169;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mode Estimation with Partial Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#24577;&#20272;&#35745;&#20013;&#21033;&#29992;&#37096;&#20998;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#32534;&#30721;&#23454;&#29616;&#26368;&#20248;&#20449;&#24687;&#33719;&#21462;&#65292;&#24320;&#21457;&#20102;&#31895;&#31961;&#30340;&#20805;&#20998;&#32479;&#35745;&#29992;&#20110;&#27169;&#24577;&#35782;&#21035;&#65292;&#24182;&#23558;&#36172;&#21338;&#31639;&#27861;&#35843;&#25972;&#20026;&#26032;&#35774;&#32622;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13079v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#65292;&#36731;&#24230;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#32452;&#21512;&#22312;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#26032;&#30340;&#23398;&#20064;&#27969;&#31243;&#38656;&#35201;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#38382;&#39064;&#65292;&#24418;&#24335;&#21270;&#24369;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26680;&#24515;&#26041;&#38754;&#65306;&#20351;&#29992;&#37096;&#20998;&#21453;&#39304;&#20272;&#35745;&#20998;&#24067;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29109;&#32534;&#30721;&#20174;&#37096;&#20998;&#21453;&#39304;&#20013;&#23454;&#29616;&#26368;&#20248;&#20449;&#24687;&#33719;&#21462;&#65292;&#20026;&#27169;&#24577;&#35782;&#21035;&#24320;&#21457;&#20102;&#31895;&#31961;&#30340;&#20805;&#20998;&#32479;&#35745;&#65292;&#24182;&#23558;&#36172;&#21338;&#31639;&#27861;&#35843;&#25972;&#20026;&#25105;&#20204;&#30340;&#26032;&#35774;&#32622;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36129;&#29486;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32479;&#35745;&#19978;&#21644;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13079v1 Announce Type: cross  Abstract: The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13033</link><description>&lt;p&gt;
&#29992;&#36229;&#36793;&#22686;&#24378;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Real-World Complex Network Representations with Hyperedge Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22270;&#22686;&#24378;&#26041;&#27861;&#22312;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#25200;&#21160;&#22270;&#32467;&#26500;&#65292;&#36890;&#24120;&#38480;&#20110;&#25104;&#23545;&#33410;&#28857;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25104;&#23545;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#20110;&#24418;&#25104;&#39640;&#38454;&#36793;&#30340;&#25968;&#25454;&#65292;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#20027;&#35201;&#34987;&#24314;&#27169;&#20026;&#31616;&#21333;&#22270;&#12290;&#22240;&#27492;&#65292;&#23558;&#39640;&#38454;&#36793;&#37325;&#26032;&#37197;&#32622;&#20026;&#22270;&#22686;&#24378;&#31574;&#30053;&#30340;&#19968;&#37096;&#20998;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36335;&#24452;&#65292;&#21487;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36793;&#22686;&#24378;&#65288;HyperAug&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#26500;&#24314;&#34394;&#25311;&#36229;&#36793;&#65292;&#24182;&#20135;&#29983;&#36741;&#21161;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 Announce Type: new  Abstract: Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary nod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#22270;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#65288;DR-GNN&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#65288;DRO&#65289;&#34701;&#20837;&#21040;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#20013;&#65292;&#35299;&#20915;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#36896;&#25104;&#30340;&#25928;&#26524;&#19979;&#38477;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.12994</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#22270;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Graph-based Recommendation System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12994
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#22270;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#65288;DR-GNN&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#65288;DRO&#65289;&#34701;&#20837;&#21040;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#20013;&#65292;&#35299;&#20915;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#36896;&#25104;&#30340;&#25928;&#26524;&#19979;&#38477;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25429;&#25417;&#39640;&#38454;&#21327;&#20316;&#20449;&#21495;&#33021;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20013;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#21462;&#20915;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#65288;&#21363;IID&#20551;&#35774;&#65289;&#65292;&#24182;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#20986;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#20998;&#24067;&#36716;&#31227;&#22312;RS&#20013;&#24120;&#35265;&#65292;&#36890;&#24120;&#24402;&#22240;&#20110;&#29992;&#25143;&#21916;&#22909;&#30340;&#21160;&#24577;&#24615;&#25110;RS&#20013;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#30340;&#26222;&#36941;&#20559;&#35265;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#38024;&#23545;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#23545;&#25239;&#20998;&#24067;&#36716;&#31227;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#65288;DRO&#65289;&#24341;&#20837;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;GNN&#65288;DR-GNN&#65289;&#12290;DR-GNN&#35299;&#20915;&#20102;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;1&#65289;&#20351;DRO&#33021;&#22815;&#36866;&#24212;&#19982;GNN&#20132;&#32455;&#30340;&#22270;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;GNN&#37325;&#26032;&#35299;&#37322;&#20026;&#22270;&#24179;&#28369;&#27491;&#21017;&#21270;&#22120;&#65292;&#20174;&#32780;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12994v1 Announce Type: new  Abstract: With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitatin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.12993</link><description>&lt;p&gt;
&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#25968;&#25454;&#25366;&#25496;&#30340;&#33258;&#20027;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Large Language Model Agent for Chemical Literature Data Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12993
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21512;&#25104;&#23545;&#20110;&#25512;&#21160;&#26448;&#26009;&#21512;&#25104;&#21644;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#24433;&#21709;&#30528;&#21253;&#25324;&#29615;&#22659;&#31185;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#21270;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#19978;&#21319;&#20351;&#24471;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21270;&#23398;&#25968;&#25454;&#65292;&#25361;&#25112;&#30740;&#31350;&#20154;&#21592;&#21435;&#35782;&#21035;&#27169;&#24335;&#24182;&#32454;&#21270;&#21512;&#25104;&#36807;&#31243;&#12290;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#26469;&#20248;&#21270;&#21512;&#25104;&#24182;&#25552;&#39640;&#20135;&#37327;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#25991;&#29486;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21270;&#23398;&#25991;&#29486;&#30340;&#32467;&#26500;&#19981;&#35268;&#25972;&#65292;&#20889;&#20316;&#39118;&#26684;&#22810;&#26679;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#24191;&#27867;&#30340;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24555;&#36895;&#29983;&#25104;&#21644;&#36845;&#20195;&#20248;&#21270;&#12290;&#23427;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#33410;&#30465;&#20154;&#21147;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12993v1 Announce Type: cross  Abstract: Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#65292;&#36890;&#36807;&#23545;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#26435;&#34913;&#20851;&#38190;&#20803;&#32032;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12784</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#32531;&#35299;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#65292;&#36890;&#36807;&#23545;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#26435;&#34913;&#20851;&#38190;&#20803;&#32032;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Vec2Text&#25216;&#26415;&#65292;&#19968;&#31181;&#29992;&#20110;&#21453;&#36716;&#25991;&#26412;&#23884;&#20837;&#30340;&#25216;&#26415;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#20005;&#37325;&#38544;&#31169;&#38382;&#39064;&#30340;&#25285;&#24551;&#65292;&#21253;&#25324;&#37027;&#20123;&#20351;&#29992;OpenAI&#21644;Cohere&#25552;&#20379;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#31995;&#32479;&#12290;&#36825;&#31181;&#23041;&#32961;&#26469;&#33258;&#20110;&#19968;&#20010;&#24694;&#24847;&#25915;&#20987;&#32773;&#36890;&#36807;&#35775;&#38382;&#25991;&#26412;&#23884;&#20837;&#26469;&#37325;&#26500;&#21407;&#22987;&#25991;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20351;&#29992;Vec2Text&#24674;&#22797;&#25991;&#26412;&#30340;&#23884;&#20837;&#27169;&#22411;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#28041;&#21450;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#12289;&#21152;&#22122;&#22768;&#35757;&#32451;&#12289;&#23884;&#20837;&#37327;&#21270;&#21644;&#23884;&#20837;&#32500;&#24230;&#31561;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#21407;&#22987;Vec2Text&#35770;&#25991;&#20013;&#23578;&#26410;&#34987;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#24433;&#21709;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#20043;&#38388;&#26435;&#34913;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12784v1 Announce Type: cross  Abstract: The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text.   In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems. This a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CONVINV&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#37325;&#20889;&#23558;&#19981;&#36879;&#26126;&#30340;&#23545;&#35805;&#24335;&#20250;&#35805;&#23884;&#20837;&#36716;&#25442;&#20026;&#26126;&#30830;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12774</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#30340;&#37325;&#20889;&#26469;&#35299;&#37322;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#30340;&#20250;&#35805;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CONVINV&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#37325;&#20889;&#23558;&#19981;&#36879;&#26126;&#30340;&#23545;&#35805;&#24335;&#20250;&#35805;&#23884;&#20837;&#36716;&#25442;&#20026;&#26126;&#30830;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#24050;&#34987;&#35777;&#26126;&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#30340;&#19968;&#20010;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#30452;&#35266;&#29702;&#35299;&#20197;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CONVINV&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25581;&#31034;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#12290;CONVINV&#23558;&#19981;&#36879;&#26126;&#30340;&#23545;&#35805;&#24335;&#20250;&#35805;&#23884;&#20837;&#36716;&#25442;&#20026;&#26126;&#30830;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#24544;&#23454;&#22320;&#20445;&#25345;&#20854;&#21407;&#22987;&#26816;&#32034;&#24615;&#33021;&#12290;&#36825;&#31181;&#36716;&#25442;&#26159;&#36890;&#36807;&#35757;&#32451;&#19968;&#31181;&#22522;&#20110;&#19987;&#38376;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;Vec2Text&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#21033;&#29992;&#20102;&#20250;&#35805;&#21644;&#26597;&#35810;&#23884;&#20837;&#22312;&#29616;&#26377;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#20013;&#20849;&#20139;&#30456;&#21516;&#31354;&#38388;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22806;&#37096;&#21487;&#35299;&#37322;&#30340;&#26597;&#35810;&#37325;&#20889;&#32435;&#20837;&#36716;&#25442;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12774v1 Announce Type: new  Abstract: Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational dense retrieval models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval. To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;BMLP&#65292;&#36890;&#36807;&#34892;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;&#36141;&#20080;&#24847;&#22270;&#24863;&#30693;&#27169;&#22359;&#25429;&#25417;&#29992;&#25143;&#30340;&#24322;&#26500;&#20852;&#36259;&#21644;&#36141;&#20080;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12733</link><description>&lt;p&gt;
BMLP&#65306;&#29992;&#20110;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#30340;&#34892;&#20026;&#24863;&#30693;MLP
&lt;/p&gt;
&lt;p&gt;
BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;BMLP&#65292;&#36890;&#36807;&#34892;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;&#36141;&#20080;&#24847;&#22270;&#24863;&#30693;&#27169;&#22359;&#25429;&#25417;&#29992;&#25143;&#30340;&#24322;&#26500;&#20852;&#36259;&#21644;&#36141;&#20080;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#30340;&#34892;&#20026;&#65292;&#27604;&#22914;&#28857;&#20987;&#21644;&#36141;&#20080;&#12290;&#29616;&#26377;&#30740;&#31350;&#26041;&#27861;&#34920;&#26126;&#65292;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#34892;&#20026;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#30340;&#24322;&#26500;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22810;&#34892;&#20026;&#26041;&#27861;&#22312;&#23398;&#20064;&#19981;&#21516;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#65292;&#21363;&#34892;&#20026;&#24863;&#30693;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;BMLP&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#65292;&#21363;&#24322;&#26500;&#20852;&#36259;&#24863;&#30693;&#65288;HIP&#65289;&#27169;&#22359;&#65292;&#36890;&#36807;&#34892;&#20026;&#31867;&#22411;&#21644;&#36716;&#25442;&#20851;&#31995;&#22312;&#22810;&#20010;&#31890;&#24230;&#19978;&#24314;&#27169;&#34892;&#20026;&#65292;&#24182;&#36141;&#20080;&#24847;&#22270;&#24863;&#30693;&#65288;PIP&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#36741;&#21161;&#34892;&#20026;&#30340;&#23376;&#24207;&#21015;&#20197;&#25429;&#25417;&#29992;&#25143;&#30340;&#36141;&#20080;&#24847;&#22270;&#12290;&#19982;&#20027;&#27969;&#24207;&#21015;&#27169;&#22411;&#30456;&#27604;&#65292;MLP&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12733v1 Announce Type: cross  Abstract: In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;&#39046;&#22495;&#22806;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.12663</link><description>&lt;p&gt;
SoftQE: LLM&#25193;&#23637;&#30340;&#26597;&#35810;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SoftQE: Learned Representations of Queries Expanded by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12663
&lt;/p&gt;
&lt;p&gt;
SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;&#39046;&#22495;&#22806;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#26597;&#35810;&#32534;&#30721;&#22120;&#20013;&#65292;&#20197;&#25913;&#21892;&#23494;&#38598;&#26816;&#32034;&#65292;&#21516;&#26102;&#36991;&#20813;&#22312;&#25512;&#26029;&#26102;&#20381;&#36182;LLMs&#22686;&#21152;&#24310;&#36831;&#21644;&#25104;&#26412;&#12290;SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#26469;&#25972;&#21512;LLMs&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#23545;&#20110;&#39046;&#22495;&#20869;MS-MARCO&#25351;&#26631;&#65292;SoftQE&#30456;&#23545;&#20110;&#21508;&#31181;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#25913;&#21892;&#26377;&#38480;&#65292;&#20294;&#22312;&#20116;&#20010;&#39046;&#22495;&#22806;BEIR&#20219;&#21153;&#19978;&#65292;SoftQE&#22312;&#24179;&#22343;&#24615;&#33021;&#19978;&#25552;&#39640;&#20102;2.83&#20010;&#32477;&#23545;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12663v1 Announce Type: new  Abstract: We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Opposite Gender Interaction Ratio (OGIR)&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#32447;&#32422;&#20250;&#25512;&#33616;&#20013;&#38024;&#23545;&#20855;&#26377;&#19981;&#21516;&#24322;&#24615;&#20559;&#22909;&#30340;&#29992;&#25143;&#28508;&#22312;&#19981;&#20844;&#24179;&#24615;&#65307;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;&#25512;&#33616;&#31639;&#27861;&#21487;&#33021;&#26681;&#25454;OGIR&#23384;&#22312;&#20998;&#32452;&#19981;&#20844;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.12541</link><description>&lt;p&gt;
&#21033;&#29992;&#30456;&#21453;&#24615;&#21035;&#20114;&#21160;&#27604;&#20316;&#20026;&#22522;&#20110;&#29992;&#25143;&#24615;&#21462;&#21521;&#30340;&#22312;&#32447;&#32422;&#20250;&#25512;&#33616;&#20013;&#20844;&#24179;&#24615;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12541
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Opposite Gender Interaction Ratio (OGIR)&#65292;&#29992;&#20110;&#30740;&#31350;&#22312;&#32447;&#32422;&#20250;&#25512;&#33616;&#20013;&#38024;&#23545;&#20855;&#26377;&#19981;&#21516;&#24322;&#24615;&#20559;&#22909;&#30340;&#29992;&#25143;&#28508;&#22312;&#19981;&#20844;&#24179;&#24615;&#65307;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;&#25512;&#33616;&#31639;&#27861;&#21487;&#33021;&#26681;&#25454;OGIR&#23384;&#22312;&#20998;&#32452;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32422;&#20250;&#24179;&#21488;&#20316;&#20026;&#20010;&#20154;&#23547;&#25214;&#28508;&#22312;&#24651;&#29233;&#20851;&#31995;&#30340;&#25163;&#27573;&#24050;&#32463;&#24191;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#26469;&#25913;&#21892;&#32422;&#20250;&#24179;&#21488;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#20294;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#22686;&#21152;&#20851;&#27880;&#20419;&#20351;&#20174;&#21508;&#31181;&#35282;&#24230;&#65288;&#20363;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65289;&#24320;&#21457;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#20851;&#31995;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#30340;&#24615;&#21462;&#21521;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#30456;&#21453;&#24615;&#21035;&#20114;&#21160;&#27604;&#65288;OGIR&#65289;&#65292;&#20316;&#20026;&#30740;&#31350;&#20855;&#26377;&#19981;&#21516;&#23545;&#24322;&#24615;&#20559;&#22909;&#30340;&#29992;&#25143;&#28508;&#22312;&#19981;&#20844;&#24179;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#22312;&#32447;&#32422;&#20250;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#25512;&#33616;&#31639;&#27861;&#26681;&#25454;OGIR&#21487;&#33021;&#20250;&#21463;&#21040;&#20998;&#32452;&#19981;&#20844;&#24179;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;&#36896;&#25104;&#36825;&#31181;&#19981;&#20844;&#24179;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12541v1 Announce Type: new  Abstract: Online dating platforms have gained widespread popularity as a means for individuals to seek potential romantic relationships. While recommender systems have been designed to improve the user experience in dating platforms by providing personalized recommendations, increasing concerns about fairness have encouraged the development of fairness-aware recommender systems from various perspectives (e.g., gender and race). However, sexual orientation, which plays a significant role in finding a satisfying relationship, is under-investigated. To fill this crucial gap, we propose a novel metric, Opposite Gender Interaction Ratio (OGIR), as a way to investigate potential unfairness for users with varying preferences towards the opposite gender. We empirically analyze a real online dating dataset and observe existing recommender algorithms could suffer from group unfairness according to OGIR. We further investigate the potential causes for such g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65288;SECP&#65289;&#65292;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#24178;&#20928;&#35821;&#38899;&#24182;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#36718;&#36845;&#20195;&#35266;&#23519;&#21040;&#22686;&#24378;&#36755;&#20986;&#20316;&#20026;&#22522;&#20934;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#27979;&#35797;&#35777;&#26126;&#20248;&#21270;&#25968;&#25454;&#22312;&#24863;&#30693;&#19978;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.12482</link><description>&lt;p&gt;
SECP&#65306;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#33719;&#21462;&#24178;&#20928;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65288;SECP&#65289;&#65292;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#24178;&#20928;&#35821;&#38899;&#24182;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#36718;&#36845;&#20195;&#35266;&#23519;&#21040;&#22686;&#24378;&#36755;&#20986;&#20316;&#20026;&#22522;&#20934;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#27979;&#35797;&#35777;&#26126;&#20248;&#21270;&#25968;&#25454;&#22312;&#24863;&#30693;&#19978;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#35821;&#38899;&#25216;&#26415;&#20381;&#36182;&#20110;&#20197;&#24178;&#20928;&#35821;&#38899;&#20026;&#22522;&#20934;&#30340;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#35268;&#27169;&#19978;&#25509;&#20837;&#36825;&#20123;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#23545;&#20154;&#31867;&#21548;&#35273;&#21644;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#21482;&#22312;&#38656;&#35201;&#26102;&#38656;&#35201;&#20154;&#31867;&#20171;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#27010;&#36848;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65288;SECP&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#29992;&#26469;&#25509;&#20837;&#24178;&#20928;&#35821;&#38899;&#12290;&#36825;&#20123;&#24178;&#20928;&#35821;&#38899;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20851;&#38381;&#36845;&#20195;&#24490;&#29615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#36718;&#36845;&#20195;&#30340;&#23454;&#39564;&#35266;&#23519;&#21040;&#65292;&#20316;&#20026;&#22522;&#20934;&#30340;&#22686;&#24378;&#36755;&#20986;&#19981;&#20250;&#20351;&#27169;&#22411;&#24615;&#33021;&#26681;&#25454;&#26412;&#25991;&#20351;&#29992;&#30340; $\Delta_{PESQ}$ &#25351;&#26631;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22522;&#20110;&#27604;&#36739;&#22343;&#20540;&#24847;&#35265;&#35780;&#20998;&#65288;CMOS&#65289;&#30340;&#20027;&#35266;&#27979;&#35797;&#34920;&#26126;&#65292;&#20248;&#21270;&#25968;&#25454;&#30340;&#26368;&#39640;&#21644;&#26368;&#20302;&#36793;&#30028;&#22312;&#24863;&#30693;&#19978;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12482v1 Announce Type: cross  Abstract: As more speech technologies rely on a supervised deep learning approach with clean speech as the ground truth, a methodology to onboard said speech at scale is needed. However, this approach needs to minimize the dependency on human listening and annotation, only requiring a human-in-the-loop when needed. In this paper, we address this issue by outlining Speech Enhancement-based Curation Pipeline (SECP) which serves as a framework to onboard clean speech. This clean speech can then train a speech enhancement model, which can further refine the original dataset and thus close the iterative loop. By running two iterative rounds, we observe that enhanced output used as ground truth does not degrade model performance according to $\Delta_{PESQ}$, a metric used in this paper. We also show through comparative mean opinion score (CMOS) based subjective tests that the highest and lowest bound of refined data is perceptually better than the ori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20302;&#21516;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Social Heterophily-alleviating Rewiring (SHaRe)&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#25429;&#25417;&#39640;&#21516;&#36136;&#30340;&#31038;&#20132;&#20851;&#31995;&#24182;&#21098;&#20999;&#20302;&#21516;&#36136;&#20851;&#31995;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#25552;&#21462;&#20102;&#20559;&#22909;&#24863;&#30693;&#21516;&#36136;&#24615;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14606</link><description>&lt;p&gt;
&#25361;&#25112;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20302;&#21516;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Challenging Low Homophily in Social Recommendation. (arXiv:2401.14606v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20302;&#21516;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Social Heterophily-alleviating Rewiring (SHaRe)&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#25429;&#25417;&#39640;&#21516;&#36136;&#30340;&#31038;&#20132;&#20851;&#31995;&#24182;&#21098;&#20999;&#20302;&#21516;&#36136;&#20851;&#31995;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#25552;&#21462;&#20102;&#20559;&#22909;&#24863;&#30693;&#21516;&#36136;&#24615;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#25512;&#33616;&#20013;&#65292;&#21033;&#29992;&#31038;&#20132;&#20851;&#31995;&#26469;&#35299;&#20915;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#22522;&#20110;&#31038;&#20132;&#21516;&#36136;&#24615;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#25512;&#33616;&#33539;&#24335;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#21516;&#36136;&#24615;&#12290;&#34429;&#28982;&#31038;&#20132;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#25928;&#26524;&#65292;&#20294;&#23427;&#19982;&#29992;&#25143;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#24182;&#19981;&#20445;&#35777;&#65292;&#20174;&#32780;&#21487;&#33021;&#24341;&#20837;&#20449;&#24687;&#20887;&#20313;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#22270;&#23637;&#29616;&#20986;&#20302;&#20559;&#22909;&#24863;&#30693;&#30340;&#21516;&#36136;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#20840;&#38754;&#25552;&#21462;&#31038;&#20132;&#22270;&#20013;&#28508;&#22312;&#30340;&#20559;&#22909;&#24863;&#30693;&#21516;&#36136;&#24615;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Social Heterophily-alleviating Rewiring (SHaRe)&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#22270;&#37325;&#36830;&#25216;&#26415;&#26469;&#25429;&#25417;&#21644;&#28155;&#21152;&#39640;&#24230;&#21516;&#36136;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#24182;&#21098;&#20999;&#20302;&#21516;&#36136;&#65288;&#25110;&#24322;&#36136;&#65289;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20248;&#21270;&#20174;&#31038;&#20132;&#22270;&#20013;&#25552;&#21462;&#30340;&#25512;&#33616;&#27169;&#24335;&#30340;&#21051;&#30011;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#22522;&#20110;&#22270;&#30340;&#29992;&#25143;&#20559;&#22909;&#20998;&#24067;&#20462;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the u
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item></channel></rss>