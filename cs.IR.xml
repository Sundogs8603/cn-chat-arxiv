<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#25345;&#32493;&#25991;&#26723;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#26816;&#32034;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21160;&#24577;&#26816;&#32034;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16767</link><description>&lt;p&gt;
CorpusBrain++: &#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#32487;&#32493;&#29983;&#25104;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#25345;&#32493;&#25991;&#26723;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#26816;&#32034;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21160;&#24577;&#26816;&#32034;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20174;&#21487;&#20449;&#30340;&#35821;&#26009;&#24211;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#20197;&#29983;&#25104;&#29305;&#23450;&#31572;&#26696;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;CorpusBrain&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26816;&#32034;&#24615;&#33021;&#26368;&#20248;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20851;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;CorpusBrain&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38598;&#20013;&#22312;&#38745;&#24577;&#25991;&#26723;&#38598;&#19978;&#65292;&#24573;&#35270;&#20102;&#29616;&#23454;&#22330;&#26223;&#30340;&#21160;&#24577;&#24615;&#36136;&#65292;&#20854;&#20013;&#26032;&#25991;&#26723;&#25345;&#32493;&#22320;&#34987;&#32435;&#20837;&#28304;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25506;&#32034;&#26816;&#32034;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#21160;&#24577;&#26816;&#32034;&#22330;&#26223;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#25345;&#32493;&#25991;&#26723;&#23398;&#20064;&#65288;CDL&#65289;&#20219;&#21153;&#65292;&#24182;&#22522;&#20110;&#21407;&#22987;KILT&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;KILT++&#30340;&#26032;&#22411;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16767v1 Announce Type: cross  Abstract: Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers. Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance. However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus. To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs.   In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel benchmark dataset named KILT++ based on the original KILT dataset for evaluation. Then, we conduct a comprehensi
&lt;/p&gt;</description></item><item><title>LLM&#36741;&#21161;&#30340;&#22810;&#25945;&#24072;&#25345;&#32493;&#23398;&#20064;&#20026;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#26356;&#26032;&#25552;&#20379;&#20102;&#35299;&#20915;&#26032;&#20219;&#21153;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22806;&#31185;&#39046;&#22495;&#20013;&#30340;&#22823;&#39046;&#22495;&#36716;&#21464;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16664</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#30340;&#22810;&#25945;&#24072;&#25345;&#32493;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16664
&lt;/p&gt;
&lt;p&gt;
LLM&#36741;&#21161;&#30340;&#22810;&#25945;&#24072;&#25345;&#32493;&#23398;&#20064;&#20026;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#26356;&#26032;&#25552;&#20379;&#20102;&#35299;&#20915;&#26032;&#20219;&#21153;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22806;&#31185;&#39046;&#22495;&#20013;&#30340;&#22823;&#39046;&#22495;&#36716;&#21464;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#22312;&#20419;&#36827;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#25945;&#32946;&#26041;&#38754;&#21487;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23398;&#21592;&#30340;&#38656;&#27714;&#19981;&#26029;&#21457;&#23637;&#65292;&#27604;&#22914;&#23398;&#20064;&#26356;&#22810;&#31181;&#31867;&#30340;&#25163;&#26415;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#21450;&#20026;&#19968;&#31181;&#25163;&#26415;&#23398;&#20064;&#26032;&#30340;&#22806;&#31185;&#22120;&#26800;&#21644;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#38656;&#35201;&#36890;&#36807;&#22810;&#20010;&#36164;&#28304;&#30340;&#39034;&#24207;&#25968;&#25454;&#27969;&#25345;&#32493;&#26356;&#26032;VQA&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#22312;&#22806;&#31185;&#22330;&#26223;&#20013;&#65292;&#23384;&#20648;&#25104;&#26412;&#21644;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#36890;&#24120;&#38480;&#21046;&#20102;&#22312;&#26356;&#26032;&#27169;&#22411;&#26102;&#26087;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#26080;&#26679;&#26412;&#30340;&#25345;&#32493;&#23398;&#20064;(CL)&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#22806;&#31185;&#39046;&#22495;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;i)&#26469;&#33258;&#19981;&#21516;&#31185;&#23460;&#25110;&#20020;&#24202;&#20013;&#24515;&#25910;&#38598;&#30340;&#21508;&#31181;&#22806;&#31185;&#25163;&#26415;&#30340;&#22823;&#39046;&#22495;&#36716;&#21464;&#65292;ii)&#30001;&#20110;&#22806;&#31185;&#22120;&#26800;&#25110;&#27963;&#21160;&#30340;&#19981;&#22343;&#21248;&#20986;&#29616;&#32780;&#23548;&#33268;&#30340;&#20005;&#37325;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16664v1 Announce Type: new  Abstract: Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types, adapting to different robots, and learning new surgical instruments and techniques for one surgery. Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the storage cost and patient data privacy often restrict the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities du
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;BOXREC&#30340;&#30418;&#23376;&#25512;&#33616;&#26694;&#26550;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#29992;&#25143;&#23545;&#21333;&#21697;&#20215;&#26684;&#33539;&#22260;&#30340;&#20559;&#22909;&#21644;&#25972;&#20307;&#36141;&#29289;&#39044;&#31639;&#65292;&#20197;&#29983;&#25104;&#19968;&#32452;&#31526;&#21512;&#29992;&#25143;&#21916;&#22909;&#30340;&#26381;&#35013;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.16660</link><description>&lt;p&gt;
BOXREC&#65306;&#22312;&#32447;&#36141;&#29289;&#20013;&#25512;&#33616;&#19968;&#32452;&#21916;&#22909;&#30340;&#26381;&#35013;
&lt;/p&gt;
&lt;p&gt;
BOXREC: Recommending a Box of Preferred Outfits in Online Shopping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;BOXREC&#30340;&#30418;&#23376;&#25512;&#33616;&#26694;&#26550;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#29992;&#25143;&#23545;&#21333;&#21697;&#20215;&#26684;&#33539;&#22260;&#30340;&#20559;&#22909;&#21644;&#25972;&#20307;&#36141;&#29289;&#39044;&#31639;&#65292;&#20197;&#29983;&#25104;&#19968;&#32452;&#31526;&#21512;&#29992;&#25143;&#21916;&#22909;&#30340;&#26381;&#35013;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#26381;&#35013;&#25645;&#37197;&#30340;&#33258;&#21160;&#21270;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26381;&#35013;&#25512;&#33616;&#31995;&#32479;&#20391;&#37325;&#20110;&#39044;&#27979;&#20004;&#20004;&#29289;&#21697;&#30340;&#20860;&#23481;&#24615;&#65288;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65289;&#26469;&#35780;&#20998;&#19968;&#20010;&#21253;&#21547;&#22810;&#20214;&#29289;&#21697;&#30340;&#26381;&#35013;&#32452;&#21512;&#65292;&#28982;&#21518;&#25512;&#33616;&#21069;n&#22871;&#26381;&#35013;&#25110;&#19968;&#22871;&#22522;&#20110;&#29992;&#25143;&#26102;&#23578;&#21697;&#21619;&#30340;&#26381;&#35013;&#33014;&#22218;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#37117;&#27809;&#26377;&#32771;&#34385;&#29992;&#25143;&#23545;&#21333;&#20010;&#26381;&#35013;&#31867;&#22411;&#30340;&#20215;&#26684;&#33539;&#22260;&#20559;&#22909;&#25110;&#19968;&#32452;&#29289;&#21697;&#30340;&#25972;&#20307;&#36141;&#29289;&#39044;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30418;&#23376;&#25512;&#33616;&#26694;&#26550; - BOXREC - &#39318;&#20808;&#25910;&#38598;&#29992;&#25143;&#22312;&#19981;&#21516;&#29289;&#21697;&#31867;&#22411;&#65288;&#21363;&#19978;&#35013;&#12289;&#19979;&#35013;&#21644;&#38795;&#31867;&#65289;&#19978;&#30340;&#20559;&#22909;&#65292;&#21253;&#25324;&#27599;&#31181;&#31867;&#22411;&#30340;&#20215;&#26684;&#33539;&#22260;&#21644;&#29305;&#23450;&#36141;&#29289;&#20250;&#35805;&#30340;&#26368;&#22823;&#36141;&#29289;&#39044;&#31639;&#12290;&#28982;&#21518;&#36890;&#36807;&#20174;&#25152;&#26377;&#31867;&#22411;&#30340;&#20559;&#22909;&#29289;&#21697;&#20013;&#26816;&#32034;&#65292;&#29983;&#25104;&#19968;&#32452;&#21916;&#22909;&#30340;&#26381;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16660v1 Announce Type: new  Abstract: Over the past few years, automation of outfit composition has gained much attention from the research community. Most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user's fashion taste. However, none of these consider user's preference of price-range for individual clothing types or an overall shopping budget for a set of items. In this paper, we propose a box recommendation framework - BOXREC - which at first, collects user preferences across different item types (namely, top-wear, bottom-wear and foot-wear) including price-range of each type and a maximum shopping budget for a particular shopping session. It then generates a set of preferred outfits by retrieving all types of preferred items from the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PAQA&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16608</link><description>&lt;p&gt;
PAQA&#65306;&#38754;&#21521;&#31215;&#26497;&#24320;&#25918;&#22411;&#26816;&#32034;&#38382;&#31572;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PAQA: Toward ProActive Open-Retrieval Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PAQA&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#31995;&#32479;&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#34987;&#21160;&#35282;&#33394;&#65292;&#30446;&#21069;&#20316;&#20026;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#25552;&#20379;&#24102;&#26377;&#25903;&#25345;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#30340;&#26631;&#35760;&#27169;&#26865;&#20004;&#21487;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#27495;&#20041;&#26469;&#35299;&#20915;&#29983;&#25104;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAQA&#65292;&#36825;&#26159;&#29616;&#26377;AmbiNQ&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#21253;&#21547;&#28548;&#28165;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#27573;&#33853;&#26816;&#32034;&#22914;&#20309;&#24433;&#21709;&#27169;&#26865;&#20004;&#21487;&#26816;&#27979;&#21644;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20197;&#22686;&#24378;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16608v1 Announce Type: new  Abstract: Conversational systems have made significant progress in generating natural language responses. However, their potential as conversational search systems is currently limited due to their passive role in the information-seeking process. One major limitation is the scarcity of datasets that provide labelled ambiguous questions along with a supporting corpus of documents and relevant clarifying questions. This work aims to tackle the challenge of generating relevant clarifying questions by taking into account the inherent ambiguities present in both user queries and documents. To achieve this, we propose PAQA, an extension to the existing AmbiNQ dataset, incorporating clarifying questions. We then evaluate various models and assess how passage retrieval impacts ambiguity detection and the generation of clarifying questions. By addressing this gap in conversational search systems, we aim to provide additional supervision to enhance their ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMGR&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.16539</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models with Graphical Session-Based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMGR&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#25506;&#32034;&#65292;&#21033;&#29992;LLMs&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#24320;&#21019;&#24615;&#30340;&#31574;&#30053;&#20027;&#35201;&#26159;&#23558;&#20256;&#32479;&#25512;&#33616;&#20219;&#21153;&#36716;&#21464;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25361;&#25112;&#65292;&#20294;&#22312;&#20250;&#35805;&#25512;&#33616;&#65288;SBR&#65289;&#39046;&#22495;&#30340;&#25506;&#32034;&#30456;&#23545;&#36739;&#23569;&#65292;&#22240;&#20026;&#20854;&#20855;&#20307;&#24615;&#12290;SBR&#20027;&#35201;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#23548;&#65292;&#30001;&#20110;&#20854;&#25429;&#33719;&#30456;&#37051;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#21644;&#26174;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#32467;&#26524;&#12290;&#22270;&#30340;&#32467;&#26500;&#24615;&#36136;&#19982;&#33258;&#28982;&#35821;&#35328;&#30340;&#26412;&#36136;&#24418;&#25104;&#23545;&#27604;&#65292;&#20026;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;LLMGR&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21644;&#35856;&#22320;&#24357;&#21512;&#19978;&#36848;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16539v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16508</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#21512;&#25104;&#30417;&#30563;&#36827;&#34892;&#36328;&#35821;&#35328;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#38382;&#31572;&#65288;CLQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;&#28982;&#21518;&#22312;&#33521;&#35821;&#25110;&#26597;&#35810;&#35821;&#35328;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;CLQA&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38142;&#25509;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26469;&#21512;&#25104;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19968;&#31181;&#22635;&#31354;&#26597;&#35810;&#24418;&#24335;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#26597;&#35810;&#20197;&#30417;&#30563;&#31572;&#26696;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CLASS&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#38646;-shot&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#21487;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;IPC&#21644;IPCCAT API&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#26126;&#32773;&#20860;&#23398;&#26415;&#20316;&#32773;&#30340;&#36523;&#20221;&#21305;&#37197;&#38382;&#39064;&#65292;&#25104;&#21151;&#21305;&#37197;&#20102;&#19987;&#21033;&#19982;&#35770;&#25991;&#65292;&#35299;&#20915;&#20102;&#20316;&#32773;&#36523;&#20221;&#28040;&#27495;&#30340;&#25361;&#25112;&#65292;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#12290;</title><link>https://arxiv.org/abs/2402.16440</link><description>&lt;p&gt;
&#37325;&#25342;&#21457;&#26126;&#32773;-&#20316;&#32773;&#65306;&#35299;&#20915;&#19987;&#21033;&#19982;&#31185;&#23398;&#20986;&#29256;&#29289;&#20043;&#38388;&#20316;&#32773;&#36523;&#20221;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Retrouver l'inventeur-auteur : la lev{\'e}e d'homonymies d'autorat entre les brevets et les publications scientifiques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16440
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;IPC&#21644;IPCCAT API&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#26126;&#32773;&#20860;&#23398;&#26415;&#20316;&#32773;&#30340;&#36523;&#20221;&#21305;&#37197;&#38382;&#39064;&#65292;&#25104;&#21151;&#21305;&#37197;&#20102;&#19987;&#21033;&#19982;&#35770;&#25991;&#65292;&#35299;&#20915;&#20102;&#20316;&#32773;&#36523;&#20221;&#28040;&#27495;&#30340;&#25361;&#25112;&#65292;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#21033;&#21644;&#31185;&#23398;&#35770;&#25991;&#26159;&#34913;&#37327;&#31185;&#23398;&#25216;&#26415;&#20135;&#20986;&#30340;&#37325;&#35201;&#26469;&#28304;&#65292;&#21487;&#20316;&#20026;&#21508;&#31181;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#30340;&#22522;&#30784;&#12290;&#20316;&#32773;&#21644;&#21457;&#26126;&#32773;&#30340;&#22995;&#21517;&#26159;&#36827;&#34892;&#36825;&#20123;&#20998;&#26512;&#30340;&#20851;&#38190;&#26631;&#35782;&#31526;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#36973;&#36935;&#20102;&#28040;&#27495;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#22269;&#38469;&#19987;&#21033;&#20998;&#31867;&#65288;IPC&#65289;&#21644;IPCCAT API&#26469;&#35780;&#20272;&#32473;&#23450;&#21457;&#26126;&#32773;&#30340;&#19987;&#21033;&#21644;&#35770;&#25991;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#65292;&#20197;&#21305;&#37197;&#20004;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20174;&#22269;&#38469;EPO&#25968;&#25454;&#24211;Espacenet&#25552;&#21462;&#30340;&#19977;&#20010;&#19987;&#21033;&#35821;&#26009;&#24211;&#24320;&#21457;&#21644;&#25163;&#21160;&#35780;&#20272;&#12290;&#22312;4679&#31687;&#19987;&#21033;&#21644;7720&#20301;&#21457;&#26126;&#32773;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;2501&#20301;&#20316;&#32773;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#35299;&#20915;&#20102;&#28040;&#27495;&#30340;&#19968;&#33324;&#38382;&#39064;&#65292;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16440v1 Announce Type: new  Abstract: Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes. Authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation. By extension identifying inventors who are also academic authors is a non-trivial challenge. We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents. The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors. The proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#35821;&#38899;&#26448;&#26009;&#30340;&#20869;&#23481;&#23545;&#20351;&#29992;&#20108;&#38454;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28082;&#20307;&#38899;&#21644;&#28369;&#38899;&#12289;&#20803;&#38899;&#65292;&#29305;&#21035;&#26159;&#40763;&#20803;&#38899;&#21644;&#40763;&#36741;&#38899;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#29305;&#21035;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.16429</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#38454;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#35828;&#35805;&#20154;&#35782;&#21035;&#26102;&#35805;&#35821;&#25345;&#32493;&#26102;&#38388;&#21644;&#35821;&#38899;&#20869;&#23481;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of utterance duration and phonetic content on speaker identification using second-order statistical methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#35821;&#38899;&#26448;&#26009;&#30340;&#20869;&#23481;&#23545;&#20351;&#29992;&#20108;&#38454;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28082;&#20307;&#38899;&#21644;&#28369;&#38899;&#12289;&#20803;&#38899;&#65292;&#29305;&#21035;&#26159;&#40763;&#20803;&#38899;&#21644;&#40763;&#36741;&#38899;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#29305;&#21035;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#38454;&#32479;&#35745;&#26041;&#27861;&#22312;&#21463;&#25511;&#24405;&#38899;&#26465;&#20214;&#19979;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#25972;&#20010;&#21487;&#29992;&#30340;&#35821;&#38899;&#26448;&#26009;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27979;&#35797;&#35821;&#38899;&#26448;&#26009;&#30340;&#20869;&#23481;&#23545;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#21363;&#26356;&#22810;&#30340;&#20998;&#26512;&#24615;&#26041;&#27861;&#12290;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#30340;&#20449;&#24687;&#31867;&#22411;&#65292;&#20197;&#21450;&#36825;&#20123;&#20449;&#24687;&#22312;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#20301;&#32622;&#12290;&#28082;&#20307;&#38899;&#21644;&#28369;&#38899;&#12289;&#20803;&#38899;&#65292;&#29305;&#21035;&#26159;&#40763;&#20803;&#38899;&#21644;&#40763;&#36741;&#38899;&#65292;&#34987;&#21457;&#29616;&#26159;&#29305;&#23450;&#20110;&#35828;&#35805;&#20154;&#30340;&#65306;&#30001;&#36825;&#20123;&#31867;&#20013;&#22823;&#37096;&#20998;&#22768;&#23398;&#26448;&#26009;&#32452;&#25104;&#30340;1&#31186;&#27979;&#35797;&#35805;&#35821;&#25552;&#20379;&#27604;&#35821;&#38899;&#24179;&#34913;&#30340;&#27979;&#35797;&#35805;&#35821;&#26356;&#22909;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#32467;&#26524;&#65292;&#21363;&#20351;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#37117;&#26159;&#29992;15&#31186;&#30340;&#35821;&#38899;&#24179;&#34913;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#20182;&#24773;&#20917;&#30340;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16429v1 Announce Type: new  Abstract: Second-order statistical methods show very good results for automatic speaker identification in controlled recording conditions. These approaches are generally used on the entire speech material available. In this paper, we study the influence of the content of the test speech material on the performances of such methods, i.e. under a more analytical approach. The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal. Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker specific: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identification results than phonetically balanced test utterances, even though the training is done, in both cases, with 15 seconds of phonetically balanced speech. Nevertheless, results with oth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16358</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Integrated Data Processing Framework for Pretraining Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#32463;&#24120;&#38656;&#35201;&#25163;&#21160;&#20174;&#19981;&#21516;&#26469;&#28304;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#27599;&#20010;&#25968;&#25454;&#23384;&#20648;&#24211;&#24320;&#21457;&#19987;&#38376;&#30340;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#36825;&#19968;&#36807;&#31243;&#37325;&#22797;&#32780;&#32321;&#29712;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#22788;&#29702;&#27169;&#22359;&#21253;&#25324;&#19968;&#31995;&#21015;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#20998;&#26512;&#27169;&#22359;&#25903;&#25345;&#23545;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#25506;&#26597;&#21644;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26131;&#20110;&#20351;&#29992;&#19988;&#39640;&#24230;&#28789;&#27963;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#24182;&#23637;&#31034;&#23427;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#20013;&#26032;&#29992;&#25143;&#30340;&#35780;&#20998;&#35843;&#26597;&#65292;&#21487;&#20197;&#19968;&#27425;&#36873;&#25321;&#25152;&#26377;&#31181;&#23376;&#39033;&#30446;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38750;&#32447;&#24615;&#20132;&#20114;&#20316;&#29992;&#26469;&#25512;&#26029;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.16327</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#20013;&#38024;&#23545;&#26032;&#29992;&#25143;&#30340;&#28145;&#24230;&#35780;&#20998;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Rating Elicitation for New Users in Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#20013;&#26032;&#29992;&#25143;&#30340;&#35780;&#20998;&#35843;&#26597;&#65292;&#21487;&#20197;&#19968;&#27425;&#36873;&#25321;&#25152;&#26377;&#31181;&#23376;&#39033;&#30446;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38750;&#32447;&#24615;&#20132;&#20114;&#20316;&#29992;&#26469;&#25512;&#26029;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25512;&#33616;&#31995;&#32479;&#24320;&#22987;&#20351;&#29992;&#35780;&#20998;&#35843;&#26597;&#26469;&#25913;&#36827;&#21021;&#22987;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#26032;&#29992;&#25143;&#23545;&#19968;&#20010;&#23567;&#30340;&#31181;&#23376;&#39033;&#30446;&#38598;&#36827;&#34892;&#35780;&#20998;&#20197;&#25512;&#26029;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#35780;&#20998;&#35843;&#26597;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#36873;&#25321;&#33021;&#22815;&#26368;&#22909;&#25512;&#26029;&#26032;&#29992;&#25143;&#20559;&#22909;&#30340;&#31181;&#23376;&#39033;&#30446;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35780;&#20998;&#35843;&#26597;&#65288;DRE&#65289;&#65292;&#35813;&#26694;&#26550;&#19968;&#27425;&#36873;&#25321;&#25152;&#26377;&#31181;&#23376;&#39033;&#30446;&#65292;&#24182;&#32771;&#34385;&#38750;&#32447;&#24615;&#20132;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#23450;&#20041;&#20102;&#20998;&#31867;&#20998;&#24067;&#26469;&#20174;&#25972;&#20010;&#39033;&#30446;&#38598;&#20013;&#25277;&#26679;&#31181;&#23376;&#39033;&#30446;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#20998;&#24067;&#21644;&#19968;&#20010;&#31070;&#32463;&#37325;&#26500;&#32593;&#32476;&#65292;&#20174;&#25277;&#26679;&#30340;&#31181;&#23376;&#39033;&#30446;&#30340;CF&#20449;&#24687;&#20013;&#25512;&#26029;&#29992;&#25143;&#23545;&#21097;&#20313;&#39033;&#30446;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23398;&#20064;&#20102;&#20998;&#31867;&#20998;&#24067;&#20197;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#31181;&#23376;&#39033;&#30446;&#65292;&#21516;&#26102;&#21453;&#26144;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16327v1 Announce Type: new  Abstract: Recent recommender systems started to use rating elicitation, which asks new users to rate a small seed itemset for inferring their preferences, to improve the quality of initial recommendations. The key challenge of the rating elicitation is to choose the seed items which can best infer the new users' preference. This paper proposes a novel end-to-end Deep learning framework for Rating Elicitation (DRE), that chooses all the seed items at a time with consideration of the non-linear interactions. To this end, it first defines categorical distributions to sample seed items from the entire itemset, then it trains both the categorical distributions and a neural reconstruction network to infer users' preferences on the remaining items from CF information of the sampled seed items. Through the end-to-end training, the categorical distributions are learned to select the most representative seed items while reflecting the complex non-linear int
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25490;&#21517;&#20998;&#25968;&#20272;&#35745;&#25512;&#33616;&#32467;&#26524;&#30340;&#20934;&#30830;&#32622;&#20449;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#32622;&#20449;&#24230;&#22312;&#25945;&#23398;&#21644;&#21830;&#21697;&#21576;&#29616;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.16325</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Confidence Calibration for Recommender Systems and Its Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16325
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25490;&#21517;&#20998;&#25968;&#20272;&#35745;&#25512;&#33616;&#32467;&#26524;&#30340;&#20934;&#30830;&#32622;&#20449;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#32622;&#20449;&#24230;&#22312;&#25945;&#23398;&#21644;&#21830;&#21697;&#21576;&#29616;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25512;&#33616;&#32467;&#26524;&#20934;&#30830;&#24230;&#26041;&#38754;&#21463;&#21040;&#37325;&#35270;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;&#25317;&#26377;&#19968;&#31181;&#23545;&#25512;&#33616;&#32467;&#26524;&#32622;&#20449;&#24230;&#30340;&#34913;&#37327;&#26041;&#24335;&#21364;&#20986;&#22855;&#30340;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#25490;&#21517;&#20998;&#25968;&#20272;&#35745;&#25512;&#33616;&#32467;&#26524;&#30340;&#20934;&#30830;&#32622;&#20449;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#38543;&#21518;&#20171;&#32461;&#20102;&#32622;&#20449;&#24230;&#22312;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65306;(1) &#36890;&#36807;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#35270;&#20026;&#38468;&#21152;&#23398;&#20064;&#25351;&#23548;&#65292;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;(2) &#22522;&#20110;&#26657;&#20934;&#21518;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#39044;&#26399;&#29992;&#25143;&#25928;&#29992;&#35843;&#25972;&#21576;&#29616;&#21830;&#21697;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16325v1 Announce Type: new  Abstract: Despite the importance of having a measure of confidence in recommendation results, it has been surprisingly overlooked in the literature compared to the accuracy of the recommendation. In this dissertation, I propose a model calibration framework for recommender systems for estimating accurate confidence in recommendation results based on the learned ranking scores. Moreover, I subsequently introduce two real-world applications of confidence on recommendations: (1) Training a small student model by treating the confidence of a big teacher model as additional learning guidance, (2) Adjusting the number of presented items based on the expected user utility estimated with calibrated probability.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22823;&#23567;&#30340;Top-K&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#27169;&#22411;PerK&#65292;&#36890;&#36807;&#20272;&#35745;&#29992;&#25143;&#25928;&#29992;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#25928;&#29992;&#26469;&#36873;&#25321;&#25512;&#33616;&#22823;&#23567;</title><link>https://arxiv.org/abs/2402.16304</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#22823;&#23567;&#30340;Top-K&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Top-Personalized-K Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16304
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22823;&#23567;&#30340;Top-K&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#27169;&#22411;PerK&#65292;&#36890;&#36807;&#20272;&#35745;&#29992;&#25143;&#25928;&#29992;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#25928;&#29992;&#26469;&#36873;&#25321;&#25512;&#33616;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;Top-K&#25512;&#33616;&#23558;&#21576;&#29616;&#20855;&#26377;&#26368;&#39640;&#25490;&#21517;&#24471;&#20998;&#30340;&#21069;K&#20010;&#29289;&#21697;&#65292;&#36825;&#26159;&#29983;&#25104;&#20010;&#24615;&#21270;&#25490;&#21517;&#21015;&#34920;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#22266;&#23450;&#22823;&#23567;&#30340;Top-K&#25512;&#33616;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#26159;&#21542;&#26159;&#26368;&#20339;&#26041;&#27861;&#65311;&#26410;&#24517;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#29992;&#25143;&#25928;&#29992;&#32780;&#25552;&#20379;&#22266;&#23450;&#22823;&#23567;&#30340;&#25512;&#33616;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#26080;&#20851;&#39033;&#30446;&#25110;&#38480;&#21046;&#19982;&#30456;&#20851;&#39033;&#30446;&#30340;&#25509;&#35302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Top-Personalized-K&#25512;&#33616;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22823;&#23567;&#25490;&#21517;&#21015;&#34920;&#20197;&#26368;&#22823;&#21270;&#20010;&#20154;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#26032;&#25512;&#33616;&#20219;&#21153;&#12290;&#20316;&#20026;&#25552;&#20986;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;PerK&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#12290;PerK&#36890;&#36807;&#21033;&#29992;&#26657;&#20934;&#30340;&#20132;&#20114;&#27010;&#29575;&#26469;&#20272;&#35745;&#26399;&#26395;&#30340;&#29992;&#25143;&#25928;&#29992;&#65292;&#38543;&#21518;&#36873;&#25321;&#26368;&#22823;&#21270;&#36825;&#31181;&#26399;&#26395;&#25928;&#29992;&#30340;&#25512;&#33616;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16304v1 Announce Type: new  Abstract: The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction? Not necessarily. We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility. Through
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DWHRec&#31639;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#25512;&#33616;&#20013;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#36229;&#22270;&#23884;&#20837;&#23398;&#20064;&#26469;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16299</link><description>&lt;p&gt;
&#23545;&#25239;&#31579;&#36873;&#27668;&#27873;&#65306;&#22522;&#20110;&#21152;&#26435;&#36229;&#22270;&#23884;&#20837;&#23398;&#20064;&#30340;&#38899;&#20048;&#25512;&#33616;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16299
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DWHRec&#31639;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#25512;&#33616;&#20013;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#36229;&#22270;&#23884;&#20837;&#23398;&#20064;&#26469;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#29992;&#25143;&#26377;&#30528;&#21452;&#37325;&#20316;&#29992;&#65306;&#36807;&#28388;&#19981;&#21512;&#36866;&#25110;&#19981;&#21305;&#37197;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20934;&#30830;&#35782;&#21035;&#31526;&#21512;&#20854;&#20559;&#22909;&#30340;&#39033;&#30446;&#12290;&#35768;&#22810;&#25512;&#33616;&#31639;&#27861;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#38453;&#21015;&#65292;&#20197;&#28385;&#36275;&#20854;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#20010;&#24615;&#21270;&#21487;&#33021;&#20250;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#8220;&#31579;&#36873;&#27668;&#27873;&#8221;&#20013;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#20013;&#33719;&#24471;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#26159;&#19968;&#39033;&#36843;&#20999;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#38899;&#20048;&#25512;&#33616;&#20026;&#20363;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#26679;&#21270;&#21152;&#26435;&#36229;&#22270;&#38899;&#20048;&#25512;&#33616;&#31639;&#27861;&#65288;DWHRec&#65289;&#12290;&#22312;DWHRec&#31639;&#27861;&#20013;&#65292;&#29992;&#25143;&#21644;&#24050;&#21548;&#26354;&#30446;&#20043;&#38388;&#30340;&#21021;&#22987;&#36830;&#25509;&#30001;&#21152;&#26435;&#36229;&#22270;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#36824;&#23558;&#33402;&#26415;&#23478;&#12289;&#19987;&#36753;&#21644;&#26631;&#35760;&#19982;&#26354;&#30446;&#30340;&#20851;&#32852;&#20063;&#38468;&#21152;&#21040;&#36229;&#22270;&#20013;&#12290;&#20026;&#20102;&#25506;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#19968;&#20010;&#36229;&#22270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16299v1 Announce Type: cross  Abstract: Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a "filter bubble". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergrap
&lt;/p&gt;</description></item><item><title>PerLTQA&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#30340;&#21019;&#26032;QA&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#21644;&#35760;&#24518;&#25972;&#21512;&#12289;&#26816;&#32034;&#12289;&#21512;&#25104;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.16288</link><description>&lt;p&gt;
PerLTQA: &#19968;&#20010;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#35760;&#24518;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#21512;&#25104;&#30340;&#20010;&#20154;&#38271;&#26399;&#35760;&#24518;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16288
&lt;/p&gt;
&lt;p&gt;
PerLTQA&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#30340;&#21019;&#26032;QA&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#21644;&#35760;&#24518;&#25972;&#21512;&#12289;&#26816;&#32034;&#12289;&#21512;&#25104;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#35760;&#24518;&#22312;&#20010;&#20154;&#20114;&#21160;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32771;&#34385;&#21040;&#38271;&#26399;&#35760;&#24518;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#12289;&#21382;&#21490;&#20449;&#24687;&#21644;&#23545;&#35805;&#20013;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;PerLTQA&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#65292;&#21253;&#25324;&#19990;&#30028;&#30693;&#35782;&#12289;&#29992;&#25143;&#36164;&#26009;&#12289;&#31038;&#20250;&#20851;&#31995;&#12289;&#20107;&#20214;&#21644;&#23545;&#35805;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#25910;&#38598;&#29992;&#20110;&#25506;&#35752;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20132;&#20114;&#21160;&#21644;&#20107;&#20214;&#12290;PerLTQA&#20855;&#26377;&#20004;&#31181;&#35760;&#24518;&#31867;&#22411;&#21644;&#19968;&#20010;&#21253;&#21547;8,593&#20010;&#38382;&#39064;&#30340;30&#20010;&#23383;&#31526;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20419;&#36827;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25506;&#32034;&#21644;&#24212;&#29992;&#20010;&#24615;&#21270;&#35760;&#24518;&#12290;&#22522;&#20110;PerLTQA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35760;&#24518;&#25972;&#21512;&#21644;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#35760;&#24518;&#20998;&#31867;&#12289;&#35760;&#24518;&#26816;&#32034;&#21644;&#35760;&#24518;&#21512;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;LLM&#21644;&#19977;&#20010;&#35780;&#20272;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16288v1 Announce Type: cross  Abstract: Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and thre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16261</link><description>&lt;p&gt;
UniRetriever&#65306;&#21508;&#31181;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16261
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26816;&#32034;&#26159;&#25351;&#20197;&#36845;&#20195;&#21644;&#20132;&#20114;&#26041;&#24335;&#36816;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#38656;&#35201;&#26816;&#32034;&#21508;&#31181;&#22806;&#37096;&#36164;&#28304;&#65288;&#22914;&#20154;&#35774;&#12289;&#30693;&#35782;&#29978;&#33267;&#22238;&#24212;&#65289;&#20197;&#26377;&#25928;&#19982;&#29992;&#25143;&#20132;&#20114;&#24182;&#25104;&#21151;&#23436;&#25104;&#23545;&#35805;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#20316;&#20026;&#19977;&#20010;&#20027;&#35201;&#26816;&#32034;&#20219;&#21153;&#30340;&#36890;&#29992;&#26816;&#32034;&#22120;&#65306;&#20154;&#35774;&#36873;&#25321;&#12289;&#30693;&#35782;&#36873;&#25321;&#21644;&#22238;&#24212;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20505;&#36873;&#32773;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#31616;&#21333;&#30340;&#28857;&#31215;&#20851;&#27880;&#38271;&#23545;&#35805;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#24182;&#26816;&#32034;&#21512;&#36866;&#30340;&#20505;&#36873;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#20197;&#25429;&#25417;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HASH-CODE&#30340;&#39640;&#39057;&#24863;&#30693;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16240</link><description>&lt;p&gt;
&#38024;&#23545;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#39640;&#39057;&#24863;&#30693;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HASH-CODE&#30340;&#39640;&#39057;&#24863;&#30693;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#65292;&#20854;&#20013;&#33410;&#28857;&#20851;&#32852;&#26377;&#25991;&#26412;&#20449;&#24687;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#32534;&#30721;&#32593;&#32476;&#21644;&#25991;&#26412;&#20449;&#21495;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#31934;&#32454;&#22320;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32806;&#21512;&#22312;TAGs&#19978;&#30340;&#27880;&#24847;&#21147;&#36739;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;GNNs&#24456;&#23569;&#20197;&#19968;&#31181;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#23545;&#27599;&#20010;&#33410;&#28857;&#20013;&#30340;&#25991;&#26412;&#36827;&#34892;&#24314;&#27169;&#65307;&#29616;&#26377;&#30340;PLMs&#30001;&#20110;&#20854;&#24207;&#21015;&#26550;&#26500;&#65292;&#20960;&#20046;&#26080;&#27861;&#24212;&#29992;&#20110;&#34920;&#24449;&#22270;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HASH-CODE&#65292;&#19968;&#31181;&#39640;&#39057;&#24863;&#30693;&#30340;&#35889;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;GNNs&#21644;PLMs&#25972;&#21512;&#21040;&#32479;&#19968;&#27169;&#22411;&#20013;&#12290;&#19982;&#20043;&#21069;&#30340;&#8220;&#32423;&#32852;&#26550;&#26500;&#8221;&#19981;&#21516;&#65292;&#30452;&#25509;&#22312;PLM&#20043;&#19978;&#28155;&#21152;GNN&#23618;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;HASH-CODE&#20381;&#38752;&#20116;&#20010;&#33258;&#30417;&#30563;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20419;&#36827;&#24443;&#24213;&#30340;&#30456;&#20114;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16240v1 Announce Type: new  Abstract: We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous "cascaded architectures" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual e
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;</title><link>https://arxiv.org/abs/2402.16200</link><description>&lt;p&gt;
IR2&#65306;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IR2: Information Regularization for Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16200
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IR2&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#22797;&#26434;&#26597;&#35810;&#29305;&#24449;&#30340;&#19977;&#20010;&#26368;&#36817;&#30340;IR&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;DORIS-MAE&#12289;ArguAna&#21644;WhatsThatBook&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#19981;&#20165;&#22312;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50&#65285;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#19981;&#21516;&#38454;&#27573;&#30340;&#19977;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#36755;&#20837;&#12289;&#25552;&#31034;&#21644;&#36755;&#20986;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25506;&#32034;&#65292;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#22343;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32544;&#32467;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DGVAE&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#21644;&#25512;&#33616;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16110</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#35299;&#32544;&#32467;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#19982;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16110
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32544;&#32467;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DGVAE&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#21644;&#25512;&#33616;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#65288;&#20363;&#22914;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#65289;&#34701;&#21512;&#21040;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32416;&#32544;&#30340;&#25968;&#20540;&#21521;&#37327;&#34920;&#31034;&#29992;&#25143;&#21644;&#21830;&#21697;&#65292;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#21644;&#25512;&#33616;&#21487;&#35299;&#37322;&#24615;&#30340;&#35299;&#32544;&#32467;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DGVAE&#65289;&#12290;DGVAE&#39318;&#20808;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25216;&#26415;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#25237;&#24433;&#21040;&#25991;&#26412;&#20869;&#23481;&#20013;&#65292;&#20363;&#22914;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#23427;&#26500;&#24314;&#19968;&#20010;&#20923;&#32467;&#30340;&#21830;&#21697;-&#21830;&#21697;&#22270;&#65292;&#24182;&#21033;&#29992;&#31616;&#21270;&#30340;&#27531;&#20313;&#22270;&#21367;&#31215;&#32593;&#32476;&#23558;&#20869;&#23481;&#21644;&#20132;&#20114;&#32534;&#30721;&#25104;&#20004;&#32452;&#35299;&#32544;&#34920;&#31034;&#12290;DGVAE&#36827;&#19968;&#27493;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16110v1 Announce Type: new  Abstract: Multimodal recommender systems amalgamate multimodal information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations. While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both model and recommendation interpretability. DGVAE initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies. It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network. DGVAE further re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16073</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20960;&#20046;&#23454;&#26102;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16073
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#23884;&#20837;&#26469;&#32534;&#30721;&#29992;&#25143;&#21160;&#20316;&#21644;&#39033;&#30446;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#26816;&#32034;&#65292;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#29992;&#25143;&#23884;&#20837;&#21487;&#33021;&#38480;&#21046;&#25152;&#25429;&#33719;&#30340;&#20852;&#36259;&#22810;&#26679;&#24615;&#65292;2&#65289;&#20445;&#25345;&#23427;&#20204;&#26368;&#26032;&#38656;&#35201;&#26114;&#36149;&#30340;&#23454;&#26102;&#22522;&#30784;&#35774;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#26356;&#26032;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27599;&#20004;&#20998;&#38047;&#32452;&#25104;&#19968;&#20010;&#20449;&#24687;&#27969;&#65292;&#21033;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#21450;&#20854;&#21508;&#33258;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#33655;&#20848;&#21644;&#27604;&#21033;&#26102;&#26368;&#22823;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#19968;Bol&#19978;&#27979;&#35797;&#24182;&#37096;&#32626;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#23548;&#33268;&#36716;&#21270;&#29575;&#26174;&#33879;&#25552;&#39640;&#20102;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15925</link><description>&lt;p&gt;
MultiContrievers: &#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MultiContrievers: Analysis of Dense Retrieval Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#22120;&#23558;&#28304;&#25991;&#26723;&#21387;&#32553;&#20026;&#65288;&#21487;&#33021;&#26159;&#26377;&#25439;&#30340;&#65289;&#21521;&#37327;&#34920;&#31034;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;&#22833;&#21435;&#21644;&#20445;&#30041;&#30340;&#20449;&#24687;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#26512;&#36739;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#23545;&#27604;&#31264;&#23494;&#26816;&#32034;&#22120;&#25429;&#33719;&#30340;&#20449;&#24687;&#19982;&#23427;&#20204;&#22522;&#20110;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#19982;Contriever&#65289;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;25&#20010;MultiBert&#26816;&#26597;&#28857;&#20316;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#26469;&#35757;&#32451;MultiContrievers&#65292;&#36825;&#26159;&#19968;&#32452;25&#20010;contriever&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#29305;&#23450;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#21644;&#32844;&#19994;&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26723;&#30340;contriever&#21521;&#37327;&#20013;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#25506;&#27979;&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#25552;&#21462;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#32467;&#26524;&#23545;&#35768;&#22810;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#27927;&#29260;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;contriever&#27169;&#22411;&#26377;&#26174;&#33879;&#22686;&#21152;&#30340;&#21487;&#25552;&#21462;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
&lt;/p&gt;</description></item><item><title>ListT5&#36890;&#36807;Fusion-in-Decoder&#25216;&#26415;&#23454;&#29616;&#21015;&#34920;&#37325;&#25490;&#65292;&#22312;&#38646;-shot&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25928;&#29575;&#39640;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#21015;&#34920;&#37325;&#25490;&#22120;&#30340;&#20013;&#38388;&#27573;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15838</link><description>&lt;p&gt;
ListT5: &#22522;&#20110;&#35299;&#30721;&#22120;&#20869;&#34701;&#21512;&#30340;&#21015;&#34920;&#37325;&#25490;&#26041;&#27861;&#25913;&#21892;&#38646;-shot&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15838
&lt;/p&gt;
&lt;p&gt;
ListT5&#36890;&#36807;Fusion-in-Decoder&#25216;&#26415;&#23454;&#29616;&#21015;&#34920;&#37325;&#25490;&#65292;&#22312;&#38646;-shot&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25928;&#29575;&#39640;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#21015;&#34920;&#37325;&#25490;&#22120;&#30340;&#20013;&#38388;&#27573;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ListT5&#65292;&#19968;&#31181;&#22522;&#20110;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#22788;&#29702;&#22810;&#20010;&#20505;&#36873;&#27573;&#33853;&#30340;Fusion-in-Decoder&#65288;FiD&#65289;&#30340;&#26032;&#22411;&#37325;&#25490;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;m&#20803;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#36755;&#20986;&#32531;&#23384;&#30340;&#21015;&#34920;&#25490;&#24207;&#30340;&#39640;&#25928;&#25512;&#26029;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;BEIR&#22522;&#20934;&#19978;&#35780;&#20272;&#21644;&#27604;&#36739;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;ListT5&#65288;1&#65289;&#22312;&#24179;&#22343;NDCG@10&#24471;&#20998;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;RankT5&#22522;&#32447;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;+1.3&#22686;&#30410;&#65292;&#65288;2&#65289;&#20855;&#26377;&#19982;&#36880;&#28857;&#25490;&#21517;&#27169;&#22411;&#21487;&#27604;&#25311;&#30340;&#25928;&#29575;&#65292;&#24182;&#36229;&#36234;&#20197;&#21069;&#30340;&#21015;&#34920;&#25490;&#24207;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#65288;3&#65289;&#20811;&#26381;&#20102;&#20197;&#21069;&#21015;&#34920;&#37325;&#25490;&#22120;&#30340;&#20013;&#38388;&#27573;&#20002;&#22833;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#35780;&#20272;&#26694;&#26550;&#23436;&#20840;&#24320;&#28304;&#22312; \url{https://github.com/soyoung97/ListT5}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15838v1 Announce Type: new  Abstract: We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \url{https://github.com/soyoung97/ListT5}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iDMIR&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#22240;&#26524;&#26426;&#21046;&#30340;&#20559;&#35265;&#28040;&#38500;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#20811;&#26381;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#27969;&#34892;&#24230;&#21644;&#25277;&#26679;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15819</link><description>&lt;p&gt;
&#28040;&#38500;&#20559;&#35265;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Debiased Model-based Interactive Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iDMIR&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#22240;&#26524;&#26426;&#21046;&#30340;&#20559;&#35265;&#28040;&#38500;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#20811;&#26381;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#27969;&#34892;&#24230;&#21644;&#25277;&#26679;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#26597;&#35810;&#19990;&#30028;&#27169;&#22411;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#20294;&#20174;&#21382;&#21490;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#24456;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#22914;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#25277;&#26679;&#20559;&#35265;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#28040;&#38500;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;1&#65289;&#24573;&#30053;&#26102;&#38388;&#21464;&#21270;&#27969;&#34892;&#24230;&#30340;&#21160;&#24577;&#20250;&#23548;&#33268;&#23545;&#39033;&#30446;&#30340;&#38169;&#35823;&#37325;&#26032;&#21152;&#26435;&#12290; 2&#65289;&#23558;&#26410;&#30693;&#26679;&#26412;&#35270;&#20026;&#36127;&#26679;&#26412;&#22312;&#36127;&#37319;&#26679;&#20013;&#20250;&#23548;&#33268;&#25277;&#26679;&#20559;&#35265;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35782;&#21035;&#21435;&#20559;&#35265;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#27169;&#22411;&#65288;&#31616;&#31216;iDMIR&#65289;&#12290;&#22312;iDMIR&#20013;&#65292;&#38024;&#23545;&#31532;&#19968;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#26426;&#21046;&#30340;&#20559;&#35265;&#28040;&#38500;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#65292;&#20445;&#35777;&#20102;&#37492;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15819v1 Announce Type: cross  Abstract: Existing model-based interactive recommendation systems are trained by querying a world model to capture the user preference, but learning the world model from historical logged data will easily suffer from bias issues such as popularity bias and sampling bias. This is why some debiased methods have been proposed recently. However, two essential drawbacks still remain: 1) ignoring the dynamics of the time-varying popularity results in a false reweighting of items. 2) taking the unknown samples as negative samples in negative sampling results in the sampling bias. To overcome these two drawbacks, we develop a model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based \textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In iDMIR, for the first drawback, we devise a debiased causal world model based on the causal mechanism of the time-varying recommendation generation process with identification guarantee
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21152;&#23494;&#36827;&#34892;&#30772;&#35299;&#21644;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#26497;&#22823;&#22320;&#21152;&#23494;&#36890;&#20449;&#26426;&#21046;&#65292;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#24182;&#20943;&#23569;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15779</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21152;&#23494;&#30340;&#30772;&#35299;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21152;&#23494;&#36827;&#34892;&#30772;&#35299;&#21644;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#26497;&#22823;&#22320;&#21152;&#23494;&#36890;&#20449;&#26426;&#21046;&#65292;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#24182;&#20943;&#23569;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#26085;&#28176;&#27969;&#34892;&#20197;&#21450;&#36890;&#36807;&#20113;&#21644;&#25968;&#25454;&#20013;&#24515;&#24191;&#27867;&#20351;&#29992;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#65292;&#20010;&#20154;&#21644;&#32452;&#32455;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#21464;&#24471;&#26497;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#21152;&#23494;&#36890;&#36807;&#20445;&#25252;&#20844;&#20849;&#20449;&#24687;&#20132;&#27969;&#26469;&#26377;&#25928;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20102;&#21508;&#31181;&#21152;&#23494;&#31639;&#27861;&#26469;&#28385;&#36275;&#35813;&#39046;&#22495;&#30340;&#19981;&#21516;&#35201;&#27714;&#65292;&#24182;&#22312;&#24037;&#20316;&#36807;&#31243;&#20013;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#21152;&#23494;&#36890;&#20449;&#26426;&#21046;&#65292;&#20197;&#23613;&#21487;&#33021;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15779v1 Announce Type: cross  Abstract: With the rising popularity of the internet and the widespread use of networks and information systems via the cloud and data centers, the privacy and security of individuals and organizations have become extremely crucial. In this perspective, encryption consolidates effective technologies that can effectively fulfill these requirements by protecting public information exchanges. To achieve these aims, the researchers used a wide assortment of encryption algorithms to accommodate the varied requirements of this field, as well as focusing on complex mathematical issues during their work to substantially complicate the encrypted communication mechanism. as much as possible to preserve personal information while significantly reducing the possibility of attacks. Depending on how complex and distinct the requirements established by these various applications are, the potential of trying to break them continues to occur, and systems for eva
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.15708</link><description>&lt;p&gt;
&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#26597;&#35810;&#35821;&#20041;&#30340;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Augmentation by Decoding Semantics from Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#29992;&#20110;&#32454;&#21270;&#35821;&#20041;&#19981;&#20934;&#30830;&#26597;&#35810;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#26597;&#35810;&#25193;&#23637;&#20381;&#36182;&#20110;&#20174;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#12289;&#28508;&#22312;&#30456;&#20851;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22914;&#26524;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#36136;&#37327;&#36739;&#20302;&#65292;&#21017;&#26597;&#35810;&#25193;&#23637;&#30340;&#26377;&#25928;&#24615;&#20063;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Brain-Aug&#65292;&#36890;&#36807;&#23558;&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#26597;&#35810;&#20013;&#26469;&#22686;&#24378;&#26597;&#35810;&#12290;Brain-Aug&#20351;&#29992;&#20102;&#22312;&#33041;&#20449;&#21495;&#20449;&#24687;&#26500;&#24314;&#30340;&#25552;&#31034;&#21644;&#38754;&#21521;&#25490;&#21517;&#30340;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#21407;&#22987;&#26597;&#35810;&#30340;&#24310;&#32493;&#37096;&#20998;&#12290;&#23545;fMRI&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Brain-Aug&#29983;&#25104;&#30340;&#26597;&#35810;&#22312;&#35821;&#20041;&#19978;&#26356;&#20934;&#30830;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#12290;&#33041;&#20449;&#21495;&#24102;&#26469;&#30340;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#27169;&#31946;&#26597;&#35810;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.15666</link><description>&lt;p&gt;
&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#20013;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Universal Model in Online Customer Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#22312; typcial &#21830;&#19994;&#22330;&#26223;&#20013;&#24120;&#24120;&#38656;&#35201;&#25968;&#26376;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#33268;&#24615;&#24182;&#32771;&#34385;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23450;&#26399;&#30340;&#37325;&#26032;&#35757;&#32451;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30005;&#23376;&#21830;&#21153;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#23545;&#35805;&#20013;&#26631;&#35760;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#38382;&#39064;&#21450;&#30456;&#24212;&#26631;&#31614;&#30340;&#23384;&#20648;&#24211;&#12290;&#24403;&#23458;&#25143;&#35831;&#27714;&#24110;&#21161;&#26102;&#65292;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#23384;&#20648;&#24211;&#20013;&#25628;&#32034;&#30456;&#20284;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#20998;&#26512;&#26469;&#39044;&#27979;&#30456;&#24212;&#30340;&#26631;&#31614;&#12290;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15666v1 Announce Type: cross  Abstract: Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predict-ing labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#31995;&#32479;&#65292;&#20026;&#37325;&#30151;&#30417;&#25252;&#21518;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24615;&#35270;&#35273;&#33402;&#26415;&#20307;&#39564;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#25512;&#33616;&#33021;&#22815;&#25552;&#21319;&#26102;&#38388;&#24773;&#24863;&#29366;&#24577;</title><link>https://arxiv.org/abs/2402.15643</link><description>&lt;p&gt;
&#31934;&#22937;&#30340;&#24247;&#22797;&#20043;&#36335;&#65306;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#20197;&#39044;&#38450;&#21644;&#20943;&#23569;&#37325;&#30151;&#30417;&#25252;&#21518;&#36951;&#30151;
&lt;/p&gt;
&lt;p&gt;
Artful Path to Healing: Using Machine Learning for Visual Art Recommendation to Prevent and Reduce Post-Intensive Care
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15643
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#31995;&#32479;&#65292;&#20026;&#37325;&#30151;&#30417;&#25252;&#21518;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24615;&#35270;&#35273;&#33402;&#26415;&#20307;&#39564;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#25512;&#33616;&#33021;&#22815;&#25552;&#21319;&#26102;&#38388;&#24773;&#24863;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24453;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#37324;&#32463;&#24120;&#20250;&#24102;&#26469;&#21019;&#20260;&#65292;&#23548;&#33268;&#37325;&#30151;&#30417;&#25252;&#21518;&#32508;&#21512;&#24449;&#65288;PICS&#65289;&#65292;&#21253;&#25324;&#36523;&#20307;&#12289;&#24515;&#29702;&#21644;&#35748;&#30693;&#38556;&#30861;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;PICS&#65292;&#21487;&#29992;&#30340;&#24178;&#39044;&#26377;&#38480;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25509;&#35302;&#35270;&#35273;&#33402;&#26415;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;PICS&#30340;&#24515;&#29702;&#26041;&#38754;&#65292;&#22914;&#26524;&#20010;&#24615;&#21270;&#23450;&#21046;&#21487;&#33021;&#25928;&#26524;&#26356;&#20339;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#31995;&#32479;&#65288;VA RecSys&#65289;&#65292;&#20026;&#37325;&#30151;&#30417;&#25252;&#21518;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24615;&#35270;&#35273;&#33402;&#26415;&#20307;&#39564;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;VA RecSys&#24341;&#25806;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#27835;&#30103;&#30446;&#30340;&#32780;&#35328;&#27604;&#36739;&#19987;&#23478;&#31574;&#21010;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19987;&#23478;&#35797;&#28857;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#65288;n=150&#65289;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#25512;&#33616;&#30340;&#36866;&#24403;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#25512;&#33616;&#22343;&#25552;&#21319;&#20102;&#26102;&#38388;&#24773;&#24863;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15643v1 Announce Type: new  Abstract: Staying in the intensive care unit (ICU) is often traumatic, leading to post-intensive care syndrome (PICS), which encompasses physical, psychological, and cognitive impairments. Currently, there are limited interventions available for PICS. Studies indicate that exposure to visual art may help address the psychological aspects of PICS and be more effective if it is personalized. We develop Machine Learning-based Visual Art Recommendation Systems (VA RecSys) to enable personalized therapeutic visual art experiences for post-ICU patients. We investigate four state-of-the-art VA RecSys engines, evaluating the relevance of their recommendations for therapeutic purposes compared to expert-curated recommendations. We conduct an expert pilot test and a large-scale user study (n=150) to assess the appropriateness and effectiveness of these recommendations. Our results suggest all recommendations enhance temporal affective states. Visual and mul
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;Language-based Factorization Model (LFM)&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.15623</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#20559;&#22909;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Based User Profiles for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15623
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;Language-based Factorization Model (LFM)&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#65289;&#23558;&#29992;&#25143;&#20559;&#22909;&#34920;&#31034;&#20026;&#39640;&#32500;&#21521;&#37327;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#21521;&#37327;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#19979;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65288;LFM&#65289;&#65292;&#23427;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#22343;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#32534;&#30721;&#22120;LLM&#20174;&#29992;&#25143;&#30340;&#35780;&#20998;&#21382;&#21490;&#29983;&#25104;&#29992;&#25143;&#20852;&#36259;&#30340;&#31616;&#27905;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#35299;&#30721;&#22120;LLM&#20351;&#29992;&#36825;&#20010;&#31616;&#35201;&#25551;&#36848;&#26469;&#23436;&#25104;&#39044;&#27979;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;MovieLens&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LFM&#26041;&#27861;&#65292;&#23558;&#20854;&#19982;&#30697;&#38453;&#20998;&#35299;&#21644;&#30452;&#25509;&#20174;&#29992;&#25143;&#35780;&#20998;&#21382;&#21490;&#39044;&#27979;&#30340;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h
&lt;/p&gt;</description></item><item><title>RecWizard&#26159;&#19968;&#31181;&#23545;&#35805;&#24335;&#25512;&#33616;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#27169;&#22411;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#25552;&#39640;CRS&#30740;&#31350;&#25928;&#29575;&#24182;&#20943;&#23569;&#39069;&#22806;&#24037;&#20316;&#37327;</title><link>https://arxiv.org/abs/2402.15591</link><description>&lt;p&gt;
RecWizard&#65306;&#19968;&#31181;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#27169;&#22411;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
RecWizard: A Toolkit for Conversational Recommendation with Modular, Portable Models and Interactive User Interface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15591
&lt;/p&gt;
&lt;p&gt;
RecWizard&#26159;&#19968;&#31181;&#23545;&#35805;&#24335;&#25512;&#33616;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#27169;&#22411;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#25552;&#39640;CRS&#30740;&#31350;&#25928;&#29575;&#24182;&#20943;&#23569;&#39069;&#22806;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecWizard&#30340;&#26032;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#12290;RecWizard&#25903;&#25345;&#27169;&#22411;&#24320;&#21457;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#65292;&#20511;&#37492;&#20102;Huggingface&#29983;&#24577;&#31995;&#32479;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;CRS&#19982;RecWizard&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#12289;&#20132;&#20114;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21451;&#22909;&#24615;&#65292;&#20197;&#31616;&#21270;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;CRS&#30740;&#31350;&#30340;&#39069;&#22806;&#24037;&#20316;&#37327;&#12290;&#26377;&#20851;RecWizard&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#35831;&#26597;&#30475;&#25105;&#20204;&#30340;GitHub https://github.com/McAuley-Lab/RecWizard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15591v1 Announce Type: cross  Abstract: We present a new Python toolkit called RecWizard for Conversational Recommender Systems (CRS). RecWizard offers support for development of models and interactive user interface, drawing from the best practices of the Huggingface ecosystems. CRS with RecWizard are modular, portable, interactive and Large Language Models (LLMs)-friendly, to streamline the learning process and reduce the additional effort for CRS research. For more comprehensive information about RecWizard, please check our GitHub https://github.com/McAuley-Lab/RecWizard.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#22522;&#20110;&#35268;&#33539;&#30340;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14539</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#35268;&#33539;&#30340;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14539
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#22522;&#20110;&#35268;&#33539;&#30340;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#24773;&#22823;&#27969;&#34892;&#20197;&#20854;&#28145;&#36828;&#30340;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#65292;&#23545;&#20840;&#29699;&#20581;&#24247;&#12289;&#27515;&#20129;&#29575;&#12289;&#32463;&#27982;&#31283;&#23450;&#24615;&#21644;&#25919;&#27835;&#26684;&#23616;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#24212;&#23545;&#26032;&#20852;&#21644;&#20877;&#29616;&#30123;&#24773;&#24102;&#26469;&#30340;&#25345;&#32493;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#26102;&#31354;&#27169;&#22411;&#26469;&#22686;&#24378;&#25105;&#20204;&#23545;&#36825;&#20123;&#22797;&#26434;&#29616;&#35937;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#12290;&#36825;&#20123;&#26102;&#31354;&#27169;&#22411;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31354;&#38388;&#31867;&#21035;&#65306;&#22522;&#20110;&#35268;&#33539;&#21644;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#36127;&#25285;&#21644;&#21487;&#34920;&#31034;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#36825;&#20123;&#27169;&#22411;&#20174;&#22522;&#20110;&#35268;&#33539;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#31354;&#38388;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#20351;&#29992;&#24191;&#27867;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#26041;&#27861;&#30340;&#21313;&#20108;&#31181;&#21487;&#33021;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#23454;&#29616;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14539v1 Announce Type: new  Abstract: Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes. In response to the persistent challenges posed by emerging and reemerging pandemics, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena. These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and graph-based trade-offering between accuracy, computational burden, and representational feasibility. In this study, we explore the ability to transform from norm-based to graph-based spatial representation for these models. We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches. Our findings show that by leveraging agent-based simulations and heuristic algorithms for the graph
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.12430</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#25913;&#36827;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12430
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;RAG&#26041;&#27861;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#22120;&#22312;&#25552;&#21319;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25581;&#31034;&#27599;&#23545;&#26597;&#35810;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#38656;&#35201;&#21453;&#22797;&#23545;&#26597;&#35810;&#21644;&#22823;&#37327;&#38271;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#23548;&#33268;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#38480;&#21046;&#20102;&#26816;&#32034;&#25991;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20934;&#30830;&#24615;&#12290;&#20316;&#20026;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#36895;&#24230;&#25552;&#39640;20&#20493;&#33267;40&#20493;&#65292;&#36229;&#36807;&#22522;&#20934;&#36890;&#36947;&#37325;&#26032;&#25490;&#24207;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Sigmoid Trick&#65292;&#19968;&#31181;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#22312;&#20174;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#32463;&#39564;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#26469;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;&#65292;&#22635;&#34917;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#20840;&#38754;&#36861;&#36394;&#12289;&#35782;&#21035;&#21644;&#26144;&#23556;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2311.11157</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#23545;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Contextualizing Internet Memes Across Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#26469;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;&#65292;&#22635;&#34917;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#20840;&#38754;&#36861;&#36394;&#12289;&#35782;&#21035;&#21644;&#26144;&#23556;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#36855;&#22240;&#24050;&#25104;&#20026;&#32593;&#32476;&#19978;&#20132;&#27969;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#26032;&#39062;&#26684;&#24335;&#12290;&#23427;&#20204;&#30340;&#27969;&#21160;&#24615;&#21644;&#21019;&#36896;&#24615;&#20307;&#29616;&#22312;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#36890;&#24120;&#36328;&#24179;&#21488;&#20256;&#25773;&#65292;&#20598;&#23572;&#20063;&#29992;&#20110;&#19981;&#36947;&#24503;&#25110;&#26377;&#23475;&#30340;&#30446;&#30340;&#12290; &#34429;&#28982;&#35745;&#31639;&#24037;&#20316;&#24050;&#32463;&#20998;&#26512;&#20102;&#23427;&#20204;&#38543;&#26102;&#38388;&#30340;&#39640;&#32423;&#21035;&#30149;&#27602;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#21162;&#21147;&#26088;&#22312;&#20840;&#38754;&#36319;&#36394;&#12289;&#35782;&#21035;&#21644;&#26144;&#23556;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#12290; &#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30693;&#35782;&#24211;&#65292;&#21363;&#30693;&#35782;&#22270;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;Reddit&#21644;Discord&#25910;&#38598;&#20102;&#25968;&#21315;&#26465;&#28508;&#22312;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#24086;&#23376;&#65292;&#24182;&#24320;&#21457;&#20102;&#25552;&#21462;-&#36716;&#25442;-&#21152;&#36733;&#36807;&#31243;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#20505;&#36873;&#36855;&#22240;&#24086;&#23376;&#30340;&#25968;&#25454;&#28246;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11157v2 Announce Type: replace-cross  Abstract: Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and develop an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>ITEm&#26159;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#20851;&#27880;&#19981;&#21516;&#27169;&#24577;&#65292;&#25193;&#23637;&#20102;BERT&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.02084</link><description>&lt;p&gt;
ITEm&#65306;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#26080;&#30417;&#30563;&#22270;&#29255;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ITEm: Unsupervised Image-Text Embedding Learning for eCommerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02084
&lt;/p&gt;
&lt;p&gt;
ITEm&#26159;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#20851;&#27880;&#19981;&#21516;&#27169;&#24577;&#65292;&#25193;&#23637;&#20102;BERT&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23884;&#20837;&#26159;&#30005;&#23376;&#21830;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#30707;&#12290;&#36890;&#36807;&#20174;&#22810;&#31181;&#27169;&#24577;&#23398;&#20064;&#30340;&#20135;&#21697;&#23884;&#20837;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#25552;&#20379;&#20102;&#20114;&#34917;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#27169;&#24577;&#27604;&#20854;&#20182;&#27169;&#24577;&#26356;&#20855;&#20449;&#24687;&#20248;&#21183;&#12290;&#22914;&#20309;&#25945;&#23548;&#27169;&#22411;&#20174;&#19981;&#21516;&#27169;&#24577;&#23398;&#20064;&#23884;&#20837;&#32780;&#19981;&#24573;&#35270;&#36739;&#19981;&#26174;&#33879;&#27169;&#24577;&#30340;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#29255;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65288;ITEm&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#26469;&#26356;&#22909;&#22320;&#20851;&#27880;&#22270;&#29255;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;1&#65289;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#23884;&#20837;&#32780;&#19981;&#30693;&#36947;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65307;&#65288;2&#65289;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#34920;&#31034;&#26469;&#39044;&#27979;&#25513;&#30721;&#21333;&#35789;&#24182;&#26500;&#24314;&#25513;&#30721;&#22270;&#20687;&#34917;&#19969;&#32780;&#19981;&#23545;&#23427;&#20204;&#30340;&#21333;&#29420;&#34920;&#31034;&#36827;&#34892;&#36807;&#31243;&#26469;&#25193;&#23637;BERT&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;ITEm&#65306;&#23547;&#25214;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02084v2 Announce Type: replace-cross  Abstract: Product embedding serves as a cornerstone for a wide range of applications in eCommerce. The product embedding learned from multiple modalities shows significant improvement over that from a single modality, since different modalities provide complementary information. However, some modalities are more informatively dominant than others. How to teach a model to learn embedding from different modalities without neglecting information from the less dominant modality is challenging. We present an image-text embedding model (ITEm), an unsupervised learning method that is designed to better attend to image and text modalities. We extend BERT by (1) learning an embedding from text and image without knowing the regions of interest; (2) training a global representation to predict masked words and to construct masked image patches without their individual representations. We evaluate the pre-trained ITEm on two tasks: the search for ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;</title><link>https://arxiv.org/abs/2310.13995</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On Bilingual Lexicon Induction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#65288;BLI&#65289;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#30446;&#21069;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#20381;&#36182;&#20110;&#35745;&#31639;&#36328;&#35821;&#35328;&#21333;&#35789;&#34920;&#31034;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#33539;&#24335;&#36716;&#21464;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#26032;&#19968;&#20195;LLMs&#22312;&#21452;&#35821;&#35789;&#27719;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20419;&#20351;&#21644;&#24494;&#35843;&#22810;&#35821;&#35328;LLMs&#65288;mLLMs&#65289;&#20197;&#36827;&#34892;BLI&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#22914;&#20309;&#20197;&#21450;&#22914;&#20309;&#34917;&#20805;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;1&#65289;&#29992;&#20110;&#26080;&#30417;&#30563;BLI&#30340;&#38646;&#27425;&#25552;&#31034;&#21644;2&#65289;&#20351;&#29992;&#19968;&#32452;&#31181;&#23376;&#32763;&#35793;&#23545;&#36827;&#34892;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#22343;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;LLM&#24494;&#35843;&#65292;&#20197;&#21450;3&#65289;&#23545;&#36739;&#23567;LLMs&#36827;&#34892;&#26631;&#20934;BLI&#23548;&#21521;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#19981;&#21516;&#22823;&#23567;&#65288;&#20174;0.3B&#21040;13B&#21442;&#25968;&#65289;&#30340;18&#20010;&#24320;&#28304;&#25991;&#26412;&#23545;&#25991;&#26412;mLLMs&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#28085;&#30422;&#20004;&#20010;&#26631;&#20934;BLI&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
&lt;/p&gt;</description></item><item><title>RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2308.14296</link><description>&lt;p&gt;
RecMind&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25512;&#33616;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RecMind: Large Language Model Powered Agent For Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14296
&lt;/p&gt;
&lt;p&gt;
RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;RS&#26041;&#27861;&#36890;&#24120;&#22312;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26032;&#25512;&#33616;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#27169;&#22411;&#35268;&#27169;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;RecMind&#65292;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21033;&#29992;&#35880;&#24910;&#35268;&#21010;&#30340;&#24037;&#20855;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Self-Inspiring&#31639;&#27861;&#26469;&#25552;&#39640;&#35268;&#21010;&#33021;&#21147;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#27493;&#39588;&#65292;LLM&#33258;&#25105;&#28608;&#21169;&#20197;&#32771;&#34385;&#25152;&#26377;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#29366;&#24577;&#26469;&#35268;&#21010;&#19979;&#19968;&#27493;&#12290;&#36825;&#19968;&#26426;&#21046;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#29702;&#35299;&#21644;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#35268;&#21010;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RecMind&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14296v2 Announce Type: replace-cross  Abstract: While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Item-aligned Federated Aggregation (IFedRec)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#21697;&#23646;&#24615;&#21644;&#20132;&#20114;&#35760;&#24405;&#21516;&#26102;&#23398;&#20064;&#20004;&#32452;&#29289;&#21697;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#29289;&#21697;&#34920;&#31034;&#23545;&#40784;&#26426;&#21046;&#65292;&#20174;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2305.12650</link><description>&lt;p&gt;
&#24403;&#32852;&#37030;&#25512;&#33616;&#36935;&#21040;&#20919;&#21551;&#21160;&#38382;&#39064;&#65306;&#20998;&#31163;&#29289;&#21697;&#23646;&#24615;&#21644;&#29992;&#25143;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
When Federated Recommendation Meets Cold-Start Problem: Separating Item Attributes and User Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Item-aligned Federated Aggregation (IFedRec)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#21697;&#23646;&#24615;&#21644;&#20132;&#20114;&#35760;&#24405;&#21516;&#26102;&#23398;&#20064;&#20004;&#32452;&#29289;&#21697;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#29289;&#21697;&#34920;&#31034;&#23545;&#40784;&#26426;&#21046;&#65292;&#20174;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12650v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#29992;&#25143;&#22312;&#33258;&#24049;&#35774;&#22791;&#19978;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25512;&#33616;&#27169;&#22411;&#19982;&#29992;&#25143;&#31169;&#20154;&#25968;&#25454;&#30340;&#20998;&#31163;&#22312;&#25552;&#20379;&#20248;&#36136;&#26381;&#21153;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#38024;&#23545;&#26032;&#39033;&#30446;&#65292;&#21363;&#20919;&#21551;&#21160;&#25512;&#33616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#21697;&#23545;&#40784;&#32852;&#37030;&#32858;&#21512;&#65288;IFedRec&#65289;&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#36825;&#26159;&#32852;&#37030;&#25512;&#33616;&#20013;&#19987;&#38376;&#30740;&#31350;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#24037;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#29289;&#21697;&#23646;&#24615;&#21644;&#20132;&#20114;&#35760;&#24405;&#23398;&#20064;&#20004;&#32452;&#29289;&#21697;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#21697;&#34920;&#31034;&#23545;&#40784;&#26426;&#21046;&#26469;&#23545;&#40784;&#20004;&#32452;&#29289;&#21697;&#34920;&#31034;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#22312;&#26381;&#21153;&#22120;&#31471;&#23398;&#20064;&#20803;&#23646;&#24615;&#32593;&#32476;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12650v2 Announce Type: replace  Abstract: Federated recommendation system usually trains a global model on the server without direct access to users' private data on their own devices. However, this separation of the recommendation model and users' private data poses a challenge in providing quality service, particularly when it comes to new items, namely cold-start recommendations in federated settings. This paper introduces a novel method called Item-aligned Federated Aggregation (IFedRec) to address this challenge. It is the first research work in federated recommendation to specifically study the cold-start scenario. The proposed method learns two sets of item representations by leveraging item attributes and interaction records simultaneously. Additionally, an item representation alignment mechanism is designed to align two item representations and learn the meta attribute network at the server within a federated learning framework. Experiments on four benchmark dataset
&lt;/p&gt;</description></item><item><title>Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#20026;&#35299;&#20915;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#20013;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2111.02168</link><description>&lt;p&gt;
Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65306;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#20803;&#32032;&#25552;&#21517;
&lt;/p&gt;
&lt;p&gt;
The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.02168
&lt;/p&gt;
&lt;p&gt;
Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#20026;&#35299;&#20915;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#20013;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#33258;&#21160;&#21270;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#29992;&#25143;&#19982;&#25968;&#23383;&#19990;&#30028;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#35745;&#31639;&#26041;&#27861;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#24110;&#21161;&#65292;&#31616;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#19968;&#28436;&#36827;&#36807;&#31243;&#20013;&#65292;&#32593;&#32476;&#20803;&#32032;&#25552;&#21517;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#35782;&#21035;&#32593;&#39029;&#19978;&#30340;&#29420;&#29305;&#20803;&#32032;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#30340;&#21457;&#23637;&#21463;&#21040;&#20840;&#38754;&#21644;&#30495;&#23454;&#21453;&#26144;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#22797;&#26434;&#24615;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;8,175&#20010;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;51,701&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#20135;&#21697;&#39029;&#38754;&#65292;&#35206;&#30422;&#20102;&#20843;&#20010;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#38468;&#24102;&#20102;&#19968;&#32452;&#28210;&#26579;&#39029;&#38754;&#25130;&#22270;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24320;&#22987;&#30740;&#31350;Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.02168v4 Announce Type: replace-cross  Abstract: Web automation holds the potential to revolutionize how users interact with the digital world, offering unparalleled assistance and simplifying tasks via sophisticated computational methods. Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmic designs for web automation is hampered by the scarcity of comprehensive and realistic datasets that reflect the complexity faced by real-world applications on the Web. To address this, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection of webpages that surpasses existing datasets in richness and variety. The dataset features 51,701 manually labeled product pages from 8,175 e-commerce websites across eight geographic regions, accompanied by a dataset of rendered page screenshots. To initiate research on the Klarna Product Page Dataset, we empirical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05200</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65306;&#29992;&#25143;&#35780;&#20272;&#21644;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05200
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#31649;&#29702;&#30693;&#35782;&#23545;&#32452;&#32455;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#25805;&#20316;&#24037;&#21378;&#21464;&#24471;&#36234;&#26469;&#36234;&#20381;&#36182;&#30693;&#35782;&#65292;&#36825;&#32473;&#24037;&#21378;&#22521;&#35757;&#21644;&#25903;&#25345;&#26032;&#25805;&#20316;&#21592;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#21387;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#24037;&#21378;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#24191;&#27867;&#30693;&#35782;&#65292;&#39640;&#25928;&#22238;&#31572;&#25805;&#20316;&#21592;&#30340;&#26597;&#35810;&#24182;&#20419;&#36827;&#26032;&#30693;&#35782;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24037;&#21378;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#30340;&#22909;&#22788;&#65292;&#21363;&#33021;&#22815;&#26356;&#24555;&#22320;&#26816;&#32034;&#20449;&#24687;&#21644;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#26356;&#20542;&#21521;&#20110;&#21521;&#20154;&#24037;&#19987;&#23478;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#20960;&#31181;&#38381;&#28304;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;GPT-4&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20687;StableBe
&lt;/p&gt;
&lt;p&gt;
Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15950</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15950
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;ID&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#23548;&#33268;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#22815;&#23500;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#30340;&#21033;&#29992;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#20559;&#24046;&#65292;&#32473;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#23613;&#31649;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#32467;&#21512;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#23454;&#26045;&#36824;&#38656;&#35201;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#26088;&#22312;&#36890;&#36807;LLM&#24378;&#21270;&#34920;&#31034;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26053;&#28216;&#39046;&#22495;&#30340;&#20852;&#36259;&#28857;&#25512;&#33616;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#24322;&#26500;&#25968;&#25454;&#35299;&#20915;&#26053;&#36884;&#20013;&#20852;&#36259;&#28857;&#25512;&#33616;&#38382;&#39064;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.07426</link><description>&lt;p&gt;
&#21033;&#29992;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#20852;&#36259;&#28857;&#25512;&#33616;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous Data. (arXiv:2308.07426v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26053;&#28216;&#39046;&#22495;&#30340;&#20852;&#36259;&#28857;&#25512;&#33616;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#24322;&#26500;&#25968;&#25454;&#35299;&#20915;&#26053;&#36884;&#20013;&#20852;&#36259;&#28857;&#25512;&#33616;&#38382;&#39064;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#28216;&#26159;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#36127;&#36131;&#20026;&#20132;&#36890;&#12289;&#20303;&#23487;&#12289;&#20852;&#36259;&#28857;&#25110;&#26053;&#28216;&#26381;&#21153;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#23545;&#20010;&#20307;&#28216;&#23458;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#20852;&#36259;&#28857;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#28216;&#23458;&#8220;&#26053;&#36884;&#20013;&#8221;&#25552;&#20379;&#20852;&#36259;&#28857;&#25512;&#33616;&#21487;&#33021;&#20250;&#38754;&#20020;&#29305;&#27530;&#25361;&#25112;&#65292;&#22240;&#20026;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#21464;&#21270;&#22810;&#26679;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24403;&#20170;&#21508;&#31181;&#22312;&#32447;&#26381;&#21153;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21508;&#31181;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#24050;&#32463;&#21464;&#24471;&#21487;&#29992;&#65292;&#36825;&#20123;&#24322;&#26500;&#25968;&#25454;&#28304;&#20026;&#35299;&#20915;&#26053;&#36884;&#20013;&#20852;&#36259;&#28857;&#25512;&#33616;&#38382;&#39064;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#24322;&#26500;&#25968;&#25454;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;2017&#24180;&#33267;2022&#24180;&#38388;&#24050;&#21457;&#34920;&#30340;&#20852;&#36259;&#28857;&#25512;&#33616;&#30740;&#31350;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tourism is an important application domain for recommender systems. In this domain, recommender systems are for example tasked with providing personalized recommendations for transportation, accommodation, points-of-interest (POIs), or tourism services. Among these tasks, in particular the problem of recommending POIs that are of likely interest to individual tourists has gained growing attention in recent years. Providing POI recommendations to tourists \emph{during their trip} can however be especially challenging due to the variability of the users' context. With the rapid development of the Web and today's multitude of online services, vast amounts of data from various sources have become available, and these heterogeneous data sources represent a huge potential to better address the challenges of in-trip POI recommendation problems. In this work, we provide a comprehensive survey of published research on POI recommendation between 2017 and 2022 from the perspective of heterogeneou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14651</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23454;&#20307;&#23545;&#40784;&#21450;&#36229;&#36234;&#65306;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;EEA&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#38382;&#39064;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#31867;&#20284;&#20110;&#20856;&#22411;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#65292;&#22522;&#20110;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;EEA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20182;&#20204;&#19981;&#23436;&#25972;&#30340;&#30446;&#26631;&#38480;&#21046;&#20102;&#23454;&#20307;&#23545;&#40784;&#21644;&#23454;&#20307;&#21512;&#25104;&#65288;&#21363;&#29983;&#25104;&#26032;&#23454;&#20307;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#30340;EEA&#65288;abbr.&#65292;GEEA&#65289;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;M-VAE&#21487;&#20197;&#23558;&#19968;&#20010;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;GEEA&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37327;&#21270;&#25506;&#32034;&#23545;&#20869;&#23481;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#25506;&#32034;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#38271;&#26399;&#22909;&#22788;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#31070;&#32463;&#32447;&#24615;&#21248;&#36895;&#33218;&#31639;&#27861;&#26500;&#24314;&#22522;&#20110;&#25506;&#32034;&#30340;&#25490;&#21517;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.07764</link><description>&lt;p&gt;
&#25506;&#32034;&#30340;&#20215;&#20540;&#65306;&#24230;&#37327;&#12289;&#21457;&#29616;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Value of Exploration: Measurements, Findings and Algorithms. (arXiv:2305.07764v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37327;&#21270;&#25506;&#32034;&#23545;&#20869;&#23481;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#25506;&#32034;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#38271;&#26399;&#22909;&#22788;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#31070;&#32463;&#32447;&#24615;&#21248;&#36895;&#33218;&#31639;&#27861;&#26500;&#24314;&#22522;&#20110;&#25506;&#32034;&#30340;&#25490;&#21517;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25506;&#32034;&#34987;&#35748;&#20026;&#23545;&#25512;&#33616;&#24179;&#21488;&#19978;&#29992;&#25143;&#20307;&#39564;&#30340;&#38271;&#26399;&#24433;&#21709;&#26377;&#31215;&#26497;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#20854;&#30830;&#20999;&#30340;&#22909;&#22788;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25506;&#32034;&#30340;&#24120;&#35268;A/B&#27979;&#35797;&#36890;&#24120;&#27979;&#37327;&#20013;&#24615;&#29978;&#33267;&#28040;&#26497;&#30340;&#21442;&#19982;&#24230;&#25351;&#26631;&#65292;&#21516;&#26102;&#26410;&#33021;&#25429;&#25417;&#20854;&#38271;&#26399;&#25928;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#26816;&#26597;&#25506;&#32034;&#23545;&#20869;&#23481;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#26469;&#27491;&#24335;&#37327;&#21270;&#25506;&#32034;&#30340;&#20215;&#20540;&#65292;&#36825;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#30340;&#20851;&#38190;&#23454;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#30456;&#20851;&#23454;&#39564;&#35774;&#35745;&#26469;&#27979;&#37327;&#25506;&#32034;&#23545;&#35821;&#26009;&#24211;&#21464;&#21270;&#30340;&#30410;&#22788;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#35821;&#26009;&#24211;&#21464;&#21270;&#19982;&#38271;&#26399;&#29992;&#25143;&#20307;&#39564;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#20837;&#31070;&#32463;&#32447;&#24615;&#21248;&#36895;&#33218;&#31639;&#27861;&#26500;&#24314;&#22522;&#20110;&#25506;&#32034;&#30340;&#25490;&#21517;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#30340;&#39592;&#24178;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#26102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective exploration is believed to positively influence the long-term user experience on recommendation platforms. Determining its exact benefits, however, has been challenging. Regular A/B tests on exploration often measure neutral or even negative engagement metrics while failing to capture its long-term benefits. To address this, we present a systematic study to formally quantify the value of exploration by examining its effects on the content corpus, a key entity in the recommender system that directly affects user experiences. Specifically, we introduce new metrics and the associated experiment design to measure the benefit of exploration on the corpus change, and further connect the corpus change to the long-term user experience. Furthermore, we investigate the possibility of introducing the Neural Linear Bandit algorithm to build an exploration-based ranking system, and use it as the backbone algorithm for our case study. We conduct extensive live experiments on a large-scale 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#21487;&#20197;&#20811;&#26381;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#20449;&#24687;&#38656;&#27714;&#30340;&#20869;&#23481;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23548;&#20869;&#23481;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.03516</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#25512;&#33616;&#65306;&#36208;&#21521;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Recommendation: Towards Next-generation Recommender Paradigm. (arXiv:2304.03516v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03516
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#21487;&#20197;&#20811;&#26381;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#20449;&#24687;&#38656;&#27714;&#30340;&#20869;&#23481;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23548;&#20869;&#23481;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20174;&#39033;&#30446;&#38598;&#21512;&#20013;&#26816;&#32034;&#39033;&#30446;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25512;&#33616;&#33539;&#24335;&#38754;&#20020;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#35821;&#26009;&#24211;&#20013;&#30340;&#20154;&#24037;&#29983;&#25104;&#39033;&#30446;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20449;&#24687;&#38656;&#27714;&#65292;2&#65289;&#29992;&#25143;&#36890;&#24120;&#36890;&#36807;&#28857;&#20987;&#31561;&#34987;&#21160;&#19988;&#20302;&#25928;&#30340;&#21453;&#39304;&#26041;&#24335;&#35843;&#25972;&#25512;&#33616;&#20869;&#23481;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#20855;&#26377;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#28508;&#21147;&#65306;1&#65289;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#20869;&#23481;&#20197;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;2&#65289;&#26032;&#20852;&#30340;ChatGPT&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#20934;&#30830;&#34920;&#36798;&#20449;&#24687;&#38656;&#27714;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#22823;&#29190;&#21457;&#25351;&#24341;&#25105;&#20204;&#36208;&#21521;&#19979;&#19968;&#20195;&#25512;&#33616;&#33539;&#24335;&#65292;&#20855;&#26377;&#20004;&#20010;&#26032;&#30340;&#30446;&#26631;&#65306;1&#65289;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20010;&#24615;&#21270;&#20869;&#23481;&#65292;2&#65289;&#25972;&#21512;&#29992;&#25143;&#25351;&#20196;&#20197;&#25351;&#23548;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems typically retrieve items from an item corpus for personalized recommendations. However, such a retrieval-based recommender paradigm faces two limitations: 1) the human-generated items in the corpus might fail to satisfy the users' diverse information needs, and 2) users usually adjust the recommendations via passive and inefficient feedback such as clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success across various domains, offering the potential to overcome these limitations: 1) generative AI can produce personalized items to meet users' specific information needs, and 2) the newly emerged ChatGPT significantly facilitates users to express information needs more precisely via natural language instructions. In this light, the boom of AIGC points the way towards the next-generation recommender paradigm with two new objectives: 1) generating personalized content through generative AI, and 2) integrating user instructions to guide content gene
&lt;/p&gt;</description></item><item><title>CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11916</link><description>&lt;p&gt;
CompoDiff: &#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#22810;&#21151;&#33021;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion. (arXiv:2303.11916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11916
&lt;/p&gt;
&lt;p&gt;
CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411; CompoDiff&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#65288;CIR&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001; 1800 &#19975;&#20010;&#21442;&#32771;&#22270;&#20687;&#12289;&#26465;&#20214;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#22270;&#20687;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;CompoDiff &#19981;&#20165;&#22312;&#20687; FashionIQ &#36825;&#26679;&#30340; CIR &#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#25509;&#25910;&#21508;&#31181;&#26465;&#20214;&#65288;&#22914;&#36127;&#25991;&#26412;&#21644;&#22270;&#20687;&#36974;&#32617;&#26465;&#20214;&#65289;&#65292;&#20351;&#24471; CIR &#26356;&#21152;&#22810;&#21151;&#33021;&#65292;&#36825;&#26159;&#29616;&#26377; CIR &#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;&#27492;&#22806;&#65292;CompoDiff &#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#12290;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#26435;&#37325;&#21487;&#22312; https://github.com/navervision/CompoDiff &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff not only achieves a new zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also enables a more versatile CIR by accepting various conditions, such as negative text and image mask conditions, which are unavailable with existing CIR methods. In addition, the CompoDiff features are on the intact CLIP embedding space so that they can be directly used for all existing models exploiting the CLIP space. The code and dataset used for the training, and the pre-trained weights are available at https://github.com/navervision/CompoDiff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;(BroadCF)&#65292;&#20351;&#29992;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;(BLS)&#20316;&#20026;&#26144;&#23556;&#20989;&#25968;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;BroadCF&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29992;&#25143;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.11602</link><description>&lt;p&gt;
&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach. (arXiv:2204.11602v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;(BroadCF)&#65292;&#20351;&#29992;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;(BLS)&#20316;&#20026;&#26144;&#23556;&#20989;&#25968;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;BroadCF&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29992;&#25143;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24341;&#20837;&#21040;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#20013;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25429;&#33719;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DNN&#30340;&#27169;&#22411;&#36890;&#24120;&#36973;&#21463;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#28040;&#32791;&#38750;&#24120;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#24182;&#23384;&#20648;&#22823;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;&#23485;&#27867;&#21327;&#21516;&#36807;&#28388;&#65288;BroadCF&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#34987;&#29992;&#20316;&#26144;&#23556;&#20989;&#25968;&#65292;&#20197;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#36991;&#20813;&#19978;&#36848;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#38750;&#24120;&#20196;&#20154;&#28385;&#24847;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#23558;&#21407;&#22987;&#35780;&#20998;&#25968;&#25454;&#30452;&#25509;&#39304;&#36865;&#21040;BLS&#20013;&#24182;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;BroadCF&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep Neural Networks (DNNs) have been widely introduced into Collaborative Filtering (CF) to produce more accurate recommendation results due to their capability of capturing the complex nonlinear relationships between items and users.However, the DNNs-based models usually suffer from high computational complexity, i.e., consuming very long training time and storing huge amount of trainable parameters. To address these problems, we propose a new broad recommender system called Broad Collaborative Filtering (BroadCF), which is an efficient nonlinear collaborative filtering approach. Instead of DNNs, Broad Learning System (BLS) is used as a mapping function to learn the complex nonlinear relationships between users and items, which can avoid the above issues while achieving very satisfactory recommendation performance. However, it is not feasible to directly feed the original rating data into BLS. To this end, we propose a user-item rating collaborative vector preprocessing pro
&lt;/p&gt;</description></item></channel></rss>