<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.11551</link><description>&lt;p&gt;
&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;VTR&#65289;&#26159;&#20114;&#32852;&#32593;&#19978;&#28023;&#37327;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#26102;&#20195;&#20013;&#19968;&#39033;&#20851;&#38190;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#20351;&#29992;&#21452;&#27969;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#35270;&#39057;&#25991;&#26412;&#23545;&#30340;&#32852;&#21512;&#34920;&#31034;&#25104;&#20026;VTR&#20219;&#21153;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20551;&#35774;&#35270;&#39057;&#25991;&#26412;&#23545;&#24212;&#26159;&#21452;&#23556;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#24573;&#35270;&#20102;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#35270;&#39057;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#22810;&#20010;&#20107;&#20214;&#65292;&#32780;&#29992;&#25143;&#26597;&#35810;&#25110;&#32593;&#39029;&#20803;&#25968;&#25454;&#31561;&#25991;&#26412;&#24448;&#24448;&#26159;&#20855;&#20307;&#30340;&#65292;&#24182;&#23545;&#24212;&#21333;&#20010;&#20107;&#20214;&#12290;&#36825;&#36896;&#25104;&#20102;&#20043;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#26089;&#26399;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#38024;&#23545;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#22330;&#26223;&#65292;&#20316;&#20026;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#19968;&#20010;&#21033;&#22522;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
&lt;/p&gt;</description></item><item><title>BERT4CTR&#26159;&#19968;&#31181;&#39640;&#25928;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23427;&#25506;&#32034;&#20102;&#20004;&#31181;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11527</link><description>&lt;p&gt;
BERT4CTR:&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11527
&lt;/p&gt;
&lt;p&gt;
BERT4CTR&#26159;&#19968;&#31181;&#39640;&#25928;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23427;&#25506;&#32034;&#20102;&#20004;&#31181;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#24037;&#19994;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#30410;&#65292;&#21253;&#25324;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#21482;&#22788;&#29702;&#25991;&#26412;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#20855;&#26377;&#38750;&#25991;&#26412;&#29305;&#24449;&#30340;&#39044;&#27979;&#27969;&#31243;&#30456;&#32467;&#21512;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#26377;&#20004;&#20010;&#26041;&#21521;&#26469;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#24182;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#19968;&#20010;&#26041;&#21521;&#26159;&#36890;&#36807;&#32858;&#21512;&#23618;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#38750;&#25991;&#26412;&#29305;&#24449;&#30340;&#32467;&#26524;&#36827;&#34892;&#34701;&#21512;&#65292;&#24418;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#20854;&#20013;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#20165;&#22312;&#32858;&#21512;&#23618;&#20013;&#23398;&#20064;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#26159;&#23558;&#38750;&#25991;&#26412;&#29305;&#24449;&#20998;&#21106;&#25104;&#32454;&#31890;&#24230;&#29255;&#27573;&#65292;&#24182;&#23558;&#36825;&#20123;&#29255;&#27573;&#36716;&#25442;&#20026;&#19982;&#25991;&#26412;&#29255;&#27573;&#30456;&#32467;&#21512;&#30340;&#26032;&#26631;&#35760;&#65292;&#20197;&#20415;&#21487;&#20197;&#30452;&#25509;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#23618;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#23398;&#20064;&#21644;&#25512;&#26029;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep pre-trained language models have shown promising benefit in a large set of industrial scenarios, including Click-Through-Rate (CTR) prediction, how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging.  Up to now two directions have been explored to integrate multi-modal inputs in fine-tuning of pre-trained language models. One consists of fusing the outcome of language models and non-textual features through an aggregation layer, resulting into ensemble framework, where the cross-information between textual and non-textual inputs are only learned in the aggregation layer. The second one consists of splitting non-textual features into fine-grained fragments and transforming the fragments to new tokens combined with textual ones, so that they can be fed directly to transformer layers in language models. However, this approach increases the complexity of the learning and inference becau
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#30340;&#23551;&#21629;&#23398;&#20064;&#65292;&#37325;&#28857;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26723;&#12290;&#22312;&#35813;&#35774;&#32622;&#19979;&#65292;&#20316;&#32773;&#26088;&#22312;&#36890;&#36807;&#27169;&#22411;&#26356;&#26032;&#36798;&#21040;&#20004;&#20010;&#30446;&#26631;&#65306;&#65288;1&#65289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26032;&#25991;&#26723;&#25968;&#25454;&#65292;&#65288;2&#65289;&#36991;&#20813;&#27599;&#27425;&#27169;&#22411;&#26356;&#26032;&#26102;&#37325;&#26032;&#35745;&#31639;&#25152;&#26377;&#26087;&#25991;&#26723;&#30340;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.11512</link><description>&lt;p&gt;
L^2R: &#23551;&#21629;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#21521;&#21518;&#20860;&#23481;&#34920;&#31034;&#30340;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
L^2R: Lifelong Learning for First-stage Retrieval with Backward-Compatible Representations. (arXiv:2308.11512v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#30340;&#23551;&#21629;&#23398;&#20064;&#65292;&#37325;&#28857;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26723;&#12290;&#22312;&#35813;&#35774;&#32622;&#19979;&#65292;&#20316;&#32773;&#26088;&#22312;&#36890;&#36807;&#27169;&#22411;&#26356;&#26032;&#36798;&#21040;&#20004;&#20010;&#30446;&#26631;&#65306;&#65288;1&#65289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26032;&#25991;&#26723;&#25968;&#25454;&#65292;&#65288;2&#65289;&#36991;&#20813;&#27599;&#27425;&#27169;&#22411;&#26356;&#26032;&#26102;&#37325;&#26032;&#35745;&#31639;&#25152;&#26377;&#26087;&#25991;&#26723;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#22823;&#35268;&#27169;&#30340;&#25991;&#26723;&#38598;&#21512;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#20505;&#36873;&#25991;&#26723;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#24573;&#35270;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;Web&#19978;&#30340;&#25968;&#25454;&#19981;&#26029;&#22686;&#38271;&#65292;&#24182;&#21487;&#33021;&#20986;&#29616;&#20998;&#24067;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#22312;&#38745;&#24577;&#26087;&#25968;&#25454;&#19978;&#30340;&#26816;&#32034;&#22120;&#21487;&#33021;&#19981;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#20110;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#30340;&#23551;&#21629;&#23398;&#20064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#26032;&#20986;&#29616;&#30340;&#25991;&#26723;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#30446;&#26631;&#65306;&#65288;1&#65289;&#26377;&#25928;&#36866;&#24212;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#19981;&#26029;&#21464;&#21270;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26032;&#25968;&#25454;&#65292;&#65288;2&#65289;&#36991;&#20813;&#37325;&#26032;&#25512;&#26029;&#25152;&#26377;&#26087;&#25991;&#26723;&#30340;&#23884;&#20837;&#20197;&#22312;&#27599;&#27425;&#27169;&#22411;&#26356;&#26032;&#26102;&#39640;&#25928;&#22320;&#26356;&#26032;&#32034;&#24341;&#12290;
&lt;/p&gt;
&lt;p&gt;
First-stage retrieval is a critical task that aims to retrieve relevant document candidates from a large-scale collection. While existing retrieval models have achieved impressive performance, they are mostly studied on static data sets, ignoring that in the real-world, the data on the Web is continuously growing with potential distribution drift. Consequently, retrievers trained on static old data may not suit new-coming data well and inevitably produce sub-optimal results. In this work, we study lifelong learning for first-stage retrieval, especially focusing on the setting where the emerging documents are unlabeled since relevance annotation is expensive and may not keep up with data emergence. Under this setting, we aim to develop model updating with two goals: (1) to effectively adapt to the evolving distribution with the unlabeled new-coming data, and (2) to avoid re-inferring all embeddings of old documents to efficiently update the index each time the model is updated.  We firs
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26041;&#38754;-&#20869;&#23481;&#25991;&#26412;&#30456;&#20114;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#20540;&#20043;&#38388;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11474</link><description>&lt;p&gt;
&#20351;&#29992;&#26041;&#38754;-&#20869;&#23481;&#25991;&#26412;&#30456;&#20114;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval. (arXiv:2308.11474v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26041;&#38754;-&#20869;&#23481;&#25991;&#26412;&#30456;&#20114;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#20540;&#20043;&#38388;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#24050;&#32463;&#23545;&#32431;&#25991;&#26412;&#19978;&#30340;&#23494;&#38598;&#26816;&#32034;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20351;&#29992;&#23494;&#38598;&#27169;&#22411;&#36827;&#34892;&#22810;&#26041;&#38754;&#25968;&#25454;&#26816;&#32034;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#22312;&#20135;&#21697;&#25628;&#32034;&#31561;&#22330;&#26223;&#20013;&#65292;&#26041;&#38754;&#20449;&#24687;&#22312;&#30456;&#20851;&#21305;&#37197;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20363;&#22914;&#31867;&#21035;&#65306;&#30005;&#23376;&#35774;&#22791;&#12289;&#35745;&#31639;&#26426;&#21644;&#23456;&#29289;&#29992;&#21697;&#31561;&#12290;&#21033;&#29992;&#26041;&#38754;&#20449;&#24687;&#36827;&#34892;&#22810;&#26041;&#38754;&#26816;&#32034;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#24341;&#20837;&#36741;&#21161;&#20998;&#31867;&#30446;&#26631;&#65292;&#21363;&#20351;&#29992;&#29289;&#21697;&#20869;&#23481;&#39044;&#27979;&#29289;&#21697;&#26041;&#38754;&#30340;&#27880;&#37322;&#20540;ID&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20174;&#22836;&#23398;&#20064;&#20540;&#23884;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#20540;&#20043;&#38388;&#30340;&#21508;&#31181;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#26041;&#38754;&#20449;&#24687;&#20316;&#20026;&#25991;&#26412;&#23383;&#31526;&#20018;&#32780;&#19981;&#26159;&#31867;&#21035;ID&#65292;&#20197;&#20415;&#22312;PLMs&#20013;&#33258;&#28982;&#22320;&#25429;&#25417;&#23427;&#20204;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20415;&#20110;&#20351;&#29992;&#26041;&#38754;&#23383;&#31526;&#20018;&#36827;&#34892;&#26377;&#25928;&#30340;&#26816;&#32034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounded on pre-trained language models (PLMs), dense retrieval has been studied extensively on plain text. In contrast, there has been little research on retrieving data with multiple aspects using dense models. In the scenarios such as product search, the aspect information plays an essential role in relevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A common way of leveraging aspect information for multi-aspect retrieval is to introduce an auxiliary classification objective, i.e., using item contents to predict the annotated value IDs of item aspects. However, by learning the value embeddings from scratch, this approach may not capture the various semantic similarities between the values sufficiently. To address this limitation, we leverage the aspect information as text strings rather than class IDs during pre-training so that their semantic similarities can be naturally captured in the PLMs. To facilitate effective retrieval with the aspect strings, we p
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20174;&#32447;&#19979;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#24212;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.11336</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. (arXiv:2308.11336v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11336
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20174;&#32447;&#19979;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#24212;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24314;&#27169;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36817;&#24180;&#26469;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#65306;&#22240;&#20026;&#20854;&#20132;&#20114;&#24615;&#65292;&#20854;&#25968;&#25454;&#25928;&#29575;&#36739;&#20302;&#12290;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#38656;&#35201;&#26114;&#36149;&#30340;&#22312;&#32447;&#20132;&#20114;&#26469;&#31215;&#32047;&#36275;&#22815;&#30340;&#36712;&#36857;&#65292;&#36825;&#23545;&#20110;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#20302;&#25928;&#24615;&#20351;&#24471;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25104;&#20026;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#25506;&#32034;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#35265;&#35299;&#65292;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#12290;&#37492;&#20110;&#25512;&#33616;&#31995;&#32479;&#25317;&#26377;&#24191;&#27867;&#30340;&#32447;&#19979;&#25968;&#25454;&#38598;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19982;&#20043;&#32039;&#23494;&#30456;&#31526;&#12290;&#23613;&#31649;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#20294;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#26524;&#36880;&#28176;&#22686;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28909;&#38376;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11288</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#23545;&#28909;&#38376;&#20559;&#35265;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Test Time Embedding Normalization for Popularity Bias Mitigation. (arXiv:2308.11288v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28909;&#38376;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#38376;&#20559;&#35265;&#26159;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28909;&#38376;&#29289;&#21697;&#20542;&#21521;&#20110;&#20027;&#23548;&#25512;&#33616;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#28909;&#38376;&#20559;&#35265;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#38454;&#27573;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#30340;&#22823;&#23567;&#65292;&#32780;&#23884;&#20837;&#30340;&#22823;&#23567;&#19982;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#39640;&#24230;&#30456;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#37319;&#26679;softmax&#25439;&#22833;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21487;&#20197;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#27969;&#34892;&#31243;&#24230;&#12290;&#36825;&#19968;&#20998;&#26512;&#35299;&#37322;&#20102;&#25105;&#20204;&#26041;&#27861;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11175</link><description>&lt;p&gt;
MISSRec: &#38754;&#21521;&#25512;&#33616;&#30340;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#22810;&#27169;&#24577;&#20852;&#36259;&#24863;&#30693;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation. (arXiv:2308.11175v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#24207;&#21015;&#39044;&#27979;&#20854;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24207;&#21015;&#25512;&#33616;&#22120;&#26159;&#22522;&#20110;ID&#29305;&#24449;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#20351;&#29992;&#31232;&#30095;ID&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19981;&#19968;&#33268;&#30340;ID&#26144;&#23556;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#25512;&#33616;&#39046;&#22495;&#26080;&#27861;&#36827;&#34892;&#20849;&#21516;&#20248;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MISSRec&#65292;&#19968;&#31181;&#38754;&#21521;SR&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#29992;&#25143;&#31471;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#23398;&#20064;&#25429;&#25417;&#24207;&#21015;&#32423;&#30340;&#22810;&#27169;&#24577;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;&#26032;&#39062;&#30340;&#20852;&#36259;&#24863;&#30693;&#35299;&#30721;&#22120;&#21017;&#29992;&#20110;&#25226;&#25569;&#29289;&#21697;-&#27169;&#24577;-&#20852;&#36259;&#20851;&#31995;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of sequential recommendation (SR) is to predict a user's potential interested items based on her/his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model's transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence represent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#30740;&#31350;&#20102;&#20351;&#29992;&#35780;&#35770;&#25968;&#25454;&#38598;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#36138;&#23146;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11137</link><description>&lt;p&gt;
&#38754;&#21521;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#38271;&#26399;&#29992;&#25143;&#21453;&#39304;&#39564;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems. (arXiv:2308.11137v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#30740;&#31350;&#20102;&#20351;&#29992;&#35780;&#35770;&#25968;&#25454;&#38598;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#36138;&#23146;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;IRS&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#27169;&#25311;&#29992;&#25143;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#36807;&#31243;&#12290;&#35768;&#22810;&#26041;&#27861;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;IRS&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20351;&#29992;&#20844;&#24320;&#30340;&#35780;&#35770;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#21644;&#35780;&#20272;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#29992;&#25143;&#21453;&#39304;&#21482;&#21253;&#25324;&#21363;&#26102;&#21453;&#24212;&#65288;&#20363;&#22914;&#65292;&#35780;&#20998;&#65289;&#65292;&#27809;&#26377;&#21253;&#25324;&#24310;&#36831;&#21453;&#24212;&#65288;&#20363;&#22914;&#20572;&#30041;&#26102;&#38388;&#21644;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65289;&#12290;&#22240;&#27492;&#65292;&#38382;&#39064;&#22312;&#20110;&#36825;&#20123;&#35780;&#35770;&#25968;&#25454;&#38598;&#26159;&#21542;&#36866;&#21512;&#35780;&#20272;IRS&#30340;&#38271;&#26399;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#30740;&#31350;&#20102;&#20351;&#29992;&#35780;&#35770;&#25968;&#25454;&#38598;&#30340;IRS&#23454;&#39564;&#65292;&#24182;&#23558;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#19982;&#19968;&#31181;&#31616;&#21333;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21518;&#32773;&#20197;&#36138;&#23146;&#30340;&#26041;&#24335;&#25512;&#33616;&#20855;&#26377;&#26368;&#39640;&#21333;&#27493;&#22870;&#21169;&#30340;&#39033;&#30446;&#12290;&#32463;&#36807;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#65306;&#39318;&#20808;&#65292;&#31616;&#21333;&#30340;&#36138;&#23146;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Recommender Systems (IRSs) have attracted a lot of attention, due to their ability to model interactive processes between users and recommender systems. Numerous approaches have adopted Reinforcement Learning (RL) algorithms, as these can directly maximize users' cumulative rewards. In IRS, researchers commonly utilize publicly available review datasets to compare and evaluate algorithms. However, user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value). Thus, the question remains whether these review datasets are an appropriate choice to evaluate the long-term effects of the IRS. In this work, we revisited experiments on IRS with review datasets and compared RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Following extensive analysis, we can reveal three main findings: First, a simple greedy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11127</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21033;&#29992;&#22270;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21327;&#20316;&#36807;&#28388;&#20449;&#21495;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#29702;&#35770;&#34920;&#36848;&#38750;&#24120;&#31232;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;GNNs&#30340;&#19968;&#33324;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;GNNs&#33267;&#22810;&#19982;Weisfeiler-Lehman&#27979;&#35797;&#19968;&#26679;&#24378;&#22823;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#33410;&#28857;&#21021;&#22987;&#21270;&#30456;&#32467;&#21512;&#30340;GNNs&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#8220;&#34920;&#36798;&#33021;&#21147;&#8221;&#27010;&#24565;&#20173;&#28982;&#23450;&#20041;&#27169;&#31946;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#22270;&#21516;&#26500;&#27979;&#35797;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#31181;&#22270;&#32423;&#20219;&#21153;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#21306;&#20998;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#38598;&#24182;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#32858;&#31867;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10999</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Eigenvalue-based Incremental Spectral Clustering. (arXiv:2308.10999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#38598;&#24182;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#32858;&#31867;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20043;&#21069;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#65288;&#30701;&#65289;&#25991;&#26723;&#30340;&#23376;&#38598;&#21512;&#65288;&#21253;&#21547;&#20960;&#30334;&#20010;&#26465;&#30446;&#65289;&#22312;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#20540;&#35889;&#19978;&#26377;&#20849;&#21516;&#30340;&#24402;&#19968;&#21270;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35889;&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#65306;&#65288;1&#65289;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#21487;&#31649;&#29702;&#30340;&#23376;&#38598;&#65292;&#65288;2&#65289;&#23545;&#27599;&#20010;&#23376;&#38598;&#36827;&#34892;&#32858;&#31867;&#65292;&#65288;3&#65289;&#22522;&#20110;&#29305;&#24449;&#20540;&#35889;&#30340;&#30456;&#20284;&#24615;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#23376;&#38598;&#30340;&#32858;&#31867;&#65292;&#24418;&#25104;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#26679;&#26412;&#37327;&#22823;&#23567;&#21457;&#29983;&#24378;&#28872;&#21464;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20363;&#22914;&#20856;&#22411;&#30340;&#35889;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#38469;&#19978;&#23545;&#23376;&#38598;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#21487;&#20197;&#24471;&#21040;&#19982;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#30456;&#36817;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
&lt;/p&gt;</description></item><item><title>DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10807</link><description>&lt;p&gt;
DynED: &#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10807
&lt;/p&gt;
&lt;p&gt;
DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#24615;&#21464;&#21270;&#65292;&#20063;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290; &#22312;&#38598;&#21512;&#20869;&#37096;&#30340;&#26356;&#22823;&#22810;&#26679;&#24615;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#38598;&#21512;&#20869;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#24456;&#39640;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#32452;&#20214;&#37117;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#23545;&#25972;&#20307;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MMR&#65288;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65289;&#30340;&#26032;&#22411;&#38598;&#21512;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#65292;&#22312;&#32452;&#21512;&#38598;&#21512;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#22320;&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#21644;11&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65288;DynED&#65289;&#30456;&#27604;&#20110;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03881</link><description>&lt;p&gt;
&#22270;&#20687;&#25628;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#20851;&#20110;&#20174;&#22270;&#20687;&#26816;&#32034;&#19982;&#21435;&#20559;&#35265;&#35282;&#24230;&#25506;&#31350;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25628;&#32034;&#24341;&#25806;&#36817;&#24180;&#26469;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#21644;&#24191;&#27867;&#30340;&#20351;&#29992;&#65292;&#25104;&#20026;&#32487;&#20449;&#24687;&#26816;&#32034;&#20043;&#21518;&#31532;&#20108;&#24120;&#35265;&#30340;&#20114;&#32852;&#32593;&#20351;&#29992;&#26041;&#24335;&#12290;&#23613;&#31649;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#65292;&#20294;&#22270;&#20687;&#25628;&#32034;&#39046;&#22495;&#26368;&#36817;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#30340;&#28966;&#28857;&#65292;&#22240;&#20026;&#24120;&#35328;&#36947;&#8220;&#19968;&#22270;&#32988;&#21315;&#35328;&#8221;&#12290;&#34429;&#28982;&#20687;&#35895;&#27468;&#36825;&#26679;&#30340;&#27969;&#34892;&#25628;&#32034;&#24341;&#25806;&#22312;&#22270;&#20687;&#25628;&#32034;&#31934;&#24230;&#21644;&#25935;&#25463;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25628;&#32034;&#32467;&#26524;&#26159;&#21542;&#20250;&#23384;&#22312;&#24615;&#21035;&#12289;&#35821;&#35328;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#25991;&#21270;&#26041;&#38754;&#30340;&#20559;&#35265;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#31181;&#28508;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#30340;&#35748;&#30693;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#24433;&#21709;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20960;&#31181;&#20559;&#35265;&#31867;&#22411;&#20197;&#21450;&#20026;&#20160;&#20040;&#26377;&#24517;&#35201;&#21152;&#20197;&#32531;&#35299;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#37325;&#28857;&#32553;&#23567;&#21040;&#35780;&#20272;&#21644;&#32531;&#35299;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#21487;&#33021;&#23545;&#20010;&#20154;&#21644;&#25972;&#20010;&#31038;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal search engines have experienced significant growth and widespread use in recent years, making them the second most common internet use. While search engine systems offer a range of services, the image search field has recently become a focal point in the information retrieval community, as the adage goes, "a picture is worth a thousand words". Although popular search engines like Google excel at image search accuracy and agility, there is an ongoing debate over whether their search results can be biased in terms of gender, language, demographics, socio-cultural aspects, and stereotypes. This potential for bias can have a significant impact on individuals' perceptions and influence their perspectives.  In this paper, we present our study on bias and fairness in web search, with a focus on keyword-based image search. We first discuss several kinds of biases that exist in search systems and why it is important to mitigate them. We narrow down our study to assessing and mitigat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#24402;&#20860;&#23481;&#30340;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22238;&#24402;&#21644;&#25490;&#24207;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23610;&#24230;&#26657;&#20934;&#30340;&#20998;&#25968;&#12290;&#22312;&#20108;&#36827;&#21046;&#26631;&#31614;&#30340;LTR&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#21644;&#25490;&#24207;&#24230;&#37327;&#26041;&#38754; consistently achieves either best or competitive result&#65292;&#24182;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.01494</link><description>&lt;p&gt;
&#22238;&#24402;&#20860;&#23481;&#30340;&#21015;&#34920;&#24335;&#30446;&#26631;&#20989;&#25968;&#29992;&#20110;&#20855;&#26377;&#20108;&#36827;&#21046;&#30456;&#20851;&#24615;&#30340;&#26657;&#20934;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Regression Compatible Listwise Objectives for Calibrated Ranking with Binary Relevance. (arXiv:2211.01494v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#24402;&#20860;&#23481;&#30340;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22238;&#24402;&#21644;&#25490;&#24207;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23610;&#24230;&#26657;&#20934;&#30340;&#20998;&#25968;&#12290;&#22312;&#20108;&#36827;&#21046;&#26631;&#31614;&#30340;LTR&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#21644;&#25490;&#24207;&#24230;&#37327;&#26041;&#38754; consistently achieves either best or competitive result&#65292;&#24182;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65288;LTR&#65289;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#25490;&#24207;&#36136;&#37327;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#20998;&#25968;&#35774;&#35745;&#19978;&#24182;&#19981;&#26159;&#32463;&#36807;&#23610;&#24230;&#26657;&#20934;&#30340;&#12290;&#36825;&#20174;&#26681;&#26412;&#19978;&#38480;&#21046;&#20102;LTR&#22312;&#23545;&#20998;&#25968;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#34429;&#28982;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#30446;&#26631;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32452;&#21512;&#22238;&#24402;&#21644;&#25490;&#24207;&#30446;&#26631;&#26469;&#23398;&#20064;&#32463;&#36807;&#23610;&#24230;&#26657;&#20934;&#30340;&#20998;&#25968;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#30446;&#26631;&#19981;&#19968;&#23450;&#20860;&#23481;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#23545;&#20110;&#20219;&#19968;&#20010;&#37117;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22238;&#24402;&#20860;&#23481;&#25490;&#24207;&#65288;RCR&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#65292;&#20854;&#20013;&#35777;&#26126;&#20102;&#20004;&#20010;&#25490;&#24207;&#32452;&#20214;&#21644;&#22238;&#24402;&#32452;&#20214;&#26159;&#30456;&#20114;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#30456;&#21516;&#30340;&#24605;&#36335;&#36866;&#29992;&#20110;&#20855;&#26377;&#20108;&#36827;&#21046;&#21644;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#25490;&#24207;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20108;&#36827;&#21046;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20844;&#20849;LTR&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22238;&#24402;&#21644;&#25490;&#24207;&#24230;&#37327;&#26041;&#38754; consistently achieves either best or competitive result, and significantl&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#22238;&#24402;&#21644;&#25490;&#24207;&#24230;&#37327;&#26041;&#38754; consistently achieves either best or competitive result&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;, and significantly improved the state-of-the-art&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Learning-to-Rank (LTR) approaches primarily seek to improve ranking quality, their output scores are not scale-calibrated by design. This fundamentally limits LTR usage in score-sensitive applications. Though a simple multi-objective approach that combines a regression and a ranking objective can effectively learn scale-calibrated scores, we argue that the two objectives are not necessarily compatible, which makes the trade-off less ideal for either of them. In this paper, we propose a practical regression compatible ranking (RCR) approach that achieves a better trade-off, where the two ranking and regression components are proved to be mutually aligned. Although the same idea applies to ranking with both binary and graded relevance, we mainly focus on binary labels in this paper. We evaluate the proposed approach on several public LTR benchmarks and show that it consistently achieves either best or competitive result in terms of both regression and ranking metrics, and significantl
&lt;/p&gt;</description></item></channel></rss>