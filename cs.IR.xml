<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>AgentCF &#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#27169;&#25311;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#36825;&#20004;&#31867;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.09233</link><description>&lt;p&gt;
AgentCF: &#22522;&#20110;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21327;&#20316;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems. (arXiv:2310.09233v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09233
&lt;/p&gt;
&lt;p&gt;
AgentCF &#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#27169;&#25311;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#36825;&#20004;&#31867;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#20855;&#26377;&#20986;&#33394;&#20915;&#31574;&#33021;&#21147;&#30340;LLM&#65288;&#35821;&#35328;&#28151;&#21512;&#27169;&#22411;&#65289;&#20195;&#29702;&#20316;&#20026;&#21487;&#20449;&#30340;&#20154;&#31867;&#20195;&#29702;&#20986;&#29616;&#20102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#19978;&#12290;&#20154;&#31867;&#30340;&#38750;&#35821;&#35328;&#34892;&#20026;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#28857;&#20987;&#65292;&#34429;&#28982;&#38544;&#21547;&#30528;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#33021;&#25552;&#21319;&#29992;&#25143;&#24314;&#27169;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#35821;&#35328;&#24314;&#27169;&#19982;&#34892;&#20026;&#24314;&#27169;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;LLMs&#23545;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#30340;&#19981;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentCF&#65292;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#30340;&#21327;&#21516;&#36807;&#28388;&#26469;&#27169;&#25311;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#32771;&#34385;&#29992;&#25143;&#21644;&#29289;&#21697;&#20316;&#20026;&#20195;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20004;&#31181;&#20195;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#25105;&#20204;&#39318;&#20808;&#20419;&#20351;&#29992;&#25143;&#20195;&#29702;&#21644;&#29289;&#21697;&#20195;&#29702;&#33258;&#20027;&#22320;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#24046;&#24322;&#24615;&#23545;&#36825;&#20004;&#31867;&#20195;&#29702;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations.  To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities b
&lt;/p&gt;</description></item><item><title>EHI&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23618;&#27425;&#32034;&#24341;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#12290;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08891</link><description>&lt;p&gt;
EHI: &#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#30340;&#23618;&#27425;&#32034;&#24341;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval. (arXiv:2310.08891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08891
&lt;/p&gt;
&lt;p&gt;
EHI&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23618;&#27425;&#32034;&#24341;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#12290;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#23884;&#20837;&#24335;&#26816;&#32034;&#29616;&#24050;&#25104;&#20026;&#35821;&#20041;&#25628;&#32034;&#21644;&#25490;&#21517;&#38382;&#39064;&#30340;&#34892;&#19994;&#26631;&#20934;&#65292;&#22914;&#33719;&#21462;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#32593;&#32476;&#25991;&#26723;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;(a)&#23545;&#27604;&#23398;&#20064;&#26469;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120;&#20197;&#23884;&#20837;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#20197;&#21450;(b)&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#20197;&#26597;&#25214;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20284;&#25991;&#26723;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#26159;&#19981;&#30456;&#20132;&#30340;&#65307;&#23398;&#24471;&#30340;&#23884;&#20837;&#21487;&#33021;&#19981;&#36866;&#21512;ANNS&#26041;&#27861;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31471;&#21040;&#31471;&#23618;&#27425;&#32034;&#24341;(EHI)&#30340;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;EHI&#20351;&#29992;&#26631;&#20934;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#23884;&#20837;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#20498;&#25490;&#25991;&#20214;&#32034;&#24341;(IVF)&#39118;&#26684;&#30340;&#26641;&#29366;&#32467;&#26500;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;ANNS&#12290;&#20026;&#20102;&#30830;&#20445;&#31163;&#25955;&#22522;&#20110;&#26641;&#30340;ANNS&#32467;&#26500;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#23398;&#20064;&#65292;EHI&#24341;&#20837;&#20102;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#30340;&#27010;&#24565;&#65292;&#29992;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#19982;&#20854;&#20182;&#21307;&#23398;QA&#20219;&#21153;&#19981;&#21516;&#65292;EHR QA&#36890;&#36807;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#21462;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;EHR QA&#20316;&#21697;&#25552;&#20379;&#20102;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2310.08759</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#38382;&#39064;&#22238;&#31572;&#65306;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#33539;&#22260;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Question Answering for Electronic Health Records: A Scoping Review of datasets and models. (arXiv:2310.08759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#19982;&#20854;&#20182;&#21307;&#23398;QA&#20219;&#21153;&#19981;&#21516;&#65292;EHR QA&#36890;&#36807;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#21462;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;EHR QA&#20316;&#21697;&#25552;&#20379;&#20102;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#24739;&#32773;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#12290;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20570;&#20915;&#31574;&#65292;&#24182;&#20351;&#24739;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20182;&#20204;&#30340;&#30149;&#21382;&#12290;&#22823;&#37327;&#30340;&#24739;&#32773;&#25968;&#25454;&#23384;&#20648;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#65292;&#20351;&#24471;EHR QA&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;EHR QA&#20013;&#65292;&#31572;&#26696;&#26159;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#24471;&#30340;&#12290;&#30001;&#20110;&#25968;&#25454;&#26684;&#24335;&#21644;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#36825;&#19982;&#20854;&#20182;&#20351;&#29992;&#21307;&#23398;&#32593;&#31449;&#25110;&#31185;&#23398;&#35770;&#25991;&#26816;&#32034;&#31572;&#26696;&#30340;&#21307;&#23398;QA&#20219;&#21153;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;EHR&#38382;&#39064;&#22238;&#31572;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#29616;&#26377;&#20851;&#20110;EHR QA&#30340;&#20316;&#21697;&#36827;&#34892;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Google Scholar&#12289;ACL Anthology&#12289;ACM Digital Library&#21644;PubMed&#22312;&#20869;&#30340;&#22235;&#20010;&#25968;&#23383;&#36164;&#28304;&#20013;&#25628;&#32034;&#20102;&#20174;2005&#24180;1&#26376;1&#26085;&#21040;2023&#24180;9&#26376;30&#26085;&#30340;&#25991;&#31456;&#65292;&#20197;&#25910;&#38598;&#26377;&#20851;EHR QA&#30340;&#30456;&#20851;&#20986;&#29256;&#29289;&#12290;&#20849;&#21457;&#29616;&#20102;4111&#31687;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) systems on patient-related data can assist both clinicians and patients. They can, for example, assist clinicians in decision-making and enable patients to have a better understanding of their medical history. Significant amounts of patient data are stored in Electronic Health Records (EHRs), making EHR QA an important research area. In EHR QA, the answer is obtained from the medical record of the patient. Because of the differences in data format and modality, this differs greatly from other medical QA tasks that employ medical websites or scientific papers to retrieve answers, making it critical to research EHR question answering. This study aimed to provide a methodological review of existing works on QA over EHRs. We searched for articles from January 1st, 2005 to September 30th, 2023 in four digital sources including Google Scholar, ACL Anthology, ACM Digital Library, and PubMed to collect relevant publications on EHR QA. 4111 papers were identified for our
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#25193;&#23637;&#30340;&#22810;&#30149;&#21407;SIR&#27169;&#22411;&#65292;&#32771;&#34385;&#20010;&#20307;&#24046;&#24322;&#21644;&#40509;&#23376;&#30340;&#31227;&#21160;&#21160;&#24577;&#65292;&#22312;&#40509;&#23376;&#35775;&#38382;&#22902;&#29275;&#22330;&#30340;&#36807;&#31243;&#20013;&#30740;&#31350;&#20102;&#30123;&#24773;&#30340;&#35268;&#27169;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20943;&#36731;&#20892;&#19994;&#29615;&#22659;&#20013;&#30142;&#30149;&#20256;&#25773;&#30340;&#39118;&#38505;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.08613</link><description>&lt;p&gt;
&#20010;&#20307;&#24046;&#24322;&#24433;&#21709;&#20102;&#40509;&#23376;&#35775;&#38382;&#22902;&#29275;&#22330;&#30340;&#25193;&#23637;&#22810;&#30149;&#21407;SIR&#27169;&#22411;&#20013;&#30340;&#30123;&#24773;&#35268;&#27169;&#21644;&#21487;&#39044;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
Individual Variation Affects Outbreak Magnitude and Predictability in an Extended Multi-Pathogen SIR Model of Pigeons Vising Dairy Farms. (arXiv:2310.08613v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#25193;&#23637;&#30340;&#22810;&#30149;&#21407;SIR&#27169;&#22411;&#65292;&#32771;&#34385;&#20010;&#20307;&#24046;&#24322;&#21644;&#40509;&#23376;&#30340;&#31227;&#21160;&#21160;&#24577;&#65292;&#22312;&#40509;&#23376;&#35775;&#38382;&#22902;&#29275;&#22330;&#30340;&#36807;&#31243;&#20013;&#30740;&#31350;&#20102;&#30123;&#24773;&#30340;&#35268;&#27169;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20943;&#36731;&#20892;&#19994;&#29615;&#22659;&#20013;&#30142;&#30149;&#20256;&#25773;&#30340;&#39118;&#38505;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#20154;&#30044;&#20849;&#24739;&#30149;&#20256;&#25773;&#39118;&#38505;&#26085;&#30410;&#22686;&#21152;&#65292;&#20892;&#19994;&#29615;&#22659;&#20316;&#20026;&#20256;&#25773;&#30340;&#21487;&#33021;&#28857;&#65292;&#20010;&#20307;&#24046;&#24322;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#35201;&#32032;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#37326;&#29983;&#21160;&#29289;&#21644;&#30044;&#29287;&#19994;&#30028;&#38754;&#19978;&#30142;&#30149;&#20256;&#25773;&#30340;&#21160;&#24577;&#23545;&#20110;&#20943;&#36731;&#36825;&#20123;&#20256;&#25773;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#40509;&#23376;&#19982;&#22902;&#29275;&#22312;&#22902;&#29275;&#22330;&#20869;&#30340;&#30456;&#20114;&#20316;&#29992;&#20250;&#23548;&#33268;&#37325;&#22823;&#30340;&#30142;&#30149;&#20256;&#25773;&#21644;&#20892;&#27665;&#30340;&#32463;&#27982;&#25439;&#22833;&#65292;&#20174;&#32780;&#35753;&#30044;&#29287;&#21160;&#29289;&#12289;&#30456;&#37051;&#20154;&#21475;&#21644;&#20854;&#20182;&#37326;&#29983;&#21160;&#29289;&#29289;&#31181;&#38754;&#20020;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#22810;&#30149;&#21407;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36830;&#32493;&#30340;&#31354;&#38388;&#31227;&#21160;&#12290;&#35813;&#27169;&#22411;&#22312;&#26131;&#24863;-&#26292;&#38706;-&#24863;&#26579;-&#24247;&#22797;-&#27515;&#20129;&#65288;SEIRD&#65289;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#24182;&#32771;&#34385;&#20102;&#30149;&#21407;&#20307;&#30340;&#31181;&#20869;&#21644;&#31181;&#38388;&#20256;&#25773;&#65292;&#20197;&#21450;&#40509;&#23376;&#22312;&#24863;&#26579;&#20256;&#25773;&#20013;&#25198;&#28436;&#30340;&#25506;&#32034;-&#21033;&#29992;&#31227;&#21160;&#21160;&#24577;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zoonotic disease transmission between animals and humans is a growing risk and the agricultural context acts as a likely point of transition, with individual heterogeneity acting as an important contributor. Thus, understanding the dynamics of disease spread in the wildlife-livestock interface is crucial for mitigating these risks of transmission. Specifically, the interactions between pigeons and in-door cows at dairy farms can lead to significant disease transmission and economic losses for farmers; putting livestock, adjacent human populations, and other wildlife species at risk. In this paper, we propose a novel spatio-temporal multi-pathogen model with continuous spatial movement. The model expands on the Susceptible-Exposed-Infected-Recovered-Dead (SEIRD) framework and accounts for both within-species and cross-species transmission of pathogens, as well as the exploration-exploitation movement dynamics of pigeons, which play a critical role in the spread of infection agents. In a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#20020;&#24202;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#39044;&#27979;&#32954;&#30284;&#36716;&#31227;&#30340;&#31354;&#38388;&#25193;&#25955;&#65292;&#39564;&#35777;&#20102;&#22312;&#36716;&#31227;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#20986;&#20102;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#22312;&#32954;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08596</link><description>&lt;p&gt;
&#39044;&#27979;&#32954;&#30284;&#36716;&#31227;&#20301;&#32622;&#30340;&#29983;&#29289;&#20020;&#24202;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting Lung Cancer's Metastats' Locations Using Bioclinical Model. (arXiv:2310.08596v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#20020;&#24202;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#39044;&#27979;&#32954;&#30284;&#36716;&#31227;&#30340;&#31354;&#38388;&#25193;&#25955;&#65292;&#39564;&#35777;&#20102;&#22312;&#36716;&#31227;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#20986;&#20102;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#22312;&#32954;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#30142;&#30149;&#20174;&#21407;&#21457;&#37096;&#20301;&#25193;&#25955;&#21040;&#32954;&#37096;&#20854;&#20182;&#37096;&#20301;&#65292;&#21363;&#36716;&#31227;&#65292;&#23545;&#27835;&#30103;&#36807;&#31243;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#21450;&#26089;&#35782;&#21035;&#36716;&#31227;&#30149;&#28790;&#23545;&#20110;&#21450;&#26102;&#26377;&#25928;&#30340;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#24433;&#20687;&#25216;&#26415;&#22312;&#26816;&#27979;&#23567;&#30340;&#36716;&#31227;&#30149;&#28790;&#19978;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#20020;&#24202;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#39044;&#27979;&#32954;&#30284;&#36716;&#31227;&#30340;&#31354;&#38388;&#25193;&#25955;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19977;&#23618;&#29983;&#29289;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#36716;&#31227;&#32467;&#33410;&#39640;&#27010;&#29575;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;10&#21517;&#24739;&#32773;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#29983;&#29289;&#20020;&#24202;&#27169;&#22411;&#65292;&#34920;&#26126;&#22312;&#36716;&#31227;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;74%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#22312;&#32954;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a leading cause of cancer-related deaths worldwide. The spread of the disease from its primary site to other parts of the lungs, known as metastasis, significantly impacts the course of treatment. Early identification of metastatic lesions is crucial for prompt and effective treatment, but conventional imaging techniques have limitations in detecting small metastases. In this study, we develop a bioclinical model for predicting the spatial spread of lung cancer's metastasis using a three-dimensional computed tomography (CT) scan. We used a three-layer biological model of cancer spread to predict locations with a high probability of metastasis colonization. We validated the bioclinical model on real-world data from 10 patients, showing promising 74% accuracy in the metastasis location prediction. Our study highlights the potential of the combination of biophysical and ML models to advance the way that lung cancer is diagnosed and treated, by providing a more comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#21644;&#28388;&#27874;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#21644;&#32771;&#34385;&#21464;&#24418;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15010</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#27169;&#26495;&#21305;&#37197;&#20013;&#30340;&#39640;&#25928;&#21521;&#37327;&#37327;&#21270;&#26368;&#36817;&#37051;&#22330;
&lt;/p&gt;
&lt;p&gt;
Efficient High-Resolution Template Matching with Vector Quantized Nearest Neighbour Fields. (arXiv:2306.15010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#21644;&#28388;&#27874;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#21644;&#32771;&#34385;&#21464;&#24418;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#26495;&#21305;&#37197;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#24182;&#22312;&#29289;&#20307;&#26816;&#27979;&#12289;&#22270;&#20687;&#37197;&#20934;&#21644;&#29289;&#20307;&#36319;&#36394;&#31561;&#39046;&#22495;&#26377;&#24212;&#29992;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#20381;&#36182;&#20110;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#21305;&#37197;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#23558;&#26597;&#35810;&#29305;&#24449;&#31354;&#38388;&#36716;&#25442;&#20026;NN&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#26597;&#35810;&#20687;&#32032;&#29992;&#27169;&#26495;&#20687;&#32032;&#20013;&#30340;&#26368;&#36817;&#37051;&#34920;&#31034;&#12290;NN&#21305;&#37197;&#22312;&#36974;&#25377;&#12289;&#22806;&#35266;&#21464;&#21270;&#12289;&#20809;&#29031;&#21464;&#21270;&#21644;&#38750;&#21018;&#24615;&#21464;&#25442;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;NN&#21305;&#37197;&#22312;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#21644;&#39640;&#32500;&#29305;&#24449;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NN&#30340;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;NN&#35745;&#31639;&#37327;&#65292;&#24182;&#22312;NN&#22330;&#20013;&#24341;&#20837;&#28388;&#27874;&#20197;&#32771;&#34385;&#21464;&#24418;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#23558;&#27169;&#26495;&#34920;&#31034;&#20026;k&#20010;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#28388;&#27874;&#27604;&#36739;&#27169;&#26495;&#21644;&#26597;&#35810;&#22312;k&#20010;&#29305;&#24449;&#19978;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Template matching is a fundamental problem in computer vision and has applications in various fields, such as object detection, image registration, and object tracking. The current state-of-the-art methods rely on nearest-neighbour (NN) matching in which the query feature space is converted to NN space by representing each query pixel with its NN in the template pixels. The NN-based methods have been shown to perform better in occlusions, changes in appearance, illumination variations, and non-rigid transformations. However, NN matching scales poorly with high-resolution data and high feature dimensions. In this work, we present an NN-based template-matching method which efficiently reduces the NN computations and introduces filtering in the NN fields to consider deformations. A vector quantization step first represents the template with $k$ features, then filtering compares the template and query distributions over the $k$ features. We show that state-of-the-art performance was achiev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#21644;reranker&#27169;&#22411;&#65292;&#33976;&#39311;&#20026;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.00807</link><description>&lt;p&gt;
UDAPDR: &#22522;&#20110;LLM&#25552;&#31034;&#19982;reranker&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. (arXiv:2303.00807v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#21644;reranker&#27169;&#22411;&#65292;&#33976;&#39311;&#20026;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#38656;&#35201;&#22823;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#19988;&#22312;&#24212;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#20013;&#26102;&#21487;&#33021;&#20250;&#22240;&#20026;&#39046;&#22495;&#28418;&#31227;&#32780;&#36805;&#36895;&#22833;&#21435;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24265;&#20215;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#26114;&#36149;&#30340;LLM&#29983;&#25104;&#23569;&#37327;&#21512;&#25104;&#26597;&#35810;&#65292;&#28982;&#21518;&#20877;&#21033;&#29992;&#25104;&#26412;&#36739;&#20302;&#30340;LLM&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#26597;&#35810;&#20197;&#24494;&#35843;&#19968;&#32452;reranker&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;reranker&#20250;&#34987;&#33976; distill &#25104;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#38271;&#23614;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;2K&#20010;&#21512;&#25104;&#26597;&#35810;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#30340;reranking&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20379;&#23436;&#25972;&#30340;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21253;&#25324;&#21512;&#25104;&#25968;&#25454;&#38598;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains, even where only 2K synthetic queries are used for fine-tuning, and that it achieves substantially lower latency than standard reranking methods. We make our end-to-end approach, including our synthetic datasets an
&lt;/p&gt;</description></item></channel></rss>