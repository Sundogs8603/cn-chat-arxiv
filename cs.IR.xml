<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20449;&#24687;&#21487;&#33719;&#21462;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#25512;&#23548;&#20986;&#24230;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#29992;&#25143;&#23545;&#25991;&#26723;&#30340;&#21487;&#21457;&#29616;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24230;&#37327;&#25351;&#26631;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09508</link><description>&lt;p&gt;
&#21487;&#21457;&#29616;&#24615;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#21487;&#33719;&#21462;&#24230;&#37327;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Findability: A Novel Measure of Information Accessibility. (arXiv:2310.09508v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20449;&#24687;&#21487;&#33719;&#21462;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#25512;&#23548;&#20986;&#24230;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#29992;&#25143;&#23545;&#25991;&#26723;&#30340;&#21487;&#21457;&#29616;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24230;&#37327;&#25351;&#26631;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#21644;&#32034;&#24341;&#30340;&#22823;&#37327;&#25968;&#25454;&#23545;&#20110;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#26816;&#32034;&#25991;&#26723;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#21363;&#20351;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#26597;&#35810;&#65292;&#19968;&#20123;&#30456;&#20851;&#25991;&#26723;&#32463;&#24120;&#34987;&#28153;&#27809;&#22312;&#31454;&#20105;&#25991;&#26723;&#30340;&#28023;&#37327;&#20013;&#65292;&#23548;&#33268;&#25152;&#38656;&#25991;&#26723;&#30340;&#21487;&#33719;&#21462;&#24615;&#25110;"&#21487;&#21457;&#29616;&#24615;"&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24615;&#33021;&#20013;&#30340;&#36825;&#20010;&#32500;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#27979;&#37327;&#25991;&#26723;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#32780;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#19968;&#31181;&#24230;&#37327;&#25351;&#26631;&#26469;&#37327;&#21270;&#22312;&#32473;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26723;&#30340;&#21487;&#21457;&#29616;&#24615;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25163;&#21160;&#25805;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23450;&#20041;&#21644;&#25512;&#23548;&#20986;&#19968;&#31181;&#24230;&#37327;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#26368;&#32456;&#29992;&#25143;&#23545;&#25991;&#26723;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#26816;&#32034;&#38598;&#21512;&#23545;&#21487;&#21457;&#29616;&#24615;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The overwhelming volume of data generated and indexed by search engines poses a significant challenge in retrieving documents from the index efficiently and effectively. Even with a well-crafted query, several relevant documents often get buried among a multitude of competing documents, resulting in reduced accessibility or `findability' of the desired document. Consequently, it is crucial to develop a robust methodology for assessing this dimension of Information Retrieval (IR) system performance. While previous studies have focused on measuring document accessibility disregarding user queries and document relevance, there exists no metric to quantify the findability of a document within a given IR system without resorting to manual labor. This paper aims to address this gap by defining and deriving a metric to evaluate the findability of documents as perceived by end-users. Through experiments, we demonstrate the varying impact of different retrieval models and collections on the fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09497</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#39640;&#25928;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models. (arXiv:2310.09497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;&#26679;&#26412;&#25991;&#26723;&#25490;&#21517;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#38024;&#23545;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36880;&#28857;&#65292;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#23454;&#39564;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#26631;&#35760;&#28040;&#32791;&#65292;&#24310;&#36831;&#31561;&#22240;&#32032;&#12290;&#36825;&#31181;&#39318;&#27425;&#30340;&#27604;&#36739;&#35780;&#20272;&#35753;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#27599;&#31181;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36880;&#28857;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#24471;&#20998;&#24456;&#39640;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#36880;&#23545;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;LLM&#25512;&#29702;&#30340;&#27425;&#25968;&#21644;&#25490;&#21517;&#36807;&#31243;&#20013;&#30340;&#25552;&#31034;&#26631;&#35760;&#28040;&#32791;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the rankin
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CollabContext&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#23558;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#24773;&#22659;&#35821;&#20041;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#21327;&#21516;&#20449;&#21495;&#21644;&#24773;&#22659;&#21270;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.09400</link><description>&lt;p&gt;
&#21327;&#20316;&#24773;&#22659;&#21270;&#65306;&#22635;&#34917;&#21327;&#21516;&#36807;&#28388;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Collaborative Contextualization: Bridging the Gap between Collaborative Filtering and Pre-trained Language Model. (arXiv:2310.09400v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CollabContext&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#23558;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#24773;&#22659;&#35821;&#20041;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#21327;&#21516;&#20449;&#21495;&#21644;&#24773;&#22659;&#21270;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#24314;&#27169;&#29992;&#25143;&#21644;&#29289;&#21697;&#26102; heavily relied on identity representations (IDs)&#65292;&#32780;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) &#30340;&#20852;&#36215;&#20016;&#23500;&#20102;&#23545;&#24773;&#22659;&#21270;&#29289;&#21697;&#25551;&#36848;&#30340;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649; PLM &#22312;&#35299;&#20915; few-shot&#12289;zero-shot &#25110;&#32479;&#19968;&#24314;&#27169;&#22330;&#26223;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#12290;&#36825;&#31181;&#24573;&#35270;&#24102;&#26469;&#20102;&#20004;&#20010;&#32039;&#36843;&#30340;&#25361;&#25112;&#65306;(1) &#21327;&#20316;&#24773;&#22659;&#21270;&#65292;&#21363;&#21327;&#21516;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;(2) &#22312;&#20445;&#30041;&#23427;&#20204;&#30340;&#24773;&#22659;&#35821;&#20041;&#30340;&#21516;&#26102;&#65292;&#24357;&#21512;&#22522;&#20110;ID&#30340;&#34920;&#31034;&#21644;&#24773;&#22659;&#21270;&#34920;&#31034;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CollabContext&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#24039;&#22937;&#22320;&#23558;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#23545;&#40784;&#22312;&#24773;&#22659;&#31354;&#38388;&#20869;&#65292;&#20445;&#30041;&#20102;&#37325;&#35201;&#30340;&#24773;&#22659;&#35821;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Traditional recommender systems have heavily relied on identity representations (IDs) to model users and items, while the ascendancy of pre-trained language model (PLM) encoders has enriched the modeling of contextual item descriptions. However, PLMs, although effective in addressing few-shot, zero-shot, or unified modeling scenarios, often neglect the crucial collaborative filtering signal. This neglect gives rise to two pressing challenges: (1) Collaborative Contextualization, the seamless integration of collaborative signals with contextual representations. (2) the imperative to bridge the representation gap between ID-based representations and contextual representations while preserving their contextual semantics. In this paper, we propose CollabContext, a novel model that adeptly combines collaborative filtering signals with contextual representations and aligns these representations within the contextual space, preserving essential contextual semantics. Experimental results acros
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36229;&#31435;&#26041;&#22270;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#35780;&#20998;&#25968;&#37327;&#30830;&#23450;&#29992;&#25143;&#20559;&#22909;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09341</link><description>&lt;p&gt;
&#35299;&#20915;&#20351;&#29992;&#36229;&#31435;&#26041;&#22270;&#22312;&#38544;&#31169;&#20445;&#25252;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing the cold start problem in privacy preserving content-based recommender systems using hypercube graphs. (arXiv:2310.09341v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36229;&#31435;&#26041;&#22270;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#35780;&#20998;&#25968;&#37327;&#30830;&#23450;&#29992;&#25143;&#20559;&#22909;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#21021;&#22987;&#20114;&#21160;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#25152;&#35859;&#30340;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#65292;&#25512;&#33616;&#31995;&#32479;&#23545;&#29992;&#25143;&#30340;&#20449;&#24687;&#20102;&#35299;&#38750;&#24120;&#26377;&#38480;&#65292;&#29978;&#33267;&#27809;&#26377;&#20219;&#20309;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#65292;&#29992;&#25143;&#38656;&#35201;&#36890;&#36807;&#35780;&#20215;&#29289;&#21697;&#19982;&#26381;&#21153;&#25552;&#20379;&#32773;&#20849;&#20139;&#33258;&#24049;&#30340;&#20559;&#22909;&#65292;&#32780;&#22312;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#20013;&#21017;&#19981;&#38656;&#35201;&#36825;&#26679;&#30340;&#20449;&#24687;&#20849;&#20139;&#12290;&#25105;&#20204;&#26368;&#36817;&#21457;&#29616;&#20351;&#29992;&#36229;&#31435;&#26041;&#22270;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35780;&#20998;&#25968;&#37327;&#19979;&#30830;&#23450;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#39184;&#39302;&#21644;&#30005;&#24433;&#39046;&#22495;&#36229;&#36807;1000&#21517;&#29992;&#25143;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21487;&#29992;&#35780;&#20998;&#25968;&#37327;&#19981;&#36229;&#36807;10&#26102;&#20248;&#20110;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#36739;&#22823;&#30340;&#35757;&#32451;&#38598;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#31616;&#21333;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The initial interaction of a user with a recommender system is problematic because, in such a so-called cold start situation, the recommender system has very little information about the user, if any. Moreover, in collaborative filtering, users need to share their preferences with the service provider by rating items while in content-based filtering there is no need for such information sharing. We have recently shown that a content-based model that uses hypercube graphs can determine user preferences with a very limited number of ratings while better preserving user privacy. In this paper, we confirm these findings on the basis of experiments with more than 1,000 users in the restaurant and movie domains. We show that the proposed method outperforms standard machine learning algorithms when the number of available ratings is at most 10, which often happens, and is competitive with larger training sets. In addition, training is simple and does not require large computational efforts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07786</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#38598;&#25104;&#25277;&#26679;&#30340;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24773;&#22659;&#36172;&#21338;&#24212;&#29992;&#24120;&#24120;&#22240;&#23395;&#33410;&#24615;&#12289;&#20598;&#28982;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31038;&#20132;&#36235;&#21183;&#32780;&#21576;&#38750;&#31283;&#24577;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23545;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#30340;&#20248;&#20808;&#32771;&#34385;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#26102;&#36807;&#24230;&#65292;&#25110;&#32773;&#35774;&#35745;&#26041;&#24335;&#38590;&#20197;&#22312;&#20855;&#26377;&#39640;&#32500;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#21644;&#22823;&#35268;&#27169;&#21160;&#20316;&#38598;&#30340;&#29616;&#20195;&#24212;&#29992;&#20013;&#25193;&#23637;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#23427;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25112;&#30053;&#24615;&#22320;&#20248;&#20808;&#25910;&#38598;&#20855;&#26377;&#26368;&#25345;&#20037;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23637;&#31034;&#26126;&#26174;&#38750;&#31283;&#24577;&#30340;&#20004;&#20010;&#23454;&#38469;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>VIP5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#20849;&#20139;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14302</link><description>&lt;p&gt;
VIP5&#65306;&#38754;&#21521;&#25512;&#33616;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VIP5: Towards Multimodal Foundation Models for Recommendation. (arXiv:2305.14302v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14302
&lt;/p&gt;
&lt;p&gt;
VIP5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#20849;&#20139;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#26159;&#19977;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#23427;&#20204;&#20256;&#32479;&#19978;&#29420;&#31435;&#21457;&#23637;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#24314;&#27169;&#21644;&#24037;&#31243;&#26041;&#27861;&#12290;&#36825;&#22952;&#30861;&#20102;&#36825;&#20123;&#39046;&#22495;&#30452;&#25509;&#20174;&#24444;&#27492;&#30340;&#36827;&#23637;&#20013;&#21463;&#30410;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#21644;&#38382;&#39064;&#34920;&#36848;&#30340;&#28508;&#22312;&#36890;&#29992;&#25509;&#21475;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MFM&#65289;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#22312;P5&#25512;&#33616;&#33539;&#24335;&#19979;&#32479;&#19968;&#21508;&#31181;&#27169;&#24577;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#22240;&#27492;&#21629;&#21517;&#20026;VIP5&#65288;Visual P5&#65289;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#21151;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#25552;&#31034;&#26469;&#36866;&#24212;&#22810;&#20010;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.09598</link><description>&lt;p&gt;
&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#29992;&#20110;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query-as-context Pre-training for Dense Passage Retrieval. (arXiv:2212.09598v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20986;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#25552;&#39640;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#31616;&#21333;&#22320;&#35748;&#20026;&#26469;&#33258;&#21516;&#19968;&#25991;&#26723;&#30340;&#20004;&#20010;&#36890;&#36947;&#26159;&#30456;&#20851;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#29992;&#20110;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#20551;&#23450;&#20174;&#36890;&#36947;&#20013;&#25552;&#21462;&#30340;&#26597;&#35810;&#26356;&#21487;&#33021;&#19982;&#35813;&#36890;&#36947;&#30456;&#20851;&#65292;&#24182;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#12290;&#36825;&#20123;&#36890;&#36947;-&#26597;&#35810;&#23545;&#28982;&#21518;&#29992;&#20110;&#23545;&#27604;&#24615;&#25110;&#29983;&#25104;&#24615;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#36890;&#36947;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#21644;&#36328;&#39046;&#22495;&#38646;-shot&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#22686;&#30410;&#65292;&#21516;&#26102;&#21152;&#36895;&#20102;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20250;&#22312;https://github.com/deepset-ai/haystack&#19978;&#25552;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the possibility of weakly correlated pairs. Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue. Query-as-context pre-training assumes that the query derived from a passage is more likely to be relevant to that passage and forms a passage-query pair. These passage-query pairs are then used in contrastive or generative context-supervised pre-training. The pre-trained models are evaluated on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks. Experimental results show that query-as-context pre-training brings considerable gains and meanwhile speeds up training, demonstrating its effectiveness and efficiency. Our code will be available at https://g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03536</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces. (arXiv:2211.03536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#20302;&#32500;&#35821;&#20041;&#31354;&#38388;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#38142;&#25509;&#39044;&#27979;&#65292;&#30693;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#34917;&#20840;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KGE&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#19977;&#20010;&#25968;&#23398;&#35282;&#24230;&#65288;&#20195;&#25968;&#35282;&#24230;&#12289;&#20960;&#20309;&#35282;&#24230;&#21644;&#20998;&#26512;&#35282;&#24230;&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#20171;&#32461;&#20102;&#22522;&#26412;&#25968;&#23398;&#31354;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#28982;&#21518;&#28145;&#20837;&#30740;&#31350;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#25968;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#19981;&#21516;KGE&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#31354;&#38388;&#20248;&#21183;&#22312;&#19981;&#21516;&#23884;&#20837;&#38656;&#27714;&#19978;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#25972;&#29702;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;KGE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.05521</link><description>&lt;p&gt;
&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#31264;&#23494;&#26816;&#32034;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval. (arXiv:2210.05521v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20498;&#25490;&#25991;&#20214;&#32467;&#26500;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#30340;&#25216;&#26415;&#12290;&#23427;&#26681;&#25454;&#23884;&#20837;&#23558;&#25991;&#26723;&#32858;&#31867;&#65307;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#26597;&#35810;&#25506;&#27979;&#38468;&#36817;&#30340;&#32858;&#31867;&#65292;&#24182;&#19988;&#20165;&#23545;&#20854;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#21518;&#32493;&#30340;&#35299;&#30721;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31351;&#20030;&#36941;&#21382;&#30340;&#26114;&#36149;&#20195;&#20215;&#12290;&#28982;&#32780;&#65292;&#32858;&#31867;&#36807;&#31243;&#24635;&#26159;&#26377;&#25439;&#30340;&#65292;&#36825;&#23548;&#33268;&#25506;&#27979;&#21040;&#30340;&#32858;&#31867;&#20013;&#32570;&#22833;&#20102;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26816;&#32034;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#35789;&#27719;&#21305;&#37197;&#65292;&#22914;&#26174;&#33879;&#35789;&#27719;&#30340;&#37325;&#21472;&#65292;&#26356;&#23481;&#26131;&#35782;&#21035;&#30456;&#20851;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341; (HI$^2$)&#65292;&#20854;&#20013;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#20849;&#21516;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#12290;&#20026;&#20102;&#20860;&#39038;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32858;&#31867;&#36873;&#25321;&#22120;&#21644;&#19968;&#20010;&#35789;&#27719;&#36873;&#25321;&#22120;&#65292;&#29992;&#20110;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#24555;&#36895;&#25628;&#32034;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#31639;&#27861;&#21644;&#31471;&#21040;&#31471;&#23398;&#20064;&#26469;&#25552;&#39640;&#32034;&#24341;&#36136;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35789;&#21521;&#37327;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#21152;&#26435;&#24179;&#22343;&#35789;&#21521;&#37327;&#29305;&#24449;&#26469;&#23398;&#20064;&#21644;&#20272;&#35745;&#35780;&#35770;&#30340;&#26497;&#24615;&#65292;&#21516;&#26102;&#19982;&#24050;&#26377;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2002.05606</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#35789;&#21521;&#37327;&#29305;&#24449;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis Using Averaged Weighted Word Vector Features. (arXiv:2002.05606v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.05606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35789;&#21521;&#37327;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#21152;&#26435;&#24179;&#22343;&#35789;&#21521;&#37327;&#29305;&#24449;&#26469;&#23398;&#20064;&#21644;&#20272;&#35745;&#35780;&#35770;&#30340;&#26497;&#24615;&#65292;&#21516;&#26102;&#19982;&#24050;&#26377;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24191;&#27867;&#20351;&#29992;&#20114;&#32852;&#32593;&#20998;&#20139;&#20182;&#20204;&#23545;&#20135;&#21697;&#12289;&#26381;&#21153;&#25110;&#26053;&#34892;&#30446;&#30340;&#22320;&#30340;&#20307;&#39564;&#12290;&#22312;&#32447;&#21453;&#39304;&#35780;&#35770;&#30340;&#25991;&#23383;&#23545;&#20110;&#28040;&#36153;&#32773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#20135;&#21697;&#25110;&#26381;&#21153;&#28385;&#24847;&#24230;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#35782;&#21035;&#36825;&#20123;&#25991;&#26412;&#29255;&#27573;&#20013;&#34920;&#36798;&#30340;&#35266;&#28857;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#35789;&#21521;&#37327;&#32467;&#21512;&#36215;&#26469;&#23398;&#20064;&#21644;&#20272;&#35745;&#35780;&#35770;&#30340;&#26497;&#24615;&#12290;&#25105;&#20204;&#20174;&#35789;&#21521;&#37327;&#20013;&#21019;&#24314;&#24179;&#22343;&#35780;&#35770;&#21521;&#37327;&#65292;&#24182;&#22312;&#27491;&#38754;&#21644;&#36127;&#38754;&#25935;&#24863;&#26631;&#35760;&#30340;&#35780;&#35770;&#20013;&#20351;&#29992;&#35789;&#39057;&#32473;&#36825;&#20123;&#35780;&#35770;&#21521;&#37327;&#28155;&#21152;&#26435;&#37325;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#34987;&#29992;&#20316;&#24773;&#24863;&#20998;&#26512;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#21644;&#24050;&#26377;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#65292;&#24182;&#19982;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
People use the world wide web heavily to share their experience with entities such as products, services, or travel destinations. Texts that provide online feedback in the form of reviews and comments are essential to make consumer decisions. These comments create a valuable source that may be used to measure satisfaction related to products or services. Sentiment analysis is the task of identifying opinions expressed in such text fragments. In this work, we develop two methods that combine different types of word vectors to learn and estimate polarity of reviews. We develop average review vectors from word vectors and add weights to this review vectors using word frequencies in positive and negative sensitivity-tagged reviews. We applied the methods to several datasets from different domains that are used as standard benchmarks for sentiment analysis. We ensemble the techniques with each other and existing methods, and we make a comparison with the approaches in the literature. The re
&lt;/p&gt;</description></item></channel></rss>