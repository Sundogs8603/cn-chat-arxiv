<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.10563</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21450;&#20854;&#35299;&#37322;&#24120;&#24120;&#38754;&#20020;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#32479;&#19968;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#19978;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23558;&#36825;&#19968;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#31232;&#32570;&#26631;&#27880;&#27861;&#24459;&#25991;&#20214;&#8221;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;MESc&#65288;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#26694;&#26550;&#26469;&#25506;&#32034;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#21644;&#38271;&#25991;&#26723;&#30340;&#29305;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#20010;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21478;&#19968;&#32452;Transformer&#32534;&#30721;&#22120;&#23618;&#23398;&#20064;&#37096;&#20998;&#20043;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#26426;&#32593;&#32476;&#30340;&#25552;&#26696;&#65292;&#26088;&#22312;&#36830;&#25509;Web&#21644;&#35821;&#20041;Web&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#65292;&#20197;&#23454;&#29616;&#21508;&#31181;&#20449;&#24687;&#36164;&#28304;&#30340;&#31934;&#32454;&#20114;&#32852;&#65292;&#24182;&#25552;&#20379;&#21327;&#20316;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.10531</link><description>&lt;p&gt;
&#20026;&#26377;&#26426;&#32593;&#32476;&#25552;&#20986;&#30340;&#35758;&#26696;&#65292;&#36830;&#25509;Web&#21644;&#35821;&#20041;Web&#30340;&#32570;&#22833;&#38142;&#65292;&#31532;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposal for an Organic Web, The missing link between the Web and the Semantic Web, Part 1. (arXiv:2309.10531v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#26426;&#32593;&#32476;&#30340;&#25552;&#26696;&#65292;&#26088;&#22312;&#36830;&#25509;Web&#21644;&#35821;&#20041;Web&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#65292;&#20197;&#23454;&#29616;&#21508;&#31181;&#20449;&#24687;&#36164;&#28304;&#30340;&#31934;&#32454;&#20114;&#32852;&#65292;&#24182;&#25552;&#20379;&#21327;&#20316;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#37327;&#24222;&#22823;&#30340;&#20449;&#24687;&#20197;&#25968;&#23383;&#24418;&#24335;&#20135;&#29983;&#12290;&#35821;&#20041;Web&#30340;&#20986;&#29616;&#26159;&#22240;&#20026;&#25105;&#20204;&#24847;&#35782;&#21040;&#65292;&#35201;&#26377;&#25928;&#22788;&#29702;&#36825;&#20123;&#20449;&#24687;&#30340;&#20135;&#29983;&#65292;&#38656;&#35201;&#26356;&#22909;&#22320;&#23558;&#25968;&#23383;&#20449;&#24687;&#36164;&#28304;&#30456;&#20114;&#36830;&#25509;&#12290;&#23427;&#30340;&#37325;&#28857;&#26159;&#36830;&#25509;&#25968;&#25454;&#12290;&#20294;&#20165;&#20165;&#36830;&#25509;&#25968;&#25454;&#26159;&#19981;&#22815;&#30340;&#12290;&#25105;&#20204;&#38656;&#35201;&#20026;&#36830;&#25509;&#21508;&#31181;&#20449;&#24687;&#36164;&#28304;&#25552;&#20379;&#22522;&#30784;&#35774;&#26045;&#25903;&#25345;&#65292;&#21253;&#25324;&#37027;&#20123;&#29702;&#35299;&#21644;&#31934;&#32454;&#36830;&#25509;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#36164;&#28304;&#12290;&#22312;&#24456;&#22810;&#38382;&#39064;&#35268;&#27169;&#36798;&#21040;&#20840;&#29699;&#27700;&#24179;&#30340;&#26102;&#20505;&#65292;&#25552;&#21319;&#20449;&#24687;&#22788;&#29702;&#21644;&#20449;&#24687;&#20135;&#29983;&#30340;&#21327;&#20316;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21516;&#26102;&#20063;&#19981;&#33021;&#25918;&#24323;&#19987;&#19994;&#30693;&#35782;&#21644;&#28145;&#24230;&#20998;&#26512;&#65292;&#20063;&#19981;&#33021;&#25226;&#21482;&#36866;&#29992;&#20110;&#26576;&#20123;&#26234;&#33021;&#24418;&#24335;&#30340;&#35821;&#35328;&#21644;&#24418;&#24335;&#20027;&#20041;&#24378;&#21152;&#32473;&#24605;&#32771;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#21019;&#26032;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35821;&#20041;Web&#25512;&#23815;&#30340;&#30456;&#20114;&#36830;&#25509;&#30340;&#24605;&#24819;&#19968;&#33268;&#30340;&#26041;&#21521;&#19978;&#30340;&#25552;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
A huge amount of information is produced in digital form. The Semantic Web stems from the realisation that dealing efficiently with this production requires getting better at interlinking digital informational resources together. Its focus is on linking data. Linking data isn't enough. We need to provide infrastructural support for linking all sorts of informational resources including resources whose understanding and fine interlinking requires domain-specific human expertise. At times when many problems scale to planetary dimensions, it is essential to scale coordination of information processing and information production, without giving up on expertise and depth of analysis, nor forcing languages and formalisms onto thinkers, decision-makers and innovators that are only suitable to some forms of intelligence. This article makes a proposal in this direction and in line with the idea of interlinking championed by the Semantic Web.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;DJI Mini 3 Pro&#21644;DJI RC&#30340;&#25968;&#23383;&#21462;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#36825;&#20004;&#31181;&#26080;&#20154;&#26426;&#35774;&#22791;&#19978;&#30340;&#25968;&#23383;&#30165;&#36857;&#65292;&#20026;&#21462;&#35777;&#35843;&#26597;&#21592;&#25552;&#20379;&#20102;&#36861;&#36394;&#26080;&#20154;&#26426;&#20351;&#29992;&#24773;&#20917;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.10487</link><description>&lt;p&gt;
DJI Mini 3 Pro&#21644;DJI RC&#30340;&#25968;&#23383;&#21462;&#35777;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Digital Forensics Case Study of the DJI Mini 3 Pro and DJI RC. (arXiv:2309.10487v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;DJI Mini 3 Pro&#21644;DJI RC&#30340;&#25968;&#23383;&#21462;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#36825;&#20004;&#31181;&#26080;&#20154;&#26426;&#35774;&#22791;&#19978;&#30340;&#25968;&#23383;&#30165;&#36857;&#65292;&#20026;&#21462;&#35777;&#35843;&#26597;&#21592;&#25552;&#20379;&#20102;&#36861;&#36394;&#26080;&#20154;&#26426;&#20351;&#29992;&#24773;&#20917;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#22411;&#26080;&#20154;&#26426;&#27169;&#22411;&#25512;&#20986;&#65292;&#28040;&#36153;&#32423;&#26080;&#20154;&#26426;&#24066;&#22330;&#36805;&#36895;&#25193;&#22823;&#65292;&#36825;&#20123;&#26080;&#20154;&#26426;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#21464;&#21270;&#12290;&#26080;&#20154;&#26426;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#20250;&#32473;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#21592;&#21644;&#24037;&#20855;&#24102;&#26469;&#22256;&#38590;&#65292;&#38590;&#20197;&#36319;&#19978;&#27493;&#20240;&#24182;&#26377;&#25928;&#22320;&#25552;&#21462;&#21644;&#20998;&#26512;&#26080;&#20154;&#26426;&#30340;&#25968;&#23383;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26080;&#20154;&#26426;&#22312;&#38750;&#27861;&#21644;&#26377;&#23475;&#27963;&#21160;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#22914;&#36208;&#31169;&#12289;&#38388;&#35853;&#27963;&#21160;&#29978;&#33267;&#24656;&#24598;&#20027;&#20041;&#27963;&#21160;&#65292;&#23548;&#33268;&#20102;&#25191;&#27861;&#37096;&#38376;&#22788;&#29702;&#26080;&#20154;&#26426;&#21462;&#35777;&#26696;&#20363;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#24110;&#21161;&#21462;&#35777;&#35843;&#26597;&#21592;&#65292;&#23545;&#22823;&#30086;&#21019;&#26032;&#65288;DJI&#65289;&#26368;&#36817;&#21457;&#24067;&#30340;&#20004;&#27454;&#26080;&#20154;&#26426;&#35774;&#22791;&#36827;&#34892;&#20102;&#38745;&#24577;&#25968;&#23383;&#21462;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#21035;&#26159;Mini 3 Pro&#26080;&#20154;&#26426;&#21644;&#20854;&#36965;&#25511;&#22120;DJI RC&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#36825;&#20004;&#31181;&#35774;&#22791;&#19978;&#23384;&#22312;&#30528;&#20960;&#20010;&#25968;&#23383;&#30165;&#36857;&#65292;&#21253;&#25324;&#35760;&#24405;&#30340;&#23186;&#20307;&#12289;&#39134;&#34892;&#26085;&#24535;&#21644;&#20854;&#20182;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#35843;&#26597;&#21592;&#36861;&#36394;&#26080;&#20154;&#26426;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The consumer drone market is rapidly expanding with new drone models featuring unique variations of hardware and software. The rapid development of drone technology and variability in drone systems can make it difficult for digital forensic investigators and tools to keep pace and effectively extract and analyse digital evidence from drones. Furthermore, the growing popularity of drones and their increased use in illegal and harmful activities, such as smuggling, espionage, and even terrorism, has led to an increase in the number of drone forensic cases for authorities to manage. To assist forensic investigators, a static digital forensic case study was conducted on two drone devices recently released by Da-Jiang Innovations (DJI): the Mini 3 Pro drone, and its remote controller, the DJI RC. The study discovered the presence of several digital artefacts on both devices, including recorded media, flight logs, and other information that could help investigators trace the drone's usage an
&lt;/p&gt;</description></item><item><title>RUEL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;Edge&#27983;&#35272;&#22120;&#26085;&#24535;&#30340;&#22806;&#37096;&#21311;&#21517;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#25552;&#21319;&#25512;&#33616;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#36328;&#24179;&#21488;&#29992;&#25143;ID&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10469</link><description>&lt;p&gt;
RUEL: &#20351;&#29992;&#36793;&#32536;&#27983;&#35272;&#22120;&#26085;&#24535;&#25552;&#21319;&#26816;&#32034;&#22686;&#24378;&#30340;&#29992;&#25143;&#34920;&#31034;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation. (arXiv:2309.10469v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10469
&lt;/p&gt;
&lt;p&gt;
RUEL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;Edge&#27983;&#35272;&#22120;&#26085;&#24535;&#30340;&#22806;&#37096;&#21311;&#21517;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#25552;&#21319;&#25512;&#33616;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#36328;&#24179;&#21488;&#29992;&#25143;ID&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#23558;&#29992;&#25143;&#38656;&#27714;&#19982;&#21508;&#31181;&#24179;&#21488;&#19978;&#30340;&#22823;&#37327;&#36164;&#28304;&#21305;&#37197;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#26465;&#20214;&#19979;&#20934;&#30830;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#24179;&#21488;&#30340;&#22806;&#37096;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#20016;&#23500;&#29992;&#25143;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#38656;&#35201;&#19968;&#20010;&#36328;&#24179;&#21488;&#30340;&#19968;&#33268;&#29992;&#25143;ID&#65292;&#24182;&#19988;&#24573;&#30053;&#20102;&#31867;&#20284;&#29992;&#25143;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RUEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#26469;&#33258;Edge&#27983;&#35272;&#22120;&#26085;&#24535;&#30340;&#22806;&#37096;&#21311;&#21517;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#34701;&#20837;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#19968;&#24180;&#30340;&#22823;&#37327;Edge&#27983;&#35272;&#22120;&#26085;&#24535;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#30340;&#20505;&#36873;&#39033;&#30456;&#23545;&#24212;&#30340;&#30446;&#26631;&#23454;&#20307;&#36827;&#34892;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21160;&#37327;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35760;&#24518;&#24211;&#65292;&#29992;&#20110;&#26816;&#32034;&#26368;&#30456;&#20851;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#27983;&#35272;&#22120;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online recommender systems (RS) aim to match user needs with the vast amount of resources available on various platforms. A key challenge is to model user preferences accurately under the condition of data sparsity. To address this challenge, some methods have leveraged external user behavior data from multiple platforms to enrich user representation. However, all of these methods require a consistent user ID across platforms and ignore the information from similar users. In this study, we propose RUEL, a novel retrieval-based sequential recommender that can effectively incorporate external anonymous user behavior data from Edge browser logs to enhance recommendation. We first collect and preprocess a large volume of Edge browser logs over a one-year period and link them to target entities that correspond to candidate items in recommendation datasets. We then design a contrastive learning framework with a momentum encoder and a memory bank to retrieve the most relevant and diverse brow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#20219;&#21153;&#22612;&#30340;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#36890;&#36807;&#22312;&#36739;&#39640;&#23618;&#27425;&#19978;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#21487;&#20197;&#20248;&#21270;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#21644;&#22312;&#32447;AB&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10357</link><description>&lt;p&gt;
&#36328;&#20219;&#21153;&#22612;&#30340;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#23545;&#20110;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deep Mutual Learning across Task Towers for Effective Multi-Task Recommender Learning. (arXiv:2309.10357v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#20219;&#21153;&#22612;&#30340;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#36890;&#36807;&#22312;&#36739;&#39640;&#23618;&#27425;&#19978;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#21487;&#20197;&#20248;&#21270;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#21644;&#22312;&#32447;AB&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#22240;&#20026;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26159;&#22810;&#26041;&#38754;&#30340;&#12290;&#20856;&#22411;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#36739;&#20302;&#23618;&#27425;&#19978;&#24314;&#31435;&#36866;&#24403;&#30340;&#21442;&#25968;&#20849;&#20139;&#65292;&#32780;&#22312;&#36739;&#39640;&#23618;&#27425;&#19978;&#20026;&#27599;&#20010;&#20219;&#21153;&#20445;&#30041;&#29420;&#31435;&#30340;&#20219;&#21153;&#22612;&#12290;&#30001;&#20110;&#20219;&#21153;&#22612;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#30452;&#25509;&#24433;&#21709;&#65292;&#25105;&#20204;&#35748;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#22612;&#32467;&#26500;&#23545;&#20110;&#20419;&#36827;&#27491;&#38754;&#30693;&#35782;&#20849;&#20139;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#20219;&#21153;&#22612;&#30340;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19982;&#21508;&#31181;&#20027;&#24178;&#22810;&#20219;&#21153;&#32593;&#32476;&#20860;&#23481;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#31163;&#32447;&#23454;&#39564;&#21644;&#22312;&#32447;AB&#27979;&#35797;&#26469;&#35780;&#20272;&#21644;&#39564;&#35777;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems usually leverage multi-task learning methods to simultaneously optimize several objectives because of the multi-faceted user behavior data. The typical way of conducting multi-task learning is to establish appropriate parameter sharing across multiple tasks at lower layers while reserving a separate task tower for each task at upper layers. Since the task towers exert direct impact on the prediction results, we argue that the architecture of standalone task towers is sub-optimal for promoting positive knowledge sharing. Accordingly, we propose the framework of Deep Mutual Learning across task towers, which is compatible with various backbone multi-task networks. Extensive offline experiments and online AB tests are conducted to evaluate and verify the proposed approach's effectiveness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#22270;&#29255;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#38646;-shot&#25552;&#21462;UI&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26041;&#27861;&#23454;&#29616;&#24212;&#29992;&#21040;&#24212;&#29992;&#30340;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10328</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#24212;&#29992;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computational Approaches for App-to-App Retrieval and Design Consistency Check. (arXiv:2309.10328v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#22270;&#29255;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#38646;-shot&#25552;&#21462;UI&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#26041;&#27861;&#23454;&#29616;&#24212;&#29992;&#21040;&#24212;&#29992;&#30340;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31227;&#21160;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#20013;&#25552;&#21462;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#29992;&#20110;&#35774;&#35745;&#24072;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#26377;&#25928;&#30340;&#35745;&#31639;&#35774;&#35745;&#25903;&#25345;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#23567;&#35268;&#27169;&#31227;&#21160;UI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#20041;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#23631;&#24149;&#25130;&#22270;&#36827;&#34892;&#23545;&#27604;&#26469;&#26816;&#32034;&#32473;&#23450;&#26597;&#35810;&#25130;&#22270;&#30340;&#30456;&#20284;UI&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#19988;&#23545;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#35757;&#32451;&#27969;&#31243;&#22797;&#26434;&#65292;&#24182;&#19988;&#26080;&#27861;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#21040;&#24212;&#29992;&#31243;&#24207;&#30340;&#26816;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#22270;&#29255;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#24182;&#27979;&#35797;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20197;&#38646;-shot&#26041;&#24335;&#25552;&#21462;UI&#34920;&#31034;&#65292;&#24182;&#36229;&#36234;&#29616;&#26377;&#30340;&#19987;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#25968;&#23398;&#26041;&#27861;&#23454;&#29616;&#24212;&#29992;&#21040;&#24212;&#29992;&#30340;&#26816;&#32034;&#21644;&#35774;&#35745;&#19968;&#33268;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Extracting semantic representations from mobile user interfaces (UI) and using the representations for designers' decision-making processes have shown the potential to be effective computational design support tools. Current approaches rely on machine learning models trained on small-sized mobile UI datasets to extract semantic vectors and use screenshot-to-screenshot comparison to retrieve similar-looking UIs given query screenshots. However, the usability of these methods is limited because they are often not open-sourced and have complex training pipelines for practitioners to follow, and are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval. To this end, we (1) employ visual models trained with large web-scale images and test whether they could extract a UI representation in a zero-shot way and outperform existing specialized models, and (2) use mathematically founded methods to enable app-to-app retrieval and design consistency analysis. Our experiments show tha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10302</link><description>&lt;p&gt;
&#35299;&#32806;&#35757;&#32451;&#65306;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#21333;&#22810;&#39046;&#22495;&#23398;&#20064;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#37325;&#21472;&#20294;&#38750;&#30456;&#21516;&#30340;&#39046;&#22495;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20559;&#24046;&#21644;&#39046;&#22495;&#20248;&#21183;&#30340;&#25361;&#25112;&#65292;&#20174;&#23545;&#40784;&#20998;&#24067;&#20943;&#23569;&#39046;&#22495;&#24046;&#36317;&#30340;&#35282;&#24230;&#25110;&#36890;&#36807;&#23454;&#26045;&#39046;&#22495;&#29305;&#23450;&#30340;&#22612;&#12289;&#38376;&#29978;&#33267;&#19987;&#23478;&#26469;&#20445;&#30041;&#24046;&#24322;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;MDL&#26041;&#27861;&#12290;MDL&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#25110;&#25439;&#22833;&#20989;&#25968;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#24182;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#12290;D-Train&#26159;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20174;&#19968;&#33324;&#21040;&#29305;&#27530;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#22312;&#25152;&#26377;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#28909;&#36523;&#19968;&#20010;&#26681;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#20854;&#25286;&#20998;&#20026;&#22810;&#20010;&#22836;&#37096;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36890;&#36807;&#22266;&#23450;&#39592;&#24178;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#25490;&#21517;&#27010;&#24565;&#21644;&#25351;&#26631;&#65292;&#30740;&#31350;&#20102;&#32593;&#26684;&#24067;&#23616;&#20013;&#25552;&#20379;&#32773;&#26041;&#38754;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#19981;&#21516;&#35774;&#22791;&#19978;&#30340;&#34892;&#20026;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10271</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#27979;&#37327;&#32593;&#26684;&#24067;&#23616;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring Fairness in Grid Layout in Recommender Systems. (arXiv:2309.10271v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#25490;&#21517;&#27010;&#24565;&#21644;&#25351;&#26631;&#65292;&#30740;&#31350;&#20102;&#32593;&#26684;&#24067;&#23616;&#20013;&#25552;&#20379;&#32773;&#26041;&#38754;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#19981;&#21516;&#35774;&#22791;&#19978;&#30340;&#34892;&#20026;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#65292;&#23545;&#20110;&#30830;&#20445;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20869;&#23481;&#25552;&#20379;&#32773;&#33021;&#22815;&#20844;&#24179;&#23545;&#24453;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#32467;&#26524;&#23545;&#20854;&#20316;&#21697;&#36827;&#34892;&#23637;&#31034;&#26041;&#38754;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;&#25351;&#26631;&#37117;&#26159;&#38024;&#23545;&#32447;&#24615;&#25490;&#21517;&#21015;&#34920;&#35774;&#35745;&#21644;&#27979;&#35797;&#30340;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29992;&#20110;&#32447;&#24615;&#24067;&#23616;&#30340;&#20844;&#24179;&#25490;&#21517;&#25351;&#26631;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#32593;&#26684;&#30340;&#26174;&#31034;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#29992;&#25143;&#20351;&#29992;&#30340;&#35774;&#22791;&#65288;&#25163;&#26426;&#12289;&#24179;&#26495;&#25110;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#65292;&#20351;&#29992;&#21015;&#20943;&#23569;&#26041;&#27861;&#35843;&#25972;&#21015;&#22823;&#23567;&#20197;&#22312;&#32593;&#26684;&#35270;&#22270;&#20013;&#26174;&#31034;&#12290;&#22312;&#32593;&#26684;&#24067;&#23616;&#20013;&#65292;&#25512;&#33616;&#39033;&#30340;&#21487;&#35265;&#24615;&#25110;&#26333;&#20809;&#24230;&#26681;&#25454;&#21015;&#22823;&#23567;&#21644;&#21015;&#20943;&#23569;&#26041;&#27861;&#32780;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20844;&#24179;&#25490;&#21517;&#27010;&#24565;&#21644;&#25351;&#26631;&#25193;&#23637;&#21040;&#30740;&#31350;&#32593;&#26684;&#24067;&#23616;&#20013;&#25552;&#20379;&#32773;&#26041;&#38754;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#32593;&#26684;&#35843;&#25972;&#20844;&#24179;&#25490;&#21517;&#25351;&#26631;&#34892;&#20026;&#30340;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#19981;&#21516;&#35774;&#22791;&#19978;&#30340;&#34892;&#20026;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant research in the last five years on ensuring the providers of items in a recommender system are treated fairly, particularly in terms of the exposure the system provides to their work through its results. However, the metrics developed to date have all been designed and tested for linear ranked lists. It is unknown whether and how existing fair ranking metrics for linear layouts can be applied to grid-based displays. Moreover, depending on the device (phone, tab, or laptop) users use to interact with systems, column size is adjusted using column reduction approaches in a grid-view. The visibility or exposure of recommended items in grid layouts varies based on column sizes and column reduction approaches as well. In this paper, we extend existing fair ranking concepts and metrics to study provider-side group fairness in grid layouts, present an analysis of the behavior of these grid adaptations of fair ranking metrics, and study how their behavior changes acro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ANT&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#39033;&#30446;&#20449;&#24687;&#65292;inc&#12290;</title><link>http://arxiv.org/abs/2309.10195</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30456;&#36935;&#20877;&#23398;&#20064;&#65306;&#20943;&#36731;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-modality Meets Re-learning: Mitigating Negative Transfer in Sequential Recommendation. (arXiv:2309.10195v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ANT&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#39033;&#30446;&#20449;&#24687;&#65292;inc&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31232;&#30095;&#29992;&#25143;&#20132;&#20114;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#25512;&#33616;&#27169;&#22411;&#26159;&#21457;&#23637;&#29616;&#20195;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#39044;&#35757;&#32451;&#20174;&#30456;&#20851;&#20219;&#21153;&#65288;&#21363;&#36741;&#21161;&#20219;&#21153;&#65289;&#20013;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#35813;&#30693;&#35782;&#35843;&#25972;&#21040;&#30446;&#26631;&#20219;&#21153;&#65288;&#21363;&#30446;&#26631;&#20219;&#21153;&#65289;&#20013;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#25152;&#28508;&#21147;&#65292;&#26412;&#25991;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#30340;&#24694;&#21517;&#26157;&#33879;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#20013;&#34920;&#29616;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;ANT&#30340;&#21487;&#36801;&#31227;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ANT&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#65306;1&#65289;&#32467;&#21512;&#22810;&#27169;&#24577;&#39033;&#30446;&#20449;&#24687;&#65292;inc
&lt;/p&gt;
&lt;p&gt;
Learning effective recommendation models from sparse user interactions represents a fundamental challenge in developing modern sequential recommendation methods. Recently, pre-training-based methods have been developed to tackle this challenge. The key idea behind these methods is to learn transferable knowledge from related tasks (i.e., auxiliary tasks) via pre-training and adapt the knowledge to the task of interest (i.e., target task) to mitigate its data sparsity, thereby enabling more accurate recommendations. Though promising, in this paper, we show that existing methods suffer from the notorious negative transfer issue, where the model adapted from the pre-trained model results in worse performance compared to the model learned from scratch in the target task. To address this issue, we develop a method, denoted as ANT, for transferable sequential recommendation. Compared to existing methods, ANT mitigates negative transfer by 1) incorporating multi-modality item information, inc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08622</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;Slate&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in Low-rank Slate-based Recommender Systems. (arXiv:2309.08622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#35813;&#29615;&#22659;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#21644;&#25506;&#32034;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;slate&#25512;&#33616;&#35774;&#32622;&#65292;&#23558;&#20854;&#35270;&#20026;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#22312;&#32447;RL&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#30340;&#35774;&#32622;&#21644;&#37319;&#26679;&#26041;&#27861;&#26500;&#24314;&#20102;&#25512;&#33616;&#27169;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) in recommendation systems offers the potential to optimize recommendations for long-term user engagement. However, the environment often involves large state and action spaces, which makes it hard to efficiently learn and explore. In this work, we propose a sample-efficient representation learning algorithm, using the standard slate recommendation setup, to treat this as an online RL problem with low-rank Markov decision processes (MDPs). We also construct the recommender simulation environment with the proposed setup and sampling method.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2307.07130</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Digital Health Discussion Through Articles Published Until the Year 2021: A Digital Topic Modeling Approach. (arXiv:2307.07130v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
The digital health industry has grown in popularity since the 2010s, but there has been limited analysis of the topics discussed in the field across academic disciplines. This study aims to analyze the research trends of digital health-related articles published on the Web of Science until 2021, in order to understand the concentration, scope, and characteristics of the research. 15,950 digital health-related papers from the top 10 academic fields were analyzed using the Web of Science. The papers were grouped into three domains: public health, medicine, and electrical engineering and computer science (EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent Dirichlet Allocation (LDA) for topic modeling. The number of topics was determined based on coherence score, and topic compositions were compared using a homogeneity test. The number of optimal topics varied across domains and time periods. For public health, the first and second halves had 13 and 19 topics, res
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
Probe&#65306;&#23398;&#20064;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#25414;&#32465;&#36873;&#25321;&#20013;&#30340;&#20010;&#24615;&#21270;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36328;&#24230;&#30340;&#36873;&#25321;&#38656;&#35201;&#26435;&#34913;&#29616;&#22312;&#30340;&#25104;&#26412;&#21644;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#20854;&#20013;&#19968;&#31181;&#20855;&#20307;&#30340;&#36873;&#25321;&#26159;&#20915;&#23450;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#36824;&#26159;&#36873;&#25321;&#21253;&#21547;&#35813;&#29289;&#21697;&#30340;&#25414;&#32465;&#38144;&#21806;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#20010;&#20154;&#23545;&#36825;&#20123;&#36873;&#25321;&#20013;&#28041;&#21450;&#30340;&#22240;&#32032;&#26377;&#20934;&#30830;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#24863;&#30693;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#38750;&#29702;&#24615;&#21644;&#27425;&#20248;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#20559;&#24046;&#65306;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21152;&#26435;&#20989;&#25968;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#24046;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#26469;&#32771;&#34385;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#24341;&#20837;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#21069;&#26223;&#29702;&#35770;&#26469;&#32452;&#21512;&#21152;&#26435;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#29992;&#25143;&#36141;&#20080;&#25414;&#32465;&#38144;&#21806;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#22359;&#21305;&#37197;&#31639;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;&#65292;&#36890;&#36807;&#35745;&#31639;&#33258;&#30456;&#20284;&#30697;&#38453;&#26469;&#36798;&#21040;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.15356</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#22359;&#21305;&#37197;&#20998;&#21106;&#31639;&#27861;&#36827;&#34892;&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convolutive Block-Matching Segmentation Algorithm with Application to Music Structure Analysis. (arXiv:2210.15356v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#22359;&#21305;&#37197;&#31639;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;&#65292;&#36890;&#36807;&#35745;&#31639;&#33258;&#30456;&#20284;&#30697;&#38453;&#26469;&#36798;&#21040;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;&#65288;MSA&#65289;&#21253;&#25324;&#23558;&#19968;&#39318;&#27468;&#26354;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#37096;&#20998;&#65288;&#22914;&#8220;&#21103;&#27468;&#8221;&#65292;&#8220;&#35799;&#27468;&#8221;&#65292;&#8220;&#29420;&#22863;&#8221;&#31561;&#65289;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#23547;&#25214;&#27468;&#26354;&#30340;&#31616;&#21270;&#32452;&#32455;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#21367;&#31215;&#22359;&#21305;&#37197;&#65288;CBM&#65289;&#31639;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;MSA&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CBM&#31639;&#27861;&#26159;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#30456;&#20284;&#30697;&#38453;&#65292;&#36825;&#26159;MSA&#20013;&#30340;&#19968;&#31181;&#26631;&#20934;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#33258;&#30456;&#20284;&#30697;&#38453;&#26159;&#20174;&#38899;&#39057;&#20449;&#21495;&#30340;&#29305;&#24449;&#34920;&#31034;&#20013;&#35745;&#31639;&#20986;&#26469;&#30340;&#65292;&#26102;&#38388;&#26681;&#25454;&#23567;&#33410;&#21051;&#24230;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#35745;&#31639;&#33258;&#30456;&#20284;&#30697;&#38453;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;4&#20010;&#25351;&#26631;&#20013;&#26377;3&#20010;&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#19982;&#26377;&#30417;&#30563;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#65292;&#21516;&#26102;&#23427;&#26159;&#26080;&#30417;&#30563;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music Structure Analysis (MSA) consists of representing a song in sections (such as ``chorus'', ``verse'', ``solo'' etc), and can be seen as the retrieval of a simplified organization of the song. This work presents a new algorithm, called Convolutive Block-Matching (CBM) algorithm, devoted to MSA. In particular, the CBM algorithm is a dynamic programming algorithm, applying on autosimilarity matrices, a standard tool in MSA. In this work, autosimilarity matrices are computed from the feature representation of an audio signal, and time is sampled on the barscale. We study three different similarity functions for the computation of autosimilarity matrices. We report that the proposed algorithm achieves a level of performance competitive to that of supervised State-of-the-Art methods on 3 among 4 metrics, while being unsupervised.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.02355</link><description>&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#20316;&#20026;&#24320;&#20070;&#32771;&#35797;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#26080;&#27861;&#25512;&#24191;&#21040;&#37027;&#20123;&#32597;&#35265;&#25110;&#22256;&#38590;&#30340;&#27169;&#24335;&#20013;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25277;&#21462;&#35270;&#20026;&#19968;&#31181;&#24320;&#25918;&#24335;&#32771;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#29992;&#20110;&#26816;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#20363;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#20851;&#31995;&#26631;&#31614;&#20316;&#20026;&#35760;&#24518;&#30340;&#38190;&#20540;&#23545;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#22522;&#20110;PLM&#30340;&#22522;&#26412;&#36755;&#20986;&#19982;&#23384;&#20648;&#24211;&#19978;&#30340;&#38750;&#21442;&#25968;&#26368;&#36817;&#37051;&#20998;&#24067;&#26469;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.04392</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25110;&#28436;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#25628;&#32034;&#31163;&#25955;&#25110;&#36830;&#32493;&#25552;&#31034;&#25110;&#20248;&#21270;&#35821;&#35328;&#34920;&#36798;&#32773;&#65292;&#20294;&#23545;&#20110;&#28436;&#31034;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#28436;&#31034;&#31034;&#20363;&#23545;&#20110;&#26368;&#32456;&#30340;&#25552;&#31034;&#35843;&#20248;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25554;&#25300;&#12289;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#65292;&#23427;&#19981;&#38656;&#35201;&#36827;&#34892;&#28436;&#31034;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#65306;&#65288;i&#65289;&#23884;&#20837;&#21040;&#20219;&#20309;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#20013;&#65307;&#65288;ii&#65289;&#25193;&#23637;&#21040;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#24191;&#27867;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;LM-BFF&#21644;P-tuning&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
&lt;/p&gt;</description></item><item><title>D-HAN&#26159;&#19968;&#31181;&#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#37319;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#23558;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#26080;&#32541;&#25972;&#21512;&#65292;&#26377;&#25928;&#34920;&#31034;&#26032;&#38395;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21160;&#24577;&#36127;&#37319;&#26679;&#26041;&#27861;&#20248;&#21270;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2112.10085</link><description>&lt;p&gt;
D-HAN: &#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#19982;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
D-HAN: Dynamic News Recommendation with Hierarchical Attention Network. (arXiv:2112.10085v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10085
&lt;/p&gt;
&lt;p&gt;
D-HAN&#26159;&#19968;&#31181;&#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#37319;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#23558;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#26080;&#32541;&#25972;&#21512;&#65292;&#26377;&#25928;&#34920;&#31034;&#26032;&#38395;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21160;&#24577;&#36127;&#37319;&#26679;&#26041;&#27861;&#20248;&#21270;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38745;&#24577;&#30340;&#29992;&#25143;&#26032;&#38395;&#20132;&#20114;&#26041;&#24335;&#65292;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#23558;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#25972;&#21512;&#21040;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#21477;&#23376;&#12289;&#20803;&#32032;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#26032;&#38395;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#36127;&#37319;&#26679;&#26041;&#27861;&#26469;&#20248;&#21270;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation models often fall short in capturing users' preferences due to their static approach to user-news interactions. To address this limitation, we present a novel dynamic news recommender model that seamlessly integrates continuous time information to a hierarchical attention network that effectively represents news information at the sentence, element, and sequence levels. Moreover, we introduce a dynamic negative sampling method to optimize users' implicit feedback. To validate our model's effectiveness, we conduct extensive experiments on three real-world datasets. The results demonstrate the effectiveness of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#31185;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;ResearchGate&#31038;&#21306;&#65292;&#24182;&#21457;&#29616;&#20102;&#22320;&#29702;&#20301;&#32622;&#21644;&#27665;&#26063;&#23646;&#24615;&#23545;&#23398;&#26415;&#21327;&#20316;&#32593;&#32476;&#30340;&#24314;&#31435;&#21644;&#31185;&#30740;&#20135;&#20986;&#27969;&#31243;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36825;&#26377;&#21161;&#20110;&#20419;&#36827;&#23398;&#26415;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2003.05591</link><description>&lt;p&gt;
&#30740;&#31350;ResearchGate&#30340;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of ResearchGate, A Community Detection Approach. (arXiv:2003.05591v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.05591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#31185;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;ResearchGate&#31038;&#21306;&#65292;&#24182;&#21457;&#29616;&#20102;&#22320;&#29702;&#20301;&#32622;&#21644;&#27665;&#26063;&#23646;&#24615;&#23545;&#23398;&#26415;&#21327;&#20316;&#32593;&#32476;&#30340;&#24314;&#31435;&#21644;&#31185;&#30740;&#20135;&#20986;&#27969;&#31243;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36825;&#26377;&#21161;&#20110;&#20419;&#36827;&#23398;&#26415;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22788;&#20110;&#25968;&#25454;&#26102;&#20195;&#12290;&#22312;&#31185;&#23398;&#32593;&#32476;&#20013;&#30340;&#20132;&#27969;&#20026;&#24076;&#26395;&#22312;&#36825;&#20123;&#24040;&#22823;&#30340;&#30693;&#35782;&#24211;&#20013;&#21457;&#29616;&#38544;&#34255;&#27169;&#24335;&#30340;&#30740;&#31350;&#20154;&#21592;&#21019;&#36896;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#31185;&#23398;&#26041;&#27861;&#21019;&#24314;&#20102;&#20234;&#26391;&#31185;&#23398;&#26426;&#26500;&#30340;&#21327;&#20316;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;&#27169;&#22359;&#24615;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#32593;&#32476;&#31038;&#21306;&#12290;&#20998;&#26512;&#21327;&#20316;&#32593;&#32476;&#23545;&#20110;&#29702;&#35299;&#31185;&#30740;&#20135;&#20986;&#27969;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22320;&#29702;&#20301;&#32622;&#30340;&#20146;&#36817;&#24615;&#21644;&#27665;&#26063;&#23646;&#24615;&#22312;&#23398;&#26415;&#21327;&#20316;&#32593;&#32476;&#30340;&#24314;&#31435;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#26174;&#31034;&#20234;&#26391;&#39318;&#37117;&#24503;&#40657;&#20848;&#30340;&#33879;&#21517;&#31185;&#30740;&#20013;&#24515;&#23545;&#31185;&#30740;&#27963;&#21160;&#30340;&#20135;&#20986;&#27969;&#31243;&#20855;&#26377;&#24378;&#22823;&#24433;&#21709;&#21147;&#12290;&#36825;&#20123;&#23398;&#26415;&#35770;&#25991;&#20027;&#35201;&#34987;&#32654;&#22269;&#12289;&#20013;&#22269;&#12289;&#21360;&#24230;&#21644;&#20234;&#26391;&#27983;&#35272;&#21644;&#19979;&#36733;&#12290;&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#22312;&#20110;&#36890;&#36807;&#21457;&#29616;&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#31038;&#21306;&#24182;&#25214;&#21040;&#26426;&#26500;&#38388;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#20419;&#36827;&#23398;&#26415;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are living in the data age. Communications over scientific networks creates new opportunities for researchers who aim to discover the hidden pattern in these huge repositories. This study utilizes network science to create collaboration network of Iranian Scientific Institutions. A modularity-based approach applied to find network communities. To reach a big picture of science production flow, analysis of the collaboration network is crucial. Our results demonstrated that geographic location closeness and ethnic attributes has important roles in academic collaboration network establishment. Besides, it shows that famous scientific centers in the capital city of Iran, Tehran has strong influence on the production flow of scientific activities. These academic papers are mostly viewed and downloaded from the United State of America, China, India, and Iran. The motivation of this research is that by discovering hidden communities in the network and finding the structure of intuitions co
&lt;/p&gt;</description></item></channel></rss>