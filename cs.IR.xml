<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#31687;&#35770;&#25991;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20869;&#32622;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#19978;&#19979;&#25991;&#28436;&#31034;&#30452;&#25509;&#29983;&#25104;Web URLs&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.09612</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20869;&#32622;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Built-in Autoregressive Search Engines. (arXiv:2305.09612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20869;&#32622;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#19978;&#19979;&#25991;&#28436;&#31034;&#30452;&#25509;&#29983;&#25104;Web URLs&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#26816;&#32034;&#26159;&#26631;&#20934;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#29616;&#26377;&#30340;&#21452;&#32534;&#30721;&#22120;&#23494;&#38598;&#26816;&#32034;&#22120;&#29420;&#31435;&#22320;&#33719;&#21462;&#38382;&#39064;&#21644;&#25991;&#26723;&#30340;&#34920;&#31034;&#65292;&#21482;&#20801;&#35768;&#23427;&#20204;&#20043;&#38388;&#30340;&#27973;&#23618;&#20132;&#20114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26368;&#36817;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#27744;&#20013;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#26469;&#26367;&#25442;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#30340;&#35757;&#32451;&#25104;&#26412;&#38543;&#30528;&#20505;&#36873;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#24613;&#21095;&#19978;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#25351;&#31034;&#30452;&#25509;&#29983;&#25104;&#25991;&#26723;&#26816;&#32034;&#30340;URL&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25552;&#20379;&#19968;&#20123;{Query-URL}&#23545;&#20316;&#20026;&#19978;&#19979;&#25991;&#28436;&#31034;&#26102;&#65292;LLMs&#21487;&#20197;&#29983;&#25104;Web URL&#65292;&#20854;&#20013;&#36817;90&#65285;&#30340;&#30456;&#24212;&#25991;&#26723;&#21253;&#21547;&#24320;&#25918;&#22495;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#12290;&#36825;&#26679;&#65292;LLMs&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20869;&#32622;&#25628;&#32034;&#24341;&#25806;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#20197;&#26144;&#23556;&#38382;&#39064;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.  Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#21151;&#32791;&#65292;&#24182;&#22312;&#19987;&#26377;ACR&#25968;&#25454;&#38598;&#31934;&#24230;&#12289;&#26816;&#32034;&#36895;&#24230;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#40065;&#26834;&#24615;&#31561;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#26368;&#23567;&#21704;&#24076;&#30340;&#38899;&#39057;&#25351;&#32441;&#12290;</title><link>http://arxiv.org/abs/2305.09559</link><description>&lt;p&gt;
&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#20013;&#30340;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust and lightweight audio fingerprint for Automatic Content Recognition. (arXiv:2305.09559v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#21151;&#32791;&#65292;&#24182;&#22312;&#19987;&#26377;ACR&#25968;&#25454;&#38598;&#31934;&#24230;&#12289;&#26816;&#32034;&#36895;&#24230;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#40065;&#26834;&#24615;&#31561;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#26368;&#23567;&#21704;&#24076;&#30340;&#38899;&#39057;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#65288;ACR&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#21644;&#32479;&#35745;&#21464;&#25442;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#38899;&#39057;&#29255;&#27573;&#30340;&#32039;&#20945;&#25351;&#32441;&#65292;&#36825;&#20123;&#25351;&#32441;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38477;&#32423;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#20351;&#29992;&#26469;&#33258;&#25968;&#30334;&#19975;&#21488;&#30005;&#35270;&#30340;&#25351;&#32441;&#35782;&#21035;&#25968;&#21315;&#23567;&#26102;&#30340;&#20869;&#23481;&#12290;&#25351;&#32441;&#30340;&#39640;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#21033;&#29992;&#29616;&#26377;&#30340;GPU&#20860;&#23481;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#65288;ANN&#65289;&#25628;&#32034;&#31639;&#27861;&#20351;&#36825;&#19968;&#28857;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25351;&#32441;&#29983;&#25104;&#21487;&#20197;&#22312;&#35745;&#31639;&#21463;&#38480;&#30340;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#20197;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents a novel audio fingerprinting system for Automatic Content Recognition (ACR). By using signal processing techniques and statistical transformations, our proposed method generates compact fingerprints of audio segments that are robust to noise degradations present in real-world audio. The system is designed to be highly scalable, with the ability to identify thousands of hours of content using fingerprints generated from millions of TVs. The fingerprint's high temporal correlation and utilization of existing GPU-compatible Approximate Nearest Neighbour (ANN) search algorithms make this possible. Furthermore, the fingerprint generation can run on low-power devices with limited compute, making it accessible to a wide range of applications. Experimental results show improvements in our proposed system compared to a min-hash based audio fingerprint on all evaluated metrics, including accuracy on proprietary ACR datasets, retrieval speed, memory usage, and robustn
&lt;/p&gt;</description></item><item><title>&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09550</link><description>&lt;p&gt;
PII&#30340;&#29983;&#21629;--&#19968;&#31181;PII&#28151;&#28102;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09550
&lt;/p&gt;
&lt;p&gt;
&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#19990;&#30028;&#20013;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25200;&#21160;&#25216;&#26415;&#26469;&#20943;&#23569;(&#25935;&#24863;)&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#20449;&#24687;(PII)&#25968;&#25454;&#30340;&#36807;&#24230;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;&#25968;&#25454;&#25200;&#21160;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#26174;&#30528;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;PII&#30340;&#29983;&#21629;&#8221;--&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;API&#26469;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#36827;&#34892;&#25509;&#21475;&#65292;&#19968;&#20010;&#22522;&#20110;&#37197;&#32622;&#30340;&#28151;&#28102;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;LLMs&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#19978;&#19979;&#25991;&#20445;&#23384;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#21407;&#22987;PII&#21644;&#20854;&#36716;&#25442;&#21518;&#30340;&#20154;&#36896;PII&#23545;&#24212;&#30340;&#26144;&#23556;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09330</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20844;&#24179;&#24615;: &#26041;&#27861;&#21644;&#35780;&#20272;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation. (arXiv:2305.09330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09330
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#27700;&#24179;&#19981;&#26029;&#25552;&#39640;&#30340;&#24403;&#21069;&#31038;&#20250;&#20013;&#65292;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#23548;&#33322;&#26085;&#30410;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#65292;&#20197;&#21450;&#24110;&#21161;&#20379;&#24212;&#21830;&#21521;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#33829;&#38144;&#20135;&#21697;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#27495;&#35270;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#65292;&#36825;&#20419;&#20351;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30740;&#31350;&#22914;&#20309;&#30830;&#20445;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#32844;&#19994;&#25512;&#33616;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#20307;&#29616;&#65292;&#21382;&#21490;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#23558;&#19968;&#20010;&#24615;&#21035;&#19982;&#36739;&#20302;&#30340;&#24037;&#36164;&#25110;&#21051;&#26495;&#21360;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#22320;&#65292;&#29992;&#25143;&#20844;&#24179;&#24615;&#20851;&#27880;&#22914;&#20309;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#20102;&#24456;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#27495;&#35270;&#12290;&#25152;&#36848;&#27495;&#35270;&#30340;&#24615;&#36136;&#21462;&#20915;&#20110;&#25152;&#22788;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to scalability. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. The growing awareness of discrimination in machine learning methods has recently motivated both academia and industry to research how fairness can be ensured in recommender systems. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches for addressing different types of discrimination. The nature of said discrimination depends on the setting 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#35775;&#38382;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#12290;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#26159;&#20004;&#20010;&#29420;&#31435;&#20294;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#25512;&#21160;&#22810;&#26679;&#24615;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.09319</link><description>&lt;p&gt;
&#20449;&#24687;&#35775;&#38382;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness and Diversity in Information Access Systems. (arXiv:2305.09319v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20449;&#24687;&#35775;&#38382;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#12290;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#26159;&#20004;&#20010;&#29420;&#31435;&#20294;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#25512;&#21160;&#22810;&#26679;&#24615;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#22996;&#21592;&#20250;&#65288;EC&#65289;&#35774;&#31435;&#30340;&#20154;&#24037;&#26234;&#33021;&#39640;&#32423;&#19987;&#23478;&#32452;&#25552;&#20986;&#20102;&#23454;&#29616;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19971;&#20010;&#37325;&#35201;&#35201;&#27714;&#20043;&#19968;&#26159;&#22810;&#26679;&#24615;&#12289;&#38750;&#27495;&#35270;&#21644;&#20844;&#24179;&#24615;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#20851;&#27880;&#20449;&#24687;&#35775;&#38382;&#31995;&#32479;&#21644;&#25490;&#21517;&#25991;&#29486;&#65292;&#38416;&#36848;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#20004;&#20010;&#29420;&#31435;&#27010;&#24565;&#30340;&#23494;&#20999;&#30456;&#20851;&#24615;&#12290;&#36825;&#20004;&#20010;&#27010;&#24565;&#19981;&#24212;&#35813;&#34987;&#20114;&#25442;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20195;&#34920;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20215;&#20540;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#20063;&#19981;&#33021;&#34987;&#35748;&#20026;&#26159;&#23436;&#20840;&#19981;&#30456;&#20851;&#30340;&#12290;&#25512;&#21160;&#22810;&#26679;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#20844;&#24179;&#65292;&#20294;&#20419;&#36827;&#22810;&#26679;&#24615;&#30830;&#23454;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#30340;&#30452;&#35273;&#65292;&#26088;&#22312;&#20943;&#23569;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the seven key requirements to achieve trustworthy AI proposed by the High-Level Expert Group on Artificial Intelligence (AI-HLEG) established by the European Commission (EC), the fifth requirement ("Diversity, non-discrimination and fairness") declares: "In order to achieve Trustworthy AI, we must enable inclusion and diversity throughout the entire AI system's life cycle. [...] This requirement is closely linked with the principle of fairness". In this paper, we try to shed light on how closely these two distinct concepts, diversity and fairness, may be treated by focusing on information access systems and ranking literature. These concepts should not be used interchangeably because they do represent two different values, but what we argue is that they also cannot be considered totally unrelated or divergent. Having diversity does not imply fairness, but fostering diversity can effectively lead to fair outcomes, an intuition behind several methods proposed to mitigate the dispar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybRank&#30340;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#23454;&#29616;&#27573;&#33853;&#21327;&#20316;&#65292;&#20877;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#34987;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09313</link><description>&lt;p&gt;
&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid and Collaborative Passage Reranking. (arXiv:2305.09313v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybRank&#30340;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#23454;&#29616;&#27573;&#33853;&#21327;&#20316;&#65292;&#20877;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#34987;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27573;&#33853;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21021;&#22987;&#26816;&#32034;&#32467;&#26524;&#21487;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#38656;&#35201;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26041;&#26696;&#36827;&#34892;&#25913;&#21892;&#12290;&#29616;&#26377;&#30340;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#20110;&#20016;&#23500;&#26597;&#35810;&#21644;&#27599;&#20010;&#27573;&#33853;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#30053;&#20102;&#22312;&#21021;&#22987;&#26816;&#32034;&#21015;&#34920;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#22810;&#20010;&#27573;&#33853;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65288;HybRank&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#36827;&#34892;&#27573;&#33853;&#21327;&#20316;&#65292;&#24182;&#32467;&#21512;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#29616;&#25104;&#30340;&#26816;&#32034;&#22120;&#29305;&#24449;&#65292;HybRank&#26159;&#19968;&#20010;&#25554;&#20214;&#20877;&#25490;&#24207;&#22120;&#65292;&#33021;&#22815;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#27604;&#26222;&#36941;&#30340;&#26816;&#32034;&#21644;&#20877;&#25490;&#24207;&#26041;&#27861;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#65292;&#24182;&#39564;&#35777;&#20102;HybRank&#30340;&#26680;&#24515;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of upstream retrievers for passage collaboration and incorporates the lexical and semantic properties of sparse and dense retrievers for reranking. Besides, built on off-the-shelf retriever features, HybRank is a plug-in reranker capable of enhancing arbitrary passage lists including previously reranked ones. Extensive experiments demonstrate the stable improvements of performance over prevalent retrieval and reranking methods, and verify the effectiveness of the core components of HybRank.
&lt;/p&gt;</description></item><item><title>HyHTM&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21452;&#26354;&#20960;&#20309;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#32435;&#20837;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;HyHTM&#26356;&#22909;&#22320;&#20851;&#27880;&#20027;&#39064;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#65292;&#24182;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;HyHTM&#30340;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.09258</link><description>&lt;p&gt;
HyHTM: &#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HyHTM: Hyperbolic Geometry based Hierarchical Topic Models. (arXiv:2305.09258v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09258
&lt;/p&gt;
&lt;p&gt;
HyHTM&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21452;&#26354;&#20960;&#20309;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#32435;&#20837;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;HyHTM&#26356;&#22909;&#22320;&#20851;&#27880;&#20027;&#39064;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#65292;&#24182;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;HyHTM&#30340;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#23545;&#20110;&#21457;&#29616;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#24120;&#24120;&#20135;&#29983;&#20302;&#23618;&#27425;&#20027;&#39064;&#19982;&#20854;&#39640;&#23618;&#27425;&#20027;&#39064;&#26080;&#20851;&#19988;&#19981;&#22815;&#20855;&#20307;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HyHTM &#30340;&#21452;&#26354;&#20960;&#20309;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21452;&#26354;&#20960;&#20309;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#32435;&#20837;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#19982;&#22235;&#20010;&#22522;&#32447;&#20570;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HyHTM &#21487;&#20197;&#26356;&#22909;&#22320;&#20851;&#27880;&#20027;&#39064;&#20043;&#38388;&#29238;&#23376;&#20851;&#31995;&#12290;HyHTM &#20135;&#29983;&#36830;&#36143;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#36890;&#29992;&#30340;&#39640;&#23618;&#27425;&#20027;&#39064;&#21040;&#20855;&#20307;&#30340;&#20302;&#23618;&#27425;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23567;&#12290;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#31639;&#27861;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lowerlevel topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry based Hierarchical Topic Models - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to explicitly model hierarchies in topic models. Experimental results with four baselines show that HyHTM can better attend to parent-child relationships among topics. HyHTM produces coherent topic hierarchies that specialise in granularity from generic higher-level topics to specific lowerlevel topics. Further, our model is significantly faster and leaves a much smaller memory footprint than our best-performing baseline.We have made the source code for our algorithm publicly accessible.
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;EMKD&#65292;&#23427;&#37319;&#29992;&#22810;&#20010;&#24182;&#34892;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#26681;&#25454;&#25152;&#26377;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EMKD&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14668</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensemble Modeling with Contrastive Knowledge Distillation for Sequential Recommendation. (arXiv:2304.14668v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;EMKD&#65292;&#23427;&#37319;&#29992;&#22810;&#20010;&#24182;&#34892;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#26681;&#25454;&#25152;&#26377;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EMKD&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#26088;&#22312;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20852;&#36259;&#65292;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#27425;&#30340;&#20559;&#22909;&#29289;&#21697;&#12290;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#29983;&#25104;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20391;&#37325;&#20110;&#35774;&#35745;&#26356;&#24378;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#23581;&#35797;&#20351;&#29992;&#35757;&#32451;&#19968;&#32452;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36825;&#27604;&#21333;&#20010;&#32593;&#32476;&#26356;&#24378;&#22823;&#65292;&#22240;&#20026;&#19968;&#32452;&#24182;&#34892;&#32593;&#32476;&#21487;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#21363;EMKD&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#24182;&#26681;&#25454;&#25152;&#26377;&#36825;&#20123;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#25512;&#33616;&#29289;&#21697;&#12290;&#20026;&#20102;&#20419;&#36827;&#24182;&#34892;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23427;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#32593;&#32476;&#36716;&#31227;&#21040;&#22810;&#20010;&#23398;&#29983;&#32593;&#32476;&#20013;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;EMKD&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#21644;&#38598;&#25104;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation aims to capture users' dynamic interest and predicts the next item of users' preference. Most sequential recommendation methods use a deep neural network as sequence encoder to generate user and item representations. Existing works mainly center upon designing a stronger sequence encoder. However, few attempts have been made with training an ensemble of networks as sequence encoders, which is more powerful than a single network because an ensemble of parallel networks can yield diverse prediction results and hence better accuracy. In this paper, we present Ensemble Modeling with contrastive Knowledge Distillation for sequential recommendation (EMKD). Our framework adopts multiple parallel networks as an ensemble of sequence encoders and recommends items based on the output distributions of all these networks. To facilitate knowledge transfer between parallel networks, we propose a novel contrastive knowledge distillation approach, which performs knowledge tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07763</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-optimized Contrastive Learning for Sequential Recommendation. (arXiv:2304.07763v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26159;&#35299;&#20915;&#31232;&#30095;&#19988;&#21547;&#22122;&#22768;&#25512;&#33616;&#25968;&#25454;&#30340;&#19968;&#20010;&#26032;&#20852;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#38024;&#23545;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#22686;&#24378;&#65292;&#35201;&#20040;&#21482;&#20351;&#29992;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#24456;&#38590;&#25512;&#24191;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard 
&lt;/p&gt;</description></item><item><title>BERT4Loc&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20301;&#32622;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24314;&#35758;&#65292;&#30456;&#36739;&#20110;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;POI&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.01375</link><description>&lt;p&gt;
BERT4Loc&#65306;&#22522;&#20110;BERT&#30340;&#20301;&#32622;&#25512;&#33616;&#31995;&#32479;--POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
BERT4Loc: BERT for Location -- POI Recommender System. (arXiv:2208.01375v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01375
&lt;/p&gt;
&lt;p&gt;
BERT4Loc&#26159;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20301;&#32622;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24314;&#35758;&#65292;&#30456;&#36739;&#20110;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;POI&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20174;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#20301;&#32622;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#20379;&#26377;&#25928;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#25512;&#33616;&#65292;&#20998;&#26512;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#21644;&#20559;&#22909;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20301;&#32622;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;Transformers&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#30456;&#27604;&#20110;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;POI&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#38468;&#21152;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommending points of interest (POIs) is a challenging task that requires extracting comprehensive location data from location-based social media platforms. To provide effective location-based recommendations, it's important to analyze users' historical behavior and preferences. In this study, we present a sophisticated location-aware recommendation system that uses Bidirectional Encoder Representations from Transformers (BERT) to offer personalized location-based suggestions. Our model combines location information and user preferences to provide more relevant recommendations compared to models that predict the next POI in a sequence. Our experiments on two benchmark dataset show that our BERT-based model outperforms various state-of-the-art sequential models. Moreover, we see the effectiveness of the proposed model for quality through additional experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>