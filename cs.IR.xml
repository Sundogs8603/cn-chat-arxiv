<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#27969;&#25968;&#25454;&#30340;&#27969;&#24335;CTR&#39044;&#27979;&#20219;&#21153;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#23558;CTR&#39044;&#27979;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#36716;&#21464;&#20026;&#27969;&#24335;&#25968;&#25454;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#35774;&#32622;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.07509</link><description>&lt;p&gt;
&#27969;&#24335;CTR&#39044;&#27979;&#65306;&#37325;&#26032;&#24605;&#32771;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25512;&#33616;&#20219;&#21153;&#30340;&#30495;&#23454;&#19990;&#30028;&#27969;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Streaming CTR Prediction: Rethinking Recommendation Task for Real-World Streaming Data. (arXiv:2307.07509v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#27969;&#25968;&#25454;&#30340;&#27969;&#24335;CTR&#39044;&#27979;&#20219;&#21153;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#23558;CTR&#39044;&#27979;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#36716;&#21464;&#20026;&#27969;&#24335;&#25968;&#25454;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#35774;&#32622;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#20219;&#21153;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#27169;&#22411;&#36890;&#24120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;&#22312;&#21160;&#24577;&#27969;&#25968;&#25454;&#19978;&#12290;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36825;&#20123;&#27969;&#25968;&#25454;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20363;&#22914;&#20998;&#24067;&#20559;&#31227;&#12289;&#26102;&#38388;&#38750;&#24179;&#31283;&#24615;&#21644;&#31995;&#32479;&#20559;&#24046;&#65292;&#36825;&#32473;&#25512;&#33616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21033;&#29992;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#23558;CTR&#39044;&#27979;&#35270;&#20026;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#65288;&#21363;i.i.d.&#20551;&#35774;&#65289;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#27969;&#24335;&#25968;&#25454;&#24773;&#26223;&#19979;&#23558;CTR&#39044;&#27979;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#27969;&#24335;CTR&#39044;&#27979;&#20219;&#21153;&#12290;&#30456;&#24212;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#38376;&#30340;&#22522;&#20934;&#35774;&#32622;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#35780;&#20272;&#21644;&#20998;&#26512;&#27169;&#22411;&#22312;&#27969;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#19982;&#20256;&#32479;CTR&#39044;&#27979;&#20219;&#21153;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#24433;&#21709;&#27969;&#25968;&#25454;&#20013;CTR&#39044;&#27979;&#20219;&#21153;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Click-Through Rate (CTR) prediction task is critical in industrial recommender systems, where models are usually deployed on dynamic streaming data in practical applications. Such streaming data in real-world recommender systems face many challenges, such as distribution shift, temporal non-stationarity, and systematic biases, which bring difficulties to the training and utilizing of recommendation models. However, most existing studies approach the CTR prediction as a classification task on static datasets, assuming that the train and test sets are independent and identically distributed (a.k.a, i.i.d. assumption). To bridge this gap, we formulate the CTR prediction problem in streaming scenarios as a Streaming CTR Prediction task. Accordingly, we propose dedicated benchmark settings and metrics to evaluate and analyze the performance of the models in streaming data. To better understand the differences compared to traditional CTR prediction tasks, we delve into the factors that m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiTL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#25551;&#36848;&#22270;&#20687;&#65292;&#20197;&#23454;&#29616;&#24369;&#30417;&#30563;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#36827;&#34892;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#23545;&#35937;&#26816;&#27979;&#22120;&#29983;&#25104;&#30340;&#23545;&#35937;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2307.07341</link><description>&lt;p&gt;
PiTL: &#36890;&#36807;&#25552;&#31034;&#36827;&#34892;&#24369;&#30417;&#30563;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#36827;&#34892;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting. (arXiv:2307.07341v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiTL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#25551;&#36848;&#22270;&#20687;&#65292;&#20197;&#23454;&#29616;&#24369;&#30417;&#30563;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#36827;&#34892;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#23545;&#35937;&#26816;&#27979;&#22120;&#29983;&#25104;&#30340;&#23545;&#35937;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#24456;&#22909;&#22320;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#20013;&#25512;&#24191;VLP&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#23427;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36825;&#38656;&#35201;&#32321;&#29712;&#21644;&#26114;&#36149;&#30340;&#31574;&#21010;&#12290;&#30456;&#21453;&#65292;&#24369;&#30417;&#30563;VLP&#65288;W-VLP&#65289;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;OD&#65289;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#30340;&#23545;&#35937;&#26631;&#31614;&#26469;&#25506;&#32034;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38656;&#35201;&#37197;&#23545;&#30340;&#20449;&#24687;&#65292;&#21363;&#22270;&#20687;&#21644;&#23545;&#35937;&#32423;&#27880;&#37322;&#65292;&#20316;&#20026;&#35757;&#32451;OD&#30340;&#30417;&#30563;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#30417;&#30563;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#30693;&#35782;&#26469;&#25551;&#36848;&#22270;&#20687;&#30340;"PiTL"&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#22270;&#20687;&#30340;&#31867;&#21035;&#26631;&#31614;&#65292;&#20363;&#22914;&#28860;&#27833;&#21378;&#65292;&#30001;LLM&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#28860;&#27833;&#21378;&#21487;&#20197;&#24102;&#26377;&#22823;&#22411;&#20648;&#32592;&#12289;&#31649;&#36947;&#31561;&#65292;&#29992;&#20316;&#35821;&#35328;&#26041;&#38754;&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#20123;&#30693;&#35782;&#34917;&#20805;&#20102;&#26368;&#26377;&#21487;&#33021;&#20986;&#29616;&#22312;&#22330;&#26223;&#20013;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20849;&#21516;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD.  To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#25512;&#33616;&#31995;&#32479;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#21306;&#22495;&#36827;&#34892;&#21010;&#20998;&#24182;&#20351;&#29992;AdaSim&#23545;&#21306;&#22495;&#36171;&#20104;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#24418;&#25104;&#27491;&#36127;&#26679;&#26412;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#38598;&#36873;&#25321;&#27169;&#22411;&#26469;&#32553;&#23567;&#26680;&#24515;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.07321</link><description>&lt;p&gt;
NS4AR: &#19968;&#31181;&#26032;&#30340;&#12289;&#19987;&#27880;&#20110;&#37319;&#26679;&#21306;&#22495;&#30340;&#22270;&#24418;&#25512;&#33616;&#31995;&#32479;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NS4AR: A new, focused on sampling areas sampling method in graphical recommendation Systems. (arXiv:2307.07321v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#25512;&#33616;&#31995;&#32479;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#21306;&#22495;&#36827;&#34892;&#21010;&#20998;&#24182;&#20351;&#29992;AdaSim&#23545;&#21306;&#22495;&#36171;&#20104;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#24418;&#25104;&#27491;&#36127;&#26679;&#26412;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#38598;&#36873;&#25321;&#27169;&#22411;&#26469;&#32553;&#23567;&#26680;&#24515;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#36127;&#37319;&#26679;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;&#26412;&#25991;&#36873;&#25321;&#20102;&#19968;&#20123;&#20856;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#19978;&#30340;&#19968;&#20123;&#26368;&#26032;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#20316;&#20026;&#22522;&#32447;&#12290;&#22522;&#20110;&#20856;&#22411;&#30340;&#22270;&#24418;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#26679;&#26412;&#21306;&#22495;&#21010;&#20998;&#20026;&#25351;&#23450;&#30340;n&#20010;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;AdaSim&#23545;&#36825;&#20123;&#21306;&#22495;&#36171;&#20104;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#24418;&#25104;&#27491;&#26679;&#26412;&#38598;&#21644;&#36127;&#26679;&#26412;&#38598;&#12290;&#30001;&#20110;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#21644;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#38598;&#36873;&#25321;&#27169;&#22411;&#26469;&#32553;&#23567;&#26680;&#24515;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of graphical recommender system depends on the quantity and quality of negative sampling. This paper selects some typical recommender system models, as well as some latest negative sampling strategies on the models as baseline. Based on typical graphical recommender model, we divide sample region into assigned-n areas and use AdaSim to give different weight to these areas to form positive set and negative set. Because of the volume and significance of negative items, we also proposed a subset selection model to narrow the core negative samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#20351;&#29992;&#30340;&#28151;&#21512;&#23457;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;&#26469;&#25903;&#25345;&#20182;&#20204;&#22312;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#24615;&#33021;&#12290;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25509;&#21463;&#20102;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07317</link><description>&lt;p&gt;
&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#30340;&#28151;&#21512;&#23457;&#26680;&#65306;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;
&lt;/p&gt;
&lt;p&gt;
Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#20351;&#29992;&#30340;&#28151;&#21512;&#23457;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;&#26469;&#25903;&#25345;&#20182;&#20204;&#22312;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#24615;&#33021;&#12290;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25509;&#21463;&#20102;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#27491;&#21162;&#21147;&#22788;&#29702;&#35780;&#35770;&#21306;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#23457;&#26680;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25903;&#25345;&#21644;&#25480;&#26435;&#23457;&#26680;&#21592;&#36873;&#25321;&#29305;&#33394;&#24086;&#23376;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27979;&#35797;&#38598;&#19978;&#30340;&#26368;&#20339;&#20998;&#31867;F1&#20998;&#25968;&#20026;0.44&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#39564;&#35777;&#25991;&#31456;&#19978;&#35266;&#23519;&#21040;&#20102;&#22343;&#20540;NDCG@5&#30340;&#26368;&#20339;&#20540;&#20026;0.87&#12290;&#22312;&#19987;&#23478;&#35780;&#20272;&#20013;&#65292;&#20869;&#23481;&#31649;&#29702;&#21592;&#26681;&#25454;&#25512;&#33616;&#32467;&#26524;&#36873;&#25321;&#35201;&#25512;&#33616;&#30340;&#35780;&#35770;&#65292;&#24471;&#21040;&#20102;0.83&#30340;NDCG&#20998;&#25968;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#39318;&#20808;&#65292;&#28155;&#21152;&#25991;&#26412;&#29305;&#24449;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24471;&#20998;&#65307;&#20854;&#27425;&#65292;&#34429;&#28982;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#20173;&#28982;&#26377;&#19968;&#23450;&#30340;&#20027;&#35266;&#24615;&#65292;&#20294;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#25152;&#26377;&#34987;&#35780;&#20272;&#30340;&#25512;&#33616;&#20013;&#37117;&#25214;&#21040;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#38500;&#20102;&#19968;&#20010;&#20363;&#22806;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#36808;&#21521;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online news outlets are grappling with the moderation of user-generated content within their comment section. We present a recommender system based on ranking class probabilities to support and empower the moderator in choosing featured posts, a time-consuming task. By combining user and textual content features we obtain an optimal classification F1-score of 0.44 on the test set. Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of validation articles. As an expert evaluation, content moderators assessed the output of a random selection of articles by choosing comments to feature based on the recommendations, which resulted in a NDCG score of 0.83. We conclude that first, adding text features yields the best score and second, while choosing featured content remains somewhat subjective, content moderators found suitable comments in all but one evaluated recommendations. We end the paper by analyzing our best-performing model, a step towards transparency and explaina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07164</link><description>&lt;p&gt;
&#23398;&#20064;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25152;&#36873;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#21487;&#20197;&#20026;LLMs&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#35757;&#32451;&#22522;&#20110;LLM&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#35780;&#20272;&#20505;&#36873;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#22312;30&#20010;&#20219;&#21153;&#22871;&#20214;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#26816;&#32034;&#20855;&#26377;&#30456;&#20284;&#27169;&#24335;&#30340;&#31034;&#20363;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#36825;&#31181;&#22686;&#30410;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#20013;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2307.07130</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Digital Health Discussion Through Articles Published Until the Year 2021: A Digital Topic Modeling Approach. (arXiv:2307.07130v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
The digital health industry has grown in popularity since the 2010s, but there has been limited analysis of the topics discussed in the field across academic disciplines. This study aims to analyze the research trends of digital health-related articles published on the Web of Science until 2021, in order to understand the concentration, scope, and characteristics of the research. 15,950 digital health-related papers from the top 10 academic fields were analyzed using the Web of Science. The papers were grouped into three domains: public health, medicine, and electrical engineering and computer science (EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent Dirichlet Allocation (LDA) for topic modeling. The number of topics was determined based on coherence score, and topic compositions were compared using a homogeneity test. The number of optimal topics varied across domains and time periods. For public health, the first and second halves had 13 and 19 topics, res
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#20020;&#24202;&#35760;&#24405;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.07051</link><description>&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#65306;&#39044;&#27979;&#33021;&#21147;&#38543;&#20020;&#24202;&#35760;&#24405;&#31867;&#22411;&#21644;&#35760;&#24405;&#37096;&#20998;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#20020;&#24202;&#35760;&#24405;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20852;&#36259;&#37325;&#26032;&#29123;&#36215;&#12290;&#20020;&#24202;&#35760;&#24405;&#30340;&#19968;&#20010;&#21306;&#21035;&#29305;&#28857;&#26159;&#23427;&#20204;&#36328;&#36234;&#22810;&#20010;&#38271;&#25991;&#26723;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#12290;&#20020;&#24202;&#35760;&#24405;&#30340;&#29420;&#29305;&#32467;&#26500;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#36873;&#25321;&#65306;&#24403;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#22120;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#26102;&#65292;&#24212;&#36873;&#25321;&#20020;&#24202;&#35760;&#24405;&#30340;&#21738;&#20010;&#37096;&#20998;&#20316;&#20026;&#36755;&#20837;&#65311;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#36873;&#25321;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#36755;&#20837;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25130;&#26029;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#39640;&#39044;&#27979;&#33021;&#21147;&#37096;&#20998;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;MIMIC-III&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;1&#65289;&#39044;&#27979;&#33021;&#21147;&#20998;&#24067;&#22312;&#25252;&#29702;&#35760;&#24405;&#21644;&#20986;&#38498;&#35760;&#24405;&#20043;&#38388;&#26159;&#19981;&#21516;&#30340;&#65307;2&#65289;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item></channel></rss>