<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.02059</link><description>&lt;p&gt;
IISAN&#65306;&#20351;&#29992;&#35299;&#32806;PEFT&#26377;&#25928;&#22320;&#35843;&#25972;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02059
&lt;/p&gt;
&lt;p&gt;
IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#36716;&#21464;&#24615;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#24120;&#29992;&#20110;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#20197;&#36827;&#34892;&#25512;&#33616;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20248;&#20808;&#32771;&#34385;&#21442;&#25968;&#25928;&#29575;&#65292;&#36890;&#24120;&#24573;&#30053;GPU&#20869;&#23384;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#31561;&#20851;&#38190;&#22240;&#32032;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;IISAN&#65288;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#20391;&#38754;&#36866;&#24212;&#32593;&#32476;&#65289;&#65292;&#19968;&#20010;&#20351;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#30340;&#31616;&#21333;&#21363;&#25554;&#21363;&#29992;&#26550;&#26500;&#12290;IISAN&#19982;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;GPU&#20869;&#23384;&#20351;&#29992;&#37327; - &#23545;&#20110;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;47GB&#38477;&#20302;&#21040;&#20165;3GB&#12290;&#27492;&#22806;&#65292;&#19982;FFT&#30456;&#27604;&#65292;&#23427;&#23558;&#27599;&#20010;&#26102;&#20195;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;443&#31186;&#21152;&#36895;&#21040;22&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02059v1 Announce Type: new  Abstract: Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.06567</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06567
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Content-based image retrieval&#65288;CBIR&#65289;&#26377;&#26395;&#26174;&#33879;&#25913;&#21892;&#25918;&#23556;&#23398;&#20013;&#30340;&#35786;&#26029;&#36741;&#21161;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#30340;&#29616;&#25104;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#22235;&#31181;&#27169;&#24577;&#21644;161&#31181;&#30149;&#29702;&#23398;&#30340;160&#19975;&#24352;2D&#25918;&#23556;&#22270;&#20687;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#24369;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;P@1&#21487;&#36798;0.594&#12290;&#36825;&#31181;&#24615;&#33021;&#19981;&#20165;&#19982;&#19987;&#38376;&#21270;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26816;&#32034;&#30149;&#29702;&#23398;&#19982;&#35299;&#21078;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#34920;&#26126;&#20934;&#30830;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06567v1 Announce Type: cross  Abstract: Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our
&lt;/p&gt;</description></item></channel></rss>