<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#31038;&#21306;&#24212;&#35813;&#37325;&#26032;&#23558;&#30740;&#31350;&#35758;&#31243;&#32858;&#28966;&#20110;&#31038;&#20250;&#38656;&#27714;&#65292;&#28040;&#38500;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#36879;&#26126;&#24230;&#21644;&#36947;&#24503;&#30740;&#31350;&#19982;&#20449;&#24687;&#26816;&#32034;&#20854;&#20182;&#39046;&#22495;&#20043;&#38388;&#30340;&#20154;&#20026;&#38548;&#31163;&#65292;&#31215;&#26497;&#35774;&#23450;&#30740;&#31350;&#35758;&#31243;&#65292;&#28608;&#21169;&#26500;&#24314;&#26126;&#30830;&#38472;&#36848;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#25152;&#21551;&#21457;&#30340;&#31995;&#32479;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17901</link><description>&lt;p&gt;
&#25628;&#32034;&#19982;&#31038;&#20250;&#65306;&#37325;&#26032;&#26500;&#24819;&#28608;&#36827;&#26410;&#26469;&#30340;&#20449;&#24687;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Search and Society: Reimagining Information Access for Radical Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17901
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#24212;&#35813;&#37325;&#26032;&#23558;&#30740;&#31350;&#35758;&#31243;&#32858;&#28966;&#20110;&#31038;&#20250;&#38656;&#27714;&#65292;&#28040;&#38500;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#36879;&#26126;&#24230;&#21644;&#36947;&#24503;&#30740;&#31350;&#19982;&#20449;&#24687;&#26816;&#32034;&#20854;&#20182;&#39046;&#22495;&#20043;&#38388;&#30340;&#20154;&#20026;&#38548;&#31163;&#65292;&#31215;&#26497;&#35774;&#23450;&#30740;&#31350;&#35758;&#31243;&#65292;&#28608;&#21169;&#26500;&#24314;&#26126;&#30830;&#38472;&#36848;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#25152;&#21551;&#21457;&#30340;&#31995;&#32479;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2403.17901v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#25216;&#26415;&#21644;&#30740;&#31350;&#27491;&#22312;&#32463;&#21382;&#21464;&#38761;&#12290;&#25105;&#20204;&#35748;&#20026;&#31038;&#21306;&#24212;&#35813;&#25235;&#20303;&#36825;&#20010;&#26426;&#20250;&#65292;&#37325;&#26032;&#23558;&#30740;&#31350;&#35758;&#31243;&#32858;&#28966;&#20110;&#31038;&#20250;&#38656;&#27714;&#65292;&#21516;&#26102;&#28040;&#38500;IR&#30340;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#36879;&#26126;&#24230;&#21644;&#36947;&#24503;&#30740;&#31350;&#19982;IR&#20854;&#20182;&#39046;&#22495;&#20043;&#38388;&#30340;&#20154;&#20026;&#38548;&#31163;&#12290;&#31038;&#21306;&#19981;&#24212;&#37319;&#21462;&#35797;&#22270;&#20943;&#36731;&#26032;&#20852;&#25216;&#26415;&#21487;&#33021;&#24102;&#26469;&#31038;&#20250;&#23475;&#22788;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#65292;&#32780;&#24212;&#35813;&#31215;&#26497;&#35774;&#23450;&#30740;&#31350;&#35758;&#31243;&#65292;&#28608;&#21169;&#25105;&#20204;&#26500;&#24314;&#21508;&#31181;&#26126;&#30830;&#38472;&#36848;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#25152;&#21551;&#21457;&#30340;&#31995;&#32479;&#31867;&#22411;&#12290;&#25903;&#25745;&#20449;&#24687;&#33719;&#21462;&#25216;&#26415;&#35774;&#35745;&#21644;&#24320;&#21457;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#38656;&#35201;&#26126;&#30830;&#34920;&#36798;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#36825;&#20123;&#19981;&#21516;&#35270;&#35282;&#30340;&#32972;&#26223;&#19979;&#21457;&#23637;&#21464;&#38761;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#25351;&#23548;&#26410;&#26469;&#24819;&#35937;&#21147;&#24517;&#39035;&#21463;&#21040;&#20854;&#20182;&#23398;&#26415;&#39046;&#22495;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17901v1 Announce Type: new  Abstract: Information retrieval (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic field
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; xMIND&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#20174;&#33521;&#35821; MIND &#25968;&#25454;&#38598;&#34893;&#29983;&#32780;&#26469;&#30340;&#24320;&#25918;&#12289;&#22810;&#35821;&#35328;&#30340;&#26032;&#38395;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#21644;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#26041;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.17876</link><description>&lt;p&gt;
MIND &#20320;&#30340;&#35821;&#35328;&#65306;&#29992;&#20110;&#36328;&#35821;&#35328;&#26032;&#38395;&#25512;&#33616;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17876
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; xMIND&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#20174;&#33521;&#35821; MIND &#25968;&#25454;&#38598;&#34893;&#29983;&#32780;&#26469;&#30340;&#24320;&#25918;&#12289;&#22810;&#35821;&#35328;&#30340;&#26032;&#38395;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#21644;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#26041;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17876v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#20351;&#29992;&#26032;&#38395;&#25512;&#33616;&#20316;&#20026;&#28385;&#36275;&#35835;&#32773;&#20010;&#20154;&#20449;&#24687;&#38656;&#27714;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#32593;&#32476;&#31038;&#21306;&#35821;&#35328;&#26085;&#30410;&#22810;&#26679;&#65292;&#35768;&#22810;&#20114;&#32852;&#32593;&#29992;&#25143;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#38405;&#35835;&#26032;&#38395;&#65292;&#20294;&#22810;&#25968;&#26032;&#38395;&#25512;&#33616;&#20173;&#38598;&#20013;&#22312;&#20027;&#35201;&#30340;&#12289;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#19978;&#65292;&#23588;&#20854;&#26159;&#33521;&#35821;&#12290;&#27492;&#22806;&#65292;&#20960;&#20046;&#25152;&#26377;&#26032;&#38395;&#25512;&#33616;&#24037;&#20316;&#37117;&#20551;&#35774;&#21333;&#35821;&#35328;&#26032;&#38395;&#28040;&#36153;&#65292;&#28982;&#32780;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#20542;&#21521;&#20110;&#20197;&#33267;&#23569;&#20004;&#31181;&#35821;&#35328;&#26469;&#33719;&#21462;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#30740;&#31350;&#32570;&#20047;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#21644;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20013;&#26377;&#25928;&#24320;&#21457;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; xMIND&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#26032;&#38395;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#20174;&#33521;&#35821; MIND &#25968;&#25454;&#38598;&#34893;&#29983;&#32780;&#26469;&#65292;&#35206;&#30422;&#20102;&#19968;&#32452;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17876v1 Announce Type: new  Abstract: Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers. Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular. Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages. Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 1
&lt;/p&gt;</description></item><item><title>ArabicaQA&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;AraDPR&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#21644;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.17848</link><description>&lt;p&gt;
ArabicaQA&#65306;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#38382;&#31572;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArabicaQA: A Comprehensive Dataset for Arabic Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17848
&lt;/p&gt;
&lt;p&gt;
ArabicaQA&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;AraDPR&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#21644;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;ArabicaQA&#35299;&#20915;&#20102;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#20013;&#30340;&#24040;&#22823;&#32570;&#21475;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#21019;&#24314;&#65292;&#21253;&#25324;89,095&#20010;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#21644;3,701&#20010;&#19981;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#30475;&#36215;&#26469;&#19982;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#31867;&#20284;&#65292;&#36824;&#38468;&#24102;&#20102;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#26631;&#31614;&#65292;&#26631;&#24535;&#30528;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;AraDPR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#25991;&#26412;&#26816;&#32034;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21253;&#25324;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38463;&#25289;&#20271;&#35821;&#38382;&#31572;&#20013;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#23427;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#24635;&#20043;&#65292;ArabicaQA&#12289;AraDPR&#20197;&#21450;LLMs&#22312;&#38463;&#25289;&#20271;&#35821;&#38382;&#31572;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#31561;&#24037;&#20316;&#23558;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17848v1 Announce Type: new  Abstract: In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking of LLMs in Arabic question
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#26696;&#20363;&#38388;&#30340;&#36830;&#25509;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17780</link><description>&lt;p&gt;
CaseLink:&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#24402;&#32435;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CaseLink: Inductive Graph Learning for Legal Case Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#26696;&#20363;&#38388;&#30340;&#36830;&#25509;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26696;&#20363;&#27861;&#20013;&#65292;&#20808;&#20363;&#26159;&#29992;&#26469;&#25903;&#25345;&#27861;&#23448;&#20570;&#20986;&#20915;&#23450;&#20197;&#21450;&#24459;&#24072;&#23545;&#29305;&#23450;&#26696;&#20363;&#30340;&#35266;&#28857;&#30340;&#30456;&#20851;&#26696;&#20363;&#12290;&#20026;&#20102;&#20174;&#22823;&#37327;&#26696;&#20363;&#27744;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#30456;&#20851;&#26696;&#20363;&#65292;&#27861;&#24459;&#20174;&#19994;&#32773;&#24191;&#27867;&#20351;&#29992;&#26816;&#32034;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#27604;&#36739;&#21333;&#20010;&#26696;&#20363;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#24037;&#20316;&#12290;&#23613;&#31649;&#23427;&#20204;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#65292;&#20294;&#26696;&#20363;&#20043;&#38388;&#30340;&#22266;&#26377;&#36830;&#25509;&#20851;&#31995;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#20110;&#26696;&#20363;&#32534;&#30721;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#26696;&#20363;&#27744;&#20013;&#65292;&#26377;&#19977;&#31181;&#26696;&#20363;&#36830;&#25509;&#20851;&#31995;&#65306;&#26696;&#20363;&#24341;&#29992;&#20851;&#31995;&#12289;&#26696;&#20363;&#35821;&#20041;&#20851;&#31995;&#21644;&#26696;&#20363;&#27861;&#24459;&#25351;&#25511;&#20851;&#31995;&#12290;&#30001;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#30340;&#24402;&#32435;&#26041;&#24335;&#30340;&#29305;&#28857;&#65292;&#20351;&#29992;&#26696;&#20363;&#24341;&#29992;&#20316;&#20026;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17780v1 Announce Type: new  Abstract: In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case. This relevance is referred to as the case-to-case reference relation. To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners. Existing legal case retrieval models mainly work by comparing the text representations of individual cases. Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance. In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship. Due to the inductive manner in the task of legal case retrieval, using case reference as input 
&lt;/p&gt;</description></item><item><title>TWOLAR&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20998;&#31574;&#30053;&#21644;&#33976;&#39311;&#36807;&#31243;&#65292;&#21019;&#24314;&#20102;&#22810;&#26679;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#33021;&#21147;&#65292;&#19982;&#19977;&#20010;&#25968;&#37327;&#32423;&#26356;&#22810;&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#21305;&#37197;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.17759</link><description>&lt;p&gt;
TWOLAR&#65306;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#33976;&#39311;&#26041;&#27861;&#30340;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#30340;&#20004;&#27493;&#39588;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17759
&lt;/p&gt;
&lt;p&gt;
TWOLAR&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20998;&#31574;&#30053;&#21644;&#33976;&#39311;&#36807;&#31243;&#65292;&#21019;&#24314;&#20102;&#22810;&#26679;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#33021;&#21147;&#65292;&#19982;&#19977;&#20010;&#25968;&#37327;&#32423;&#26356;&#22810;&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#21305;&#37197;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOLAR&#65306;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30693;&#35782;&#33976;&#39311;&#30340;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#30340;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#12290;TWOLAR&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#31574;&#30053;&#21644;&#19968;&#20010;&#33976;&#39311;&#36807;&#31243;&#65292;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#19988;&#22810;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;20K&#20010;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#19982;&#36890;&#36807;&#22235;&#31181;&#19981;&#21516;&#26816;&#32034;&#26041;&#27861;&#26816;&#32034;&#21040;&#30340;&#19968;&#32452;&#25991;&#26723;&#30456;&#20851;&#32852;&#65292;&#20197;&#30830;&#20445;&#22810;&#26679;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#33021;&#21147;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#24341;&#20837;&#30340;&#27599;&#20010;&#26032;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TWOLAR&#26174;&#33879;&#22686;&#24378;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#33021;&#21147;&#65292;&#22312;TREC-DL&#27979;&#35797;&#38598;&#21644;&#38646;-shot&#35780;&#20272;&#22522;&#20934;BEIR&#19978;&#19982;&#25317;&#26377;&#19977;&#20010;&#25968;&#37327;&#32423;&#26356;&#22810;&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#21305;&#37197;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#24037;&#20316;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17759v1 Announce Type: new  Abstract: In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM). TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate future work we release our data 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;</title><link>https://arxiv.org/abs/2403.17740</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#24322;&#36136;&#20132;&#20114;&#24314;&#27169;&#29992;&#20110;&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17740
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#20363;&#22914;&#21327;&#21516;&#36807;&#28388;&#12289;&#31038;&#20132;&#25512;&#33616;&#21644;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65292;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#19981;&#21516;&#35282;&#33394;&#20043;&#38388;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#26174;&#24335;&#20851;&#31995;&#21487;&#33021;&#19981;&#21487;&#38752;&#19988;&#26080;&#20851;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29305;&#23450;&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#38480;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#12290;HIRE&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#39044;&#20808;&#23450;&#20041;&#30340;&#20132;&#20114;&#27169;&#24335;&#25110;&#25163;&#21160;&#26500;&#24314;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#65292;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
&lt;/p&gt;</description></item><item><title>EulerFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#32479;&#19968;&#20102;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.17729</link><description>&lt;p&gt;
EulerFormer&#65306;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17729
&lt;/p&gt;
&lt;p&gt;
EulerFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#32479;&#19968;&#20102;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#12290;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#26680;&#24515;&#22312;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#35745;&#31639;&#24207;&#21015;&#20013;&#30340;&#25104;&#23545;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#30001;&#20110;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;&#29305;&#24615;&#65292;&#20301;&#32622;&#32534;&#30721;&#29992;&#20110;&#22686;&#24378;&#20196;&#29260;&#34920;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25104;&#23545;&#27880;&#24847;&#21147;&#20998;&#25968;&#21487;&#20197;&#36890;&#36807;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#20004;&#32773;&#34893;&#29983;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#24120;&#20197;&#19981;&#21516;&#26041;&#24335;&#24314;&#27169;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24046;&#24322;&#27979;&#37327;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EulerFormer&#30340;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#34920;&#36848;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#12290; EulerFormer&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17729v1 Announce Type: cross  Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. Fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21327;&#21516;&#36807;&#28388;&#65288;LLM-CF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21516;&#36807;&#28388;&#20449;&#24687;</title><link>https://arxiv.org/abs/2403.17688</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Enhanced Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17688
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21327;&#21516;&#36807;&#28388;&#65288;LLM-CF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21516;&#36807;&#28388;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20852;&#36259;&#65292;&#20182;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#21033;&#29992;LLM&#29983;&#25104;&#30693;&#35782;&#20016;&#23500;&#30340;&#25991;&#26412;&#65292;&#25110;&#32773;&#21033;&#29992;LLM&#34893;&#29983;&#30340;&#23884;&#20837;&#20316;&#20026;&#29305;&#24449;&#26469;&#25913;&#36827;RS&#12290;&#34429;&#28982;LLM&#20013;&#21253;&#21547;&#30340;&#24191;&#27867;&#19990;&#30028;&#30693;&#35782;&#36890;&#24120;&#26377;&#21033;&#20110;RS&#65292;&#20294;&#35813;&#24212;&#29992;&#21482;&#33021;&#25509;&#21463;&#26377;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21327;&#21516;&#36807;&#28388;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#21327;&#21516;&#36807;&#28388;&#22312;RS&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#21033;&#29992;LLM&#22686;&#24378;RS&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#36890;&#36807;LLM&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21516;&#36807;&#28388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;LLM&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21327;&#21516;&#36807;&#28388;&#65288;LLM-CF&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#28860;&#20102;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17688v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have attracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs). Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs. Al- though the extensive world knowledge embedded in LLMs generally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collaborative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs. In this paper, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which distils the world knowledge and reasoning capabilities of LLMs
&lt;/p&gt;</description></item><item><title>S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17643</link><description>&lt;p&gt;
S+t-SNE - &#23558;&#38477;&#32500;&#24341;&#20837;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
S+t-SNE - Bringing dimensionality reduction to data streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17643
&lt;/p&gt;
&lt;p&gt;
S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;S+t-SNE&#65292;&#36825;&#26159;t-SNE&#31639;&#27861;&#30340;&#19968;&#31181;&#25913;&#36827;&#65292;&#26088;&#22312;&#22788;&#29702;&#26080;&#38480;&#25968;&#25454;&#27969;&#12290;S+t-SNE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#36880;&#27493;&#26356;&#26032;t-SNE&#23884;&#20837;&#65292;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#22788;&#29702;&#27969;&#24335;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#27599;&#19968;&#27493;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#28857;&#65292;&#35813;&#31639;&#27861;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;&#37319;&#29992;&#30450;&#30446;&#26041;&#27861;&#36827;&#34892;&#28418;&#31227;&#31649;&#29702;&#35843;&#25972;&#23884;&#20837;&#31354;&#38388;&#65292;&#20419;&#36827;&#19981;&#26029;&#21487;&#35270;&#21270;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;S+t-SNE&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;&#20854;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#25429;&#25417;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#23454;&#26102;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17643v1 Announce Type: new  Abstract: We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39034;&#24207;&#20915;&#31574;&#24314;&#27169;&#20026;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36974;&#32617;&#37197;&#32622;&#26469;&#37325;&#26032;&#35299;&#37322;RLRS&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17634</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#36974;&#32617;&#30340;&#20445;&#30041;&#20915;&#31574;&#21464;&#21387;&#22120;&#29992;&#20110;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39034;&#24207;&#20915;&#31574;&#24314;&#27169;&#20026;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36974;&#32617;&#37197;&#32622;&#26469;&#37325;&#26032;&#35299;&#37322;RLRS&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#65288;RLRS&#65289;&#22312;&#19968;&#31995;&#21015;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20174;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21040;&#27969;&#23186;&#20307;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21046;&#23450;&#22870;&#21169;&#20989;&#25968;&#21644;&#21033;&#29992;RL&#26694;&#26550;&#20013;&#30340;&#22823;&#22411;&#29616;&#26377;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#31163;&#32447;RLRS&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#26102;&#21487;&#33021;&#20250;&#24341;&#20837;&#19982;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25104;&#26412;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20027;&#27969;&#26041;&#27861;&#20351;&#29992;&#22266;&#23450;&#38271;&#24230;&#30340;&#36755;&#20837;&#36712;&#36857;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#33719;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;&#21916;&#22909;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RLRS&#26041;&#27861;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#39034;&#24207;&#20915;&#31574;&#24314;&#27169;&#20026;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36974;&#32617;&#37197;&#32622;&#26469;&#37325;&#26032;&#35299;&#37322;RLRS&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17634v1 Announce Type: cross  Abstract: Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#34892;&#20026;&#24207;&#36143;&#25512;&#33616;&#30340;&#39640;&#25928;&#21435;&#22122;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#39640;&#25928;&#34892;&#20026;&#24207;&#21015;&#25366;&#25496;&#22120;&#12289;&#35774;&#35745;&#30828;&#19982;&#36719;&#21435;&#22122;&#27169;&#22359;&#65292;&#25506;&#32034;&#34892;&#20026;&#19982;&#22122;&#38899;&#20851;&#31995;&#65292;&#20197;&#21450;&#24341;&#20837;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21644;&#24341;&#23548;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17603</link><description>&lt;p&gt;
END4Rec&#65306;&#38754;&#21521;&#22810;&#34892;&#20026;&#24207;&#36143;&#25512;&#33616;&#30340;&#39640;&#25928;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#34892;&#20026;&#24207;&#36143;&#25512;&#33616;&#30340;&#39640;&#25928;&#21435;&#22122;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#39640;&#25928;&#34892;&#20026;&#24207;&#21015;&#25366;&#25496;&#22120;&#12289;&#35774;&#35745;&#30828;&#19982;&#36719;&#21435;&#22122;&#27169;&#22359;&#65292;&#25506;&#32034;&#34892;&#20026;&#19982;&#22122;&#38899;&#20851;&#31995;&#65292;&#20197;&#21450;&#24341;&#20837;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21644;&#24341;&#23548;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#32463;&#24120;&#21442;&#19982;&#22810;&#31181;&#31867;&#22411;&#30340;&#34892;&#20026;&#65292;&#27604;&#22914;&#28857;&#20987;&#12289;&#21152;&#20837;&#36141;&#29289;&#36710;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#34892;&#20026;&#25968;&#25454;&#65292;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#22312;&#30701;&#26399;&#20869;&#20250;&#21464;&#24471;&#38750;&#24120;&#38271;&#65292;&#36825;&#32473;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#30340;&#25928;&#29575;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19968;&#20123;&#34892;&#20026;&#25968;&#25454;&#20063;&#20250;&#32473;&#29992;&#25143;&#20852;&#36259;&#24314;&#27169;&#24102;&#26469;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#34892;&#20026;&#24207;&#21015;&#25366;&#25496;&#22120;&#65288;EBM&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#38899;&#35774;&#35745;&#20102;&#30828;&#24615;&#21644;&#36719;&#24615;&#21435;&#22122;&#27169;&#22359;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#34892;&#20026;&#21644;&#22122;&#38899;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#24341;&#23548;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#27604;&#36739;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#19982;&#22122;&#38899;&#20449;&#21495;&#65292;&#20174;&#32780;&#26080;&#32541;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17603v1 Announce Type: new  Abstract: In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38590;&#20197;&#20272;&#35745;&#30340;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26032;&#39062;PU&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26723;&#38598;&#25193;&#23637;&#20219;&#21153;&#20013;&#27491;-&#26410;&#26631;&#35760;&#23398;&#20064;&#38754;&#20020;&#30340;&#31867;&#20808;&#39564;&#20551;&#35774;&#19981;&#20999;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17473</link><description>&lt;p&gt;
&#20351;&#29992;&#38590;&#20197;&#20272;&#35745;&#30340;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#27491;-&#26410;&#26631;&#35760;&#23398;&#20064;&#36827;&#34892;&#25991;&#26723;&#38598;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Document Set Expansion with Positive-Unlabelled Learning Using Intractable Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38590;&#20197;&#20272;&#35745;&#30340;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26032;&#39062;PU&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26723;&#38598;&#25193;&#23637;&#20219;&#21153;&#20013;&#27491;-&#26410;&#26631;&#35760;&#23398;&#20064;&#38754;&#20020;&#30340;&#31867;&#20808;&#39564;&#20551;&#35774;&#19981;&#20999;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#38598;&#25193;&#23637;&#65288;DSE&#65289;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#19968;&#32452;&#26377;&#38480;&#30340;&#31034;&#20363;&#25991;&#26723;&#20174;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#35782;&#21035;&#30456;&#20851;&#25991;&#26723;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#27491;&#21521;&#21644;&#26410;&#26631;&#35760;&#65288;PU&#65289;&#23398;&#20064;&#20316;&#20026;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;PU&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#38598;&#21512;&#20013;&#27491;&#26679;&#26412;&#30340;&#31867;&#20808;&#39564;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#38590;&#20197;&#20272;&#35745;&#30340;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26032;&#39062;PU&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;PubMed&#21644;Covid&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#36716;&#23548;&#35774;&#32622;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;DSE&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#20174; https://github.com/Beautifuldog01/Document-set-expansion-puDE &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17473v1 Announce Type: new  Abstract: The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents. Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task. However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection. To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models. Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE. Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE.
&lt;/p&gt;</description></item><item><title>&#22312;&#28151;&#21512;&#30446;&#26631;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#31163;&#25955;&#36716;&#21270;&#34892;&#20026;&#19982;&#36830;&#32493;&#36716;&#21270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#26680;&#24515;&#22238;&#24402;&#20219;&#21153;&#23545;&#20854;&#20182;&#20219;&#21153;&#24433;&#21709;&#36739;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17442</link><description>&lt;p&gt;
&#35302;&#21450;&#26680;&#24515;&#65306;&#25506;&#32034;&#28151;&#21512;&#30446;&#26631;&#25512;&#33616;&#20013;&#20219;&#21153;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17442
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#30446;&#26631;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#31163;&#25955;&#36716;&#21270;&#34892;&#20026;&#19982;&#36830;&#32493;&#36716;&#21270;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#26680;&#24515;&#22238;&#24402;&#20219;&#21153;&#23545;&#20854;&#20182;&#20219;&#21153;&#24433;&#21709;&#36739;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29992;&#25143;&#34892;&#20026;&#22312;&#21830;&#19994;&#24179;&#21488;&#19978;&#21464;&#24471;&#22797;&#26434;&#65292;&#22312;&#32447;&#25512;&#33616;&#26356;&#21152;&#20851;&#27880;&#22914;&#20309;&#35302;&#21450;&#26680;&#24515;&#36716;&#21270;&#65292;&#36825;&#20123;&#36716;&#21270;&#19982;&#24179;&#21488;&#30340;&#20852;&#36259;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#20123;&#26680;&#24515;&#36716;&#21270;&#36890;&#24120;&#26159;&#36830;&#32493;&#30340;&#30446;&#26631;&#65292;&#22914;&#8220;&#35266;&#30475;&#26102;&#38388;&#8221;&#12289;&#8220;&#25910;&#20837;&#8221;&#31561;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#21487;&#20197;&#36890;&#36807;&#20043;&#21069;&#30340;&#31163;&#25955;&#36716;&#21270;&#34892;&#20026;&#26469;&#22686;&#24378;&#12290;&#22240;&#27492;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21487;&#20197;&#34987;&#37319;&#29992;&#20316;&#20026;&#23398;&#20064;&#36825;&#20123;&#28151;&#21512;&#30446;&#26631;&#30340;&#33539; paradigm&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#24378;&#35843;&#30740;&#31350;&#31163;&#25955;&#36716;&#21270;&#34892;&#20026;&#20043;&#38388;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#24573;&#35270;&#20102;&#31163;&#25955;&#36716;&#21270;&#19982;&#26368;&#32456;&#36830;&#32493;&#36716;&#21270;&#20043;&#38388;&#30340;&#20381;&#36182;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#20248;&#21270;&#20855;&#26377;&#26356;&#24378;&#20219;&#21153;&#20381;&#36182;&#24615;&#30340;&#28151;&#21512;&#20219;&#21153;&#23558;&#38754;&#20020;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#26680;&#24515;&#22238;&#24402;&#20219;&#21153;&#21487;&#33021;&#23545;&#20854;&#20182;&#20219;&#21153;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#28151;&#21512;&#30446;&#26631;&#30340;MTL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17442v1 Announce Type: new  Abstract: As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as \textit{watch time}, \textit{revenue}, and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Masked Multi-Domain Network&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;&#36716;&#21270;&#29575;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20415;&#21033;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17425</link><description>&lt;p&gt;
Masked Multi-Domain Network: &#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#22810;&#22495;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Masked Multi-Domain Network&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;&#36716;&#21270;&#29575;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20415;&#21033;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24191;&#21578;&#31995;&#32479;&#20013;&#65292;&#36716;&#21270;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#22411;&#65292;&#24191;&#21578;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#23637;&#31034;&#22330;&#26223;&#20013;&#23637;&#31034;&#65292;&#36825;&#20004;&#32773;&#37117;&#26497;&#22823;&#22320;&#24433;&#21709;&#23454;&#38469;&#30340;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;CVR&#39044;&#27979;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29702;&#24819;&#27169;&#22411;&#24212;&#28385;&#36275;&#20197;&#19979;&#35201;&#27714;&#65306;1&#65289;&#20934;&#30830;&#24615;&#65306;&#27169;&#22411;&#24212;&#38024;&#23545;&#20219;&#20309;&#36716;&#21270;&#31867;&#22411;&#22312;&#20219;&#20309;&#23637;&#31034;&#22330;&#26223;&#19978;&#23454;&#29616;&#31934;&#32454;&#30340;&#20934;&#30830;&#24615;&#12290;2&#65289;&#21487;&#25193;&#23637;&#24615;&#65306;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#24212;&#35813;&#26159;&#21487;&#25215;&#21463;&#30340;&#12290;3&#65289;&#20415;&#21033;&#24615;&#65306;&#27169;&#22411;&#19981;&#24212;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#21306;&#12289;&#23376;&#38598;&#22788;&#29702;&#21644;&#21333;&#29420;&#23384;&#20648;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#20026;&#27599;&#20010;&#65288;&#36716;&#21270;&#31867;&#22411;&#65292;&#23637;&#31034;&#22330;&#26223;&#65289;&#23545;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26082;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;&#20063;&#19981;&#20415;&#20110;&#25805;&#20316;&#12290;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#65292;&#21253;&#25324;&#36716;&#21270;&#31867;&#22411;&#21644;&#23637;&#31034;&#22330;&#26223;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17425v1 Announce Type: cross  Abstract: In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario incl
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17421</link><description>&lt;p&gt;
MA4DIV&#65306;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17421
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#65288;SRD&#65289;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#25152;&#36873;&#25991;&#26723;&#28085;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#21516;&#23376;&#20027;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#8220;&#36138;&#23146;&#36873;&#25321;&#8221;&#33539;&#24335;&#65292;&#21363;&#19968;&#27425;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#39640;&#22810;&#26679;&#24615;&#20998;&#25968;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#25928;&#29575;&#20302;&#19979;&#65292;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#26088;&#22312;&#36817;&#20284;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20294;&#32467;&#26524;&#20173;&#28982;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;MA4DIV&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#25991;&#26723;&#37117;&#26159;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#34987;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#22312;&#38477;&#20302;GNN&#34920;&#31034;&#21644;&#25512;&#33616;&#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#33268;&#21147;&#20110;&#35299;&#20915;&#22914;&#20309;&#20943;&#36731;&#36807;&#24230;&#20851;&#32852;&#30340;&#24433;&#21709;&#21516;&#26102;&#20445;&#30041;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#36825;&#19968;&#37325;&#22823;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17416</link><description>&lt;p&gt;
AFDGCF&#65306;&#33258;&#36866;&#24212;&#29305;&#24449;&#21435;&#30456;&#20851;&#22270;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#22312;&#38477;&#20302;GNN&#34920;&#31034;&#21644;&#25512;&#33616;&#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#33268;&#21147;&#20110;&#35299;&#20915;&#22914;&#20309;&#20943;&#36731;&#36807;&#24230;&#20851;&#32852;&#30340;&#24433;&#21709;&#21516;&#26102;&#20445;&#30041;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#36825;&#19968;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#20102;&#23427;&#20204;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#25429;&#33719;&#22797;&#26434;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#20013;&#30340;&#21327;&#21516;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;GNN&#30340;RS&#19981;&#32463;&#24847;&#22320;&#24341;&#20837;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#20043;&#38388;&#36807;&#22810;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#36829;&#21453;&#20102;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23558;&#36825;&#19968;&#32570;&#38519;&#24402;&#22240;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#22312;&#38477;&#20302;GNN&#34920;&#31034;&#21644;&#38543;&#21518;&#25512;&#33616;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#19988;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20316;&#29992;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;RS&#20013;&#23545;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#23578;&#26410;&#36827;&#34892;&#25506;&#35752;&#12290;&#21516;&#26102;&#65292;&#22914;&#20309;&#20943;&#36731;&#36807;&#24230;&#20851;&#32852;&#30340;&#24433;&#21709;&#21516;&#26102;&#20445;&#30041;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17416v1 Announce Type: new  Abstract: Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms. However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations. While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DRIP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#21644;&#39033;&#30446;&#20004;&#20010;&#23618;&#38754;&#23545;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#35299;&#20915;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#20013;&#29992;&#25143;&#20559;&#22909;&#21644;&#39046;&#22495;&#26144;&#23556;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17374</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#25512;&#33616;&#36890;&#36807;&#39046;&#22495;&#20559;&#22909;&#24314;&#27169;&#21560;&#24341;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Recommendation to Attract Users via Domain Preference Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DRIP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#21644;&#39033;&#30446;&#20004;&#20010;&#23618;&#38754;&#23545;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#35299;&#20915;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#20013;&#29992;&#25143;&#20559;&#22909;&#21644;&#39046;&#22495;&#26144;&#23556;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32593;&#32476;&#24179;&#21488;&#21516;&#26102;&#36816;&#33829;&#21508;&#31181;&#26381;&#21153;&#39046;&#22495;&#12290; &#38024;&#23545;&#21516;&#26102;&#36816;&#33829;&#22810;&#20010;&#26381;&#21153;&#39046;&#22495;&#30340;&#24179;&#21488;&#65292;&#25105;&#20204;&#24341;&#20837;&#26032;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#22312;&#8220;&#24050;&#35265;&#8221;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20174;&#22810;&#20010;&#8220;&#26410;&#35265;&#8221;&#39046;&#22495;&#25512;&#33616;&#39033;&#30446;&#65292;&#20197;&#21560;&#24341;&#29992;&#25143;&#30340;&#22810;&#39046;&#22495;&#25512;&#33616;&#65288;MDRAU&#65289;&#12290; &#26412;&#25991;&#25351;&#20986;&#20102;MDRAU&#20219;&#21153;&#30340;&#20004;&#20010;&#25361;&#25112;&#12290; &#39318;&#20808;&#65292;&#30001;&#20110;&#29992;&#25143;&#36890;&#24120;&#19982;&#19981;&#21516;&#23376;&#38598;&#30340;&#26381;&#21153;&#39046;&#22495;&#20114;&#21160;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22823;&#37327;&#21487;&#33021;&#30340;&#20174;&#24050;&#35265;&#21040;&#26410;&#35265;&#39046;&#22495;&#30340;&#26144;&#23556;&#32452;&#21512;&#12290; &#20854;&#27425;&#65292;&#29992;&#25143;&#21487;&#33021;&#23545;&#27599;&#20010;&#30446;&#26631;&#26410;&#35265;&#39046;&#22495;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#36825;&#35201;&#27714;&#25512;&#33616;&#26082;&#21453;&#26144;&#29992;&#25143;&#23545;&#39046;&#22495;&#30340;&#20559;&#22909;&#65292;&#20063;&#21453;&#26144;&#23545;&#39033;&#30446;&#30340;&#20559;&#22909;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRIP&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#32423;&#21035;&#65288;&#21363;&#39046;&#22495;&#21644;&#39033;&#30446;&#65289;&#23545;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23398;&#20064;&#21508;&#31181;&#24050;&#35265;-&#26410;&#35265;&#39046;&#22495;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17374v1 Announce Type: new  Abstract: Recently, web platforms have been operating various service domains simultaneously. Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU), which recommends items from multiple ``unseen'' domains with which each user has not interacted yet, by using knowledge from the user's ``seen'' domains. In this paper, we point out two challenges of MDRAU task. First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains. Second, a user might have different preferences for each of the target unseen domains, which requires that recommendations reflect the user's preferences on domains as well as items. To tackle these challenges, we propose DRIP framework that models users' preferences at two levels (i.e., domain and item) and learns various seen-unseen domain m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.17372</link><description>&lt;p&gt;
&#35757;&#32451;&#29420;&#31435;&#20110;ID&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#22120;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#26410;&#26469;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#35768;&#22810;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#38598;&#20013;&#22312;&#29992;&#25143;ID&#21644;&#29289;&#21697;ID&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#24863;&#30693;&#19990;&#30028;&#30340;&#26041;&#24335;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22914;&#20309;&#26500;&#24314;&#19981;&#20351;&#29992;ID&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#65288;MMSR&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#23558;&#31934;&#21326;&#25552;&#28860;&#25104;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#12290;&#27839;&#30528;&#36825;&#20123;&#32500;&#24230;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#27169;&#22411;&#35774;&#35745;&#65292;&#24182;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17372v1 Announce Type: new  Abstract: Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the foll
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#22312;&#19982;&#31639;&#27861;&#20559;&#35823;&#30340;&#25628;&#32034;&#32467;&#26524;&#20114;&#21160;&#26102;&#65292;&#21463;&#35748;&#30693;&#20559;&#35823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#20105;&#35758;&#35805;&#39064;&#19978;&#30340;&#25972;&#20010;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#31526;&#21512;&#20854;&#29616;&#26377;&#20449;&#24565;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30830;&#35748;&#20559;&#35823;&#21644;&#32467;&#26524;&#21576;&#29616;&#23545;&#25628;&#32034;&#34892;&#20026;&#21644;&#32467;&#26524;&#29087;&#24713;&#24230;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.17286</link><description>&lt;p&gt;
&#19982;&#31639;&#27861;&#20559;&#35823;&#32467;&#26524;&#20114;&#21160;&#30340;&#35748;&#30693;&#20559;&#35823;&#29992;&#25143;&#22312;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#25972;&#20010;&#25628;&#32034;&#20013;
&lt;/p&gt;
&lt;p&gt;
Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17286
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#19982;&#31639;&#27861;&#20559;&#35823;&#30340;&#25628;&#32034;&#32467;&#26524;&#20114;&#21160;&#26102;&#65292;&#21463;&#35748;&#30693;&#20559;&#35823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#20105;&#35758;&#35805;&#39064;&#19978;&#30340;&#25972;&#20010;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#31526;&#21512;&#20854;&#29616;&#26377;&#20449;&#24565;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30830;&#35748;&#20559;&#35823;&#21644;&#32467;&#26524;&#21576;&#29616;&#23545;&#25628;&#32034;&#34892;&#20026;&#21644;&#32467;&#26524;&#29087;&#24713;&#24230;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#25143;&#19982;&#20449;&#24687;&#26816;&#32034;(IR)&#31995;&#32479;&#20132;&#20114;&#26102;&#65292;&#21463;&#21040;&#30830;&#35748;&#20559;&#35265;&#24433;&#21709;&#65292;&#24448;&#24448;&#36873;&#25321;&#30830;&#35748;&#20182;&#20204;&#22312;&#31038;&#20250;&#37325;&#35201;&#20105;&#35758;&#38382;&#39064;&#19978;&#30340;&#29616;&#26377;&#20449;&#24565;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#20026;&#20102;&#29702;&#35299;&#22312;&#32447;&#25628;&#32034;&#29992;&#25143;&#30340;&#21028;&#26029;&#21644;&#24577;&#24230;&#21464;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#23519;&#20102;&#35748;&#30693;&#20559;&#35823;&#29992;&#25143;&#22914;&#20309;&#19982;&#31639;&#27861;&#24615;&#20559;&#35823;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;(SERPs)&#20114;&#21160;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#19981;&#21516;&#20559;&#24046;&#26465;&#20214;&#19979;&#30340;&#20105;&#35758;&#24615;&#35805;&#39064;&#19978;&#30340;&#19977;&#27425;&#26597;&#35810;&#25628;&#32034;&#20250;&#35805;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;1,321&#21517;&#20247;&#21253;&#21442;&#19982;&#32773;&#65292;&#25506;&#35752;&#20102;&#20182;&#20204;&#30340;&#24577;&#24230;&#21464;&#21270;&#12289;&#25628;&#32034;&#20114;&#21160;&#20197;&#21450;&#30830;&#35748;&#20559;&#35823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;1) &#22823;&#22810;&#25968;&#24577;&#24230;&#21464;&#21270;&#21457;&#29983;&#22312;&#25628;&#32034;&#20250;&#35805;&#30340;&#21021;&#22987;&#26597;&#35810;&#20013;&#65307;2) &#30830;&#35748;&#20559;&#35823;&#21644;SERPs&#19978;&#30340;&#32467;&#26524;&#21576;&#29616;&#24433;&#21709;&#24403;&#21069;&#26597;&#35810;&#20013;&#30340;&#25628;&#32034;&#34892;&#20026;&#20197;&#21450;&#20043;&#21518;&#26597;&#35810;&#20013;&#28857;&#20987;&#32467;&#26524;&#30340;&#24863;&#30693;&#29087;&#24713;&#24230;&#12290;&#20559;&#35823;&#31435;&#22330;&#20063;&#24433;&#21709;&#24577;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17286v1 Announce Type: new  Abstract: When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) confirmation bias and result presentation on SERPs affect search behaviors in the current query and perceived familiarity with clicked results in subsequent queries. The bias position also affect attitude change
&lt;/p&gt;</description></item><item><title>EXPLORA&#26159;&#19968;&#20010;&#25945;&#24072;-&#23398;&#24466;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#20808;&#35266;&#23519;&#35775;&#35848;&#25910;&#38598;&#25968;&#25454;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20799;&#31461;&#29305;&#24449;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#31526;&#21512;&#20799;&#31461;&#38656;&#27714;&#30340;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.17264</link><description>&lt;p&gt;
EXPLORA&#65306;&#29992;&#20110;&#24341;&#21457;&#33258;&#28982;&#20799;&#31461;-&#35745;&#31639;&#26426;&#20132;&#20114;&#30340;&#25945;&#24072;-&#23398;&#24466;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17264
&lt;/p&gt;
&lt;p&gt;
EXPLORA&#26159;&#19968;&#20010;&#25945;&#24072;-&#23398;&#24466;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#20808;&#35266;&#23519;&#35775;&#35848;&#25910;&#38598;&#25968;&#25454;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20799;&#31461;&#29305;&#24449;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#31526;&#21512;&#20799;&#31461;&#38656;&#27714;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20799;&#31461;-&#35745;&#31639;&#26426;&#20132;&#20114;&#22312;&#20854;&#32972;&#26223;&#20013;&#30340;&#24773;&#20917;&#23545;&#20110;&#35774;&#35745;&#28385;&#36275;&#20799;&#31461;&#38656;&#27714;&#30340;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#35774;&#35745;&#20799;&#31461;&#20026;&#20013;&#24515;&#30340;&#25216;&#26415;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;EXPLORA&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#38454;&#27573;&#30340;&#22312;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;&#65288;1&#65289;&#24314;&#31435;&#25945;&#24072;-&#23398;&#24466;&#20851;&#31995;&#65292;&#65288;2&#65289;&#21521;&#20799;&#31461;&#25945;&#24072;&#23398;&#20064;&#65292;&#20197;&#21450;&#65288;3&#65289;&#35780;&#20272;&#21644;&#21152;&#24378;&#30740;&#31350;&#32773;-&#23398;&#24466;&#23398;&#20064;&#12290;EXPLORA&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#35266;&#21069;&#35775;&#35848;&#25910;&#38598;&#24577;&#24230;&#25968;&#25454;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20799;&#31461;&#30340;&#29305;&#24449;&#21644;&#32972;&#26223;&#12290;&#36825;&#20026;&#38543;&#21518;&#30340;&#22312;&#32447;&#35266;&#23519;&#25552;&#20379;&#20102;&#20449;&#24687;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20851;&#27880;&#39057;&#32321;&#30340;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#19982;&#20799;&#31461;&#39564;&#35777;&#21021;&#27493;&#20551;&#35774;&#12290;&#19968;&#31181;&#25163;&#27573;-&#30446;&#30340;&#20998;&#26512;&#26694;&#26550;&#26377;&#21161;&#20110;&#23545;&#25968;&#25454;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#38416;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17264v1 Announce Type: cross  Abstract: Investigating child-computer interactions within their contexts is vital for designing technology that caters to children's needs. However, determining what aspects of context are relevant for designing child-centric technology remains a challenge. We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning. Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children's characteristics and contexts. This informs subsequent online observations, allowing researchers to focus on frequent interactions. Furthermore, researchers can validate preliminary assumptions with children. A means-ends analysis framework aids in the systematic analysis of data, shedding light on cont
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.17210</link><description>&lt;p&gt;
CADGL: &#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#30740;&#31350;&#26159;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#12290;DDIs&#21457;&#29983;&#22312;&#19968;&#20010;&#33647;&#29289;&#30340;&#24615;&#36136;&#21463;&#20854;&#20182;&#33647;&#29289;&#21253;&#21547;&#30340;&#24433;&#21709;&#26102;&#12290;&#26816;&#27979;&#26377;&#21033;&#30340;DDIs&#26377;&#21487;&#33021;&#20026;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#21019;&#26032;&#33647;&#29289;&#30340;&#21019;&#36896;&#21644;&#25512;&#36827;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#12289;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#29616;&#23454;&#24212;&#29992;&#21487;&#33021;&#24615;&#26041;&#38754;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;CADGL&#30340;&#26032;&#39062;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22522;&#20110;&#23450;&#21046;&#30340;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19978;&#19979;&#25991;&#39044;&#22788;&#29702;&#22120;&#20174;&#20004;&#20010;&#19981;&#21516;&#35270;&#35282;&#65306;&#23616;&#37096;&#37051;&#22495;&#21644;&#20998;&#23376;&#19978;&#19979;&#25991;&#65292;&#22312;&#24322;&#36136;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25429;&#33719;&#20851;&#38190;&#30340;&#32467;&#26500;&#21644;&#29983;&#29702;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.17089</link><description>&lt;p&gt;
GOLF&#65306;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#20449;&#24687;&#33719;&#21462;&#36807;&#31243;&#12290;&#21033;&#29992;LLMs&#20316;&#20026;&#25628;&#32034;&#24341;&#25806;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#26681;&#25454;&#20854;&#26597;&#35810;&#23450;&#21046;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22312;&#23548;&#33322;&#22823;&#37327;&#20449;&#24687;&#36164;&#28304;&#26102;&#25152;&#24102;&#26469;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#31181;&#36716;&#21464;&#20984;&#26174;&#20102;LLMs&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#24687;&#33719;&#21462;&#33539;&#24335;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#20219;&#21153;&#28966;&#28857;&#20449;&#24687;&#26816;&#32034;&#21644;LLMs&#30340;&#20219;&#21153;&#35268;&#21010;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#33021;&#21147;&#33539;&#22260;&#25193;&#23637;&#21040;&#25903;&#25345;&#29992;&#25143;&#23548;&#33322;&#38271;&#26399;&#21644;&#37325;&#35201;&#30340;&#29983;&#27963;&#20219;&#21153;&#12290;&#23427;&#24341;&#20837;&#20102;GOLF&#26694;&#26550;&#65288;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65289;&#65292;&#20391;&#37325;&#20110;&#22686;&#24378;LLMs&#36890;&#36807;&#30446;&#26631;&#23450;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#26469;&#21327;&#21161;&#29992;&#25143;&#20570;&#20986;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#35770;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#27604;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17089v1 Announce Type: cross  Abstract: The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.09963</link><description>&lt;p&gt;
&#22788;&#29702;&#22909;&#24744;&#30340;&#25552;&#31034;&#20559;&#35265;&#65281;&#35843;&#26597;&#21644;&#20943;&#36731;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#25552;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#21363;&#25552;&#31034;&#24448;&#24448;&#20250;&#24341;&#20837;&#23545;&#29305;&#23450;&#26631;&#31614;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20869;&#37096;&#25552;&#31034;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#23454;&#39564;&#20013;&#30340;&#25152;&#26377;&#25552;&#31034;&#37117;&#34920;&#29616;&#20986;&#19981;&#21487;&#24573;&#35270;&#30340;&#20559;&#35265;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25552;&#31034;&#22914;AutoPrompt&#21644;OptiPrompt&#26174;&#31034;&#20986;&#26356;&#39640;&#27700;&#24179;&#30340;&#20559;&#35265;&#65307;2&#65289;&#25552;&#31034;&#20559;&#35265;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#27979;&#35797;&#25968;&#25454;&#38598;&#19981;&#21512;&#29702;&#22320;&#25918;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;LAMA&#36825;&#26679;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25552;&#31034;&#20559;&#35265;&#65292;&#22312;&#25512;&#26029;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20165;&#25552;&#31034;&#26597;&#35810;&#26469;&#20272;&#35745;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#20013;&#21024;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21019;&#26032;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#23548;&#33268;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#20002;&#22833;&#21644;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08744</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Signal Diffusion Model for Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21019;&#26032;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#23548;&#33268;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#20002;&#22833;&#21644;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#33539;&#24335;&#26159;&#22522;&#20110;&#21382;&#21490;&#35266;&#23519;&#37325;&#24314;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#26368;&#36817;&#21457;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#32570;&#20047;&#23545;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#24314;&#27169;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#30340;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#26410;&#33021;&#32771;&#34385;&#29289;&#21697;&#20043;&#38388;&#30340;&#24322;&#36136;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#19982;&#20132;&#20114;&#31354;&#38388;&#30340;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38543;&#26426;&#22122;&#22768;&#30772;&#22351;&#20102;&#20132;&#20114;&#21521;&#37327;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#23548;&#33268;&#21453;&#21521;&#37325;&#24314;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#25913;&#36827;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65288;&#31216;&#20026;GiffCF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08744v2 Announce Type: replace-cross  Abstract: Collaborative filtering is a critical technique in recommender systems. Among various methods, an increasingly popular paradigm is to reconstruct user-item interactions based on the historical observations. This can be viewed as a conditional generative task, where recently developed diffusion model demonstrates great potential. However, existing studies on diffusion models lack effective solutions for modeling implicit feedback data. Particularly, the isotropic nature of the standard diffusion process fails to account for the heterogeneous dependencies among items, leading to a misalignment with the graphical structure of the interaction space. Meanwhile, random noise destroying personalized information in interaction vectors, causing difficulty in reverse reconstruction. In this paper, we make novel adaptions of diffusion model and propose Graph Signal Diffusion Model for Collaborative Filtering (named GiffCF). To better repr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#20013;&#29305;&#24449;&#38598;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2301.10909</link><description>&lt;p&gt;
&#20248;&#21270;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#29305;&#24449;&#38598;
&lt;/p&gt;
&lt;p&gt;
Optimizing Feature Set for Click-Through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#20013;&#29305;&#24449;&#38598;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20987;&#29575;&#39044;&#27979;&#65288;CTR&#65289;&#27169;&#22411;&#23558;&#29305;&#24449;&#36716;&#25442;&#20026;&#28508;&#22312;&#21521;&#37327;&#65292;&#24182;&#21015;&#20030;&#21487;&#33021;&#30340;&#29305;&#24449;&#20132;&#20114;&#20197;&#25913;&#21892;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#38598;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#38598;&#26102;&#65292;&#25105;&#20204;&#24212;&#32771;&#34385;&#29305;&#24449;&#21450;&#20854;&#20132;&#20114;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#20316;&#21697;&#20391;&#37325;&#20110;&#29305;&#24449;&#23383;&#27573;&#36873;&#25321;&#65292;&#25110;&#20165;&#26681;&#25454;&#22266;&#23450;&#29305;&#24449;&#38598;&#36873;&#25321;&#29305;&#24449;&#20132;&#20114;&#20197;&#29983;&#25104;&#29305;&#24449;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10909v2 Announce Type: replace  Abstract: Click-through prediction (CTR) models transform features into latent vectors and enumerate possible feature interactions to improve performance based on the input feature set. Therefore, when selecting an optimal feature set, we should consider the influence of both feature and its interaction. However, most previous works focus on either feature field selection or only select feature interaction based on the fixed feature set to produce the feature set. The former restricts search space to the feature field, which is too coarse to determine subtle features. They also do not filter useless feature interactions, leading to higher computation costs and degraded model performance. The latter identifies useful feature interaction from all available features, resulting in many redundant features in the feature set. In this paper, we propose a novel method named OptFS to address these problems. To unify the selection of feature and its int
&lt;/p&gt;</description></item><item><title>&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;</title><link>https://arxiv.org/abs/2207.01262</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21644;Leaderboarding&#29702;&#35299;&#38271;&#25991;&#26723;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01262
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;20&#22810;&#20010;&#29992;&#20110;&#38271;&#25991;&#26723;&#25490;&#21517;&#30340;Transformer&#27169;&#22411;&#65288;&#21253;&#25324;&#26368;&#36817;&#20351;&#29992;FlashAttention&#35757;&#32451;&#30340;LongP&#27169;&#22411;&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#31616;&#21333;&#30340;FirstP&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65288;&#23558;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#20110;&#36755;&#20837;&#25130;&#26029;&#20026;&#21069;512&#20010;&#26631;&#35760;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;MS MARCO&#25991;&#26723;v1&#20316;&#20026;&#20027;&#35201;&#35757;&#32451;&#38598;&#65292;&#24182;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#23545;&#20854;&#20182;&#25910;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.01262v2 Announce Type: replace-cross  Abstract: We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with simple FirstP baselines (applying the same model to input truncated to the first 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated models in the zero-shot scenario as well as after fine-tuning on other collections.   In our initial experiments with standard collections we found that long-document models underperformed FirstP or outperformed it by at most 5% on average in terms of MRR or NDCG. We then conjectured that this was not due to models inability to process long context but rather due to a positional bias of relevant passages, which tended to be among the first 512 document tokens. We found evidence that this bias was, indeed, present in at least two test sets, which motivated us to create a new collection MS MARCO FarRelevant where the relev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#26102;&#38388;&#22686;&#24378;&#21644;&#30693;&#35782;&#22686;&#24378;&#24494;&#35843;&#26469;&#25913;&#36827;&#39034;&#24207;&#25512;&#33616;&#65292;&#21512;&#25104;&#30495;&#23454;&#30340;&#20266;&#20808;&#21069;&#39033;&#30446;&#65292;&#20445;&#30041;&#29992;&#25143;&#20559;&#22909;&#24182;&#25429;&#25417;&#26356;&#28145;&#23618;&#27425;&#30340;&#39033;&#30446;&#35821;&#20041;&#30456;&#20851;&#24615;</title><link>https://arxiv.org/abs/2112.06460</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#21452;&#21521;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;&#25913;&#36827;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Improving Sequential Recommendations via Bidirectional Temporal Data Augmentation with Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.06460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#26102;&#38388;&#22686;&#24378;&#21644;&#30693;&#35782;&#22686;&#24378;&#24494;&#35843;&#26469;&#25913;&#36827;&#39034;&#24207;&#25512;&#33616;&#65292;&#21512;&#25104;&#30495;&#23454;&#30340;&#20266;&#20808;&#21069;&#39033;&#30446;&#65292;&#20445;&#30041;&#29992;&#25143;&#20559;&#22909;&#24182;&#25429;&#25417;&#26356;&#28145;&#23618;&#27425;&#30340;&#39033;&#30446;&#35821;&#20041;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#23545;&#35782;&#21035;&#29992;&#25143;&#30340;&#26102;&#38388;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#31616;&#21270;&#30340;&#29992;&#25143;&#20132;&#20114;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#12290;&#25968;&#25454;&#22686;&#24378;&#34987;&#30830;&#23450;&#20026;&#22686;&#24378;&#36825;&#20123;&#24207;&#21015;&#20449;&#24687;&#20016;&#23500;&#24615;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#22914;&#39033;&#30446;&#38543;&#26426;&#21270;&#65292;&#21487;&#33021;&#30772;&#22351;&#22266;&#26377;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#36870;&#26102;&#38388;&#20266;&#39033;&#30446;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#33258;&#28982;&#26102;&#38388;&#39034;&#24207;&#19978;&#35780;&#20272;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#24341;&#20837;&#26102;&#38388;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;Bidirectional temporal data Augmentation with pre-training&#65288;BARec&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21452;&#21521;&#26102;&#38388;&#22686;&#24378;&#21644;&#30693;&#35782;&#22686;&#24378;&#24494;&#35843;&#26469;&#21512;&#25104;&#30495;&#23454;&#30340;&#20266;&#20808;&#21069;&#39033;&#30446;&#65292;&#20174;&#32780;&#20445;&#30041;&#29992;&#25143;&#20559;&#22909;&#24182;&#25429;&#25417;&#26356;&#28145;&#23618;&#27425;&#30340;&#39033;&#30446;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.06460v5 Announce Type: replace  Abstract: Sequential recommendation systems are integral to discerning temporal user preferences. Yet, the task of learning from abbreviated user interaction sequences poses a notable challenge. Data augmentation has been identified as a potent strategy to enhance the informational richness of these sequences. Traditional augmentation techniques, such as item randomization, may disrupt the inherent temporal dynamics. Although recent advancements in reverse chronological pseudo-item generation have shown promise, they can introduce temporal discrepancies when assessed in a natural chronological context. In response, we introduce a sophisticated approach, Bidirectional temporal data Augmentation with pre-training (BARec). Our approach leverages bidirectional temporal augmentation and knowledge-enhanced fine-tuning to synthesize authentic pseudo-prior items that \emph{retain user preferences and capture deeper item semantic correlations}, thus bo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#29305;&#23450;&#24615;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#24615;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.17078</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21453;&#39304;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20449;&#24687;&#26816;&#32034;&#19978;&#19979;&#25991;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback. (arXiv:2309.17078v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#29305;&#23450;&#24615;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#24615;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;(IR)&#26159;&#23547;&#25214;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#30340;&#36807;&#31243;&#65292;&#22312;&#29616;&#20195;&#20154;&#30340;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26480;&#20986;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;IR&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#32463;&#24120;&#38754;&#20020;&#29983;&#25104;&#32570;&#20047;&#29305;&#23450;&#24615;&#22238;&#22797;&#30340;&#38382;&#39064;&#12290;&#36825;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#38480;&#21046;&#20102;LLMs&#22312;IR&#20013;&#30340;&#25972;&#20307;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;&#23545;&#27604;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;(RLCF)&#65292;&#23427;&#36171;&#20104;LLMs&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#36136;&#37327;&#21448;&#19982;IR&#20219;&#21153;&#38656;&#27714;&#30456;&#31526;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#20010;&#25991;&#26723;&#19982;&#20854;&#30456;&#20284;&#25991;&#26723;&#36827;&#34892;&#27604;&#36739;&#26500;&#24314;&#23545;&#27604;&#21453;&#39304;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Batched-MRR&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25945;&#23548;LLMs&#29983;&#25104;&#33021;&#22815;&#25429;&#25417;&#21306;&#20998;&#25991;&#26723;&#19982;&#20854;&#30456;&#20284;&#25991;&#26723;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Retrieval (IR), the process of finding information to satisfy user's information needs, plays an essential role in modern people's lives. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, some of which are important for IR. Nonetheless, LLMs frequently confront the issue of generating responses that lack specificity. This has limited the overall effectiveness of LLMs for IR in many cases. To address these issues, we present an unsupervised alignment framework called Reinforcement Learning from Contrastive Feedback (RLCF), which empowers LLMs to generate both high-quality and context-specific responses that suit the needs of IR tasks. Specifically, we construct contrastive feedback by comparing each document with its similar documents, and then propose a reward function named Batched-MRR to teach LLMs to generate responses that captures the fine-grained information that distinguish documents from their similar ones. To dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#36807;&#21435;&#21313;&#24180;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#30740;&#31350;&#36235;&#21183;&#21644;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#22686;&#24378;&#12289;&#35780;&#20272;&#21644;&#22797;&#29992;&#20197;&#21450;&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13186</link><description>&lt;p&gt;
&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#21313;&#24180;&#23398;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Decade of Scholarly Research on Open Knowledge Graphs. (arXiv:2306.13186v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#36807;&#21435;&#21313;&#24180;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#30740;&#31350;&#36235;&#21183;&#21644;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#22686;&#24378;&#12289;&#35780;&#20272;&#21644;&#22797;&#29992;&#20197;&#21450;&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#35813;&#35805;&#39064;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#28608;&#22686;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;2013&#24180;&#33267;2023&#24180;&#38388;&#20986;&#29256;&#30340;&#26377;&#20851;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#25991;&#29486;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#35813;&#39046;&#22495;&#20013;&#30340;&#36235;&#21183;&#65292;&#27169;&#24335;&#21644;&#30740;&#31350;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20986;&#29616;&#30340;&#20851;&#38190;&#20027;&#39064;&#21644;&#30740;&#31350;&#38382;&#39064;&#12290;&#35813;&#20316;&#21697;&#20351;&#29992;&#25991;&#29486;&#35745;&#37327;&#25216;&#26415;&#20998;&#26512;&#20102;&#20174;Scopus&#26816;&#32034;&#21040;&#30340;4445&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;&#26679;&#26412;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27599;&#24180;&#20851;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#36798;&#22269;&#23478;(+50 per year)&#12290;&#36825;&#20123;&#25104;&#26524;&#21457;&#34920;&#22312;&#39640;&#24230;&#24341;&#29992;&#30340;&#23398;&#26415;&#26399;&#21002;&#21644;&#20250;&#35758;&#19978;&#12290;&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#65306;(1)&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#22686;&#24378;&#65292;(2)&#35780;&#20272;&#21644;&#22797;&#29992;&#65292;&#20197;&#21450;(3)&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#20013;&#12290;&#22312;&#36825;&#20123;&#20027;&#39064;&#20013;&#65292;&#30740;&#31350;&#30830;&#23450;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#20855;&#20307;&#20219;&#21153;&#65292;&#20363;&#22914;&#23454;&#20307;&#38142;&#25509;&#65292;&#20851;&#31995;&#25552;&#21462;&#21644;&#26412;&#20307;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of open knowledge graphs has led to a surge in scholarly research on the topic over the past decade. This paper presents a bibliometric analysis of the scholarly literature on open knowledge graphs published between 2013 and 2023. The study aims to identify the trends, patterns, and impact of research in this field, as well as the key topics and research questions that have emerged. The work uses bibliometric techniques to analyze a sample of 4445 scholarly articles retrieved from Scopus. The findings reveal an ever-increasing number of publications on open knowledge graphs published every year, particularly in developed countries (+50 per year). These outputs are published in highly-referred scholarly journals and conferences. The study identifies three main research themes: (1) knowledge graph construction and enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into NLP systems. Within these themes, the study identifies specific tasks that have 
&lt;/p&gt;</description></item></channel></rss>