<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19651</link><description>&lt;p&gt;
MagicLens&#65306;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#19982;&#24320;&#25918;&#24335;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#65292;&#21363;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#26597;&#25214;&#25152;&#38656;&#22270;&#20687;&#65292;&#22266;&#26377;&#22320;&#21253;&#21547;&#38590;&#20197;&#20165;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#24230;&#37327;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#12289;&#22810;&#26041;&#38754;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20801;&#35768;&#29992;&#25143;&#26356;&#33258;&#30001;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#37027;&#20123;&#35270;&#35273;&#19978;&#30456;&#20284;&#21644;/&#25110;&#21487;&#20197;&#29992;&#19968;&#23567;&#32452;&#39044;&#23450;&#20041;&#20851;&#31995;&#26469;&#34920;&#24449;&#30340;&#22270;&#20687;&#23545;&#19978;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#35770;&#28857;&#26159;&#25991;&#26412;&#25351;&#20196;&#21487;&#20197;&#20351;&#22270;&#20687;&#26816;&#32034;&#33021;&#22815;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#12290;MagicLens&#24314;&#31435;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#39062;&#35265;&#35299;&#19978;&#65306;&#33258;&#28982;&#21457;&#29983;&#22312;&#21516;&#19968;&#32593;&#39029;&#19978;&#30340;&#22270;&#20687;&#23545;&#21253;&#21547;&#30528;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20869;&#37096;&#35270;&#22270;&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32508;&#21512;&#25351;&#20196;&#23558;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#21464;&#20026;&#26174;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>BAHE&#25552;&#20986;&#20102;&#34892;&#20026;&#32858;&#21512;&#20998;&#23618;&#32534;&#30721;&#65288;BAHE&#65289;&#26469;&#22686;&#24378;LLM-based CTR&#24314;&#27169;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#35299;&#32806;&#29992;&#25143;&#34892;&#20026;&#30340;&#32534;&#30721;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.19347</link><description>&lt;p&gt;
&#25171;&#30772;&#38271;&#24230;&#38480;&#21046;&#65306;LLM&#22686;&#24378;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19347
&lt;/p&gt;
&lt;p&gt;
BAHE&#25552;&#20986;&#20102;&#34892;&#20026;&#32858;&#21512;&#20998;&#23618;&#32534;&#30721;&#65288;BAHE&#65289;&#26469;&#22686;&#24378;LLM-based CTR&#24314;&#27169;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#35299;&#32806;&#29992;&#25143;&#34892;&#20026;&#30340;&#32534;&#30721;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLMs&#25552;&#39640;&#20102;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;LLMs&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38556;&#30861;&#65306;LLMs&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#26102;&#30340;&#25928;&#29575;&#12290;&#38543;&#30528;&#29992;&#25143;&#24207;&#21015;&#21464;&#24471;&#26356;&#38271;&#65292;&#24403;&#21069;&#30340;LLMs&#25928;&#29575;&#19981;&#36275;&#20197;&#22312;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#39033;&#30446;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#31361;&#30772;LLMs&#30340;&#25928;&#29575;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#20026;&#32858;&#21512;&#20998;&#23618;&#32534;&#30721;&#65288;BAHE&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;LLM&#30340;CTR&#24314;&#27169;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#22320;&#65292;BAHE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#26550;&#26500;&#65292;&#23558;&#29992;&#25143;&#34892;&#20026;&#30340;&#32534;&#30721;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20132;&#20114;&#35299;&#32806;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#38450;&#27490;&#30001;&#20110;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#29992;&#25143;&#34892;&#20026;&#32780;&#20135;&#29983;&#30340;&#35745;&#31639;&#20887;&#20313;&#65292;BAHE&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#27973;&#23618;&#26469;&#25552;&#21462;&#26368;&#31890;&#24230;&#30340;&#21407;&#23376;&#29992;&#25143;&#34892;&#20026;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19347v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from ext
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#30005;&#23376;&#21830;&#21153;&#21830;&#21697;&#20998;&#31867;&#31995;&#32479;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#36816;&#20316;&#26426;&#21046;&#65292;&#38416;&#36848;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#28145;&#20837;&#25506;&#35752;&#20102;&#30456;&#20851;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19345</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#20998;&#31867;&#19982;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#30005;&#23376;&#21830;&#21153;&#21830;&#21697;&#20998;&#31867;&#31995;&#32479;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#36816;&#20316;&#26426;&#21046;&#65292;&#38416;&#36848;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#28145;&#20837;&#25506;&#35752;&#20102;&#30456;&#20851;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#20449;&#24687;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#29992;&#25143;&#36935;&#21040;&#20102;&#20449;&#24687;&#36807;&#36733;&#21644;&#36873;&#25321;&#22256;&#22659;&#12290;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#24110;&#21161;&#29992;&#25143;&#31579;&#36873;&#21644;&#36873;&#25321;&#31526;&#21512;&#20854;&#20559;&#22909;&#21644;&#38656;&#27714;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#32531;&#35299;&#36825;&#19968;&#36127;&#25285;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#31995;&#32479;&#19981;&#20165;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#21644;&#28385;&#24847;&#24230;&#65292;&#36824;&#20026;&#20225;&#19994;&#21644;&#24179;&#21488;&#25552;&#20379;&#20102;&#22686;&#21152;&#29992;&#25143;&#21442;&#19982;&#24230;&#12289;&#38144;&#21806;&#39069;&#21644;&#24191;&#21578;&#25928;&#26524;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#23545;&#20256;&#32479;&#30005;&#23376;&#21830;&#21153;&#21830;&#21697;&#20998;&#31867;&#31995;&#32479;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#36816;&#20316;&#26426;&#21046;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#38416;&#36848;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#12289;&#20869;&#23481;&#20449;&#24687;&#21644;&#23186;&#20307;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19345v1 Announce Type: cross  Abstract: With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#20197;&#22686;&#24378;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#19981;&#21516;LLMs&#23454;&#29616;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TREC iKAT&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.19302</link><description>&lt;p&gt;
&#29983;&#25104;&#28982;&#21518;&#26816;&#32034;&#65306;&#20351;&#29992;LLM&#20316;&#20026;&#31572;&#26696;&#21644;&#26597;&#35810;&#29983;&#25104;&#22120;&#30340;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#20197;&#22686;&#24378;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#19981;&#21516;LLMs&#23454;&#29616;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TREC iKAT&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CIS&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#24320;&#21457;&#20132;&#20114;&#24335;&#30693;&#35782;&#21161;&#25163;&#12290;&#36825;&#20123;&#31995;&#32479;&#24517;&#39035;&#33021;&#22815;&#29087;&#32451;&#22320;&#29702;&#35299;&#29992;&#25143;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24182;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#31216;&#20026;&#37325;&#20889;&#26597;&#35810;&#30340;&#26597;&#35810;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24182;&#23558;&#27492;&#26597;&#35810;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#20197;&#22686;&#24378;&#26816;&#32034;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#65292;&#20197;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21253;&#25324;GPT-4&#21644;Llama-2&#22312;&#38646;-shot&#21644;&#23569;-shot&#35774;&#32622;&#20013;&#30340;&#21508;&#31181;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;gpt 3.5&#30340;&#21028;&#26029;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TREC iKAT&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19302v1 Announce Type: new  Abstract: CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectivenes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#25490;&#24207;&#30446;&#26631;Hard-BPR&#65292;&#19987;&#38376;&#20026;&#21160;&#24577;&#22256;&#38590;&#36127;&#20363;&#25277;&#26679;&#32780;&#35774;&#35745;&#65292;&#20197;&#32531;&#35299;&#20551;&#36127;&#20363;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19276</link><description>&lt;p&gt;
&#21152;&#24378;&#30340;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#25490;&#24207;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20581;&#22766;&#36127;&#20363;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#25490;&#24207;&#30446;&#26631;Hard-BPR&#65292;&#19987;&#38376;&#20026;&#21160;&#24577;&#22256;&#38590;&#36127;&#20363;&#25277;&#26679;&#32780;&#35774;&#35745;&#65292;&#20197;&#32531;&#35299;&#20551;&#36127;&#20363;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#24335;&#21327;&#21516;&#36807;&#28388;&#20013;&#65292;&#21457;&#23637;&#20102;&#22256;&#38590;&#30340;&#36127;&#20363;&#25366;&#25496;&#25216;&#26415;&#26469;&#21152;&#36895;&#21644;&#22686;&#24378;&#25512;&#33616;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22256;&#38590;&#36127;&#20363;&#25277;&#26679;&#20013;&#65292;&#24847;&#22806;&#36873;&#25321;&#20551;&#36127;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#20551;&#36127;&#20363;&#21487;&#33021;&#25552;&#20379;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#24182;&#35823;&#23548;&#27169;&#22411;&#23398;&#20064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#20551;&#36127;&#20363;&#38382;&#39064;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#25277;&#26679;&#31639;&#27861;&#26469;&#36807;&#28388;&#20551;&#36127;&#20363;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#23558;&#28966;&#28857;&#36716;&#21521;&#20102;&#32454;&#21270;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#22343;&#21248;&#36127;&#20363;&#25277;&#26679;&#30340;&#21407;&#22987;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#25490;&#24207;&#65288;BPR&#65289;&#22312;&#36866;&#24212;&#22256;&#38590;&#25277;&#26679;&#22330;&#26223;&#26102;&#26159;&#19981;&#36275;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#25490;&#24207;&#30446;&#26631;&#65292;&#21629;&#21517;&#20026;Hard-BPR&#65292;&#19987;&#38376;&#20026;&#21160;&#24577;&#22256;&#38590;&#36127;&#20363;&#25277;&#26679;&#32780;&#35774;&#35745;&#65292;&#20197;&#20943;&#36731;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19276v1 Announce Type: new  Abstract: In implicit collaborative filtering, hard negative mining techniques are developed to accelerate and enhance the recommendation model learning. However, the inadvertent selection of false negatives remains a major concern in hard negative sampling, as these false negatives can provide incorrect information and mislead the model learning. To date, only a small number of studies have been committed to solve the false negative problem, primarily focusing on designing sophisticated sampling algorithms to filter false negatives. In contrast, this paper shifts its focus to refining the loss function. We find that the original Bayesian Personalized Ranking (BPR), initially designed for uniform negative sampling, is inadequate in adapting to hard sampling scenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking objective, named as Hard-BPR, which is specifically crafted for dynamic hard negative sampling to mitigate the influence
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35780;&#20272;&#27573;&#33853;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#23454;&#39564;&#21457;&#29616;&#21463;&#36807;&#33391;&#22909;&#25351;&#23548;&#30340;LLMs&#21487;&#20197;...</title><link>https://arxiv.org/abs/2403.19216</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23454;&#29992;&#24615;&#21028;&#26029;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Good at Utility Judgments?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19216
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35780;&#20272;&#27573;&#33853;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#23454;&#39564;&#21457;&#29616;&#21463;&#36807;&#33391;&#22909;&#25351;&#23548;&#30340;LLMs&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#35748;&#20026;&#26159;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24187;&#35273;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36817;&#26399;&#24050;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#30001;&#20110;&#26816;&#32034;&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;RAG&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;LLMs&#35782;&#21035;&#20855;&#26377;&#23454;&#29992;&#24615;&#30340;&#27573;&#33853;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#35780;&#20272;&#26816;&#32034;&#20013;&#27573;&#33853;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#35780;&#20272;&#25903;&#25345;&#38382;&#31572;&#30340;&#27573;&#33853;&#23454;&#29992;&#24615;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;LLMs&#22312;&#24320;&#25918;&#22495;QA&#23454;&#29992;&#24615;&#35780;&#20272;&#26041;&#38754;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#31243;&#24207;&#21644;&#19981;&#21516;&#29305;&#24449;&#30340;&#20505;&#36873;&#27573;&#33853;&#38598;&#21512;&#65292;&#20419;&#36827;&#20102;&#19982;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;i&#65289;&#21463;&#36807;&#33391;&#22909;&#25351;&#23548;&#30340;LLMs&#21487;&#20197;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19216v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#21040;&#22270;&#39044;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#25552;&#20379;&#20855;&#20307;&#20219;&#21153;&#30340;&#26126;&#30830;&#25351;&#23548;&#65292;&#20197;&#20811;&#26381;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.19063</link><description>&lt;p&gt;
&#22522;&#20110;&#25351;&#20196;&#30340;&#36229;&#22270;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Instruction-based Hypergraph Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19063
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#21040;&#22270;&#39044;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#25552;&#20379;&#20855;&#20307;&#20219;&#21153;&#30340;&#26126;&#30830;&#25351;&#23548;&#65292;&#20197;&#20811;&#26381;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20197;&#22686;&#24378;&#22270;&#23398;&#20064;&#27169;&#22411;&#23545;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20256;&#36755;&#30693;&#35782;&#33267;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#38142;&#25509;&#39044;&#27979;&#25110;&#20998;&#31867;&#65289;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#38459;&#30861;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#36716;&#31227;&#12290;&#21463;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25351;&#20196;&#24341;&#20837;&#22270;&#39044;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#36229;&#22270;&#39044;&#35757;&#32451;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#20026;&#20102;&#20811;&#26381;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24212;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#25351;&#20196;&#26469;&#20026;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#29305;&#23450;&#20219;&#21153;&#25552;&#20379;&#26126;&#30830;&#25351;&#23548;&#12290;&#19982;&#21487;&#23398;&#20064;&#25552;&#31034;&#30456;&#27604;&#65292;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25991;&#26412;&#25351;&#20196;&#22266;&#26377;&#22320;&#34164;&#21547;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19063v1 Announce Type: new  Abstract: Pretraining has been widely explored to augment the adaptability of graph learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification. However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the pretrained knowledge. Inspired by instruction-based prompts widely used in pretrained language models, we introduce instructions into graph pretraining. In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining. To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for representation learning. Compared to learnable prompts, whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>&#39640;&#26597;&#20934;&#29575;&#12289;&#23569;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;&#29616;&#22330;&#27861;&#24459;&#25628;&#32034;&#31995;&#32479;&#20869;&#35780;&#20272;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#24120;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#19981;&#22815;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2403.18962</link><description>&lt;p&gt;
&#39640;&#26597;&#20934;&#29575;&#12289;&#23569;&#25968;&#25454;&#65306;&#29616;&#22330;&#27861;&#24459;&#25628;&#32034;&#31995;&#32479;&#20013;&#31995;&#32479;&#20869;&#35780;&#20272;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18962
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26597;&#20934;&#29575;&#12289;&#23569;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;&#29616;&#22330;&#27861;&#24459;&#25628;&#32034;&#31995;&#32479;&#20869;&#35780;&#20272;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#24120;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#19981;&#22815;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#26126;&#20102;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#24120;&#35265;&#25490;&#21517;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#29616;&#22330;&#27861;&#24459;&#25628;&#32034;&#31995;&#32479;&#30340;&#26085;&#24535;&#25968;&#25454;&#21644;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#27861;&#24459;IR&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20197;&#21450;&#36825;&#20123;&#26041;&#38754;&#23545;&#20110;&#26399;&#26395;&#20013;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#30340;&#25361;&#25112;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#30340;&#27979;&#35797;&#38598;&#12289;&#29992;&#25143;&#35843;&#26597;&#21644;A/B&#27979;&#35797;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#22330;&#21830;&#19994;&#27861;&#24459;&#25628;&#32034;&#24341;&#25806;&#30340;&#25968;&#25454;&#26469;&#38416;&#26126;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#21333;&#20010;IR&#31995;&#32479;&#38543;&#26102;&#38388;&#36830;&#32493;&#26356;&#25913;&#25991;&#26723;&#25490;&#21517;&#30340;&#26377;&#25928;&#24615;&#30417;&#35270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27861;&#24459;IR&#31995;&#32479;&#29305;&#24615;&#19982;&#26377;&#38480;&#29992;&#25143;&#25968;&#25454;&#32467;&#21512;&#21487;&#33021;&#23548;&#33268;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35752;&#35770;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#19981;&#22815;&#29702;&#24819;&#12290;&#22312;&#25105;&#20204;&#30340;&#26410;&#26469;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19987;&#27880;&#20110;&#36739;&#23569;&#35265;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18962v1 Announce Type: new  Abstract: This paper illustrates some challenges of common ranking evaluation methods for legal information retrieval (IR). We show these challenges with log data from a live legal search system and two user studies. We provide an overview of aspects of legal IR, and the implications of these aspects for the expected challenges of common evaluation methods: test collections based on explicit and implicit feedback, user surveys, and A/B testing. Next, we illustrate the challenges of common evaluation methods using data from a live, commercial, legal search engine. We specifically focus on methods for monitoring the effectiveness of (continuous) changes to document ranking by a single IR system over time. We show how the combination of characteristics in legal IR systems and limited user data can lead to challenges that cause the common evaluation methods discussed to be sub-optimal. In our future work we will therefore focus on less common evaluati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#33258;&#21160;&#23637;&#31034;&#30456;&#20851;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#20041;&#34920;&#31034;&#22312;&#25512;&#33616;&#21644;&#25490;&#21517;&#20219;&#21153;&#19978;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18855</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#39044;&#27979;&#36827;&#34892;&#23450;&#21521;&#26631;&#20934;&#24341;&#25991;&#25512;&#33616;&#21644;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Directed Criteria Citation Recommendation and Ranking Through Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18855
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#33258;&#21160;&#23637;&#31034;&#30456;&#20851;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#20041;&#34920;&#31034;&#22312;&#25512;&#33616;&#21644;&#25490;&#21517;&#20219;&#21153;&#19978;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#33258;&#21160;&#23637;&#31034;&#19982;&#26032;&#25991;&#26723;&#22312;&#20027;&#39064;&#25110;&#19978;&#19979;&#25991;&#19978;&#21487;&#33021;&#30456;&#20851;&#30340;&#29616;&#26377;&#25991;&#29486;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22270;&#23884;&#20837;&#26469;&#32534;&#30721;&#27599;&#20010;&#25991;&#26723;&#30340;&#21547;&#20041;&#65292;&#21576;&#29616;&#20026;&#24341;&#25991;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#22815;&#22312;&#25512;&#33616;&#21644;&#25490;&#21517;&#20219;&#21153;&#20013;&#32988;&#36807;&#20854;&#20182;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#36825;&#20026;&#25506;&#32034;&#24341;&#25991;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#37027;&#20123;&#36825;&#20123;&#25991;&#26723;&#27491;&#30830;&#20114;&#30456;&#24341;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#20197;&#20415;&#26368;&#23567;&#21270;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18855v1 Announce Type: cross  Abstract: We explore link prediction as a proxy for automatically surfacing documents from existing literature that might be topically or contextually relevant to a new document. Our model uses transformer-based graph embeddings to encode the meaning of each document, presented as a node within a citation network. We show that the semantic representations that our model generates can outperform other content-based methods in recommendation and ranking tasks. This provides a holistic approach to exploring citation graphs in domains where it is critical that these documents properly cite each other, so as to minimize the possibility of any inconsistencies
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29992;&#25143;/&#39033;&#30446;&#30340;&#32034;&#24341;&#21040;&#20803;&#23884;&#20837;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18479</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#23884;&#20837;&#29992;&#20110;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Lightweight Embeddings for Graph Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18479
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29992;&#25143;/&#39033;&#30446;&#30340;&#32034;&#24341;&#21040;&#20803;&#23884;&#20837;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30446;&#21069;&#26159;&#26368;&#26377;&#25928;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#23884;&#20837;&#34920;&#26469;&#34920;&#31034;&#27599;&#20010;&#29992;&#25143;/&#39033;&#30446;&#20026;&#19981;&#21516;&#30340;&#21521;&#37327;&#65292;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#32487;&#25215;&#20102;&#21442;&#25968;&#25928;&#29575;&#20302;&#19979;&#30340;&#38271;&#26399;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#21363;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#21160;&#23398;&#20064;&#29992;&#25143;/&#39033;&#30446;&#30340;&#32034;&#24341;&#21040;&#23545;&#24212;&#30340;&#20803;&#23884;&#20837;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18479v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are currently one of the most performant collaborative filtering methods. Meanwhile, owing to the use of an embedding table to represent each user/item as a distinct vector, GNN-based recommenders have inherited the long-standing defect of parameter inefficiency. As a common practice for scalable embeddings, parameter sharing enables the use of fewer embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most existing methods are a heuristically designed, predefined mapping from each user's/item's ID to the corresponding meta-embedding indexes, thus simplifying the optimization problem into learning only the meta-embeddings. However, in the context of GNN-based collaborative filtering, such a fixed mapping omits the semantic correlations between entities that are evident in the user-item interaction graph, leading to suboptimal recommendation performance. To this end, we propose Lightweigh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;</title><link>https://arxiv.org/abs/2403.17740</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#24322;&#36136;&#20132;&#20114;&#24314;&#27169;&#29992;&#20110;&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17740
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#20363;&#22914;&#21327;&#21516;&#36807;&#28388;&#12289;&#31038;&#20132;&#25512;&#33616;&#21644;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65292;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#19981;&#21516;&#35282;&#33394;&#20043;&#38388;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#26174;&#24335;&#20851;&#31995;&#21487;&#33021;&#19981;&#21487;&#38752;&#19988;&#26080;&#20851;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29305;&#23450;&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#38480;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#12290;HIRE&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#39044;&#20808;&#23450;&#20041;&#30340;&#20132;&#20114;&#27169;&#24335;&#25110;&#25163;&#21160;&#26500;&#24314;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#65292;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.17210</link><description>&lt;p&gt;
CADGL: &#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#30740;&#31350;&#26159;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#12290;DDIs&#21457;&#29983;&#22312;&#19968;&#20010;&#33647;&#29289;&#30340;&#24615;&#36136;&#21463;&#20854;&#20182;&#33647;&#29289;&#21253;&#21547;&#30340;&#24433;&#21709;&#26102;&#12290;&#26816;&#27979;&#26377;&#21033;&#30340;DDIs&#26377;&#21487;&#33021;&#20026;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#21019;&#26032;&#33647;&#29289;&#30340;&#21019;&#36896;&#21644;&#25512;&#36827;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#12289;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#29616;&#23454;&#24212;&#29992;&#21487;&#33021;&#24615;&#26041;&#38754;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;CADGL&#30340;&#26032;&#39062;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22522;&#20110;&#23450;&#21046;&#30340;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19978;&#19979;&#25991;&#39044;&#22788;&#29702;&#22120;&#20174;&#20004;&#20010;&#19981;&#21516;&#35270;&#35282;&#65306;&#23616;&#37096;&#37051;&#22495;&#21644;&#20998;&#23376;&#19978;&#19979;&#25991;&#65292;&#22312;&#24322;&#36136;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25429;&#33719;&#20851;&#38190;&#30340;&#32467;&#26500;&#21644;&#29983;&#29702;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10667</link><description>&lt;p&gt;
&#36890;&#21521;&#32479;&#19968;&#22810;&#27169;&#24335;&#20010;&#24615;&#21270;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#36164;&#28304;&#24182;&#28385;&#36275;&#21508;&#31181;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#19968;&#30452;&#26159;&#31038;&#21306;&#28212;&#26395;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26085;&#24120;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#23578;&#21644;&#38646;&#21806;&#31561;&#39046;&#22495;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#24577;&#19981;&#20165;&#25552;&#20379;&#30452;&#35266;&#30340;&#25351;&#23548;&#65292;&#36824;&#36814;&#21512;&#20010;&#24615;&#21270;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#20027;&#35201;&#32858;&#28966;&#20110;&#22522;&#20110;ID&#25110;&#25991;&#26412;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#26410;&#33021;&#29702;&#35299;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#25110;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#30784;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04260</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#33391;&#22909;&#25512;&#29702;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Small Language Models be Good Reasoners for Sequential Recommendation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#24320;&#25299;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#25104;&#21151;&#23454;&#29616;&#30001;LLMs&#36171;&#33021;&#30340;&#39034;&#24207;&#25512;&#33616;&#36824;&#26377;&#35768;&#22810;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#36890;&#24120;&#22797;&#26434;&#65292;&#20165;&#20165;&#20381;&#38752;LLMs&#30340;&#19968;&#27493;&#25512;&#29702;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;LLMs&#65288;&#20363;&#22914;ChatGPT-175B&#65289;&#26497;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#26159;&#38590;&#20197;&#25215;&#21463;&#19988;&#22312;&#23454;&#38469;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#22120;&#20197;&#8220;&#30246;&#8221;&#65288;&#21363;&#36164;&#28304;&#39640;&#25928;&#65289;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#38138;&#24179;&#20102;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;CoT&#25552;&#31034;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25361;&#25112;&#20102;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#22270;&#20013;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#33258;&#28982;&#21453;&#26144;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#34913;&#37327;&#36825;&#31181;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.10370</link><description>&lt;p&gt;
&#30456;&#20284;&#30340;&#23454;&#20307;&#26159;&#21542;&#20855;&#26377;&#30456;&#20284;&#30340;&#23884;&#20837;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Similar Entities have Similar Embeddings?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#22270;&#20013;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#33258;&#28982;&#21453;&#26144;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#34913;&#37327;&#36825;&#31181;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#32780;&#24320;&#21457;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#23398;&#20064;&#30693;&#35782;&#22270;&#20013;&#23454;&#20307;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21363;&#23884;&#20837;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#40664;&#35748;&#20551;&#35774;&#26159;KGE&#23454;&#20307;&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#21363;&#36825;&#20123;KGEMs&#22312;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#30041;&#22270;&#30340;&#32467;&#26500;&#65292;&#21363;&#23558;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#22270;&#20013;&#24444;&#27492;&#38752;&#36817;&#12290;&#36825;&#31181;&#29702;&#24819;&#30340;&#24615;&#36136;&#20351;&#24471;KGEMs&#22312;&#25512;&#33616;&#31995;&#32479;&#25110;&#33647;&#29289;&#20877;&#21033;&#29992;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23454;&#20307;&#30340;&#30456;&#20284;&#24615;&#19982;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#24456;&#23569;&#34987;&#27491;&#24335;&#35780;&#20272;&#12290;&#36890;&#24120;&#65292;KGEMs&#26159;&#22522;&#20110;&#20854;&#21807;&#19968;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#20351;&#29992;&#31867;&#20284;Hits@K&#25110;Mean Rank&#30340;&#25490;&#21517;&#25351;&#26631;&#12290;&#26412;&#25991;&#36136;&#30097;&#20102;&#22270;&#20013;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#22825;&#28982;&#21453;&#26144;&#36825;&#19968;&#27969;&#34892;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10370v2 Announce Type: replace  Abstract: Knowledge graph embedding models (KGEMs) developed for link prediction learn vector representations for entities in a knowledge graph, known as embeddings. A common tacit assumption is the KGE entity similarity assumption, which states that these KGEMs retain the graph's structure within their embedding space, \textit{i.e.}, position similar entities within the graph close to one another. This desirable property make KGEMs widely used in downstream tasks such as recommender systems or drug repurposing. Yet, the relation of entity similarity and similarity in the embedding space has rarely been formally evaluated. Typically, KGEMs are assessed based on their sole link prediction capabilities, using ranked-based metrics such as Hits@K or Mean Rank. This paper challenges the prevailing assumption that entity similarity in the graph is inherently mirrored in the embedding space. Therefore, we conduct extensive experiments to measure the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#30340;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26597;&#35810;&#25193;&#23637;&#20013;&#24050;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2310.19056</link><description>&lt;p&gt;
MILL&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#30340;&#30456;&#20114;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#30340;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26597;&#35810;&#25193;&#23637;&#20013;&#24050;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26597;&#35810;-&#26597;&#35810;-&#25991;&#26723;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23376;&#26597;&#35810;&#21644;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#30456;&#20114;&#39564;&#35777;&#36807;&#31243;&#21327;&#21516;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#25991;&#26723;&#20197;&#23454;&#29616;&#26368;&#20339;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23436;&#20840;&#26159;&#38646;-shot&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19056v3 Announce Type: replace-cross  Abstract: Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs' zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#30693;&#35782;&#22270;&#35889;&#20013;&#20855;&#26377;&#22810;&#26679;&#21270;&#31572;&#26696;&#30340;&#36923;&#36753;&#26597;&#35810;</title><link>https://arxiv.org/abs/2306.10367</link><description>&lt;p&gt;
Query2GMM&#65306;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10367
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#30693;&#35782;&#22270;&#35889;&#20013;&#20855;&#26377;&#22810;&#26679;&#21270;&#31572;&#26696;&#30340;&#36923;&#36753;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#30693;&#35782;&#22270;&#35889;&#20013;&#19968;&#20010;&#22522;&#30784;&#19988;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;Query2GMM&#26469;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#23558;&#26597;&#35810;&#21644;&#23454;&#20307;&#20849;&#21516;&#23884;&#20837;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20855;&#26377;&#22810;&#26679;&#21270;&#31572;&#26696;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10367v2 Announce Type: replace  Abstract: Logical query answering over Knowledge Graphs (KGs) is a fundamental yet complex task. A promising approach to achieve this is to embed queries and entities jointly into the same embedding space. Research along this line suggests that using multi-modal distribution to represent answer entities is more suitable than uni-modal distribution, as a single query may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations. However, existing methods based on multi-modal distribution roughly represent each subset without capturing its accurate cardinality, or even degenerate into uni-modal distribution learning during the reasoning process due to the lack of an effective similarity measure. To better model queries with diversified answers, we propose Query2GMM for answering logical queries over knowledge graphs. In Query2GMM, we present the GMM embedding to re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#21363;&#20197;&#27491;&#26679;&#26412;&#20026;&#20027;&#23548;&#30340;&#36127;&#26679;&#26412;&#21512;&#25104;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2211.13912</link><description>&lt;p&gt;
&#21152;&#24378;&#25512;&#33616;&#31995;&#32479;&#65306;&#20943;&#36731;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Enhancing Recommender Systems: A Strategy to Mitigate False Negative Impact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#21363;&#20197;&#27491;&#26679;&#26412;&#20026;&#20027;&#23548;&#30340;&#36127;&#26679;&#26412;&#21512;&#25104;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#38544;&#24335;&#21327;&#21516;&#36807;&#28388;&#20219;&#21153;&#20013;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#32467;&#26500;&#35774;&#35745;&#65292;&#20351;&#29992;&#20102;&#19968;&#20123;&#24456;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#19968;&#20010;&#25361;&#25112;&#26159;&#29616;&#26377;&#30340;&#30828;&#36127;&#37319;&#26679;&#22120;&#24448;&#24448;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#36973;&#21463;&#26356;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#21363;&#36890;&#36807;&#19981;&#27491;&#30830;&#22320;&#36873;&#25321;&#20551;&#36127;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#36870;&#21521;&#29616;&#35937;&#65292;&#21363;&#29992;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#27491;&#26679;&#26412;&#23884;&#20837;&#26469;&#27745;&#26579;&#30828;&#36127;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#23558;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#20986;&#29616;&#26126;&#26174;&#25552;&#21319;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#21363;&#20197;&#27491;&#26679;&#26412;&#20026;&#20027;&#23548;&#30340;&#36127;&#26679;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.13912v2 Announce Type: replace  Abstract: In implicit collaborative filtering (CF) task of recommender systems, recent works mainly focus on model structure design with promising techniques like graph neural networks (GNNs). Effective and efficient negative sampling methods that suit these models, however, remain underdeveloped. One challenge is that existing hard negative samplers tend to suffer from severer over-fitting in model training. In this work, we first study the reason behind the over-fitting, and illustrate it with the incorrect selection of false negative instances with the support of experiments. In addition, we empirically observe a counter-intuitive phenomenon, that is, polluting hard negative samples' embeddings with a quite large proportional of positive samples' embeddings will lead to remarkable performance gains for prediction accuracy. On top of this finding, we present a novel negative sampling strategy, i.e., positive-dominated negative synthesizing (
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20849;&#21516;&#35774;&#35745;&#19968;&#20010;MRMC&#26694;&#26550;&#65292;&#35299;&#20915;&#32479;&#35745;MIMO&#38647;&#36798;&#21644;&#22522;&#39057;&#20840;&#21452;&#24037;&#22810;&#29992;&#25143;MIMO&#36890;&#20449;&#31995;&#32479;&#22312;&#30456;&#21516;&#39057;&#27573;&#20869;&#36816;&#34892;&#26102;&#36935;&#21040;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2006.14774</link><description>&lt;p&gt;
&#32479;&#35745;MIMO&#38647;&#36798;&#21644;&#22522;&#39057;&#20840;&#21452;&#24037;&#22810;&#29992;&#25143;MIMO&#36890;&#20449;&#30340;&#32852;&#21512;&#35774;&#35745;--&#31532;&#19968;&#37096;&#20998;: &#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications -- Part I: Signal Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.14774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20849;&#21516;&#35774;&#35745;&#19968;&#20010;MRMC&#26694;&#26550;&#65292;&#35299;&#20915;&#32479;&#35745;MIMO&#38647;&#36798;&#21644;&#22522;&#39057;&#20840;&#21452;&#24037;&#22810;&#29992;&#25143;MIMO&#36890;&#20449;&#31995;&#32479;&#22312;&#30456;&#21516;&#39057;&#27573;&#20869;&#36816;&#34892;&#26102;&#36935;&#21040;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#39057;&#35889;&#20849;&#20139;&#38382;&#39064;&#65292;&#21363;&#22312;&#21516;&#19968;&#39057;&#27573;&#20869;&#21516;&#26102;&#36816;&#34892;&#32479;&#35745;&#65288;&#25110;&#24191;&#27867;&#20998;&#24067;&#30340;&#65289;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#38647;&#36798;&#21644;&#22522;&#39057;&#20840;&#21452;&#24037;&#65288;IBFD&#65289;&#22810;&#29992;&#25143;MIMO&#65288;MU-MIMO&#65289;&#36890;&#20449;&#31995;&#32479;&#12290;&#20197;&#24448;&#20851;&#20110;&#32852;&#21512;MIMO&#38647;&#36798;-MIMO&#36890;&#20449;&#65288;MRMC&#65289;&#31995;&#32479;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20849;&#20301;MIMO&#38647;&#36798;&#12289;&#21322;&#21452;&#24037;MIMO&#36890;&#20449;&#12289;&#21333;&#29992;&#25143;&#22330;&#26223;&#65292;&#30465;&#30053;&#20102;&#23454;&#38469;&#32422;&#26463;&#65288;&#26434;&#27874;&#12289;&#19978;&#34892;/&#19979;&#34892;&#21457;&#36865;&#21151;&#29575;&#12289;&#19978;&#34892;/&#19979;&#34892;&#26381;&#21153;&#36136;&#37327;&#21644;&#23792;&#22343;&#21151;&#29575;&#27604;&#65289;&#65292;&#25110;&#32773;&#37319;&#29992;&#20998;&#24320;&#30340;&#21457;&#36865;/&#25509;&#25910;&#21333;&#20803;&#30340;MRMC&#20849;&#23384;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20849;&#21516;&#35774;&#35745;&#19968;&#20010;MRMC&#26694;&#26550;&#65292;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;IBFD MRMC&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#38647;&#36798;&#25509;&#25910;&#26426;&#34987;&#35774;&#35745;&#20026;&#39069;&#22806;&#21033;&#29992;&#19979;&#34892;&#36890;&#20449;&#20449;&#21495;&#30340;&#21453;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.14774v5 Announce Type: replace-cross  Abstract: We consider a spectral sharing problem in which a statistical (or widely distributed) multiple-input multiple-output (MIMO) radar and an in-band full-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system concurrently operate within the same frequency band. Prior works on joint MIMO-radar-MIMO-communications (MRMC) systems largely focus on either colocated MIMO radars, half-duplex MIMO communications, single-user scenarios, omit practical constraints (clutter, uplink [UL]/downlink [DL] transmit powers, UL/DL quality-of-service, and peak-to-average-power ratio), or MRMC co-existence that employs separate transmit/receive units. The purpose of this and companion papers (Part II and III) is to co-design an MRMC framework that addresses all of these issues. In this paper, we propose signal processing for a distributed IBFD MRMC, where radar receiver is designed to additionally exploit the downlink communications signals refl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#25991;&#26412;&#25490;&#24207;&#22120;&#65292;&#20855;&#26377;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#30452;&#25509;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#25991;&#26723;&#36755;&#20837;&#25552;&#31034;&#36827;&#34892;&#25991;&#26723;&#25490;&#24207;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#23454;&#29992;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24456;&#38590;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#31934;&#35843;&#22522;&#20934;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#30340;&#28857;&#23545;&#28857;&#21644;&#21015;&#34920;&#25490;&#24207;&#25552;&#31034;&#65292;&#24182;&#35748;&#20026;&#29616;&#25104;&#30340;LLM&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#25490;&#24207;&#20844;&#24335;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;LLM&#30340;&#35757;&#32451;&#26041;&#24335;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#65288;PRP&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;LLM&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;&#24320;&#28304;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;&#22312;TREC-DL2020&#19978;&#65292;&#22522;&#20110;20B&#21442;&#25968;&#30340;Flan-UL2&#27169;&#22411;&#30340;PRP&#36229;&#36807;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#21830;&#19994;&#40657;&#30418;GPT-4&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>