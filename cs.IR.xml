<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#24314;&#27169;&#25903;&#25345;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#30142;&#30149;&#25968;&#25454;&#65292;&#20197;&#20415;&#23558;&#20854;&#29992;&#20110;&#21307;&#24072;&#30740;&#31350;&#27963;&#21160;&#65292;&#35299;&#20915;EHRs&#25968;&#25454;&#26684;&#24335;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01218</link><description>&lt;p&gt;
&#25903;&#25345;&#21307;&#24072;&#30740;&#31350;&#27963;&#21160;&#30340;&#31995;&#32479;&#24314;&#27169;&#20197;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#30142;&#30149;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards System Modelling to Support Diseases Data Extraction from the Electronic Health Records for Physicians Research Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#24314;&#27169;&#25903;&#25345;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#30142;&#30149;&#25968;&#25454;&#65292;&#20197;&#20415;&#23558;&#20854;&#29992;&#20110;&#21307;&#24072;&#30740;&#31350;&#27963;&#21160;&#65292;&#35299;&#20915;EHRs&#25968;&#25454;&#26684;&#24335;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;15&#24180;&#20013;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#20351;&#29992;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#22240;&#20026;&#23427;&#34987;&#35748;&#20026;&#26159;&#31649;&#29702;&#30149;&#20154;&#25968;&#25454;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;EHRs&#26159;&#19990;&#30028;&#33539;&#22260;&#20869;&#30142;&#30149;&#35786;&#26029;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35832;&#22914;&#30740;&#31350;&#20043;&#31867;&#30340;&#27425;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#20351;&#36825;&#20123;&#25968;&#25454;&#21487;&#29992;&#20110;&#30417;&#27979;&#29305;&#23450;&#20154;&#32676;&#30340;&#30142;&#30149;&#32479;&#35745;&#25968;&#25454;&#31561;&#30740;&#31350;&#27963;&#21160;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#30446;&#26631;&#32676;&#20307;&#30340;&#30149;&#22240;&#19982;&#34892;&#20026;&#21644;&#29983;&#27963;&#26041;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;EHRs&#31995;&#32479;&#30340;&#23616;&#38480;&#20043;&#19968;&#26159;&#25968;&#25454;&#19981;&#20197;&#26631;&#20934;&#26684;&#24335;&#32780;&#26159;&#20197;&#21508;&#31181;&#24418;&#24335;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#39318;&#20808;&#23558;&#30142;&#30149;&#21517;&#31216;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36716;&#25442;&#20026;&#19968;&#20010;&#26631;&#20934;&#26684;&#24335;&#65292;&#20197;&#20351;&#20854;&#21487;&#29992;&#20110;&#30740;&#31350;&#27963;&#21160;&#12290;&#26377;&#22823;&#37327;&#30340;EHRs&#21487;&#29992;&#65292;&#35299;&#20915;&#26631;&#20934;&#21270;&#38382;&#39064;&#38656;&#35201;&#19968;&#20123;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01218v1 Announce Type: new  Abstract: The use of Electronic Health Records (EHRs) has increased dramatically in the past 15 years, as, it is considered an important source of managing data od patients. The EHRs are primary sources of disease diagnosis and demographic data of patients worldwide. Therefore, the data can be utilized for secondary tasks such as research. This paper aims to make such data usable for research activities such as monitoring disease statistics for a specific population. As a result, the researchers can detect the disease causes for the behavior and lifestyle of the target group. One of the limitations of EHRs systems is that the data is not available in the standard format but in various forms. Therefore, it is required to first convert the names of the diseases and demographics data into one standardized form to make it usable for research activities. There is a large amount of EHRs available, and solving the standardizing issues requires some optim
&lt;/p&gt;</description></item><item><title>&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#25512;&#33616;&#31995;&#32479;&#23384;&#22312;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#30340;&#23433;&#20840;&#38544;&#24739;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#23545;&#31574;&#12290;</title><link>https://arxiv.org/abs/2404.01177</link><description>&lt;p&gt;
&#20013;&#27602;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#25512;&#33616;&#31995;&#32479;&#21450;&#20854;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Poisoning Decentralized Collaborative Recommender System and Its Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01177
&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#25512;&#33616;&#31995;&#32479;&#23384;&#22312;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#30340;&#23433;&#20840;&#38544;&#24739;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;&#38544;&#31169;&#21644;&#25928;&#29575;&#33150;&#20986;&#31354;&#38388;&#65292;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#30340;&#37096;&#32626;&#27491;&#22312;&#20174;&#20013;&#22830;&#26381;&#21153;&#22120;&#36716;&#21521;&#20010;&#20154;&#35774;&#22791;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRecs&#65289;&#21644;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#25512;&#33616;&#31995;&#32479;&#65288;DecRecs&#65289;&#21487;&#20197;&#35828;&#26159;&#20004;&#31181;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33539;&#24335;&#12290;&#34429;&#28982;&#20004;&#32773;&#37117;&#21033;&#29992;&#30693;&#35782;&#65288;&#22914;&#26799;&#24230;&#65289;&#20849;&#20139;&#26469;&#20419;&#36827;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#65292;&#20294;FedRecs&#20381;&#36182;&#20110;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#35843;&#20248;&#21270;&#36807;&#31243;&#65292;&#32780;&#22312;DecRecs&#20013;&#65292;&#30693;&#35782;&#20849;&#20139;&#30452;&#25509;&#21457;&#29983;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;&#30693;&#35782;&#20849;&#20139;&#20063;&#20026;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#25171;&#24320;&#20102;&#21518;&#38376;&#65292;&#20854;&#20013;&#23545;&#25163;&#20266;&#35013;&#25104;&#33391;&#24615;&#23458;&#25143;&#31471;&#65292;&#20256;&#25773;&#27745;&#26579;&#30693;&#35782;&#20197;&#36798;&#21040;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#25552;&#39640;&#39033;&#30446;&#30340;&#26333;&#20809;&#29575;&#12290;&#23613;&#31649;&#20851;&#20110;&#36825;&#31181;&#20013;&#27602;&#25915;&#20987;&#30340;&#30740;&#31350;&#20026;&#21457;&#29616;&#23433;&#20840;&#28431;&#27934;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#25915;&#20987;&#20027;&#35201;&#38598;&#20013;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01177v1 Announce Type: cross  Abstract: To make room for privacy and efficiency, the deployment of many recommender systems is experiencing a shift from central servers to personal devices, where the federated recommender systems (FedRecs) and decentralized collaborative recommender systems (DecRecs) are arguably the two most representative paradigms. While both leverage knowledge (e.g., gradients) sharing to facilitate learning local models, FedRecs rely on a central server to coordinate the optimization process, yet in DecRecs, the knowledge sharing directly happens between clients. Knowledge sharing also opens a backdoor for model poisoning attacks, where adversaries disguise themselves as benign clients and disseminate polluted knowledge to achieve malicious goals like promoting an item's exposure rate. Although research on such poisoning attacks provides valuable insights into finding security loopholes and corresponding countermeasures, existing attacks mostly focus on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Chao's Population Size Estimator&#30340;&#20572;&#27490;&#20934;&#21017;&#65292;&#29992;&#20110;&#25351;&#23548;&#25216;&#26415;&#36741;&#21161;&#23457;&#38405;&#36807;&#31243;&#65292;&#20197;&#26368;&#23567;&#21270;&#38169;&#36807;&#30340;&#30456;&#20851;&#25991;&#26723;&#21644;&#38405;&#35835;&#30340;&#19981;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01176</link><description>&lt;p&gt;
&#20351;&#29992;Chao's&#20272;&#35745;&#22120;&#20316;&#20026;&#25216;&#26415;&#36741;&#21161;&#23457;&#38405;&#30340;&#20572;&#27490;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Using Chao's Estimator as a Stopping Criterion for Technology-Assisted Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Chao's Population Size Estimator&#30340;&#20572;&#27490;&#20934;&#21017;&#65292;&#29992;&#20110;&#25351;&#23548;&#25216;&#26415;&#36741;&#21161;&#23457;&#38405;&#36807;&#31243;&#65292;&#20197;&#26368;&#23567;&#21270;&#38169;&#36807;&#30340;&#30456;&#20851;&#25991;&#26723;&#21644;&#38405;&#35835;&#30340;&#19981;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01176v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#20869;&#23481; &#25216;&#26415;&#36741;&#21161;&#23457;&#38405;&#65288;TAR&#65289;&#26088;&#22312;&#20943;&#23569;&#29992;&#20110;&#31579;&#36873;&#36807;&#31243;&#65288;&#22914;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#25688;&#35201;&#31579;&#36873;&#65289;&#25152;&#38656;&#30340;&#20154;&#21147;&#24037;&#20316;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#23457;&#38405;&#21592;&#26631;&#35760;&#25991;&#26723;&#20026;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#65292;&#32780;&#31995;&#32479;&#26681;&#25454;&#23457;&#38405;&#21592;&#20808;&#21069;&#30340;&#20915;&#23450;&#36880;&#27493;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#27599;&#27425;&#27169;&#22411;&#26356;&#26032;&#21518;&#65292;&#31995;&#32479;&#25552;&#20986;&#26032;&#30340;&#25991;&#26723;&#65292;&#35748;&#20026;&#36825;&#20123;&#25991;&#26723;&#26159;&#30456;&#20851;&#30340;&#65292;&#20197;&#20248;&#20808;&#32771;&#34385;&#30456;&#20851;&#25991;&#26723;&#32780;&#19981;&#26159;&#19981;&#30456;&#20851;&#25991;&#26723;&#12290;&#38656;&#35201;&#19968;&#20010;&#20572;&#27490;&#20934;&#21017;&#26469;&#25351;&#23548;&#29992;&#25143;&#20572;&#27490;&#23457;&#38405;&#36807;&#31243;&#65292;&#20197;&#26368;&#23567;&#21270;&#38169;&#36807;&#30340;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#21644;&#38405;&#35835;&#30340;&#19981;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21644;&#19968;&#20010;&#22522;&#20110;Chao&#30340;&#31181;&#32676;&#22823;&#23567;&#20272;&#35745;&#22120;&#30340;&#20572;&#27490;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#30456;&#20851;&#25991;&#26723;&#30340;&#27969;&#34892;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#20934;&#21017;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01176v1 Announce Type: new  Abstract: Technology-Assisted Review (TAR) aims to reduce the human effort required for screening processes such as abstract screening for systematic literature reviews. Human reviewers label documents as relevant or irrelevant during this process, while the system incrementally updates a prediction model based on the reviewers' previous decisions. After each model update, the system proposes new documents it deems relevant, to prioritize relevant documentsover irrelevant ones. A stopping criterion is necessary to guide users in stopping the review process to minimize the number of missed relevant documents and the number of read irrelevant documents. In this paper, we propose and evaluate a new ensemble-based Active Learning strategy and a stopping criterion based on Chao's Population Size Estimator that estimates the prevalence of relevant documents in the dataset. Our simulation study demonstrates that this criterion performs well on several da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#21457;&#29616;&#20854;&#22312;&#38899;&#20048;&#27969;&#27966;&#35782;&#21035;&#20219;&#21153;&#20013;&#24182;&#19981;&#27604;&#20256;&#32479;&#30340;Mel&#39057;&#35889;&#22270;&#26356;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2404.01058</link><description>&lt;p&gt;
&#38899;&#20048;&#27969;&#27966;&#35782;&#21035;&#20013;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Novel Audio Representation for Music Genre Identification in MIR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#21457;&#29616;&#20854;&#22312;&#38899;&#20048;&#27969;&#27966;&#35782;&#21035;&#20219;&#21153;&#20013;&#24182;&#19981;&#27604;&#20256;&#32479;&#30340;Mel&#39057;&#35889;&#22270;&#26356;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26368;&#24120;&#35265;&#30340;&#38899;&#39057;&#34920;&#31034;&#24418;&#24335;&#26159;&#22522;&#20110;&#26102;&#39057;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;Mel&#39057;&#35889;&#22270;&#12290;&#20026;&#20102;&#35782;&#21035;&#38899;&#20048;&#27969;&#27966;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#29992;&#20110;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;MIR&#19979;&#28216;&#20219;&#21153;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#29992;&#28145;&#24230;&#21521;&#37327;&#37327;&#21270;&#31163;&#25955;&#22320;&#32534;&#30721;&#38899;&#20048;&#65307;&#20026;&#21019;&#26032;&#30340;&#29983;&#25104;&#24335;&#38899;&#20048;&#27169;&#22411;Jukebox&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20046;&#31561;&#21516;&#20110;&#19994;&#30028;&#39046;&#20808;&#27700;&#24179;(SOTA)&#21644;&#20960;&#20046;&#30456;&#21516;&#30340;&#21464;&#21387;&#22120;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;Jukebox&#30340;&#38899;&#39057;&#34920;&#31034;&#19982;Mel&#39057;&#35889;&#22270;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#65292;&#33267;&#23569;&#24403;&#21464;&#21387;&#22120;&#20351;&#29992;&#38750;&#24120;&#36866;&#24230;&#30340;&#25968;&#25454;&#38598;&#65288;20k&#39318;&#38899;&#36712;&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;Jukebox&#30340;&#38899;&#39057;&#34920;&#31034;&#24182;&#19981;&#20248;&#20110;Mel&#39057;&#35889;&#22270;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;Jukebox&#30340;&#38899;&#39057;&#34920;&#31034;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01058v1 Announce Type: cross  Abstract: For Music Information Retrieval downstream tasks, the most common audio representation is time-frequency-based, such as Mel spectrograms. In order to identify musical genres, this study explores the possibilities of a new form of audio representation one of the most usual MIR downstream tasks. Therefore, to discretely encoding music using deep vector quantization; a novel audio representation was created for the innovative generative music model i.e. Jukebox. The effectiveness of Jukebox's audio representation is compared to Mel spectrograms using a dataset that is almost equivalent to State-of-the-Art (SOTA) and an almost same transformer design. The results of this study imply that, at least when the transformers are pretrained using a very modest dataset of 20k tracks, Jukebox's audio representation is not superior to Mel spectrograms. This could be explained by the fact that Jukebox's audio representation does not sufficiently take
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340; RAG &#26041;&#27861;&#23545;&#26816;&#32034;&#31934;&#24230;&#21644;&#31572;&#26696;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20551;&#35774;&#25991;&#26723;&#23884;&#20837;&#65288;HyDE&#65289;&#21644;LLM &#37325;&#26032;&#25490;&#24207;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#65292;&#32780;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#21644; Cohere &#37325;&#26032;&#25490;&#24207;&#21017;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01037</link><description>&lt;p&gt;
ARAGOG: &#39640;&#32423; RAG &#36755;&#20986;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
ARAGOG: Advanced RAG Output Grading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340; RAG &#26041;&#27861;&#23545;&#26816;&#32034;&#31934;&#24230;&#21644;&#31572;&#26696;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20551;&#35774;&#25991;&#26723;&#23884;&#20837;&#65288;HyDE&#65289;&#21644;LLM &#37325;&#26032;&#25490;&#24207;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#65292;&#32780;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#21644; Cohere &#37325;&#26032;&#25490;&#24207;&#21017;&#27809;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01037v1 &#36890;&#30693;&#31867;&#22411;: &#26032; &#25552;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23545;&#20110;&#23558;&#22806;&#37096;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20986;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#20851;RAG&#30340;&#25991;&#29486;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#19982;&#20854;&#21069;&#36523;&#36827;&#34892;&#31995;&#32479;&#24615;&#23457;&#26597;&#21644;&#27604;&#36739;&#65292;&#23384;&#22312;&#22823;&#37327;&#23454;&#39564;&#24615;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#24320;&#22987;&#30528;&#25163;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;RAG&#26041;&#27861;&#23545;&#26816;&#32034;&#31934;&#24230;&#21644;&#31572;&#26696;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20551;&#35774;&#25991;&#26723;&#23884;&#20837;&#65288;HyDE&#65289;&#21644;LLM &#37325;&#26032;&#25490;&#24207;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#21644;Cohere &#37325;&#26032;&#25490;&#24207;&#24182;&#26410;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#22810;&#26597;&#35810;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#21477;&#31383;&#26816;&#32034;&#22312;&#26816;&#32034;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#26368;&#20026;&#26377;&#25928;&#65292;&#23613;&#31649;&#23427;&#22312;&#31572;&#26696;&#30456;&#20284;&#24615;&#19978;&#30340;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;&#35813;&#30740;&#31350;&#30830;&#35748;&#20102;&#25991;&#26723;&#25688;&#35201;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01037v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary I
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#23545;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20998;&#26512;&#33021;&#21147;&#31561;&#25216;&#33021;&#65292;&#20294;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#38480;&#21046;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#37096;&#20998;&#23398;&#31185;&#35780;&#20272;&#20013;&#36825;&#20123;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01036</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26102;&#20195;&#30340;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Higher education assessment practice in the era of generative AI tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#23545;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20998;&#26512;&#33021;&#21147;&#31561;&#25216;&#33021;&#65292;&#20294;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#38480;&#21046;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#37096;&#20998;&#23398;&#31185;&#35780;&#20272;&#20013;&#36825;&#20123;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#65288;HE&#65289;&#37096;&#38376;&#23545;&#27599;&#20010;&#22269;&#23478;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#37117;&#26377;&#30410;&#22788;&#65292;&#20294;&#23427;&#20204;&#30340;&#36129;&#29486;&#21463;&#21040;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;GenAI&#24037;&#20855;&#23545;&#35780;&#20272;&#21644;&#25945;&#23398;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#24182;&#38543;&#21518;&#35752;&#35770;&#20102;&#28508;&#22312;&#24433;&#21709;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#31569;&#31649;&#29702;&#23398;&#31185;&#30340;&#19977;&#31181;&#35780;&#20272;&#24037;&#20855;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#32467;&#26524;&#26174;&#31034;GenAI&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12289;&#20998;&#26512;&#33021;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#26576;&#20123;&#23398;&#31185;&#35780;&#20272;&#30340;&#35774;&#35745;&#25581;&#31034;&#20102;GenAI&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#25945;&#23398;&#19982;&#23398;&#20064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01036v1 Announce Type: cross  Abstract: The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>EEG-SVRec&#26159;&#31532;&#19968;&#20010;&#22312;&#30701;&#35270;&#39057;&#25512;&#33616;&#20013;&#24102;&#26377;&#29992;&#25143;&#22810;&#32500;&#24773;&#24863;&#21442;&#19982;&#26631;&#31614;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#29992;&#25143;&#24773;&#24863;&#32463;&#39564;&#21644;&#35748;&#30693;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2404.01008</link><description>&lt;p&gt;
EEG-SVRec&#65306;&#19968;&#20221;&#24102;&#26377;&#29992;&#25143;&#22810;&#32500;&#24773;&#24863;&#21442;&#19982;&#26631;&#31614;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#22312;&#30701;&#35270;&#39057;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01008
&lt;/p&gt;
&lt;p&gt;
EEG-SVRec&#26159;&#31532;&#19968;&#20010;&#22312;&#30701;&#35270;&#39057;&#25512;&#33616;&#20013;&#24102;&#26377;&#29992;&#25143;&#22810;&#32500;&#24773;&#24863;&#21442;&#19982;&#26631;&#31614;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#29992;&#25143;&#24773;&#24863;&#32463;&#39564;&#21644;&#35748;&#30693;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30701;&#35270;&#39057;&#24179;&#21488;&#24191;&#21463;&#27426;&#36814;&#65292;&#20351;&#24471;&#35270;&#39057;&#25512;&#33616;&#30340;&#36136;&#37327;&#23545;&#20110;&#30041;&#20303;&#29992;&#25143;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#20110;&#34892;&#20026;&#25968;&#25454;&#65292;&#20294;&#22312;&#25512;&#26029;&#29992;&#25143;&#21916;&#22909;&#26041;&#38754;&#38754;&#20020;&#38480;&#21046;&#65292;&#22240;&#20026;&#23384;&#22312;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#20197;&#21450;&#26469;&#33258;&#24847;&#22806;&#20132;&#20114;&#25110;&#20010;&#20154;&#20064;&#24815;&#30340;&#24178;&#25200;&#22122;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20379;&#23545;&#29992;&#25143;&#24773;&#24863;&#20307;&#39564;&#21644;&#35748;&#30693;&#27963;&#21160;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EEG-SVRec&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#29992;&#25143;&#22810;&#32500;&#24773;&#24863;&#21442;&#19982;&#26631;&#31614;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#22312;&#30701;&#35270;&#39057;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;30&#21517;&#21442;&#19982;&#32773;&#65292;&#25910;&#38598;&#20102;3,657&#27425;&#20114;&#21160;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#29992;&#25143;&#21916;&#22909;&#21644;&#35748;&#30693;&#27963;&#21160;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#25216;&#26415;&#21644;&#23454;&#26102;&#12289;&#20302;&#25104;&#26412;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#29992;&#25143;&#24773;&#24863;&#20307;&#39564;&#26356;&#35814;&#32454;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01008v1 Announce Type: new  Abstract: In recent years, short video platforms have gained widespread popularity, making the quality of video recommendations crucial for retaining users. Existing recommendation systems primarily rely on behavioral data, which faces limitations when inferring user preferences due to issues such as data sparsity and noise from accidental interactions or personal habits. To address these challenges and provide a more comprehensive understanding of user affective experience and cognitive activity, we propose EEG-SVRec, the first EEG dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation. The study involves 30 participants and collects 3,657 interactions, offering a rich dataset that can be used for a deeper exploration of user preference and cognitive activity. By incorporating selfassessment techniques and real-time, low-cost EEG signals, we offer a more detailed understanding user affective experiences (vale
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#36890;&#36947;&#38646;&#21806;&#20013;&#30340;&#36328;&#36890;&#36947;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20197;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#25972;&#21512;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00972</link><description>&lt;p&gt;
&#36328;&#36890;&#36947;&#25512;&#33616;&#22312;&#22810;&#36890;&#36947;&#38646;&#21806;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-channel Recommendation for Multi-channel Retail
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#36890;&#36947;&#38646;&#21806;&#20013;&#30340;&#36328;&#36890;&#36947;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20197;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#25972;&#21512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20256;&#32479;&#38646;&#21806;&#21830;&#27491;&#22312;&#23558;&#19994;&#21153;&#25299;&#23637;&#21040;&#22312;&#32447;&#39046;&#22495;&#65292;&#36716;&#21464;&#20026;&#22810;&#36890;&#36947;&#38646;&#21806;&#21830;&#12290;&#36825;&#31181;&#36716;&#21464;&#24378;&#35843;&#20102;&#36328;&#36890;&#36947;&#25512;&#33616;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#26088;&#22312;&#22686;&#24378;&#32447;&#19979;&#21644;&#32447;&#19978;&#28192;&#36947;&#30340;&#25910;&#20837;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#36328;&#36890;&#36947;&#38646;&#21806;&#25512;&#33616;&#20013;&#30340;&#21407;&#21019;&#24037;&#20316;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#36890;&#36947;&#38646;&#21806;&#21830;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25972;&#21512;&#29992;&#25143;&#20559;&#22909;&#22312;&#20004;&#20010;&#36890;&#36947;&#20043;&#38388;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00972v1 Announce Type: new  Abstract: An increasing number of brick-and-mortar retailers are expanding their channels to the online domain, transforming them into multi-channel retailers. This transition emphasizes the need for cross-channel recommender systems, aiming to enhance revenue across both offline and online channels. Given that each retail channel represents a separate domain with a unique context, this can be regarded as a cross-domain recommendation (CDR). However, the existing studies on CDR did not address the scenarios where both users and items partially overlap across multi-retail channels which we define as "cross-channel retail recommendation (CCRR)". This paper introduces our original work on CCRR using real-world datasets from a multi-channel retail store. Specifically, (1) we present significant challenges in integrating user preferences across both channels. (2) Accordingly, we propose a novel model for CCRR using a channel-wise attention mechanism to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;TQM&#22242;&#38431;&#22312;COLIEE2024&#20013;&#25506;&#32034;&#30340;&#35789;&#27719;&#21305;&#37197;&#12289;&#35821;&#20041;&#26816;&#32034;&#27169;&#22411;&#21644;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#65292;&#37325;&#28857;&#22312;&#20110;&#25552;&#21319;&#26696;&#20363;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#22312;COLIEE2024&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#32489;&#65292;&#24182;&#26377;&#26395;&#20026;&#27861;&#24459;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.00947</link><description>&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#30340;&#27861;&#24459;&#26816;&#32034;&#65292;&#28145;&#20837;&#29702;&#35299;&#26696;&#20363;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards an In-Depth Comprehension of Case Relevance for Better Legal Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;TQM&#22242;&#38431;&#22312;COLIEE2024&#20013;&#25506;&#32034;&#30340;&#35789;&#27719;&#21305;&#37197;&#12289;&#35821;&#20041;&#26816;&#32034;&#27169;&#22411;&#21644;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#65292;&#37325;&#28857;&#22312;&#20110;&#25552;&#21319;&#26696;&#20363;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#22312;COLIEE2024&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#32489;&#65292;&#24182;&#26377;&#26395;&#20026;&#27861;&#24459;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#26816;&#32034;&#25216;&#26415;&#22312;&#32500;&#25252;&#21496;&#27861;&#20844;&#24179;&#21644;&#24179;&#31561;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20316;&#20026;&#19968;&#39033;&#22269;&#38469;&#30693;&#21517;&#30340;&#24180;&#24230;&#27604;&#36187;&#65292;COLIEE&#26088;&#22312;&#25512;&#21160;&#27861;&#24459;&#25991;&#26412;&#26368;&#20808;&#36827;&#26816;&#32034;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;TQM&#22242;&#38431;&#22312;COLIEE2024&#20013;&#37319;&#29992;&#30340;&#26041;&#27861;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#35789;&#27719;&#21305;&#37197;&#21644;&#35821;&#20041;&#26816;&#32034;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#25552;&#21319;&#26696;&#20363;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#38598;&#25104;&#21508;&#31181;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;&#21551;&#21457;&#24335;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#20197;&#20943;&#23569;&#26080;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COLIEE2024&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22312;&#20219;&#21153;1&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#65292;&#22312;&#20219;&#21153;3&#20013;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#20026;&#27861;&#24459;&#26816;&#32034;&#30340;&#36827;&#27493;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00947v1 Announce Type: new  Abstract: Legal retrieval techniques play an important role in preserving the fairness and equality of the judicial system. As an annually well-known international competition, COLIEE aims to advance the development of state-of-the-art retrieval models for legal texts. This paper elaborates on the methodology employed by the TQM team in COLIEE2024.Specifically, we explored various lexical matching and semantic retrieval models, with a focus on enhancing the understanding of case relevance. Additionally, we endeavor to integrate various features using the learning-to-rank technique. Furthermore, fine heuristic pre-processing and post-processing methods have been proposed to mitigate irrelevant information. Consequently, our methodology achieved remarkable performance in COLIEE2024, securing first place in Task 1 and third place in Task 3. We anticipate that our proposed approach can contribute valuable insights to the advancement of legal retrieval
&lt;/p&gt;</description></item><item><title>&#23558;LLMOps&#38598;&#25104;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#25512;&#21160;&#29983;&#25104;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20026;&#26410;&#26469;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.00903</link><description>&lt;p&gt;
&#36890;&#36807;LLMOps&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26368;&#22823;&#21270;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00903
&lt;/p&gt;
&lt;p&gt;
&#23558;LLMOps&#38598;&#25104;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#25512;&#21160;&#29983;&#25104;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20026;&#26410;&#26469;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLMOps&#38598;&#25104;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26631;&#24535;&#30528;&#22312;&#31649;&#29702;LLM&#39537;&#21160;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#19968;&#21019;&#26032;&#20026;&#20225;&#19994;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#22242;&#38431;&#22312;&#20248;&#20808;&#32771;&#34385;&#25968;&#25454;&#23433;&#20840;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#22788;&#29702;&#24037;&#31243;&#25216;&#26415;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;LLMOps&#65292;&#20225;&#19994;&#21487;&#20197;&#25552;&#21319;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#25512;&#21160;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#23613;&#31649;&#23384;&#22312;&#20262;&#29702;&#32771;&#37327;&#65292;&#20294;LLMOps&#24050;&#20934;&#22791;&#22909;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#25215;&#35834;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#26356;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#65292;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#24182;&#22609;&#36896;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00903v1 Announce Type: cross  Abstract: The integration of LLMOps into personalized recommendation systems marks a significant advancement in managing LLM-driven applications. This innovation presents both opportunities and challenges for enterprises, requiring specialized teams to navigate the complexity of engineering technology while prioritizing data security and model interpretability. By leveraging LLMOps, enterprises can enhance the efficiency and reliability of large-scale machine learning models, driving personalized recommendations aligned with user preferences. Despite ethical considerations, LLMOps is poised for widespread adoption, promising more efficient and secure machine learning services that elevate user experience and shape the future of personalized recommendation systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00712</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#32508;&#36848;&#65306;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Survey of Computerized Adaptive Testing: A Machine Learning Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#37327;&#36523;&#23450;&#21046;&#30340;&#35780;&#20272;&#32771;&#29983;&#29087;&#32451;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#20182;&#20204;&#30340;&#34920;&#29616;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#12290;CAT&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#12289;&#20307;&#32946;&#21644;&#31038;&#20250;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#27979;&#35797;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#35268;&#27169;&#27979;&#35797;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;CAT&#24050;&#32463;&#34701;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#37325;&#28857;&#30340;CAT&#32508;&#36848;&#65292;&#20174;&#26032;&#30340;&#35282;&#24230;&#35299;&#35835;&#36825;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;CAT&#36866;&#24212;&#24615;&#26680;&#24515;&#30340;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20854;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;CAT&#20013;&#30340;&#27979;&#35797;&#25511;&#21046;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#20248;&#21270;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#24773;&#20917;&#30340;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00712v1 Announce Type: cross  Abstract: Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of curre
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;UniLLMRec&#23558;&#22810;&#38454;&#27573;&#20219;&#21153;&#25972;&#21512;&#20026;&#31471;&#21040;&#31471;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#22823;&#35268;&#27169;&#29289;&#21697;&#38598;&#21512;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.00702</link><description>&lt;p&gt;
&#21388;&#20518;&#20102;&#25554;&#20214;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#31471;&#21040;&#31471;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tired of Plugins? Large Language Models Can Be End-To-End Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00702
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;UniLLMRec&#23558;&#22810;&#38454;&#27573;&#20219;&#21153;&#25972;&#21512;&#20026;&#31471;&#21040;&#31471;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#22823;&#35268;&#27169;&#29289;&#21697;&#38598;&#21512;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#39044;&#27979;&#29992;&#25143;&#20852;&#36259;&#12290;&#23427;&#20204;&#20027;&#35201;&#35774;&#35745;&#20026;&#39034;&#24207;&#27969;&#27700;&#32447;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#19981;&#21516;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#22495;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20351;&#19968;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#21270;&#25512;&#33616;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#32431;&#31929;&#21033;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#27969;&#27700;&#32447;&#30340;&#21333;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#30528;&#20197;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#21521;LLM&#21576;&#29616;&#22823;&#35268;&#27169;&#29289;&#21697;&#38598;&#21512;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31471;&#21040;&#31471;&#25512;&#33616;&#26694;&#26550;&#65306;UniLLMRec&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UniLLMRec&#36890;&#36807;&#25512;&#33616;&#38142;&#38598;&#25104;&#22810;&#38454;&#27573;&#20219;&#21153;&#65288;&#20363;&#22914;&#21484;&#22238;&#12289;&#25490;&#24207;&#12289;&#37325;&#26032;&#25490;&#24207;&#65289;&#12290;&#20026;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#29289;&#21697;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00702v1 Announce Type: new  Abstract: Recommender systems aim to predict user interest based on historical behavioral data. They are mainly designed in sequential pipelines, requiring lots of data to train different sub-systems, and are hard to scale to new domains. Recently, Large Language Models (LLMs) have demonstrated remarkable generalized capabilities, enabling a singular model to tackle diverse recommendation tasks across various scenarios. Nonetheless, existing LLM-based recommendation systems utilize LLM purely for a single task of the recommendation pipeline. Besides, these systems face challenges in presenting large-scale item sets to LLMs in natural language format, due to the constraint of input length. To address these challenges, we introduce an LLM-based end-to-end recommendation framework: UniLLMRec. Specifically, UniLLMRec integrates multi-stage tasks (e.g. recall, ranking, re-ranking) via chain-of-recommendations. To deal with large-scale items, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20849;&#20139;&#30456;&#21516;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#25991;&#26723;&#19982;&#26597;&#35810;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00684</link><description>&lt;p&gt;
&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20013;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generative Retrieval as Multi-Vector Dense Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20849;&#20139;&#30456;&#21516;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#25991;&#26723;&#19982;&#26597;&#35810;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00684v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#29983;&#25104;&#24335;&#26816;&#32034;&#20197;&#31471;&#23545;&#31471;&#26041;&#24335;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26550;&#26500;&#20026;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#12290;&#29983;&#25104;&#24335;&#26816;&#32034;&#19982;&#20854;&#20182;&#26816;&#32034;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#20869;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24102;&#26377;&#21407;&#23376;&#26631;&#35782;&#31526;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#31561;&#25928;&#20110;&#21333;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#12290;&#22240;&#27492;&#65292;&#24403;&#20351;&#29992;&#20998;&#23618;&#35821;&#20041;&#26631;&#35782;&#31526;&#26102;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#23494;&#38598;&#26816;&#32034;&#20013;&#26641;&#32034;&#24341;&#20869;&#30340;&#20998;&#23618;&#25628;&#32034;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#19987;&#27880;&#20110;&#26816;&#32034;&#38454;&#27573;&#65292;&#26410;&#32771;&#34385;&#29983;&#25104;&#24335;&#26816;&#32034;&#35299;&#30721;&#22120;&#20869;&#30340;&#28145;&#23618;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00684v1 Announce Type: cross  Abstract: Generative retrieval generates identifiers of relevant documents in an end-to-end manner using a sequence-to-sequence architecture for a given query. The relation between generative retrieval and other retrieval methods, especially those based on matching within dense retrieval models, is not yet fully comprehended. Prior work has demonstrated that generative retrieval with atomic identifiers is equivalent to single-vector dense retrieval. Accordingly, generative retrieval exhibits behavior analogous to hierarchical search within a tree index in dense retrieval when using hierarchical semantic identifiers. However, prior work focuses solely on the retrieval stage without considering the deep interactions within the decoder of generative retrieval.   In this paper, we fill this gap by demonstrating that generative retrieval and multi-vector dense retrieval share the same framework for measuring the relevance to a query of a document. Sp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#25506;&#32034;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#36712;&#36857;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.00621</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#29992;&#20110;&#25512;&#33616;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00621
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#25506;&#32034;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#36712;&#36857;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#20316;&#20026;&#29992;&#25143;&#21457;&#29616;&#26681;&#25454;&#20854;&#20852;&#36259;&#23450;&#21046;&#30340;&#20449;&#24687;&#25110;&#29289;&#21697;&#30340;&#26222;&#36941;&#28192;&#36947;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#21807;&#19968;ID&#21644;&#20998;&#31867;&#29305;&#24449;&#36827;&#34892;&#29992;&#25143;-&#39033;&#30446;&#21305;&#37197;&#65292;&#21487;&#33021;&#24573;&#35270;&#36328;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#30340;&#21407;&#22987;&#39033;&#30446;&#20869;&#23481;&#30340;&#24494;&#22937;&#26412;&#36136;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20302;&#21033;&#29992;&#29575;&#23545;&#25512;&#33616;&#31995;&#32479;&#26500;&#25104;&#20102;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#38395;&#12289;&#38899;&#20048;&#21644;&#30701;&#35270;&#39057;&#24179;&#21488;&#31561;&#22810;&#23186;&#20307;&#26381;&#21153;&#20013;&#12290;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#24320;&#21457;&#20869;&#23481;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#20840;&#38754;&#25506;&#35752;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00621v1 Announce Type: new  Abstract: Personalized recommendation serves as a ubiquitous channel for users to discover information or items tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in pretrained multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications to recommender systems. Furthermore, we discuss open chal
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#35009;&#20915;&#30340;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20107;&#23454;&#21644;&#35770;&#28857;&#65292;&#23637;&#31034;&#20102;&#21028;&#20363;&#23454;&#36341;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.00596</link><description>&lt;p&gt;
ECtHR-PCR&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#30340;&#21028;&#20363;&#29702;&#35299;&#21644;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#35009;&#20915;&#30340;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#22320;&#20998;&#31163;&#20107;&#23454;&#21644;&#35770;&#28857;&#65292;&#23637;&#31034;&#20102;&#21028;&#20363;&#23454;&#36341;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26222;&#36890;&#27861;&#31649;&#36758;&#21306;&#65292;&#27861;&#24459;&#23454;&#36341;&#32773;&#20381;&#36182;&#20110;&#21028;&#20363;&#26469;&#26500;&#24314;&#35770;&#28857;&#65292;&#31526;&#21512;\emph{stare decisis}&#21407;&#21017;&#12290;&#38543;&#30528;&#22810;&#24180;&#26469;&#26696;&#20214;&#25968;&#37327;&#30340;&#22686;&#38271;&#65292;&#20808;&#21069;&#26696;&#20363;&#26816;&#32034;&#65288;PCR&#65289;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#29616;&#26377;&#30340;PCR&#25968;&#25454;&#38598;&#38500;&#20102;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#35268;&#27169;&#22806;&#65292;&#24182;&#26410;&#27169;&#25311;&#29616;&#23454;&#24773;&#22659;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#26597;&#35810;&#20351;&#29992;&#23436;&#25972;&#26696;&#20214;&#25991;&#20214;&#65292;&#21482;&#25513;&#30422;&#20808;&#21069;&#26696;&#20363;&#30340;&#24341;&#29992;&#12290;&#22240;&#27492;&#65292;&#26597;&#35810;&#23601;&#20250;&#26292;&#38706;&#20110;&#23578;&#26410;&#24418;&#25104;&#23545;&#26410;&#20915;&#26696;&#36827;&#34892;&#35770;&#35777;&#26102;&#25152;&#38656;&#30340;&#27861;&#24459;&#25512;&#29702;&#65292;&#20197;&#21450;&#21487;&#33021;&#20250;&#24178;&#25200;&#26696;&#20214;&#20107;&#23454;&#21644;&#27861;&#24459;&#21407;&#21017;&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#24341;&#29992;&#25513;&#30721;&#25152;&#30041;&#19979;&#30340;&#34394;&#20551;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECtHR&#65289;&#35009;&#20915;&#30340;PCR&#25968;&#25454;&#38598;&#65292;&#26126;&#30830;&#21306;&#20998;&#20107;&#23454;&#21644;&#35770;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21028;&#20363;&#23454;&#36341;&#65292;&#24110;&#21161;&#25105;&#20204;&#21457;&#23637;&#36825;&#19968;PCR&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00596v1 Announce Type: new  Abstract: In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of \emph{stare decisis}. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#21644;&#38646;-shot&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#22788;&#29702;&#20998;&#24067;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00595</link><description>&lt;p&gt;
&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;
&lt;/p&gt;
&lt;p&gt;
Query-driven Relevant Paragraph Extraction from Legal Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#21644;&#38646;-shot&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#22788;&#29702;&#20998;&#24067;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#33510;&#20110;&#22312;&#28459;&#38271;&#30340;&#27861;&#24459;&#35009;&#20915;&#20013;&#23547;&#25214;&#30452;&#25509;&#35299;&#31572;&#20182;&#20204;&#38382;&#39064;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20851;&#27880;&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#22522;&#20110;&#26597;&#35810;&#25552;&#21462;&#30456;&#20851;&#27573;&#33853;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECtHR&#65289;&#30340;&#26696;&#20363;&#27861;&#25351;&#21335;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20197;&#38646;-shot&#26041;&#24335;&#35780;&#20272;&#24403;&#21069;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20351;&#29992;&#21508;&#31181;&#27169;&#22411;&#30340;&#24494;&#35843;&#22522;&#20934;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#24494;&#35843;&#21644;&#38646;-shot&#24615;&#33021;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24378;&#35843;&#20102;&#22788;&#29702;&#27861;&#24459;&#39046;&#22495;&#20013;&#20998;&#24067;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#27861;&#24459;&#39044;&#35757;&#32451;&#22312;&#35821;&#26009;&#24211;&#20391;&#22788;&#29702;&#20998;&#24067;&#21464;&#21270;&#65292;&#20294;&#22312;&#26597;&#35810;&#20391;&#20998;&#24067;&#21464;&#21270;&#65288;&#30475;&#19981;&#35265;&#30340;&#27861;&#24459;&#26597;&#35810;&#65289;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00595v1 Announce Type: new  Abstract: Legal professionals often grapple with navigating lengthy legal judgements to pinpoint information that directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a zero-shot way and also establish fine-tuning benchmarks using various models. The results highlight the significant gap between fine-tuned and zero-shot performance, emphasizing the challenge of handling distribution shift in the legal domain. We notice that the legal pre-training handles distribution shift on the corpus side but still struggles on query side distribution shift, with unseen legal queries. We also explore various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their practicality within the cont
&lt;/p&gt;</description></item><item><title>CuSINeS&#36890;&#36807;&#35838;&#31243;&#39537;&#21160;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#32467;&#26500;&#21270;&#27861;&#35268;&#20449;&#24687;&#21644;&#21160;&#24577;&#35821;&#20041;&#38590;&#24230;&#35780;&#20272;&#19977;&#26041;&#38754;&#36129;&#29486;&#65292;&#25552;&#21319;&#20102;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00590</link><description>&lt;p&gt;
CuSINeS&#65306;&#22522;&#20110;&#35838;&#31243;&#39537;&#21160;&#30340;&#32467;&#26500;&#35825;&#23548;&#36127;&#37319;&#26679;&#29992;&#20110;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CuSINeS: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00590
&lt;/p&gt;
&lt;p&gt;
CuSINeS&#36890;&#36807;&#35838;&#31243;&#39537;&#21160;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#32467;&#26500;&#21270;&#27861;&#35268;&#20449;&#24687;&#21644;&#21160;&#24577;&#35821;&#20041;&#38590;&#24230;&#35780;&#20272;&#19977;&#26041;&#38754;&#36129;&#29486;&#65292;&#25552;&#21319;&#20102;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuSINeS&#65292;&#36825;&#26159;&#19968;&#31181;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27861;&#23450;&#25991;&#31456;&#26816;&#32034;&#65288;SAR&#65289;&#30340;&#24615;&#33021;&#12290;CuSINeS&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24341;&#23548;&#27169;&#22411;&#39318;&#20808;&#20851;&#27880;&#26356;&#23481;&#26131;&#30340;&#36127;&#26679;&#26412;&#65292;&#36880;&#28176;&#22788;&#29702;&#26356;&#38590;&#30340;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#23427;&#21033;&#29992;&#20174;&#27861;&#35268;&#32467;&#26500;&#32452;&#32455;&#20013;&#33719;&#24471;&#30340;&#20998;&#23618;&#21644;&#24207;&#21015;&#20449;&#24687;&#26469;&#35780;&#20272;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;&#26368;&#21518;&#65292;&#23427;&#24341;&#20837;&#20102;&#21160;&#24577;&#35821;&#20041;&#38590;&#24230;&#35780;&#20272;&#65292;&#21033;&#29992;&#27491;&#22312;&#35757;&#32451;&#30340;&#27169;&#22411;&#26412;&#36523;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#38745;&#24577;&#26041;&#27861;&#22914;BM25&#65292;&#20351;&#36127;&#26679;&#26412;&#36866;&#24212;&#27169;&#22411;&#19981;&#26029;&#36827;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;SAR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;CuSINeS&#22312;&#22235;&#31181;&#19981;&#21516;&#22522;&#32447;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00590v1 Announce Type: cross  Abstract: In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the model's evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22797;&#26434;&#25968;&#25454;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00579</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#65288;Gen-RecSys&#65289;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00579
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22797;&#26434;&#25968;&#25454;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#24120;&#20351;&#29992;&#29992;&#25143;-&#39033;&#30446;&#35780;&#20998;&#21382;&#21490;&#20316;&#20026;&#20027;&#35201;&#25968;&#25454;&#26469;&#28304;&#65292;&#21327;&#21516;&#36807;&#28388;&#26159;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#20855;&#22791;&#20102;&#24314;&#27169;&#21644;&#37319;&#26679;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#21487;&#20197;&#28085;&#30422;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#65292;&#36824;&#21487;&#20197;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20026;&#26032;&#39062;&#30340;&#25512;&#33616;&#20219;&#21153;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31687;&#20840;&#38754;&#30340;&#12289;&#22810;&#23398;&#31185;&#30340;&#35843;&#30740;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;Gen-RecSys&#65289;&#22312;RS&#20013;&#30340;&#20851;&#38190;&#36827;&#23637;&#65292;&#21253;&#25324;&#65306;&#22522;&#20110;&#20132;&#20114;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#27010;&#36848;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#25512;&#33616;&#12289;&#26816;&#32034;&#21644;&#23545;&#35805;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;&#65307;&#20197;&#21450;&#29992;&#20110;&#22788;&#29702;&#21644;&#29983;&#25104;RS&#20013;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#30340;&#25972;&#20307;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#24378;&#35843;&#35780;&#20272;&#25152;&#38656;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00579v1 Announce Type: cross  Abstract: Traditional recommender systems (RS) have used user-item rating histories as their primary data source, with collaborative filtering being one of the principal methods. However, generative models have recently developed abilities to model and sample from complex data distributions, including not only user-item interaction histories but also text, images, and videos - unlocking this rich data for novel recommendation tasks. Through this comprehensive and multi-disciplinary survey, we aim to connect the key advancements in RS using Generative Models (Gen-RecSys), encompassing: a foundational overview of interaction-driven generative models; the application of large language models (LLM) for generative recommendation, retrieval, and conversational recommendation; and the integration of multimodal models for processing and generating image and video content in RS. Our holistic perspective allows us to highlight necessary paradigms for eval
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23884;&#20837;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00458</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20992;&#20999;&#65306;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#29992;&#20110;&#23884;&#20837;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for Embedding Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00458
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26368;&#26377;&#25928;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#22823;&#37327;&#22686;&#21152;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00458v1 Announce Type: new  Abstract: This position paper proposes a systematic approach towards developing a framework to help select the most effective embedding models for natural language processing (NLP) tasks, addressing the challenge posed by the proliferation of both proprietary and open-source encoder models.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25968;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;GPT-4&#22312;Math Stack Exchange&#19978;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2404.00344</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#25484;&#25569;&#25968;&#23398;&#21527;&#65311;&#22312;&#25968;&#23398;&#22534;&#26632;&#20132;&#25442;&#19978;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00344
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25968;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;GPT-4&#22312;Math Stack Exchange&#19978;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24322;&#24120;&#33021;&#21147;&#65292;&#36890;&#24120;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#25968;&#23398;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#19987;&#38376;&#30340;&#32467;&#26500;&#21644;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#20004;&#27493;&#26041;&#27861;&#26469;&#35843;&#26597;LLMs&#22312;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#22312;&#25968;&#23398;&#38382;&#39064;-&#31572;&#26696;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;LLMs&#26469;&#22238;&#31572;Math Stack Exchange&#65288;MSE&#65289;&#20013;&#30340;78&#20010;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;LLM&#36827;&#34892;&#26696;&#20363;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#31572;&#26696;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20026;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#24494;&#35843;&#30340;&#29616;&#26377;LLMs&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65288;nDCG&#20026;0.48&#65292;P@10&#20026;0.37&#65289;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00344v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;AREIL&#65292;&#29992;&#20110;&#22312;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#34920;&#31034;&#22686;&#24378;&#21644;&#21453;&#21521;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#22686;&#24378;&#21644;&#20449;&#24687;&#20256;&#36755;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.00268</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#22686;&#24378;&#21644;&#21453;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;AREIL&#65292;&#29992;&#20110;&#22312;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#34920;&#31034;&#22686;&#24378;&#21644;&#21453;&#21521;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#22686;&#24378;&#21644;&#20449;&#24687;&#20256;&#36755;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00268v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#36328;&#39046;&#22495;&#25512;&#33616; (CDR) &#26088;&#22312;&#25552;&#21462;&#21644;&#36328;&#39046;&#22495;&#20256;&#36755;&#30693;&#35782;&#65292;&#22240;&#20854;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290; &#23613;&#31649;&#22312;&#34920;&#31034;&#35299;&#32544;&#20197;&#25429;&#33719;&#21508;&#31181;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#34920;&#31034;&#22686;&#24378;&#24182;&#32570;&#20047;&#20005;&#35880;&#30340;&#35299;&#32806;&#32422;&#26463;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30456;&#20851;&#20449;&#24687;&#30340;&#20256;&#36755;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#22686;&#24378;&#21644;&#21453;&#21521;&#23398;&#20064;&#65288;AREIL&#65289;&#12290; &#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#29992;&#25143;&#23884;&#20837;&#21010;&#20998;&#20026;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#29305;&#23450;&#32452;&#20214;&#65292;&#20197;&#35299;&#24320;&#28151;&#21512;&#29992;&#25143;&#20559;&#22909;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#39046;&#22495;&#20869;&#21644;&#36328;&#39046;&#22495;&#20449;&#24687;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#29992;&#25143;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#21367;&#31215;&#27169;&#22359;&#26469;&#25429;&#33719;&#39640;&#38454;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#25105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00268v1 Announce Type: new  Abstract: Cross-domain recommendation (CDR), aiming to extract and transfer knowledge across domains, has attracted wide attention for its efficacy in addressing data sparsity and cold-start problems. Despite significant advances in representation disentanglement to capture diverse user preferences, existing methods usually neglect representation enhancement and lack rigorous decoupling constraints, thereby limiting the transfer of relevant information. To this end, we propose a Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation (AREIL). Specifically, we first divide user embeddings into domain-shared and domain-specific components to disentangle mixed user preferences. Then, we incorporate intra-domain and inter-domain information to adaptively enhance the ability of user representations. In particular, we propose a graph convolution module to capture high-order information, and a self-a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26679;&#21270;&#31867;&#21035;&#24863;&#30693;&#20851;&#27880;SBRS(DCA-SBRS)&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#29992;&#20110;&#36741;&#21161;&#29616;&#26377;&#30340;SBRSs&#22312;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#21015;&#34920;</title><link>https://arxiv.org/abs/2404.00261</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#26377;&#25928;&#30340;&#22810;&#26679;&#21270;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Yet Effective Approach for Diversified Session-Based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00261
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26679;&#21270;&#31867;&#21035;&#24863;&#30693;&#20851;&#27880;SBRS(DCA-SBRS)&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#29992;&#20110;&#36741;&#21161;&#29616;&#26377;&#30340;SBRSs&#22312;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;(SBRSs)&#22312;&#25429;&#25417;&#30701;&#26399;&#21644;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#30340;&#26680;&#24515;&#33021;&#21147;&#26041;&#38754;&#21464;&#24471;&#26497;&#20026;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SBRSs&#20027;&#35201;&#22312;&#20110;&#26368;&#22823;&#21270;&#25512;&#33616;&#20934;&#30830;&#24230;&#65292;&#20294;&#24573;&#30053;&#20102;&#29992;&#25143;&#30340;&#27425;&#35201;&#20559;&#22909;&#65292;&#20174;&#32780;&#26368;&#32456;&#23548;&#33268;&#38271;&#26399;&#30340;&#31579;&#27873;&#12290;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#25552;&#39640;&#22810;&#26679;&#24615;&#65292;&#20381;&#36182;&#20110;&#29420;&#29305;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#26657;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#36731;&#26131;&#22320;&#36866;&#24212;&#29616;&#26377;&#30340;&#38754;&#21521;&#20934;&#30830;&#24230;&#30340;SBRSs&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#19968;&#31181;&#26082;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;&#35774;&#35745;&#26159;&#20540;&#24471;&#30340;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#29992;&#20110;&#36741;&#21161;&#29616;&#26377;&#30340;SBRSs&#22312;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#21015;&#34920;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27599;&#19968;&#20010;&#29616;&#26377;&#30340;&#20195;&#34920;&#24615;(&#38754;&#21521;&#20934;&#30830;&#24230;)SBRS&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#31867;&#21035;&#24863;&#30693;&#20851;&#27880;SBRS(DCA-SBRS)&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00261v1 Announce Type: cross  Abstract: Session-based recommender systems (SBRSs) have become extremely popular in view of the core capability of capturing short-term and dynamic user preferences. However, most SBRSs primarily maximize recommendation accuracy but ignore user minor preferences, thus leading to filter bubbles in the long run. Only a handful of works, being devoted to improving diversity, depend on unique model designs and calibrated loss functions, which cannot be easily adapted to existing accuracy-oriented SBRSs. It is thus worthwhile to come up with a simple yet effective design that can be used as a plugin to facilitate existing SBRSs on generating a more diversified list in the meantime preserving the recommendation accuracy. In this case, we propose an end-to-end framework applied for every existing representative (accuracy-oriented) SBRS, called diversified category-aware attentive SBRS (DCA-SBRS), to boost the performance on recommendation diversity. I
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#30693;&#35782;&#23545;&#40784;&#65292;&#36890;&#36807;&#27169;&#25311;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25805;&#20316;&#29983;&#25104;&#36741;&#21161;&#20219;&#21153;&#25968;&#25454;&#26679;&#26412;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25512;&#33616;&#29305;&#23450;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00245</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#30693;&#35782;&#36827;&#34892;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models with Recommendation Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00245
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#30693;&#35782;&#23545;&#40784;&#65292;&#36890;&#36807;&#27169;&#25311;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25805;&#20316;&#29983;&#25104;&#36741;&#21161;&#20219;&#21153;&#25968;&#25454;&#26679;&#26412;&#26469;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25512;&#33616;&#29305;&#23450;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#29992;&#20316;&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#20219;&#21153;(&#22914;&#26816;&#32034;)&#20013;&#30340;&#24615;&#33021;&#24448;&#24448;&#33853;&#21518;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;LLMs&#30693;&#35782;&#19982;&#26377;&#25928;&#25512;&#33616;&#25152;&#38656;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;LLMs&#25797;&#38271;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#20294;&#26080;&#27861;&#24314;&#27169;&#25512;&#33616;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24357;&#21512;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;LLMs&#25552;&#20379;&#25512;&#33616;&#29305;&#23450;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#35832;&#22914;&#36974;&#30422;&#29289;&#21697;&#24314;&#27169;(MIM)&#21644;&#36125;&#21494;&#26031;&#20010;&#24615;&#21270;&#25490;&#21517;(BPR)&#20043;&#31867;&#30340;&#25805;&#20316;&#22312;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#36825;&#20123;&#25805;&#20316;&#65292;&#29983;&#25104;&#32534;&#30721;&#29289;&#21697;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#36741;&#21161;&#20219;&#21153;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#36825;&#31181;&#36741;&#21161;&#20219;&#21153;&#25968;&#25454;&#26679;&#26412;&#19978;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#34701;&#20837;&#26356;&#22810;&#20449;&#24687;&#24615;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00245v1 Announce Type: new  Abstract: Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs' knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommend
&lt;/p&gt;</description></item><item><title>&#23558;&#22797;&#26434;&#30340;&#22330;&#26223;&#36335;&#32447;&#25490;&#21517;&#22240;&#32032;&#21270;&#20026;&#20960;&#20010;&#35299;&#32806;&#21512;&#22240;&#23376;&#22330;&#26223;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#35299;&#32806;&#21512;&#22330;&#26223;&#22240;&#23376;&#21270;&#32593;&#32476;&#65288;DSFNet&#65289;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#22810;&#22330;&#26223;&#36335;&#32447;&#25490;&#21517;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00243</link><description>&lt;p&gt;
DSFNet: &#23398;&#20064;&#35299;&#32806;&#21512;&#22330;&#26223;&#22240;&#23376;&#21270;&#20197;&#36827;&#34892;&#22810;&#22330;&#26223;&#36335;&#32447;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00243
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22797;&#26434;&#30340;&#22330;&#26223;&#36335;&#32447;&#25490;&#21517;&#22240;&#32032;&#21270;&#20026;&#20960;&#20010;&#35299;&#32806;&#21512;&#22240;&#23376;&#22330;&#26223;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#35299;&#32806;&#21512;&#22330;&#26223;&#22240;&#23376;&#21270;&#32593;&#32476;&#65288;DSFNet&#65289;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#22810;&#22330;&#26223;&#36335;&#32447;&#25490;&#21517;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22330;&#26223;&#36335;&#32447;&#25490;&#21517;&#65288;MSRR&#65289;&#22312;&#35768;&#22810;&#24037;&#19994;&#22320;&#22270;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#30028;&#20027;&#35201;&#37319;&#29992;&#20132;&#20114;&#30028;&#38754;&#40723;&#21169;&#29992;&#25143;&#36873;&#25321;&#39044;&#23450;&#20041;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#19979;&#28216;&#25490;&#21517;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#26415;&#30028;&#65292;&#22810;&#22330;&#26223;&#25490;&#21517;&#30340;&#24037;&#20316;&#20165;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#65292;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;MSRR&#25968;&#25454;&#38598;&#65292;&#27809;&#26377;&#19987;&#38376;&#20851;&#27880;&#36335;&#32447;&#25968;&#25454;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#22810;&#22330;&#26223;&#24037;&#20316;&#20173;&#26080;&#27861;&#21516;&#26102;&#35299;&#20915;MSRR&#30340;&#19977;&#20010;&#29305;&#23450;&#25361;&#25112;&#65292;&#21363;&#22330;&#26223;&#25968;&#37327;&#29190;&#28856;&#12289;&#39640;&#24230;&#20132;&#32455;&#21644;&#39640;&#23481;&#37327;&#38656;&#27714;&#12290;&#19982;&#20197;&#24448;&#19981;&#21516;&#65292;&#20026;&#20102;&#35299;&#20915;MSRR&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#36335;&#32447;&#25490;&#21517;&#20013;&#22797;&#26434;&#30340;&#22330;&#26223;&#22240;&#23376;&#21270;&#20026;&#20960;&#20010;&#35299;&#32806;&#21512;&#30340;&#22240;&#23376;&#22330;&#26223;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#35299;&#32806;&#21512;&#22330;&#26223;&#22240;&#23376;&#21270;&#32593;&#32476;&#65288;DSFNet&#65289;&#65292;&#23427;&#33021;&#28789;&#27963;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00243v1 Announce Type: new  Abstract: Multi-scenario route ranking (MSRR) is crucial in many industrial mapping systems. However, the industrial community mainly adopts interactive interfaces to encourage users to select pre-defined scenarios, which may hinder the downstream ranking performance. In addition, in the academic community, the multi-scenario ranking works only come from other fields, and there are no works specifically focusing on route data due to lacking a publicly available MSRR dataset. Moreover, all the existing multi-scenario works still fail to address the three specific challenges of MSRR simultaneously, i.e. explosion of scenario number, high entanglement, and high-capacity demand. Different from the prior, to address MSRR, our key idea is to factorize the complicated scenario in route ranking into several disentangled factor scenario patterns. Accordingly, we propose a novel method, Disentangled Scenario Factorization Network (DSFNet), which flexibly co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoID&#30340;&#35821;&#20041;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#23545;&#40784;&#29992;&#25143;/&#39033;&#30446;ID&#21644;&#20869;&#23481;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.00236</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Content-based Recommendation via Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoID&#30340;&#35821;&#20041;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#23545;&#40784;&#29992;&#25143;/&#39033;&#30446;ID&#21644;&#20869;&#23481;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#22312;&#19982;&#19981;&#21516;&#39033;&#30446;&#20114;&#21160;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#38544;&#24335;&#30340;&#28857;&#20987;/&#28857;&#36190;&#20114;&#21160;&#20197;&#21450;&#26174;&#24335;&#30340;&#35780;&#35770;/&#35780;&#20215;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#25512;&#33616;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#36890;&#36807;&#38544;&#24335;&#30340;&#28857;&#20987;/&#28857;&#36190;&#20114;&#21160;&#26469;&#25551;&#36848;&#29992;&#25143;&#20559;&#22909;&#65292;&#20197;&#25214;&#21040;&#20154;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#12290;&#23545;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#26174;&#24335;&#35780;&#35770;/&#35780;&#20215;&#20114;&#21160;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#23427;&#20204;&#26469;&#25366;&#25496;&#35821;&#20041;&#30693;&#35782;&#20197;&#22686;&#24378;&#25512;&#33616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#24573;&#35270;&#20102;&#20197;&#19979;&#20004;&#28857;&#65306;&#65288;1&#65289;&#20869;&#23481;&#35821;&#20041;&#26159;&#26222;&#36866;&#30340;&#19990;&#30028;&#30693;&#35782;&#65307;&#25105;&#20204;&#22914;&#20309;&#25552;&#21462;&#22810;&#26041;&#38754;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#65311;&#65288;2&#65289;&#29992;&#25143;/&#39033;&#30446;ID&#29305;&#24449;&#26159;&#25512;&#33616;&#27169;&#22411;&#30340;&#22522;&#30784;&#35201;&#32032;&#65307;&#25105;&#20204;&#22914;&#20309;&#23545;&#40784;ID&#21644;&#20869;&#23481;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25554;&#20214;&#8221;&#35821;&#20041;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;LoID&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00236v1 Announce Type: cross  Abstract: In real-world applications, users express different behaviors when they interact with different items, including implicit click/like interactions, and explicit comments/reviews interactions. Nevertheless, almost all recommender works are focused on how to describe user preferences by the implicit click/like interactions, to find the synergy of people. For the content-based explicit comments/reviews interactions, some works attempt to utilize them to mine the semantic knowledge to enhance recommender models. However, they still neglect the following two points: (1) The content semantic is a universal world knowledge; how do we extract the multi-aspect semantic information to empower different domains? (2) The user/item ID feature is a fundamental element for recommender models; how do we align the ID and content semantic feature space? In this paper, we propose a `plugin' semantic knowledge transferring method \textbf{LoID}, which inclu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.17372</link><description>&lt;p&gt;
&#35757;&#32451;&#29420;&#31435;&#20110;ID&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#22120;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#26410;&#26469;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#35768;&#22810;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#38598;&#20013;&#22312;&#29992;&#25143;ID&#21644;&#29289;&#21697;ID&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#24863;&#30693;&#19990;&#30028;&#30340;&#26041;&#24335;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22914;&#20309;&#26500;&#24314;&#19981;&#20351;&#29992;ID&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#65288;MMSR&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#23558;&#31934;&#21326;&#25552;&#28860;&#25104;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#12290;&#27839;&#30528;&#36825;&#20123;&#32500;&#24230;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#27169;&#22411;&#35774;&#35745;&#65292;&#24182;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17372v1 Announce Type: new  Abstract: Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the foll
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;Flickr30K-CFQ&#65289;&#65292;&#27169;&#25311;&#32771;&#34385;&#22810;&#20010;&#26597;&#35810;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#22686;&#24378;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13317</link><description>&lt;p&gt;
Flickr30K-CFQ&#65306;&#29992;&#20110;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;Flickr30K-CFQ&#65289;&#65292;&#27169;&#25311;&#32771;&#34385;&#22810;&#20010;&#26597;&#35810;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#22686;&#24378;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#21333;&#27169;&#24577;&#25628;&#32034;&#26080;&#27861;&#28385;&#36275;&#20114;&#32852;&#32593;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#38656;&#35201;&#36827;&#34892;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30740;&#31350;&#65292;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#39640;&#36136;&#37327;&#39640;&#25928;&#30340;&#26816;&#32034;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#36890;&#29992;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;&#65288;&#22914;MS-COCO&#12289;Flickr30K&#65289;&#65292;&#20854;&#20013;&#26597;&#35810;&#35805;&#35821;&#21051;&#26495;&#32780;&#19981;&#33258;&#28982;&#65288;&#21363;&#20887;&#38271;&#21644;&#36807;&#20110;&#27491;&#24335;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;&#21517;&#20026;Flickr30K-CFQ&#65289;&#65292;&#20197;&#24314;&#27169;&#32771;&#34385;&#22810;&#20010;&#26597;&#35810;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#21253;&#25324;&#32039;&#20945;&#21644;&#32454;&#31890;&#24230;&#30340;&#23454;&#20307;&#20851;&#31995;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#22411;&#26597;&#35810;&#22686;&#24378;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Flickr30-CFQ&#25581;&#31034;&#20102;&#29616;&#26377;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;&#22312;&#29616;&#23454;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13317v1 Announce Type: new  Abstract: With the explosive growth of multi-modal information on the Internet, unimodal search cannot satisfy the requirement of Internet applications. Text-image retrieval research is needed to realize high-quality and efficient retrieval between different modalities. Existing text-image retrieval research is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K), in which the query utterance is rigid and unnatural (i.e. verbosity and formality). To overcome the shortcoming, we construct a new Compact and Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image retrieval task considering multiple query content and style, including compact and fine-grained entity-relation corpus. We propose a novel query-enhanced text-image retrieval method using prompt engineering based on LLM. Experiments show that our proposed Flickr30-CFQ reveals the insufficiency of existing vision-language datasets in realistic text-i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20165;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26694;&#26550;LinCIR&#65292;&#24182;&#24341;&#20837;&#33258;&#36974;&#34109;&#25237;&#24433;&#65288;SMP&#65289;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2312.01998</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#35821;&#35328;&#39640;&#25928;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Language-only Efficient Training of Zero-shot Composed Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20165;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26694;&#26550;LinCIR&#65292;&#24182;&#24341;&#20837;&#33258;&#36974;&#34109;&#25237;&#24433;&#65288;SMP&#65289;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#65288;CIR&#65289;&#20219;&#21153;&#28041;&#21450;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32452;&#21512;&#26597;&#35810;&#65292;&#26088;&#22312;&#25628;&#32034;&#31526;&#21512;&#36825;&#20004;&#20010;&#26465;&#20214;&#30340;&#30456;&#20851;&#22270;&#20687;&#12290;&#20256;&#32479;&#30340;CIR&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26597;&#35810;&#22270;&#20687;&#12289;&#26597;&#35810;&#25991;&#26412;&#21644;&#30446;&#26631;&#22270;&#20687;&#30340;&#19977;&#20803;&#32452;&#65292;&#36825;&#22312;&#25910;&#38598;&#36807;&#31243;&#20013;&#38750;&#24120;&#26114;&#36149;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#37319;&#29992;&#38646;&#26679;&#26412;&#65288;ZS&#65289;CIR&#33539;&#24335;&#65292;&#20197;&#35299;&#20915;&#22312;&#27809;&#26377;&#20351;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#19977;&#20803;&#32452;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ZS-CIR&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30001;&#20110;&#36755;&#20837;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#32570;&#20047;&#65292;&#26174;&#31034;&#20986;&#20102;&#26377;&#38480;&#30340;&#20027;&#24178;&#21487;&#25193;&#23637;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CIR&#26694;&#26550;&#65292;&#20165;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;LinCIR&#65288;Language-only training for CIR&#65289;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#33258;&#36974;&#34109;&#25237;&#24433;&#65288;SMP&#65289;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#28508;&#22312;&#23884;&#20837;&#25237;&#24433;&#21040;&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21407;&#22987;&#25991;&#26412;&#30340;&#20851;&#38190;&#23383;&#20196;&#29260;&#26500;&#36896;&#19968;&#20010;&#26032;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01998v2 Announce Type: replace-cross  Abstract: Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, 
&lt;/p&gt;</description></item><item><title>ARES&#26159;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#35780;&#20272;&#22120;&#65292;&#26377;&#25928;&#35780;&#20272;RAG&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.09476</link><description>&lt;p&gt;
ARES: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09476
&lt;/p&gt;
&lt;p&gt;
ARES&#26159;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#35780;&#20272;&#22120;&#65292;&#26377;&#25928;&#35780;&#20272;RAG&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#36755;&#20837;&#26597;&#35810;&#12289;&#26816;&#32034;&#27573;&#33853;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ARES&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;RAG&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#31995;&#32479;&#65292;&#35780;&#20272;&#32500;&#24230;&#21253;&#25324;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12289;&#31572;&#26696;&#24544;&#23454;&#24230;&#21644;&#31572;&#26696;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#33258;&#24049;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;ARES&#24494;&#35843;&#36731;&#37327;&#32423;LM&#35780;&#20272;&#22120;&#20197;&#35780;&#20272;&#21333;&#20010;RAG&#32452;&#20214;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#28508;&#22312;&#30340;&#39044;&#27979;&#38169;&#35823;&#65292;ARES&#21033;&#29992;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#65288;PPI&#65289;&#12290;&#22312;KILT&#12289;SuperGLUE&#21644;AIS&#30340;&#20843;&#20010;&#19981;&#21516;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;ARES&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#23601;&#20934;&#30830;&#35780;&#20272;RAG&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;ARES&#35780;&#20272;&#22120;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#20173;&#28982;&#26377;&#25928;&#65292;&#21363;&#20351;&#22312;&#26356;&#25913;&#29992;&#20110;&#35780;&#20272;&#30340;&#26597;&#35810;&#21644;/&#25110;&#25991;&#26723;&#31867;&#22411;&#21518;&#20173;&#28982;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;&#26694;&#26550; Juno&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#20013;&#25991;&#26412;&#36328;&#24230;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#35821;&#20041;&#30456;&#20284;&#20803;&#32452;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20449;&#24687;&#26816;&#32034;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#21644;&#35265;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2303.00720</link><description>&lt;p&gt;
&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30340;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Entity Matching for Visually Rich Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00720
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;&#26694;&#26550; Juno&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#20013;&#25991;&#26412;&#36328;&#24230;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#35821;&#20041;&#30456;&#20284;&#20803;&#32452;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20449;&#24687;&#26816;&#32034;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#21644;&#35265;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;&#22914;&#20256;&#21333;&#12289;&#27178;&#24133;&#12289;&#26434;&#24535;&#25991;&#31456;&#65289;&#26159;&#21033;&#29992;&#35270;&#35273;&#32447;&#32034;&#26469;&#22686;&#24378;&#35821;&#20041;&#30340;&#23454;&#20307;&#25110;&#25968;&#23383;&#21270;&#25991;&#26723;&#12290;&#36825;&#20123;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#24448;&#24448;&#26159;&#20020;&#26102;&#30340;&#65292;&#32463;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#29616;&#26377;&#30340;&#20801;&#35768;&#23545;&#36825;&#20123;&#25991;&#26723;&#36827;&#34892;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#36825;&#20351;&#24471;&#22312;&#20174;&#36825;&#20123;&#25991;&#26723;&#20013;&#36827;&#34892;&#26597;&#35810;&#24182;&#20174;&#20013;&#33719;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#26102;&#65292;&#24456;&#38590;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Juno - &#19968;&#20010;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#12290;&#23427;&#36890;&#36807;&#23558;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#36328;&#24230;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#35821;&#20041;&#31867;&#20284;&#30340;&#20803;&#32452;&#36827;&#34892;&#21305;&#37197;&#65292;&#20174;&#32780;&#20026;&#24322;&#26500;&#25991;&#26723;&#25552;&#20379;&#34917;&#20805;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#24102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#21305;&#37197;&#65292;&#36890;&#36807;&#22312;&#22810;&#27169;&#24577;&#32534;&#30721;&#31354;&#38388;&#19978;&#23545;&#40784;&#25991;&#26412;&#36328;&#24230;&#21644;&#20851;&#31995;&#20803;&#32452;&#26469;&#25214;&#21040;&#21305;&#37197;&#30340;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00720v2 Announce Type: replace  Abstract: Visually rich documents (e.g. leaflets, banners, magazine articles) are physical or digital documents that utilize visual cues to augment their semantics. Information contained in these documents are ad-hoc and often incomplete. Existing works that enable structured querying on these documents do not take this into account. This makes it difficult to contextualize the information retrieved from querying these documents and gather actionable insights from them. We propose Juno -- a cross-modal entity matching framework to address this limitation. It augments heterogeneous documents with supplementary information by matching a text span in the document with semantically similar tuples from an external database. Our main contribution in this is a deep neural network with attention that goes beyond traditional keyword-based matching and finds matching tuples by aligning text spans and relational tuples on a multimodal encoding space with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFM&#30340;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#33258;&#28982;&#34920;&#31034;&#29305;&#24449;&#65292;&#24182;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#38598;&#25104;&#21040;GNN&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#33021;&#22815;&#27169;&#25311;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2105.11866</link><description>&lt;p&gt;
GraphFM&#65306;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#29992;&#20110;&#29305;&#24449;&#20132;&#20114;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GraphFM: Graph Factorization Machines for Feature Interaction Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.11866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFM&#30340;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#33258;&#28982;&#34920;&#31034;&#29305;&#24449;&#65292;&#24182;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#38598;&#25104;&#21040;GNN&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#33021;&#22815;&#27169;&#25311;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#20998;&#35299;&#26426;&#65288;FM&#65289;&#26159;&#22788;&#29702;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#26102;&#24314;&#27169;&#25104;&#23545;&#65288;&#20108;&#38454;&#65289;&#29305;&#24449;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;FM&#26410;&#33021;&#25429;&#25417;&#21040;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#65292;&#21463;&#21040;&#32452;&#21512;&#25193;&#23637;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32771;&#34385;&#27599;&#23545;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Graph Factorization Machine&#65288;GraphFM&#65289;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#33258;&#28982;&#34920;&#31034;&#25104;&#22270;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#36873;&#25321;&#26377;&#30410;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#29305;&#24449;&#20043;&#38388;&#30340;&#36793;&#12290;&#28982;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#25972;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#36890;&#36807;&#22534;&#21472;&#23618;&#26469;&#27169;&#25311;&#22270;&#32467;&#26500;&#29305;&#24449;&#19978;&#30340;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.11866v4 Announce Type: replace-cross  Abstract: Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion. On the other hand, taking into account interactions between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach, Graph Factorization Machine (GraphFM), by naturally representing features in the graph structure. In particular, we design a mechanism to select the beneficial feature interactions and formulate them as edges between features. Then the proposed model, which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experime
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;PrivaSeer&#32593;&#39029;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#30334;&#19975;&#20221;&#33521;&#35821;&#32593;&#31449;&#38544;&#31169;&#25919;&#31574;&#65292;&#21033;&#29992;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21019;&#24314;&#32780;&#25104;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#32570;&#20047;&#30340;&#22823;&#35268;&#27169;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2004.11131</link><description>&lt;p&gt;
&#38544;&#31169;&#23610;&#24230;&#65306;&#20171;&#32461;PrivaSeer&#32593;&#39029;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2004.11131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;PrivaSeer&#32593;&#39029;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#30334;&#19975;&#20221;&#33521;&#35821;&#32593;&#31449;&#38544;&#31169;&#25919;&#31574;&#65292;&#21033;&#29992;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21019;&#24314;&#32780;&#25104;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#32570;&#20047;&#30340;&#22823;&#35268;&#27169;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#36890;&#36807;&#22312;&#20854;&#32593;&#31449;&#19978;&#21457;&#24067;&#38544;&#31169;&#25919;&#31574;&#26469;&#25259;&#38706;&#20854;&#38544;&#31169;&#20570;&#27861;&#12290;&#34429;&#28982;&#29992;&#25143;&#36890;&#24120;&#20851;&#24515;&#20182;&#20204;&#30340;&#25968;&#23383;&#38544;&#31169;&#65292;&#20294;&#20182;&#20204;&#36890;&#24120;&#19981;&#38405;&#35835;&#38544;&#31169;&#25919;&#31574;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#21162;&#21147;&#25237;&#36164;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#38544;&#31169;&#25919;&#31574;&#65292;&#20294;&#20197;&#24448;&#19968;&#30452;&#32570;&#20047;&#21487;&#29992;&#20110;&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#31616;&#21270;&#38544;&#31169;&#25919;&#31574;&#30340;&#22823;&#35268;&#27169;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;PrivaSeer&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20221;&#33521;&#35821;&#32593;&#31449;&#38544;&#31169;&#25919;&#31574;&#30340;&#35821;&#26009;&#24211;&#65292;&#26126;&#26174;&#22823;&#20110;&#20197;&#24448;&#20219;&#20309;&#21487;&#29992;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35821;&#26009;&#24211;&#21019;&#24314;&#27969;&#31243;&#65292;&#21253;&#25324;&#29228;&#21462;&#32593;&#39029;&#65292;&#28982;&#21518;&#20351;&#29992;&#35821;&#35328;&#26816;&#27979;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#37325;&#22797;&#21644;&#36817;&#20284;&#25991;&#26723;&#21024;&#38500;&#20197;&#21450;&#20869;&#23481;&#25552;&#21462;&#36827;&#34892;&#25991;&#26723;&#31579;&#36873;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#35821;&#26009;&#24211;&#30340;&#32452;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#38405;&#35835;&#33021;&#21147;&#27979;&#35797;&#21644;&#25991;&#26723;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2004.11131v2 Announce Type: replace  Abstract: Organisations disclose their privacy practices by posting privacy policies on their website. Even though users often care about their digital privacy, they often don't read privacy policies since they require a significant investment in time and effort. Although natural language processing can help in privacy policy understanding, there has been a lack of large scale privacy policy corpora that could be used to analyse, understand, and simplify privacy policies. Thus, we create PrivaSeer, a corpus of over one million English language website privacy policies, which is significantly larger than any previously available corpus. We design a corpus creation pipeline which consists of crawling the web followed by filtering documents using language detection, document classification, duplicate and near-duplication removal, and content extraction. We investigate the composition of the corpus and show results from readability tests, document
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RecRanker&#65292;&#19987;&#38376;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;LLM&#20197;&#20316;&#20026;&#21069;k&#39033;&#25512;&#33616;&#30340;&#25490;&#21517;&#22120;&#12290;</title><link>http://arxiv.org/abs/2312.16018</link><description>&lt;p&gt;
RecRanker: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25490;&#21517;&#22120;&#36827;&#34892;&#21069;k&#39033;&#25512;&#33616;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. (arXiv:2312.16018v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RecRanker&#65292;&#19987;&#38376;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;LLM&#20197;&#20316;&#20026;&#21069;k&#39033;&#25512;&#33616;&#30340;&#25490;&#21517;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#19987;&#38376;&#30340;&#8220;&#25552;&#31034;&#8221;&#26469;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;LLMs&#34987;&#25552;&#31034;&#20026;&#38646;-shot&#25490;&#21517;&#22120;&#65292;&#29992;&#20110;&#23545;&#30001;&#26816;&#32034;&#27169;&#22411;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#36827;&#34892;&#21015;&#34920;&#25490;&#21517;&#65292;&#20197;&#29992;&#20110;&#25512;&#33616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#26469;&#25552;&#20379;&#26356;&#26377;&#21069;&#26223;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#25972;&#21512;&#22810;&#20010;&#25490;&#21517;&#20219;&#21153;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#30340;&#20449;&#21495;&#26410;&#19982;LLM&#25972;&#21512;&#65292;&#38480;&#21046;&#20102;&#24403;&#21069;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.  In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \textbf{Ranker} for top-\textit{k} \textbf{Rec}ommendations. Specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.15651</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#25512;&#33616;&#31995;&#32479;&#20013;&#30830;&#20445;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ensuring User-side Fairness in Dynamic Recommender Systems. (arXiv:2308.15651v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20391;&#32676;&#20307;&#20844;&#24179;&#24615;&#23545;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#26088;&#22312;&#20943;&#36731;&#30001;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#24180;&#40836;&#65289;&#23450;&#20041;&#30340;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#24046;&#24322;&#24448;&#24448;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#25345;&#32493;&#23384;&#22312;&#29978;&#33267;&#22686;&#21152;&#12290;&#36825;&#38656;&#35201;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#26377;&#25928;&#35299;&#20915;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#25506;&#35752;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#30830;&#20445;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;&#65288;&#21363;&#20943;&#23569;&#24615;&#33021;&#24046;&#24322;&#65289;&#30340;&#20856;&#22411;&#26041;&#27861;&#8212;&#8212;&#20844;&#24179;&#32422;&#26463;&#37325;&#26032;&#25490;&#21517;&#65292;&#22312;&#21160;&#24577;&#35774;&#23450;&#20013;&#38754;&#20020;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22522;&#20110;&#25490;&#21517;&#30340;&#20844;&#24179;&#32422;&#26463;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#33539;&#24335;&#65307;&#65288;2&#65289;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#65292;&#38459;&#30861;&#20102;&#23545;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#24615;&#33021;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FADE&#25552;&#20986;&#20102;&#19968;&#31181; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-side group fairness is crucial for modern recommender systems, as it aims to alleviate performance disparity between groups of users defined by sensitive attributes such as gender, race, or age. We find that the disparity tends to persist or even increase over time. This calls for effective ways to address user-side fairness in a dynamic environment, which has been infrequently explored in the literature. However, fairness-constrained re-ranking, a typical method to ensure user-side fairness (i.e., reducing performance disparity), faces two fundamental challenges in the dynamic setting: (1) non-differentiability of the ranking-based fairness constraint, which hinders the end-to-end training paradigm, and (2) time-inefficiency, which impedes quick adaptation to changes in user preferences. In this paper, we propose FAir Dynamic rEcommender (FADE), an end-to-end framework with fine-tuning strategy to dynamically alleviate performance disparity. To tackle the above challenges, FADE u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01157</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#30784;&#36923;&#36753;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#20960;&#20309;&#26469;&#23884;&#20837;&#23454;&#20307;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#25805;&#20316;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#22797;&#26434;&#26597;&#35810;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#30693;&#35782;&#22270;&#35889;&#25277;&#35937;&#25512;&#29702;&#65288;LARK&#65289;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#20197;&#20998;&#21035;&#21033;&#29992;&#22270;&#24418;&#25552;&#21462;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
&lt;/p&gt;</description></item></channel></rss>