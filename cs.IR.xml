<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Thistle&#26159;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#22238;&#31572;&#25628;&#32034;&#26597;&#35810;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#39046;&#22495;&#38382;&#39064;&#65292;&#24050;&#32463;&#22312;MS MARCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#25512;&#36827;Rust ML&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.16780</link><description>&lt;p&gt;
Thistle: Rust&#20013;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Thistle: A Vector Database in Rust. (arXiv:2303.16780v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16780
&lt;/p&gt;
&lt;p&gt;
Thistle&#26159;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#22238;&#31572;&#25628;&#32034;&#26597;&#35810;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#39046;&#22495;&#38382;&#39064;&#65292;&#24050;&#32463;&#22312;MS MARCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#25512;&#36827;Rust ML&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Thistle&#65292;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#12290;Thistle&#26159;Latent Knowledge Use&#22312;&#22238;&#31572;&#25628;&#32034;&#26597;&#35810;&#26041;&#38754;&#30340;&#20998;&#25903;&#65292;&#36825;&#26159;&#21021;&#21019;&#20844;&#21496;&#21644;&#25628;&#32034;&#24341;&#25806;&#20844;&#21496;&#30340;&#25345;&#32493;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#20010;&#33879;&#21517;&#31639;&#27861;&#23454;&#29616;Thistle&#65292;&#24182;&#22312;MS MARCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#26377;&#21161;&#20110;&#28548;&#28165;&#28508;&#22312;&#30693;&#35782;&#39046;&#22495;&#20197;&#21450;&#19981;&#26029;&#22686;&#38271;&#30340;Rust ML&#29983;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Thistle, a fully functional vector database. Thistle is an entry into the domain of latent knowledge use in answering search queries, an ongoing research topic at both start-ups and search engine companies. We implement Thistle with several well-known algorithms, and benchmark results on the MS MARCO dataset. Results help clarify the latent knowledge domain as well as the growing Rust ML ecosystem.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16767</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#19987;&#21033;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65306;&#35821;&#20041;&#36317;&#31163;&#21644;&#25216;&#26415;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
A Novel Patent Similarity Measurement Methodology: Semantic Distance and Technological Distance. (arXiv:2303.16767v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#30830;&#20445;&#21019;&#26032;&#30340;&#26032;&#39062;&#24615;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#19987;&#21033;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#19987;&#23478;&#25163;&#21160;&#20998;&#31867;&#19987;&#21033;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#33258;&#21160;&#21270;&#26041;&#27861;&#21482;&#20851;&#27880;&#19987;&#21033;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19987;&#21033;&#25991;&#26412;&#20351;&#29992;BERT&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;Jaccard&#30456;&#20284;&#24615;&#35745;&#31639;&#19987;&#21033;&#30340;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#26435;&#37325;&#26469;&#23454;&#29616;&#28151;&#21512;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity between patents is an essential step to ensure novelty of innovation. However, a large number of methods of measuring the similarity between patents still rely on manual classification of patents by experts. Another body of research has proposed automated methods; nevertheless, most of it solely focuses on the semantic similarity of patents. In order to tackle these limitations, we propose a hybrid method for automatically measuring the similarity between patents, considering both semantic and technological similarities. We measure the semantic similarity based on patent texts using BERT, calculate the technological similarity with IPC codes using Jaccard similarity, and perform hybridization by assigning weights to the two similarity methods. Our evaluation result demonstrates that the proposed method outperforms the baseline that considers the semantic similarity only.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#20449;&#24687;&#30340;&#35745;&#31639;&#26377;&#25928;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.16766</link><description>&lt;p&gt;
&#29992;&#38750;&#20020;&#24202;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#23454;&#29616;&#32959;&#30244;&#30456;&#20851;&#35770;&#22363;&#24086;&#23376;&#30340;&#35745;&#31639;&#26377;&#25928;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval. (arXiv:2303.16766v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#20449;&#24687;&#30340;&#35745;&#31639;&#26377;&#25928;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#23384;&#22312;&#30528;&#22823;&#37327;&#20851;&#20110;&#30284;&#30151;&#30340;&#20449;&#24687;&#65292;&#20294;&#20998;&#31867;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#24456;&#22256;&#38590;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#22788;&#29702;&#30740;&#31350;&#37117;&#28041;&#21450;&#27491;&#24335;&#30340;&#20020;&#24202;&#25968;&#25454;&#65292;&#20294;&#38750;&#20020;&#24202;&#25968;&#25454;&#20013;&#20063;&#26377;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#23558;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#28548;&#28165;&#30284;&#30151;&#24739;&#32773;&#30340;&#30149;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21407;&#22411;&#65292;&#21487;&#20197;&#20174;&#38750;&#20020;&#24202;&#35770;&#22363;&#24086;&#23376;&#20013;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65288;MR-DBSCAN&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#35843;&#25972;&#21518;&#30340;&#20848;&#24503;&#25351;&#25968;&#21644;&#24635;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20316;&#20026;&#26816;&#32034;&#30340;&#24086;&#23376;&#25968;&#37327;&#21644;&#37051;&#22495;&#21322;&#24452;&#20989;&#25968;&#12290;&#32858;&#31867;&#32467;&#26524;&#26174;&#31034;&#65292;&#37051;&#22495;&#21322;&#24452;&#23545;&#32858;&#31867;&#32467;&#26524;&#26377;&#26368;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
An abundance of information about cancer exists online, but categorizing and extracting useful information from it is difficult. Almost all research within healthcare data processing is concerned with formal clinical data, but there is valuable information in non-clinical data too. The present study combines methods within distributed computing, text retrieval, clustering, and classification into a coherent and computationally efficient system, that can clarify cancer patient trajectories based on non-clinical and freely available information. We produce a fully-functional prototype that can retrieve, cluster and present information about cancer trajectories from non-clinical forum posts. We evaluate three clustering algorithms (MR-DBSCAN, DBSCAN, and HDBSCAN) and compare them in terms of Adjusted Rand Index and total run time as a function of the number of posts retrieved and the neighborhood radius. Clustering results show that neighborhood radius has the most significant impact on c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#20351;&#29992;&#23545;&#35805;&#20316;&#20026;&#25628;&#32034;&#25551;&#36848;&#31526;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35270;&#39057;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16761</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dialogue-to-Video Retrieval. (arXiv:2303.16761v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#20351;&#29992;&#23545;&#35805;&#20316;&#20026;&#25628;&#32034;&#25551;&#36848;&#31526;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35270;&#39057;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#31561;&#32593;&#32476;&#24179;&#21488;&#19978;&#65292;&#20154;&#20204;&#36827;&#34892;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#35805;&#12290;&#36825;&#21551;&#21457;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#26816;&#32034;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#20855;&#26377;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#19981;&#21516;&#20110;&#20854;&#20182;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#65292;&#23545;&#35805;&#21040;&#35270;&#39057;&#26816;&#32034;&#20351;&#29992;&#20197;&#29992;&#25143;&#29983;&#25104;&#30340;&#23545;&#35805;&#20026;&#25628;&#32034;&#25551;&#36848;&#31526;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#34701;&#21512;&#20102;&#32467;&#26500;&#21270;&#30340;&#23545;&#35805;&#20449;&#24687;&#12290;&#22312;AVSD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20351;&#29992;&#32431;&#25991;&#26412;&#26597;&#35810;&#30340;&#26041;&#27861;&#22312;R@1&#19978;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#25552;&#39640;&#20102;15.8%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#35805;&#20316;&#20026;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#22312;R@1&#12289;R@5&#21644;R@10&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;4.2%&#12289;6.2%&#21644;8.6%&#65292;&#22312;R@1&#12289;R@5&#21644;R@10&#19978;&#20998;&#21035;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;0.7%&#12289;3.6%&#21644;6.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increasing amount of dialogue/conversation on the web especially on social media. That inspires the development of dialogue-based retrieval, in which retrieving videos based on dialogue is of increasing interest for recommendation systems. Different from other video retrieval tasks, dialogue-to-video retrieval uses structured queries in the form of user-generated dialogue as the search descriptor. We present a novel dialogue-to-video retrieval system, incorporating structured conversational information. Experiments conducted on the AVSD dataset show that our proposed approach using plain-text queries improves over the previous counterpart model by 15.8% on R@1. Furthermore, our approach using dialogue as a query, improves retrieval performance by 4.2%, 6.2%, 8.6% on R@1, R@5 and R@10 and outperforms the state-of-the-art model by 0.7%, 3.6% and 6.0% on R@1, R@5 and R@10 respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#23545; COVID-19 &#30123;&#24773;&#20013;&#30340;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#25512;&#25991;&#65292;&#21457;&#29616;&#20844;&#20247;&#20154;&#29289;&#30340;&#20449;&#24687;&#23545;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16759</link><description>&lt;p&gt;
&#25506;&#31350;&#21517;&#20154;&#23545;&#20844;&#20247;&#24577;&#24230;&#24433;&#21709;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#24773;&#24863;&#20998;&#26512;&#30340; COVID-19 &#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis. (arXiv:2303.16759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#23545; COVID-19 &#30123;&#24773;&#20013;&#30340;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#25512;&#25991;&#65292;&#21457;&#29616;&#20844;&#20247;&#20154;&#29289;&#30340;&#20449;&#24687;&#23545;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#20026;&#20581;&#24247;&#27807;&#36890;&#24102;&#26469;&#20102;&#26032;&#26426;&#36935;&#65292;&#22686;&#21152;&#20102;&#20844;&#20247;&#20351;&#29992;&#22312;&#32447;&#28192;&#36947;&#33719;&#21462;&#19982;&#20581;&#24247;&#30456;&#20851;&#24773;&#32490;&#30340;&#26426;&#20250;&#12290;&#20154;&#20204;&#24050;&#32463;&#36716;&#21521;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#20998;&#20139;&#19982; COVID-19 &#30123;&#24773;&#24433;&#21709;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#65288;&#21363;&#36816;&#21160;&#21592;&#12289;&#25919;&#27835;&#23478;&#12289;&#26032;&#38395;&#24037;&#20316;&#32773;&#65289;&#20849;&#20139;&#30340;&#31038;&#20132;&#20449;&#24687;&#22312;&#20915;&#23450;&#25972;&#20307;&#20844;&#20849;&#35805;&#35821;&#26041;&#21521;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174; 2020 &#24180; 1 &#26376; 1 &#26085;&#21040; 2022 &#24180; 3 &#26376; 1 &#26085;&#25910;&#38598;&#20102;&#32422; 1300 &#19975;&#26465;&#25512;&#29305;&#12290;&#20351;&#29992;&#19968;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340; DistilRoBERTa &#27169;&#22411;&#35745;&#31639;&#20102;&#27599;&#26465;&#25512;&#25991;&#30340;&#24773;&#32490;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#27604;&#36739;&#19982;&#20844;&#20247;&#20154;&#29289;&#25552;&#21450;&#21516;&#26102;&#20986;&#29616;&#30340; COVID-19 &#30123;&#33495;&#30456;&#20851;&#25512;&#29305;&#21457;&#24067;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312; COVID-19 &#30123;&#24773;&#30340;&#21069;&#20004;&#24180;&#37324;&#65292;&#19982;&#20844;&#20247;&#20154;&#29289;&#20849;&#20139;&#30340;&#20449;&#24687;&#21516;&#26102;&#20986;&#29616;&#30340;&#24773;&#24863;&#20869;&#23481;&#20855;&#26377;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24433;&#21709;&#20102;&#20844;&#20247;&#33286;&#35770;&#21644;&#22823;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has introduced new opportunities for health communication, including an increase in the public use of online outlets for health-related emotions. People have turned to social media networks to share sentiments related to the impacts of the COVID-19 pandemic. In this paper we examine the role of social messaging shared by Persons in the Public Eye (i.e. athletes, politicians, news personnel) in determining overall public discourse direction. We harvested approximately 13 million tweets ranging from 1 January 2020 to 1 March 2022. The sentiment was calculated for each tweet using a fine-tuned DistilRoBERTa model, which was used to compare COVID-19 vaccine-related Twitter posts (tweets) that co-occurred with mentions of People in the Public Eye. Our findings suggest the presence of consistent patterns of emotional content co-occurring with messaging shared by Persons in the Public Eye for the first two years of the COVID-19 pandemic influenced public opinion and larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.16751</link><description>&lt;p&gt;
&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#31995;&#32479;&#65306;&#20174;&#31163;&#23130;&#26696;&#20214;&#20013;&#25552;&#21462;&#20107;&#20214;&#20197;&#26816;&#27979;&#35009;&#21028;&#20013;&#30340;&#20105;&#35758;
&lt;/p&gt;
&lt;p&gt;
Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge. (arXiv:2303.16751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27665;&#20107;&#26696;&#20214;&#30340;&#27491;&#24335;&#31243;&#24207;&#20013;&#65292;&#30001;&#19981;&#21516;&#24403;&#20107;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#36164;&#26009;&#25551;&#36848;&#20102;&#26696;&#20214;&#30340;&#21457;&#23637;&#36807;&#31243;&#12290;&#20174;&#36825;&#20123;&#25991;&#26412;&#26448;&#26009;&#20013;&#25552;&#21462;&#26696;&#20214;&#30340;&#20851;&#38190;&#20449;&#24687;&#24182;&#28548;&#28165;&#30456;&#20851;&#24403;&#20107;&#20154;&#30340;&#20105;&#35758;&#28966;&#28857;&#26159;&#19968;&#39033;&#22256;&#38590;&#32780;&#24517;&#35201;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25353;&#29031;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
In formal procedure of civil cases, the textual materials provided by different parties describe the development process of the cases. It is a difficult but necessary task to extract the key information for the cases from these textual materials and to clarify the dispute focus of related parties. Currently, officers read the materials manually and use methods, such as keyword searching and regular matching, to get the target information. These approaches are time-consuming and heavily depending on prior knowledge and carefulness of the officers. To assist the officers to enhance working efficiency and accuracy, we propose an approach to detect disputes from divorce cases based on a two-round-labeling event extracting technique in this paper. We implement the Judicial Intelligent Assistant (JIA) system according to the proposed approach to 1) automatically extract focus events from divorce case materials, 2) align events by identifying co-reference among them, and 3) detect conflicts a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#38590;&#20197;&#36827;&#34892;&#21407;&#21017;&#27604;&#36739;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#30340;&#31639;&#27861;&#27604;&#36739;&#32467;&#26524;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#36873;&#25321;&#31639;&#27861;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.16750</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Gold Standard Dataset for the Reviewer Assignment Problem. (arXiv:2303.16750v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#38590;&#20197;&#36827;&#34892;&#21407;&#21017;&#27604;&#36739;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#30340;&#31639;&#27861;&#27604;&#36739;&#32467;&#26524;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#36873;&#25321;&#31639;&#27861;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#25110;&#20250;&#35758;&#27491;&#22312;&#20351;&#29992;&#25110;&#35797;&#22270;&#20351;&#29992;&#31639;&#27861;&#23558;&#25237;&#31295;&#20998;&#37197;&#32473;&#23457;&#31295;&#20154;&#12290;&#36825;&#20123;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#8220;&#30456;&#20284;&#24230;&#20998;&#25968;&#8221;&#65292;&#21363;&#23545;&#23457;&#31295;&#20154;&#22312;&#23457;&#26597;&#35770;&#25991;&#20013;&#30340;&#19987;&#19994;&#27700;&#24179;&#30340;&#25968;&#20540;&#20272;&#35745;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#35745;&#31639;&#36825;&#20123;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#23578;&#26410;&#32463;&#36807;&#26377;&#21407;&#21017;&#30340;&#27604;&#36739;&#65292;&#36825;&#20351;&#24471;&#21033;&#30410;&#30456;&#20851;&#32773;&#38590;&#20197;&#20197;&#22522;&#20110;&#35777;&#25454;&#30340;&#26041;&#24335;&#36873;&#25321;&#31639;&#27861;&#12290;&#27604;&#36739;&#29616;&#26377;&#31639;&#27861;&#21644;&#24320;&#21457;&#26356;&#22909;&#31639;&#27861;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23558;&#29992;&#20110;&#36827;&#34892;&#21487;&#37325;&#22797;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#26032;&#30340;&#30456;&#20284;&#24230;&#24471;&#20998;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#32473;&#30740;&#31350;&#31038;&#21306;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30001;58&#20301;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#30340;477&#20010;&#33258;&#25105;&#25253;&#21578;&#30340;&#19987;&#19994;&#27700;&#24179;&#20998;&#25968;&#32452;&#25104;&#65292;&#29992;&#20110;&#35780;&#20272;&#20182;&#20204;&#20808;&#21069;&#38405;&#35835;&#30340;&#35770;&#25991;&#30340;&#23457;&#26597;&#32463;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#27604;&#36739;&#21508;&#31181;&#31639;&#27861;&#65292;&#24182;&#23545;&#26631;&#20934;&#25968;&#25454;&#38598;&#30340;&#35774;&#35745;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the "similarity score"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.  We use this data to compare s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.16604</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26159;&#26681;&#25454;&#21253;&#21547;&#21442;&#32771;&#22270;&#20687;&#21644;&#25551;&#36848;&#25152;&#38656;&#26356;&#25913;&#30340;&#20462;&#25913;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;&#26469;&#25628;&#32034;&#30446;&#26631;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#23398;&#20064;&#20174;&#65288;&#21442;&#32771;&#22270;&#20687;&#65292;&#20462;&#25913;&#25991;&#26412;&#65289;&#23545;&#21040;&#22270;&#20687;&#23884;&#20837;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#22823;&#22411;&#22270;&#20687;&#35821;&#26009;&#24211;&#36827;&#34892;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#36825;&#31181;&#21453;&#21521;&#26597;&#35810;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#12290;&#20026;&#20102;&#32534;&#30721;&#21452;&#21521;&#26597;&#35810;&#65292;&#25105;&#20204;&#22312;&#20462;&#25913;&#25991;&#26412;&#21069;&#38754;&#28155;&#21152;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20196;&#29260;&#65292;&#25351;&#23450;&#26597;&#35810;&#30340;&#26041;&#21521;&#65292;&#28982;&#21518;&#24494;&#35843;&#25991;&#26412;&#23884;&#20837;&#27169;&#22359;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#27809;&#26377;&#23545;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20854;&#20182;&#26356;&#25913;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21452;&#21521;&#35757;&#32451;&#22312;&#25552;&#39640;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#23545;&#19981;&#21516;&#36716;&#31227;&#37096;&#20301;&#30340;&#21069;&#21015;&#33146;&#30284;&#32959;&#30244;&#36827;&#34892;&#20102;&#22522;&#22240;&#20998;&#26512;&#65292;&#31579;&#36873;&#20986;&#20102;&#19982;&#21069;&#21015;&#33146;&#30284;&#36716;&#31227;&#30456;&#20851;&#30340;13&#20010;&#22522;&#22240;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;92%&#12290;</title><link>http://arxiv.org/abs/2303.15851</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#27861;&#22312;&#21069;&#21015;&#33146;&#30284;&#36951;&#20256;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Genetic Analysis of Prostate Cancer with Computer Science Methods. (arXiv:2303.15851v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#23545;&#19981;&#21516;&#36716;&#31227;&#37096;&#20301;&#30340;&#21069;&#21015;&#33146;&#30284;&#32959;&#30244;&#36827;&#34892;&#20102;&#22522;&#22240;&#20998;&#26512;&#65292;&#31579;&#36873;&#20986;&#20102;&#19982;&#21069;&#21015;&#33146;&#30284;&#36716;&#31227;&#30456;&#20851;&#30340;13&#20010;&#22522;&#22240;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;92%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#24615;&#21069;&#21015;&#33146;&#30284;&#26159;&#30007;&#24615;&#26368;&#24120;&#35265;&#30340;&#30284;&#30151;&#20043;&#19968;&#12290;&#26412;&#25991;&#37319;&#29992;&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#23545;&#19981;&#21516;&#36716;&#31227;&#37096;&#20301;&#30340;&#21069;&#21015;&#33146;&#30284;&#32959;&#30244;&#36827;&#34892;&#22522;&#22240;&#20998;&#26512;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#36807;&#28388;&#26174;&#33879;&#22522;&#22240;&#65292;&#24182;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27425;&#35201;&#32959;&#30244;&#20998;&#31867;&#26469;&#36827;&#19968;&#27493;&#36807;&#28388;&#20851;&#38190;&#22522;&#22240;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#23545;&#19981;&#21516;&#31867;&#22411;&#21069;&#21015;&#33146;&#30284;&#32454;&#32990;&#31995;&#26679;&#26412;&#36827;&#34892;&#20102;&#22522;&#22240;&#20849;&#34920;&#36798;&#32593;&#32476;&#20998;&#26512;&#21644;&#31038;&#21306;&#26816;&#27979;&#12290;&#25991;&#31456;&#31579;&#36873;&#20986;&#20102;&#19982;&#21069;&#21015;&#33146;&#30284;&#36716;&#31227;&#30456;&#20851;&#30340;13&#20010;&#22522;&#22240;&#65292;&#20132;&#21449;&#39564;&#35777;&#19979;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;92%&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#20849;&#34920;&#36798;&#27169;&#24335;&#30340;&#21021;&#27493;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metastatic prostate cancer is one of the most common cancers in men. In the advanced stages of prostate cancer, tumours can metastasise to other tissues in the body, which is fatal. In this thesis, we performed a genetic analysis of prostate cancer tumours at different metastatic sites using data science, machine learning and topological network analysis methods. We presented a general procedure for pre-processing gene expression datasets and pre-filtering significant genes by analytical methods. We then used machine learning models for further key gene filtering and secondary site tumour classification. Finally, we performed gene co-expression network analysis and community detection on samples from different prostate cancer secondary site types. In this work, 13 of the 14,379 genes were selected as the most metastatic prostate cancer related genes, achieving approximately 92% accuracy under cross-validation. In addition, we provide preliminary insights into the co-expression patterns
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22270;&#20687;&#32858;&#31867;&#30340;&#20247;&#21253;&#31995;&#32479;&#65292;&#23454;&#39564;&#35777;&#26126;&#21482;&#36890;&#36807;&#20247;&#21253;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#32858;&#31867;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.10267</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#30693;&#36947;&#22914;&#20309;&#32858;&#31867;&#65306;&#24212;&#29992;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Clustering Without Knowing How To: Application and Evaluation. (arXiv:2209.10267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22270;&#20687;&#32858;&#31867;&#30340;&#20247;&#21253;&#31995;&#32479;&#65292;&#23454;&#39564;&#35777;&#26126;&#21482;&#36890;&#36807;&#20247;&#21253;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#32858;&#31867;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#20801;&#35768;&#22312;&#22823;&#35268;&#27169;&#24037;&#20154;&#32676;&#20307;&#19978;&#36816;&#34892;&#31616;&#21333;&#30340;&#20154;&#31867;&#26234;&#33021;&#20219;&#21153;&#65292;&#21487;&#20197;&#35299;&#20915;&#38590;&#20197;&#21046;&#23450;&#31639;&#27861;&#25110;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;&#25353;&#19981;&#20805;&#20998;&#30340;&#26631;&#20934;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#31616;&#21333;&#65292;&#20294;&#23545;&#26426;&#22120;&#26469;&#35828;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20247;&#21253;&#31995;&#32479;&#29992;&#20110;&#22270;&#20687;&#32858;&#31867;&#65292;&#24182;&#22312;https://github.com/Toloka/crowdclustering &#19978;&#21457;&#24067;&#20854;&#20195;&#30721;&#65292;&#24182;&#20379;&#22823;&#23478;&#20813;&#36153;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21363; Zalando &#30340; FEIDEGGER&#35033;&#23376;&#21644; Toloka &#38795;&#23376;&#25968;&#25454;&#38598;&#65292;&#30830;&#35748;&#21487;&#20197;&#20165;&#36890;&#36807;&#20247;&#21253;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#32858;&#31867;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing allows running simple human intelligence tasks on a large crowd of workers, enabling solving problems for which it is difficult to formulate an algorithm or train a machine learning model in reasonable time. One of such problems is data clustering by an under-specified criterion that is simple for humans, but difficult for machines. In this demonstration paper, we build a crowdsourced system for image clustering and release its code under a free license at https://github.com/Toloka/crowdclustering. Our experiments on two different image datasets, dresses from Zalando's FEIDEGGER and shoes from the Toloka Shoes Dataset, confirm that one can yield meaningful clusters with no machine learning algorithms purely with crowdsourcing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26816;&#32034;&#21644;&#25490;&#21517;&#20004;&#38454;&#27573;&#24037;&#20316;&#27969;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26159;&#29420;&#31435;&#35757;&#32451;&#25110;&#20351;&#29992;&#31616;&#21333;&#30340;&#32423;&#32852;&#31649;&#36947;&#65292;&#25928;&#26524;&#19981;&#20339;&#12290;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#32852;&#21512;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#65292;&#20294;&#20173;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36824;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.14649</link><description>&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21327;&#20316;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cooperative Retriever and Ranker in Deep Recommenders. (arXiv:2206.14649v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26816;&#32034;&#21644;&#25490;&#21517;&#20004;&#38454;&#27573;&#24037;&#20316;&#27969;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26159;&#29420;&#31435;&#35757;&#32451;&#25110;&#20351;&#29992;&#31616;&#21333;&#30340;&#32423;&#32852;&#31649;&#36947;&#65292;&#25928;&#26524;&#19981;&#20339;&#12290;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#32852;&#21512;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#65292;&#20294;&#20173;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36824;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;(DRS)&#22312;&#29616;&#20195;&#32593;&#32476;&#26381;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#22788;&#29702;&#28023;&#37327;&#32593;&#32476;&#20869;&#23481;&#65292;DRS&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#24037;&#20316;&#27969;&#31243;&#65306;&#26816;&#32034;&#21644;&#25490;&#21517;&#65292;&#20197;&#29983;&#25104;&#20854;&#25512;&#33616;&#32467;&#26524;&#12290;&#26816;&#32034;&#22120;&#26088;&#22312;&#39640;&#25928;&#22320;&#20174;&#25972;&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#19968;&#23567;&#32452;&#30456;&#20851;&#20505;&#36873;&#39033;&#65307;&#32780;&#25490;&#21517;&#22120;&#36890;&#24120;&#26356;&#31934;&#30830;&#20294;&#26102;&#38388;&#28040;&#32791;&#26356;&#22823;&#65292;&#24212;&#36827;&#19968;&#27493;&#20174;&#26816;&#32034;&#20505;&#36873;&#39033;&#20013;&#20248;&#21270;&#26368;&#20339;&#39033;&#30446;&#12290;&#20256;&#32479;&#19978;&#65292;&#20004;&#20010;&#32452;&#20214;&#35201;&#20040;&#29420;&#31435;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#31616;&#21333;&#30340;&#32423;&#32852;&#31649;&#36947;&#20869;&#35757;&#32451;&#65292;&#36825;&#23481;&#26131;&#20135;&#29983;&#21512;&#20316;&#25928;&#26524;&#24046;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#24314;&#35758;&#32852;&#21512;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#65292;&#20294;&#20173;&#23384;&#22312;&#35768;&#22810;&#20005;&#37325;&#38480;&#21046;&#65306;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#30340;&#39033;&#20998;&#24067;&#36716;&#31227;&#12289;&#20551;&#38452;&#24615;&#21644;&#25490;&#21517;&#39034;&#24207;&#19981;&#23545;&#40784;&#31561;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep recommender systems (DRS) are intensively applied in modern web services. To deal with the massive web contents, DRS employs a two-stage workflow: retrieval and ranking, to generate its recommendation results. The retriever aims to select a small set of relevant candidates from the entire items with high efficiency; while the ranker, usually more precise but time-consuming, is supposed to further refine the best items from the retrieved candidates. Traditionally, the two components are trained either independently or within a simple cascading pipeline, which is prone to poor collaboration effect. Though some latest works suggested to train retriever and ranker jointly, there still exist many severe limitations: item distribution shift between training and inference, false negative, and misalignment of ranking order. As such, it remains to explore effective collaborations between retriever and ranker.
&lt;/p&gt;</description></item></channel></rss>