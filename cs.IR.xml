<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#26469;&#32531;&#35299;&#24191;&#21578;&#33829;&#38144;&#39046;&#22495;&#20013;&#20301;&#32622;&#20559;&#24046;&#31232;&#30095;&#24615;&#38382;&#39064;&#30340;&#22238;&#24402;EM&#31639;&#27861;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.13931</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#25913;&#36827;&#22312;&#31232;&#30095;&#21644;&#20542;&#26012;&#25968;&#25454;&#38598;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Improving position bias estimation against sparse and skewed dataset with item embedding. (arXiv:2305.13931v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13931
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#26469;&#32531;&#35299;&#24191;&#21578;&#33829;&#38144;&#39046;&#22495;&#20013;&#20301;&#32622;&#20559;&#24046;&#31232;&#30095;&#24615;&#38382;&#39064;&#30340;&#22238;&#24402;EM&#31639;&#27861;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#20272;&#35745;&#20301;&#32622;&#20559;&#24046;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25361;&#25112;&#12290;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28857;&#20987;&#25968;&#25454;&#65288;&#20363;&#22914;&#24191;&#21578;&#23450;&#20301;&#21644;&#25628;&#32034;&#24341;&#25806;&#65289;&#25552;&#20379;&#20102;&#38544;&#21547;&#20294;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20197;&#25913;&#36827;&#20010;&#24615;&#21270;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#28857;&#20987;&#25968;&#25454;&#26412;&#36136;&#19978;&#21253;&#25324;&#21508;&#31181;&#20559;&#24046;&#65292;&#20363;&#22914;&#20301;&#32622;&#20559;&#24046;&#12290;&#28857;&#20987;&#24314;&#27169;&#26088;&#22312;&#21435;&#22122;&#26377;&#20559;&#30340;&#28857;&#20987;&#25968;&#25454;&#24182;&#25552;&#21462;&#21487;&#38752;&#30340;&#20449;&#21495;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#38543;&#26426;&#21270;&#32467;&#26524;&#21644;&#22238;&#24402;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#20301;&#32622;&#20559;&#24046;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#21508;&#31181;&#35266;&#23519;&#20540;&#23545;&#65288;&#39033;&#30446;&#12289;&#20301;&#32622;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#24191;&#21578;&#33829;&#38144;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33829;&#38144;&#20154;&#21592;&#32463;&#24120;&#25353;&#22266;&#23450;&#30340;&#39044;&#23450;&#39034;&#24207;&#26174;&#31034;&#24191;&#21578;&#65292;&#20272;&#35745;&#22240;&#27492;&#32780;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20301;&#32622;&#20559;&#24046;&#20272;&#35745;&#20013;&#30340;&#65288;&#39033;&#30446;&#12289;&#20301;&#32622;&#65289;&#31232;&#30095;&#24615;&#38382;&#39064;&#20316;&#20026;&#26032;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#26469;&#32531;&#35299;&#31232;&#30095;&#38382;&#39064;&#30340;&#22238;&#24402;EM&#31639;&#27861;&#21464;&#20307;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating position bias is a well-known challenge in Learning to rank (L2R). Click data in e-commerce applications, such as advertisement targeting and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently include various biases like position bias. Click modeling is aimed at denoising biases in click data and extracting reliable signals. Result Randomization and Regression Expectation-maximization algorithm have been proposed to solve position bias. Both methods require various pairs of observations (item, position). However, in real cases of advertising, marketers frequently display advertisements in a fixed pre-determined order, and estimation suffers from it. We propose this sparsity of (item, position) in position bias estimation as a novel problem, and we propose a variant of the Regression EM algorithm which utilizes item embeddings to alleviate the issue of the sparsity. With a synthetic dataset, we first evalua
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#26694;&#26550;AutoTSG&#65292;&#20854;&#29305;&#28857;&#26159;&#22522;&#20110;&#26080;&#24207;&#26415;&#35821;&#38598;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;&#29983;&#25104;&#31649;&#36947;&#65292;&#22823;&#22823;&#25918;&#26494;&#20102;&#23545;&#26631;&#35782;&#31526;&#31934;&#30830;&#29983;&#25104;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.13859</link><description>&lt;p&gt;
&#26415;&#35821;&#38598;&#21487;&#20197;&#25104;&#20026;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#30340;&#24378;&#25991;&#26723;&#26631;&#35782;&#31526;
&lt;/p&gt;
&lt;p&gt;
Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search Engines. (arXiv:2305.13859v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#26694;&#26550;AutoTSG&#65292;&#20854;&#29305;&#28857;&#26159;&#22522;&#20110;&#26080;&#24207;&#26415;&#35821;&#38598;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;&#29983;&#25104;&#31649;&#36947;&#65292;&#22823;&#22823;&#25918;&#26494;&#20102;&#23545;&#26631;&#35782;&#31526;&#31934;&#30830;&#29983;&#25104;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#26159;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;Seq2Seq&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#26597;&#35810;&#21487;&#20197;&#30452;&#25509;&#26144;&#23556;&#21040;&#20854;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22240;&#20855;&#26377;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#24615;&#31561;&#20248;&#28857;&#32780;&#21463;&#21040;&#36190;&#25196;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#22312;&#26816;&#32034;&#36136;&#37327;&#19978;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#23545;&#25991;&#26723;&#26631;&#35782;&#31526;&#36827;&#34892;&#31934;&#30830;&#29983;&#25104;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22914;&#26524;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#20219;&#20309;&#19968;&#27493;&#20013;&#23545;&#20854;&#26631;&#35782;&#31526;&#20570;&#20986;&#20102;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#21017;&#30446;&#26631;&#25991;&#26723;&#23558;&#20174;&#26816;&#32034;&#32467;&#26524;&#20013;&#28431;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;AutoTSG(&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#19982;&#26415;&#35821;&#38598;&#29983;&#25104;)&#65292;&#20854;&#29305;&#28857;&#26159;1)&#26080;&#24207;&#22522;&#20110;&#26415;&#35821;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#21644;2)&#22522;&#20110;&#38598;&#21512;&#30340;&#29983;&#25104;&#31649;&#36947;&#12290;&#21033;&#29992;AutoTSG&#65292;&#26415;&#35821;&#38598;&#26631;&#35782;&#31526;&#30340;&#20219;&#20309;&#25490;&#21015;&#37117;&#23558;&#23548;&#33268;&#30456;&#24212;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#20174;&#32780;&#22823;&#22823;&#25918;&#26494;&#20102;&#23545;&#26631;&#35782;&#31526;&#31934;&#30830;&#29983;&#25104;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-regressive search engines emerge as a promising paradigm for next-gen information retrieval systems. These methods work with Seq2Seq models, where each query can be directly mapped to the identifier of its relevant document. As such, they are praised for merits like being end-to-end differentiable. However, auto-regressive search engines also confront challenges in retrieval quality, given the requirement for the exact generation of the document identifier. That's to say, the targeted document will be missed from the retrieval result if a false prediction about its identifier is made in any step of the generation process. In this work, we propose a novel framework, namely AutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is featured by 1) the unordered term-based document identifier and 2) the set-oriented generation pipeline. With AutoTSG, any permutation of the term-set identifier will lead to the retrieval of the corresponding document, thus largely relaxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#26041;&#27861;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.13843</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#19982;&#25361;&#25112;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advances and Challenges of Multi-task Learning Method in Recommender System: A Survey. (arXiv:2305.13843v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#26041;&#27861;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24050;&#32463;&#28044;&#29616;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#25991;&#29486;&#24635;&#32467;&#36825;&#20123;&#25104;&#26524;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31687;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24555;&#36895;&#20102;&#35299;&#36825;&#20010;&#26041;&#21521;&#30340;&#24403;&#21069;&#36827;&#23637;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#21253;&#25324;&#20219;&#21153;&#20851;&#31995;&#21457;&#29616;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#25512;&#33616;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#24320;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning has been widely applied in computational vision, natural language processing and other fields, which has achieved well performance. In recent years, a lot of work about multi-task learning recommender system has been yielded, but there is no previous literature to summarize these works. To bridge this gap, we provide a systematic literature survey about multi-task recommender systems, aiming to help researchers and practitioners quickly understand the current progress in this direction. In this survey, we first introduce the background and the motivation of the multi-task learning-based recommender systems. Then we provide a taxonomy of multi-task learning-based recommendation methods according to the different stages of multi-task learning techniques, which including task relationship discovery, model architecture and optimization strategy. Finally, we raise discussions on the application and promising future directions in this area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#23398;&#20064;&#26032;&#27169;&#24335;&#21644;&#20445;&#30041;&#26087;&#27169;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.13825</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#38548;&#31163;&#30340;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning on Dynamic Graphs via Parameter Isolation. (arXiv:2305.13825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13825
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#23398;&#20064;&#26032;&#27169;&#24335;&#21644;&#20445;&#30041;&#26087;&#27169;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#26032;&#33410;&#28857;&#21644;&#36793;&#20986;&#29616;&#30340;&#21160;&#24577;&#22270;&#12290;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36973;&#36935;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21363;&#20026;&#20197;&#21069;&#30340;&#22270;&#25152;&#23398;&#30340;&#30693;&#35782;&#20250;&#34987;&#26032;&#22270;&#30340;&#26356;&#26032;&#35206;&#30422;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#24182;&#32500;&#25252;&#26087;&#30340;&#27169;&#24335;&#65292;&#20294;&#20351;&#29992;&#30456;&#21516;&#22266;&#23450;&#22823;&#23567;&#30340;&#21442;&#25968;&#38598;&#65292;&#22240;&#27492;&#38754;&#20020;&#20004;&#31181;&#30446;&#26631;&#20043;&#38388;&#30340;&#26681;&#26412;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#65292;&#29992;&#20110;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#22312;&#20110;&#19981;&#21516;&#30340;&#21442;&#25968;&#23545;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#22270;&#27169;&#24335;&#26377;&#36129;&#29486;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#27169;&#22411;&#21442;&#25968;&#20197;&#25345;&#32493;&#23398;&#20064;&#20986;&#29616;&#30340;&#22270;&#27169;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20445;&#23384;&#26410;&#21463;&#24433;&#21709;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25214;&#21040;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world graph learning tasks require handling dynamic graphs where new nodes and edges emerge. Dynamic graph learning methods commonly suffer from the catastrophic forgetting problem, where knowledge learned for previous graphs is overwritten by updates for new graphs. To alleviate the problem, continual graph learning methods are proposed. However, existing continual graph learning methods aim to learn new patterns and maintain old ones with the same set of parameters of fixed size, and thus face a fundamental tradeoff between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN) for continual learning on dynamic graphs that circumvents the tradeoff via parameter isolation and expansion. Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameter
&lt;/p&gt;</description></item><item><title>GenSpectrum&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;GPT-4&#20316;&#20026;LLM&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19968;&#33324;&#22823;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#19982;COVID-19&#30456;&#20851;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.13821</link><description>&lt;p&gt;
GenSpectrum&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GenSpectrum Chat: Data Exploration in Public Health Using Large Language Models. (arXiv:2305.13821v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13821
&lt;/p&gt;
&lt;p&gt;
GenSpectrum&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;GPT-4&#20316;&#20026;LLM&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19968;&#33324;&#22823;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#19982;COVID-19&#30456;&#20851;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;COVID-19&#22823;&#27969;&#34892;&#20984;&#26174;&#20102;&#20351;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#21644;&#31185;&#23398;&#35265;&#35299;&#26131;&#20110;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#19968;&#33324;&#22823;&#20247;&#21644;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#20998;&#20139;&#25968;&#25454;&#21644;&#35265;&#35299;&#30340;&#20808;&#36827;&#26041;&#27861;&#21253;&#25324;&#23450;&#26399;&#26356;&#26032;&#25253;&#21578;&#21644;Web&#20202;&#34920;&#26495;&#12290;&#20294;&#23427;&#20204;&#38754;&#20020;&#25968;&#25454;&#25506;&#32034;&#31616;&#21333;&#24615;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#21033;&#29992;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#31181;&#26435;&#34913;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#8220;GenSpectrum Chat&#8221;&#65288;https://cov-spectrum.org/chat&#65289;&#65292;&#23427;&#20351;&#29992;GPT-4&#20316;&#20026;&#22522;&#30784;LLM&#26469;&#25506;&#32034;SARS-CoV-2&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#12290;&#22312;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;&#30340;500&#20010;&#36755;&#20837;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#20026;453&#20010;&#25552;&#31034;&#25552;&#20379;&#20102;&#27491;&#30830;&#31572;&#26696;&#65307; 13&#20010;&#25552;&#31034;&#25552;&#20379;&#20102;&#38169;&#35823;&#31572;&#26696;&#65292;&#23613;&#31649;34&#20010;&#25552;&#31034;&#30340;&#38382;&#39064;&#22312;&#33539;&#22260;&#20869;&#65292;&#20294;&#23427;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#26469;&#33258;10&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#36755;&#20837;&#65292;&#23613;&#31649;GPT-4&#27809;&#26377;&#36827;&#34892;&#22810;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#20173;&#20026;&#26576;&#20123;&#26597;&#35810;&#25552;&#20379;&#20102;&#27491;&#30830;&#31572;&#26696;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#26426;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19968;&#33324;&#22823;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#30452;&#35266;&#21644;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#19982;COVID-19&#30456;&#20851;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: The COVID-19 pandemic highlighted the importance of making epidemiological data and scientific insights easily accessible and explorable for public health agencies, the general public, and researchers. State-of-the-art approaches for sharing data and insights included regularly updated reports and web dashboards. However, they face a trade-off between the simplicity and flexibility of data exploration. With the capabilities of recent large language models (LLMs) such as GPT-4, this trade-off can be overcome.  Results: We developed the chatbot "GenSpectrum Chat" (https://cov-spectrum.org/chat) which uses GPT-4 as the underlying large language model (LLM) to explore SARS-CoV-2 genomic sequencing data. Out of 500 inputs from real-world users, the chatbot provided a correct answer for 453 prompts; an incorrect answer for 13 prompts, and no answer although the question was within scope for 34 prompts. We also tested the chatbot with inputs from 10 different languages, and desp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#35282;&#24230;&#23545;&#27969;&#34892;&#30340;&#22810;&#26679;&#24615;&#25512;&#33616;&#30446;&#26631;&#8212;&#8212;&#21015;&#34920;&#20869;&#36317;&#31163;&#21644;&#31163;&#25955;&#24230;&#36827;&#34892;&#20851;&#38190;&#30340;&#37325;&#26032;&#23457;&#35270;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#28508;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2305.13801</link><description>&lt;p&gt;
&#23545;&#21015;&#34920;&#20869;&#36317;&#31163;&#21644;&#31163;&#25955;&#24230;&#30340;&#20851;&#38190;&#37325;&#26032;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
A Critical Reexamination of Intra-List Distance and Dispersion. (arXiv:2305.13801v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#35282;&#24230;&#23545;&#27969;&#34892;&#30340;&#22810;&#26679;&#24615;&#25512;&#33616;&#30446;&#26631;&#8212;&#8212;&#21015;&#34920;&#20869;&#36317;&#31163;&#21644;&#31163;&#25955;&#24230;&#36827;&#34892;&#20851;&#38190;&#30340;&#37325;&#26032;&#23457;&#35270;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#28508;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#32467;&#26524;&#30340;&#22810;&#26679;&#21270;&#26159;&#24212;&#23545;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#26679;&#21270;&#25512;&#33616;&#20013;&#65292;&#23588;&#20854;&#37325;&#35201;&#30340;&#26159;&#23450;&#20041;&#21644;&#20248;&#21270;&#36866;&#24403;&#30340;&#22810;&#26679;&#24615;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#27969;&#34892;&#30340;&#22810;&#26679;&#24615;&#30446;&#26631;&#20043;&#19968;&#8212;&#8212;&#21015;&#34920;&#20869;&#36317;&#31163;&#65288;ILD&#65289;&#65292;&#23450;&#20041;&#20026;&#25152;&#36873;&#39033;&#30446;&#20043;&#38388;&#30340;&#24179;&#22343;&#25104;&#23545;&#36317;&#31163;&#65292;&#20197;&#21450;&#19968;&#31181;&#31867;&#20284;&#20294;&#36739;&#19981;&#20026;&#20154;&#30693;&#30340;&#30446;&#26631;&#8212;&#8212;&#31163;&#25955;&#24230;&#65292;&#23427;&#26159;&#26368;&#23567;&#25104;&#23545;&#36317;&#31163;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#31616;&#21333;&#21644;&#28789;&#27963;&#24615;&#65292;ILD&#21644;&#31163;&#25955;&#24230;&#24050;&#22312;&#20247;&#22810;&#22810;&#26679;&#21270;&#25512;&#33616;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23454;&#38469;&#19978;&#19981;&#30693;&#36947;&#23427;&#20204;&#20559;&#22909;&#21738;&#20123;&#31867;&#22411;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#35282;&#24230;&#23545;ILD&#21644;&#31163;&#25955;&#24230;&#36827;&#34892;&#20102;&#20851;&#38190;&#30340;&#37325;&#26032;&#23457;&#35270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#30446;&#26631;&#28508;&#22312;&#30340;&#32570;&#28857;&#65306;ILD&#21487;&#33021;&#36873;&#25321;&#38750;&#24120;&#25509;&#36817;&#30340;&#37325;&#22797;&#39033;&#30446;&#65292;&#32780;&#31163;&#25955;&#24230;&#21487;&#33021;&#23548;&#33268;&#37325;&#22797;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversification of recommendation results is a promising approach for coping with the uncertainty associated with users' information needs. Of particular importance in diversified recommendation is to define and optimize an appropriate diversity objective. In this study, we revisit the most popular diversity objective called intra-list distance (ILD), defined as the average pairwise distance between selected items, and a similar but lesser known objective called dispersion, which is the minimum pairwise distance. Owing to their simplicity and flexibility, ILD and dispersion have been used in a plethora of diversified recommendation research. Nevertheless, we do not actually know what kind of items are preferred by them.  We present a critical reexamination of ILD and dispersion from theoretical and experimental perspectives. Our theoretical results reveal that these objectives have potential drawbacks: ILD may select duplicate items that are very close to each other, whereas dispersion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Recformer&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#29992;&#25143;&#21916;&#22909;&#21644;&#39033;&#30446;&#29305;&#24449;&#24314;&#27169;&#20026;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#21644;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21452;&#21521;Transformer&#26469;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#65292;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13731</link><description>&lt;p&gt;
&#25991;&#26412;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#65306;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#30340;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Text Is All You Need: Learning Language Representations for Sequential Recommendation. (arXiv:2305.13731v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Recformer&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#29992;&#25143;&#21916;&#22909;&#21644;&#39033;&#30446;&#29305;&#24449;&#24314;&#27169;&#20026;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#21644;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21452;&#21521;Transformer&#26469;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#65292;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#26088;&#22312;&#20174;&#21382;&#21490;&#20132;&#20114;&#20013;&#24314;&#27169;&#21160;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#26126;&#30830;&#30340;&#39033;&#30446;ID&#25110;&#19968;&#33324;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#29702;&#35299;&#29992;&#25143;&#21916;&#22909;&#12290;&#23613;&#31649;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#24314;&#27169;&#20919;&#21551;&#21160;&#39033;&#30446;&#25110;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#29992;&#25143;&#21916;&#22909;&#21644;&#39033;&#30446;&#29305;&#24449;&#24314;&#27169;&#20026;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#21644;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Recformer&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#23398;&#20064;&#24207;&#21015;&#25512;&#33616;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23637;&#24179;&#30001;&#25991;&#26412;&#25551;&#36848;&#30340;&#39033;&#30446;&#38190;&#20540;&#23646;&#24615;&#65292;&#23558;&#39033;&#30446;&#20316;&#20026;&#8220;&#21477;&#23376;&#8221;&#65288;&#21333;&#35789;&#24207;&#21015;&#65289;&#26469;&#32534;&#20889;&#65292;&#20197;&#20415;&#29992;&#25143;&#30340;&#39033;&#30446;&#24207;&#21015;&#25104;&#20026;&#21477;&#23376;&#24207;&#21015;&#12290;&#20026;&#25512;&#33616;&#65292;Recformer&#34987;&#35757;&#32451;&#20197;&#29702;&#35299;&#8220;&#21477;&#23376;&#8221;&#24207;&#21015;&#24182;&#26816;&#32034;&#19979;&#19968;&#20010;&#8220;&#21477;&#23376;&#8221;&#12290;&#20026;&#20102;&#32534;&#30721;&#39033;&#30446;&#24207;&#21015;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#21521;Transformer&#65292;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;Recformer&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a "sentence" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the "sentence" sequence and retrieve the next "sentence". To encode item sequences, we design a bi-directional T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13729</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#22312;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker. (arXiv:2305.13729v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#25490;&#22120;&#26159;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#35780;&#20998;&#19979;&#23545;&#26816;&#32034;&#30340;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#19982;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21516;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#20855;&#26377;&#20248;&#24322;&#32467;&#26524;&#30340;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#12290;&#34429;&#28982; LLM &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25552;&#31034;&#35821;&#65292;&#20294;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#25552;&#31034;&#35821;&#30340;&#24433;&#21709;&#21644;&#20248;&#21270;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#38500;&#20102;&#24378;&#35843;&#20248;&#21270;&#23545;&#38646;&#26679;&#26412;&#37325;&#25490;&#22120;&#30340;&#24433;&#21709;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#32422;&#26463;&#25552;&#31034;&#29983;&#25104;&#65288;Co-Prompt&#65289;&#65292;&#36890;&#36807;&#20272;&#31639;&#26368;&#20339;&#25490;&#24207;&#26469;&#24341;&#23548; PLM &#29983;&#25104;&#30340;&#25991;&#26412;&#26397;&#21521;&#26368;&#20248;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-Prompt &#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#37325;&#25490;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Co-Prompt &#29983;&#25104;&#30340;&#25552;&#31034;&#26356;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans aga
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20197;&#26816;&#32034;&#26041;&#24335;&#36827;&#34892;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#34920;&#31034;&#20026;&#26597;&#35810;&#65292;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#24453;&#26816;&#32034;&#30340;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;BM25&#30340;&#26816;&#32034;&#22120;&#36827;&#34892;&#25512;&#33616;&#12290;&#30456;&#27604;&#20110;&#20351;&#29992;&#22797;&#26434;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#20934;&#32447;&#65292;&#35813;&#26041;&#27861;&#31616;&#21333;&#19988;&#26356;&#20855;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13725</link><description>&lt;p&gt;
&#20197;&#26816;&#32034;&#26041;&#24335;&#36827;&#34892;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#65306;&#19968;&#20010;&#31616;&#21333;&#12289;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Conversational Recommendation as Retrieval: A Simple, Strong Baseline. (arXiv:2305.13725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20197;&#26816;&#32034;&#26041;&#24335;&#36827;&#34892;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#34920;&#31034;&#20026;&#26597;&#35810;&#65292;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#24453;&#26816;&#32034;&#30340;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;BM25&#30340;&#26816;&#32034;&#22120;&#36827;&#34892;&#25512;&#33616;&#12290;&#30456;&#27604;&#20110;&#20351;&#29992;&#22797;&#26434;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#20934;&#32447;&#65292;&#35813;&#26041;&#27861;&#31616;&#21333;&#19988;&#26356;&#20855;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#21521;&#29992;&#25143;&#25512;&#33616;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#23545;&#35805;&#25552;&#20379;&#30340;&#20449;&#21495;&#12290;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26174;&#24335;&#30340;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#29289;&#21697;&#21644;&#23646;&#24615;&#30340;&#29702;&#35299;&#65292;&#36825;&#24456;&#38590;&#25193;&#23637;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#23558;&#23545;&#35805;&#34920;&#31034;&#20026;&#26597;&#35810;&#65292;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#24453;&#26816;&#32034;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29992;&#20110;&#26816;&#32034;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#23545;&#35805;&#20449;&#24687;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#22522;&#20110;BM25&#30340;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20219;&#21153;&#35774;&#32622;&#22312;&#27969;&#34892;&#30340;CRS&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#22797;&#26434;&#30340;&#12289;&#20351;&#29992;&#22797;&#26434;&#22806;&#37096;&#30693;&#35782;&#30340;&#22522;&#20934;&#32447;&#30456;&#27604;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#24314;&#27169;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23545;&#25239;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommendation systems (CRS) aim to recommend suitable items to users through natural language conversation. However, most CRS approaches do not effectively utilize the signal provided by these conversations. They rely heavily on explicit external knowledge e.g., knowledge graphs to augment the models' understanding of the items and attributes, which is quite hard to scale. To alleviate this, we propose an alternative information retrieval (IR)-styled approach to the CRS item recommendation task, where we represent conversations as queries and items as documents to be retrieved. We expand the document representation used for retrieval with conversations from the training set. With a simple BM25-based retriever, we show that our task formulation compares favorably with much more complex baselines using complex external knowledge on a popular CRS benchmark. We demonstrate further improvements using user-centric modeling and data augmentation to counter the cold start probl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAS2S&#30340;&#22810;&#27880;&#24847;&#21147;Seq2Seq&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25552;&#38382;&#20197;&#28548;&#28165;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#33719;&#21462;&#20013;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13690</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#20449;&#24687;&#35831;&#27714;&#20013;&#30340;&#28548;&#28165;&#38382;&#39064;&#25552;&#38382;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Asking Clarification Questions for Information Seeking on Task-Oriented Dialogues. (arXiv:2305.13690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAS2S&#30340;&#22810;&#27880;&#24847;&#21147;Seq2Seq&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25552;&#38382;&#20197;&#28548;&#28165;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#33719;&#21462;&#20013;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#29305;&#23450;&#20219;&#21153;&#30340;&#26381;&#21153;&#12290;&#36825;&#31181;&#31995;&#32479;&#30340;&#29992;&#25143;&#36890;&#24120;&#19981;&#30693;&#36947;&#20182;&#20204;&#27491;&#22312;&#23436;&#25104;&#30340;&#20219;&#21153;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#38656;&#35201;&#23547;&#27714;&#20851;&#20110;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#25552;&#20379;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#32467;&#26524;&#65292;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#28508;&#22312;&#38382;&#39064;&#65306;1) &#29992;&#25143;&#26080;&#27861;&#22312;&#35831;&#27714;&#20013;&#25551;&#36848;&#20182;&#20204;&#22797;&#26434;&#30340;&#20449;&#24687;&#38656;&#27714;&#65307;2) &#31995;&#32479;&#23545;&#29992;&#25143;&#30340;&#20449;&#24687;&#23384;&#22312;&#27169;&#31946;/&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27880;&#24847;&#21147;Seq2Seq&#32593;&#32476;&#65292;&#21629;&#21517;&#20026;MAS2S&#65292;&#23427;&#21487;&#20197;&#25552;&#38382;&#20197;&#28548;&#28165;&#29992;&#25143;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#20013;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;100k&#20010;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#23545;&#35805;&#65292;&#36825;&#20123;&#23545;&#35805;&#26159;&#20844;&#24320;&#30340;\footnote{ &#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;\href{http://link/to/dataset}{http://link/to/dataset}&#25214;&#21040;}&#12290;&#22312;&#25193;&#23637;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#20449;&#24687;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems aim at providing users with task-specific services. Users of such systems often do not know all the information about the task they are trying to accomplish, requiring them to seek information about the task. To provide accurate and personalized task-oriented information seeking results, task-oriented dialogue systems need to address two potential issues: 1) users' inability to describe their complex information needs in their requests; and 2) ambiguous/missing information the system has about the users. In this paper, we propose a new Multi-Attention Seq2Seq Network, named MAS2S, which can ask questions to clarify the user's information needs and the user's profile in task-oriented information seeking. We also extend an existing dataset for task-oriented information seeking, leading to the \ourdataset which contains about 100k task-oriented information seeking dialogues that are made publicly available\footnote{Dataset and code is available at \href{http
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#20013;&#39044;&#25490;&#21517;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36820;&#22238;&#26368;&#20248;&#30340;&#26080;&#24207;&#36873;&#25321;&#38598;&#65292;&#20197;&#20351;&#24471;&#21518;&#32493;&#30340;&#25490;&#21517;&#26356;&#21152;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#26356;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.13647</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#39044;&#25490;&#21517;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Role of Pre-ranking in Large-scale E-Commerce Searching System. (arXiv:2305.13647v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#20013;&#39044;&#25490;&#21517;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36820;&#22238;&#26368;&#20248;&#30340;&#26080;&#24207;&#36873;&#25321;&#38598;&#65292;&#20197;&#20351;&#24471;&#21518;&#32493;&#30340;&#25490;&#21517;&#26356;&#21152;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#26356;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#26368;&#20559;&#29233;&#30340;&#29289;&#21697;&#65288;&#20363;&#22914;&#20135;&#21697;&#65289;&#12290;&#30001;&#20110;&#28023;&#37327;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#21709;&#24212;&#26102;&#38388;&#65292;&#20856;&#22411;&#30340;&#24037;&#19994;&#25490;&#21517;&#31995;&#32479;&#30001;&#19977;&#20010;&#25110;&#26356;&#22810;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#21253;&#25324;&#21305;&#37197;&#12289;&#39044;&#25490;&#21517;&#21644;&#25490;&#21517;&#12290;&#39044;&#25490;&#21517;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#20010;&#36855;&#20320;&#25490;&#21517;&#27169;&#22359;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#27604;&#25490;&#21517;&#27169;&#22411;&#26356;&#22810;&#30340;&#29289;&#21697;&#12290;&#29616;&#26377;&#30740;&#31350;&#30528;&#37325;&#20110;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#27169;&#20223;&#25490;&#21517;&#27169;&#22411;&#12290;&#39044;&#25490;&#21517;&#27169;&#22411;&#30340;&#25351;&#26631;&#20351;&#29992;&#31163;&#32447;&#35780;&#20272;&#30340; ROC &#26354;&#32447;&#19979;&#38754;&#31215;(AUC) &#26469;&#36319;&#36394;&#25490;&#21517;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25351;&#26631;&#19982;&#22312;&#32447;A / B&#27979;&#35797;&#22312;&#23454;&#36341;&#20013;&#19981;&#19968;&#33268;&#65292;&#22240;&#27492;&#24037;&#31243;&#24072;&#24517;&#39035;&#36827;&#34892;&#26114;&#36149;&#30340;&#22312;&#32447;&#27979;&#35797;&#25165;&#33021;&#24471;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#35770;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#39044;&#25490;&#21517;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#39044;&#25490;&#21517;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36820;&#22238;&#26368;&#20248;&#30340;&#26080;&#24207;&#36873;&#25321;&#38598;&#65292;&#36825;&#20351;&#24471;&#21518;&#32493;&#30340;&#25490;&#21517;&#26356;&#21152;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#26356;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce search systems such as Taobao Search, the largest e-commerce searching system in China, aim at providing users with the most preferred items (e.g., products). Due to the massive data and limited time for response, a typical industrial ranking system consists of three or more modules, including matching, pre-ranking, and ranking. The pre-ranking is widely considered a mini-ranking module, as it needs to rank hundreds of times more items than the ranking under limited latency. Existing researches focus on building a lighter model that imitates the ranking model. As such, the metric of a pre-ranking model follows the ranking model using Area Under ROC (AUC) for offline evaluation. However, such a metric is inconsistent with online A/B tests in practice, so engineers have to perform costly online tests to reach a convincing conclusion. In our work, we rethink the role of the pre-ranking. We argue that the primary goal of the pre-ranking stage is to return an optimal unordered se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13631</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#20869;&#23481;&#22270;&#20687;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23454;&#38469;&#25628;&#32034;&#24212;&#29992;&#20013;&#23454;&#29616;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#38656;&#35201;&#22312;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#23454;&#20307;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse tex
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#38382;&#39064;&#30340;&#20135;&#29983;&#37096;&#20998;&#21407;&#22240;&#26159;&#20351;&#29992;&#20102;&#20302;&#32500;&#24230;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#24341;&#36215;&#65292;&#28857;&#31215;&#27169;&#22411;&#23545;&#20110;&#34920;&#29616;&#33021;&#21147;&#26377;&#38480;&#65292;&#38656;&#35201;&#36275;&#22815;&#39640;&#30340;&#32500;&#24230;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#12289;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#65307;&#20302;&#32500;&#24230;&#23548;&#33268;&#27969;&#34892;&#24230;&#20559;&#24046;&#24102;&#26469;&#30340;&#36129;&#29486;&#25193;&#22823;&#20102;&#27969;&#34892;&#21644;&#38271;&#23614;&#29289;&#21697;&#22312;&#25490;&#21517;&#20301;&#32622;&#19978;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.13597</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013; &#8220;&#20302;&#8221; &#32500;&#24230;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Curse of "Low" Dimensionality in Recommender Systems. (arXiv:2305.13597v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13597
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#38382;&#39064;&#30340;&#20135;&#29983;&#37096;&#20998;&#21407;&#22240;&#26159;&#20351;&#29992;&#20102;&#20302;&#32500;&#24230;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#24341;&#36215;&#65292;&#28857;&#31215;&#27169;&#22411;&#23545;&#20110;&#34920;&#29616;&#33021;&#21147;&#26377;&#38480;&#65292;&#38656;&#35201;&#36275;&#22815;&#39640;&#30340;&#32500;&#24230;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#12289;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#65307;&#20302;&#32500;&#24230;&#23548;&#33268;&#27969;&#34892;&#24230;&#20559;&#24046;&#24102;&#26469;&#30340;&#36129;&#29486;&#25193;&#22823;&#20102;&#27969;&#34892;&#21644;&#38271;&#23614;&#29289;&#21697;&#22312;&#25490;&#21517;&#20301;&#32622;&#19978;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#36136;&#37327;&#19981;&#20165;&#20165;&#19982;&#20934;&#30830;&#24230;&#26377;&#20851;&#65292;&#36824;&#21253;&#25324;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#26041;&#38754;&#12290;&#26412;&#25991;&#35748;&#20026;&#25512;&#33616;&#31995;&#32479;&#20013;&#35768;&#22810;&#38382;&#39064;&#30340;&#20135;&#29983;&#37096;&#20998;&#21407;&#22240;&#26159;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#20302;&#32500;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#30697;&#38453;&#20998;&#35299;&#31561;&#28857;&#31215;&#27169;&#22411;&#26102;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#23545;&#20110;&#23884;&#20837;&#21521;&#37327;&#65292;&#32500;&#24230;&#38656;&#35201;&#36275;&#22815;&#39640;&#25165;&#33021;&#36798;&#21040;&#22810;&#26679;&#21270;&#12289;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#36824;&#23545;&#28857;&#31215;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#29289;&#21697;&#22240;&#23376;&#30340;&#32500;&#24230;&#19978;&#65292;&#28857;&#31215;&#27169;&#22411;&#33021;&#22815;&#34920;&#36798;&#30340;&#21487;&#33021;&#25490;&#21517;&#25968;&#26159;&#25351;&#25968;&#32423;&#32465;&#23450;&#30340;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20302;&#32500;&#24230;&#23545;&#20110;&#27969;&#34892;&#24230;&#20559;&#24046;&#24102;&#26469;&#30340;&#36129;&#29486;&#25193;&#22823;&#20102;&#27969;&#34892;&#21644;&#38271;&#23614;&#29289;&#21697;&#22312;&#25490;&#21517;&#20301;&#32622;&#19978;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond accuracy, there are a variety of aspects to the quality of recommender systems, such as diversity, fairness, and robustness. We argue that many of the prevalent problems in recommender systems are partly due to low-dimensionality of user and item embeddings, particularly when dot-product models, such as matrix factorization, are used.  In this study, we showcase empirical evidence suggesting the necessity of sufficient dimensionality for user/item embeddings to achieve diverse, fair, and robust recommendation. We then present theoretical analyses of the expressive power of dot-product models. Our theoretical results demonstrate that the number of possible rankings expressible under dot-product models is exponentially bounded by the dimension of item factors. We empirically found that the low-dimensionality contributes to a popularity bias, widening the gap between the rank positions of popular and long-tail items; we also give a theoretical justification for this phenomenon.
&lt;/p&gt;</description></item><item><title>DSI&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#34429;&#28982;&#20854;&#33021;&#22815;&#29983;&#25104;&#25991;&#20214;&#26631;&#35782;&#31526;&#30340;&#25490;&#24207;&#21015;&#34920;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#20851;&#25991;&#26723;&#21644;&#38543;&#26426;&#25991;&#26723;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#20197;&#25552;&#39640;&#20854;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02073</link><description>&lt;p&gt;
&#20102;&#35299;&#25991;&#26412;&#26816;&#32034;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Differential Search Index for Text Retrieval. (arXiv:2305.02073v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02073
&lt;/p&gt;
&lt;p&gt;
DSI&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#34429;&#28982;&#20854;&#33021;&#22815;&#29983;&#25104;&#25991;&#20214;&#26631;&#35782;&#31526;&#30340;&#25490;&#24207;&#21015;&#34920;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#20851;&#25991;&#26723;&#21644;&#38543;&#26426;&#25991;&#26723;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#20197;&#25552;&#39640;&#20854;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;&#65288;DSI&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#20989;&#25968;&#23545;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#19968;&#20010;&#25991;&#20214;&#26631;&#35782;&#31526;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26550;&#26500;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#20173;&#38656;&#20102;&#35299;DSI&#20855;&#22791;&#22522;&#26412;&#32034;&#24341;&#21644;&#26816;&#32034;&#33021;&#21147;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#26816;&#39564;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;IR&#26694;&#26550;&#24212;&#20855;&#22791;&#30340;&#19977;&#20010;&#37325;&#35201;&#33021;&#21147;&#65292;&#21363;&#25490;&#20182;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#30456;&#20851;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;DSI&#22312;&#35760;&#24518;&#20174;&#20266;&#26597;&#35810;&#21040;&#25991;&#26723;&#26631;&#35782;&#31526;&#30340;&#21333;&#21521;&#26144;&#23556;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#31243;&#24230;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#20851;&#25991;&#26723;&#21644;&#38543;&#26426;&#25991;&#26723;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#20174;&#32780;&#23545;&#20854;&#26816;&#32034;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#32780;&#19981;&#25913;&#21464;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Differentiable Search Index (DSI) is a novel information retrieval (IR) framework that utilizes a differentiable function to generate a sorted list of document identifiers in response to a given query. However, due to the black-box nature of the end-to-end neural architecture, it remains to be understood to what extent DSI possesses the basic indexing and retrieval abilities. To mitigate this gap, in this study, we define and examine three important abilities that a functioning IR framework should possess, namely, exclusivity, completeness, and relevance ordering. Our analytical experimentation shows that while DSI demonstrates proficiency in memorizing the unidirectional mapping from pseudo queries to document identifiers, it falls short in distinguishing relevant documents from random ones, thereby negatively impacting its retrieval effectiveness. To address this issue, we propose a multi-task distillation approach to enhance the retrieval quality without altering the structure o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;AdaMS&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#21442;&#25968;&#26367;&#25442;&#36229;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#25552;&#39640;&#22768;&#23398;&#21333;&#35789;&#36776;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14564</link><description>&lt;p&gt;
AdaMS: &#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#22312;&#22768;&#23398;&#21333;&#35789;&#36776;&#21035;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination. (arXiv:2210.14564v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;AdaMS&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#21442;&#25968;&#26367;&#25442;&#36229;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#25552;&#39640;&#22768;&#23398;&#21333;&#35789;&#36776;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#25439;&#22833;&#20989;&#25968;&#37319;&#29992;&#23545;&#25968;&#21644;&#25351;&#25968;&#24418;&#24335;&#65292;&#38656;&#35201;&#20351;&#29992;&#36793;&#30028;&#21644;&#23610;&#24230;&#31561;&#36229;&#21442;&#25968;&#12290;&#30001;&#20110;&#27599;&#20010;&#25968;&#25454;&#31867;&#37117;&#20855;&#26377;&#22266;&#26377;&#29305;&#24449;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36793;&#30028;&#26469;&#23398;&#20064;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#27809;&#26377;&#20851;&#20110;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#35748;&#20026;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#35813;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36793;&#30028;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AdaMS&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36793;&#30028;&#21644;&#23610;&#24230;&#30340;&#36229;&#21442;&#25968;&#34987;&#26367;&#25442;&#20026;&#27599;&#20010;&#31867;&#30340;&#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#21326;&#23572;&#34903;&#26085;&#25253;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21333;&#35789;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#34920;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent loss functions in deep metric learning are expressed with logarithmic and exponential forms, and they involve margin and scale as essential hyper-parameters. Since each data class has an intrinsic characteristic, several previous works have tried to learn embedding space close to the real distribution by introducing adaptive margins. However, there was no work on adaptive scales at all. We argue that both margin and scale should be adaptively adjustable during the training. In this paper, we propose a method called Adaptive Margin and Scale (AdaMS), where hyper-parameters of margin and scale are replaced with learnable parameters of adaptive margins and adaptive scales for each class. Our method is evaluated on Wall Street Journal dataset, and we achieve outperforming results for word discrimination tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20809;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32034;&#24341;&#39640;&#25928;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#21644;LITE&#21484;&#22238;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.01371</link><description>&lt;p&gt;
&#20809;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Efficiency and Generalization of Light Hybrid Retrievers. (arXiv:2210.01371v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20809;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32034;&#24341;&#39640;&#25928;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#21644;LITE&#21484;&#22238;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21484;&#22238;&#22120;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#21484;&#22238;&#22120;&#30340;&#20248;&#28857;&#12290;&#20197;&#21069;&#30340;&#28151;&#21512;&#21484;&#22238;&#22120;&#21033;&#29992;&#32034;&#24341;&#23494;&#38598;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#8220;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#32034;&#24341;&#20869;&#23384;&#8221;&#65311;&#21463;&#27492;&#38382;&#39064;&#39537;&#21160;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#32034;&#24341;&#39640;&#25928;&#30340;&#23494;&#38598;&#21484;&#22238;&#22120;&#65288;&#21363;DrBoost&#65289;&#65292;&#24182;&#24341;&#20837;LITE&#21484;&#22238;&#22120;&#36827;&#19968;&#27493;&#20943;&#23569;DrBoost&#30340;&#20869;&#23384;&#12290;LITE&#21516;&#26102;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#24182;&#23558;BM25&#31232;&#30095;&#21484;&#22238;&#22120;&#19982;LITE&#25110;DrBoost&#30456;&#32467;&#21512;&#24418;&#25104;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#12290;&#25105;&#20204;&#30340;Hybrid-LITE&#21484;&#22238;&#22120;&#22312;&#20445;&#25345;BM25&#21644;DPR&#28151;&#21512;&#21484;&#22238;&#22120;98.0&#65285;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#33410;&#30465;&#20102;13&#20493;&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#23545;&#25239;&#24615;&#25915;&#20987;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#23481;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36731;&#28151;&#21512;&#21484;&#22238;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#21484;&#22238;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study "Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance"? Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge distillation from DrBoost. Then, we integrate BM25, a sparse retriever, with either LITE or DrBoost to form light hybrid retrievers. Our Hybrid-LITE retriever saves 13X memory while maintaining 98.0% performance of the hybrid retriever of BM25 and DPR. In addition, we study the generalization capacity of our light hybrid retrievers on out-of-domain dataset and a set of adversarial attacks datasets. Experiments showcase that light hybrid retrievers achieve better generalization performance than individ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#21453;&#20107;&#23454;&#23398;&#20064;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20272;&#35745;&#25512;&#33616;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#30830;&#23450;&#26377;&#21161;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2208.06746</link><description>&lt;p&gt;
&#22240;&#26524;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#30340;&#23545;&#27604;&#21453;&#20107;&#23454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Counterfactual Learning for Causality-aware Interpretable Recommender Systems. (arXiv:2208.06746v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#21453;&#20107;&#23454;&#23398;&#20064;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20272;&#35745;&#25512;&#33616;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#30830;&#23450;&#26377;&#21161;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#19979;&#29983;&#25104;&#25512;&#33616;&#30340;&#30740;&#31350;&#26377;&#25152;&#22686;&#21152;&#65292;&#25512;&#33616;&#34987;&#35270;&#20026;&#19968;&#31181;&#22788;&#29702;&#65292;&#26088;&#22312;&#21152;&#24378;&#25105;&#20204;&#23545;&#25512;&#33616;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#20801;&#35768;&#30830;&#23450;&#26377;&#21161;&#20110;&#35813;&#24433;&#21709;&#30340;&#22240;&#32032;&#12290;&#35768;&#22810;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#19987;&#27880;&#20110;&#20351;&#29992;&#20542;&#21521;&#20998;&#25968;&#65292;&#36825;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#65292;&#20294;&#21487;&#33021;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#24046;&#24322;&#12290;&#20854;&#20182;&#30740;&#31350;&#21017;&#25552;&#20986;&#20351;&#29992;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#26080;&#20559;&#25968;&#25454;&#65292;&#19981;&#36807;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#28385;&#36275;&#19968;&#23450;&#30340;&#20551;&#35774;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38590;&#20197;&#28385;&#36275;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#25512;&#33616;&#30340;&#22240;&#26524;&#24863;&#30693;&#35299;&#37322;&#65292;&#24182;&#34920;&#26126;&#24213;&#23618;&#30340;&#26292;&#38706;&#26426;&#21046;&#21487;&#20197;&#20559;&#21521;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;&#35266;&#27979;&#21453;&#39304;&#12290;&#37492;&#20110;&#28151;&#28102;&#22240;&#32032;&#21487;&#33021;&#26080;&#27861;&#27979;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;S&#23545;&#21453;&#20107;&#23454;&#23398;&#20064;&#65288;CCL&#65289;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20272;&#35745;&#25512;&#33616;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#30830;&#23450;&#26377;&#21161;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent surge in the study of generating recommendations within the framework of causal inference, with the recommendation being treated as a treatment. This approach enhances our understanding of how recommendations influence user behaviour and allows for identification of the factors that contribute to this impact. Many researchers in the field of causal inference for recommender systems have focused on using propensity scores, which can reduce bias but may also introduce additional variance. Other studies have proposed the use of unbiased data from randomized controlled trials, though this approach requires certain assumptions that may be difficult to satisfy in practice. In this paper, we first explore the causality-aware interpretation of recommendations and show that the underlying exposure mechanism can bias the maximum likelihood estimation (MLE) of observational feedback. Given that confounders may be inaccessible for measurement, we propose using contrastive S
&lt;/p&gt;</description></item></channel></rss>