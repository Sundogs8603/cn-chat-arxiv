<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#65288;PEEL&#65289;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#35774;&#22791;&#21644;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#19982;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;&#65292;&#24182;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#23884;&#20837;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.10532</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Personalized Elastic Embedding Learning for On-Device Recommendation. (arXiv:2306.10532v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#65288;PEEL&#65289;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#35774;&#22791;&#21644;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#19982;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;&#65292;&#24182;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#23884;&#20837;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#24182;&#20943;&#23569;&#32593;&#32476;&#24310;&#36831;&#65292;&#36817;&#24180;&#26469;&#19968;&#30452;&#26377;&#23558;&#22312;&#20113;&#31471;&#35757;&#32451;&#30340;&#33219;&#32959;&#30340;&#25512;&#33616;&#27169;&#22411;&#21387;&#32553;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#32039;&#20945;&#30340;&#25512;&#33616;&#22120;&#27169;&#22411;&#20197;&#36827;&#34892;&#23454;&#26102;&#25512;&#33616;&#30340;&#36235;&#21183;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#24573;&#35270;&#20102;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#29992;&#25143;&#24322;&#36136;&#24615;&#12290;&#23427;&#20204;&#35201;&#20040;&#35201;&#27714;&#25152;&#26377;&#35774;&#22791;&#20849;&#20139;&#30456;&#21516;&#30340;&#21387;&#32553;&#27169;&#22411;&#65292;&#35201;&#20040;&#35201;&#27714;&#20855;&#26377;&#30456;&#21516;&#36164;&#28304;&#39044;&#31639;&#30340;&#35774;&#22791;&#20849;&#20139;&#30456;&#21516;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#30456;&#21516;&#35774;&#22791;&#30340;&#29992;&#25143;&#21487;&#33021;&#20063;&#20855;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20551;&#35774;&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;&#22120;&#21487;&#29992;&#36164;&#28304;&#65288;&#22914;&#20869;&#23384;&#65289;&#26159;&#24658;&#23450;&#30340;&#65292;&#36825;&#19982;&#29616;&#23454;&#24773;&#20917;&#19981;&#31526;&#12290;&#37492;&#20110;&#35774;&#22791;&#21644;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#65288;PEEL&#65289;&#65292;&#35813;&#26694;&#26550;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#20026;&#20855;&#26377;&#19981;&#21516;&#20869;&#23384;&#39044;&#31639;&#30340;&#35774;&#22791;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address privacy concerns and reduce network latency, there has been a recent trend of compressing cumbersome recommendation models trained on the cloud and deploying compact recommender models to resource-limited devices for real-time recommendation. Existing solutions generally overlook device heterogeneity and user heterogeneity. They either require all devices to share the same compressed model or the devices with the same resource budget to share the same model. However, even users with the same devices may have different preferences. In addition, they assume the available resources (e.g., memory) for the recommender on a device are constant, which is not reflective of reality. In light of device and user heterogeneities as well as dynamic resource constraints, this paper proposes a Personalized Elastic Embedding Learning framework (PEEL) for on-device recommendation, which generates personalized embeddings for devices with various memory budgets in once-for-all manner, efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.11755</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#65306;&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25512;&#33616;&#25552;&#20379;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#26159;&#23454;&#29616;&#36879;&#26126;&#19988;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#36755;&#20986;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#30784;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#35299;&#37322;&#30446;&#26631;&#12289;&#35299;&#37322;&#33539;&#22260;&#12289;&#35299;&#37322;&#26679;&#24335;&#21644;&#35299;&#37322;&#26684;&#24335;&#36825;&#22235;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#23457;&#26597;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#35299;&#37322;&#30340;&#25991;&#29486;&#12290;&#35748;&#35782;&#21040;&#21487;&#35270;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20174;&#35299;&#37322;&#24615;&#35270;&#35273;&#26041;&#24335;&#30340;&#35282;&#24230;&#36884;&#24452;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#65292;&#21363;&#20351;&#29992;&#21487;&#35270;&#21270;&#20316;&#20026;&#35299;&#37322;&#30340;&#26174;&#31034;&#26679;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizat
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item></channel></rss>