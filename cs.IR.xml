<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02630</link><description>&lt;p&gt;
FedHCDR: &#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#22791;&#21463;&#20851;&#27880;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#25968;&#25454;&#26469;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDR&#26041;&#27861;&#38656;&#35201;&#36328;&#39046;&#22495;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#65292;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;FedCDR&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCDR&#65292;&#19968;&#31181;&#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#65288;HSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#29305;&#24449;&#35299;&#32806;&#20026;&#39046;&#22495;&#29420;&#26377;&#21644;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#36229;&#22270;&#28388;&#27874;&#22120;&#26469;&#36827;&#34892;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CONVINV&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#37325;&#20889;&#23558;&#19981;&#36879;&#26126;&#30340;&#23545;&#35805;&#24335;&#20250;&#35805;&#23884;&#20837;&#36716;&#25442;&#20026;&#26126;&#30830;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12774</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#30340;&#37325;&#20889;&#26469;&#35299;&#37322;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#30340;&#20250;&#35805;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CONVINV&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#37325;&#20889;&#23558;&#19981;&#36879;&#26126;&#30340;&#23545;&#35805;&#24335;&#20250;&#35805;&#23884;&#20837;&#36716;&#25442;&#20026;&#26126;&#30830;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#24050;&#34987;&#35777;&#26126;&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#30340;&#19968;&#20010;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#30452;&#35266;&#29702;&#35299;&#20197;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CONVINV&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25581;&#31034;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#12290;CONVINV&#23558;&#19981;&#36879;&#26126;&#30340;&#23545;&#35805;&#24335;&#20250;&#35805;&#23884;&#20837;&#36716;&#25442;&#20026;&#26126;&#30830;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#24544;&#23454;&#22320;&#20445;&#25345;&#20854;&#21407;&#22987;&#26816;&#32034;&#24615;&#33021;&#12290;&#36825;&#31181;&#36716;&#25442;&#26159;&#36890;&#36807;&#35757;&#32451;&#19968;&#31181;&#22522;&#20110;&#19987;&#38376;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;Vec2Text&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#21033;&#29992;&#20102;&#20250;&#35805;&#21644;&#26597;&#35810;&#23884;&#20837;&#22312;&#29616;&#26377;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#20013;&#20849;&#20139;&#30456;&#21516;&#31354;&#38388;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22806;&#37096;&#21487;&#35299;&#37322;&#30340;&#26597;&#35810;&#37325;&#20889;&#32435;&#20837;&#36716;&#25442;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12774v1 Announce Type: new  Abstract: Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational dense retrieval models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval. To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04514</link><description>&lt;p&gt;
&#37325;&#20889;&#20195;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20195;&#30721;&#25628;&#32034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#65292;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#31034;&#20363;&#20195;&#30721;&#29255;&#27573;&#26469;&#22686;&#24378;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#20195;&#30721;&#29255;&#27573;&#21644;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20043;&#38388;&#30340;&#20027;&#35201;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21457;&#29616;&#65292;LLM&#22686;&#24378;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#25913;&#36827;&#26377;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#22312;&#21151;&#33021;&#19978;&#20934;&#30830;&#65292;&#20294;&#22312;&#20195;&#30721;&#24211;&#20013;&#19982;&#22522;&#20934;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#39118;&#26684;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;GAR&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#24211;&#20013;&#30340;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#65288;ReCo&#65289;&#26469;&#36827;&#34892;&#39118;&#26684;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReCo&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
&lt;/p&gt;</description></item><item><title>TSRankLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#25490;&#24207;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25913;&#36827;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16720</link><description>&lt;p&gt;
TSRankLLM: &#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#25490;&#24207;&#30340;&#20004;&#38454;&#27573;LLM&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TSRankLLM: A Two-Stage Adaptation of LLMs for Text Ranking. (arXiv:2311.16720v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16720
&lt;/p&gt;
&lt;p&gt;
TSRankLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#25490;&#24207;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25913;&#36827;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#24207;&#26159;&#21508;&#31181;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26368;&#36817;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#25991;&#26412;&#25490;&#24207;&#20013;&#30340;&#24212;&#29992;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#28040;&#38500;PLMs&#21644;&#25991;&#26412;&#25490;&#24207;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#32773;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#20351;&#29992;&#26377;&#30417;&#30563;&#25490;&#24207;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;PLMs&#19978;&#65292;&#32570;&#20047;&#23545;&#20165;&#35299;&#30721;&#22120;LLM&#30340;&#30740;&#31350;&#12290;&#19968;&#20010;&#20363;&#22806;&#26159;RankLLaMA&#65292;&#23427;&#24314;&#35758;&#30452;&#25509;&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65288;SFT&#65289;&#26469;&#20840;&#38754;&#25506;&#32034;LLaMA&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#20004;&#38454;&#27573;&#28176;&#36827;&#33539;&#24335;&#20250;&#26356;&#26377;&#30410;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#35821;&#26009;&#24211;&#23545;LLMs&#36827;&#34892;&#36830;&#32493;&#39044;&#35757;&#32451;&#65288;CPT&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25191;&#34892;&#19982;RankLLaMA&#19968;&#33268;&#30340;SFT&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text ranking is a critical task in various information retrieval applications, and the recent success of pre-trained language models (PLMs), especially large language models (LLMs), has sparked interest in their application to text ranking. To eliminate the misalignment between PLMs and text ranking, fine-tuning with supervised ranking data has been widely explored. However, previous studies focus mainly on encoder-only and encoder-decoder PLMs, and decoder-only LLM research is still lacking. An exception to this is RankLLaMA, which suggests direct supervised fine-tuning (SFT) to explore LLaMA fully. In our work, we argue that a two-stage progressive paradigm would be more beneficial. First, we suggest continual pre-training (CPT) on LLMs by using a large-scale weakly-supervised corpus. Second, we perform SFT consistent with RankLLaMA, and propose an improved optimization strategy further. Our experimental results on multiple benchmarks demonstrate the superior performance of our metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03972</link><description>&lt;p&gt;
Mixer: &#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixer: Image to Multi-Modal Retrieval Learning for Industrial Application. (arXiv:2305.03972v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#19968;&#30452;&#26159;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#20869;&#23481;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38656;&#27714;&#65292;&#20854;&#20013;&#26597;&#35810;&#26159;&#19968;&#24352;&#22270;&#29255;&#65292;&#25991;&#26723;&#26159;&#20855;&#26377;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#31181;&#26816;&#32034;&#20219;&#21153;&#20173;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixer&#30340;&#26032;&#22411;&#21487;&#20280;&#32553;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#26597;&#35810;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;&#33539;&#24335;&#12290;Mixer&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12289;&#26356;&#39640;&#25928;&#22320;&#25366;&#25496;&#20559;&#26012;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#39640;&#36127;&#36733;&#37327;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval, where the query is an image and the doc is an item with both image and text description, is ubiquitous in e-commerce platforms and content-sharing social media. However, little research attention has been paid to this important application. This type of retrieval task is challenging due to the facts: 1)~domain gap exists between query and doc. 2)~multi-modality alignment and fusion. 3)~skewed training data and noisy labels collected from user behaviors. 4)~huge number of queries and timely responses while the large-scale candidate docs exist. To this end, we propose a novel scalable and efficient image query to multi-modal retrieval learning paradigm called Mixer, which adaptively integrates multi-modality data, mines skewed and noisy data more efficiently and scalable to high traffic. The Mixer consists of three key ingredients: First, for query and doc image, a shared encoder network followed by separate transformation networks are utilized to account for their
&lt;/p&gt;</description></item><item><title>RLTP&#31639;&#27861;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24191;&#21578;&#39044;&#21152;&#36733;&#36807;&#31243;&#20013;&#30340;&#24310;&#36831;&#21360;&#35937;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.02592</link><description>&lt;p&gt;
RLTP&#31639;&#27861;&#65306;&#29992;&#20110;&#39044;&#21152;&#36733;&#24191;&#21578;&#20013;&#30340;&#24310;&#36831;&#21360;&#35937;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads. (arXiv:2302.02592v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02592
&lt;/p&gt;
&lt;p&gt;
RLTP&#31639;&#27861;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24191;&#21578;&#39044;&#21152;&#36733;&#36807;&#31243;&#20013;&#30340;&#24310;&#36831;&#21360;&#35937;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#21697;&#29260;&#30693;&#21517;&#24230;&#65292;&#35768;&#22810;&#24191;&#21578;&#21830;&#19982;&#24191;&#21578;&#24179;&#21488;&#31614;&#35746;&#21512;&#21516;&#36141;&#20080;&#24191;&#21578;&#27969;&#37327;&#65292;&#28982;&#21518;&#23558;&#24191;&#21578;&#25237;&#25918;&#21040;&#30446;&#26631;&#21463;&#20247;&#20013;&#12290;&#22312;&#25972;&#20010;&#24191;&#21578;&#25237;&#25918;&#26399;&#38388;&#65292;&#24191;&#21578;&#21830;&#36890;&#24120;&#24076;&#26395;&#24191;&#21578;&#33719;&#24471;&#29305;&#23450;&#30340;&#21360;&#35937;&#25968;&#65292;&#24182;&#26399;&#26395;&#24191;&#21578;&#23637;&#31034;&#30340;&#25928;&#26524;&#36234;&#22909;&#36234;&#22909;&#65288;&#22914;&#39640;&#28857;&#20987;&#29575;&#65289;&#12290;&#24191;&#21578;&#24179;&#21488;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#27969;&#37327;&#35831;&#27714;&#30340;&#36873;&#25321;&#27010;&#29575;&#26469;&#28385;&#36275;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21457;&#24067;&#32773;&#30340;&#31574;&#30053;&#20063;&#20250;&#24433;&#21709;&#24191;&#21578;&#25237;&#25918;&#36807;&#31243;&#65292;&#36825;&#26159;&#24191;&#21578;&#24179;&#21488;&#26080;&#27861;&#25511;&#21046;&#30340;&#12290;&#39044;&#21152;&#36733;&#26159;&#35768;&#22810;&#31867;&#22411;&#24191;&#21578;&#65288;&#22914;&#35270;&#39057;&#24191;&#21578;&#65289;&#30340;&#24120;&#29992;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22312;&#27969;&#37327;&#35831;&#27714;&#21518;&#26174;&#31034;&#30340;&#21709;&#24212;&#26102;&#38388;&#26159;&#21512;&#29702;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#24310;&#36831;&#21360;&#35937;&#29616;&#35937;&#12290;&#20256;&#32479;&#30340;&#37197;&#36895;&#31639;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#39044;&#21152;&#36733;&#30340;&#29305;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#21363;&#26102;&#21453;&#39304;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and then deliver advertisements to target audiences. In a whole delivery period, advertisers usually desire a certain impression count for the ads, and they also expect that the delivery performance is as good as possible (e.g., obtaining high click-through rate). Advertising platforms employ pacing algorithms to satisfy the demands via adjusting the selection probabilities to traffic requests in real-time. However, the delivery procedure is also affected by the strategies from publishers, which cannot be controlled by advertising platforms. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying after a traffic request is legitimate, which results in delayed impression phenomenon. Traditional pacing algorithms cannot handle the preloading nature well because they rely on immediate feedback signals, and m
&lt;/p&gt;</description></item></channel></rss>