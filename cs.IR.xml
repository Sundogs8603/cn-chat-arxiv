<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25991;&#26723;&#65292;&#20316;&#20026;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;ChatGPT-RetrievalQA&#65292;&#26368;&#32456;&#21457;&#29616;ChatGPT&#21453;&#24212;&#35757;&#32451;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#27169;&#22411;&#27604;&#20351;&#29992;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.02320</link><description>&lt;p&gt;
&#20135;&#29983;&#29992;&#20110;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#21512;&#25104;&#25991;&#26723;: ChatGPT&#21644;&#20154;&#31867;&#19987;&#23478;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts. (arXiv:2305.02320v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25991;&#26723;&#65292;&#20316;&#20026;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;ChatGPT-RetrievalQA&#65292;&#26368;&#32456;&#21457;&#29616;ChatGPT&#21453;&#24212;&#35757;&#32451;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#27169;&#22411;&#27604;&#20351;&#29992;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#29992;&#24615;&#65292;&#20197;&#20379;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#37325;&#26032;&#25490;&#24207;&#36827;&#34892;&#23545;&#27604;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#21512;&#25104;&#25991;&#26723;&#32780;&#19981;&#26159;&#21512;&#25104;&#26597;&#35810;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;ChatGPT-RetrievalQA&#65292;&#24182;&#27604;&#36739;&#20102;&#22312;LLM-generated&#21644;human-generated&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#29983;&#25104;&#30340;LLMs&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#39046;&#22495;&#20013;&#12290;&#25105;&#20204;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21363;&#30001;&#20844;&#20849;&#38382;&#39064;&#38598;&#21644;ChatGPT&#30340;&#20154;&#31867;&#22238;&#31572;&#21644;&#31572;&#26696;&#32452;&#25104;&#30340;&#20154;&#31867;ChatGPT&#27604;&#36739;&#35821;&#26009;&#24211;&#65288;HC3&#65289;&#26500;&#24314;&#20102;ChatGPT-RetrievalQA&#12290;&#25105;&#20204;&#22312;MS MARCO DEV&#65292;TREC DL'19&#21644;TREC DL'20&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;ChatGPT&#21709;&#24212;&#26041;&#38754;&#21463;&#36807;&#35757;&#32451;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#26159;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#22120;&#27604;&#37027;&#20123;&#25509;&#21463;&#20102;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#26174;&#33879;&#26356;&#39640;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the usefulness of generative Large Language Models (LLMs) in generating training data for cross-encoder re-rankers in a novel direction: generating synthetic documents instead of synthetic queries. We introduce a new dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models fine-tuned on LLM-generated and human-generated data. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public question collections with human responses and answers from ChatGPT. We fine-tune a range of cross-encoder re-rankers on either human-generated or ChatGPT-generated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;ChatGPT&#22312;&#28857;&#12289;&#23545;&#12289;&#21015;&#34920;&#19977;&#31181;&#25490;&#21517;&#31574;&#30053;&#19979;&#30340;&#25512;&#33616;&#33021;&#21147;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24182;&#21457;&#29616;ChatGPT&#22312;&#19977;&#31181;&#25490;&#21517;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21015;&#34920;&#25490;&#21517;&#20013;&#33021;&#22815;&#36798;&#21040;&#25104;&#26412;&#21644;&#24615;&#33021;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.02182</link><description>&lt;p&gt;
&#25581;&#31034;ChatGPT&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncovering ChatGPT's Capabilities in Recommender Systems. (arXiv:2305.02182v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;ChatGPT&#22312;&#28857;&#12289;&#23545;&#12289;&#21015;&#34920;&#19977;&#31181;&#25490;&#21517;&#31574;&#30053;&#19979;&#30340;&#25512;&#33616;&#33021;&#21147;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24182;&#21457;&#29616;ChatGPT&#22312;&#19977;&#31181;&#25490;&#21517;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21015;&#34920;&#25490;&#21517;&#20013;&#33021;&#22815;&#36798;&#21040;&#25104;&#26412;&#21644;&#24615;&#33021;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#38382;&#31572;&#21151;&#33021;&#21560;&#24341;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30028;&#21450;&#22806;&#30028;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#27979;&#35797;ChatGPT&#22312;&#25512;&#33616;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#26412;&#30740;&#31350;&#20174;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;ChatGPT&#22312;&#28857;&#12289;&#23545;&#12289;&#21015;&#34920;&#19977;&#31181;&#25490;&#21517;&#31574;&#30053;&#19979;&#30340;&#25512;&#33616;&#33021;&#21147;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#19977;&#31181;&#25490;&#21517;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22522;&#20110;&#21333;&#20301;&#25104;&#26412;&#25913;&#36827;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;ChatGPT&#22312;&#21015;&#34920;&#25490;&#21517;&#20013;&#33021;&#22815;&#22312;&#25104;&#26412;&#21644;&#24615;&#33021;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#24179;&#34913;&#65292;&#32780;&#22312;&#23545;&#21644;&#28857;&#25490;&#21517;&#20013;&#34920;&#29616;&#30456;&#23545;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
The debut of ChatGPT has recently attracted the attention of the natural language processing (NLP) community and beyond. Existing studies have demonstrated that ChatGPT shows significant improvement in a range of downstream NLP tasks, but the capabilities and limitations of ChatGPT in terms of recommendations remain unclear. In this study, we aim to conduct an empirical analysis of ChatGPT's recommendation ability from an Information Retrieval (IR) perspective, including point-wise, pair-wise, and list-wise ranking. To achieve this goal, we re-formulate the above three recommendation policies into a domain-specific prompt format. Through extensive experiments on four datasets from different domains, we demonstrate that ChatGPT outperforms other large language models across all three ranking policies. Based on the analysis of unit cost improvements, we identify that ChatGPT with list-wise ranking achieves the best trade-off between cost and performance compared to point-wise and pair-wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21015;&#34920;&#24335;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#37325;&#26032;&#25490;&#24207;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02156</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21015;&#34920;&#24335;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Listwise Document Reranking with a Large Language Model. (arXiv:2305.02156v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21015;&#34920;&#24335;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#37325;&#26032;&#25490;&#24207;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#25110;&#20132;&#21449;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#24050;&#32463;&#22312;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#24207;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30456;&#20851;&#24615;&#21028;&#26029;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21015;&#34920;&#24335;&#37325;&#26032;&#25490;&#24207;&#22120;(LRL)&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#37325;&#26032;&#25490;&#24207;&#25928;&#26524;&#12290;LRL&#19982;&#29616;&#26377;&#30340;&#28857;&#24335;&#25490;&#21517;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#28857;&#24335;&#25490;&#21517;&#26041;&#27861;&#20013;&#65292;&#25991;&#26723;&#26159;&#29420;&#31435;&#24471;&#20998;&#24182;&#25353;&#20998;&#25968;&#25490;&#21517;&#65292;&#32780;LRL&#30452;&#25509;&#29983;&#25104;&#32473;&#23450;&#20505;&#36873;&#25991;&#26723;&#30340;&#37325;&#26032;&#25490;&#24207;&#25991;&#26723;&#26631;&#35782;&#31526;&#21015;&#34920;&#12290;&#22312;&#19977;&#20010;TREC&#32593;&#32476;&#25628;&#32034;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LRL&#19981;&#20165;&#21487;&#20197;&#22312;&#37325;&#26032;&#25490;&#24207;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#32467;&#26524;&#26102;&#20248;&#20110;&#38646;&#26679;&#26412;&#28857;&#24335;&#26041;&#27861;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#28857;&#24335;&#26041;&#27861;&#30340;&#26368;&#32456;&#38454;&#27573;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#25552;&#39640;&#21069;&#20960;&#21517;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;MIRACL&#30340;&#23376;&#38598;&#65292;&#36825;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#33719;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised ranking methods based on bi-encoder or cross-encoder architectures have shown success in multi-stage text ranking tasks, but they require large amounts of relevance judgments as training data. In this work, we propose Listwise Reranker with a Large Language Model (LRL), which achieves strong reranking effectiveness without using any task-specific training data. Different from the existing pointwise ranking methods, where documents are scored independently and ranked according to the scores, LRL directly generates a reordered list of document identifiers given the candidate documents. Experiments on three TREC web search datasets demonstrate that LRL not only outperforms zero-shot pointwise methods when reranking first-stage retrieval results, but can also act as a final-stage reranker to improve the top-ranked results of a pointwise method for improved efficiency. Additionally, we apply our approach to subsets of MIRACL, a recent multilingual retrieval dataset, with results 
&lt;/p&gt;</description></item><item><title>DSI&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#34429;&#28982;&#20854;&#33021;&#22815;&#29983;&#25104;&#25991;&#20214;&#26631;&#35782;&#31526;&#30340;&#25490;&#24207;&#21015;&#34920;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#20851;&#25991;&#26723;&#21644;&#38543;&#26426;&#25991;&#26723;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#20197;&#25552;&#39640;&#20854;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02073</link><description>&lt;p&gt;
&#20102;&#35299;&#25991;&#26412;&#26816;&#32034;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Differential Search Index for Text Retrieval. (arXiv:2305.02073v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02073
&lt;/p&gt;
&lt;p&gt;
DSI&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#34429;&#28982;&#20854;&#33021;&#22815;&#29983;&#25104;&#25991;&#20214;&#26631;&#35782;&#31526;&#30340;&#25490;&#24207;&#21015;&#34920;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#20851;&#25991;&#26723;&#21644;&#38543;&#26426;&#25991;&#26723;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#20197;&#25552;&#39640;&#20854;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#25628;&#32034;&#25351;&#25968;&#65288;DSI&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#20989;&#25968;&#23545;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#19968;&#20010;&#25991;&#20214;&#26631;&#35782;&#31526;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26550;&#26500;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#20173;&#38656;&#20102;&#35299;DSI&#20855;&#22791;&#22522;&#26412;&#32034;&#24341;&#21644;&#26816;&#32034;&#33021;&#21147;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#26816;&#39564;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;IR&#26694;&#26550;&#24212;&#20855;&#22791;&#30340;&#19977;&#20010;&#37325;&#35201;&#33021;&#21147;&#65292;&#21363;&#25490;&#20182;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#30456;&#20851;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;DSI&#22312;&#35760;&#24518;&#20174;&#20266;&#26597;&#35810;&#21040;&#25991;&#26723;&#26631;&#35782;&#31526;&#30340;&#21333;&#21521;&#26144;&#23556;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#31243;&#24230;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#20851;&#25991;&#26723;&#21644;&#38543;&#26426;&#25991;&#26723;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#20174;&#32780;&#23545;&#20854;&#26816;&#32034;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#32780;&#19981;&#25913;&#21464;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Differentiable Search Index (DSI) is a novel information retrieval (IR) framework that utilizes a differentiable function to generate a sorted list of document identifiers in response to a given query. However, due to the black-box nature of the end-to-end neural architecture, it remains to be understood to what extent DSI possesses the basic indexing and retrieval abilities. To mitigate this gap, in this study, we define and examine three important abilities that a functioning IR framework should possess, namely, exclusivity, completeness, and relevance ordering. Our analytical experimentation shows that while DSI demonstrates proficiency in memorizing the unidirectional mapping from pseudo queries to document identifiers, it falls short in distinguishing relevant documents from random ones, thereby negatively impacting its retrieval effectiveness. To address this issue, we propose a multi-task distillation approach to enhance the retrieval quality without altering the structure o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Demure&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#21435;&#38500;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#22122;&#38899;&#65292;&#20197;&#27492;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01915</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21435;&#22122;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Denoising Multi-modal Sequential Recommenders with Contrastive Learning. (arXiv:2305.01915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Demure&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#21435;&#38500;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#22122;&#38899;&#65292;&#20197;&#27492;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#24314;&#27169;&#24050;&#25104;&#20026;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#22810;&#23186;&#20307;&#25512;&#33616;&#22120;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#27169;&#24577;&#24182;&#35774;&#35745;&#31934;&#32454;&#30340;&#27169;&#22359;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20294;&#26159;&#65292;&#24403;&#29992;&#25143;&#20915;&#23450;&#19982;&#29289;&#21697;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#20182;&#20204;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23436;&#20840;&#38405;&#35835;&#25152;&#26377;&#27169;&#24577;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#31216;&#30452;&#25509;&#23548;&#33268;&#29992;&#25143;&#34892;&#20026;&#30340;&#27169;&#24577;&#20026;&#20852;&#36259;&#28857;&#65292;&#36825;&#20123;&#28857;&#26159;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#19981;&#30452;&#25509;&#23548;&#33268;&#29992;&#25143;&#34892;&#20026;&#30340;&#27169;&#24577;&#21017;&#21487;&#33021;&#26159;&#28508;&#22312;&#30340;&#22122;&#38899;&#65292;&#24182;&#21487;&#33021;&#35823;&#23548;&#25512;&#33616;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#30001;&#20110;&#26080;&#27861;&#35775;&#38382;&#29992;&#25143;&#26174;&#24335;&#21453;&#39304;&#20854;&#20852;&#36259;&#28857;&#65292;&#22240;&#27492;&#25991;&#29486;&#20013;&#24456;&#23569;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#21435;&#38500;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#38899;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#22122;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65288;&#31216;&#20026;Demure&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly-growing research interest in engaging users with multi-modal data for accurate user modeling on recommender systems. Existing multimedia recommenders have achieved substantial improvements by incorporating various modalities and devising delicate modules. However, when users decide to interact with items, most of them do not fully read the content of all modalities. We refer to modalities that directly cause users' behaviors as point-of-interests, which are important aspects to capture users' interests. In contrast, modalities that do not cause users' behaviors are potential noises and might mislead the learning of a recommendation model. Not surprisingly, little research in the literature has been devoted to denoising such potential noises due to the inaccessibility of users' explicit feedback on their point-of-interests. To bridge the gap, we propose a weakly-supervised framework based on contrastive learning for denoising multi-modal recommenders (dubbed Demure). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#25104;&#26412;&#27169;&#22411;&#30340;&#39640;&#25928;&#20998;&#29255;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#25628;&#32034;&#30830;&#23450;&#26368;&#20339;&#20998;&#29255;&#35745;&#21010;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23884;&#20837;&#34920;&#20998;&#29255;&#20219;&#21153;&#20013;&#34920;&#29616;&#24456;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01868</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#25628;&#32034;&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#31070;&#32463;&#25104;&#26412;&#27169;&#22411;&#30340;&#39640;&#25928;&#23884;&#20837;&#34920;&#20998;&#29255;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-train and Search: Efficient Embedding Table Sharding with Pre-trained Neural Cost Models. (arXiv:2305.01868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#25104;&#26412;&#27169;&#22411;&#30340;&#39640;&#25928;&#20998;&#29255;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#25628;&#32034;&#30830;&#23450;&#26368;&#20339;&#20998;&#29255;&#35745;&#21010;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23884;&#20837;&#34920;&#20998;&#29255;&#20219;&#21153;&#20013;&#34920;&#29616;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#23558;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#29255;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#20197;&#24179;&#34913;&#25104;&#26412;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#20998;&#21306;&#26159;NP&#38590;&#38382;&#39064;&#19988;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#31639;&#25104;&#26412;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#8220;&#39044;&#35757;&#32451;&#21644;&#25628;&#32034;&#8221;&#33539;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#29255;&#12290;&#35813;&#26041;&#27861;&#26159;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#27704;&#20037;&#23384;&#22312;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#39044;&#27979;&#25152;&#26377;&#21487;&#33021;&#30340;&#20998;&#29255;&#30340;&#25104;&#26412;&#65292;&#36825;&#20010;&#32593;&#32476;&#23601;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20998;&#29255;&#27169;&#25311;&#22120;&#12290;&#22312;&#27492;&#39044;&#35757;&#32451;&#25104;&#26412;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#22312;&#32447;&#25628;&#32034;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#20219;&#20309;&#29305;&#23450;&#20998;&#29255;&#20219;&#21153;&#30340;&#26368;&#20339;&#20998;&#29255;&#35745;&#21010;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#65288;DLRMs&#65289;&#20013;&#65292;&#25105;&#20204;&#23558;&#27492;&#24605;&#24819;&#23454;&#20363;&#21270;&#65292;&#24182;&#25552;&#35758;&#20102;NeuroShard&#29992;&#20110;&#23884;&#20837;&#34920;&#20998;&#29255;&#12290;NeuroShard&#22312;&#25193;&#23637;&#34920;&#19978;&#39044;&#20808;&#35757;&#32451;&#31070;&#32463;&#25104;&#26412;&#27169;&#22411;&#65292;&#20197;&#28085;&#30422;&#21508;&#31181;&#20998;&#29255;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#24515;&#32593;&#26684;&#25628;&#32034;&#65292;&#20998;&#21035;&#30830;&#23450;&#26368;&#20339;&#30340;&#21015;&#21644;&#34920;&#20998;&#29255;&#35745;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Sharding a large machine learning model across multiple devices to balance the costs is important in distributed training. This is challenging because partitioning is NP-hard, and estimating the costs accurately and efficiently is difficult. In this work, we explore a "pre-train, and search" paradigm for efficient sharding. The idea is to pre-train a universal and once-for-all neural network to predict the costs of all the possible shards, which serves as an efficient sharding simulator. Built upon this pre-trained cost model, we then perform an online search to identify the best sharding plans given any specific sharding task. We instantiate this idea in deep learning recommendation models (DLRMs) and propose NeuroShard for embedding table sharding. NeuroShard pre-trains neural cost models on augmented tables to cover various sharding scenarios. Then it identifies the best column-wise and table-wise sharding plans with beam search and greedy grid search, respectively. Experiments show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#25991;&#26723;&#25805;&#20316;&#8221;&#65288;IDEM&#65289;&#12290;IDEM&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#32467;&#21477;&#65292;&#26080;&#27861;&#24341;&#20837;&#26131;&#20110;&#26816;&#27979;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#20351;&#29992;&#21333;&#29420;&#30340;&#20301;&#32622;&#21512;&#24182;&#31574;&#30053;&#26469;&#24179;&#34913;&#25200;&#21160;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDEM&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20154;&#31867;&#35780;&#20272;&#24471;&#20998;&#30340;&#21516;&#26102;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.01860</link><description>&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#25991;&#26723;&#31713;&#25913;
&lt;/p&gt;
&lt;p&gt;
Towards Imperceptible Document Manipulations against Neural Ranking Models. (arXiv:2305.01860v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#25991;&#26723;&#25805;&#20316;&#8221;&#65288;IDEM&#65289;&#12290;IDEM&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#32467;&#21477;&#65292;&#26080;&#27861;&#24341;&#20837;&#26131;&#20110;&#26816;&#27979;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#20351;&#29992;&#21333;&#29420;&#30340;&#20301;&#32622;&#21512;&#24182;&#31574;&#30053;&#26469;&#24179;&#34913;&#25200;&#21160;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDEM&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20154;&#31867;&#35780;&#20272;&#24471;&#20998;&#30340;&#21516;&#26102;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#21457;&#29616;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65288;NRMs&#65289;&#20013;&#30340;&#28508;&#22312;&#28431;&#27934;&#65292;&#20294;&#26159;&#24403;&#21069;&#25915;&#20987;&#26041;&#27861;&#24120;&#24120;&#20250;&#24341;&#20837;&#35821;&#27861;&#38169;&#35823;&#65292;&#26080;&#24847;&#20041;&#30340;&#34920;&#36798;&#65292;&#25110;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#36825;&#20123;&#37117;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20351;&#29992;&#19982;&#30495;&#23454;&#30340;NRM&#30456;&#20284;&#30340;&#27169;&#25311;NRM&#26469;&#20445;&#35777;&#25915;&#20987;&#25928;&#26524;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#25991;&#26723;&#25805;&#20316;&#8221;&#65288;IDEM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#31639;&#27861;&#21644;&#20154;&#31867;&#26469;&#35828;&#37117;&#19981;&#22826;&#26126;&#26174;&#30340;&#23545;&#25239;&#25991;&#26723;&#12290;IDEM&#25351;&#31034;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#24314;&#31435;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;BART&#65289;&#29983;&#25104;&#36830;&#25509;&#21477;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#26131;&#20110;&#26816;&#27979;&#30340;&#38169;&#35823;&#65292;&#24182;&#37319;&#29992;&#21333;&#29420;&#30340;&#36880;&#20301;&#32622;&#21512;&#24182;&#31574;&#30053;&#26469;&#24179;&#34913;&#25200;&#21160;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;&#22312;&#27969;&#34892;&#30340;MS MARCO&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDEM&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20154;&#31867;&#35780;&#20272;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks have gained traction in order to identify potential vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce grammatical errors, nonsensical expressions, or incoherent text fragments, which can be easily detected. Additionally, current methods rely heavily on the use of a well-imitated surrogate NRM to guarantee the attack effect, which makes them difficult to use in practice. To address these issues, we propose a framework called Imperceptible DocumEnt Manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM instructs a well-established generative language model, such as BART, to generate connection sentences without introducing easy-to-detect errors, and employs a separate position-wise merging strategy to balance relevance and coherence of the perturbed text. Experimental results on the popular MS MARCO benchmark demonstrate that IDEM can outperform strong baselines while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#26469;&#34913;&#37327;&#20854;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22312;IMDB&#21644;Yelp&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01801</link><description>&lt;p&gt;
&#24403;&#26032;&#30340;&#19981;&#19968;&#23450;&#26159;&#26356;&#22909;&#30340;&#65306;&#28145;&#24230;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#21463;&#30410;&#20110;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#30340;&#25512;&#33616;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?. (arXiv:2305.01801v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#26469;&#34913;&#37327;&#20854;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22312;IMDB&#21644;Yelp&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#31070;&#32463;&#27169;&#22411;&#34987;&#22810;&#27425;&#23459;&#20256;&#20026;&#25512;&#33616;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22810;&#20010;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#24182;&#19981;&#33021;&#21487;&#38752;&#22320;&#22797;&#29616;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#30340;&#35780;&#20272;&#26159;&#22312;&#19981;&#19968;&#33268;&#30340;&#21327;&#35758;&#19979;&#36827;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#20351;&#20154;&#20204;&#38590;&#20197;&#20102;&#35299;&#23454;&#38469;&#19978;&#21487;&#20197;&#20174;&#36825;&#20123;&#31070;&#32463;&#27169;&#22411;&#20013;&#33719;&#24471;&#22810;&#23569;&#30410;&#22788;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#20844;&#24179;&#32780;&#20840;&#38754;&#30340;&#32489;&#25928;&#27604;&#36739;&#26469;&#27604;&#36739;&#20256;&#32479;&#27169;&#22411;&#21644;&#31070;&#32463;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#12289;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#38544;&#24335;&#25968;&#25454;&#30340;&#39030;&#37096;&#25512;&#33616;&#30340;&#26368;&#26032;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#21644;&#20256;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#34913;&#37327;&#25512;&#33616;&#27169;&#22411;&#30340;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported state-of-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed.  Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-n recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#25552;&#39640;&#20250;&#35805;&#24335;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13874</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#29992;&#25143;&#21453;&#39304;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#20250;&#35805;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Simulated User Feedback for Conversational Search: Ranking, Rewriting, and Beyond. (arXiv:2304.13874v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#25552;&#39640;&#20250;&#35805;&#24335;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#22312;&#28151;&#21512;&#20513;&#35758;&#30340;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#34429;&#28982;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#31995;&#32479;&#20013;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#31995;&#32479;-&#29992;&#25143;&#23545;&#35805;&#20132;&#20114;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#19982;&#21508;&#31181;&#28151;&#21512;&#20513;&#35758;&#30340;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#19968;&#26086;&#21021;&#22987;&#21270;&#20102;&#20449;&#24687;&#38656;&#27714;&#25551;&#36848;&#65292;&#23601;&#33021;&#22815;&#23545;&#31995;&#32479;&#30340;&#21709;&#24212;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#22238;&#31572;&#28508;&#22312;&#30340;&#28548;&#28165;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27573;&#33853;&#26816;&#32034;&#21644;&#31070;&#32463;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#23548;&#33268;&#22312;nDCG@3&#26041;&#38754;16%&#30340;&#26816;&#32034;&#24615;&#33021;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#30528;n&#30340;&#22686;&#21152;&#65292;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to explore various methods for assessing user feedback in mixed-initiative conversational search (CS) systems. While CS systems enjoy profuse advancements across multiple aspects, recent research fails to successfully incorporate feedback from the users. One of the main reasons for that is the lack of system-user conversational interaction data. To this end, we propose a user simulator-based framework for multi-turn interactions with a variety of mixed-initiative CS systems. Specifically, we develop a user simulator, dubbed ConvSim, that, once initialized with an information need description, is capable of providing feedback to a system's responses, as well as answering potential clarifying questions. Our experiments on a wide variety of state-of-the-art passage retrieval and neural re-ranking models show that effective utilization of user feedback can lead to 16% retrieval performance increase in terms of nDCG@3. Moreover, we observe consistent improvements as the n
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;MIND&#25968;&#25454;&#38598;&#22312;&#26032;&#38395;&#25512;&#33616;&#22810;&#26679;&#24615;&#30740;&#31350;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.08253</link><description>&lt;p&gt;
&#20320;&#22312;&#24847;&#21527;&#65311;&#23545;&#20110;MIND&#25968;&#25454;&#38598;&#22312;&#26032;&#38395;&#25512;&#33616;&#22810;&#26679;&#24615;&#30740;&#31350;&#20013;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Do you MIND? Reflections on the MIND dataset for research on diversity in news recommendations. (arXiv:2304.08253v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;MIND&#25968;&#25454;&#38598;&#22312;&#26032;&#38395;&#25512;&#33616;&#22810;&#26679;&#24615;&#30740;&#31350;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MIND&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#21487;&#29992;&#20110;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#26368;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#22810;&#26679;&#21270;&#26032;&#38395;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25512;&#33616;&#27969;&#31243;&#20013;&#19981;&#21516;&#27493;&#39588;&#23545;&#25991;&#31456;&#31867;&#21035;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#26816;&#26597;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#26159;&#21542;&#36275;&#20197;&#36827;&#34892;&#26356;&#22797;&#26434;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#34429;&#28982;MIND&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MIND dataset is at the moment of writing the most extensive dataset available for the research and development of news recommender systems. This work analyzes the suitability of the dataset for research on diverse news recommendations. On the one hand we analyze the effect the different steps in the recommendation pipeline have on the distribution of article categories, and on the other hand we check whether the supplied data would be sufficient for more sophisticated diversity analysis. We conclude that while MIND is a great step forward, there is still a lot of room for improvement.
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#30340;IDRec&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#27169;&#24577;&#30340;MoRec&#27169;&#22411;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36873;&#25321;&#36866;&#21512;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13835</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20309;&#21435;&#20309;&#20174;&#65311;ID- vs. &#22522;&#20110;&#27169;&#24577;&#30340;&#25512;&#33616;&#27169;&#22411;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Where to Go Next for Recommender Systems? ID- vs. Modality-based recommender models revisited. (arXiv:2303.13835v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13835
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#30340;IDRec&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#27169;&#24577;&#30340;MoRec&#27169;&#22411;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36873;&#25321;&#36866;&#21512;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#21033;&#29992;&#21807;&#19968;&#26631;&#35782;&#65288;ID&#65289;&#26469;&#34920;&#31034;&#19981;&#21516;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25512;&#33616;&#27169;&#22411;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#24182;&#19988;&#22312;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;&#22914;BERT&#21644;ViT&#65289;&#22312;&#23545;&#29289;&#21697;&#30340;&#21407;&#22987;&#27169;&#24577;&#29305;&#24449;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#36890;&#36807;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#24577;&#32534;&#30721;&#22120;&#26367;&#25442;&#29289;&#21697;ID&#23884;&#20837;&#21521;&#37327;&#65292;&#19968;&#20010;&#32431;&#31929;&#30340;&#22522;&#20110;&#27169;&#24577;&#30340;&#25512;&#33616;&#27169;&#22411;&#65288;MoRec&#65289;&#33021;&#21542;&#32988;&#36807;&#25110;&#19982;&#32431;ID&#22522;&#30784;&#27169;&#22411;&#65288;IDRec&#65289;&#30456;&#21305;&#37197;&#65311;&#23454;&#38469;&#19978;&#65292;&#26089;&#22312;&#21313;&#24180;&#21069;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#34987;&#22238;&#31572;&#20102;&#65292;IDRec&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#36828;&#36828;&#32988;&#36807;MoRec&#12290;&#25105;&#20204;&#26088;&#22312;&#37325;&#26032;&#23457;&#35270;&#36825;&#20010;&#8220;&#32769;&#38382;&#39064;&#8221;&#65292;&#20174;&#22810;&#20010;&#26041;&#38754;&#23545;MoRec&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#23376;&#38382;&#39064;&#65306;&#65288;i&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MoRec&#25110;IDRec&#21738;&#20010;&#25512;&#33616;&#27169;&#24335;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#33324;&#24773;&#20917;&#21644;......
&lt;/p&gt;
&lt;p&gt;
Recommendation models that utilize unique identities (IDs) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT and ViT, have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this `old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and 
&lt;/p&gt;</description></item><item><title>&#30005;&#21830;PQA&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#38382;&#39064;&#22810;&#12289;&#25968;&#25454;&#38590;&#25910;&#38598;&#12289;&#31572;&#26696;&#19981;&#30830;&#23450;&#31561;&#29305;&#27530;&#25361;&#25112;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;PQA&#30740;&#31350;&#30340;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.08092</link><description>&lt;p&gt;
&#30005;&#21830;&#20135;&#21697;&#38382;&#31572;&#65306;&#19968;&#39033;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Product Question Answering in E-Commerce: A Survey. (arXiv:2302.08092v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08092
&lt;/p&gt;
&lt;p&gt;
&#30005;&#21830;PQA&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#38382;&#39064;&#22810;&#12289;&#25968;&#25454;&#38590;&#25910;&#38598;&#12289;&#31572;&#26696;&#19981;&#30830;&#23450;&#31561;&#29305;&#27530;&#25361;&#25112;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;PQA&#30740;&#31350;&#30340;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#38382;&#31572;&#65288;PQA&#65289;&#33268;&#21147;&#20110;&#22312;&#30005;&#21830;&#24179;&#21488;&#19978;&#33258;&#21160;&#24555;&#36895;&#22238;&#22797;&#23458;&#25143;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#20856;&#22411;&#30340;&#38382;&#31572;&#38382;&#39064;&#30456;&#27604;&#65292;PQA&#34920;&#29616;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#30005;&#21830;&#24179;&#21488;&#19978;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#20027;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#21508;&#31181;&#38382;&#39064;&#35774;&#32622;&#21644;&#26032;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#25429;&#25417;&#36825;&#20123;&#29305;&#27530;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#29616;&#26377;&#20851;&#20110;PQA&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;PQA&#30740;&#31350;&#25353;&#25552;&#20379;&#30340;&#31572;&#26696;&#24418;&#24335;&#23558;&#20854;&#20998;&#31867;&#20026;&#22235;&#20010;&#38382;&#39064;&#35774;&#32622;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#35774;&#32622;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#34920;&#24449;PQA&#19982;&#19968;&#33324;QA&#24212;&#29992;&#30340;&#26368;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20960;&#20010;&#26410;&#26469;&#26041;&#21521;&#26469;&#32467;&#26463;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Product question answering (PQA), aiming to automatically provide instant responses to customer's questions in E-Commerce platforms, has drawn increasing attention in recent years. Compared with typical QA problems, PQA exhibits unique challenges such as the subjectivity and reliability of user-generated contents in E-commerce platforms. Therefore, various problem settings and novel methods have been proposed to capture these special characteristics. In this paper, we aim to systematically review existing research efforts on PQA. Specifically, we categorize PQA studies into four problem settings in terms of the form of provided answers. We analyze the pros and cons, as well as present existing datasets and evaluation protocols for each setting. We further summarize the most significant challenges that characterize PQA from general QA applications and discuss their corresponding solutions. Finally, we conclude this paper by providing the prospect on several future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;Top-K&#25512;&#33616;&#19979;&#30340;&#31454;&#20105;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29992;&#25143;&#31119;&#21033;&#25439;&#22833;&#21463;&#23567;&#24120;&#25968;&#19978;&#30028;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01971</link><description>&lt;p&gt;
&#31454;&#20105;&#24615;&#20869;&#23481;&#21019;&#20316;&#32773;&#19979;&#30340;Top-K&#25512;&#33616;&#26377;&#22810;&#31967;&#31957;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Bad is Top-$K$ Recommendation under Competing Content Creators?. (arXiv:2302.01971v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;Top-K&#25512;&#33616;&#19979;&#30340;&#31454;&#20105;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29992;&#25143;&#31119;&#21033;&#25439;&#22833;&#21463;&#23567;&#24120;&#25968;&#19978;&#30028;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;&#25512;&#33616;&#24179;&#21488;&#19978;&#31454;&#20105;&#26333;&#20809;&#29575;&#65292;&#36825;&#31181;&#25112;&#30053;&#34892;&#20026;&#23548;&#33268;&#20102;&#20869;&#23481;&#20998;&#24067;&#30340;&#21160;&#24577;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#21019;&#20316;&#32773;&#30340;&#31454;&#20105;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#31119;&#21033;&#65292;&#20197;&#21450;&#30456;&#20851;&#25512;&#33616;&#22914;&#20309;&#24433;&#21709;&#38271;&#26399;&#21160;&#24577;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36825;&#20123;&#30740;&#31350;&#38382;&#39064;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#20551;&#35774;&#19979;&#24314;&#27169;&#21019;&#20316;&#32773;&#30340;&#31454;&#20105;&#65306;1&#65289;&#24179;&#21488;&#37319;&#29992;&#26080;&#23475;&#30340;top-K&#25512;&#33616;&#31574;&#30053;&#65307;2&#65289;&#29992;&#25143;&#20915;&#31574;&#36981;&#24490;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65307;3&#65289;&#20869;&#23481;&#21019;&#20316;&#32773;&#31454;&#20105;&#29992;&#25143;&#20114;&#21160;&#65292;&#19981;&#30693;&#36947;&#20107;&#20808;&#20182;&#20204;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#22240;&#27492;&#24212;&#29992;&#20219;&#24847;&#30340;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#26469;&#26356;&#26032;&#20182;&#20204;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#27931;&#22478;&#20215;&#26684;&#30340;&#35282;&#24230;&#30740;&#31350;&#29992;&#25143;&#31119;&#21033;&#30340;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#30001;&#20110;&#21019;&#20316;&#32773;&#31454;&#20105;&#23548;&#33268;&#30340;&#29992;&#25143;&#31119;&#21033;&#25439;&#22833;&#20221;&#39069;&#22987;&#32456;&#21463;&#21040;$K$&#21644;&#29992;&#25143;&#20915;&#31574;&#38543;&#26426;&#24615;&#24433;&#21709;&#30340;&#23567;&#24120;&#25968;&#30340;&#19978;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creators compete for exposure on recommendation platforms, and such strategic behavior leads to a dynamic shift over the content distribution. However, how the creators' competition impacts user welfare and how the relevance-driven recommendation influences the dynamics in the long run are still largely unknown.  This work provides theoretical insights into these research questions. We model the creators' competition under the assumptions that: 1) the platform employs an innocuous top-$K$ recommendation policy; 2) user decisions follow the Random Utility model; 3) content creators compete for user engagement and, without knowing their utility function in hindsight, apply arbitrary no-regret learning algorithms to update their strategies. We study the user welfare guarantee through the lens of Price of Anarchy and show that the fraction of user welfare loss due to creator competition is always upper bounded by a small constant depending on $K$ and randomness in user decisions; w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03760</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65306;&#20016;&#23500;&#20219;&#21153;&#29305;&#23450;&#21644;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#32479;&#19968;&#29992;&#25143;&#24314;&#27169;&#26694;&#26550;&#12290;&#20854;&#20013;&#35768;&#22810;&#21463;&#30410;&#20110;&#23558;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#20316;&#20026;&#32431;&#25991;&#26412;&#20351;&#29992;&#65292;&#20195;&#34920;&#30528;&#20219;&#20309;&#39046;&#22495;&#25110;&#31995;&#32479;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#32780;&#19981;&#22833;&#36890;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#38382;&#39064;&#20135;&#29983;&#20102;&#65306;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#33021;&#21542;&#24110;&#21161;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#65311;&#34429;&#28982;&#35821;&#35328;&#24314;&#27169;&#30340;&#22810;&#21151;&#33021;&#24615;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#28145;&#20837;&#25506;&#35752;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#24212;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#21382;&#21490;&#30340;&#35821;&#35328;&#24314;&#27169;&#22312;&#19981;&#21516;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#26410;&#30693;&#22495;&#21644;&#26381;&#21153;&#19978;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.
&lt;/p&gt;</description></item></channel></rss>