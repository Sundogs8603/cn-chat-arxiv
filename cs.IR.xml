<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13478</link><description>&lt;p&gt;
SciMMIR:&#31185;&#23398;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#22522;&#20934;&#35780;&#27979;
&lt;/p&gt;
&lt;p&gt;
SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13478
&lt;/p&gt;
&lt;p&gt;
SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#65288;MMIR&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#23545;&#40784;&#30740;&#31350;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#39046;&#22495;&#20869;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;MMIR&#24615;&#33021;&#30340;&#24403;&#21069;&#22522;&#20934;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#23398;&#26415;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#22270;&#20687;&#36890;&#24120;&#19981;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31185;&#23398;MMIR&#65288;SciMMIR&#65289;&#22522;&#20934;&#65292;&#20197;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#20102;530K&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#20174;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#36825;&#20123;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26469;&#33258;&#20110;&#20855;&#26377;&#35814;&#32454;&#26631;&#39064;&#30340;&#31185;&#23398;&#25991;&#26723;&#20013;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#32423;&#23376;&#38598;-&#23376;&#31867;&#21035;&#23618;&#27425;&#27880;&#37322;&#23545;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20197;&#20419;&#36827;&#23545;&#22522;&#20934;&#27169;&#22411;&#30340;&#26356;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations o
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05967</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20998;&#22359;&#23545;&#35282;&#27491;&#20132;&#20851;&#31995;&#21644;&#30697;&#38453;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#20197;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26059;&#36716;-based&#26041;&#27861;&#22914;RotatE&#21644;QuatE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#19982;&#23454;&#20307;&#32500;&#24230;&#25104;&#27604;&#20363;&#22320;&#22686;&#21152;&#20851;&#31995;&#22823;&#23567;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26059;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OrthogonalE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#19982;Riemannian&#20248;&#21270;&#34920;&#31034;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#26082;&#20855;&#26377;&#24191;&#27867;&#24615;&#21448;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#20851;&#31995;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.04913</link><description>&lt;p&gt;
LLaMA-E&#65306;&#22810;&#26041;&#38754;&#25351;&#23548;&#19979;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04913
&lt;/p&gt;
&lt;p&gt;
LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#28041;&#21450;&#21019;&#24314;&#21560;&#24341;&#20154;&#12289;&#20016;&#23500;&#19988;&#26377;&#38024;&#23545;&#24615;&#30340;&#20419;&#38144;&#20869;&#23481;&#65292;&#20197;&#25512;&#21160;&#20135;&#21697;&#38144;&#21806;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#20363;&#65292;&#20026;&#35299;&#20915;&#36825;&#31181;&#24773;&#26223;&#20013;&#30340;&#21508;&#31181;&#21019;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36890;&#29992;&#35821;&#26009;&#24211;&#21644;&#24120;&#35782;&#30693;&#35782;&#35757;&#32451;&#30340;&#20027;&#27969;LLM&#22312;&#36866;&#24212;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#21644;&#23458;&#25143;&#29420;&#29305;&#30340;&#22797;&#26434;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#36825;&#26679;&#30340;LLM&#38656;&#35201;&#36827;&#34892;&#36828;&#31243;&#35775;&#38382;&#65292;&#24341;&#21457;&#20102;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20445;&#25252;&#22823;&#37327;&#23458;&#25143;&#38544;&#31169;&#25968;&#25454;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-E&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39046;&#22495;&#19987;&#23478;&#20174;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#21019;&#24314;&#20102;&#31181;&#23376;&#25351;&#23548;&#38598;&#21512;&#12290;&#36825;&#20123;&#20219;&#21153;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&amp;A. These tasks enabl
&lt;/p&gt;</description></item></channel></rss>