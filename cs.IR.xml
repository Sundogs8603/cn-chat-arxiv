<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Fundus&#26159;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#38395;&#29228;&#34411;&#24037;&#20855;&#65292;&#36890;&#36807;&#25163;&#24037;&#23450;&#21046;&#30340;&#20869;&#23481;&#25552;&#21462;&#22120;&#65292;&#38024;&#23545;&#27599;&#20010;&#25903;&#25345;&#30340;&#22312;&#32447;&#25253;&#32440;&#26684;&#24335;&#25351;&#21335;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#25552;&#21462;&#65292;&#21516;&#26102;&#32467;&#21512;&#29228;&#21462;&#21644;&#20869;&#23481;&#25552;&#21462;&#20110;&#19968;&#20307;&#65292;&#20026;&#38750;&#25216;&#26415;&#29992;&#25143;&#25552;&#20379;&#32479;&#19968;&#20351;&#29992;&#30028;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.15279</link><description>&lt;p&gt;
Fundus&#65306;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#38395;&#29228;&#34411;&#65292;&#20248;&#21270;&#39640;&#36136;&#37327;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15279
&lt;/p&gt;
&lt;p&gt;
Fundus&#26159;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#38395;&#29228;&#34411;&#24037;&#20855;&#65292;&#36890;&#36807;&#25163;&#24037;&#23450;&#21046;&#30340;&#20869;&#23481;&#25552;&#21462;&#22120;&#65292;&#38024;&#23545;&#27599;&#20010;&#25903;&#25345;&#30340;&#22312;&#32447;&#25253;&#32440;&#26684;&#24335;&#25351;&#21335;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#25552;&#21462;&#65292;&#21516;&#26102;&#32467;&#21512;&#29228;&#21462;&#21644;&#20869;&#23481;&#25552;&#21462;&#20110;&#19968;&#20307;&#65292;&#20026;&#38750;&#25216;&#26415;&#29992;&#25143;&#25552;&#20379;&#32479;&#19968;&#20351;&#29992;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fundus&#65292;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26032;&#38395;&#29228;&#34411;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20165;&#20973;&#20960;&#34892;&#20195;&#30721;&#33719;&#24471;&#25968;&#30334;&#19975;&#39640;&#36136;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#19982;&#29616;&#26377;&#30340;&#26032;&#38395;&#29228;&#34411;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#24037;&#23450;&#21046;&#30340;&#12289;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#25903;&#25345;&#30340;&#22312;&#32447;&#25253;&#32440;&#30340;&#26684;&#24335;&#25351;&#21335;&#30340;&#20869;&#23481;&#25552;&#21462;&#22120;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20248;&#21270;&#25105;&#20204;&#30340;&#29228;&#21462;&#36136;&#37327;&#65292;&#20197;&#30830;&#20445;&#26816;&#32034;&#21040;&#30340;&#26032;&#38395;&#25991;&#31456;&#23436;&#25972;&#19988;&#27809;&#26377;HTML&#30165;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#29228;&#21462;&#65288;&#20174;&#32593;&#32476;&#25110;&#22823;&#22411;&#32593;&#32476;&#24402;&#26723;&#20013;&#26816;&#32034;HTML&#65289;&#21644;&#20869;&#23481;&#25552;&#21462;&#32467;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#27969;&#27700;&#32447;&#20013;&#12290;&#36890;&#36807;&#20026;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#25253;&#32440;&#25552;&#20379;&#32479;&#19968;&#30340;&#30028;&#38754;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;Fundus&#21363;&#20351;&#23545;&#38750;&#25216;&#26415;&#29992;&#25143;&#20063;&#26131;&#20110;&#20351;&#29992;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#26694;&#26550;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#38024;&#23545;&#20854;&#20182;&#27969;&#34892;&#30340;&#26032;&#38395;&#29228;&#34411;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Fundus&#21462;&#24471;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15279v1 Announce Type: new  Abstract: This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code. Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper. This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts. Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline. By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users. This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers. Our evaluation shows that Fundus yie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BusGCL&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20851;&#31995;&#23494;&#24230;&#30340;&#21452;&#20391;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#29992;&#25143;&#21644;&#39033;&#30446;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.15075</link><description>&lt;p&gt;
&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15075
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BusGCL&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20851;&#31995;&#23494;&#24230;&#30340;&#21452;&#20391;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#29992;&#25143;&#21644;&#39033;&#30446;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#22312;&#22270;&#32467;&#26500;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#25968;&#25454;&#20013;&#36827;&#34892;&#21327;&#21516;&#36807;&#28388;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#20851;&#31995;&#23494;&#24230;&#23548;&#33268;&#22810;&#36339;&#22270;&#20132;&#20114;&#35745;&#31639;&#21518;&#21452;&#21521;&#33410;&#28857;&#30340;&#22270;&#36866;&#24212;&#24615;&#19981;&#21516;&#65292;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#27169;&#22411;&#23454;&#29616;&#29702;&#24819;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#26694;&#26550;&#65292;&#31216;&#20026;&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BusGCL&#65289;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20851;&#31995;&#23494;&#24230;&#30340;&#21452;&#20391;&#19981;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#21452;&#20391;&#20999;&#29255;&#23545;&#27604;&#35757;&#32451;&#26356;&#22909;&#22320;&#25512;&#29702;&#29992;&#25143;&#21644;&#39033;&#30446;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#32771;&#34385;&#22522;&#20110;&#36229;&#22270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#25366;&#25496;&#38544;&#24335;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#32858;&#21512;&#33021;&#21147;&#26356;&#36866;&#21512;&#29992;&#25143;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15075v1 Announce Type: cross  Abstract: Recent methods utilize graph contrastive Learning within graph-structured user-item interaction data for collaborative filtering and have demonstrated their efficacy in recommendation tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of graphs on bilateral nodes to be different after multi-hop graph interaction calculation, which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for recommendation tasks called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item graph reasoning better with bilateral slicing contrastive training. Especially, taking into account the aggregation ability of hypergraph-based graph convolutional network (GCN) in digging implicit similarities is more suitable for user nodes, emb
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;GPT-3.5&#21644;GPT-4 Turbo&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#25972;&#21512;&#21040;&#39640;&#31561;&#25945;&#32946;&#20013;&#65292;&#20197;&#25552;&#21319;&#22269;&#38469;&#21270;&#65292;&#24182;&#21033;&#29992;&#25968;&#23383;&#21270;&#36716;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25552;&#20379;&#20840;&#38754;&#22238;&#22797;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#20302;&#38169;&#35823;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#23637;&#31034;&#20102;&#25552;&#21319;&#21487;&#35775;&#38382;&#24615;&#12289;&#25928;&#29575;&#21644;&#28385;&#24847;&#24230;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14702</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#22269;&#38469;&#21270;&#23398;&#29983;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Large language model-powered chatbots for internationalizing student support in higher education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;GPT-3.5&#21644;GPT-4 Turbo&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#25972;&#21512;&#21040;&#39640;&#31561;&#25945;&#32946;&#20013;&#65292;&#20197;&#25552;&#21319;&#22269;&#38469;&#21270;&#65292;&#24182;&#21033;&#29992;&#25968;&#23383;&#21270;&#36716;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25552;&#20379;&#20840;&#38754;&#22238;&#22797;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#20302;&#38169;&#35823;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#23637;&#31034;&#20102;&#25552;&#21319;&#21487;&#35775;&#38382;&#24615;&#12289;&#25928;&#29575;&#21644;&#28385;&#24847;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#30001;GPT-3.5&#21644;GPT-4 Turbo&#25552;&#20379;&#21160;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#25972;&#21512;&#21040;&#39640;&#31561;&#25945;&#32946;&#20013;&#65292;&#20197;&#22686;&#24378;&#22269;&#38469;&#21270;&#24182;&#21033;&#29992;&#25968;&#23383;&#21270;&#36716;&#22411;&#12290;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#24212;&#29992;&#65292;&#20197;&#25913;&#21892;&#23398;&#29983;&#21442;&#19982;&#12289;&#20449;&#24687;&#33719;&#21462;&#21644;&#25903;&#25345;&#12290;&#21033;&#29992;Python 3&#12289;GPT API&#12289;LangChain&#21644;Chroma Vector Store&#31561;&#25216;&#26415;&#65292;&#30740;&#31350;&#24378;&#35843;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#21019;&#36896;&#39640;&#36136;&#37327;&#12289;&#21450;&#26102;&#21644;&#30456;&#20851;&#30340;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25552;&#20379;&#20840;&#38754;&#22238;&#22797;&#12289;&#29992;&#25143;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#20559;&#22909;&#20197;&#21450;&#20302;&#38169;&#35823;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24378;&#35843;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23454;&#26102;&#21442;&#19982;&#12289;&#35760;&#24518;&#33021;&#21147;&#21644;&#20851;&#38190;&#25968;&#25454;&#35775;&#38382;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#25552;&#21319;&#21487;&#35775;&#38382;&#24615;&#12289;&#25928;&#29575;&#21644;&#28385;&#24847;&#24230;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#25552;&#20986;&#32842;&#22825;&#26426;&#22120;&#20154;&#26174;&#33879;&#26377;&#21161;&#20110;&#39640;&#31561;&#25945;&#32946;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14702v1 Announce Type: cross  Abstract: This research explores the integration of chatbot technology powered by GPT-3.5 and GPT-4 Turbo into higher education to enhance internationalization and leverage digital transformation. It delves into the design, implementation, and application of Large Language Models (LLMs) for improving student engagement, information access, and support. Utilizing technologies like Python 3, GPT API, LangChain, and Chroma Vector Store, the research emphasizes creating a high-quality, timely, and relevant transcript dataset for chatbot testing. Findings indicate the chatbot's efficacy in providing comprehensive responses, its preference over traditional methods by users, and a low error rate. Highlighting the chatbot's real-time engagement, memory capabilities, and critical data access, the study demonstrates its potential to elevate accessibility, efficiency, and satisfaction. Concluding, the research suggests the chatbot significantly aids higher
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#12289;&#38754;&#21521;&#26381;&#21153;&#30340;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;A2CI&#65292;&#26088;&#22312;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;&#65292;&#33021;&#26377;&#25928;&#24212;&#23545;&#25910;&#38598;&#21644;&#25972;&#29702;&#30340;&#22823;&#37327;&#22320;&#29699;&#31185;&#23398;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14693</link><description>&lt;p&gt;
A2CI&#65306;&#22522;&#20110;&#20113;&#30340;&#38754;&#21521;&#26381;&#21153;&#30340;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;&#65292;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to Support Atmospheric Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#12289;&#38754;&#21521;&#26381;&#21153;&#30340;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;A2CI&#65292;&#26088;&#22312;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;&#65292;&#33021;&#26377;&#25928;&#24212;&#23545;&#25910;&#38598;&#21644;&#25972;&#29702;&#30340;&#22823;&#37327;&#22320;&#29699;&#31185;&#23398;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22320;&#31185;&#23398;&#25968;&#25454;&#20026;&#31185;&#23398;&#30028;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#26426;&#36935;&#12290;&#21033;&#29992;&#36965;&#24863;&#21355;&#26143;&#12289;&#22320;&#38754;&#20256;&#24863;&#22120;&#32593;&#32476;&#29978;&#33267;&#31038;&#20132;&#23186;&#20307;&#36755;&#20837;&#25910;&#38598;&#21040;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#29616;&#22312;&#21487;&#20197;&#36827;&#34892;&#26356;&#22810;&#22823;&#35268;&#27169;&#12289;&#38271;&#26399;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;NASA&#21644;&#20854;&#20182;&#25919;&#24220;&#26426;&#26500;&#27599;&#23567;&#26102;&#25910;&#38598;&#21644;&#25972;&#29702;&#30340;&#25968;&#30334;TB&#20449;&#24687;&#23545;&#20110;&#24076;&#26395;&#25913;&#21892;&#23545;&#22320;&#29699;&#22823;&#27668;&#31995;&#32479;&#30340;&#29702;&#35299;&#30340;&#22823;&#27668;&#31185;&#23398;&#23478;&#26469;&#35828;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#22823;&#37327;&#25968;&#25454;&#30340;&#26377;&#25928;&#21457;&#29616;&#12289;&#32452;&#32455;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#20010;&#30001;NSF&#36164;&#21161;&#30340;&#39033;&#30446;&#30340;&#25104;&#26524;&#65292;&#35813;&#39033;&#30446;&#24320;&#21457;&#20102;&#19968;&#20010;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;&#8212;&#8212;A2CI&#65288;&#22823;&#27668;&#20998;&#26512;&#19979;&#23618;&#32467;&#26500;&#65289;&#65292;&#20197;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;&#12290;&#39318;&#20808;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#26381;&#21153;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#28982;&#21518;&#35814;&#32454;&#25551;&#36848;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14693v1 Announce Type: cross  Abstract: Big earth science data offers the scientific community great opportunities. Many more studies at large-scales, over long-terms and at high resolution can now be conducted using the rich information collected by remote sensing satellites, ground-based sensor networks, and even social media input. However, the hundreds of terabytes of information collected and compiled on an hourly basis by NASA and other government agencies present a significant challenge for atmospheric scientists seeking to improve the understanding of the Earth atmospheric system. These challenges include effective discovery, organization, analysis and visualization of large amounts of data. This paper reports the outcomes of an NSF-funded project that developed a geospatial cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) -- to support atmospheric research. We first introduce the service-oriented system framework then describe in detail the
&lt;/p&gt;</description></item><item><title>SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.14666</link><description>&lt;p&gt;
SyllabusQA&#65306;&#19968;&#20010;&#35838;&#31243;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SyllabusQA: A Course Logistics Question Answering Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14666
&lt;/p&gt;
&lt;p&gt;
SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25945;&#23398;&#21161;&#29702;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26174;&#33879;&#28508;&#21147;&#20943;&#36731;&#20154;&#31867;&#25945;&#24072;&#30340;&#24037;&#20316;&#37327;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19982;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36825;&#23545;&#23398;&#29983;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#25945;&#24072;&#26469;&#35828;&#26159;&#37325;&#22797;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SyllabusQA&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#65292;&#28085;&#30422;36&#20010;&#19987;&#19994;&#65292;&#21253;&#21547;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#38382;&#39064;&#31867;&#22411;&#21644;&#31572;&#26696;&#26684;&#24335;&#37117;&#26159;&#22810;&#26679;&#30340;&#12290;&#30001;&#20110;&#35768;&#22810;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#32771;&#35797;&#26085;&#26399;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#23545;&#20960;&#20010;&#24378;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#20256;&#32479;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#25351;&#26631;&#19978;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08699</link><description>&lt;p&gt;
&#20851;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#35013;&#26377;&#25668;&#20687;&#22836;&#30340;&#26174;&#24494;&#38236;&#25110;&#20840;&#25195;&#25551;&#20202;&#33719;&#21462;&#12290;&#21033;&#29992;&#30456;&#20284;&#24615;&#35745;&#31639;&#22522;&#20110;&#36825;&#20123;&#22270;&#20687;&#21305;&#37197;&#24739;&#32773;&#65292;&#22312;&#30740;&#31350;&#21644;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#26368;&#36817;&#25628;&#32034;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;&#32454;&#32990;&#32467;&#26500;&#36827;&#34892;&#24494;&#22937;&#30340;&#37327;&#21270;&#65292;&#20419;&#36827;&#27604;&#36739;&#65292;&#24182;&#22312;&#19982;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#30340;&#30149;&#20363;&#25968;&#25454;&#24211;&#36827;&#34892;&#27604;&#36739;&#26102;&#23454;&#29616;&#20851;&#20110;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#26032;&#24739;&#32773;&#39044;&#27979;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#20197;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item></channel></rss>