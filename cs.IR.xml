<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12798</link><description>&lt;p&gt;
&#33021;&#37327;&#30340;&#26799;&#24230;&#27969;&#65306;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#30340;&#36890;&#29992;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12798
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26159;&#22312;&#38598;&#25104;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;EA&#35270;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#22686;&#24378;&#22270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;EA&#20013;&#35299;&#30721;&#36807;&#31243;-&#23545;&#20110;&#26377;&#25928;&#30340;&#25805;&#20316;&#21644;&#23545;&#40784;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;-&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#23454;&#20307;&#21644;&#39069;&#22806;&#30340;&#26174;&#24335;&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#31181;&#29305;&#27530;&#24615;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;EA&#35299;&#30721;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#22312;&#22270;&#20869;&#24341;&#23548;&#26799;&#24230;&#27969;&#65292;&#20197;&#20419;&#36827;&#22270;&#21516;&#36136;&#24615;&#12290;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19977;&#20803;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
&lt;/p&gt;</description></item><item><title>CDRNP&#26159;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23558;&#29992;&#25143;&#34920;&#31034;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12732</link><description>&lt;p&gt;
CDRNP: &#36890;&#36807;&#31070;&#32463;&#36807;&#31243;&#23454;&#29616;&#36328;&#39046;&#22495;&#25512;&#33616;&#20197;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural Process. (arXiv:2401.12732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12732
&lt;/p&gt;
&lt;p&gt;
CDRNP&#26159;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23558;&#29992;&#25143;&#34920;&#31034;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#29992;&#25143;&#20559;&#22909;&#26469;&#20026;&#30446;&#26631;&#39046;&#22495;&#30340;&#29992;&#25143;&#36827;&#34892;&#25512;&#33616;&#12290;&#20256;&#32479;&#30340;CDR&#30740;&#31350;&#36981;&#24490;&#23884;&#20837;&#21644;&#26144;&#23556;&#65288;EMCDR&#65289;&#33539; paradigm&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#29992;&#25143;&#20849;&#20139;&#26144;&#23556;&#20989;&#25968;&#23558;&#29992;&#25143;&#34920;&#31034;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#24573;&#35270;&#20102;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#12290;&#26368;&#36817;&#30340;CDR&#30740;&#31350;&#23581;&#35797;&#22312;&#20803;&#23398;&#20064;&#33539; paradigm &#19979;&#23398;&#20064;&#29992;&#25143;&#29305;&#23450;&#26144;&#23556;&#20989;&#25968;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#30340;CDR&#35270;&#20026;&#29420;&#31435;&#20219;&#21153;&#65292;&#20294;&#24573;&#35270;&#20102;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#22909;&#30456;&#20851;&#24615;&#65292;&#38480;&#21046;&#20102;&#29992;&#20110;&#34920;&#31034;&#29992;&#25143;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20004;&#20010;&#33539; paradigm &#37117;&#24573;&#30053;&#20102;&#26144;&#23556;&#36807;&#31243;&#20013;&#26469;&#33258;&#20004;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;-&#39033;&#30446;&#26174;&#24335;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CDR&#26694;&#26550;&#65292;&#20351;&#29992;&#31070;&#32463;&#36807;&#31243;&#65288;NP&#65289;&#65292;&#31216;&#20026;CDRNP&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain recommendation (CDR) has been proven as a promising way to tackle the user cold-start problem, which aims to make recommendations for users in the target domain by transferring the user preference derived from the source domain. Traditional CDR studies follow the embedding and mapping (EMCDR) paradigm, which transfers user representations from the source to target domain by learning a user-shared mapping function, neglecting the user-specific preference. Recent CDR studies attempt to learn user-specific mapping functions in meta-learning paradigm, which regards each user's CDR as an individual task, but neglects the preference correlations among users, limiting the beneficial information for user representations. Moreover, both of the paradigms neglect the explicit user-item interactions from both domains during the mapping process. To address the above issues, this paper proposes a novel CDR framework with neural process (NP), termed as CDRNP. Particularly, it develops th
&lt;/p&gt;</description></item><item><title>MOReGIn&#26159;&#19968;&#31181;&#22810;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#20840;&#23616;&#21644;&#20010;&#20307;&#23618;&#38754;&#19978;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#12290;&#36890;&#36807;&#35843;&#33410;&#25512;&#33616;&#21015;&#34920;&#65292;MORS&#21487;&#20197;&#20445;&#35777;&#25512;&#33616;&#30340;&#27969;&#27966;&#26657;&#20934;&#21644;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12593</link><description>&lt;p&gt;
MOReGIn: &#22810;&#30446;&#26631;&#25512;&#33616;&#22312;&#20840;&#23616;&#21644;&#20010;&#20307;&#23618;&#38754;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MOReGIn: Multi-Objective Recommendation at the Global and Individual Levels. (arXiv:2401.12593v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12593
&lt;/p&gt;
&lt;p&gt;
MOReGIn&#26159;&#19968;&#31181;&#22810;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#20840;&#23616;&#21644;&#20010;&#20307;&#23618;&#38754;&#19978;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#12290;&#36890;&#36807;&#35843;&#33410;&#25512;&#33616;&#21015;&#34920;&#65292;MORS&#21487;&#20197;&#20445;&#35777;&#25512;&#33616;&#30340;&#27969;&#27966;&#26657;&#20934;&#21644;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#65288;MORS&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#35777;&#22810;&#20010;&#30446;&#26631;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#38500;&#20102;&#20934;&#30830;&#24615;&#22806;&#65292;MORS&#21487;&#20197;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#20026;&#25972;&#20010;&#31995;&#32479;&#23454;&#29616;&#38500;&#20934;&#30830;&#24615;&#20043;&#22806;&#30340;&#20854;&#20182;&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#22312;&#20010;&#20307;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#36825;&#24847;&#21619;&#30528;&#25512;&#33616;&#26159;&#26681;&#25454;&#27599;&#20010;&#29992;&#25143;&#30340;&#38656;&#27714;&#23450;&#21046;&#30340;&#12290;&#29616;&#26377;&#30340;MORS&#35201;&#20040;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#35201;&#20040;&#22312;&#20010;&#20307;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#32780;&#19981;&#20551;&#35774;&#36825;&#20004;&#31181;&#35266;&#28857;&#30340;&#20849;&#23384;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#20840;&#23616;&#21644;&#20010;&#20307;&#30446;&#26631;&#20849;&#23384;&#26102;&#65292;MORS&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#31181;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#33410;&#25512;&#33616;&#21015;&#34920;&#26469;&#20445;&#35777;&#20840;&#23616;&#21644;&#20010;&#20307;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20316;&#20026;&#20010;&#20307;&#35282;&#24230;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27969;&#27966;&#26657;&#20934;&#38382;&#39064;&#65292;&#20316;&#20026;&#20840;&#23616;&#35282;&#24230;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#26412;&#25991;&#19968;&#21516;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to guarantee multiple (often conflicting) goals. Besides accuracy, a MORS can operate at the global level, where additional beyond-accuracy goals are met for the system as a whole, or at the individual level, meaning that the recommendations are tailored to the needs of each user. The state-of-the-art MORSs either operate at the global or individual level, without assuming the co-existence of the two perspectives. In this study, we show that when global and individual objectives co-exist, MORSs are not able to meet both types of goals. To overcome this issue, we present an approach that regulates the recommendation lists so as to guarantee both global and individual perspectives, while preserving its effectiveness. Specifically, as individual perspective, we tackle genre calibration and, as global perspective, provider fairness. We validate our approach on two real-world datasets, publicly released with this paper.
&lt;/p&gt;</description></item><item><title>PolyCF&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22810;&#39033;&#24335;&#22270;&#36807;&#28388;&#22120;&#22788;&#29702;&#20132;&#20114;&#20449;&#21495;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35889;&#29305;&#24449;&#65292;&#24182;&#36817;&#20284;&#24674;&#22797;&#20002;&#22833;&#30340;&#20132;&#20114;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#20248;&#30340;&#21327;&#21516;&#36807;&#28388;&#12290;</title><link>http://arxiv.org/abs/2401.12590</link><description>&lt;p&gt;
PolyCF: &#38754;&#21521;&#21327;&#21516;&#36807;&#28388;&#30340;&#26368;&#20248;&#35889;&#22270;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
PolyCF: Towards the Optimal Spectral Graph Filters for Collaborative Filtering. (arXiv:2401.12590v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12590
&lt;/p&gt;
&lt;p&gt;
PolyCF&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22810;&#39033;&#24335;&#22270;&#36807;&#28388;&#22120;&#22788;&#29702;&#20132;&#20114;&#20449;&#21495;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35889;&#29305;&#24449;&#65292;&#24182;&#36817;&#20284;&#24674;&#22797;&#20002;&#22833;&#30340;&#20132;&#20114;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#20248;&#30340;&#21327;&#21516;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#21033;&#29992;&#29992;&#25143;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#21327;&#21516;&#30456;&#20284;&#24615;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22522;&#20110;&#33410;&#28857;&#23884;&#20837;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#23884;&#20837;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23558;CF&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#22270;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#26469;&#24212;&#23545;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PolyCF&#65292;&#19968;&#20010;&#28789;&#27963;&#30340;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#65292;&#21033;&#29992;&#22810;&#39033;&#24335;&#22270;&#36807;&#28388;&#22120;&#22788;&#29702;&#20132;&#20114;&#20449;&#21495;&#12290;PolyCF&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#20041;&#26684;&#25289;&#22982;&#28388;&#27874;&#22120;&#25429;&#25417;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35889;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#36817;&#20284;&#20110;&#24674;&#22797;&#20002;&#22833;&#30340;&#20132;&#20114;&#30340;&#26368;&#20248;&#22810;&#39033;&#24335;&#21709;&#24212;&#20989;&#25968;&#12290;&#22270;&#20248;&#21270;&#30446;&#26631;&#21644;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#20849;&#21516;&#29992;&#20110;&#20248;&#21270;&#21367;&#31215;&#26680;&#30340;&#21442;&#25968;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) is a pivotal research area in recommender systems that capitalizes on collaborative similarities between users and items to provide personalized recommendations. With the remarkable achievements of node embedding-based Graph Neural Networks (GNNs), we explore the upper bounds of expressiveness inherent to embedding-based methodologies and tackle the challenges by reframing the CF task as a graph signal processing problem. To this end, we propose PolyCF, a flexible graph signal filter that leverages polynomial graph filters to process interaction signals. PolyCF exhibits the capability to capture spectral features across multiple eigenspaces through a series of Generalized Gram filters and is able to approximate the optimal polynomial response function for recovering missing interactions. A graph optimization objective and a pair-wise ranking objective are jointly used to optimize the parameters of the convolution kernel. Experiments on three widely adopted 
&lt;/p&gt;</description></item><item><title>InfoRank&#26159;&#19968;&#31181;&#26080;&#20559;&#30340;&#23398;&#20064;-&#25490;&#24207;&#33539;&#24335;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20197;&#36755;&#20837;&#29305;&#24449;&#20026;&#26465;&#20214;&#30340;&#35266;&#27979;&#20272;&#35745;&#21644;&#30456;&#20851;&#20272;&#35745;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23454;&#29616;&#20559;&#24046;-free &#30340;&#30456;&#20851;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.12553</link><description>&lt;p&gt;
InfoRank&#65306;&#36890;&#36807;&#26465;&#20214;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#23454;&#29616;&#26080;&#20559;&#30340;&#23398;&#20064;-&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
InfoRank: Unbiased Learning-to-Rank via Conditional Mutual Information Minimization. (arXiv:2401.12553v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12553
&lt;/p&gt;
&lt;p&gt;
InfoRank&#26159;&#19968;&#31181;&#26080;&#20559;&#30340;&#23398;&#20064;-&#25490;&#24207;&#33539;&#24335;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20197;&#36755;&#20837;&#29305;&#24449;&#20026;&#26465;&#20214;&#30340;&#35266;&#27979;&#20272;&#35745;&#21644;&#30456;&#20851;&#20272;&#35745;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23454;&#29616;&#20559;&#24046;-free &#30340;&#30456;&#20851;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20010;&#21035;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#25490;&#24207;&#26159;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#65289;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#23398;&#20064;&#36825;&#26679;&#30340;&#20010;&#24615;&#21270;&#25490;&#24207;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#28857;&#20987;&#34892;&#20026;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21040;&#30340;&#21453;&#39304;&#23545;&#20808;&#21069;&#25490;&#21517;&#36739;&#39640;&#30340;&#39033;&#30446;&#23384;&#22312;&#20559;&#35265;&#65292;&#30452;&#25509;&#20174;&#20013;&#23398;&#20064;&#20250;&#23548;&#33268;&#8220;&#23500;&#32773;&#24840;&#23500;&#8221;&#30340;&#29616;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#36275;&#22815;&#30340;&#26080;&#20559;&#23398;&#20064;-&#25490;&#24207;&#33539;&#24335;&#65292;&#21517;&#20026;InfoRank&#65292;&#26088;&#22312;&#21516;&#26102;&#35299;&#20915;&#20301;&#32622;&#21644;&#27969;&#34892;&#24230;&#20559;&#35265;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#35266;&#23519;&#22240;&#23376;&#20013;&#65292;&#20174;&#32780;&#20026;&#35299;&#20915;&#19982;&#20559;&#35265;&#30456;&#20851;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#20197;&#36755;&#20837;&#29305;&#24449;&#20026;&#26465;&#20214;&#30340;&#35266;&#27979;&#20272;&#35745;&#21644;&#30456;&#20851;&#20272;&#35745;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23454;&#29616;&#20559;&#24046;-free &#30340;&#30456;&#20851;&#20272;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;InfoRank&#65292;&#25105;&#20204;&#39318;&#20808;&#34701;&#20837;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Ranking items regarding individual user interests is a core technique of multiple downstream tasks such as recommender systems. Learning such a personalized ranker typically relies on the implicit feedback from users' past click-through behaviors. However, collected feedback is biased toward previously highly-ranked items and directly learning from it would result in a "rich-get-richer" phenomenon. In this paper, we propose a simple yet sufficient unbiased learning-to-rank paradigm named InfoRank that aims to simultaneously address both position and popularity biases. We begin by consolidating the impacts of those biases into a single observation factor, thereby providing a unified approach to addressing bias-related issues. Subsequently, we minimize the mutual information between the observation estimation and the relevance estimation conditioned on the input features. By doing so, our relevance estimation can be proved to be free of bias. To implement InfoRank, we first incorporate a
&lt;/p&gt;</description></item><item><title>DREditor&#26159;&#19968;&#31181;&#26102;&#38388;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#26657;&#20934;&#29616;&#26377;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#20351;&#29992;&#32447;&#24615;&#26144;&#23556;&#21644;&#32534;&#36753;&#25805;&#20316;&#31526;&#26469;&#32534;&#36753;&#21305;&#37197;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#39046;&#22495;&#30340;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12540</link><description>&lt;p&gt;
DREditor:&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#29305;&#23450;&#39046;&#22495;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#26102;&#38388;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DREditor: An Time-efficient Approach for Building a Domain-specific Dense Retrieval Model. (arXiv:2401.12540v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12540
&lt;/p&gt;
&lt;p&gt;
DREditor&#26159;&#19968;&#31181;&#26102;&#38388;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#26657;&#20934;&#29616;&#26377;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#20351;&#29992;&#32447;&#24615;&#26144;&#23556;&#21644;&#32534;&#36753;&#25805;&#20316;&#31526;&#26469;&#32534;&#36753;&#21305;&#37197;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#29305;&#23450;&#39046;&#22495;&#30340;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#65292;&#26377;&#25928;&#37096;&#32626;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#22312;&#20225;&#19994;&#25628;&#32034;&#26381;&#21153;&#20013;&#23588;&#20026;&#22914;&#27492;&#65292;&#22240;&#20026;&#38656;&#27714;&#19981;&#21516;&#39046;&#22495;&#30340;&#19981;&#21516;&#20225;&#19994;&#30340;&#26102;&#38388;&#38656;&#27714;&#19981;&#21516;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DREditor&#30340;&#26102;&#38388;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#32534;&#36753;&#29616;&#25104;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#21305;&#37197;&#35268;&#21017;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#12290;&#36890;&#36807;&#30452;&#25509;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#32447;&#24615;&#26144;&#23556;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;&#26144;&#23556;&#26159;&#30001;&#35299;&#20915;&#19968;&#20010;&#29305;&#27530;&#26500;&#24314;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#33719;&#24471;&#30340;&#32534;&#36753;&#25805;&#20316;&#31526;&#39537;&#21160;&#30340;&#12290;&#19982;&#36890;&#36807;&#38271;&#26102;&#38388;&#24494;&#35843;&#36827;&#34892;&#38544;&#24335;&#35268;&#21017;&#20462;&#25913;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DREditor&#22312;&#19981;&#21516;&#30340;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#25968;&#25454;&#38598;&#26469;&#28304;&#12289;&#26816;&#32034;&#27169;&#22411;&#21644;&#35745;&#31639;&#35774;&#22791;&#19978;&#37117;&#25552;&#20379;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22987;&#32456;&#21487;&#20197;&#25552;&#39640;&#26102;&#38388;&#25928;&#29575;100-300&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying dense retrieval models efficiently is becoming increasingly important across various industries. This is especially true for enterprise search services, where customizing search engines to meet the time demands of different enterprises in different domains is crucial. Motivated by this, we develop a time-efficient approach called DREditor to edit the matching rule of an off-the-shelf dense retrieval model to suit a specific domain. This is achieved by directly calibrating the output embeddings of the model using an efficient and effective linear mapping. This mapping is powered by an edit operator that is obtained by solving a specially constructed least squares problem. Compared to implicit rule modification via long-time finetuning, our experimental results show that DREditor provides significant advantages on different domain-specific datasets, dataset sources, retrieval models, and computing devices. It consistently enhances time efficiency by 100-300 times while maintain
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#25216;&#26415;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12520</link><description>&lt;p&gt;
&#23545;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20869;&#23481;&#36827;&#34892;&#20851;&#38190;&#20449;&#24687;&#26816;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#25216;&#26415;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#25968;&#25454;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#39044;&#27979;&#38271;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25991;&#26412;&#39044;&#27979;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#30340;&#23384;&#22312;&#65292;&#36825;&#24433;&#21709;&#20102;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25429;&#25417;&#37325;&#35201;&#35265;&#35299;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#21644;&#39044;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#23884;&#20837;&#25216;&#26415;&#26469;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#65292;&#20197;&#20943;&#23569;&#20854;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformers&#65288;BERT&#65289;&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#24800;&#36152;&#26131;&#21327;&#23450;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23884;&#20837;&#26041;&#27861;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#19981;&#20165;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#32780;&#19988;&#26377;&#21161;&#20110;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments predic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#30340;&#21464;&#24577;&#20851;&#31995;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22810;&#36718;&#23545;&#35805;&#24314;&#27169;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21464;&#24577;&#27979;&#35797;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#23545;&#35805;&#38271;&#26102;&#38388;&#32500;&#25345;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.12483</link><description>&lt;p&gt;
&#22522;&#20110;&#35282;&#33394;&#30340;&#21464;&#24577;&#20851;&#31995;&#24341;&#23548;&#22810;&#36718;&#23545;&#35805;&#24314;&#27169;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Persona-centric Metamorphic Relation guided Robustness Evaluation for Multi-turn Dialogue Modelling. (arXiv:2401.12483v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#30340;&#21464;&#24577;&#20851;&#31995;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22810;&#36718;&#23545;&#35805;&#24314;&#27169;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21464;&#24577;&#27979;&#35797;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#23545;&#35805;&#38271;&#26102;&#38388;&#32500;&#25345;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#39046;&#22495;&#20013;&#24341;&#20837;&#20102;&#24494;&#35843;&#21644;&#25552;&#31034;&#23398;&#20064;&#31561;&#35757;&#32451;&#33539;&#24335;&#30340;&#25512;&#21160;&#19979;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#35282;&#33394;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#29992;&#20110;&#20445;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#25429;&#25417;&#27169;&#22411;&#30495;&#23454;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#36807;&#24230;&#20381;&#36182;&#25968;&#25454;&#27880;&#37322;&#30340;&#36136;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21464;&#24577;&#27979;&#35797;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#29420;&#29305;&#33021;&#21147;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27880;&#37322;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#26356;&#20840;&#38754;&#22320;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#26292;&#38706;&#20102;&#22522;&#20110;&#21442;&#32771;&#39564;&#35777;&#25216;&#26415;&#25152;&#38544;&#34255;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35282;&#33394;&#30340;&#21464;&#24577;&#20851;&#31995;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#21464;&#24577;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#19968;&#33268;&#24615;&#21644;&#23545;&#35805;&#38271;&#26102;&#38388;&#32500;&#25345;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been significant progress in the field of dialogue system thanks to the introduction of training paradigms such as fine-tune and prompt learning. Persona can function as the prior knowledge for maintaining the personality consistency of dialogue systems, which makes it perform well on accuracy. Nonetheless, the conventional reference-based evaluation method falls short in capturing the genuine text comprehension prowess of the model, significantly relying on the quality of data annotation. In contrast, the application of metamorphic testing offers a more profound insight into the model's distinct capabilities without necessitating supplementary annotation labels. This approach furnishes a more comprehensive portrayal of the model's intricacies and exposes intricacies concealed within reference-based validation techniques. Consequently, we introduce a persona-centric metamorphic relation construction for metamorphic testing, aimed at evaluating both the persona consis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20250;&#35805;&#32423;&#21035;&#30340;&#25628;&#32034;&#31995;&#32479;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#20250;&#35805;&#32423;&#24402;&#19968;&#21270;&#21644;&#28857;&#20987;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#20250;&#35805;&#32423;&#25351;&#26631;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#38544;&#24335;&#21453;&#39304;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.12445</link><description>&lt;p&gt;
&#20250;&#35805;&#32423;&#24402;&#19968;&#21270;&#21644;&#28857;&#20987;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#20250;&#35805;&#35780;&#20272;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Session-level Normalization and Click-through Data Enhancement for Session-based Evaluation. (arXiv:2401.12445v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20250;&#35805;&#32423;&#21035;&#30340;&#25628;&#32034;&#31995;&#32479;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#20250;&#35805;&#32423;&#24402;&#19968;&#21270;&#21644;&#28857;&#20987;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#20250;&#35805;&#32423;&#25351;&#26631;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#38544;&#24335;&#21453;&#39304;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#21457;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#24182;&#26816;&#26597;&#22810;&#20010;&#25991;&#26723;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#30740;&#31350;&#20154;&#21592;&#26356;&#20851;&#27880;&#20110;&#20250;&#35805;&#32423;&#21035;&#32780;&#38750;&#21333;&#19968;&#26597;&#35810;&#32423;&#21035;&#19978;&#23545;&#25628;&#32034;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20250;&#35805;&#32423;&#25351;&#26631;&#37117;&#26159;&#20998;&#21035;&#35780;&#20272;&#27599;&#20010;&#26597;&#35810;&#65292;&#28982;&#21518;&#20351;&#29992;&#20250;&#35805;&#32423;&#21152;&#26435;&#20989;&#25968;&#23545;&#26597;&#35810;&#32423;&#24471;&#20998;&#36827;&#34892;&#32858;&#21512;&#12290;&#36825;&#20123;&#25351;&#26631;&#30340;&#20551;&#35774;&#26159;&#20250;&#35805;&#20013;&#30340;&#25152;&#26377;&#26597;&#35810;&#37117;&#24212;&#35813;&#21442;&#19982;&#20854;&#20013;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#39034;&#24207;&#26159;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19968;&#20010;&#25628;&#32034;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#30340;&#21069;&#20960;&#20010;&#26597;&#35810;&#20351;&#29992;&#25143;&#28385;&#24847;&#65292;&#37027;&#20040;&#22905;&#21487;&#33021;&#19981;&#38656;&#35201;&#20219;&#20309;&#21518;&#32493;&#26597;&#35810;&#20102;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#25628;&#32034;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#29992;&#25143;&#30340;&#26126;&#30830;&#21453;&#39304;&#65292;&#25105;&#20204;&#21482;&#33021;&#21033;&#29992;&#19968;&#20123;&#38544;&#24335;&#21453;&#39304;&#65292;&#22914;&#29992;&#25143;&#30340;&#28857;&#20987;&#65292;&#20316;&#20026;&#31163;&#32447;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26631;&#31614;&#12290;&#36825;&#26679;&#30340;&#38544;&#24335;&#21453;&#39304;&#19982;&#25628;&#32034;&#20250;&#35805;&#20013;&#30340;&#30495;&#23454;&#30456;&#20851;&#24615;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#65292;&#22240;&#20026;&#26377;&#20123;&#25991;&#26723;&#21487;&#33021;&#34987;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since a user usually has to issue a sequence of queries and examine multiple documents to resolve a complex information need in a search session, researchers have paid much attention to evaluating search systems at the session level rather than the single-query level. Most existing session-level metrics evaluate each query separately and then aggregate the query-level scores using a session-level weighting function. The assumptions behind these metrics are that all queries in the session should be involved, and their orders are fixed. However, if a search system could make the user satisfied with her first few queries, she may not need any subsequent queries. Besides, in most real-world search scenarios, due to a lack of explicit feedback from real users, we can only leverage some implicit feedback, such as users' clicks, as relevance labels for offline evaluation. Such implicit feedback might be different from the real relevance in a search session as some documents may be omitted in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23618;&#32593;&#32476;&#26041;&#27861;&#23545;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#30340;200&#19975;&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21487;&#35270;&#21270;&#20102;&#20027;&#35201;&#20027;&#39064;&#30340;&#28436;&#21464;&#65292;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.12228</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#23618;&#32593;&#32476;&#20998;&#26512;&#28436;&#21464;&#30340;&#20027;&#39064;&#65307;&#20998;&#26512;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#30340;200&#19975;&#26465;&#25512;&#25991;
&lt;/p&gt;
&lt;p&gt;
Topics evolution through multilayer networks; Analysing 2M tweets from 2022 Qatar FIFA World Cup. (arXiv:2401.12228v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23618;&#32593;&#32476;&#26041;&#27861;&#23545;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#30340;200&#19975;&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21487;&#35270;&#21270;&#20102;&#20027;&#35201;&#20027;&#39064;&#30340;&#28436;&#21464;&#65292;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;2022&#24180;&#21345;&#22612;&#23572;&#19990;&#30028;&#26479;&#20107;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#20351;&#29992;&#22810;&#23618;&#32593;&#32476;&#26041;&#27861;&#21487;&#35270;&#21270;&#20102;&#20027;&#35201;&#20027;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#25353;&#29031;&#27604;&#36187;&#38454;&#27573;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23618;&#27425;&#65292;&#24182;&#21033;&#29992;Gephi&#36719;&#20214;&#29983;&#25104;&#20102;&#22810;&#23618;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#23637;&#31034;&#20102;&#20027;&#39064;&#21644;&#35789;&#27719;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23637;&#31034;&#20102;&#35789;&#27719;-&#19978;&#19979;&#25991;&#20851;&#31995;&#20197;&#21450;&#21508;&#23618;&#27425;&#19978;&#26368;&#24120;&#35752;&#35770;&#20027;&#39064;&#30340;&#21160;&#24577;&#21644;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we conducted a comprehensive data collection on the 2022 Qatar FIFA World Cup event and used a multilayer network approach to visualize the main topics, while considering their context and meaning relationships. We structured the data into layers that corresponded with the stages of the tournament and utilized Gephi software to generate the multilayer networks. Our visualizations displayed both the relationships between topics and words, showing the word-context relationship, as well as the dynamics and changes over time by layer of the most frequently discussed topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>D2K&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#26816;&#32034;&#30340;&#30693;&#35782;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#30340;&#23558;&#30693;&#35782;&#23384;&#20648;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;D2K&#23384;&#20648;&#30340;&#26159;&#30001;&#29992;&#25143;&#12289;&#29289;&#21697;&#21644;&#32972;&#26223;&#26500;&#25104;&#30340;&#23436;&#25972;&#30340;&#25512;&#33616;&#22240;&#32032;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11478</link><description>&lt;p&gt;
D2K: &#23558;&#21382;&#21490;&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#26816;&#32034;&#30340;&#30693;&#35782;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
D2K: Turning Historical Data into Retrievable Knowledge for Recommender Systems. (arXiv:2401.11478v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11478
&lt;/p&gt;
&lt;p&gt;
D2K&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#26816;&#32034;&#30340;&#30693;&#35782;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#30340;&#23558;&#30693;&#35782;&#23384;&#20648;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;D2K&#23384;&#20648;&#30340;&#26159;&#30001;&#29992;&#25143;&#12289;&#29289;&#21697;&#21644;&#32972;&#26223;&#26500;&#25104;&#30340;&#23436;&#25972;&#30340;&#25512;&#33616;&#22240;&#32032;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22312;&#22823;&#22411;&#25512;&#33616;&#24179;&#21488;&#19978;&#19981;&#26029;&#31215;&#32047;&#30340;&#22823;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#35760;&#24405;&#20102;&#29992;&#25143;&#30340;&#21508;&#31181;&#20852;&#36259;&#21644;&#21697;&#21619;&#12290;&#20294;&#26159;&#65292;&#22914;&#20309;&#22312;&#26032;&#25968;&#25454;&#19981;&#26029;&#21040;&#26469;&#26102;&#20445;&#30041;&#26087;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#35797;&#22270;&#23558;&#30693;&#35782;&#38544;&#21547;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20197;&#21442;&#25968;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#8212;&#8212;&#23481;&#37327;&#38590;&#20197;&#25193;&#23637;&#65292;&#30693;&#35782;&#38590;&#20197;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#22823;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#26816;&#32034;&#30693;&#35782;&#30340;&#26694;&#26550;&#65288;D2K&#65289;&#12290;&#36825;&#26159;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#27169;&#22411;&#24182;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#19981;&#21516;&#20110;&#20165;&#23384;&#20648;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#19968;&#20803;&#30693;&#35782;&#65292;D2K&#25552;&#20986;&#23384;&#20648;&#29992;&#20110;&#25512;&#33616;&#30340;&#19977;&#20803;&#30693;&#35782;&#65292;&#35813;&#30693;&#35782;&#30001;&#23436;&#25972;&#30340;&#25512;&#33616;&#22240;&#32032;&#8212;&#8212;&#29992;&#25143;&#12289;&#29289;&#21697;&#21644;&#32972;&#26223;&#26500;&#25104;&#12290;&#30446;&#26631;&#26679;&#26412;&#21487;&#20197;&#30452;&#25509;&#26816;&#32034;&#21040;&#36825;&#20123;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast amount of user behavior data is constantly accumulating on today's large recommendation platforms, recording users' various interests and tastes. Preserving knowledge from the old data while new data continually arrives is a vital problem for recommender systems. Existing approaches generally seek to save the knowledge implicitly in the model parameters. However, such a parameter-centric approach lacks scalability and flexibility -- the capacity is hard to scale, and the knowledge is inflexible to utilize. Hence, in this work, we propose a framework that turns massive user behavior data to retrievable knowledge (D2K). It is a data-centric approach that is model-agnostic and easy to scale up. Different from only storing unary knowledge such as the user-side or item-side information, D2K propose to store ternary knowledge for recommendation, which is determined by the complete recommendation factors -user, item, and context. The knowledge retrieved by target samples can be direc
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#22909;&#21644;&#24182;&#21457;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#20851;&#27880;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#39640;&#38454;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11486</link><description>&lt;p&gt;
&#20559;&#22909;&#21644;&#24182;&#21457;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Preference and Concurrence Aware Bayesian Graph Neural Networks for Recommender Systems. (arXiv:2312.11486v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#22909;&#21644;&#24182;&#21457;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#20851;&#27880;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#39640;&#38454;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#20855;&#26377;&#31361;&#20986;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#39640;&#38454;&#20449;&#24687;&#65292;&#20854;&#20013;&#22270;&#26159;&#30001;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#20132;&#20114;&#21487;&#33021;&#20250;&#20002;&#22833;&#38142;&#25509;&#25110;&#21253;&#21547;&#34394;&#20551;&#30340;&#27491;&#20132;&#20114;&#12290;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#35774;&#35745;&#19968;&#26063;&#36866;&#21512;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#29289;&#21697;&#30340;&#24182;&#21457;&#20197;&#21450;&#19968;&#20123;&#37325;&#35201;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based collaborative filtering methods have prevailing performance for recommender systems since they can capture high-order information between users and items, in which the graphs are constructed from the observed user-item interactions that might miss links or contain spurious positive interactions in industrial scenarios. The Bayesian Graph Neural Network framework approaches this issue with generative models for the interaction graphs. The critical problem is to devise a proper family of graph generative models tailored to recommender systems. We propose an efficient generative model that jointly considers the preferences of users, the concurrence of items and some important graph structure information. Experiments on four popular benchmark datasets demonstrate the effectiveness of our proposed graph generative methods for recommender systems.
&lt;/p&gt;</description></item><item><title>GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.16716</link><description>&lt;p&gt;
GraphPro: &#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16716
&lt;/p&gt;
&lt;p&gt;
GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#27425;&#28040;&#24687;&#20256;&#36882;&#22312;&#24314;&#27169;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#21644;&#26032;&#21040;&#36798;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphPro&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;GraphPro&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#20020;&#26102;&#25552;&#31034;&#26426;&#21046;&#21644;&#22270;&#32467;&#26500;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21040;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARVEL&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#20026;&#23494;&#38598;&#26816;&#32034;&#22120;&#28155;&#21152;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14037</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#35299;&#38145;&#23494;&#38598;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin. (arXiv:2310.14037v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARVEL&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#20026;&#23494;&#38598;&#26816;&#32034;&#22120;&#28155;&#21152;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#65288;MARVEL&#65289;&#23398;&#20064;&#26597;&#35810;&#21644;&#22810;&#27169;&#24577;&#25991;&#26723;&#30340;&#23884;&#20837;&#31354;&#38388;&#20197;&#36827;&#34892;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#12290;MARVEL&#20351;&#29992;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#26597;&#35810;&#21644;&#22810;&#27169;&#24577;&#25991;&#26723;&#36827;&#34892;&#32534;&#30721;&#65292;&#26377;&#21161;&#20110;&#20943;&#23567;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#27169;&#22359;&#32534;&#30721;&#30340;&#22270;&#20687;&#29305;&#24449;&#20316;&#20026;&#20854;&#36755;&#20837;&#65292;&#20351;&#24471;&#32463;&#36807;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;T5-ANCE&#20855;&#26377;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#25105;&#20204;&#22522;&#20110;ClueWeb22&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;ClueWeb22-MM&#25968;&#25454;&#38598;&#65292;&#23558;&#38170;&#25991;&#26412;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#20174;&#38170;&#38142;&#25509;&#30340;&#32593;&#39029;&#20013;&#25552;&#21462;&#30456;&#20851;&#25991;&#26412;&#21644;&#22270;&#20687;&#25991;&#26723;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MARVEL&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#25968;&#25454;&#38598;WebQA&#21644;ClueWeb22-MM&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#26041;&#27861;&#20026;&#23454;&#29616;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#37327;&#36523;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an 
&lt;/p&gt;</description></item><item><title>"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"</title><link>http://arxiv.org/abs/2310.03025</link><description>&lt;p&gt;
"&#26816;&#32034;&#36935;&#19978;&#38271;&#31687;&#22823;&#35821;&#35328;&#27169;&#22411;"
&lt;/p&gt;
&lt;p&gt;
Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03025
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#26368;&#36817;&#65292;&#25193;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#32780;&#23558;&#26816;&#32034;&#19982;LLM&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#23384;&#22312;&#22810;&#24180;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#26816;&#32034;&#22686;&#24378;&#19982;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21738;&#20010;&#23545;&#19979;&#28216;&#20219;&#21153;&#26356;&#22909;&#65311;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#36215;&#26469;&#20197;&#20860;&#39038;&#21033;&#24330;&#21527;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;LLM&#65288;&#21363;&#19968;&#20010;&#31169;&#26377;&#30340;43B GPT&#21644;Llama2-70B&#65289;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#26816;&#32034;&#22686;&#24378;&#22312;&#29983;&#25104;&#26102;&#21487;&#20197;&#36798;&#21040;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;&#20301;&#32622;&#25554;&#20540;&#30340;&#24494;&#35843;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#35770;&#20854;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;32K&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26816;&#32034;&#22686;&#24378;Llama2-70B&#12290;"
&lt;/p&gt;
&lt;p&gt;
Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#25351;&#26631;&#23545;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#30340;&#33258;&#30001;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;&#22266;&#26377;&#30340; SERP &#23545;&#25490;&#24207;&#20851;&#31995;&#32422;&#26463;&#12290;&#35813;&#30740;&#31350;&#35748;&#20026;&#36825;&#31181;&#20851;&#31995;&#32422;&#26463;&#24212;&#35813;&#24212;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.09477</link><description>&lt;p&gt;
&#35780;&#20272;&#25351;&#26631;&#21040;&#24213;&#26377;&#22810;&#23569;&#33258;&#30001;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Freedom Does An Effectiveness Metric Really Have?. (arXiv:2309.09477v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#25351;&#26631;&#23545;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#30340;&#33258;&#30001;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;&#22266;&#26377;&#30340; SERP &#23545;&#25490;&#24207;&#20851;&#31995;&#32422;&#26463;&#12290;&#35813;&#30740;&#31350;&#35748;&#20026;&#36825;&#31181;&#20851;&#31995;&#32422;&#26463;&#24212;&#35813;&#24212;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#35823;&#20197;&#20026;&#65292;&#30001;&#20110;&#35780;&#20272;&#25351;&#26631;&#23545;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#20998;&#25968;&#65292;&#22240;&#27492;&#23545;&#20110; SERP &#23545;&#30340;&#30456;&#23545;&#39034;&#24207;&#20063;&#26377;&#31867;&#20284;&#30340;&#33258;&#30001;&#24230;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#31532;&#20108;&#31867;&#33258;&#30001;&#24230;&#26159;&#34394;&#24187;&#30340;&#12290;&#36825;&#26159;&#22240;&#20026;&#22914;&#26524;&#23545;&#20110;&#19968;&#23545; SERP &#20013;&#30340;&#19968;&#20010;&#24050;&#32463;&#34987;&#35780;&#20272;&#25351;&#26631;&#32473;&#20104;&#20102;&#29305;&#23450;&#30340;&#20998;&#25968;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#22522;&#26412;&#30340;&#25490;&#24207;&#32422;&#26463;&#21017;&#20915;&#23450;&#20102;&#23545;&#20110;&#31532;&#20108;&#20010; SERP &#30340;&#20998;&#25968;&#35201;&#20040;&#19981;&#23567;&#20110;&#65292;&#35201;&#20040;&#19981;&#22823;&#20110;&#32473;&#20104;&#31532;&#19968;&#20010; SERP &#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22266;&#23450;&#30340;&#20851;&#31995;&#31216;&#20043;&#20026;&#22266;&#26377;&#30340; SERP &#23545;&#25490;&#24207;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#20010;&#30446;&#26631;&#26159;&#25551;&#36848;&#21644;&#36777;&#25252;&#36825;&#20123; SERP &#23545;&#20851;&#31995;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#35814;&#23613;&#21644;&#32463;&#39564;&#24615;&#30340;&#23454;&#39564;&#26469;&#32479;&#35745;&#23427;&#20204;&#30340;&#30456;&#23545;&#21457;&#29983;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#22312;&#20449;&#24687;&#26816;&#32034;&#23454;&#39564;&#20013;&#24212;&#29992;&#36825;&#20123;&#22266;&#26377;&#30340; SERP &#23545;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is tempting to assume that because effectiveness metrics have free choice to assign scores to search engine result pages (SERPs) there must thus be a similar degree of freedom as to the relative order that SERP pairs can be put into. In fact that second freedom is, to a considerable degree, illusory. That's because if one SERP in a pair has been given a certain score by a metric, fundamental ordering constraints in many cases then dictate that the score for the second SERP must be either not less than, or not greater than, the score assigned to the first SERP. We refer to these fixed relationships as innate pairwise SERP orderings. Our first goal in this work is to describe and defend those pairwise SERP relationship constraints, and tabulate their relative occurrence via both exhaustive and empirical experimentation.  We then consider how to employ such innate pairwise relationships in IR experiments, leading to a proposal for a new measurement paradigm. Specifically, we argue that
&lt;/p&gt;</description></item><item><title>SynthTab&#26159;&#19968;&#20010;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21513;&#20182;&#35889;&#36716;&#24405;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#38899;&#39057;&#20445;&#25345;&#20102;&#21407;&#22987;&#25351;&#27861;&#12289;&#39118;&#26684;&#21644;&#25216;&#24039;&#30340;&#30456;&#31526;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09085</link><description>&lt;p&gt;
SynthTab: &#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21513;&#20182;&#35889;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription. (arXiv:2309.09085v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09085
&lt;/p&gt;
&lt;p&gt;
SynthTab&#26159;&#19968;&#20010;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21513;&#20182;&#35889;&#36716;&#24405;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#38899;&#39057;&#20445;&#25345;&#20102;&#21407;&#22987;&#25351;&#27861;&#12289;&#39118;&#26684;&#21644;&#25216;&#24039;&#30340;&#30456;&#31526;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21513;&#20182;&#35889;&#26159;&#21513;&#20182;&#25163;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#38899;&#20048;&#31526;&#21495;&#12290;&#23427;&#19981;&#20165;&#25429;&#25417;&#20102;&#19968;&#39318;&#20048;&#26354;&#30340;&#38899;&#20048;&#20869;&#23481;&#65292;&#36824;&#21253;&#25324;&#20102;&#22312;&#20048;&#22120;&#19978;&#30340;&#23454;&#26045;&#21644;&#35013;&#39280;&#12290;&#21513;&#20182;&#35889;&#36716;&#24405;&#65288;GTT&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22312;&#38899;&#20048;&#25945;&#32946;&#21644;&#23089;&#20048;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#33539;&#22260;&#19978;&#37117;&#26377;&#38480;&#65292;&#23548;&#33268;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;GTT&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#22312;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#20013;&#22833;&#36133;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21512;&#25104;SynthTab&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22810;&#20010;&#21830;&#29992;&#21513;&#20182;&#25554;&#20214;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#21513;&#20182;&#35889;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;DadaGP&#30340;&#21513;&#20182;&#35889;&#26500;&#24314;&#65292;DadaGP&#25552;&#20379;&#20102;&#25105;&#20204;&#24076;&#26395;&#36716;&#24405;&#30340;&#21513;&#20182;&#35889;&#30340;&#24222;&#22823;&#25910;&#34255;&#21644;&#29305;&#23450;&#31243;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#21512;&#25104;&#27969;&#31243;&#21487;&#20135;&#29983;&#19982;&#21407;&#22987;&#25351;&#27861;&#12289;&#39118;&#26684;&#21644;&#25216;&#24039;&#22312;&#38899;&#33394;&#19978;&#30456;&#31526;&#30340;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guitar tablature is a form of music notation widely used among guitarists. It captures not only the musical content of a piece, but also its implementation and ornamentation on the instrument. Guitar Tablature Transcription (GTT) is an important task with broad applications in music education and entertainment. Existing datasets are limited in size and scope, causing state-of-the-art GTT models trained on such datasets to suffer from overfitting and to fail in generalization across datasets. To address this issue, we developed a methodology for synthesizing SynthTab, a large-scale guitar tablature transcription dataset using multiple commercial acoustic and electric guitar plugins. This dataset is built on tablatures from DadaGP, which offers a vast collection and the degree of specificity we wish to transcribe. The proposed synthesis pipeline produces audio which faithfully adheres to the original fingerings, styles, and techniques specified in the tablature with diverse timbre. Exper
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#65288;EMPIRE&#65289;&#65292;&#36890;&#36807;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;&#35299;&#20915;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#21644;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#21644;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36793;&#32536;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2212.12970</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#32536;&#39044;&#27979;&#20013;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refined Edge Usage of Graph Neural Networks for Edge Prediction. (arXiv:2212.12970v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12970
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#65288;EMPIRE&#65289;&#65292;&#36890;&#36807;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;&#35299;&#20915;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#21644;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#21644;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36793;&#32536;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26368;&#21021;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#65292;&#20063;&#28608;&#21457;&#20102;&#35768;&#22810;&#20851;&#20110;&#36793;&#32536;&#39044;&#27979;&#65288;&#21363;&#38142;&#36335;&#39044;&#27979;&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#20851;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#21306;&#21035;&#26041;&#38754;&#32570;&#20047;&#31934;&#32454;&#30340;&#35774;&#35745;&#65292;&#36825;&#19968;&#28857;&#24120;&#24120;&#34987;&#24573;&#35270;&#65306;&#65288;i&#65289;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#65292;&#36793;&#20165;&#26500;&#25104;&#25299;&#25169;&#32467;&#26500;&#65292;&#20294;&#22312;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20013;&#26082;&#21487;&#20197;&#20316;&#20026;&#25299;&#25169;&#32467;&#26500;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65288;&#21363;&#26631;&#31614;&#65289;&#65307;&#65288;ii&#65289;&#33410;&#28857;&#20998;&#31867;&#26159;&#23545;&#27599;&#20010;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#36793;&#32536;&#39044;&#27979;&#21017;&#30001;&#27599;&#23545;&#33410;&#28857;&#20915;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36793;&#32536;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPIRE&#65289;&#30340;&#26032;&#22411;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#26469;&#25351;&#23450;&#27599;&#20010;&#36793;&#30340;&#20351;&#29992;&#26041;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#36793;&#20165;&#29992;&#20316;&#25299;&#25169;&#32467;&#26500;&#25110;&#30417;&#30563;&#20449;&#21495;&#65288;&#20998;&#21035;&#31216;&#20026;&#25299;&#25169;&#36793;&#25110;&#30417;&#30563;&#36793;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#29983;&#25104;&#28040;&#24687;&#30340;&#20256;&#36882;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs), originally proposed for node classification, have also motivated many recent works on edge prediction (a.k.a., link prediction). However, existing methods lack elaborate design regarding the distinctions between two tasks that have been frequently overlooked: (i) edges only constitute the topology in the node classification task but can be used as both the topology and the supervisions (i.e., labels) in the edge prediction task; (ii) the node classification makes prediction over each individual node, while the edge prediction is determinated by each pair of nodes. To this end, we propose a novel edge prediction paradigm named Edge-aware Message PassIng neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting technique to specify use of each edge where each edge is solely used as either the topology or the supervision (named as topology edge or supervision edge). We then develop a new message passing mechanism that generates the messages t
&lt;/p&gt;</description></item></channel></rss>