<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11175</link><description>&lt;p&gt;
MISSRec: &#38754;&#21521;&#25512;&#33616;&#30340;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#22810;&#27169;&#24577;&#20852;&#36259;&#24863;&#30693;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation. (arXiv:2308.11175v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#24207;&#21015;&#39044;&#27979;&#20854;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24207;&#21015;&#25512;&#33616;&#22120;&#26159;&#22522;&#20110;ID&#29305;&#24449;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#20351;&#29992;&#31232;&#30095;ID&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19981;&#19968;&#33268;&#30340;ID&#26144;&#23556;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#25512;&#33616;&#39046;&#22495;&#26080;&#27861;&#36827;&#34892;&#20849;&#21516;&#20248;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MISSRec&#65292;&#19968;&#31181;&#38754;&#21521;SR&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#29992;&#25143;&#31471;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#23398;&#20064;&#25429;&#25417;&#24207;&#21015;&#32423;&#30340;&#22810;&#27169;&#24577;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;&#26032;&#39062;&#30340;&#20852;&#36259;&#24863;&#30693;&#35299;&#30721;&#22120;&#21017;&#29992;&#20110;&#25226;&#25569;&#29289;&#21697;-&#27169;&#24577;-&#20852;&#36259;&#20851;&#31995;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of sequential recommendation (SR) is to predict a user's potential interested items based on her/his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model's transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence represent
&lt;/p&gt;</description></item><item><title>SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.05697</link><description>&lt;p&gt;
SSLRec: &#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;
&lt;/p&gt;
&lt;p&gt;
SSLRec: A Self-Supervised Learning Library for Recommendation. (arXiv:2308.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05697
&lt;/p&gt;
&lt;p&gt;
SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#31232;&#30095;&#21644;&#22122;&#22768;&#25968;&#25454;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;SSL&#31639;&#27861;&#26469;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#24615;&#33021;&#65288;&#20363;&#22914;&#22270;&#21327;&#21516;&#36807;&#28388;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#31038;&#20132;&#25512;&#33616;&#12289;&#30693;&#35782;&#22270;&#22686;&#24378;&#25512;&#33616;&#65289;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#25512;&#33616;&#31639;&#27861;&#30340;&#22522;&#30707;&#65292;&#32479;&#19968;&#29616;&#26377;&#26041;&#27861;&#30340;&#39564;&#35777;&#65292;&#24182;&#25512;&#21160;&#26032;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SSLRec&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;SSLRec&#24211;&#20855;&#26377;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21487;&#20197;&#26041;&#20415;&#29992;&#25143;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec library features a modular architecture that allows users to easily evaluate state-of-the-art m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09384</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#30340;&#26222;&#21450;&#65292;&#23545;&#35805;&#25628;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#20005;&#37325;&#38459;&#30861;&#20102;&#30417;&#30563;&#24335;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#26356;&#21152;&#20851;&#27880;&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;&#26816;&#32034;&#22120;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#32570;&#20047;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#20182;&#20204;&#26080;&#27861;&#35299;&#20915;&#22240;&#30465;&#30053;&#32780;&#23548;&#33268;&#30340;&#24120;&#35265;&#23545;&#35805;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#20808;&#21069;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#37325;&#26500;&#26597;&#35810;&#65292;&#32780;&#26080;&#38656;&#23545;&#35805;&#25628;&#32034;&#25968;&#25454;&#30340;&#30417;&#30563;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#35774;&#35745;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#26126;&#30830;&#35299;&#20915;&#20004;&#20010;&#24120;&#35265;&#30340;&#27495;&#20041;&#65306;&#21327;&#35843;&#21644;&#30465;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#65288;PEEL&#65289;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#35774;&#22791;&#21644;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#19982;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;&#65292;&#24182;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#23884;&#20837;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.10532</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Personalized Elastic Embedding Learning for On-Device Recommendation. (arXiv:2306.10532v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#65288;PEEL&#65289;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#35774;&#22791;&#21644;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#19982;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;&#65292;&#24182;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#23884;&#20837;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#24182;&#20943;&#23569;&#32593;&#32476;&#24310;&#36831;&#65292;&#36817;&#24180;&#26469;&#19968;&#30452;&#26377;&#23558;&#22312;&#20113;&#31471;&#35757;&#32451;&#30340;&#33219;&#32959;&#30340;&#25512;&#33616;&#27169;&#22411;&#21387;&#32553;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#32039;&#20945;&#30340;&#25512;&#33616;&#22120;&#27169;&#22411;&#20197;&#36827;&#34892;&#23454;&#26102;&#25512;&#33616;&#30340;&#36235;&#21183;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#24573;&#35270;&#20102;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#29992;&#25143;&#24322;&#36136;&#24615;&#12290;&#23427;&#20204;&#35201;&#20040;&#35201;&#27714;&#25152;&#26377;&#35774;&#22791;&#20849;&#20139;&#30456;&#21516;&#30340;&#21387;&#32553;&#27169;&#22411;&#65292;&#35201;&#20040;&#35201;&#27714;&#20855;&#26377;&#30456;&#21516;&#36164;&#28304;&#39044;&#31639;&#30340;&#35774;&#22791;&#20849;&#20139;&#30456;&#21516;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#30456;&#21516;&#35774;&#22791;&#30340;&#29992;&#25143;&#21487;&#33021;&#20063;&#20855;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20551;&#35774;&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;&#22120;&#21487;&#29992;&#36164;&#28304;&#65288;&#22914;&#20869;&#23384;&#65289;&#26159;&#24658;&#23450;&#30340;&#65292;&#36825;&#19982;&#29616;&#23454;&#24773;&#20917;&#19981;&#31526;&#12290;&#37492;&#20110;&#35774;&#22791;&#21644;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20010;&#24615;&#21270;&#24377;&#24615;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#65288;PEEL&#65289;&#65292;&#35813;&#26694;&#26550;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#20026;&#20855;&#26377;&#19981;&#21516;&#20869;&#23384;&#39044;&#31639;&#30340;&#35774;&#22791;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address privacy concerns and reduce network latency, there has been a recent trend of compressing cumbersome recommendation models trained on the cloud and deploying compact recommender models to resource-limited devices for real-time recommendation. Existing solutions generally overlook device heterogeneity and user heterogeneity. They either require all devices to share the same compressed model or the devices with the same resource budget to share the same model. However, even users with the same devices may have different preferences. In addition, they assume the available resources (e.g., memory) for the recommender on a device are constant, which is not reflective of reality. In light of device and user heterogeneities as well as dynamic resource constraints, this paper proposes a Personalized Elastic Embedding Learning framework (PEEL) for on-device recommendation, which generates personalized embeddings for devices with various memory budgets in once-for-all manner, efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.10050</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36793;&#24179;&#21488;&#20013;&#65292;&#24179;&#21488;&#19982;&#21334;&#23478;&#65288;&#39033;&#30446;&#65289;&#21644;&#23458;&#25143;&#65288;&#29992;&#25143;&#65289;&#31561;&#21508;&#31181;&#21508;&#26679;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#65292;&#27599;&#20010;&#30456;&#20851;&#32773;&#37117;&#26377;&#33258;&#24049;&#30340;&#26399;&#26395;&#32467;&#26524;&#65292;&#23547;&#25214;&#21512;&#36866;&#30340;&#24179;&#34913;&#28857;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#8220;&#20844;&#24179;&#25104;&#26412;&#8221;&#65292;&#23427;&#25429;&#25417;&#20102;&#24179;&#21488;&#22312;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21033;&#30410;&#26102;&#21487;&#33021;&#20570;&#20986;&#30340;&#22949;&#21327;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#25512;&#33616;&#26694;&#26550;&#65292;&#20854;&#20013;&#24179;&#21488;&#22312;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#32422;&#26463;&#26102;&#26368;&#22823;&#21270;&#20854;&#25910;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20844;&#24179;&#25512;&#33616;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24179;&#21488;&#32570;&#20047;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#30340;&#30693;&#35782;&#65292;&#21482;&#33021;&#35266;&#23519;&#20108;&#36827;&#21046;&#36141;&#20080;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#32500;&#25252;&#24179;&#21488;&#25910;&#30410;&#30340;&#21516;&#26102;&#31649;&#29702;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#21516;&#26102;&#20445;&#25345;&#39640;&#25910;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
&lt;/p&gt;</description></item><item><title>I^3 Retriever&#23558;&#38544;&#24335;&#20132;&#20114;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27573;&#33853;&#26816;&#32034;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02371</link><description>&lt;p&gt;
I^3 Retriever: &#23558;&#38544;&#24335;&#20132;&#20114;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
I^3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval. (arXiv:2306.02371v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02371
&lt;/p&gt;
&lt;p&gt;
I^3 Retriever&#23558;&#38544;&#24335;&#20132;&#20114;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27573;&#33853;&#26816;&#32034;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27573;&#33853;&#26816;&#32034;&#26159;&#35768;&#22810;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20363;&#22914;&#32593;&#32476;&#25628;&#32034;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#20854;&#20013;&#25928;&#29575;&#21644;&#25928;&#26524;&#37117;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#22914;&#21452;&#32534;&#30721;&#22120;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21452;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#38480;&#20110;&#24573;&#30053;&#26597;&#35810;&#21644;&#20505;&#36873;&#27573;&#33853;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20132;&#20114;&#33539;&#24335;&#26469;&#25913;&#21892;&#32431;&#21452;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#21518;&#26399;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#21518;&#26399;&#20132;&#20114;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23545;&#22823;&#35821;&#26009;&#24211;&#20135;&#29983;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#25928;&#65292;&#20294;&#25928;&#29575;&#21644;&#31354;&#38388;&#21344;&#29992;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#38480;&#21046;&#20132;&#20114;&#24335;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24212;&#29992;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. T
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#31232;&#30095;&#26631;&#27880;&#22312;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35777;&#25454;&#30340;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#36870;&#21521;&#26368;&#36817;&#37051;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#30456;&#20851;&#24615;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15720</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#26368;&#36817;&#37051;&#25552;&#21319;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#30340;&#25490;&#21517;&#19978;&#19979;&#25991;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Ranking Context of Dense Retrieval Methods through Reciprocal Nearest Neighbors. (arXiv:2305.15720v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15720
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31232;&#30095;&#26631;&#27880;&#22312;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35777;&#25454;&#30340;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#36870;&#21521;&#26368;&#36817;&#37051;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#30456;&#20851;&#24615;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#26631;&#27880;&#32473;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#35757;&#32451;&#24102;&#26469;&#20102;&#25345;&#20037;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#26410;&#26631;&#35760;&#30340;&#30456;&#20851;&#25991;&#26723;&#34987;&#38169;&#35823;&#22320;&#29992;&#20316;&#36127;&#26679;&#26412;&#65292;&#25197;&#26354;&#20102;&#35757;&#32451;&#20449;&#21495;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#26631;&#31614;&#24179;&#28369;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#24809;&#32602;&#27169;&#22411;&#23558;&#39640;&#30456;&#20851;&#24615;&#36171;&#20104;&#34394;&#20551;&#36127;&#26679;&#26412;&#12290;&#20026;&#20102;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#25490;&#21517;&#19978;&#19979;&#25991;&#20013;&#35745;&#31639;&#20505;&#36873;&#25991;&#26723;&#30340;&#30446;&#26631;&#30456;&#20851;&#24615;&#20998;&#24067;&#65292;&#19982;&#22522;&#26412;&#20107;&#23454;&#26368;&#30456;&#20284;&#30340;&#20505;&#36873;&#32773;&#34987;&#36171;&#20104;&#38750;&#38646;&#30456;&#20851;&#27010;&#29575;&#65292;&#35813;&#27010;&#29575;&#22522;&#20110;&#23427;&#20204;&#19982;&#22522;&#26412;&#20107;&#23454;&#25991;&#26723;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;&#20316;&#20026;&#30456;&#20851;&#24615;&#20272;&#35745;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#21521;&#26368;&#36817;&#37051;&#30340;&#25913;&#36827;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#36824;&#21487;&#21333;&#29420;&#29992;&#20110;&#21518;&#22788;&#29702;&#20013;&#37325;&#26032;&#25490;&#21517;&#20505;&#36873;&#32773;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#33258;&#36866;&#24212;&#25991;&#26412;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse annotation poses persistent challenges to training dense retrieval models, such as the problem of false negatives, i.e. unlabeled relevant documents that are spuriously used as negatives in contrastive learning, distorting the training signal. To alleviate this problem, we introduce evidence-based label smoothing, a computationally efficient method that prevents penalizing the model for assigning high relevance to false negatives. To compute the target relevance distribution over candidate documents within the ranking context of a given query, candidates most similar to the ground truth are assigned a non-zero relevance probability based on the degree of their similarity to the ground-truth document(s). As a relevance estimate we leverage an improved similarity metric based on reciprocal nearest neighbors, which can also be used independently to rerank candidates in post-processing. Through extensive experiments on two large-scale ad hoc text retrieval datasets we demonstrate th
&lt;/p&gt;</description></item><item><title>NAIL&#26159;&#19968;&#31181;&#24102;&#26377;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;&#27169;&#22411;&#65292;&#21487;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#21487;&#20197;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#21305;&#37197;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14499</link><description>&lt;p&gt;
NAIL: &#24102;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14499
&lt;/p&gt;
&lt;p&gt;
NAIL&#26159;&#19968;&#31181;&#24102;&#26377;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;&#27169;&#22411;&#65292;&#21487;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#21487;&#20197;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#21305;&#37197;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25991;&#26723;&#37325;&#26032;&#25490;&#21517;&#22120;&#22312;&#31934;&#24230;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#19987;&#29992;&#30828;&#20214;&#36827;&#34892;&#26381;&#21153;&#65292;&#36825;&#26159;&#26114;&#36149;&#24182;&#19988;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#26381;&#21153;&#26102;&#38388;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21482;&#38656;&#35201;&#27599;&#20010;&#25991;&#26723;&#36716;&#25442;&#22120;FLOP&#30340;10-6&#65285;&#30340;&#35789;&#27719;&#24471;&#20998;&#21151;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#24403;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#65292;&#35813;&#26816;&#32034;&#22120;&#20173;&#38656;&#35201;&#21152;&#36895;&#22120;&#36827;&#34892;&#26597;&#35810;&#32534;&#30721;&#12290;&#25105;&#20204;&#23558;NAIL&#65288;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#33258;&#22238;&#24402;&#32034;&#24341;&#65289;&#24341;&#20837;&#20026;&#19982;&#26368;&#36817;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;T5&#12289;GPT-3&#21644;PaLM&#65289;&#20860;&#23481;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#65292;&#24182;&#21487;&#20197;&#24494;&#35843;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#38656;&#35201;n
&lt;/p&gt;
&lt;p&gt;
Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.14232</link><description>&lt;p&gt;
&#20026;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#39044;&#35757;&#32451;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#22240;&#20854;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36328;&#22810;&#20010;&#24322;&#26500;&#20219;&#21153;&#20849;&#21516;&#21033;&#29992;&#39044;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26497;&#38480;&#22810;&#26631;&#31614;&#35770;&#25991;&#20998;&#31867;&#12289;&#24341;&#25991;&#39044;&#27979;&#21644;&#25991;&#29486;&#25628;&#32034;&#65289;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;SciMult&#65292;&#37325;&#28857;&#26159;&#20419;&#36827;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25216;&#26415;-&#20219;&#21153;&#24863;&#30693;&#30340;&#29305;&#21270;&#21644;&#25351;&#20196;&#35843;&#25972;&#12290;&#21069;&#32773;&#37319;&#29992;&#20102;&#20855;&#26377;&#20219;&#21153;&#24863;&#30693;&#23376;&#23618;&#30340;&#22810;&#19987;&#23478;&#21464;&#21387;&#22120;&#26550;&#26500;&#65307;&#21518;&#32773;&#22312;&#36755;&#20837;&#25991;&#26412;&#20043;&#21069;&#28155;&#21152;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#20197;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning framework, SciMult, with a focus on facilitating common knowledge sharing across different scientific literature understanding tasks while preventing task-specific skills from interfering with each other. To be specific, we explore two techniques -task-aware specialization and instruction tuning. The former adopts a Mixture-of-Experts Transformer architecture with task-aware sub-layers; the latter prepends task-specific instructions to the input text so as to produce t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13631</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#20869;&#23481;&#22270;&#20687;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23454;&#38469;&#25628;&#32034;&#24212;&#29992;&#20013;&#23454;&#29616;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#38656;&#35201;&#22312;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#23454;&#20307;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse tex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02996</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;CUR k-NN&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#38170;&#23450;&#39033;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;ANNCUR&#27169;&#22411;&#20013;&#39640;&#21069;k&#39033;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#30340;&#22270;&#26500;&#36896;&#21644;&#21021;&#22987;&#21270;&#26041;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12001</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Random Projection Forest Initialization for Graph Convolutional Networks. (arXiv:2302.12001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#30340;&#22270;&#26500;&#36896;&#21644;&#21021;&#22987;&#21270;&#26041;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#32467;&#26500;&#25968;&#25454;&#65288;&#22914;&#22270;&#65289;&#30340;&#19968;&#22823;&#27493;&#12290;&#20294;GCNs&#20173;&#38656;&#35201;&#26500;&#36896;&#22270;&#26469;&#36827;&#34892;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#32463;&#20856;&#22270;&#65288;&#22914;k&#36817;&#37051;&#22270;&#65289;&#26469;&#21021;&#22987;&#21270;GCN&#12290;&#23613;&#31649;&#26500;&#36896;k&#36817;&#37051;&#22270;&#30340;&#35745;&#31639;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#26500;&#36896;&#30340;&#22270;&#23545;&#20110;&#23398;&#20064;&#21487;&#33021;&#27809;&#26377;&#22826;&#22823;&#30340;&#29992;&#22788;&#12290;&#22312;k&#36817;&#37051;&#22270;&#20013;&#65292;&#28857;&#34987;&#38480;&#21046;&#20026;&#20855;&#26377;&#22266;&#23450;&#25968;&#37327;&#30340;&#36793;&#65292;&#22270;&#20013;&#30340;&#25152;&#26377;&#36793;&#37117;&#20855;&#26377;&#30456;&#31561;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#26500;&#24314;&#22270;&#24182;&#21021;&#22987;&#21270;GCN&#12290;&#23427;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#12290;rpForest&#20351;&#25105;&#20204;&#33021;&#22815;&#36171;&#20104;&#36793;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#34920;&#31034;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#12290;&#26641;&#30340;&#25968;&#37327;&#26159;rpForest&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35889;&#20998;&#26512;&#26469;&#24110;&#21161;&#25105;&#20204;&#35774;&#32622;&#27491;&#30830;&#33539;&#22260;&#30340;&#21442;&#25968;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;GCN&#30456;&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) were a great step towards extending deep learning to unstructured data such as graphs. But GCNs still need a constructed graph to work with. To solve this problem, classical graphs such as $k$-nearest neighbor are usually used to initialize the GCN. Although it is computationally efficient to construct $k$-nn graphs, the constructed graph might not be very useful for learning. In a $k$-nn graph, points are restricted to have a fixed number of edges, and all edges in the graph have equal weights. We present a new way to construct the graph and initialize the GCN. It is based on random projection forest (rpForest). rpForest enables us to assign varying weights on edges indicating varying importance, which enhanced the learning. The number of trees is a hyperparameter in rpForest. We performed spectral analysis to help us setting this parameter in the right range. In the experiments, initializing the GCN using rpForest provides better results compared t
&lt;/p&gt;</description></item><item><title>Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04858</link><description>&lt;p&gt;
Re-ViLM: &#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#30340;&#26816;&#32034;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04858
&lt;/p&gt;
&lt;p&gt;
Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#65288;&#22914;Flamingo&#65289;&#30456;&#32467;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30693;&#35782;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#24040;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#34701;&#21512;&#26032;&#25968;&#25454;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;Re-ViLM&#65292;&#22522;&#20110;Flamingo&#26500;&#24314;&#65292;&#25903;&#25345;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#26576;&#20123;&#30693;&#35782;&#26126;&#30830;&#23384;&#20648;&#22312;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#26356;&#26032;&#25968;&#25454;&#24211;&#26469;&#36731;&#26494;&#36866;&#24212;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#31181;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.10901</link><description>&lt;p&gt;
ALCAP: &#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALCAP: Alignment-Augmented Music Captioner. (arXiv:2212.10901v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;&#29992;&#20110;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;&#38899;&#20048;&#65292;&#21516;&#26102;&#32771;&#34385;&#27468;&#35789;&#21644;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#31934;&#32454;&#35843;&#25972;&#23558;&#38899;&#20048;&#26144;&#23556;&#21040;&#23383;&#24149;&#35760;&#21495;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#21508;&#20010;&#32452;&#20214;&#65292;&#24573;&#30053;&#20102;&#38899;&#39057;&#21644;&#27468;&#35789;&#20043;&#38388;&#23545;&#24212;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#26174;&#24335;&#23398;&#20064;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;-&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#25351;&#23548;&#23398;&#20064;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26032;&#30340;&#29366;&#24577;-&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing popularity of streaming media platforms for music search and recommendations has led to a need for novel methods for interpreting music that take into account both lyrics and audio. However, many previous works focus on refining individual components of encoder-decoder architecture that maps music to caption tokens, ignoring the potential benefits of correspondence between audio and lyrics. In this paper, we propose to explicitly learn the multimodal alignment through contrastive learning. By learning audio-lyrics correspondence, the model is guided to learn better cross-modal consistency, thus generating high-quality captions. We provide both theoretical and empirical results demonstrating the advantage of the proposed method, and achieve new state-of-the-art on two music captioning datasets.
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20197;&#23454;&#29616;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.15500</link><description>&lt;p&gt;
COFFEE: &#21487;&#35299;&#37322;&#24615;&#25512;&#33616;&#20013;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20197;&#23454;&#29616;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#25968;&#23383;&#29983;&#27963;&#20013;&#65292;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65288;PTG&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;PTG&#27169;&#22411;&#35757;&#32451;&#30340;&#29992;&#25143;&#32534;&#20889;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19981;&#21516;&#27700;&#24179;&#30340;&#35821;&#35328;&#36136;&#37327;&#19982;&#29992;&#25143;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#20851;&#32852;&#36215;&#26469;&#12290;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;&#29983;&#25104;&#19982;&#29992;&#25143;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#26102;&#24310;&#32493;&#19981;&#24179;&#31561;&#65292;&#23548;&#33268;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#20986;&#29616;&#19981;&#20844;&#24179;&#30340;&#23545;&#24453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;PTG&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#25104;&#30340;&#35299;&#37322;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#20844;&#24179;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20419;&#36827;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#26368;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#65288;&#32763;&#35793;&#20026;&#20013;&#25991;&#65289;</title><link>http://arxiv.org/abs/2210.03116</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Content-Based Search for Deep Generative Models. (arXiv:2210.03116v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#26368;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#65288;&#32763;&#35793;&#20026;&#20013;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23450;&#20041;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29992;&#25143;&#19981;&#21487;&#33021;&#23436;&#20840;&#20102;&#35299;&#27599;&#20010;&#23384;&#22312;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#21644;&#19968;&#32452;&#22823;&#35268;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25214;&#21040;&#19982;&#26597;&#35810;&#26368;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#19968;&#31995;&#21015;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#23558;&#25628;&#32034;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#27010;&#29575;&#30340;&#20844;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#26597;&#35810;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#33609;&#22270;&#21644;&#25991;&#26412;&#65289;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27169;&#22411;&#26816;&#32034;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#27169;&#22411;&#21160;&#29289;&#22253;&#65288;Generative Model Zoo&#65289;&#19978;&#20248;&#20110;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing proliferation of customized and pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, finding the models that best match the query. As each generative model produces a distribution of images, we formulate the search task as an optimization problem to select the model with the highest probability of generating similar content as the query. We introduce a formulation to approximate this probability given the query from different modalities, e.g., image, sketch, and text. Furthermore, we propose a contrastive learning framework for model retrieval, which learns to adapt features for various query modalities. We demonstrate that our method outperforms several baselines on Generative Model Zoo, a new benchmark we create for the model retrieval task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26469;&#33258;&#36755;&#20837;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.01845</link><description>&lt;p&gt;
&#24102;&#26377;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#20027;&#39064;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds. (arXiv:2205.01845v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26469;&#33258;&#36755;&#20837;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#65292;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#21033;&#29992;&#29992;&#25143;&#25351;&#23548;&#65292;&#25152;&#20197;&#23427;&#20204;&#21457;&#29616;&#30340;&#20027;&#39064;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#12290;&#34429;&#28982;&#23384;&#22312;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#31181;&#23376;&#35789;&#26469;&#21457;&#29616;&#20027;&#39064;&#20195;&#34920;&#35789;&#30340;&#31181;&#23376;&#24341;&#23548;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36739;&#23569;&#20851;&#27880;&#20004;&#20010;&#22240;&#32032;&#65306;(1)&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#23384;&#22312;&#21644;(2)&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31181;&#23376;&#24341;&#23548;&#20027;&#39064;&#21457;&#29616;&#30340;&#20219;&#21153;&#25512;&#24191;&#21040;&#20801;&#35768;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;SeeTopic&#65292;&#22312;&#20854;&#20013;PLM&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#20174;&#36755;&#20837;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#30340;&#23616;&#37096;&#35821;&#20041;&#21487;&#20197;&#30456;&#20114;&#21463;&#30410;&#12290;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SeeTopic&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users' particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#25972;&#21512;&#36215;&#26469;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;HiMeCat&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#22312;&#21482;&#26377;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#25991;&#26723;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2010.13556</link><description>&lt;p&gt;
&#20998;&#23618;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#24369;&#30417;&#30563;&#19979;&#25991;&#26723;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Metadata-Aware Document Categorization under Weak Supervision. (arXiv:2010.13556v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#25972;&#21512;&#36215;&#26469;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;HiMeCat&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#22312;&#21482;&#26377;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#25991;&#26723;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26222;&#36941;&#23384;&#22312;&#23618;&#27425;&#21270;&#20027;&#39064;&#32467;&#26500;&#65292;&#23558;&#25991;&#26723;&#20998;&#31867;&#21040;&#32473;&#23450;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20013;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#12290;&#23613;&#31649;&#30456;&#20851;&#30740;&#31350;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26723;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21482;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#65288;1&#65289;&#26631;&#27880;&#38750;&#24120;&#26114;&#36149;&#65292;&#38590;&#20197;&#33719;&#21462;&#21040;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#65307;&#65288;2&#65289;&#25991;&#26723;&#38468;&#24102;&#20803;&#25968;&#25454;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#25972;&#21512;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;HiMeCat&#65292;&#19968;&#20010;&#22522;&#20110;&#23884;&#20837;&#24335;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#65292;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#65292;&#29992;&#20110;&#20998;&#23618;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorizing documents into a given label hierarchy is intuitively appealing due to the ubiquity of hierarchical topic structures in massive text corpora. Although related studies have achieved satisfying performance in fully supervised hierarchical document classification, they usually require massive human-annotated training data and only utilize text information. However, in many domains, (1) annotations are quite expensive where very few training samples can be acquired; (2) documents are accompanied by metadata information. Hence, this paper studies how to integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. We develop HiMeCat, an embedding-based generative framework for our task. Specifically, we propose a novel joint representation learning module that allows simultaneous modeling of category dependencies, metadata information and textual semantics, and we introduce a data augmentation module that hierarchically synthesize
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2005.00624</link><description>&lt;p&gt;
&#25991;&#26412;&#19982;&#20803;&#25968;&#25454;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Minimally Supervised Categorization of Text with Metadata. (arXiv:2005.00624v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20998;&#31867;&#26159;&#23558;&#20027;&#39064;&#26631;&#31614;&#20998;&#37197;&#32473;&#27599;&#20010;&#25991;&#26723;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20256;&#32479;&#30417;&#30563;&#25991;&#26723;&#20998;&#31867;&#30740;&#31350;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36739;&#23569;&#20851;&#27880;&#20004;&#20010;&#23454;&#38469;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23384;&#22312;&#20803;&#25968;&#25454;&#65306;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#25991;&#26412;&#20276;&#38543;&#30528;&#21508;&#31181;&#38468;&#21152;&#20449;&#24687;&#65292;&#20363;&#22914;&#20316;&#32773;&#21644;&#26631;&#31614;&#12290;&#36825;&#20123;&#20803;&#25968;&#25454;&#20316;&#20026;&#26377;&#21147;&#30340;&#20027;&#39064;&#25351;&#31034;&#22120;&#65292;&#24212;&#35813;&#34987;&#21033;&#29992;&#21040;&#20998;&#31867;&#26694;&#26550;&#20013;&#65307;&#65288;2&#65289;&#26631;&#31614;&#31232;&#32570;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#26114;&#36149;&#30340;&#65292;&#38656;&#35201;&#21482;&#20351;&#29992;&#23569;&#37327;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#37492;&#20110;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#26469;&#25551;&#36848;&#21333;&#35789;&#12289;&#25991;&#26723;&#12289;&#26631;&#31614;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26681;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#20998;&#31867;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1) the presence of metadata: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2) label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#29702;&#35770;&#20013;&#30340;&#38468;&#21152;&#35889;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#30697;&#38453;&#36827;&#34892;&#27010;&#24565;&#20998;&#26512;&#65292;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#24418;&#25104;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#19979;&#38477;&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#25968;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#35889;&#20998;&#35299;&#26041;&#27861;&#65292;&#20063;&#23548;&#33268;&#20102;&#24191;&#20041;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#33539;&#30068;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2004.07353</link><description>&lt;p&gt;
Nucleus I: &#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#20013;&#30340;&#38468;&#21152;&#35889; (arXiv:2004.07353v4 [math.CT] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Nucleus I: Adjunction spectra in recommender systems and descent. (arXiv:2004.07353v4 [math.CT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.07353
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#29702;&#35770;&#20013;&#30340;&#38468;&#21152;&#35889;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#30697;&#38453;&#36827;&#34892;&#27010;&#24565;&#20998;&#26512;&#65292;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#24418;&#25104;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#19979;&#38477;&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#25968;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#35889;&#20998;&#35299;&#26041;&#27861;&#65292;&#20063;&#23548;&#33268;&#20102;&#24191;&#20041;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#33539;&#30068;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#23545;&#20351;&#29992;&#30697;&#38453;&#36827;&#34892;&#27010;&#24565;&#20998;&#26512;&#26469;&#26500;&#24314;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#20123;&#27010;&#24565;&#34987;&#35270;&#20026;&#35889;&#24182;&#24418;&#25104;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#19979;&#38477;&#26159;&#20195;&#25968;&#20960;&#20309;&#21644;&#25299;&#25169;&#20013;&#35889;&#20998;&#35299;&#30340;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20063;&#23548;&#33268;&#20102;&#24191;&#20041;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#29702;&#35770;&#37117;&#26159;&#24191;&#27867;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#30001;&#20110;&#25216;&#26415;&#24046;&#36317;&#36807;&#22823;&#65292;&#35797;&#22270;&#24314;&#31435;&#32852;&#31995;&#20284;&#20046;&#26159;&#24858;&#34850;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24418;&#24335;&#38142;&#25509;&#33258;&#24049;&#24418;&#25104;&#65292;&#33258;&#24213;&#21521;&#19978;&#65292;&#22312;&#20316;&#32773;&#30340;&#24847;&#22270;&#21644;&#26368;&#20339;&#21028;&#26029;&#20043;&#22806;&#12290;&#29087;&#24713;&#30340;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#23548;&#33268;&#20102;&#33539;&#30068;&#35770;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#26159;&#19968;&#31995;&#21015;&#26089;&#26399;&#21162;&#21147;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21457;&#23637;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems build user profiles using concept analysis of usage matrices. The concepts are mined as spectra and form Galois connections. Descent is a general method for spectral decomposition in algebraic geometry and topology which also leads to generalized Galois connections. Both recommender systems and descent theory are vast research areas, separated by a technical gap so large that trying to establish a link would seem foolish. Yet a formal link emerged, all on its own, bottom-up, against authors' intentions and better judgment. Familiar problems of data analysis led to a novel solution in category theory. The present paper arose from a series of earlier efforts to provide a top-down account of these developments.
&lt;/p&gt;</description></item></channel></rss>