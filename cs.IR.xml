<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#21270;&#29702;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#26356;&#21152;&#32454;&#33268;&#22320;&#20998;&#26512;&#32593;&#31449;&#20869;&#23481;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#32593;&#31449;&#20869;&#23481;&#21152;&#23494;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#20114;&#32852;&#32593;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13548</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#29992;&#20110;&#32593;&#31449;&#20869;&#23481;&#21152;&#23494;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Fuzzification-based Feature Selection for Enhanced Website Content Encryption. (arXiv:2306.13548v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#21270;&#29702;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#26356;&#21152;&#32454;&#33268;&#22320;&#20998;&#26512;&#32593;&#31449;&#20869;&#23481;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#32593;&#31449;&#20869;&#23481;&#21152;&#23494;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#20114;&#32852;&#32593;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#31946;&#21270;&#29702;&#35770;&#23545;&#32593;&#31449;&#20869;&#23481;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#36827;&#34892;&#21152;&#23494;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#21407;&#29702;&#35782;&#21035;&#21644;&#36873;&#25321;&#32593;&#31449;&#20013;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27169;&#31946;&#21270;&#20801;&#35768;&#25105;&#20204;&#23558;&#28165;&#26224;&#30340;&#32593;&#31449;&#20869;&#23481;&#36716;&#25442;&#20026;&#27169;&#31946;&#34920;&#31034;&#65292;&#20174;&#32780;&#26356;&#21152;&#32454;&#33268;&#22320;&#20998;&#26512;&#20854;&#29305;&#24449;&#12290;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#29305;&#24449;&#22312;&#19981;&#21516;&#27169;&#31946;&#31867;&#21035;&#20013;&#30340;&#25104;&#21592;&#36164;&#26684;&#31243;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#21152;&#23494;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20248;&#20808;&#20851;&#27880;&#23637;&#31034;&#20986;&#26356;&#39640;&#20250;&#21592;&#36164;&#26684;&#24230;&#30340;&#29305;&#24449;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#21152;&#23494;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#27169;&#31946;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#32593;&#31449;&#20869;&#23481;&#21152;&#23494;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#26368;&#32456;&#25913;&#21892;&#25972;&#20307;&#20114;&#32852;&#32593;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach that utilizes fuzzification theory to perform feature selection on website content for encryption purposes. Our objective is to identify and select the most relevant features from the website by harnessing the principles of fuzzy logic. Fuzzification allows us to transform the crisp website content into fuzzy representations, enabling a more nuanced analysis of their characteristics. By considering the degree of membership of each feature in different fuzzy categories, we can evaluate their importance and relevance for encryption. This approach enables us to prioritize and focus on the features that exhibit higher membership degrees, indicating their significance in the encryption process. By employing fuzzification-based feature selection, we aim to enhance the effectiveness and efficiency of website content encryption, ultimately improving the overall internet security.
&lt;/p&gt;</description></item><item><title>OptMSM&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20248;&#21270;&#22810;&#22330;&#26223;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22330;&#26223;&#20302;&#31209;&#30697;&#38453;&#37325;&#26500;&#21644;&#22330;&#26223;&#20849;&#20139;&#26426;&#21046;&#26469;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#24182;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13382</link><description>&lt;p&gt;
OptMSM: &#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#22810;&#22330;&#26223;&#24314;&#27169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
OptMSM: Optimizing Multi-Scenario Modeling for Click-Through Rate Prediction. (arXiv:2306.13382v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13382
&lt;/p&gt;
&lt;p&gt;
OptMSM&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20248;&#21270;&#22810;&#22330;&#26223;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22330;&#26223;&#20302;&#31209;&#30697;&#38453;&#37325;&#26500;&#21644;&#22330;&#26223;&#20849;&#20139;&#26426;&#21046;&#26469;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#24182;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#24037;&#19994;&#25512;&#33616;&#24179;&#21488;&#36890;&#24120;&#30001;&#22810;&#20010;&#20851;&#32852;&#22330;&#26223;&#32452;&#25104;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#26469;&#21516;&#26102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#12290;&#29616;&#26377;&#30340;&#22810;&#22330;&#26223;CTR&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#22330;&#26223;&#24863;&#30693;&#23398;&#20064;&#27169;&#22359;&#65292;&#20174;&#36755;&#20837;&#29305;&#24449;&#20013;&#23398;&#20064;&#19968;&#32452;&#20855;&#26377;&#22330;&#26223;&#20849;&#20139;&#21644;&#22330;&#26223;&#29305;&#23450;&#20449;&#24687;&#30340;&#22810;&#21151;&#33021;&#34920;&#31034;&#65292;&#20197;&#21450;ii&#65289;&#22330;&#26223;&#29305;&#23450;&#39044;&#27979;&#27169;&#22359;&#65292;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#20026;&#27599;&#20010;&#22330;&#26223;&#25552;&#20379;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21069;&#32773;&#65292;&#32780;&#24573;&#30053;&#20102;&#21518;&#32773;&#27169;&#22359;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27599;&#20010;&#22330;&#26223;&#30340;&#27169;&#22411;&#21442;&#25968;&#23610;&#23544;&#22686;&#21152;&#12289;&#35757;&#32451;&#38590;&#24230;&#22686;&#21152;&#20197;&#21450;&#24615;&#33021;&#29942;&#39048;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;OptMSM&#65288;\textbf{Opt}imizing \textbf{M}ulti-\textbf{S}cenario \textbf{M}odeling&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21270;&#32780;&#26377;&#25928;&#30340;&#22330;&#26223;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#32771;&#34385;&#22823;&#22810;&#25968;&#22330;&#26223;&#29305;&#23450;&#39044;&#27979;&#27169;&#22359;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#22330;&#26223;&#20302;&#31209;&#30697;&#38453;&#37325;&#26500;&#26469;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#20849;&#20139;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large-scale industrial recommendation platform typically consists of multiple associated scenarios, requiring a unified click-through rate (CTR) prediction model to serve them simultaneously. Existing approaches for multi-scenario CTR prediction generally consist of two main modules: i) a scenario-aware learning module that learns a set of multi-functional representations with scenario-shared and scenario-specific information from input features, and ii) a scenario-specific prediction module that serves each scenario based on these representations. However, most of these approaches primarily focus on improving the former module and neglect the latter module. This can result in challenges such as increased model parameter size, training difficulty, and performance bottlenecks for each scenario. To address these issues, we propose a novel framework called OptMSM (\textbf{Opt}imizing \textbf{M}ulti-\textbf{S}cenario \textbf{M}odeling). First, we introduce a simplified yet effective scen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#31181;&#29615;&#22659;&#20256;&#24863;&#22120;&#32467;&#21512;&#30340;&#28151;&#21512;&#20256;&#24863;&#22120;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#35782;&#21035;&#20986;&#26356;&#22810;&#30340;&#27963;&#21160;&#65292;&#26377;&#21161;&#20110;&#25512;&#23548;&#20986;&#20154;&#31867;&#27963;&#21160;&#27169;&#24335;&#25110;&#29992;&#25143;&#30011;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.13374</link><description>&lt;p&gt;
&#38271;&#26399;&#25968;&#25454;&#37319;&#38598;&#19979;&#30340;&#26234;&#33021;&#23478;&#23621;&#20154;&#31867;&#27963;&#21160;&#34892;&#20026;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Human Activity Behavioural Pattern Recognition in Smarthome with Long-hour Data Collection. (arXiv:2306.13374v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22810;&#31181;&#29615;&#22659;&#20256;&#24863;&#22120;&#32467;&#21512;&#30340;&#28151;&#21512;&#20256;&#24863;&#22120;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#35782;&#21035;&#20986;&#26356;&#22810;&#30340;&#27963;&#21160;&#65292;&#26377;&#21161;&#20110;&#25512;&#23548;&#20986;&#20154;&#31867;&#27963;&#21160;&#27169;&#24335;&#25110;&#29992;&#25143;&#30011;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#30740;&#31350;&#20026;&#21307;&#30103;&#20445;&#20581;&#12289;&#36816;&#21160;&#21644;&#29992;&#25143;&#30011;&#20687;&#31561;&#35768;&#22810;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32771;&#34385;&#21040;&#20154;&#31867;&#27963;&#21160;&#30340;&#22797;&#26434;&#24615;&#65292;&#21363;&#20351;&#26377;&#26377;&#25928;&#30340;&#20256;&#24863;&#22120;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#19987;&#27880;&#20110;&#35782;&#21035;&#22914;&#22352;&#12289;&#30561;&#30496;&#12289;&#31449;&#31435;&#12289;&#19978;&#19979;&#27004;&#26799;&#21644;&#22868;&#36305;&#31561;&#22522;&#26412;&#30340;&#20154;&#31867;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#20154;&#31867;&#34892;&#20026;&#27169;&#24335;&#38656;&#35201;&#26356;&#22810;&#30340;&#27963;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#22522;&#26412;&#20154;&#31867;&#27963;&#21160;&#65292;&#21516;&#26102;&#32467;&#21512;&#29615;&#22659;&#20256;&#24863;&#22120;&#65288;&#22914;PIR&#12289;&#21387;&#21147;&#20256;&#24863;&#22120;&#65289;&#21644;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#20256;&#24863;&#22120;&#65288;&#22914;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#65289;&#26469;&#23454;&#29616;&#28151;&#21512;&#20256;&#24863;&#22120;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;&#28151;&#21512;&#26041;&#27861;&#24110;&#21161;&#25512;&#23548;&#20986;&#27604;&#22522;&#26412;&#27963;&#21160;&#26356;&#22810;&#30340;&#27963;&#21160;&#65292;&#36825;&#20063;&#26377;&#21161;&#20110;&#25512;&#23548;&#20986;&#20154;&#31867;&#27963;&#21160;&#27169;&#24335;&#25110;&#29992;&#25143;&#30011;&#20687;&#12290;&#29992;&#25143;&#30011;&#20687;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research on human activity recognition has provided novel solutions to many applications like healthcare, sports, and user profiling. Considering the complex nature of human activities, it is still challenging even after effective and efficient sensors are available. The existing works on human activity recognition using smartphone sensors focus on recognizing basic human activities like sitting, sleeping, standing, stair up and down and running. However, more than these basic activities is needed to analyze human behavioural pattern. The proposed framework recognizes basic human activities using deep learning models. Also, ambient sensors like PIR, pressure sensors, and smartphone-based sensors like accelerometers and gyroscopes are combined to make it hybrid-sensor-based human activity recognition. The hybrid approach helped derive more activities than the basic ones, which also helped derive human activity patterns or user profiling. User profiling provides sufficient informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#36807;&#21435;&#21313;&#24180;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#30740;&#31350;&#36235;&#21183;&#21644;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#22686;&#24378;&#12289;&#35780;&#20272;&#21644;&#22797;&#29992;&#20197;&#21450;&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13186</link><description>&lt;p&gt;
&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#21313;&#24180;&#23398;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Decade of Scholarly Research on Open Knowledge Graphs. (arXiv:2306.13186v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#36807;&#21435;&#21313;&#24180;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#30740;&#31350;&#36235;&#21183;&#21644;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#22686;&#24378;&#12289;&#35780;&#20272;&#21644;&#22797;&#29992;&#20197;&#21450;&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#35813;&#35805;&#39064;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#28608;&#22686;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;2013&#24180;&#33267;2023&#24180;&#38388;&#20986;&#29256;&#30340;&#26377;&#20851;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#25991;&#29486;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#35813;&#39046;&#22495;&#20013;&#30340;&#36235;&#21183;&#65292;&#27169;&#24335;&#21644;&#30740;&#31350;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20986;&#29616;&#30340;&#20851;&#38190;&#20027;&#39064;&#21644;&#30740;&#31350;&#38382;&#39064;&#12290;&#35813;&#20316;&#21697;&#20351;&#29992;&#25991;&#29486;&#35745;&#37327;&#25216;&#26415;&#20998;&#26512;&#20102;&#20174;Scopus&#26816;&#32034;&#21040;&#30340;4445&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;&#26679;&#26412;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27599;&#24180;&#20851;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#36798;&#22269;&#23478;(+50 per year)&#12290;&#36825;&#20123;&#25104;&#26524;&#21457;&#34920;&#22312;&#39640;&#24230;&#24341;&#29992;&#30340;&#23398;&#26415;&#26399;&#21002;&#21644;&#20250;&#35758;&#19978;&#12290;&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#65306;(1)&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#22686;&#24378;&#65292;(2)&#35780;&#20272;&#21644;&#22797;&#29992;&#65292;&#20197;&#21450;(3)&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#20013;&#12290;&#22312;&#36825;&#20123;&#20027;&#39064;&#20013;&#65292;&#30740;&#31350;&#30830;&#23450;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#20855;&#20307;&#20219;&#21153;&#65292;&#20363;&#22914;&#23454;&#20307;&#38142;&#25509;&#65292;&#20851;&#31995;&#25552;&#21462;&#21644;&#26412;&#20307;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of open knowledge graphs has led to a surge in scholarly research on the topic over the past decade. This paper presents a bibliometric analysis of the scholarly literature on open knowledge graphs published between 2013 and 2023. The study aims to identify the trends, patterns, and impact of research in this field, as well as the key topics and research questions that have emerged. The work uses bibliometric techniques to analyze a sample of 4445 scholarly articles retrieved from Scopus. The findings reveal an ever-increasing number of publications on open knowledge graphs published every year, particularly in developed countries (+50 per year). These outputs are published in highly-referred scholarly journals and conferences. The study identifies three main research themes: (1) knowledge graph construction and enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into NLP systems. Within these themes, the study identifies specific tasks that have 
&lt;/p&gt;</description></item><item><title>TRECVID&#26159;&#19968;&#31181;TREC&#39118;&#26684;&#30340;&#35270;&#39057;&#20998;&#26512;&#21644;&#26816;&#32034;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#20419;&#36827;&#25968;&#23383;&#35270;&#39057;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#26816;&#32034;&#20449;&#24687;&#12290;TRECVID 2022&#35745;&#21010;&#24320;&#23637;&#20845;&#20010;&#20219;&#21153;&#65292;&#26377;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;35&#20010;&#30740;&#31350;&#32452;&#32455;&#21442;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.13118</link><description>&lt;p&gt;
TRECVID 2022 &#20013;&#35780;&#20272;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An overview on the evaluated video retrieval tasks at TRECVID 2022. (arXiv:2306.13118v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13118
&lt;/p&gt;
&lt;p&gt;
TRECVID&#26159;&#19968;&#31181;TREC&#39118;&#26684;&#30340;&#35270;&#39057;&#20998;&#26512;&#21644;&#26816;&#32034;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#20419;&#36827;&#25968;&#23383;&#35270;&#39057;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#26816;&#32034;&#20449;&#24687;&#12290;TRECVID 2022&#35745;&#21010;&#24320;&#23637;&#20845;&#20010;&#20219;&#21153;&#65292;&#26377;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;35&#20010;&#30740;&#31350;&#32452;&#32455;&#21442;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TREC &#35270;&#39057;&#26816;&#32034;&#35780;&#20272;&#65288;TRECVID&#65289;&#26159;&#19968;&#31181; TREC &#39118;&#26684;&#30340;&#35270;&#39057;&#20998;&#26512;&#21644;&#26816;&#32034;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24320;&#25918;&#12289;&#20219;&#21153;&#39537;&#21160;&#30340;&#35780;&#20272;&#21644;&#27979;&#37327;&#26469;&#20419;&#36827;&#25968;&#23383;&#35270;&#39057;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#26816;&#32034;&#20449;&#24687;&#12290;&#22810;&#24180;&#26469;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#24050;&#32463;&#22312;&#22914;&#20309;&#26377;&#25928;&#22320;&#23436;&#25104;&#22788;&#29702;&#21644;&#22914;&#20309;&#21487;&#38752;&#22320;&#23545;&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;TRECVID &#30001;&#32654;&#22269;&#22269;&#23478;&#26631;&#20934;&#25216;&#26415;&#30740;&#31350;&#25152;&#65288;NIST&#65289;&#21644;&#20854;&#20182;&#32654;&#22269;&#25919;&#24220;&#26426;&#26500;&#36164;&#21161;&#65292;&#20197;&#21450;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;&#35768;&#22810;&#32452;&#32455;&#21644;&#20010;&#20154;&#36129;&#29486;&#20102;&#37325;&#35201;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290; TRECVID 2022&#35745;&#21010;&#24320;&#23637;&#20197;&#19979;&#20845;&#20010;&#20219;&#21153;&#65306;&#33258;&#36866;&#24212;&#35270;&#39057;&#25628;&#32034;&#12289;&#35270;&#39057;&#25991;&#26412;&#23383;&#24149;&#12289;&#28798;&#38590;&#22330;&#26223;&#25551;&#36848;&#21644;&#32034;&#24341;&#12289;&#25193;&#23637;&#35270;&#39057;&#20013;&#30340;&#27963;&#21160;&#12289;&#28145;&#24230;&#35270;&#39057;&#29702;&#35299;&#21644;&#30005;&#24433;&#25688;&#35201;&#12290;&#24635;&#20849;&#65292;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;35&#20010;&#30740;&#31350;&#32452;&#32455;&#25253;&#21517;&#21442;&#21152;&#20102;TRECVID 2022&#12290;
&lt;/p&gt;
&lt;p&gt;
The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, tasks-based evaluation supported by metrology. Over the last twenty-one years this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals worldwide contribute significant time and effort. TRECVID 2022 planned for the following six tasks: Ad-hoc video search, Video to text captioning, Disaster scene description and indexing, Activity in extended videos, deep video understanding, and movie summarization. In total, 35 teams from various research organizations worldwide signed up to 
&lt;/p&gt;</description></item><item><title>CompMix&#26159;&#19968;&#20010;&#24322;&#26500;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#22810;&#20010;&#20449;&#24687;&#28304;&#21644;&#22797;&#26434;&#24847;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#20844;&#24179;&#30340;&#35780;&#20272;QA&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12235</link><description>&lt;p&gt;
CompMix: &#19968;&#31181;&#24322;&#26500;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CompMix: A Benchmark for Heterogeneous Question Answering. (arXiv:2306.12235v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12235
&lt;/p&gt;
&lt;p&gt;
CompMix&#26159;&#19968;&#20010;&#24322;&#26500;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#22810;&#20010;&#20449;&#24687;&#28304;&#21644;&#22797;&#26434;&#24847;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#20844;&#24179;&#30340;&#35780;&#20272;QA&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#20026;&#20013;&#24515;&#30340;&#38382;&#31572;&#31995;&#32479;&#32463;&#24120;&#38656;&#35201;&#35775;&#38382;&#22810;&#31181;&#24322;&#26500;&#20449;&#24687;&#28304;&#12290;&#36890;&#36807;&#20849;&#21516;&#32771;&#34385;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#22914;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#25910;&#38598;&#21644;&#26469;&#33258;&#32593;&#32476;&#30340;&#34920;&#26684;&#65292;&#38382;&#31572;&#31995;&#32479;&#21487;&#20197;&#22686;&#24378;&#20854;&#31572;&#26696;&#35206;&#30422;&#33539;&#22260;&#21644;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; QA &#22522;&#20934;&#27979;&#35797;&#22823;&#22810;&#26159;&#20026;&#20102;&#26500;&#24314;&#21333;&#19968;&#30340;&#30693;&#35782;&#36164;&#28304;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#26080;&#27861;&#20844;&#24179;&#22320;&#35780;&#20272;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#20449;&#24687;&#24211;&#30340; QA &#31995;&#32479;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102; CompMix&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20247;&#21253;&#38382;&#31572;&#26500;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#33258;&#28982;&#22320;&#35201;&#27714;&#38598;&#25104;&#22810;&#31181;&#36755;&#20837;&#28304;&#12290;CompMix &#20849;&#26377; 9,410 &#20010;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#22797;&#26434;&#24847;&#22270;&#65292;&#22914;&#36830;&#25509;&#21644;&#26102;&#38388;&#26465;&#20214;&#12290;&#22312; CompMix &#19978;&#35780;&#20272;&#19968;&#31995;&#21015; QA &#31995;&#32479;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#28304;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-centric question answering (QA) often requires access to multiple, heterogeneous, information sources. By jointly considering several sources like a knowledge base (KB), a text collection, and tables from the web, QA systems can enhance their answer coverage and confidence. However, existing QA benchmarks are mostly constructed with a single source of knowledge in mind. This limits capabilities of these benchmarks to fairly evaluate QA systems that can tap into more than one information repository. To bridge this gap, we release CompMix, a crowdsourced QA benchmark which naturally demands the integration of a mixture of input sources. CompMix has a total of 9,410 questions, and features several complex intents like joins and temporal conditions. Evaluation of a range of QA systems on CompMix highlights the need for further research on leveraging information from heterogeneous sources.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#21327;&#35843;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36328;&#23186;&#20307;&#36175;&#37329;&#27963;&#21160;&#30340;&#20449;&#24687;&#12289;&#21442;&#19982;&#32773;&#12289;&#35770;&#22363;&#35780;&#35770;&#21644;&#31038;&#20132;&#23186;&#20307;URL&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#28508;&#22312;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.06601</link><description>&lt;p&gt;
&#19968;&#20221;&#21327;&#35843;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset of Coordinated Cryptocurrency-Related Social Media Campaigns. (arXiv:2301.06601v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#21327;&#35843;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36328;&#23186;&#20307;&#36175;&#37329;&#27963;&#21160;&#30340;&#20449;&#24687;&#12289;&#21442;&#19982;&#32773;&#12289;&#35770;&#22363;&#35780;&#35770;&#21644;&#31038;&#20132;&#23186;&#20307;URL&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#28508;&#22312;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36164;&#20135;&#30340;&#26222;&#21450;&#20351;&#24471;&#35768;&#22810;&#26032;&#25163;&#25237;&#36164;&#32773;&#36827;&#20837;&#20102;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#12290;&#36825;&#20123;&#25237;&#36164;&#32773;&#21487;&#20197;&#21463;&#21040;&#20182;&#20204;&#20174;&#31038;&#20132;&#23186;&#20307;&#19978;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#30340;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26377;&#20851;&#21152;&#23494;&#36135;&#24065;&#36175;&#37329;&#27963;&#21160;&#21644;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#27963;&#21160;&#21327;&#35843;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#65292;&#21019;&#36896;&#20154;&#20026;&#30340;&#8220;&#28818;&#20316;&#8221;&#65292;&#20197;&#24433;&#21709;&#21152;&#23494;&#39033;&#30446;&#20195;&#24065;&#30340;&#20215;&#26684;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;2014&#24180;5&#26376;&#21040;2022&#24180;12&#26376;&#26399;&#38388;&#20174;BitcoinTalk&#22312;&#32447;&#35770;&#22363;&#30340;Bounties(Altcoins)&#23376;&#35770;&#22363;&#25910;&#38598;&#30340;15.8K&#20010;&#36328;&#23186;&#20307;&#36175;&#37329;&#27963;&#21160;&#30340;&#20449;&#24687;&#12289;185K&#20010;&#21442;&#19982;&#32773;&#12289;10M&#26465;&#35770;&#22363;&#35780;&#35770;&#21644;82M&#20010;&#31038;&#20132;&#23186;&#20307;URL&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#26412;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#35768;&#22810;&#39046;&#22495;&#20869;&#25552;&#20379;&#30340;&#28508;&#22312;&#30740;&#31350;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in adoption of cryptoassets has brought many new and inexperienced investors in the cryptocurrency space. These investors can be disproportionally influenced by information they receive online, and particularly from social media. This paper presents a dataset of crypto-related bounty events and the users that participate in them. These events coordinate social media campaigns to create artificial "hype" around a crypto project in order to influence the price of its token. The dataset consists of information about 15.8K cross-media bounty events, 185K participants, 10M forum comments and 82M social media URLs collected from the Bounties(Altcoins) subforum of the BitcoinTalk online forum from May 2014 to December 2022. We describe the data collection and the data processing methods employed and we present a basic characterization of the dataset. Furthermore, we discuss potential research opportunities afforded by the dataset across many disciplines and we highlight potential nov
&lt;/p&gt;</description></item></channel></rss>