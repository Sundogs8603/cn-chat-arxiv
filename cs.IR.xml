<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;(PBNR)&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;PBNR&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07862</link><description>&lt;p&gt;
PBNR&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PBNR: Prompt-based News Recommender System. (arXiv:2304.07862v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;(PBNR)&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;PBNR&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26032;&#38395;&#24179;&#21488;&#36890;&#24120;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#25991;&#31456;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#39044;&#27979;&#29992;&#25143;&#19982;&#20505;&#36873;&#25991;&#31456;&#20043;&#38388;&#30340;&#21305;&#37197;&#24471;&#20998;&#65292;&#20197;&#21453;&#26144;&#29992;&#25143;&#23545;&#35813;&#25991;&#31456;&#30340;&#20559;&#22909;&#12290;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#36807;&#21435;&#34892;&#20026;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#24182;&#29702;&#35299;&#25991;&#31456;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22914;&#26524;&#32771;&#34385;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#23601;&#38656;&#35201;&#36827;&#34892;&#35843;&#25972;&#12290;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#30528;&#21457;&#23637;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21333;&#35789;&#20851;&#31995;&#25429;&#25417;&#21644;&#29702;&#35299;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25552;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#24320;&#21457;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20026;&#26356;&#22909;&#30340;&#25991;&#26412;&#29983;&#25104;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#23548;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#65288;PBNR&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#26032;&#38395;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PBNR&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PBNR&#27169;&#22411;&#22312;&#25512;&#33616;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#26032;&#38395;&#25991;&#31456;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online news platforms often use personalized news recommendation methods to help users discover articles that align with their interests. These methods typically predict a matching score between a user and a candidate article to reflect the user's preference for the article. Some previous works have used language model techniques, such as the attention mechanism, to capture users' interests based on their past behaviors, and to understand the content of articles. However, these existing model architectures require adjustments if additional information is taken into account. Pre-trained large language models, which can better capture word relationships and comprehend contexts, have seen a significant development in recent years, and these pre-trained models have the advantages of transfer learning and reducing the training time for downstream tasks. Meanwhile, prompt learning is a newly developed technique that leverages pre-trained language models by building task-specific guidance for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cold-Start based Multi-scenario Network&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#26053;&#28216;&#24179;&#21488;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#20013;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07858</link><description>&lt;p&gt;
&#38754;&#21521;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#20919;&#21551;&#21160;&#22810;&#22330;&#26223;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cold-Start based Multi-Scenario Ranking Model for Click-Through Rate Prediction. (arXiv:2304.07858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cold-Start based Multi-scenario Network&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#26053;&#28216;&#24179;&#21488;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#20013;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22810;&#22330;&#26223;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#65292;&#21363;&#20026;&#25152;&#26377;&#22330;&#26223;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22810;&#22330;&#26223;&#30340;CTR&#26041;&#27861;&#22312;&#22312;&#32447;&#26053;&#28216;&#24179;&#21488;&#65288;OTPs&#65289;&#39046;&#22495;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#30340;&#20919;&#21551;&#21160;&#29992;&#25143;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cold-Start based Multi-scenario Network (CSMN)&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online travel platforms (OTPs), e.g., Ctrip.com or Fliggy.com, can effectively provide travel-related products or services to users. In this paper, we focus on the multi-scenario click-through rate (CTR) prediction, i.e., training a unified model to serve all scenarios. Existing multi-scenario based CTR methods struggle in the context of OTP setting due to the ignorance of the cold-start users who have very limited data. To fill this gap, we propose a novel method named Cold-Start based Multi-scenario Network (CSMN). Specifically, it consists of two basic components including: 1) User Interest Projection Network (UIPN), which firstly purifies users' behaviors by eliminating the scenario-irrelevant information in behaviors with respect to the visiting scenario, followed by obtaining users' scenario-specific interests by summarizing the purified behaviors with respect to the target item via an attention mechanism; and 2) User Representation Memory Network (URMN), which benefits cold-star
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21477;&#23376;&#20013;&#30340;&#23454;&#38469;&#20449;&#24687;&#19977;&#20803;&#32452;&#31616;&#21270;&#21477;&#23376;&#65292;&#20197;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#30340;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2304.07774</link><description>&lt;p&gt;
&#25511;&#21046;&#35821;&#27861;&#31616;&#21270;&#30340;&#21477;&#27861;&#22797;&#26434;&#24615;&#37492;&#21035;&#12289;&#24230;&#37327;&#21644;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Syntactic Complexity Identification, Measurement, and Reduction Through Controlled Syntactic Simplification. (arXiv:2304.07774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21477;&#23376;&#20013;&#30340;&#23454;&#38469;&#20449;&#24687;&#19977;&#20803;&#32452;&#31616;&#21270;&#21477;&#23376;&#65292;&#20197;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#39046;&#22495;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21270;&#26041;&#24335;&#25506;&#32034;&#26356;&#26131;&#25026;&#30340;&#25991;&#26412;&#12290;&#20294;&#26159;&#65292;&#20102;&#35299;&#24182;&#20174;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#30693;&#35782;&#36890;&#24120;&#24456;&#38590;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#37319;&#29992;&#22797;&#21512;&#21477;&#21644;&#22797;&#26434;&#21477;&#24335;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#21477;&#23376;&#20197;&#25552;&#39640;&#21487;&#35835;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#31616;&#21333;&#30340;&#33521;&#35821;&#26367;&#25442;&#35789;&#21644;&#25688;&#35201;&#21477;&#23376;&#21644;&#27573;&#33853;&#12290;&#20294;&#26159;&#65292;&#22312;&#20174;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#20013;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#36807;&#31243;&#20013;&#65292;&#25688;&#35201;&#38271;&#21477;&#23376;&#21644;&#26367;&#25442;&#35789;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21477;&#23376;&#20013;&#30340;&#23454;&#38469;&#20449;&#24687;&#19977;&#20803;&#32452;&#30340;&#25511;&#21046;&#31616;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#21477;&#27861;&#20381;&#23384;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text simplification is one of the domains in Natural Language Processing (NLP) that offers an opportunity to understand the text in a simplified manner for exploration. However, it is always hard to understand and retrieve knowledge from unstructured text, which is usually in the form of compound and complex sentences. There are state-of-the-art neural network-based methods to simplify the sentences for improved readability while replacing words with plain English substitutes and summarising the sentences and paragraphs. In the Knowledge Graph (KG) creation process from unstructured text, summarising long sentences and substituting words is undesirable since this may lead to information loss. However, KG creation from text requires the extraction of all possible facts (triples) with the same mentions as in the text. In this work, we propose a controlled simplification based on the factual information in a sentence, i.e., triple. We present a classical syntactic dependency-based approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07763</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-optimized Contrastive Learning for Sequential Recommendation. (arXiv:2304.07763v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26159;&#35299;&#20915;&#31232;&#30095;&#19988;&#21547;&#22122;&#22768;&#25512;&#33616;&#25968;&#25454;&#30340;&#19968;&#20010;&#26032;&#20852;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#38024;&#23545;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#22686;&#24378;&#65292;&#35201;&#20040;&#21482;&#20351;&#29992;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#24456;&#38590;&#25512;&#24191;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HiCON&#26694;&#26550;&#65292;&#36816;&#29992;&#23618;&#27425;&#21270;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#20998;&#23618;&#28040;&#24687;&#32858;&#21512;&#26426;&#21046;&#36991;&#20813;&#20102;&#37051;&#23621;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07506</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Hierarchical and Contrastive Representation Learning for Knowledge-aware Recommendation. (arXiv:2304.07506v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HiCON&#26694;&#26550;&#65292;&#36816;&#29992;&#23618;&#27425;&#21270;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#20998;&#23618;&#28040;&#24687;&#32858;&#21512;&#26426;&#21046;&#36991;&#20813;&#20102;&#37051;&#23621;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;&#25512;&#33616;&#26159;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#26522;&#20030;&#22270;&#30340;&#37051;&#23621;&#36827;&#34892;&#36882;&#24402;&#23884;&#20837;&#20256;&#25773;&#12290;&#38543;&#30528;&#36339;&#25968;&#30340;&#22686;&#21152;&#65292;&#33410;&#28857;&#30340;&#37051;&#23621;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27492;&#36882;&#24402;&#20256;&#25773;&#20013;&#20102;&#35299;&#22823;&#37327;&#37051;&#23621;&#20197;&#25552;&#28860;&#39640;&#38454;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#24341;&#20837;&#26356;&#22810;&#26377;&#23475;&#22122;&#22768;&#65292;&#23548;&#33268;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#24444;&#27492;&#38590;&#20197;&#21306;&#20998;&#65292;&#21363;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiCON&#30340;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;&#30340;Hierarchical and Contrastive&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#37051;&#23621;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#28040;&#24687;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#21333;&#29420;&#19982;&#20302;&#38454;&#37051;&#23621;&#21644;&#20803;&#36335;&#24452;&#21463;&#38480;&#30340;&#39640;&#38454;&#37051;&#23621;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#40723;&#21169;&#30456;&#20284;&#30340;&#33410;&#28857;&#30456;&#20114;&#38752;&#36817;&#65292;&#32780;&#25226;&#19981;&#30456;&#20284;&#30340;&#33410;&#28857;&#25512;&#24320;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating knowledge graph into recommendation is an effective way to alleviate data sparsity. Most existing knowledge-aware methods usually perform recursive embedding propagation by enumerating graph neighbors. However, the number of nodes' neighbors grows exponentially as the hop number increases, forcing the nodes to be aware of vast neighbors under this recursive propagation for distilling the high-order semantic relatedness. This may induce more harmful noise than useful information into recommendation, leading the learned node representations to be indistinguishable from each other, that is, the well-known over-smoothing issue. To relieve this issue, we propose a Hierarchical and CONtrastive representation learning framework for knowledge-aware recommendation named HiCON. Specifically, for avoiding the exponential expansion of neighbors, we propose a hierarchical message aggregation mechanism to interact separately with low-order neighbors and meta-path-constrained high-order
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; TAP-GNN&#65292;&#36890;&#36807;&#25972;&#20010;&#37051;&#22495;&#30340;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#22270;&#27969;&#22330;&#26223;&#20013;&#25903;&#25345;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.07503</link><description>&lt;p&gt;
&#21160;&#24577;&#34920;&#31034;&#30340;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Aggregation and Propagation Graph Neural Networks for Dynamic Representation. (arXiv:2304.07503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; TAP-GNN&#65292;&#36890;&#36807;&#25972;&#20010;&#37051;&#22495;&#30340;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#22270;&#27969;&#22330;&#26223;&#20013;&#25903;&#25345;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23637;&#31034;&#20102;&#33410;&#28857;&#20043;&#38388;&#22312;&#36830;&#32493;&#26102;&#38388;&#20869;&#30340;&#21160;&#24577;&#20132;&#20114;&#65292;&#20854;&#25299;&#25169;&#38543;&#26102;&#38388;&#27969;&#36893;&#32780;&#28436;&#21464;&#12290;&#33410;&#28857;&#30340;&#25972;&#20010;&#26102;&#38388;&#37051;&#22495;&#26174;&#31034;&#20102;&#33410;&#28857;&#30340;&#21464;&#21270;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#31616;&#21270;&#36215;&#35265;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#37051;&#23621;&#29983;&#25104;&#21160;&#24577;&#34920;&#31034;&#65292;&#36825;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#21644;&#22312;&#32447;&#25512;&#29702;&#24310;&#36831;&#39640;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#20010;&#37051;&#22495;&#30340;&#26102;&#38388;&#22270;&#21367;&#31215;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#26102;&#38388;&#32858;&#21512;&#21644;&#20256;&#25773;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TAP-GNN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23637;&#24320;&#26102;&#38388;&#22270;&#26469;&#20998;&#26512;&#21160;&#24577;&#34920;&#31034;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20351;&#29992;&#32858;&#21512;&#21644;&#20256;&#25773;&#65288;AP&#65289;&#22359;&#26469;&#26174;&#33879;&#20943;&#23569;&#21382;&#21490;&#37051;&#23621;&#30340;&#37325;&#22797;&#35745;&#31639;&#12290;&#26368;&#32456;&#65292;TAP-GNN&#25903;&#25345;&#22312;&#22270;&#27969;&#22330;&#26223;&#20013;&#36827;&#34892;&#22312;&#32447;&#25512;&#29702;&#65292;&#26082;&#39640;&#25928;&#21448;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graphs exhibit dynamic interactions between nodes over continuous time, whose topologies evolve with time elapsing.  The whole temporal neighborhood of nodes reveals the varying preferences of nodes.  However, previous works usually generate dynamic representation with limited neighbors for simplicity, which results in both inferior performance and high latency of online inference.  Therefore, in this paper, we propose a novel method of temporal graph convolution with the whole neighborhood, namely Temporal Aggregation and Propagation Graph Neural Networks (TAP-GNN).  Specifically, we firstly analyze the computational complexity of the dynamic representation problem by unfolding the temporal graph in a message-passing paradigm.  The expensive complexity motivates us to design the AP (aggregation and propagation) block, which significantly reduces the repeated computation of historical neighbors.  The final TAP-GNN supports online inference in the graph stream scenario, which i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TIP-GNN&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#33410;&#28857;&#36716;&#31227;&#32467;&#26500;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#33410;&#28857;&#30340;&#20010;&#24615;&#21270;&#27169;&#24335;&#65292;&#20197;&#21450;&#22788;&#29702;&#26102;&#38388;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.07501</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#20256;&#25773;&#30340;&#26102;&#38388;&#32593;&#32476;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Transition Propagation Graph Neural Networks for Temporal Networks. (arXiv:2304.07501v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TIP-GNN&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#33410;&#28857;&#36716;&#31227;&#32467;&#26500;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#33410;&#28857;&#30340;&#20010;&#24615;&#21270;&#27169;&#24335;&#65292;&#20197;&#21450;&#22788;&#29702;&#26102;&#38388;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#32593;&#32476;&#30340;&#30740;&#31350;&#32773;&#19968;&#30452;&#33268;&#21147;&#20110;&#20174;&#33410;&#28857;&#20043;&#38388;&#30340;&#21508;&#31181;&#20114;&#21160;&#20013;&#25366;&#25496;&#20986;&#21160;&#24577;&#27169;&#24335;&#12290;(TIP-GNN)&#26159;&#38024;&#23545;&#33410;&#28857;&#36716;&#31227;&#32467;&#26500;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#23545;&#33410;&#28857;&#30340;&#20010;&#24615;&#21270;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#30456;&#24212;&#22320;&#25429;&#33719;&#33410;&#28857;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers of temporal networks (e.g., social networks and transaction networks) have been interested in mining dynamic patterns of nodes from their diverse interactions.  Inspired by recently powerful graph mining methods like skip-gram models and Graph Neural Networks (GNNs), existing approaches focus on generating temporal node embeddings sequentially with nodes' sequential interactions.  However, the sequential modeling of previous approaches cannot handle the transition structure between nodes' neighbors with limited memorization capacity.  Detailedly, an effective method for the transition structures is required to both model nodes' personalized patterns adaptively and capture node dynamics accordingly.  In this paper, we propose a method, namely Transition Propagation Graph Neural Networks (TIP-GNN), to tackle the challenges of encoding nodes' transition structures.  The proposed TIP-GNN focuses on the bilevel graph structure in temporal networks: besides the explicit interacti
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#25512;&#33616;&#31639;&#27861;&#22312;&#25968;&#25454;&#37327;&#20016;&#23500;&#21644;&#25968;&#25454;&#37327;&#36139;&#20047;&#30340;&#29992;&#25143;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#21457;&#29616;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#65292;&#31934;&#24230;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#29992;&#25143;&#20013;&#22987;&#32456;&#26356;&#39640;&#65307;&#24179;&#22343;&#31934;&#24230;&#30456;&#24403;&#65292;&#20294;&#20854;&#26041;&#24046;&#24456;&#22823;&#65307;&#24403;&#35780;&#20272;&#36807;&#31243;&#20013;&#37319;&#29992;&#36127;&#26679;&#26412;&#25277;&#26679;&#26102;&#65292;&#21484;&#22238;&#29575;&#20135;&#29983;&#21453;&#30452;&#35273;&#32467;&#26524;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#26159;&#25968;&#25454;&#36139;&#20047;&#30340;&#29992;&#25143;&#65307;&#38543;&#30528;&#29992;&#25143;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#20114;&#21160;&#22686;&#21152;&#65292;&#20182;&#20204;&#25910;&#21040;&#30340;&#25512;&#33616;&#36136;&#37327;&#20250;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.07487</link><description>&lt;p&gt;
&#26356;&#22810;&#19981;&#19968;&#23450;&#23601;&#26159;&#26356;&#22909;&#65306;&#20309;&#26102;&#25512;&#33616;&#31639;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#29992;&#25143;&#20013;&#34920;&#29616;&#19981;&#20339;&#65311;
&lt;/p&gt;
&lt;p&gt;
More Is Less: When Do Recommenders Underperform for Data-rich Users?. (arXiv:2304.07487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#25512;&#33616;&#31639;&#27861;&#22312;&#25968;&#25454;&#37327;&#20016;&#23500;&#21644;&#25968;&#25454;&#37327;&#36139;&#20047;&#30340;&#29992;&#25143;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#21457;&#29616;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#65292;&#31934;&#24230;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#29992;&#25143;&#20013;&#22987;&#32456;&#26356;&#39640;&#65307;&#24179;&#22343;&#31934;&#24230;&#30456;&#24403;&#65292;&#20294;&#20854;&#26041;&#24046;&#24456;&#22823;&#65307;&#24403;&#35780;&#20272;&#36807;&#31243;&#20013;&#37319;&#29992;&#36127;&#26679;&#26412;&#25277;&#26679;&#26102;&#65292;&#21484;&#22238;&#29575;&#20135;&#29983;&#21453;&#30452;&#35273;&#32467;&#26524;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#26159;&#25968;&#25454;&#36139;&#20047;&#30340;&#29992;&#25143;&#65307;&#38543;&#30528;&#29992;&#25143;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#20114;&#21160;&#22686;&#21152;&#65292;&#20182;&#20204;&#25910;&#21040;&#30340;&#25512;&#33616;&#36136;&#37327;&#20250;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#36890;&#24120;&#22312;&#19982;&#31639;&#27861;&#20114;&#21160;&#30340;&#27700;&#24179;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#20182;&#20204;&#25910;&#21040;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#21313;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#24212;&#29992;&#30340;&#19968;&#32452;&#27969;&#34892;&#35780;&#20272;&#25351;&#26631;&#65292;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#36139;&#20047;&#30340;&#29992;&#25143;&#24615;&#33021;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#20250;&#21457;&#25955;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#31934;&#24230;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#29992;&#25143;&#20013;&#22987;&#32456;&#26356;&#39640;&#65307;&#24179;&#22343;&#31934;&#24230;&#22343;&#31561;&#65292;&#20294;&#20854;&#26041;&#24046;&#24456;&#22823;&#65307;&#21484;&#22238;&#29575;&#20135;&#29983;&#20102;&#19968;&#20010;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#65292;&#31639;&#27861;&#22312;&#25968;&#25454;&#36139;&#20047;&#30340;&#29992;&#25143;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24403;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#37319;&#29992;&#36127;&#26679;&#26412;&#25277;&#26679;&#26102;&#65292;&#36825;&#31181;&#20559;&#24046;&#26356;&#21152;&#20005;&#37325;&#12290;&#26368;&#21518;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#29992;&#25143;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#20114;&#21160;&#22686;&#21152;&#65292;&#20182;&#20204;&#25910;&#21040;&#30340;&#25512;&#33616;&#36136;&#37327;&#20250;&#38477;&#20302;&#65288;&#20197;&#21484;&#22238;&#29575;&#34913;&#37327;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#65292;&#35780;&#20272;&#21512;&#29702;&#30340;&#25512;&#33616;&#31995;&#32479;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#21516;&#29992;&#25143;&#26377;&#19981;&#21516;&#30340;&#31995;&#32479;&#20114;&#20316;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users of recommender systems tend to differ in their level of interaction with these algorithms, which may affect the quality of recommendations they receive and lead to undesirable performance disparity. In this paper we investigate under what conditions the performance for data-rich and data-poor users diverges for a collection of popular evaluation metrics applied to ten benchmark datasets. We find that Precision is consistently higher for data-rich users across all the datasets; Mean Average Precision is comparable across user groups but its variance is large; Recall yields a counter-intuitive result where the algorithm performs better for data-poor than for data-rich users, which bias is further exacerbated when negative item sampling is employed during evaluation. The final observation suggests that as users interact more with recommender systems, the quality of recommendations they receive degrades (when measured by Recall). Our insights clearly show the importance of an evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#26041;&#38754;&#24050;&#32463;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#30456;&#20284;&#24230;&#26816;&#32034;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#20449;&#21495;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36991;&#20813;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07449</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#29992;&#20110;&#22522;&#20110;&#38899;&#20048;&#30456;&#20284;&#24230;&#26816;&#32034;&#21644;&#33258;&#21160;&#26631;&#27880;&#30340;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Auxiliary Loss for Metric Learning in Music Similarity-based Retrieval and Auto-tagging. (arXiv:2304.07449v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#26041;&#38754;&#24050;&#32463;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#30456;&#20284;&#24230;&#26816;&#32034;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#21516;&#26102;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#20449;&#21495;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36991;&#20813;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26816;&#32034;&#21644;&#33258;&#21160;&#26631;&#35760;&#26159;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#32771;&#34385;&#21040;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#38480;&#21046;&#24615;&#21644;&#19981;&#21487;&#25193;&#23637;&#24615;&#65292;&#35753;&#27169;&#22411;&#20174;&#20854;&#20182;&#26469;&#28304;&#23398;&#20064;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20165;&#20381;&#36182;&#20110;&#20174;&#38899;&#20048;&#38899;&#39057;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#22312;&#33258;&#21160;&#26631;&#35760;&#30340;&#32972;&#26223;&#19979;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#36741;&#21161;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#21516;&#26102;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#20449;&#21495;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#32780;&#19981;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36991;&#20813;&#22312;&#24494;&#35843;&#38454;&#27573;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of music information retrieval, similarity-based retrieval and auto-tagging serve as essential components. Given the limitations and non-scalability of human supervision signals, it becomes crucial for models to learn from alternative sources to enhance their performance. Self-supervised learning, which exclusively relies on learning signals derived from music audio data, has demonstrated its efficacy in the context of auto-tagging. In this study, we propose a model that builds on the self-supervised learning approach to address the similarity-based retrieval challenge by introducing our method of metric learning with a self-supervised auxiliary loss. Furthermore, diverging from conventional self-supervised learning methodologies, we discovered the advantages of concurrently training the model with both self-supervision and supervision signals, without freezing pre-trained models. We also found that refraining from employing augmentation during the fine-tuning phase yields
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#8220;&#38646;&#26679;&#26412;&#20027;&#39064;&#25512;&#26029;&#8221;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Sentence-BERT&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#65292;&#32780;&#22312;&#25928;&#29575;&#26041;&#38754;&#21017;&#20248;&#20808;&#36873;&#25321;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.07382</link><description>&lt;p&gt;
&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20027;&#39064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Multi-Label Topic Inference with Sentence Encoders. (arXiv:2304.07382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#8220;&#38646;&#26679;&#26412;&#20027;&#39064;&#25512;&#26029;&#8221;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Sentence-BERT&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#65292;&#32780;&#22312;&#25928;&#29575;&#26041;&#38754;&#21017;&#20248;&#20808;&#36873;&#25321;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#32534;&#30721;&#22120;&#22312;&#35768;&#22810;&#19979;&#28216;&#25991;&#26412;&#25366;&#25496;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#26159;&#30456;&#24403;&#36890;&#29992;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35814;&#32454;&#30740;&#31350;&#65292;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#8220;&#38646;&#26679;&#26412;&#20027;&#39064;&#25512;&#26029;&#8221;&#20219;&#21153;&#65292;&#20854;&#20013;&#20027;&#39064;&#26159;&#30001;&#29992;&#25143;&#23454;&#26102;&#23450;&#20041;/&#25552;&#20379;&#30340;&#12290;&#22312;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20854;&#20182;&#32534;&#30721;&#22120;&#65292;Sentence-BERT&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36890;&#29992;&#24615;&#65292;&#32780;&#24403;&#25928;&#29575;&#25104;&#20026;&#39318;&#35201;&#32771;&#34385;&#22240;&#32032;&#26102;&#65292;&#21487;&#20197;&#20248;&#20808;&#36873;&#25321;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence encoders have indeed been shown to achieve superior performances for many downstream text-mining tasks and, thus, claimed to be fairly general. Inspired by this, we performed a detailed study on how to leverage these sentence encoders for the "zero-shot topic inference" task, where the topics are defined/provided by the users in real-time. Extensive experiments on seven different datasets demonstrate that Sentence-BERT demonstrates superior generality compared to other encoders, while Universal Sentence Encoder can be preferred when efficiency is a top priority.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DiffRec&#65289;&#26469;&#36880;&#27493;&#21435;&#22122;&#22320;&#23398;&#20064;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#30340;&#36807;&#31243;&#65292;&#24182;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#31232;&#30095;&#25968;&#25454;&#31561;&#29420;&#29305;&#25361;&#25112;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04971</link><description>&lt;p&gt;
&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Recommender Model. (arXiv:2304.04971v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DiffRec&#65289;&#26469;&#36880;&#27493;&#21435;&#22122;&#22320;&#23398;&#20064;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#30340;&#36807;&#31243;&#65292;&#24182;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#31232;&#30095;&#25968;&#25454;&#31561;&#29420;&#29305;&#25361;&#25112;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#20132;&#20114;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;GANs&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;VAEs&#30340;&#21463;&#38480;&#34920;&#24449;&#33021;&#21147;&#12290;&#36825;&#20123;&#38480;&#21046;&#22952;&#30861;&#20102;&#22797;&#26434;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#36807;&#31243;&#30340;&#20934;&#30830;&#24314;&#27169;&#65292;&#20363;&#22914;&#30001;&#21508;&#31181;&#24178;&#25200;&#22240;&#32032;&#23548;&#33268;&#30340;&#22024;&#26434;&#20132;&#20114;&#12290;&#32771;&#34385;&#21040;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;&#31216;&#20026;DiffRec&#65289;&#65292;&#20197;&#36880;&#27493;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#20445;&#30041;&#29992;&#25143;&#20132;&#20114;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;DiffRec&#20943;&#23569;&#20102;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#24182;&#36991;&#20813;&#23558;&#29992;&#25143;&#20132;&#20114;&#25439;&#22351;&#20026;&#20687;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#32431;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;DMs&#20197;&#24212;&#23545;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#22914;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#31232;&#30095;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DiffRec&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are widely utilized to model the generative process of user interactions. However, these generative models suffer from intrinsic limitations such as the instability of GANs and the restricted representation ability of VAEs. Such limitations hinder the accurate modeling of the complex user interaction generation procedure, such as noisy interactions caused by various interference factors. In light of the impressive advantages of Diffusion Models (DMs) over traditional generative models in image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec) to learn the generative process in a denoising manner. To retain personalized information in user interactions, DiffRec reduces the added noises and avoids corrupting users' interactions into pure noises like in image synthesis. In addition, we extend traditional DMs to tackle the unique challenges in practical recommender 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28120;&#23453;&#25628;&#32034;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#26816;&#32034;&#20219;&#21153;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30446;&#21069;&#26381;&#21153;&#25968;&#20159;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2304.04377</link><description>&lt;p&gt;
&#25506;&#31350;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#30005;&#21830;&#20135;&#21697;&#26816;&#32034;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Delving into E-Commerce Product Retrieval with Vision-Language Pre-training. (arXiv:2304.04377v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28120;&#23453;&#25628;&#32034;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#26816;&#32034;&#20219;&#21153;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30446;&#21069;&#26381;&#21153;&#25968;&#20159;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21830;&#25628;&#32034;&#24341;&#25806;&#21253;&#25324;&#26816;&#32034;&#38454;&#27573;&#21644;&#25490;&#21517;&#38454;&#27573;&#65292;&#20854;&#20013;&#26816;&#32034;&#38454;&#27573;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#36820;&#22238;&#20505;&#36873;&#20135;&#21697;&#38598;&#12290;&#26368;&#36817;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#21644;&#35270;&#35273;&#32447;&#32034;&#32467;&#21512;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;V+L&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28120;&#23453;&#25628;&#32034;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20248;&#20110;&#24120;&#35268;&#22522;&#20110;&#22238;&#24402;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#26816;&#32034;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22312;&#32447;&#37096;&#32626;&#32454;&#33410;&#12290;&#28145;&#20837;&#30340;&#31163;&#32447;/&#22312;&#32447;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#29992;&#20316;&#28120;&#23453;&#25628;&#32034;&#30340;&#19968;&#20010;&#26816;&#32034;&#36890;&#36947;&#65292;&#24182;&#23454;&#26102;&#26381;&#21153;&#30528;&#25968;&#20159;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce search engines comprise a retrieval phase and a ranking phase, where the first one returns a candidate product set given user queries. Recently, vision-language pre-training, combining textual information with visual clues, has been popular in the application of retrieval tasks. In this paper, we propose a novel V+L pre-training method to solve the retrieval problem in Taobao Search. We design a visual pre-training task based on contrastive learning, outperforming common regression-based visual pre-training tasks. In addition, we adopt two negative sampling schemes, tailored for the large-scale retrieval task. Besides, we introduce the details of the online deployment of our proposed method in real-world situations. Extensive offline/online experiments demonstrate the superior performance of our method on the retrieval task. Our proposed method is employed as one retrieval channel of Taobao Search and serves hundreds of millions of users in real time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.03054</link><description>&lt;p&gt;
&#25805;&#32437;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;: &#29992;&#21512;&#25104;&#29992;&#25143;&#36827;&#34892;&#25915;&#20987;&#21450;&#20854;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures. (arXiv:2304.03054v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRecs&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#12290;&#22240;&#20026;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#21487;&#20197;&#36890;&#36807;&#19978;&#20256;&#26799;&#24230;&#30452;&#25509;&#24433;&#21709;&#31995;&#32479;&#65292;&#25152;&#20197;FedRecs&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#30340;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#21512;&#25104;&#29992;&#25143;&#36827;&#34892;&#30340;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20219;&#20309;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#32452;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;FedRecs &#65288;Fed-NCF&#21644;Fed-LightGCN&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Recommender Systems (FedRecs) are considered privacy-preserving techniques to collaboratively learn a recommendation model without sharing user data. Since all participants can directly influence the systems by uploading gradients, FedRecs are vulnerable to poisoning attacks of malicious clients. However, most existing poisoning attacks on FedRecs are either based on some prior knowledge or with less effectiveness. To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items' ranks and exposure rates effectively in the top-$K$ recommendation without relying on any prior knowledge. Specifically, our attack manipulates target items' exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items' alternative products. We conduct extensive experiments with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on two real-world recommendation datasets. The experimental res
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>RL4RS&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20351;&#29992;&#20154;&#36896;&#25968;&#25454;&#38598;&#21644;&#21322;&#20223;&#30495;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2110.11073</link><description>&lt;p&gt;
RL4RS&#65306;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender System. (arXiv:2110.11073v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11073
&lt;/p&gt;
&lt;p&gt;
RL4RS&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20351;&#29992;&#20154;&#36896;&#25968;&#25454;&#38598;&#21644;&#21322;&#20223;&#30495;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30446;&#26631;&#22312;&#20110;&#20174;&#19968;&#25209;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#23558;&#25512;&#33616;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#36890;&#24120;&#23384;&#22312;&#24040;&#22823;&#30340;&#29616;&#23454;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#27425;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#8212;&#8212;RL4RS&#65292;&#26088;&#22312;&#21462;&#20195;&#20043;&#21069;RL-based RS&#39046;&#22495;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#32780;&#20351;&#29992;&#30340;&#20154;&#24037;&#21644;&#21322;&#20223;&#30495;RS&#25968;&#25454;&#38598;&#12290;&#19982;&#23398;&#26415;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;RL-based RS&#38754;&#20020;&#30528;&#37096;&#32626;&#21069;&#38656;&#35201;&#36827;&#34892;&#33391;&#22909;&#39564;&#35777;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#23581;&#35797;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#29615;&#22659;&#27169;&#25311;&#35780;&#20272;&#12289;&#29615;&#22659;&#35780;&#20272;&#12289;&#21453;&#20107;&#23454;&#31574;&#30053;&#35780;&#20272;&#20197;&#21450;&#24314;&#31435;&#20110;&#27979;&#35797;&#38598;&#30340;&#29615;&#22659;&#35780;&#20272;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#36164;&#28304;RL4RS&#65288;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#24182;&#23545;&#29616;&#23454;&#24046;&#36317;&#31561;&#29305;&#27530;&#38382;&#39064;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning based recommender systems (RL-based RS) aim at learning a good policy from a batch of collected data, by casting recommendations to multi-step decision-making tasks. However, current RL-based RS research commonly has a large reality gap. In this paper, we introduce the first open-source real-world dataset, RL4RS, hoping to replace the artificial datasets and semi-simulated RS datasets previous studies used due to the resource limitation of the RL-based RS domain. Unlike academic RL research, RL-based RS suffers from the difficulties of being well-validated before deployment. We attempt to propose a new systematic evaluation framework, including evaluation of environment simulation, evaluation on environments, counterfactual policy evaluation, and evaluation on environments built from test set. In summary, the RL4RS (Reinforcement Learning for Recommender Systems), a new resource with special concerns on the reality gaps, contains two real-world datasets, data und
&lt;/p&gt;</description></item></channel></rss>