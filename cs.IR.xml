<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35770;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30452;&#25509;&#24341;&#29992;&#12289;&#25991;&#29486;&#32806;&#21512;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#31561;&#19981;&#21516;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#32452;&#21512;&#24341;&#25991;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#20849;&#21516;&#24341;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09295</link><description>&lt;p&gt;
&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#65306;&#23545;&#30452;&#25509;&#24341;&#29992;&#12289;&#25991;&#29486;&#32806;&#21512;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09295
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30452;&#25509;&#24341;&#29992;&#12289;&#25991;&#29486;&#32806;&#21512;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#31561;&#19981;&#21516;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#32452;&#21512;&#24341;&#25991;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#20849;&#21516;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#12290;&#20351;&#29992;&#31995;&#32479;&#35780;&#23457;&#20316;&#20026;&#22522;&#20934;&#65292;&#32467;&#21512;NIH&#24320;&#25918;&#24341;&#25991;&#25910;&#38598;&#30340;&#20986;&#29256;&#25968;&#25454;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#24341;&#25991;&#30340;&#26041;&#27861;&#8212;&#8212;&#30452;&#25509;&#24341;&#29992;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;&#25991;&#29486;&#32806;&#21512;&#22312;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#20197;&#21450;&#32452;&#21512;&#26041;&#27861;&#32435;&#20837;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23545;&#20808;&#21069;&#20351;&#29992;&#24341;&#25991;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#30340;&#26089;&#26399;&#30740;&#31350;&#36827;&#34892;&#20102;&#30456;&#24403;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#32467;&#26524;&#26174;&#31034;&#20849;&#21516;&#24341;&#29992;&#20248;&#20110;&#25991;&#29486;&#32806;&#21512;&#21644;&#30452;&#25509;&#24341;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#30740;&#31350;&#20013;&#65292;&#23558;&#36825;&#19977;&#31181;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#32988;&#36807;&#20165;&#20351;&#29992;&#20849;&#21516;&#24341;&#29992;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30740;&#31350;&#19968;&#33268;&#65292;&#23558;&#22522;&#20110;&#24341;&#25991;&#30340;&#26041;&#27861;&#19982;&#25991;&#26412;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09295v1 Announce Type: new  Abstract: In this contribution, we deal with seed-based information retrieval in networks of research publications. Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures. In addition, we include the PubMed Related Article score as well as combined approaches in the comparison. We also provide a fairly comprehensive review of earlier research in which citation relations have been used for information retrieval purposes. The results show an advantage for co-citation over bibliographic coupling and direct citation. However, combining the three approaches outperforms the exclusive use of co-citation in the study. The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#25351;&#26631;&#19982;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#30340;&#32479;&#35745;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.03915</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#21152;&#36895;A/B&#27979;&#35797;&#30340;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Metrics that Maximise Power for Accelerated A/B-Tests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#25351;&#26631;&#19982;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#30340;&#32479;&#35745;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25216;&#26415;&#20844;&#21496;&#20013;&#65292;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#33258;&#20449;&#30340;&#20915;&#31574;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;&#38271;&#26399;&#25910;&#20837;&#25110;&#29992;&#25143;&#20445;&#30041;&#65289;&#65292;&#22312;A/B&#27979;&#35797;&#20013;&#65292;&#33021;&#22815;&#22312;&#36825;&#20010;&#25351;&#26631;&#19978;&#26377;&#32479;&#35745;&#26174;&#33879;&#25552;&#21319;&#30340;&#31995;&#32479;&#21464;&#20307;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#36234;&#30340;&#12290;&#28982;&#32780;&#65292;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#20855;&#26377;&#26102;&#24310;&#21644;&#19981;&#25935;&#24863;&#24615;&#12290;&#22240;&#27492;&#65292;&#23454;&#39564;&#30340;&#25104;&#26412;&#24456;&#39640;&#65306;&#23454;&#39564;&#38656;&#35201;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#21363;&#20351;&#22914;&#27492;&#65292;&#20108;&#31867;&#38169;&#35823;&#65288;&#21363;&#20551;&#38452;&#24615;&#65289;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25351;&#26631;&#30452;&#25509;&#26368;&#22823;&#21270;&#23427;&#20204;&#30456;&#23545;&#20110;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#25152;&#20855;&#26377;&#30340;&#32479;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#23481;&#26131;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#21363;&#26356;&#39640;&#30340;&#24179;&#22343;&#24230;&#37327;&#25935;&#24863;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#25913;&#36827;&#20102;&#20108;&#31867;&#38169;&#35823;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#26368;&#23567;&#21270;&#25351;&#26631;&#22312;&#36807;&#21435;&#23454;&#39564;&#30340;$log$&#19978;&#20135;&#29983;&#30340;$p$-value&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#31243;&#24207;&#20013;&#25910;&#38598;&#20102;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;WP-ULTR&#65289;&#26041;&#27861;&#22788;&#29702;&#25972;&#39029; SERP &#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.10718</link><description>&lt;p&gt;
&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Whole Page Unbiased Learning to Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.10718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;WP-ULTR&#65289;&#26041;&#27861;&#22788;&#29702;&#25972;&#39029; SERP &#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#39029;&#38754;&#21576;&#29616;&#30340;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#28857;&#20987;&#34892;&#20026;&#26041;&#38754;&#30340;&#20559;&#24046;&#65292;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#20351;&#29992;&#38544;&#24335;&#29992;&#25143;&#21453;&#39304;&#26469;&#25913;&#36827;&#25490;&#24207;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(ULTR)&#31639;&#27861;&#65292;&#36890;&#36807;&#20559;&#24046;&#28857;&#20987;&#25968;&#25454;&#26469;&#23398;&#20064;&#19968;&#20010;&#26080;&#20559;&#30340;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20943;&#36731;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#20559;&#24046;&#65292;&#20363;&#22914;&#20449;&#20219;&#20559;&#24046;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#25628;&#32034;&#32467;&#26524;&#39029;&#38754;&#21576;&#29616;(SERP)&#20013;&#20854;&#20182;&#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#20363;&#22914;&#30001;&#22810;&#23186;&#20307;&#24341;&#21457;&#30340;&#21560;&#24341;&#20559;&#24046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#20559;&#24046;&#22312;&#24037;&#19994;&#31995;&#32479;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(WP-ULTR)&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#25972;&#39029;SERP&#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#12290;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65306;(1) &#24456;&#38590;&#25214;&#21040;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411; (&#29992;&#25143;&#34892;&#20026;&#20551;&#35774;)&#65307;(2) &#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank~(ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item></channel></rss>