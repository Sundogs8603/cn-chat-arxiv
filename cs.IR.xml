<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Dolos &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#21644;&#39044;&#38450;&#25945;&#32946;&#28304;&#20195;&#30721;&#25220;&#34989;&#30340;&#24037;&#20855;&#29983;&#24577;&#31995;&#32479;&#65292;&#22312;&#26368;&#26032;&#29256;&#26412;&#20013;&#21152;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#22312;&#27983;&#35272;&#22120;&#20013;&#36816;&#34892;&#25972;&#20010;&#25220;&#34989;&#26816;&#27979;&#27969;&#31243;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.10853</link><description>&lt;p&gt;
&#20351;&#29992;Dolos&#21457;&#29616;&#21644;&#25506;&#32034;&#25945;&#32946;&#28304;&#20195;&#30721;&#25220;&#34989;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Discovering and exploring cases of educational source code plagiarism with Dolos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10853
&lt;/p&gt;
&lt;p&gt;
Dolos &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#21644;&#39044;&#38450;&#25945;&#32946;&#28304;&#20195;&#30721;&#25220;&#34989;&#30340;&#24037;&#20855;&#29983;&#24577;&#31995;&#32479;&#65292;&#22312;&#26368;&#26032;&#29256;&#26412;&#20013;&#21152;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#22312;&#27983;&#35272;&#22120;&#20013;&#36816;&#34892;&#25972;&#20010;&#25220;&#34989;&#26816;&#27979;&#27969;&#31243;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#25220;&#34989;&#22312;&#25945;&#32946;&#23454;&#36341;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#38656;&#35201;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;&#36825;&#31181;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Dolos &#30340;&#26368;&#26032;&#29256;&#26412;&#65292;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#21644;&#39044;&#38450;&#25945;&#32946;&#28304;&#20195;&#30721;&#25220;&#34989;&#30340;&#24037;&#20855;&#29983;&#24577;&#31995;&#32479;&#12290;&#22312;&#36825;&#20010;&#26032;&#29256;&#26412;&#20013;&#65292;&#20027;&#35201;&#20391;&#37325;&#20110;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;&#25945;&#32946;&#24037;&#20316;&#32773;&#29616;&#22312;&#21487;&#20197;&#22312;&#20182;&#20204;&#30340;&#27983;&#35272;&#22120;&#20013;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#36816;&#34892;&#25972;&#20010;&#25220;&#34989;&#26816;&#27979;&#27969;&#31243;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#37197;&#32622;&#12290;&#23436;&#20840;&#37325;&#26032;&#35774;&#35745;&#30340;&#20998;&#26512;&#20202;&#34920;&#26495;&#21487;&#20197;&#21363;&#26102;&#35780;&#20272;&#19968;&#32452;&#28304;&#25991;&#20214;&#26159;&#21542;&#21253;&#21547;&#30097;&#20284;&#25220;&#34989;&#26696;&#20363;&#20197;&#21450;&#38598;&#21512;&#20013;&#25220;&#34989;&#26377;&#22810;&#26222;&#36941;&#12290;&#20202;&#34920;&#26495;&#25903;&#25345;&#20998;&#23618;&#32467;&#26500;&#21270;&#23548;&#33322;&#65292;&#20197;&#20415;&#36731;&#26494;&#32553;&#25918;&#21040;&#30097;&#20284;&#26696;&#20363;&#12290;&#38598;&#32676;&#26159;&#20202;&#34920;&#26495;&#35774;&#35745;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#32452;&#20214;&#65292;&#21453;&#26144;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10853v1 Announce Type: cross  Abstract: Source code plagiarism is a significant issue in educational practice, and educators need user-friendly tools to cope with such academic dishonesty. This article introduces the latest version of Dolos, a state-of-the-art ecosystem of tools for detecting and preventing plagiarism in educational source code. In this new version, the primary focus has been on enhancing the user experience. Educators can now run the entire plagiarism detection pipeline from a new web app in their browser, eliminating the need for any installation or configuration. Completely redesigned analytics dashboards provide an instant assessment of whether a collection of source files contains suspected cases of plagiarism and how widespread plagiarism is within the collection. The dashboards support hierarchically structured navigation to facilitate zooming in and out of suspect cases. Clusters are an essential new component of the dashboard design, reflecting the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.10805</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#22270;&#20687;&#29992;&#20110;&#26816;&#32034;&#21450;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#20854;&#33021;&#22815;&#35760;&#24518;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#24182;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#12290;&#22312;&#27492;&#33021;&#21147;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#33021;&#22815;&#22312;&#20854;&#21442;&#25968;&#20869;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#29992;&#25143;&#23545;&#35270;&#35273;&#20869;&#23481;&#30340;&#26597;&#35810;&#65292;MLLM&#34987;&#26399;&#26395;&#33021;&#22815;&#20174;&#20854;&#21442;&#25968;&#20013;&#8220;&#22238;&#24518;&#8221;&#30456;&#20851;&#22270;&#20687;&#20316;&#20026;&#21709;&#24212;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;MLLM&#20869;&#32622;&#30340;&#35270;&#35273;&#35760;&#24518;&#21644;&#35270;&#35273;&#26816;&#32034;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026;&#22270;&#20687;&#20998;&#37197;&#21807;&#19968;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#65292;&#24182;&#28041;&#21450;&#20004;&#20010;&#35757;&#32451;&#27493;&#39588;&#65306;&#23398;&#20064;&#35760;&#24518;&#21644;&#23398;&#20064;&#26816;&#32034;&#12290;&#31532;&#19968;&#27493;&#20391;&#37325;&#20110;&#35757;&#32451;MLLM&#35760;&#24518;&#22270;&#20687;&#19982;&#20854;&#26631;&#35782;&#31526;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to "recall" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33976;&#39311;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGR&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#21644;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10769</link><description>&lt;p&gt;
&#33976;&#39311;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Distillation Enhanced Generative Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33976;&#39311;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGR&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#21644;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#36890;&#36807;&#29983;&#25104;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#20316;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#35813;&#33539;&#24335;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#31232;&#30095;&#25110;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#36890;&#36807;&#33976;&#39311;&#36827;&#19968;&#27493;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#21487;&#34892;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGR&#30340;&#21487;&#34892;&#26694;&#26550;&#12290;DGR&#21033;&#29992;&#35832;&#22914;&#36328;&#32534;&#30721;&#22120;&#31561;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#65292;&#22312;&#25945;&#24072;&#35282;&#33394;&#20013;&#25552;&#20379;&#27573;&#33853;&#25490;&#21517;&#21015;&#34920;&#65292;&#25429;&#33719;&#27573;&#33853;&#30340;&#19981;&#21516;&#30456;&#20851;&#31243;&#24230;&#65292;&#32780;&#19981;&#26159;&#20108;&#20803;&#30828;&#26631;&#31614;&#65307;&#38543;&#21518;&#65292;DGR&#37319;&#29992;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#32771;&#34385;&#25945;&#24072;&#27169;&#22411;&#25552;&#20379;&#30340;&#27573;&#33853;&#25490;&#21517;&#39034;&#24207;&#20316;&#20026;&#26631;&#31614;&#12290;&#35813;&#26694;&#26550;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#33976;&#39311;&#27493;&#39588;&#26469;&#22686;&#24378;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#24182;&#19981;&#22686;&#21152;&#20219;&#20309;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden
&lt;/p&gt;</description></item><item><title>FairSync&#31639;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#26816;&#32034;&#38454;&#27573;&#21644;&#25490;&#21517;&#38454;&#27573;&#65292;&#30830;&#20445;&#20102;&#20998;&#24067;&#24335;&#25512;&#33616;&#26816;&#32034;&#20013;&#30340;&#32676;&#20307;&#26333;&#20809;&#22343;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.10628</link><description>&lt;p&gt;
FairSync: &#30830;&#20445;&#22312;&#20998;&#24067;&#24335;&#25512;&#33616;&#26816;&#32034;&#20013;&#30340;&#25674;&#38144;&#32676;&#20307;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10628
&lt;/p&gt;
&lt;p&gt;
FairSync&#31639;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#26816;&#32034;&#38454;&#27573;&#21644;&#25490;&#21517;&#38454;&#27573;&#65292;&#30830;&#20445;&#20102;&#20998;&#24067;&#24335;&#25512;&#33616;&#26816;&#32034;&#20013;&#30340;&#32676;&#20307;&#26333;&#20809;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36861;&#27714;&#20844;&#24179;&#21644;&#24179;&#34913;&#21457;&#23637;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#32676;&#20307;&#20844;&#24179;&#65292;&#30830;&#20445;&#29305;&#23450;&#32676;&#20307;&#22312;&#19968;&#23450;&#21608;&#26399;&#20869;&#32500;&#25345;&#26368;&#23567;&#27700;&#24179;&#30340;&#26333;&#20809;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#30830;&#20445;&#22312;&#20998;&#24067;&#24335;&#25512;&#33616;&#26816;&#32034;&#20013;&#30340;&#32676;&#20307;&#26333;&#20809;&#24179;&#34913;&#65292;&#25552;&#20986;&#20102;FairSync&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#26816;&#32034;&#38454;&#27573;&#21644;&#25490;&#21517;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#30340;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10628v1 Announce Type: new  Abstract: In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous fairness-aware works designed for stage-2 typically require accessing and traversing all item
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#25991;&#26412;&#23884;&#20837;&#23384;&#22312;&#21508;&#21521;&#24322;&#24615;&#35821;&#20041;&#31354;&#38388;&#65292;&#20854;&#39640;&#20313;&#24358;&#30456;&#20284;&#24230;&#22952;&#30861;&#20102;&#25512;&#33616;&#27169;&#22411;&#23545;&#39033;&#30446;&#34920;&#31034;&#36827;&#34892;&#26377;&#25928;&#21306;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30333;&#21270;&#22788;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10602</link><description>&lt;p&gt;
&#26159;&#21542;&#38656;&#35201;ID Embeddings&#65311;&#20026;&#20102;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#39044;&#35757;&#32451;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#30333;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10602
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#25991;&#26412;&#23884;&#20837;&#23384;&#22312;&#21508;&#21521;&#24322;&#24615;&#35821;&#20041;&#31354;&#38388;&#65292;&#20854;&#39640;&#20313;&#24358;&#30456;&#20284;&#24230;&#22952;&#30861;&#20102;&#25512;&#33616;&#27169;&#22411;&#23545;&#39033;&#30446;&#34920;&#31034;&#36827;&#34892;&#26377;&#25928;&#21306;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30333;&#21270;&#22788;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#23558;&#29289;&#21697;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#23884;&#20837;&#19982;&#29289;&#21697;ID&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20013;&#25991;&#26412;&#29305;&#24449;&#30340;&#34920;&#29616;&#21147;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#24378;&#35843;&#25512;&#33616;&#20013;ID&#23884;&#20837;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#26356;&#36827;&#19968;&#27493;&#65292;&#30740;&#31350;&#20165;&#20381;&#36182;&#25991;&#26412;&#29305;&#24449;&#32780;&#19981;&#38656;&#35201;ID&#23884;&#20837;&#30340;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#26816;&#39564;&#39044;&#35757;&#32451;&#25991;&#26412;&#23884;&#20837;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23384;&#22312;&#20110;&#19968;&#20010;&#21508;&#21521;&#24322;&#24615;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#39033;&#30446;&#20043;&#38388;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#36229;&#36807;0.8&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#36825;&#31181;&#21508;&#21521;&#24322;&#24615;&#29305;&#24615;&#38459;&#30861;&#20102;&#25512;&#33616;&#27169;&#22411;&#26377;&#25928;&#21306;&#20998;&#39033;&#30446;&#34920;&#31034;&#65292;&#24182;&#23548;&#33268;&#24615;&#33021;&#36864;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#19968;&#20010;&#21517;&#20026;&#30333;&#21270;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10602v1 Announce Type: new  Abstract: Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance. Despite their effectiveness, the expressive power of text features in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings. Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whiten
&lt;/p&gt;</description></item><item><title>SPAR&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#65292;&#22312;&#20250;&#35805;&#32423;&#21035;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#65292;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.10555</link><description>&lt;p&gt;
SPAR&#65306;&#36890;&#36807;&#38271;&#26399;&#21442;&#19982;&#27880;&#24847;&#21147;&#23454;&#29616;&#20010;&#24615;&#21270;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
SPAR: Personalized Content-Based Recommendation via Long Engagement Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10555
&lt;/p&gt;
&lt;p&gt;
SPAR&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#65292;&#22312;&#20250;&#35805;&#32423;&#21035;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#65292;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29992;&#25143;&#38271;&#26399;&#21442;&#19982;&#21382;&#21490;&#23545;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25104;&#21151;&#23548;&#33268;&#23427;&#20204;&#34987;&#29992;&#20110;&#32534;&#30721;&#29992;&#25143;&#21382;&#21490;&#21644;&#20505;&#36873;&#39033;&#65292;&#23558;&#20869;&#23481;&#25512;&#33616;&#35270;&#20026;&#25991;&#26412;&#35821;&#20041;&#21305;&#37197;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#22312;&#22788;&#29702;&#38750;&#24120;&#38271;&#30340;&#29992;&#25143;&#21382;&#21490;&#25991;&#26412;&#21644;&#19981;&#36275;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;SPAR&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#20174;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#20013;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#30340;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#20197;&#20250;&#35805;&#20026;&#22522;&#30784;&#23545;&#29992;&#25143;&#30340;&#21382;&#21490;&#36827;&#34892;&#32534;&#30721;&#12290;&#29992;&#25143;&#21644;&#29289;&#21697;&#20391;&#29305;&#24449;&#34987;&#20805;&#20998;&#34701;&#21512;&#36827;&#34892;&#21442;&#19982;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#21452;&#26041;&#30340;&#29420;&#31435;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#27169;&#22411;&#37096;&#32626;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10555v1 Announce Type: cross  Abstract: Leveraging users' long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20010;&#24615;&#21270;&#25628;&#32034;&#65288;CoPS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#21551;&#21457;&#33258;&#20154;&#31867;&#35748;&#30693;&#30340;&#35748;&#30693;&#35760;&#24518;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.10548</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39640;&#25928;&#35760;&#24518;&#26426;&#21046;&#25972;&#21512;&#30340;&#35748;&#30693;&#20010;&#24615;&#21270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20010;&#24615;&#21270;&#25628;&#32034;&#65288;CoPS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#21551;&#21457;&#33258;&#20154;&#31867;&#35748;&#30693;&#30340;&#35748;&#30693;&#35760;&#24518;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25628;&#32034;&#24341;&#25806;&#36890;&#24120;&#20026;&#25152;&#26377;&#29992;&#25143;&#25552;&#20379;&#30456;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#24573;&#35270;&#20102;&#20010;&#20307;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#21457;&#23637;&#20102;&#20010;&#24615;&#21270;&#25628;&#32034;&#65292;&#26681;&#25454;&#26597;&#35810;&#26085;&#24535;&#20013;&#33719;&#21462;&#30340;&#29992;&#25143;&#20559;&#22909;&#37325;&#26032;&#25490;&#24207;&#32467;&#26524;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#25628;&#32034;&#26041;&#27861;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20010;&#24615;&#21270;&#25628;&#32034;&#65288;CoPS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#21551;&#21457;&#33258;&#20154;&#31867;&#35748;&#30693;&#30340;&#35748;&#30693;&#35760;&#24518;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;CoPS&#21033;&#29992;LLMs&#26469;&#22686;&#24378;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#25628;&#32034;&#20307;&#39564;&#12290;&#35748;&#30693;&#35760;&#24518;&#26426;&#21046;&#21253;&#25324;&#24863;&#35273;&#35760;&#24518;&#20197;&#24555;&#36895;&#20570;&#20986;&#24863;&#24615;&#21453;&#24212;&#65292;&#24037;&#20316;&#35760;&#24518;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#35748;&#30693;&#21453;&#24212;&#65292;&#38271;&#26399;&#35760;&#24518;&#20197;&#23384;&#20648;&#21382;&#21490;&#20114;&#21160;&#12290;CoPS&#20351;&#29992;&#19977;&#27493;&#26041;&#27861;&#22788;&#29702;&#26032;&#26597;&#35810;&#65306;&#35782;&#21035;&#37325;&#26032;&#26597;&#25214;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10548v1 Announce Type: new  Abstract: Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition. CoPS employs LLMs to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behav
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10409</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#35770;&#25991;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#30740;&#31350;&#25345;&#32493;&#36827;&#34892;&#65292;&#38590;&#20197;&#36319;&#19978;&#26032;&#30340;&#30740;&#31350;&#21644;&#27169;&#22411;&#12290;&#20026;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#32508;&#21512;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#35768;&#22810;&#20154;&#20889;&#20102;&#35843;&#30740;&#35770;&#25991;&#65292;&#20294;&#21363;&#20351;&#36825;&#20123;&#35770;&#25991;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;&#35843;&#30740;&#35770;&#25991;&#20998;&#37197;&#21040;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;144&#31687;LLM&#35843;&#30740;&#35770;&#25991;&#30340;&#20803;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#19977;&#31181;&#33539;&#20363;&#26469;&#23545;&#20998;&#31867;&#27861;&#20869;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20004;&#20010;&#33539;&#20363;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;; &#20351;&#29992;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#24179;&#22343;&#20154;&#31867;&#35782;&#21035;&#27700;&#24179;&#65292;&#24182;&#19988;&#21033;&#29992;&#36739;&#23567;&#27169;&#22411;&#29983;&#25104;&#30340;&#24369;&#26631;&#31614;&#26469;&#24494;&#35843;LLMs&#65288;&#26412;&#30740;&#31350;&#20013;&#30340;GCN&#31561;&#65289;&#21487;&#33021;&#27604;&#20351;&#29992;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#26356;&#26377;&#25928;&#65292;&#25581;&#31034;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
&lt;/p&gt;</description></item><item><title>UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10381</link><description>&lt;p&gt;
UMAIR-FPS&#65306;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10381
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#27493;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21160;&#28459;&#25554;&#30011;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#25554;&#30011;&#24050;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21160;&#28459;&#25512;&#33616;&#31995;&#32479;&#20391;&#37325;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#20294;&#20173;&#38656;&#35201;&#25972;&#21512;&#22270;&#20687;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#25512;&#33616;&#30740;&#31350;&#21463;&#21040;&#32039;&#23494;&#32806;&#21512;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#21160;&#28459;&#25554;&#30011;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;&#65288;UMAIR-FPS&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#23545;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#25105;&#20204;&#39318;&#27425;&#32467;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#19982;&#35821;&#20041;&#29305;&#24449;&#26469;&#26500;&#24314;&#21452;&#36755;&#20986;&#22270;&#20687;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#34920;&#31034;&#12290;&#23545;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#22522;&#20110;Fine-tuning Sentence-Transformers&#33719;&#24471;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10381v1 Announce Type: cross  Abstract: The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20272;&#35745;&#38899;&#39640;&#31526;&#21495;&#21644;&#35843;&#24615;&#29468;&#27979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#20840;&#23616;&#35843;&#24615;&#65292;&#36824;&#21487;&#20197;&#22312;&#25972;&#20010;&#20998;&#26512;&#20048;&#26354;&#20013;&#35782;&#21035;&#23616;&#37096;&#35843;&#24615;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10247</link><description>&lt;p&gt;
&#38613;&#21051;&#23548;&#21521;&#30340;&#38899;&#39640;&#31526;&#21495;&#21644;&#23616;&#37096;&#21450;&#20840;&#23616;&#35843;&#24615;&#32852;&#21512;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Engraving Oriented Joint Estimation of Pitch Spelling and Local and Global Keys
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20272;&#35745;&#38899;&#39640;&#31526;&#21495;&#21644;&#35843;&#24615;&#29468;&#27979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#20840;&#23616;&#35843;&#24615;&#65292;&#36824;&#21487;&#20197;&#22312;&#25972;&#20010;&#20998;&#26512;&#20048;&#26354;&#20013;&#35782;&#21035;&#23616;&#37096;&#35843;&#24615;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21253;&#21547;&#26377;&#20851;&#38899;&#31526;&#36793;&#30028;&#30340;MIDI&#25991;&#20214;&#20013;&#32852;&#21512;&#20272;&#35745;&#38899;&#39640;&#31526;&#21495;&#21644;&#35843;&#24615;&#29468;&#27979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#20840;&#23616;&#35843;&#24615;&#65292;&#36824;&#21487;&#20197;&#22312;&#25972;&#20010;&#20998;&#26512;&#20048;&#26354;&#20013;&#35782;&#21035;&#23616;&#37096;&#35843;&#24615;&#12290;&#23427;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#26469;&#25628;&#32034;&#22312;&#21360;&#21047;&#35889;&#34920;&#20013;&#23558;&#26174;&#31034;&#30340;&#24847;&#22806;&#31526;&#21495;&#25968;&#37327;&#26041;&#38754;&#30340;&#26368;&#20339;&#31526;&#21495;&#12290;&#23545;&#35813;&#25968;&#37327;&#30340;&#35780;&#20272;&#19982;&#20840;&#23616;&#35843;&#24615;&#20197;&#21450;&#19968;&#20123;&#23616;&#37096;&#35843;&#24615;&#30340;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#27599;&#20010;&#23567;&#33410;&#37117;&#26377;&#19968;&#20010;&#23616;&#37096;&#35843;&#24615;&#12290;&#36825;&#19977;&#31181;&#20449;&#24687;&#20013;&#30340;&#27599;&#19968;&#31181;&#37117;&#29992;&#20110;&#20272;&#35745;&#20854;&#20182;&#20449;&#24687;&#65292;&#22312;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#36807;&#31243;&#20013;&#12290;&#23545;&#21253;&#21547;&#24635;&#20849;216464&#20010;&#38899;&#31526;&#30340;&#21333;&#38899;&#21644;&#38050;&#29748;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#38899;&#39640;&#31526;&#21495;&#65288;&#22312;&#24052;&#36203;&#20316;&#21697;&#38598;&#20013;&#24179;&#22343;&#20026;99.5%&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#20026;98.2%&#65289;&#36824;&#26159;&#20840;&#23616;&#35843;&#24615;&#31526;&#21495;&#20272;&#35745;&#65288;&#24179;&#22343;&#20026;93.0%&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10247v1 Announce Type: cross  Abstract: We revisit the problems of pitch spelling and tonality guessing with a new algorithm for their joint estimation from a MIDI file including information about the measure boundaries. Our algorithm does not only identify a global key but also local ones all along the analyzed piece. It uses Dynamic Programming techniques to search for an optimal spelling in term, roughly, of the number of accidental symbols that would be displayed in the engraved score. The evaluation of this number is coupled with an estimation of the global key and some local keys, one for each measure. Each of the three informations is used for the estimation of the other, in a multi-steps procedure. An evaluation conducted on a monophonic and a piano dataset, comprising 216 464 notes in total, shows a high degree of accuracy, both for pitch spelling (99.5% on average on the Bach corpus and 98.2% on the whole dataset) and global key signature estimation (93.0% on avera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#22411;CTR&#27169;&#22411;CETN&#65292;&#29992;&#20110;&#30830;&#20445;&#29305;&#24449;&#20132;&#20114;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#21644;&#21516;&#36136;&#24615;</title><link>https://arxiv.org/abs/2312.09715</link><description>&lt;p&gt;
CETN: &#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#23545;&#27604;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CETN: Contrast-enhanced Through Network for CTR Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#22411;CTR&#27169;&#22411;CETN&#65292;&#29992;&#20110;&#30830;&#20445;&#29305;&#24449;&#20132;&#20114;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#21644;&#21516;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#22312;&#20010;&#24615;&#21270;&#20449;&#24687;&#26816;&#32034;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#12289;&#22312;&#32447;&#24191;&#21578;&#21644;&#32593;&#32476;&#25628;&#32034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;CTR&#39044;&#27979;&#27169;&#22411;&#21033;&#29992;&#26174;&#24335;&#29305;&#24449;&#20132;&#20114;&#26469;&#20811;&#26381;&#38544;&#24335;&#29305;&#24449;&#20132;&#20114;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24182;&#34892;&#32467;&#26500;&#65288;&#20363;&#22914;DCN&#12289;FinalMLP&#12289;xDeepFM&#65289;&#30340;&#28145;&#24230;CTR&#27169;&#22411;&#34987;&#25552;&#20986;&#65292;&#20197;&#20174;&#19981;&#21516;&#35821;&#20041;&#31354;&#38388;&#20013;&#33719;&#21462;&#32852;&#21512;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24182;&#34892;&#23376;&#32452;&#20214;&#32570;&#20047;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36825;&#20351;&#24471;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#35821;&#20041;&#31354;&#38388;&#20013;&#26377;&#20215;&#20540;&#30340;&#22810;&#35270;&#22270;&#29305;&#24449;&#20132;&#20114;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#22411;CTR&#27169;&#22411;&#65306;&#29992;&#20110;CTR&#30340;&#22686;&#24378;&#23545;&#27604;&#32593;&#32476;&#65288;CETN&#65289;&#65292;&#20197;&#30830;&#20445;&#29305;&#24449;&#20132;&#20114;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#21644;&#21516;&#36136;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CETN&#37319;&#29992;&#22522;&#20110;&#20135;&#21697;&#30340;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09715v2 Announce Type: replace  Abstract: Click-through rate (CTR) Prediction is a crucial task in personalized information retrievals, such as industrial recommender systems, online advertising, and web search. Most existing CTR Prediction models utilize explicit feature interactions to overcome the performance bottleneck of implicit feature interactions. Hence, deep CTR models based on parallel structures (e.g., DCN, FinalMLP, xDeepFM) have been proposed to obtain joint information from different semantic spaces. However, these parallel subcomponents lack effective supervisory signals, making it challenging to efficiently capture valuable multi-views feature interaction information in different semantic spaces. To address this issue, we propose a simple yet effective novel CTR model: Contrast-enhanced Through Network for CTR (CETN), so as to ensure the diversity and homogeneity of feature interaction information. Specifically, CETN employs product-based feature interaction
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21033;&#29992;&#20108;&#36827;&#21046;&#20449;&#24687;&#29983;&#25104;&#20016;&#23500;&#30340;&#22270;&#20687;&#25551;&#36848;&#31526;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#32959;&#30244;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#26816;&#27979;&#24739;&#32773;&#29305;&#24449;&#24182;&#25512;&#33616;&#26368;&#30456;&#20284;&#30149;&#20363;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2301.03701</link><description>&lt;p&gt;
&#32959;&#30244;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Model for Tumoral Clinical Decision Support Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21033;&#29992;&#20108;&#36827;&#21046;&#20449;&#24687;&#29983;&#25104;&#20016;&#23500;&#30340;&#22270;&#20687;&#25551;&#36848;&#31526;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#32959;&#30244;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#26816;&#27979;&#24739;&#32773;&#29305;&#24449;&#24182;&#25512;&#33616;&#26368;&#30456;&#20284;&#30149;&#20363;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#35786;&#26029;&#24615;&#33041;&#30244;&#35780;&#20272;&#65292;&#21033;&#29992;&#21307;&#30103;&#20013;&#24515;&#30340;&#20449;&#24687;&#26469;&#27604;&#36739;&#31867;&#20284;&#30149;&#20363;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26816;&#32034;&#32473;&#23450;&#26597;&#35810;&#30340;&#26368;&#30456;&#20284;&#30340;&#33041;&#30244;&#30149;&#20363;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#26469;&#25913;&#21892;&#35786;&#26029;&#36807;&#31243;&#65292;&#29305;&#21035;&#20851;&#27880;&#24739;&#32773;&#29305;&#23450;&#27491;&#24120;&#29305;&#24449;&#21644;&#30149;&#29702;&#12290;&#19982;&#20808;&#21069;&#27169;&#22411;&#30340;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#65292;&#23427;&#33021;&#22815;&#20165;&#20174;&#20108;&#36827;&#21046;&#20449;&#24687;&#29983;&#25104;&#20016;&#23500;&#30340;&#22270;&#20687;&#25551;&#36848;&#31526;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#24471;&#30340;&#32959;&#30244;&#20998;&#21106;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03701v2 Announce Type: replace-cross  Abstract: Comparative diagnostic in brain tumor evaluation makes possible to use the available information of a medical center to compare similar cases when a new patient is evaluated. By leveraging Artificial Intelligence models, the proposed system is able of retrieving the most similar cases of brain tumors for a given query. The primary objective is to enhance the diagnostic process by generating more accurate representations of medical images, with a particular focus on patient-specific normal features and pathologies. A key distinction from previous models lies in its ability to produce enriched image descriptors solely from binary information, eliminating the need for costly and difficult to obtain tumor segmentation.   The proposed model uses Artificial Intelligence to detect patient features to recommend the most similar cases from a database. The system not only suggests similar cases but also balances the representation of hea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#25429;&#25417;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#31163;&#24320;&#30340;&#24773;&#20917;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#29992;&#25143;&#20849;&#20139;&#30456;&#21516;&#31867;&#22411;&#26102;&#65292;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2203.13423</link><description>&lt;p&gt;
&#29992;&#31163;&#24320;&#30340;&#36172;&#21338;&#26426;&#27169;&#22411;&#24314;&#35758;&#31995;&#32479;&#20013;&#30340;&#27969;&#22833;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Modeling Attrition in Recommender Systems with Departing Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#25429;&#25417;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#31163;&#24320;&#30340;&#24773;&#20917;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#29992;&#25143;&#20849;&#20139;&#30456;&#21516;&#31867;&#22411;&#26102;&#65292;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#31574;&#30053;&#24433;&#21709;&#22870;&#21169;&#30340;&#33719;&#21462;&#65292;&#20294;&#19981;&#24433;&#21709;&#20132;&#20114;&#30340;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19981;&#28385;&#36275;&#30340;&#29992;&#25143;&#21487;&#33021;&#20250;&#31163;&#24320;&#65288;&#24182;&#27704;&#36828;&#19981;&#20877;&#22238;&#26469;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25429;&#25417;&#36825;&#31181;&#31574;&#30053;&#20381;&#36182;&#24615;&#26102;&#27573;&#30340;&#26032;&#22411;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#26377;&#38480;&#30340;&#29992;&#25143;&#31867;&#22411;&#38598;&#21512;&#65292;&#21644;&#22810;&#20010;&#20855;&#26377;&#20271;&#21162;&#21033;&#22238;&#25253;&#30340;&#33218;&#12290;&#27599;&#20010;&#65288;&#29992;&#25143;&#31867;&#22411;&#65292;&#33218;&#65289;&#20803;&#32452;&#23545;&#24212;&#19968;&#20010;&#65288;&#26410;&#30693;&#30340;&#65289;&#22870;&#21169;&#27010;&#29575;&#12290;&#27599;&#20010;&#29992;&#25143;&#30340;&#31867;&#22411;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;&#20854;&#23545;&#25512;&#33616;&#30340;&#21709;&#24212;&#26469;&#25512;&#26029;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#29992;&#25143;&#23545;&#20182;&#20204;&#30340;&#25512;&#33616;&#19981;&#28385;&#24847;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#31163;&#24320;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#20915;&#20102;&#25152;&#26377;&#29992;&#25143;&#20849;&#20139;&#30456;&#21516;&#31867;&#22411;&#30340;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#26368;&#36817;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#29992;&#25143;&#20998;&#20026;&#20004;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.13423v2 Announce Type: replace  Abstract: Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisfied users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a finite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user's type is initially unknown and can only be inferred through their response to recommendations. Moreover, if a user is dissatisfied with their recommendation, they might depart the system. We first address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16659</link><description>&lt;p&gt;
&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25628;&#32034;&#36890;&#36807;&#23454;&#29616;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#36718;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#20449;&#24687;&#26816;&#32034;&#30340;&#20415;&#21033;&#12290;&#25903;&#25345;&#36825;&#31181;&#20132;&#20114;&#38656;&#35201;&#23545;&#23545;&#35805;&#36755;&#20837;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20197;&#20415;&#26681;&#25454;&#21382;&#21490;&#20449;&#24687;&#21046;&#23450;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#29305;&#21035;&#26159;&#65292;&#25628;&#32034;&#26597;&#35810;&#24212;&#21253;&#25324;&#26469;&#33258;&#20808;&#21069;&#23545;&#35805;&#22238;&#21512;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#32463;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#19987;&#38376;&#26816;&#32034;&#22120;&#36827;&#34892;&#25972;&#20010;&#23545;&#35805;&#24335;&#25628;&#32034;&#20250;&#35805;&#30340;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#20887;&#38271;&#21644;&#22024;&#26434;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25163;&#21160;&#30417;&#30563;&#20449;&#21495;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;(HAConvDR)&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#20010;&#24605;&#24819;&#65306;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#21644;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#36827;&#34892;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item></channel></rss>