<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26032;&#38395;&#25512;&#33616;&#39046;&#22495;&#39318;&#27425;&#37319;&#29992;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#20363;&#26469;&#24320;&#21457;Prompt4NR&#26694;&#26550;&#30340;&#23454;&#39564;&#12290;&#35813;&#26694;&#26550;&#23558;&#39044;&#27979;&#28857;&#20987;&#20505;&#36873;&#26032;&#38395;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;cloze-style&#22635;&#31354;&#24335;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#35821;&#35328;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.05263</link><description>&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning for News Recommendation. (arXiv:2304.05263v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26032;&#38395;&#25512;&#33616;&#39046;&#22495;&#39318;&#27425;&#37319;&#29992;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#20363;&#26469;&#24320;&#21457;Prompt4NR&#26694;&#26550;&#30340;&#23454;&#39564;&#12290;&#35813;&#26694;&#26550;&#23558;&#39044;&#27979;&#28857;&#20987;&#20505;&#36873;&#26032;&#38395;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;cloze-style&#22635;&#31354;&#24335;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#35821;&#35328;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#26032;&#38395;&#25512;&#33616;&#65288;NR&#65289;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26469;&#32534;&#30721;&#26032;&#38395;&#34920;&#31034;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25512;&#33616;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#21644;&#30446;&#26631;&#20989;&#25968;&#26469;&#36981;&#24490;&#39321;&#33609;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#20363;&#12290;&#30001;&#20110;&#20219;&#21153;&#30446;&#26631;&#19982;PLM&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#35748;&#20026;&#20182;&#20204;&#30340;&#24314;&#27169;&#33539;&#24335;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23884;&#20837;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#35821;&#35328;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#21644;&#39044;&#27979;&#33539;&#20363;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#23581;&#35797;&#20351;&#29992;&#36825;&#31181;&#26032;&#33539;&#20363;&#26469;&#24320;&#21457;&#19968;&#20010;&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;Prompt Learning (Prompt4NR) &#26694;&#26550;&#65292;&#23558;&#39044;&#27979;&#29992;&#25143;&#26159;&#21542;&#20250;&#28857;&#20987;&#20505;&#36873;&#26032;&#38395;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;&#22635;&#31354;&#24335;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;prompt&#27169;&#26495;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;...
&lt;/p&gt;
&lt;p&gt;
Some recent \textit{news recommendation} (NR) methods introduce a Pre-trained Language Model (PLM) to encode news representation by following the vanilla pre-train and fine-tune paradigm with carefully-designed recommendation-specific neural networks and objective functions. Due to the inconsistent task objective with that of PLM, we argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process. Recently, the pre-train, prompt, and predict paradigm, called \textit{prompt learning}, has achieved many successes in natural language processing domain. In this paper, we make the first trial of this new paradigm to develop a \textit{Prompt Learning for News Recommendation} (Prompt4NR) framework, which transforms the task of predicting whether a user would click a candidate news as a cloze-style mask-prediction task. Specifically, we design a series of prompt templates, including discrete, continuous, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Audio Bank&#30340;&#38899;&#39057;&#20449;&#21495;&#34920;&#24449;&#26694;&#26550;&#65292;&#20854;&#30001;&#22312;&#26102;&#38388;-&#39057;&#29575;&#31354;&#38388;&#20013;&#34920;&#31034;&#27599;&#20010;&#38899;&#39057;&#31867;&#21035;&#30340;&#29420;&#29305;&#38899;&#39057;&#26816;&#27979;&#22120;&#32452;&#25104;&#12290;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38477;&#20302;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21487;&#21306;&#20998;&#24615;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#36827;&#34892;&#38899;&#39057;&#35782;&#21035;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05067</link><description>&lt;p&gt;
Audio Bank&#65306;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20107;&#20214;&#35782;&#21035;&#30340;&#39640;&#32423;&#22768;&#23398;&#20449;&#21495;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio Bank: A High-Level Acoustic Signal Representation for Audio Event Recognition. (arXiv:2304.05067v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Audio Bank&#30340;&#38899;&#39057;&#20449;&#21495;&#34920;&#24449;&#26694;&#26550;&#65292;&#20854;&#30001;&#22312;&#26102;&#38388;-&#39057;&#29575;&#31354;&#38388;&#20013;&#34920;&#31034;&#27599;&#20010;&#38899;&#39057;&#31867;&#21035;&#30340;&#29420;&#29305;&#38899;&#39057;&#26816;&#27979;&#22120;&#32452;&#25104;&#12290;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38477;&#20302;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21487;&#21306;&#20998;&#24615;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#36827;&#34892;&#38899;&#39057;&#35782;&#21035;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38899;&#39057;&#20107;&#20214;&#35782;&#21035;&#22312;&#20351;&#20154;&#26426;&#20132;&#20114;&#26356;&#21152;&#32039;&#23494;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#24037;&#19994;&#33258;&#21160;&#21270;&#12289;&#25511;&#21046;&#21644;&#30417;&#35270;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#38899;&#39057;&#20107;&#20214;&#30001;&#32439;&#32321;&#22797;&#26434;&#30340;&#22768;&#38901;&#27169;&#24335;&#32452;&#25104;&#65292;&#23427;&#20204;&#22312;&#35856;&#27874;&#19978;&#32416;&#32544;&#22312;&#19968;&#36215;&#12290;&#38899;&#39057;&#35782;&#21035;&#20027;&#35201;&#21463;&#20302;&#32423;&#21644;&#20013;&#32423;&#29305;&#24449;&#30340;&#25511;&#21046;&#65292;&#36825;&#20123;&#29305;&#24449;&#24050;&#35777;&#26126;&#20854;&#35782;&#21035;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#35821;&#20041;&#24847;&#20041;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#38899;&#39057;&#35782;&#21035;&#26694;&#26550;&#12290;Audio Bank&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#39640;&#32423;&#34920;&#24449;&#65292;&#30001;&#22312;&#39057;&#29575;&#26102;&#38388;&#31354;&#38388;&#20013;&#34920;&#31034;&#27599;&#20010;&#38899;&#39057;&#31867;&#21035;&#30340;&#29420;&#29305;&#38899;&#39057;&#26816;&#27979;&#22120;&#32452;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20943;&#23569;&#32467;&#26524;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21487;&#21306;&#20998;&#24615;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23545;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#65288;SVM&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#21644;k&#26368;&#36817;&#37051;&#65289;&#36827;&#34892;&#38899;&#39057;&#35782;&#21035;&#65292;&#35777;&#26126;&#20102;Audio Bank&#26694;&#26550;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic audio event recognition plays a pivotal role in making human robot interaction more closer and has a wide applicability in industrial automation, control and surveillance systems. Audio event is composed of intricate phonic patterns which are harmonically entangled. Audio recognition is dominated by low and mid-level features, which have demonstrated their recognition capability but they have high computational cost and low semantic meaning. In this paper, we propose a new computationally efficient framework for audio recognition. Audio Bank, a new high-level representation of audio, is comprised of distinctive audio detectors representing each audio class in frequency-temporal space. Dimensionality of the resulting feature vector is reduced using non-negative matrix factorization preserving its discriminability and rich semantic information. The high audio recognition performance using several classifiers (SVM, neural network, Gaussian process classification and k-nearest ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26080;&#20559;&#25104;&#23545;&#23398;&#20064;&#65288;NPLwVC&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#30340;&#20559;&#20272;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05066</link><description>&lt;p&gt;
&#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#26080;&#20559;&#25104;&#23545;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unbiased Pairwise Learning from Implicit Feedback for Recommender Systems without Biased Variance Control. (arXiv:2304.05066v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26080;&#20559;&#25104;&#23545;&#23398;&#20064;&#65288;NPLwVC&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#30340;&#20559;&#20272;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#35757;&#32451;&#19968;&#33324;&#22522;&#20110;&#26174;&#24335;&#21453;&#39304;&#21644;&#38544;&#24335;&#21453;&#39304;&#20004;&#31181;&#25968;&#25454;&#12290;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#20013;&#20165;&#21253;&#21547;&#27491;&#21453;&#39304;&#65292;&#22240;&#27492;&#24456;&#38590;&#21028;&#26029;&#26410;&#20114;&#21160;&#39033;&#21040;&#24213;&#26159;&#36127;&#21453;&#39304;&#36824;&#26159;&#26410;&#26333;&#20809;&#12290;&#21516;&#26102;&#65292;&#31232;&#26377;&#29289;&#21697;&#30340;&#30456;&#20851;&#24615;&#24448;&#24448;&#34987;&#20302;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20808;&#21069;&#25552;&#20986;&#20102;&#26080;&#20559;&#25104;&#23545;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23384;&#22312;&#20559;&#24046;&#26041;&#24046;&#25511;&#21046;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#26080;&#20559;&#25104;&#23545;&#23398;&#20064;&#65288;NPLwVC&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#20559;&#24046;&#26041;&#24046;&#25511;&#21046;&#39033;&#65292;&#20174;&#32780;&#31616;&#21270;&#31639;&#27861;&#12290;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NPLwVC&#22312;&#20445;&#25345;&#26080;&#20559;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally speaking, the model training for recommender systems can be based on two types of data, namely explicit feedback and implicit feedback. Moreover, because of its general availability, we see wide adoption of implicit feedback data, such as click signal. There are mainly two challenges for the application of implicit feedback. First, implicit data just includes positive feedback. Therefore, we are not sure whether the non-interacted items are really negative or positive but not displayed to the corresponding user. Moreover, the relevance of rare items is usually underestimated since much fewer positive feedback of rare items is collected compared with popular ones. To tackle such difficulties, both pointwise and pairwise solutions are proposed before for unbiased relevance learning. As pairwise learning suits well for the ranking tasks, the previously proposed unbiased pairwise learning algorithm already achieves state-of-the-art performance. Nonetheless, the existing unbiased 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DiffRec&#65289;&#26469;&#36880;&#27493;&#21435;&#22122;&#22320;&#23398;&#20064;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#30340;&#36807;&#31243;&#65292;&#24182;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#31232;&#30095;&#25968;&#25454;&#31561;&#29420;&#29305;&#25361;&#25112;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04971</link><description>&lt;p&gt;
&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Recommender Model. (arXiv:2304.04971v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DiffRec&#65289;&#26469;&#36880;&#27493;&#21435;&#22122;&#22320;&#23398;&#20064;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#30340;&#36807;&#31243;&#65292;&#24182;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#31232;&#30095;&#25968;&#25454;&#31561;&#29420;&#29305;&#25361;&#25112;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#20132;&#20114;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;GANs&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;VAEs&#30340;&#21463;&#38480;&#34920;&#24449;&#33021;&#21147;&#12290;&#36825;&#20123;&#38480;&#21046;&#22952;&#30861;&#20102;&#22797;&#26434;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#36807;&#31243;&#30340;&#20934;&#30830;&#24314;&#27169;&#65292;&#20363;&#22914;&#30001;&#21508;&#31181;&#24178;&#25200;&#22240;&#32032;&#23548;&#33268;&#30340;&#22024;&#26434;&#20132;&#20114;&#12290;&#32771;&#34385;&#21040;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;&#31216;&#20026;DiffRec&#65289;&#65292;&#20197;&#36880;&#27493;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#20445;&#30041;&#29992;&#25143;&#20132;&#20114;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;DiffRec&#20943;&#23569;&#20102;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#24182;&#36991;&#20813;&#23558;&#29992;&#25143;&#20132;&#20114;&#25439;&#22351;&#20026;&#20687;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#32431;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;DMs&#20197;&#24212;&#23545;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#22914;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#31232;&#30095;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DiffRec&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are widely utilized to model the generative process of user interactions. However, these generative models suffer from intrinsic limitations such as the instability of GANs and the restricted representation ability of VAEs. Such limitations hinder the accurate modeling of the complex user interaction generation procedure, such as noisy interactions caused by various interference factors. In light of the impressive advantages of Diffusion Models (DMs) over traditional generative models in image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec) to learn the generative process in a denoising manner. To retain personalized information in user interactions, DiffRec reduces the added noises and avoids corrupting users' interactions into pure noises like in image synthesis. In addition, we extend traditional DMs to tackle the unique challenges in practical recommender 
&lt;/p&gt;</description></item><item><title>AdaTT&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#27531;&#24046;&#26426;&#21046;&#21644;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#34701;&#21512;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20849;&#20139;&#30693;&#35782;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04959</link><description>&lt;p&gt;
AdaTT: &#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. (arXiv:2304.04959v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04959
&lt;/p&gt;
&lt;p&gt;
AdaTT&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#27531;&#24046;&#26426;&#21046;&#21644;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#34701;&#21512;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20849;&#20139;&#30693;&#35782;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#23545;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26377;&#25928;&#22320;&#20849;&#20139;&#30693;&#35782;&#65307;2&#65289;&#32852;&#21512;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21644;&#20849;&#20139;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#32593;&#32476;&#65288;AdaTT&#65289;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;AdaTT&#26159;&#19968;&#20010;&#28145;&#24230;&#34701;&#21512;&#32593;&#32476;&#65292;&#22312;&#22810;&#20010;&#32423;&#21035;&#19978;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#21644;&#21487;&#36873;&#20849;&#20139;&#34701;&#21512;&#21333;&#20803;&#26500;&#24314;&#12290;&#36890;&#36807;&#21033;&#29992;&#27531;&#24046;&#26426;&#21046;&#21644;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#34701;&#21512;&#65292;&#36825;&#20123;&#21333;&#20803;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20849;&#20139;&#30693;&#35782;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;&#20026;&#20102;&#35780;&#20272;AdaTT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#38598;&#21644;&#24037;&#19994;&#32423;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#32452;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AdaTT&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) aims at enhancing the performance and efficiency of machine learning models by training them on multiple tasks simultaneously. However, MTL research faces two challenges: 1) modeling the relationships between tasks to effectively share knowledge between them, and 2) jointly learning task-specific and shared knowledge. In this paper, we present a novel model Adaptive Task-to-Task Fusion Network (AdaTT) to address both challenges. AdaTT is a deep fusion network built with task specific and optional shared fusion units at multiple levels. By leveraging a residual mechanism and gating mechanism for task-to-task fusion, these units adaptively learn shared knowledge and task specific knowledge. To evaluate the performance of AdaTT, we conduct experiments on a public benchmark and an industrial recommendation dataset using various task groups. Results demonstrate AdaTT can significantly outperform existing state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;sRank&#30340;&#36890;&#29992;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26234;&#33021;&#22238;&#22797;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#31561;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;11.7%&#30340;&#31163;&#32447;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.04918</link><description>&lt;p&gt;
&#26174;&#24335;&#21644;&#38544;&#24335;&#35821;&#20041;&#25490;&#24207;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explicit and Implicit Semantic Ranking Framework. (arXiv:2304.04918v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;sRank&#30340;&#36890;&#29992;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26234;&#33021;&#22238;&#22797;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#31561;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;11.7%&#30340;&#31163;&#32447;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26680;&#24515;&#38590;&#39064;&#26159;&#23558;&#19968;&#20010;&#26597;&#35810;&#19982;&#19968;&#20010;&#21487;&#21464;&#19988;&#26377;&#38480;&#30340;&#25991;&#26723;&#38598;&#20013;&#30340;&#26368;&#20339;&#25991;&#26723;&#36827;&#34892;&#21305;&#37197;&#12290;&#29616;&#26377;&#30340;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#24310;&#36831;&#21463;&#38480;&#30340;&#26381;&#21153;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#20026;&#20102;&#36895;&#24230;&#32780;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#33258;&#25105;&#35757;&#32451;&#35821;&#20041;&#20132;&#21449;&#20851;&#27880;&#25490;&#21517;&#65288;sRank&#65289;&#12290;&#36825;&#20010;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#20351;&#29992;&#32447;&#24615;&#25104;&#23545;&#25439;&#22833;&#65292;&#20855;&#26377;&#21487;&#21464;&#30340;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#12289;&#23454;&#29616;&#36136;&#37327;&#25552;&#21319;&#21644;&#39640;&#25928;&#29575;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#24494;&#36719;&#20844;&#21496;&#30340;&#20004;&#20010;&#24037;&#19994;&#20219;&#21153;&#65306;&#26234;&#33021;&#22238;&#22797;&#65288;SR&#65289;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#65288;ACI&#65289;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#26234;&#33021;&#22238;&#22797;&#20013;&#65292;$sRank$&#36890;&#36807;&#22522;&#20110;&#28040;&#36153;&#32773;&#21644;&#25903;&#25345;&#20195;&#29702;&#20449;&#24687;&#30340;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#26368;&#20339;&#31572;&#26696;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#26102;&#33719;&#24471;&#25216;&#26415;&#25903;&#25345;&#12290;&#22312;SR&#20219;&#21153;&#19978;&#65292;$sRank$&#23454;&#29616;&#20102;11.7%&#30340;&#31163;&#32447;top-one&#20934;&#30830;&#24230;&#25552;&#21319;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-to-rank framework, Self-training Semantic Cross-attention Ranking (sRank). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, $sRank$ assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04759</link><description>&lt;p&gt;
&#21387;&#32553;&#32034;&#24341;&#23454;&#29616;&#30636;&#38388;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Similarity search in the blink of an eye with compressed indices. (arXiv:2304.04759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25968;&#25454;&#20197;&#21521;&#37327;&#34920;&#31034;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#23547;&#25214;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20284;&#30340;&#21521;&#37327;&#26159;&#19968;&#39033;&#24191;&#27867;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#24314;&#26356;&#24555;&#12289;&#26356;&#23567;&#30340;&#32034;&#24341;&#20197;&#36816;&#34892;&#36825;&#20123;&#25628;&#32034;&#30340;&#26032;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#65292;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#23427;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#25628;&#32034;&#24615;&#33021;&#65292;&#23545;&#25628;&#32034;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;LVQ&#34987;&#35774;&#35745;&#20026;&#19982;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#19968;&#36215;&#24037;&#20316;&#20197;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#31995;&#32479;&#20013;&#38024;&#23545;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#36827;&#34892;&#20851;&#38190;&#20248;&#21270;&#21518;&#65292;LVQ&#30340;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#22312;&#22788;&#29702;&#25968;&#21313;&#20159;&#20010;&#21521;&#37327;&#26102;&#65292;LVQ&#36229;&#36807;&#31532;&#20108;&#20339;&#26041;&#26696;&#65306;
&lt;/p&gt;
&lt;p&gt;
Nowadays, data is represented by vectors. Retrieving those vectors, among millions and billions, that are similar to a given query is a ubiquitous problem of relevance for a wide range of applications. In this work, we present new techniques for creating faster and smaller indices to run these searches. To this end, we introduce a novel vector compression method, Locally-adaptive Vector Quantization (LVQ), that simultaneously reduces memory footprint and improves search performance, with minimal impact on search accuracy. LVQ is designed to work optimally in conjunction with graph-based indices, reducing their effective bandwidth while enabling random-access-friendly fast similarity computations. Our experimental results show that LVQ, combined with key optimizations for graph-based indices in modern datacenter systems, establishes the new state of the art in terms of performance and memory footprint. For billions of vectors, LVQ outcompetes the second-best alternatives: (1) in the low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.03344</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#21327;&#20316;&#20449;&#21495;&#21435;&#22122;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#25429;&#25417;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#38454;&#21327;&#21516;&#20449;&#21495;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;GCF&#30340;&#21452;&#21521;&#37051;&#25509;&#30697;&#38453;&#65292;&#20854;&#23450;&#20041;&#20102;&#22522;&#20110;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#32858;&#21512;&#30340;&#37051;&#23621;&#65292;&#23545;&#20110;&#26377;&#22823;&#37327;&#20132;&#20114;&#20294;&#19981;&#36275;&#30340;&#29992;&#25143;/&#39033;&#30446;&#26469;&#35828;&#21487;&#33021;&#26159;&#22024;&#26434;&#30340;&#12290;&#27492;&#22806;&#65292;&#37051;&#25509;&#30697;&#38453;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#32858;&#21512;&#30340;&#26377;&#30410;&#37051;&#23621;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#24179;&#34913;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#25968;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#26469;&#33719;&#24471;&#29992;&#25143;/&#39033;&#30446;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;&#23545;&#31216;&#30340;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30456;&#20851;&#32452;&#20214;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.03054</link><description>&lt;p&gt;
&#25805;&#32437;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;: &#29992;&#21512;&#25104;&#29992;&#25143;&#36827;&#34892;&#25915;&#20987;&#21450;&#20854;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures. (arXiv:2304.03054v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRecs&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#12290;&#22240;&#20026;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#21487;&#20197;&#36890;&#36807;&#19978;&#20256;&#26799;&#24230;&#30452;&#25509;&#24433;&#21709;&#31995;&#32479;&#65292;&#25152;&#20197;FedRecs&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#30340;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#21512;&#25104;&#29992;&#25143;&#36827;&#34892;&#30340;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20219;&#20309;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#32452;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;FedRecs &#65288;Fed-NCF&#21644;Fed-LightGCN&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Recommender Systems (FedRecs) are considered privacy-preserving techniques to collaboratively learn a recommendation model without sharing user data. Since all participants can directly influence the systems by uploading gradients, FedRecs are vulnerable to poisoning attacks of malicious clients. However, most existing poisoning attacks on FedRecs are either based on some prior knowledge or with less effectiveness. To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items' ranks and exposure rates effectively in the top-$K$ recommendation without relying on any prior knowledge. Specifically, our attack manipulates target items' exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items' alternative products. We conduct extensive experiments with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on two real-world recommendation datasets. The experimental res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26497;&#31616;&#30340;&#25512;&#33616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(XSimGCL)&#65292;&#21457;&#29616;&#26377;&#25928;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#19982;&#20419;&#36827;&#38271;&#23614;&#29289;&#21697;&#21457;&#29616;&#24182;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#22270;&#24418;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2209.02544</link><description>&lt;p&gt;
XSimGCL&#65306;&#38754;&#21521;&#26497;&#31616;&#25512;&#33616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
XSimGCL: Towards Extremely Simple Graph Contrastive Learning for Recommendation. (arXiv:2209.02544v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26497;&#31616;&#30340;&#25512;&#33616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(XSimGCL)&#65292;&#21457;&#29616;&#26377;&#25928;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#19982;&#20419;&#36827;&#38271;&#23614;&#29289;&#21697;&#21457;&#29616;&#24182;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#22270;&#24418;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;(Contrastive learning, CL)&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#22522;&#20110;CL&#30340;&#25512;&#33616;&#27169;&#22411;&#30340;&#21407;&#29702;&#26159;: &#30830;&#20445;&#20174;&#29992;&#25143;-&#29289;&#21697;&#20108;&#20998;&#22270;&#30340;&#19981;&#21516;&#22270;&#24418;&#22686;&#24378;&#20013;&#27966;&#29983;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#36890;&#29992;&#29305;&#24449;&#65292;&#20943;&#36731;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#24456;&#26377;&#25928;&#65292;&#20294;&#26159;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#22240;&#32032;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;CL&#23545;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;CL&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#30427;&#34892;&#30340;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#20419;&#36827;&#20102;&#38271;&#23614;&#29289;&#21697;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#20043;&#21069;&#35748;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#22270;&#24418;&#22686;&#24378;&#22312;&#22522;&#20110;CL&#30340;&#25512;&#33616;&#20013;&#30456;&#23545;&#19981;&#21487;&#38752;&#19988;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) has recently been demonstrated critical in improving recommendation performance. The underlying principle of CL-based recommendation models is to ensure the consistency between representations derived from different graph augmentations of the user-item bipartite graph. This self-supervised approach allows for the extraction of general features from raw data, thereby mitigating the issue of data sparsity. Despite the effectiveness of this paradigm, the factors contributing to its performance gains have yet to be fully understood. This paper provides novel insights into the impact of CL on recommendation. Our findings indicate that CL enables the model to learn more evenly distributed user and item representations, which alleviates the prevalent popularity bias and promoting long-tail items. Our analysis also suggests that the graph augmentations, previously considered essential, are relatively unreliable and of limited significance in CL-based recommendation. B
&lt;/p&gt;</description></item></channel></rss>