<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25552;&#31034;&#24335;&#24494;&#35843;&#26694;&#26550;VPGNN&#65292;&#29992;&#20110;&#20419;&#38144;&#21048;&#28389;&#29992;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#22270;&#25552;&#31034;&#20989;&#25968;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#20026;&#19982;&#39044;&#35757;&#32451;&#20013;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#31867;&#20284;&#30340;&#27169;&#26495;&#65292;&#20174;&#32780;&#32553;&#23567;&#20102;&#30446;&#26631;&#24046;&#36317;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;VPGNN&#22312;&#23569;&#26679;&#26412;&#21644;&#21322;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;23.4%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.10028</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20419;&#38144;&#21048;&#28389;&#29992;&#26816;&#27979;&#20013;&#30340;&#25552;&#31034;&#24335;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Voucher Abuse Detection with Prompt-based Fine-tuning on Graph Neural Networks. (arXiv:2308.10028v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25552;&#31034;&#24335;&#24494;&#35843;&#26694;&#26550;VPGNN&#65292;&#29992;&#20110;&#20419;&#38144;&#21048;&#28389;&#29992;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#22270;&#25552;&#31034;&#20989;&#25968;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#20026;&#19982;&#39044;&#35757;&#32451;&#20013;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#31867;&#20284;&#30340;&#27169;&#26495;&#65292;&#20174;&#32780;&#32553;&#23567;&#20102;&#30446;&#26631;&#24046;&#36317;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;VPGNN&#22312;&#23569;&#26679;&#26412;&#21644;&#21322;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;23.4%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#38144;&#21048;&#28389;&#29992;&#26816;&#27979;&#26159;&#30005;&#23376;&#21830;&#21153;&#20013;&#37325;&#35201;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30417;&#30563;&#24335;&#33539;&#24335;&#21462;&#20915;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#24120;&#24120;&#21463;&#21040;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30446;&#26631;&#24046;&#36317;&#30340;&#22256;&#25200;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VPGNN&#65292;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#25552;&#31034;&#24335;&#24494;&#35843;&#26694;&#26550;&#29992;&#20110;&#20419;&#38144;&#21048;&#28389;&#29992;&#26816;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#25552;&#31034;&#20989;&#25968;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#25104;&#19982;&#39044;&#35757;&#32451;&#20013;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#31867;&#20284;&#30340;&#27169;&#26495;&#65292;&#20174;&#32780;&#32553;&#23567;&#20102;&#30446;&#26631;&#24046;&#36317;&#12290;&#22312;&#19987;&#26377;&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;VPGNN&#22312;&#23569;&#26679;&#26412;&#21644;&#21322;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#22312;&#32447;&#37096;&#32626;VPGNN&#26174;&#31034;&#20986;23.4%&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voucher abuse detection is an important anomaly detection problem in E-commerce. While many GNN-based solutions have emerged, the supervised paradigm depends on a large quantity of labeled data. A popular alternative is to adopt self-supervised pre-training using label-free data, and further fine-tune on a downstream task with limited labels. Nevertheless, the "pre-train, fine-tune" paradigm is often plagued by the objective gap between pre-training and downstream tasks. Hence, we propose VPGNN, a prompt-based fine-tuning framework on GNNs for voucher abuse detection. We design a novel graph prompting function to reformulate the downstream task into a similar template as the pretext task in pre-training, thereby narrowing the objective gap. Extensive experiments on both proprietary and public datasets demonstrate the strength of VPGNN in both few-shot and semi-supervised scenarios. Moreover, an online deployment of VPGNN in a production environment shows a 23.4% improvement over two ex
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20449;&#24687;&#27969;&#34892;&#24230;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#23558;&#26102;&#38388;&#23646;&#24615;&#34701;&#20837;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#28982;&#21518;&#37319;&#29992;&#32423;&#32852;&#22270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#24207;&#21015;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#26469;&#20805;&#20998;&#23398;&#20064;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.09976</link><description>&lt;p&gt;
&#26174;&#24335;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20449;&#24687;&#27969;&#34892;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explicit Time Embedding Based Cascade Attention Network for Information Popularity Prediction. (arXiv:2308.09976v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20449;&#24687;&#27969;&#34892;&#24230;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#23558;&#26102;&#38388;&#23646;&#24615;&#34701;&#20837;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#28982;&#21518;&#37319;&#29992;&#32423;&#32852;&#22270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#24207;&#21015;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#26469;&#20805;&#20998;&#23398;&#20064;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#39044;&#27979;&#20449;&#24687;&#32423;&#32852;&#30340;&#27969;&#34892;&#24230;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25429;&#25417;&#26102;&#38388;&#23646;&#24615;&#21644;&#32423;&#32852;&#35282;&#33394;&#20449;&#24687;&#65288;&#22914;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#65289;&#23545;&#20110;&#29702;&#35299;&#20449;&#24687;&#32423;&#32852;&#26159;&#24517;&#35201;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24456;&#23569;&#20851;&#27880;&#23558;&#36825;&#20123;&#20449;&#24687;&#32479;&#19968;&#36215;&#26469;&#36827;&#34892;&#27969;&#34892;&#24230;&#39044;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#22320;&#23545;&#32423;&#32852;&#30340;&#20840;&#37096;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#32423;&#32852;&#27880;&#24847;&#21147;&#32593;&#32476;(TCAN)&#20316;&#20026;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#20449;&#24687;&#32593;&#32476;&#30340;&#26032;&#22411;&#27969;&#34892;&#24230;&#39044;&#27979;&#26550;&#26500;&#12290;TCAN&#36890;&#36807;&#19968;&#31181;&#36890;&#29992;&#30340;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;(TC)&#23558;&#26102;&#38388;&#23646;&#24615;&#65288;&#21608;&#26399;&#24615;&#12289;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32553;&#25918;&#65289;&#34701;&#20837;&#33410;&#28857;&#29305;&#24449;&#20013;&#65292;&#28982;&#21518;&#37319;&#29992;&#32423;&#32852;&#22270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;(CGAT)&#21644;&#32423;&#32852;&#24207;&#21015;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;(CSAT)&#26469;&#20805;&#20998;&#23398;&#20064;&#32423;&#32852;&#22270;&#21644;&#32423;&#32852;&#24207;&#21015;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting information cascade popularity is a fundamental problem in social networks. Capturing temporal attributes and cascade role information (e.g., cascade graphs and cascade sequences) is necessary for understanding the information cascade. Current methods rarely focus on unifying this information for popularity predictions, which prevents them from effectively modeling the full properties of cascades to achieve satisfactory prediction performances. In this paper, we propose an explicit Time embedding based Cascade Attention Network (TCAN) as a novel popularity prediction architecture for large-scale information networks. TCAN integrates temporal attributes (i.e., periodicity, linearity, and non-linear scaling) into node features via a general time embedding approach (TE), and then employs a cascade graph attention encoder (CGAT) and a cascade sequence attention encoder (CSAT) to fully learn the representation of cascade graphs and cascade sequences. We use two real-world dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;TEM4CTR&#65292;&#36890;&#36807;&#26102;&#38388;&#23545;&#40784;&#21644;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09966</link><description>&lt;p&gt;
&#26102;&#38388;&#23545;&#40784;&#22686;&#24378;&#27169;&#22411;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time-aligned Exposure-enhanced Model for Click-Through Rate Prediction. (arXiv:2308.09966v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09966
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;TEM4CTR&#65292;&#36890;&#36807;&#26102;&#38388;&#23545;&#40784;&#21644;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#22522;&#20110;&#29992;&#25143;&#28857;&#20987;&#30340;&#21487;&#33021;&#24615;&#23545;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#12290;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#24314;&#27169;&#22312;&#28857;&#20987;&#29575;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#23427;&#20174;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#24207;&#21015;&#20013;&#25552;&#21462;&#28508;&#22312;&#20852;&#36259;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#24207;&#21015;&#65288;&#20363;&#22914;&#26410;&#28857;&#20987;&#30340;&#35760;&#24405;&#65289;&#26469;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#30001;&#20110;&#24207;&#21015;&#26102;&#38388;&#33539;&#22260;&#19981;&#21516;&#32780;&#23548;&#33268;&#30340;&#26102;&#38388;&#38169;&#20301;&#65292;&#20197;&#21450;2&#65289;&#32570;&#20047;&#23545;&#21453;&#39304;&#24207;&#21015;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20132;&#20114;&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;TEM4CTR&#65292;&#35813;&#26694;&#26550;&#30830;&#20445;&#24207;&#21015;&#20043;&#38388;&#30340;&#26102;&#38388;&#23545;&#40784;&#65292;&#21516;&#26102;&#21033;&#29992;&#36741;&#21161;&#21453;&#39304;&#20449;&#24687;&#36890;&#36807;&#34920;&#31034;&#25237;&#24433;&#26426;&#21046;&#22686;&#24378;&#39033;&#30446;&#32423;&#30340;&#28857;&#20987;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#35813;&#22522;&#20110;&#25237;&#24433;&#30340;&#20449;&#24687;&#20256;&#36882;&#27169;&#22359;&#21487;&#20197;&#26356;**/
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction, crucial in applications like recommender systems and online advertising, involves ranking items based on the likelihood of user clicks. User behavior sequence modeling has marked progress in CTR prediction, which extracts users' latent interests from their historical behavior sequences to facilitate accurate CTR prediction. Recent research explores using implicit feedback sequences, like unclicked records, to extract diverse user interests. However, these methods encounter key challenges: 1) temporal misalignment due to disparate sequence time ranges and 2) the lack of fine-grained interaction among feedback sequences. To address these challenges, we propose a novel framework called TEM4CTR, which ensures temporal alignment among sequences while leveraging auxiliary feedback information to enhance click behavior at the item level through a representation projection mechanism. Moreover, this projection-based information transfer module can effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;printf&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#22522;&#20110;&#29992;&#25143;&#35780;&#35770;&#12289;&#29289;&#21697;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#24314;&#27169;&#20559;&#22909;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#25429;&#25417;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09943</link><description>&lt;p&gt;
printf:&#22522;&#20110;&#29992;&#25143;&#35780;&#35770;&#21644;&#29289;&#21697;&#22270;&#20687;&#19982;&#25991;&#26412;&#20449;&#24687;&#30340;&#20559;&#22909;&#24314;&#27169;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
printf: Preference Modeling Based on User Reviews with Item Images and Textual Information via Graph Learning. (arXiv:2308.09943v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;printf&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#22522;&#20110;&#29992;&#25143;&#35780;&#35770;&#12289;&#29289;&#21697;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#24314;&#27169;&#20559;&#22909;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#25429;&#25417;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#26469;&#39044;&#27979;&#29992;&#25143;&#20559;&#22909;&#12290;&#23545;&#20110;&#25991;&#26412;&#20449;&#24687;&#26469;&#35828;&#65292;&#35780;&#35770;&#25991;&#26412;&#26159;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#26368;&#21463;&#27426;&#36814;&#20869;&#23481;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#20165;&#20351;&#29992;&#25991;&#26412;&#35780;&#35770;&#20316;&#20026;&#29305;&#24449;&#30340;&#21069;N&#20010;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35780;&#35770;&#36890;&#24120;&#22833;&#21435;&#20102;&#23427;&#20204;&#30340;&#20142;&#28857;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#20805;&#20998;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;&#23545;&#20110;&#35270;&#35273;&#20449;&#24687;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#32593;&#32476;&#26469;&#24314;&#27169;&#65292;&#20294;&#24456;&#38590;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#21512;&#29702;&#22320;&#20849;&#21516;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;printf&#65292;&#21363;&#22522;&#20110;&#29992;&#25143;&#35780;&#35770;&#19982;&#29289;&#21697;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#20559;&#22909;&#24314;&#27169;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#32500;&#24230;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#25351;&#23548;&#29992;&#25143;&#35780;&#35770;&#21644;&#20132;&#20114;&#39033;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25152;&#26377;&#37096;&#20998;&#20849;&#21516;&#23398;&#20064;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, modern recommender systems usually leverage textual and visual contents as auxiliary information to predict user preference. For textual information, review texts are one of the most popular contents to model user behaviors. Nevertheless, reviews usually lose their shine when it comes to top-N recommender systems because those that solely utilize textual reviews as features struggle to adequately capture the interaction relationships between users and items. For visual one, it is usually modeled with naive convolutional networks and also hard to capture high-order relationships between users and items. Moreover, previous works did not collaboratively use both texts and images in a proper way. In this paper, we propose printf, preference modeling based on user reviews with item images and textual information via graph learning, to address the above challenges. Specifically, the dimension-based attention mechanism directs relations between user reviews and interacted items, all
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09904</link><description>&lt;p&gt;
RAH&#65281;RecSys-Assistant-Human&#65306;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#20013;&#24515;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models. (arXiv:2308.09904v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#29983;&#24577;&#31995;&#32479;&#28041;&#21450;&#21040;&#25512;&#33616;&#31995;&#32479;&#65288;&#35745;&#31639;&#26426;&#65289;&#21644;&#29992;&#25143;&#65288;&#20154;&#31867;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#35282;&#24230;&#19981;&#21516;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#21152;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;RAH&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12289;&#21161;&#25163;&#21644;&#20154;&#31867;&#12290;&#21161;&#25163;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#20010;&#20154;&#20195;&#29702;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#21161;&#25163;&#25198;&#28436;&#38750;&#20405;&#20837;&#24615;&#30340;&#35282;&#33394;&#65292;RAH&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#29992;&#25143;&#32676;&#20307;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;RAH&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#12290;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#20351;&#29992;&#23398;&#20064;-&#34892;&#21160;-&#35780;&#35770;&#23478;&#21644;&#21453;&#24605;&#26426;&#21046;&#21487;&#20197;&#23548;&#33268;&#26356;&#21152;&#19968;&#33268;&#30340;&#20010;&#24615;&#65292;&#65288;2&#65289;&#25105;&#20204;&#30340;&#21161;&#25163;&#21487;&#20197;&#26377;&#25928;&#22320;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#24182;&#24110;&#21161;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;RAH&#26694;&#26550;&#20013;&#36827;&#19968;&#27493;&#35299;&#20915;&#20154;&#31867;&#20013;&#24515;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#29992;&#25143;``&#22842;&#26435;''&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation ecosystem involves interactions between recommender systems(Computer) and users(Human). Orthogonal to the perspective of recommender systems, we attempt to utilize LLMs from the perspective of users and propose a more human-central recommendation framework named RAH, which consists of Recommender system, Assistant and Human. The assistant is a LLM-based and personal proxy for a human to achieve user satisfaction. The assistant plays a non-invasion role and the RAH framework can adapt to different recommender systems and user groups. Subsequently, we implement and evaluate the RAH framework for learning user personalities and proxy human feedback. The experiment shows that (1) using learn-action-critic and reflection mechanisms can lead more aligned personality and (2) our assistant can effectively proxy human feedback and help adjust recommender systems. Finally, we discuss further strategies in the RAH framework to address human-central concerns including user contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#26816;&#32034;&#21040;&#21021;&#22987;&#20505;&#36873;&#25991;&#26723;&#38598;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2308.09861</link><description>&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#38024;&#23545;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65306;&#19968;&#31181;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method. (arXiv:2308.09861v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#26816;&#32034;&#21040;&#21021;&#22987;&#20505;&#36873;&#25991;&#26723;&#38598;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#65288;NRMs&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#27169;&#22411;&#22312;&#25972;&#20307;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25913;&#36827;&#12290;&#38500;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20043;&#22806;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#40065;&#26834;&#24615;&#24050;&#34987;&#35777;&#26126;&#19981;&#36275;&#65292;&#23545;&#20110;&#26680;&#24515;&#26816;&#32034;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25152;&#24320;&#21457;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25915;&#20987;NRMs&#19978;&#65292;&#23545;DR&#27169;&#22411;&#30340;&#25239;&#24615;&#40092;&#26377;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#26816;&#32034;&#25915;&#20987;&#65288;AREA&#65289;&#20219;&#21153;&#12290;AREA&#20219;&#21153;&#26088;&#22312;&#27450;&#39575;DR&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#21709;&#24212;&#26597;&#35810;&#26102;&#20174;&#26368;&#21021;&#26816;&#32034;&#20505;&#36873;&#25991;&#26723;&#38598;&#20043;&#22806;&#26816;&#32034;&#21040;&#30446;&#26631;&#25991;&#26723;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20915;&#31574;&#30340;&#40657;&#30418;&#23545;&#25239;&#35774;&#32622;&#65292;&#36825;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#26159;&#29616;&#23454;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;AREA&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29616;&#26377;&#30340;&#20026;NRMs&#35774;&#35745;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ranking models (NRMs) and dense retrieval (DR) models have given rise to substantial improvements in overall retrieval performance. In addition to their effectiveness, and motivated by the proven lack of robustness of deep learning-based approaches in other areas, there is growing interest in the robustness of deep learning-based approaches to the core retrieval problem. Adversarial attack methods that have so far been developed mainly focus on attacking NRMs, with very little attention being paid to the robustness of DR models. In this paper, we introduce the adversarial retrieval attack (AREA) task. The AREA task is meant to trick DR models into retrieving a target document that is outside the initial set of candidate documents retrieved by the DR model in response to a query. We consider the decision-based black-box adversarial setting, which is realistic in real-world search engines. To address the AREA task, we first employ existing adversarial attack methods designed for N
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09308</link><description>&lt;p&gt;
&#21487;&#24494;&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#30340;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. (arXiv:2308.09308v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26816;&#32034;&#22120;&#21644;&#22806;&#37096;&#35821;&#26009;&#24211;&#26469;&#22686;&#24378;&#19979;&#28216;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20998;&#21035;&#25110;&#24322;&#27493;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#26469;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#19982;&#31471;&#21040;&#31471;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Differentiable Retrieval Augmentation via Generative lANguage modeling&#65288;Dragan&#65289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#37325;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#19968;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#28040;&#34701;&#30740;&#31350;&#22343;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#19988;&#21512;&#29702;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose Differentiable Retrieval Augmentation via Generative lANguage modeling(Dragan), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art basel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02442</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#38468;&#21152;kNN&#22270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;kNN&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#26377;&#25928;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;kNN&#22270;&#23545;&#20110;k&#20540;&#30340;&#22266;&#23450;&#20381;&#36182;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#20998;&#31867;&#27169;&#22411;&#31867;&#20284;&#65292;&#20915;&#31574;&#36793;&#30028;&#19978;&#23384;&#22312;&#30340;&#27169;&#31946;&#26679;&#26412;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#38468;&#21152;k-&#26368;&#36817;&#37051;&#22270;&#65288;paNNG&#65289;&#65292;&#23427;&#23558;&#33258;&#36866;&#24212;&#30340;kNN&#19982;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#8220;&#25289;&#8221;&#23427;&#20204;&#22238;&#21040;&#21407;&#22987;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;paNNG&#30340;&#24615;&#33021;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03206</link><description>&lt;p&gt;
DENCLUE&#30340;&#26368;&#20248;&#24102;&#23485;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal Bandwidth Selection for DENCLUE. (arXiv:2307.03206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#24037;&#19994;&#20013;&#65292;&#32858;&#31867;&#31639;&#27861;&#26159;&#31639;&#27861;&#24037;&#31243;&#24072;&#30340;&#26085;&#24120;&#24037;&#20316;&#12290;&#23613;&#31649;&#22312;2010&#24180;&#20043;&#21069;&#65292;&#32858;&#31867;&#31639;&#27861;&#32463;&#21382;&#20102;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#23454;&#38469;&#24037;&#19994;&#26631;&#20934;&#20043;&#21518;&#65292;&#19982;&#35813;&#30740;&#31350;&#20027;&#39064;&#30456;&#20851;&#30340;&#21019;&#26032;&#20572;&#28382;&#19981;&#21069;&#12290;2007&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DENCLUE&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#25968;&#25454;&#32467;&#26500;&#30340;&#32858;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;2011&#24180;&#65292;&#35813;&#31639;&#27861;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#20173;&#28982;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern day industry, clustering algorithms are daily routines of algorithm engineers. Although clustering algorithms experienced rapid growth before 2010. Innovation related to the research topic has stagnated after deep learning became the de facto industrial standard for machine learning applications. In 2007, a density-based clustering algorithm named DENCLUE was invented to solve clustering problem for nonlinear data structures. However, its parameter selection problem was largely neglected until 2011. In this paper, we propose a new approach to compute the optimal parameters for the DENCLUE algorithm, and discuss its performance in the experiment section.
&lt;/p&gt;</description></item><item><title>OBELISC&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;141&#20159;&#20010;&#32593;&#39029;&#65292;3.53&#20159;&#20010;&#22270;&#20687;&#21644;1150&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;80&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16527</link><description>&lt;p&gt;
OBELISC&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#22270;&#20687;&#25991;&#26723;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents. (arXiv:2306.16527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16527
&lt;/p&gt;
&lt;p&gt;
OBELISC&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;141&#20159;&#20010;&#32593;&#39029;&#65292;3.53&#20159;&#20010;&#22270;&#20687;&#21644;1150&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;80&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35201;&#27714;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#22270;&#29255;&#36827;&#34892;&#25512;&#29702;&#29983;&#25104;&#25991;&#26412;&#30340;&#21508;&#31181;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#33258;&#28982;&#25991;&#26723;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#38169;&#65289;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#23578;&#26410;&#21457;&#24067;&#65292;&#24182;&#19988;&#25910;&#38598;&#36807;&#31243;&#23578;&#26410;&#23436;&#20840;&#26126;&#30830;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;OBELISC&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#30001;141&#20159;&#20010;&#20174;Common Crawl&#25552;&#21462;&#30340;&#32593;&#39029;&#65292;3.53&#20159;&#20010;&#30456;&#20851;&#22270;&#20687;&#21644;1150&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#32452;&#25104;&#30340;&#24320;&#25918;&#24335;&#32593;&#39029;&#35268;&#27169;&#31579;&#36873;&#30340;&#22270;&#20687;&#25991;&#26412;&#20132;&#38169;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36807;&#28388;&#35268;&#21017;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#30340;&#20869;&#23481;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#23637;&#31034;OBELISC&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;80&#20159;&#21442;&#25968;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#29992;&#20110;&#37325;&#29616;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#20197;&#21450;&#25968;&#25454;&#38598;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELISC dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;Epistemic Neural Recommendation (ENR)&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;Thompson&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.14834</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;Bandit&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Contextual Bandit for Recommender Systems. (arXiv:2306.14834v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;Epistemic Neural Recommendation (ENR)&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;Thompson&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24212;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#26377;&#25928;&#21644;&#25506;&#32034;&#24615;&#20114;&#21160;&#25552;&#20379;&#21019;&#26032;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#20013;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#24050;&#35782;&#21035;&#30340;&#29992;&#25143;&#20852;&#36259;&#65292;&#23545;&#20110;&#26377;&#25928;&#21457;&#29616;&#26410;&#30693;&#29992;&#25143;&#20559;&#22909;&#23384;&#22312;&#19981;&#36275;&#12290;&#23613;&#31649;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#22312;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25506;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20182;&#20204;&#23545;&#35745;&#31639;&#30340;&#35201;&#27714;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;Epistemic Neural Recommendation (ENR)&#65292;&#23427;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;Thompson&#25277;&#26679;&#12290;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;ENR&#26174;&#33879;&#25552;&#39640;&#20102;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality recommender systems ought to deliver both innovative and relevant content through effective and exploratory interactions with users. Yet, supervised learning-based neural networks, which form the backbone of many existing recommender systems, only leverage recognized user interests, falling short when it comes to efficiently uncovering unknown user preferences. While there has been some progress with neural contextual bandit algorithms towards enabling online exploration through neural networks, their onerous computational demands hinder widespread adoption in real-world recommender systems. In this work, we propose a scalable sample-efficient neural contextual bandit algorithm for recommender systems. To do this, we design an epistemic neural network architecture, Epistemic Neural Recommendation (ENR), that enables Thompson sampling at a large scale. In two distinct large-scale experiments with real-world tasks, ENR significantly boosts click-through rates and user rating
&lt;/p&gt;</description></item><item><title>CompMix&#26159;&#19968;&#20010;&#24322;&#26500;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#22810;&#20010;&#20449;&#24687;&#28304;&#21644;&#22797;&#26434;&#24847;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#20844;&#24179;&#30340;&#35780;&#20272;QA&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12235</link><description>&lt;p&gt;
CompMix: &#19968;&#31181;&#24322;&#26500;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CompMix: A Benchmark for Heterogeneous Question Answering. (arXiv:2306.12235v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12235
&lt;/p&gt;
&lt;p&gt;
CompMix&#26159;&#19968;&#20010;&#24322;&#26500;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#22810;&#20010;&#20449;&#24687;&#28304;&#21644;&#22797;&#26434;&#24847;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#20844;&#24179;&#30340;&#35780;&#20272;QA&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#20026;&#20013;&#24515;&#30340;&#38382;&#31572;&#31995;&#32479;&#32463;&#24120;&#38656;&#35201;&#35775;&#38382;&#22810;&#31181;&#24322;&#26500;&#20449;&#24687;&#28304;&#12290;&#36890;&#36807;&#20849;&#21516;&#32771;&#34385;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#22914;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#25910;&#38598;&#21644;&#26469;&#33258;&#32593;&#32476;&#30340;&#34920;&#26684;&#65292;&#38382;&#31572;&#31995;&#32479;&#21487;&#20197;&#22686;&#24378;&#20854;&#31572;&#26696;&#35206;&#30422;&#33539;&#22260;&#21644;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; QA &#22522;&#20934;&#27979;&#35797;&#22823;&#22810;&#26159;&#20026;&#20102;&#26500;&#24314;&#21333;&#19968;&#30340;&#30693;&#35782;&#36164;&#28304;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#26080;&#27861;&#20844;&#24179;&#22320;&#35780;&#20272;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#20449;&#24687;&#24211;&#30340; QA &#31995;&#32479;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102; CompMix&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20247;&#21253;&#38382;&#31572;&#26500;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#33258;&#28982;&#22320;&#35201;&#27714;&#38598;&#25104;&#22810;&#31181;&#36755;&#20837;&#28304;&#12290;CompMix &#20849;&#26377; 9,410 &#20010;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#22797;&#26434;&#24847;&#22270;&#65292;&#22914;&#36830;&#25509;&#21644;&#26102;&#38388;&#26465;&#20214;&#12290;&#22312; CompMix &#19978;&#35780;&#20272;&#19968;&#31995;&#21015; QA &#31995;&#32479;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#28304;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-centric question answering (QA) often requires access to multiple, heterogeneous, information sources. By jointly considering several sources like a knowledge base (KB), a text collection, and tables from the web, QA systems can enhance their answer coverage and confidence. However, existing QA benchmarks are mostly constructed with a single source of knowledge in mind. This limits capabilities of these benchmarks to fairly evaluate QA systems that can tap into more than one information repository. To bridge this gap, we release CompMix, a crowdsourced QA benchmark which naturally demands the integration of a mixture of input sources. CompMix has a total of 9,410 questions, and features several complex intents like joins and temporal conditions. Evaluation of a range of QA systems on CompMix highlights the need for further research on leveraging information from heterogeneous sources.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>FARA&#26159;&#19968;&#31181;&#26410;&#26469;&#24863;&#30693;&#30340;&#20844;&#24179;&#20248;&#21270;&#25490;&#21517;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#22810;&#20010;&#25490;&#21517;&#21015;&#34920;&#24182;&#23558;&#20854;&#20445;&#23384;&#21040;&#26410;&#26469;&#30340;&#20250;&#35805;&#20013;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20844;&#24179;&#24615;&#21644;&#30456;&#20851;&#24615;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FARA&#22312;&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16637</link><description>&lt;p&gt;
FARA: &#26410;&#26469;&#24863;&#30693;&#30340;&#20844;&#24179;&#20248;&#21270;&#25490;&#21517;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FARA: Future-aware Ranking Algorithm for Fairness Optimization. (arXiv:2305.16637v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16637
&lt;/p&gt;
&lt;p&gt;
FARA&#26159;&#19968;&#31181;&#26410;&#26469;&#24863;&#30693;&#30340;&#20844;&#24179;&#20248;&#21270;&#25490;&#21517;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#22810;&#20010;&#25490;&#21517;&#21015;&#34920;&#24182;&#23558;&#20854;&#20445;&#23384;&#21040;&#26410;&#26469;&#30340;&#20250;&#35805;&#20013;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20844;&#24179;&#24615;&#21644;&#30456;&#20851;&#24615;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FARA&#22312;&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#31995;&#32479;&#26159;&#29616;&#20195;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#65288;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#65289;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#38500;&#20102;&#19982;&#29992;&#25143;&#30456;&#20851;&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#22806;&#65292;&#21521;&#39033;&#30446;&#25552;&#20379;&#32773;&#20844;&#24179;&#30340;&#26333;&#20809;&#24230;&#20063;&#34987;&#35748;&#20026;&#26159;&#25490;&#21517;&#20248;&#21270;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#35768;&#22810;&#20844;&#24179;&#25490;&#21517;&#31639;&#27861;&#24050;&#34987;&#25552;&#20986;&#20197;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;&#25490;&#21517;&#26041;&#27861;&#37319;&#29992;&#36138;&#24515;&#31639;&#27861;&#65292;&#20165;&#38024;&#23545;&#19979;&#19968;&#20010;&#21363;&#26102;&#20250;&#35805;&#25110;&#35831;&#27714;&#20248;&#21270;&#25490;&#21517;&#12290;&#27491;&#22914;&#26412;&#25991;&#25152;&#31034;&#65292;&#36825;&#31181;&#30701;&#35270;&#30340;&#33539;&#24335;&#21487;&#33021;&#38480;&#21046;&#25490;&#21517;&#20248;&#21270;&#30340;&#19978;&#38480;&#65292;&#24182;&#23548;&#33268;&#38271;&#26399;&#30340;&#27425;&#20248;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FARA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26410;&#26469;&#24863;&#30693;&#25490;&#21517;&#31639;&#27861;&#65292;&#29992;&#20110;&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20844;&#24179;&#20248;&#21270;&#12290;FARA&#19981;&#26159;&#36138;&#23146;&#22320;&#20248;&#21270;&#19979;&#19968;&#20010;&#20250;&#35805;&#30340;&#25490;&#21517;&#65292;&#32780;&#26159;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#22810;&#20010;&#25490;&#21517;&#21015;&#34920;&#24182;&#23558;&#23427;&#20204;&#20445;&#23384;&#21040;&#26410;&#26469;&#20250;&#35805;&#20013;&#65292;&#26469;&#25552;&#21069;&#35268;&#21010;&#12290;&#29305;&#21035;&#22320;&#65292;FARA&#26088;&#22312;&#26368;&#23567;&#21270;&#25490;&#21517;&#21015;&#34920;&#30340;&#20844;&#24179;&#24615;&#21644;&#30456;&#20851;&#24615;&#24046;&#24322;&#65292;&#24182;&#32771;&#34385;&#26410;&#26469;&#20250;&#35805;&#23545;&#24403;&#21069;&#25490;&#21517;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FARA&#22312;&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20844;&#24179;&#25490;&#21517;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking systems are the key components of modern Information Retrieval (IR) applications, such as search engines and recommender systems. Besides the ranking relevance to users, the exposure fairness to item providers has also been considered an important factor in ranking optimization. Many fair ranking algorithms have been proposed to jointly optimize both ranking relevance and fairness. However, we find that most existing fair ranking methods adopt greedy algorithms that only optimize rankings for the next immediate session or request. As shown in this paper, such a myopic paradigm could limit the upper bound of ranking optimization and lead to suboptimal performance in the long term. To this end, we propose FARA, a novel Future-Aware Ranking Algorithm for ranking relevance and fairness optimization. Instead of greedily optimizing rankings for the next immediate session, FARA plans ahead by jointly optimizing multiple ranklists together and saving them for future sessions. Particula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;(ZS-CIR)&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;CLIP&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20266;&#35789;&#26631;&#35760;&#24182;&#19982;&#30456;&#23545;&#26631;&#39064;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#19981;&#38656;&#35201;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#12290;&#24341;&#20837;&#20102;&#21517;&#20026;CIRCO&#30340;&#24320;&#25918;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#30495;&#23454;&#31572;&#26696;&#30340;CIR&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEARLE&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15247</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#19982;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Composed Image Retrieval with Textual Inversion. (arXiv:2303.15247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;(ZS-CIR)&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;CLIP&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20266;&#35789;&#26631;&#35760;&#24182;&#19982;&#30456;&#23545;&#26631;&#39064;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#19981;&#38656;&#35201;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#12290;&#24341;&#20837;&#20102;&#21517;&#20026;CIRCO&#30340;&#24320;&#25918;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#30495;&#23454;&#31572;&#26696;&#30340;CIR&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEARLE&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;(CIR)&#26088;&#22312;&#26681;&#25454;&#30001;&#21442;&#32771;&#22270;&#20687;&#21644;&#25551;&#36848;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#24046;&#24322;&#30340;&#30456;&#23545;&#26631;&#39064;&#32452;&#25104;&#30340;&#26597;&#35810;&#26469;&#26816;&#32034;&#30446;&#26631;&#22270;&#20687;&#12290;&#26631;&#35760;CIR&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#39640;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#38459;&#30861;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#38646;&#26679;&#26412;CIR (ZS-CIR)&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;CIR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#19982;&#25991;&#26412;&#21453;&#36716;(SEARLE)&#65292;&#23427;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;CLIP&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20266;&#35789;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#19982;&#30456;&#23545;&#26631;&#39064;&#38598;&#25104;&#12290;&#20026;&#20102;&#25903;&#25345;ZS-CIR&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20849;&#21516;&#23545;&#35937;&#29615;&#22659;&#20013;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#8221;(CIRCO)&#30340;&#24320;&#25918;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#30495;&#23454;&#31572;&#26696;&#30340;CIR&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SEARLE&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed Image Retrieval (CIR) aims to retrieve a target image based on a query composed of a reference image and a relative caption that describes the difference between the two images. The high effort and cost required for labeling datasets for CIR hamper the widespread usage of existing methods, as they rely on supervised learning. In this work, we propose a new task, Zero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled training dataset. Our approach, named zero-Shot composEd imAge Retrieval with textuaL invErsion (SEARLE), maps the visual features of the reference image into a pseudo-word token in CLIP token embedding space and integrates it with the relative caption. To support research on ZS-CIR, we introduce an open-domain benchmarking dataset named Composed Image Retrieval on Common Objects in context (CIRCO), which is the first dataset for CIR containing multiple ground truths for each query. The experiments show that SEARLE exhibits better performance 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#25439;&#22833;&#26469;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.14640</link><description>&lt;p&gt;
&#29992;&#33258;&#36866;&#24212;&#21152;&#26435;&#25439;&#22833;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning with Adaptive Weighted Loss for Imbalanced Cold-Start Recommendation. (arXiv:2302.14640v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#25439;&#22833;&#26469;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#22312;&#25429;&#25417;&#29992;&#25143;&#21916;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20919;&#21551;&#21160;&#25512;&#33616;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#28041;&#21450;&#26377;&#38480;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#39034;&#24207;&#25512;&#33616;&#39046;&#22495;&#20013;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24555;&#36895;&#36866;&#24212;&#21644;&#26131;&#20110;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#31639;&#27861;&#23558;&#20919;&#21551;&#21160;&#25512;&#33616;&#25551;&#36848;&#20026;&#19968;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#37117;&#34987;&#34920;&#31034;&#20026;&#38656;&#35201;&#36866;&#24212;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20803;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#20219;&#21153;&#26679;&#26412;&#22312;&#31867;&#21035;&#25110;&#20540;&#19978;&#22343;&#21248;&#20998;&#24067;&#65292;&#32780;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#24182;&#19981;&#31526;&#21512;&#36825;&#26679;&#30340;&#20998;&#24067;&#65288;&#20363;&#22914;&#65292;&#22810;&#27425;&#35266;&#30475;&#21916;&#27426;&#30340;&#35270;&#39057;&#65292;&#21482;&#30041;&#19979;&#27491;&#38754;&#35780;&#20998;&#32780;&#27809;&#26377;&#36127;&#38754;&#35780;&#20998;&#65289;&#12290;&#22240;&#27492;&#65292;&#21344;&#25454;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#22823;&#37096;&#20998;&#30340;&#19981;&#24179;&#34913;&#29992;&#25143;&#21453;&#39304;&#21487;&#33021;&#20027;&#23548;&#30528;&#29992;&#25143;&#30340;&#36866;&#24212;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommenders have made great strides in capturing a user's preferences. Nevertheless, the cold-start recommendation remains a fundamental challenge as they typically involve limited user-item interactions for personalization. Recently, gradient-based meta-learning approaches have emerged in the sequential recommendation field due to their fast adaptation and easy-to-integrate abilities. The meta-learning algorithms formulate the cold-start recommendation as a few-shot learning problem, where each user is represented as a task to be adapted. While meta-learning algorithms generally assume that task-wise samples are evenly distributed over classes or values, user-item interactions in real-world applications do not conform to such a distribution (e.g., watching favorite videos multiple times, leaving only positive ratings without any negative ones). Consequently, imbalanced user feedback, which accounts for the majority of task training data, may dominate the user adaptation pr
&lt;/p&gt;</description></item><item><title>RECOMED&#26159;&#22522;&#20110;&#24739;&#32773;&#21644;&#33647;&#29289;&#29305;&#24449;&#35774;&#35745;&#30340;&#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#23454;&#29616;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#24739;&#32773;&#26465;&#20214;&#21644;&#21382;&#21490;&#26469;&#36873;&#25321;&#21512;&#36866;&#33647;&#29289;&#65292;&#20197;&#21450;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.00280</link><description>&lt;p&gt;
RECOMED: &#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
RECOMED: A Comprehensive Pharmaceutical Recommendation System. (arXiv:2301.00280v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00280
&lt;/p&gt;
&lt;p&gt;
RECOMED&#26159;&#22522;&#20110;&#24739;&#32773;&#21644;&#33647;&#29289;&#29305;&#24449;&#35774;&#35745;&#30340;&#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#23454;&#29616;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#24739;&#32773;&#26465;&#20214;&#21644;&#21382;&#21490;&#26469;&#36873;&#25321;&#21512;&#36866;&#33647;&#29289;&#65292;&#20197;&#21450;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20174;Drugs.com&#21644;Druglib.com&#25552;&#21462;&#30340;&#24739;&#32773;&#21644;&#33647;&#29289;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#36827;&#34892;&#21512;&#24182;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#24739;&#32773;&#21644;&#33647;&#29289;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#23545;&#24739;&#32773;&#21644;&#33647;&#29289;&#36827;&#34892;&#20102;&#32858;&#31867;&#65292;&#28982;&#21518;&#26681;&#25454;&#24739;&#32773;&#25552;&#20379;&#30340;&#19981;&#21516;&#35780;&#32423;&#20197;&#21450;&#20174;&#24739;&#32773;&#21644;&#33647;&#29289;&#35268;&#26684;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#24182;&#32771;&#34385;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#25512;&#33616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#32452;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#32771;&#34385;&#24739;&#32773;&#30340;&#29366;&#20917;&#21644;&#21382;&#21490;&#26469;&#36873;&#25321;&#29305;&#23450;&#21512;&#36866;&#33647;&#29289;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#22312;&#39044;&#22788;&#29702;&#20013;&#65292;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21644;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#23545;&#31995;&#32479;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comprehensive pharmaceutical recommendation system was designed based on the patients and drugs features extracted from Drugs.com and Druglib.com. First, data from these databases were combined, and a dataset of patients and drug information was built. Secondly, the patients and drugs were clustered, and then the recommendation was performed using different ratings provided by patients, and importantly by the knowledge obtained from patients and drug specifications, and considering drug interactions. To the best of our knowledge, we are the first group to consider patients conditions and history in the proposed approach for selecting a specific medicine appropriate for that particular user. Our approach applies artificial intelligence (AI) models for the implementation. Sentiment analysis using natural language processing approaches is employed in pre-processing along with neural network-based methods and recommender system algorithms for modeling the system. In our work, patients co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#24191;&#21578;&#23450;&#21521;&#36873;&#39033;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#36873;&#32972;&#21253;&#38382;&#39064;&#65292;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#20197;&#22686;&#21152;&#23558;&#23450;&#21521;&#21463;&#20247;&#36716;&#21270;&#20026;&#23454;&#38469;&#36141;&#20080;&#32773;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.01160</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#24191;&#21578;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Profit-Maximizing Strategy for Advertising on the e-Commerce Platforms. (arXiv:2211.01160v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#24191;&#21578;&#23450;&#21521;&#36873;&#39033;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#36873;&#32972;&#21253;&#38382;&#39064;&#65292;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#20197;&#22686;&#21152;&#23558;&#23450;&#21521;&#21463;&#20247;&#36716;&#21270;&#20026;&#23454;&#38469;&#36141;&#20080;&#32773;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#31649;&#29702;&#24179;&#21488;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#30005;&#23376;&#21830;&#21153;&#21334;&#23478;/&#24191;&#21578;&#21830;&#30340;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#26041;&#27861;&#26469;&#21560;&#24341;&#30446;&#26631;&#23458;&#25143;&#12290;&#23613;&#31649;&#26377;&#20854;&#20248;&#21183;&#65292;&#20294;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#22312;&#32447;&#21334;&#23478;&#26469;&#35828;&#65292;&#27491;&#30830;&#37197;&#32622;&#24191;&#21578;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26080;&#25928;&#30340;&#31574;&#30053;&#24448;&#24448;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#26080;&#25928;&#28857;&#20987;&#65292;&#20174;&#32780;&#23548;&#33268;&#24191;&#21578;&#36153;&#29992;&#19982;&#38144;&#21806;&#22686;&#38271;&#19981;&#25104;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#24191;&#21578;&#23450;&#21521;&#36873;&#39033;&#30340;&#26032;&#39062;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26088;&#22312;&#25214;&#21040;&#26368;&#20339;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#23558;&#23450;&#21521;&#21463;&#20247;&#36716;&#21270;&#20026;&#23454;&#38469;&#36141;&#20080;&#32773;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20248;&#21270;&#25361;&#25112;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#36873;&#32972;&#21253;&#38382;&#39064;&#65288;MCKP&#65289;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#20351;&#29992;&#26469;&#33258;&#22825;&#29483;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#24191;&#21578;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online advertising management platform has become increasingly popular among e-commerce vendors/advertisers, offering a streamlined approach to reach target customers. Despite its advantages, configuring advertising strategies correctly remains a challenge for online vendors, particularly those with limited resources. Ineffective strategies often result in a surge of unproductive ``just looking'' clicks, leading to disproportionately high advertising expenses comparing to the growth of sales. In this paper, we present a novel profit-maximing strategy for targeting options of online advertising. The proposed model aims to find the optimal set of features to maximize the probability of converting targeted audiences into actual buyers. We address the optimization challenge by reformulating it as a multiple-choice knapsack problem (MCKP). We conduct an empirical study featuring real-world data from Tmall to show that our proposed method can effectively optimize the advertising strategy
&lt;/p&gt;</description></item><item><title>FiBiNet++&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#29305;&#24449;&#20132;&#20114;&#23618;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;FiBiNet&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;12&#20493;&#33267;16&#20493;&#30340;&#21442;&#25968;&#20943;&#23567;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05016</link><description>&lt;p&gt;
FiBiNet++&#65306;&#36890;&#36807;&#20302;&#31209;&#29305;&#24449;&#20132;&#20114;&#23618;&#20943;&#23567;CTR&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
FiBiNet++: Reducing Model Size by Low Rank Feature Interaction Layer for CTR Prediction. (arXiv:2209.05016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05016
&lt;/p&gt;
&lt;p&gt;
FiBiNet++&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#29305;&#24449;&#20132;&#20114;&#23618;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;FiBiNet&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;12&#20493;&#33267;16&#20493;&#30340;&#21442;&#25968;&#20943;&#23567;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#20272;&#35745;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#28145;&#24230;&#27169;&#22411;&#12290;&#19968;&#20123;&#30740;&#31350;&#35777;&#26126;FiBiNet&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#24182;&#19988;&#22312;Avazu&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;FiBiNet&#30340;&#22823;&#22411;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#20102;&#23427;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FiBiNet++&#27169;&#22411;&#26469;&#37325;&#26032;&#35774;&#35745;FiBiNet&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25216;&#26415;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#20302;&#31209;&#23618;&#8221;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#20132;&#20114;&#65292;&#23427;&#20316;&#20026;&#23454;&#29616;&#27169;&#22411;&#20248;&#36234;&#21387;&#32553;&#27604;&#30340;&#20851;&#38190;&#39537;&#21160;&#22120;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;FiBiNet++&#26377;&#25928;&#22320;&#23558;FiBiNet&#30340;&#38750;&#23884;&#20837;&#24335;&#27169;&#22411;&#21442;&#25968;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20943;&#23567;&#20102;12&#20493;&#33267;16&#20493;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;CTR&#26041;&#27861;&#65288;&#21253;&#25324;FiBiNet&#65289;&#30456;&#27604;&#65292;FiBiNet++&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) estimation has become one of the most fundamental tasks in many real-world applications and various deep models have been proposed. Some research has proved that FiBiNet is one of the best performance models and outperforms all other models on Avazu dataset. However, the large model size of FiBiNet hinders its wider application. In this paper, we propose a novel FiBiNet++ model to redesign FiBiNet's model structure, which greatly reduces model size while further improves its performance. One of the primary techniques involves our proposed "Low Rank Layer" focused on feature interaction, which serves as a crucial driver of achieving a superior compression ratio for models. Extensive experiments on three public datasets show that FiBiNet++ effectively reduces non-embedding model parameters of FiBiNet by 12x to 16x on three datasets. On the other hand, FiBiNet++ leads to significant performance improvements compared to state-of-the-art CTR methods, including FiBiN
&lt;/p&gt;</description></item></channel></rss>