<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20298</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#23884;&#20837;&#21644;&#23618;&#27425;&#24863;&#30693;&#22495;&#35299;&#32806;&#30340;&#22522;&#20110;&#35780;&#35770;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#23545;&#25512;&#33616;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#21560;&#24341;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#25429;&#25417;&#21487;&#22312;&#39046;&#22495;&#38388;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20174;&#26356;&#20016;&#23500;&#30340;&#39046;&#22495;&#65288;&#28304;&#39046;&#22495;&#65289;&#36716;&#31227;&#21040;&#26356;&#31232;&#30095;&#30340;&#39046;&#22495;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#27431;&#20960;&#37324;&#24503;&#23884;&#20837;&#31354;&#38388;&#65292;&#22312;&#20934;&#30830;&#34920;&#31034;&#26356;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22788;&#29702;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#20513;&#23548;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#21452;&#26354;CDR&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#12290;&#39318;&#20808;&#24378;&#35843;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#21452;&#26354;&#20960;&#20309;&#20013;&#23545;&#23567;&#20462;&#25913;&#36896;&#25104;&#30340;&#24178;&#25200;&#20250;&#34987;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#23618;&#27425;&#24615;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20298v1 Announce Type: cross  Abstract: The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CUT&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#36807;&#28388;&#29992;&#25143;&#30340;&#21327;&#20316;&#20449;&#24687;&#26469;&#35299;&#20915;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.20296</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#65306;&#20026;&#36328;&#39046;&#22495;&#25512;&#33616;&#36807;&#28388;&#21327;&#21516;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CUT&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#36807;&#28388;&#29992;&#25143;&#30340;&#21327;&#20316;&#20449;&#24687;&#26469;&#35299;&#20915;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28304;&#39046;&#22495;&#20013;&#30340;&#19981;&#30456;&#20851;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#30446;&#26631;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#36825;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#25143;&#36716;&#25442;&#65288;CUT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#36807;&#28388;&#29992;&#25143;&#30340;&#21327;&#20316;&#20449;&#24687;&#26469;&#35299;&#20915;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#22312;CUT&#20013;&#65292;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#29992;&#25143;&#30456;&#20284;&#24230;&#34987;&#37319;&#29992;&#20316;&#20026;&#29992;&#25143;&#36716;&#25442;&#23398;&#20064;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#36807;&#28388;&#29992;&#25143;&#30340;&#21327;&#20316;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20296v1 Announce Type: new  Abstract: Cross-domain recommender (CDR) systems aim to enhance the performance of the target domain by utilizing data from other related domains. However, irrelevant information from the source domain may instead degrade target domain performance, which is known as the negative transfer problem. There have been some attempts to address this problem, mostly by designing adaptive representations for overlapped users. Whereas, representation adaptions solely rely on the expressive capacity of the CDR model, lacking explicit constraint to filter the irrelevant source-domain collaborative information for the target domain.   In this paper, we propose a novel Collaborative information regularized User Transformation (CUT) framework to tackle the negative transfer problem by directly filtering users' collaborative information. In CUT, user similarity in the target domain is adopted as a constraint for user transformation learning to filter the user coll
&lt;/p&gt;</description></item><item><title>&#24352;&#28023;&#27915;&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20302;&#24310;&#36831;&#35774;&#32622;&#19979;&#65292;&#36739;&#24369;&#30340;&#27973;&#23618;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#23436;&#25972;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#33021;&#21463;&#30410;&#20110;&#24191;&#20041;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;gBCE&#65289;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.20222</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#24310;&#36831;&#26816;&#32034;&#30340;&#27973;&#23618;&#20132;&#21449;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Shallow Cross-Encoders for Low-Latency Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20222
&lt;/p&gt;
&lt;p&gt;
&#24352;&#28023;&#27915;&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20302;&#24310;&#36831;&#35774;&#32622;&#19979;&#65292;&#36739;&#24369;&#30340;&#27973;&#23618;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#23436;&#25972;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#33021;&#21463;&#30410;&#20110;&#24191;&#20041;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;gBCE&#65289;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#22312;&#25991;&#26412;&#26816;&#32034;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#65288;&#22914;BERT&#25110;T5&#65289;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#19988;&#21482;&#20801;&#35768;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#24310;&#36831;&#26102;&#38388;&#31383;&#21475;&#20869;&#35780;&#20998;&#23569;&#37327;&#25991;&#26723;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#29992;&#20110;&#36825;&#20123;&#23454;&#38469;&#20302;&#24310;&#36831;&#35774;&#32622;&#30340;&#36739;&#24369;&#30340;&#27973;&#23618;Transformer&#27169;&#22411;&#65288;&#21363;&#20855;&#26377;&#26377;&#38480;&#23618;&#25968;&#30340;Transformer&#65289;&#23454;&#38469;&#19978;&#27604;&#23436;&#25972;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#21516;&#26679;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#20272;&#31639;&#20986;&#26356;&#22810;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#27973;&#23618;Transformer&#21487;&#33021;&#20250;&#21463;&#30410;&#20110;&#26368;&#36817;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#31034;&#25104;&#21151;&#30340;&#24191;&#20041;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;gBCE&#65289;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TREC&#28145;&#24230;&#23398;&#20064;&#27573;&#33853;&#25490;&#24207;&#26597;&#35810;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20222v1 Announce Type: cross  Abstract: Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;/&#26497;&#24615;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23545;&#20598;&#20307;&#31215;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21333;&#32431;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;SSMF&#31639;&#27861;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20197</link><description>&lt;p&gt;
&#21452;&#23545;&#20598;&#20307;&#31215;&#26368;&#22823;&#21270;&#29992;&#20110;&#21333;&#32431;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20197
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;/&#26497;&#24615;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23545;&#20598;&#20307;&#31215;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21333;&#32431;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;SSMF&#31639;&#27861;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Simplex-structured matrix factorization&#65288;SSMF&#65289;&#26159;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#27867;&#21270;&#65292;&#26159;&#19968;&#31181;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#65292;&#22312;&#39640;&#20809;&#35889;&#35299;&#28151;&#21644;&#21644;&#20027;&#39064;&#24314;&#27169;&#20013;&#26377;&#24212;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#35782;&#21035;&#30340;&#35299;&#65292;&#26631;&#20934;&#26041;&#27861;&#26159;&#23547;&#25214;&#26368;&#23567;&#20307;&#31215;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#38754;&#20307;&#30340;&#23545;&#20598;/&#26497;&#24615;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#20307;&#31215;SSMF&#36716;&#25442;&#20026;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#26368;&#22823;&#20307;&#31215;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#36825;&#20010;&#26368;&#22823;&#20307;&#31215;&#23545;&#20598;&#38382;&#39064;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#23545;&#20598;&#20844;&#24335;&#25552;&#20379;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;SSMF&#30340;&#20004;&#20010;&#29616;&#26377;&#31639;&#27861;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21363;&#20307;&#31215;&#26368;&#23567;&#21270;&#21644;&#38754;&#35782;&#21035;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;SSMF&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20197v1 Announce Type: cross  Abstract: Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and topic modeling. To obtain identifiable solutions, a standard approach is to find minimum-volume solutions. By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space. We first prove the identifiability of this maximum-volume dual problem. Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification. Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;CL4FedRec&#65292;&#33021;&#22815;&#36890;&#36807;&#23884;&#20837;&#22686;&#24378;&#20805;&#20998;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#31232;&#30095;&#25968;&#25454;&#65292;&#25552;&#39640;&#32852;&#37030;&#23545;&#27604;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20107</link><description>&lt;p&gt;
&#23545;&#25239;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;&#30340;&#24378;&#22823;&#32852;&#37030;&#23545;&#27604;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust Federated Contrastive Recommender System against Model Poisoning Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;CL4FedRec&#65292;&#33021;&#22815;&#36890;&#36807;&#23884;&#20837;&#22686;&#24378;&#20805;&#20998;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#31232;&#30095;&#25968;&#25454;&#65292;&#25552;&#39640;&#32852;&#37030;&#23545;&#27604;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRecs&#65289;&#22240;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#22909;&#22788;&#32780;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;FedRecs&#30340;&#21435;&#20013;&#24515;&#21270;&#21644;&#24320;&#25918;&#29305;&#24615;&#23384;&#22312;&#20004;&#20010;&#22256;&#22659;&#12290;&#39318;&#20808;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35774;&#22791;&#25968;&#25454;&#38750;&#24120;&#31232;&#30095;&#65292;&#23548;&#33268;FedRecs&#30340;&#24615;&#33021;&#21463;&#25439;&#12290;&#20854;&#27425;&#65292;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#29992;&#25143;&#21457;&#21160;&#30340;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#20174;&#32780;&#30772;&#22351;&#20102;&#31995;&#32479;&#30340;&#31283;&#22266;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23884;&#20837;&#22686;&#24378;&#20805;&#20998;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#31232;&#30095;&#25968;&#25454;&#65292;&#34987;&#31216;&#20026;CL4FedRec&#12290;&#19982;&#20808;&#21069;&#22312;FedRecs&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20849;&#20139;&#31169;&#26377;&#21442;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;CL4FedRec&#19982;&#22522;&#26412;&#30340;FedRec&#23398;&#20064;&#21327;&#35758;&#19968;&#33268;&#65292;&#30830;&#20445;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FedRec&#23454;&#29616;&#20860;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20855;&#26377;CL4FedRec&#30340;FedRecs&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#26469;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20107v1 Announce Type: new  Abstract: Federated Recommender Systems (FedRecs) have garnered increasing attention recently, thanks to their privacy-preserving benefits. However, the decentralized and open characteristics of current FedRecs present two dilemmas. First, the performance of FedRecs is compromised due to highly sparse on-device data for each client. Second, the system's robustness is undermined by the vulnerability to model poisoning attacks launched by malicious users. In this paper, we introduce a novel contrastive learning framework designed to fully leverage the client's sparse data through embedding augmentation, referred to as CL4FedRec. Unlike previous contrastive learning approaches in FedRecs that necessitate clients to share their private parameters, our CL4FedRec aligns with the basic FedRec learning protocol, ensuring compatibility with most existing FedRec implementations. We then evaluate the robustness of FedRecs equipped with CL4FedRec by subjectin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20559;&#22909;&#26469;&#31934;&#28860;&#30693;&#35782;&#22270;&#35889;&#65292;&#20445;&#30041;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#27905;&#30340;&#29289;&#21697;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.20095</link><description>&lt;p&gt;
KGUF: &#24102;&#26377;&#22522;&#20110;&#29992;&#25143;&#30340;&#35821;&#20041;&#29305;&#24449;&#36807;&#28388;&#30340;&#31616;&#21333;&#30693;&#35782;&#24863;&#30693;&#22270;&#25512;&#33616;&#22120;
&lt;/p&gt;
&lt;p&gt;
KGUF: Simple Knowledge-aware Graph-based Recommender with User-based Semantic Features Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20095
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20559;&#22909;&#26469;&#31934;&#28860;&#30693;&#35782;&#22270;&#35889;&#65292;&#20445;&#30041;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#27905;&#30340;&#29289;&#21697;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24341;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#23478;&#26063;&#65292;&#21363;&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;GCF&#65289;&#12290;&#39034;&#24212;&#21516;&#26679;&#30340;GNNs&#28010;&#28526;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#20063;&#25104;&#21151;&#22320;&#20511;&#21161;GCF&#21407;&#29702;&#26469;&#32467;&#21512;GNNs&#30340;&#34920;&#24449;&#33021;&#21147;&#21644;KGs&#20256;&#36798;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#30693;&#35782;&#24863;&#30693;&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;KGCF&#65289;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#25366;&#25496;&#38544;&#34255;&#30340;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#35745;&#31639;&#21644;&#32452;&#21512;&#29992;&#25143;&#32423;&#24847;&#22270;&#24182;&#38750;&#22987;&#32456;&#24517;&#35201;&#65292;&#22240;&#20026;&#31616;&#21333;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#20135;&#29983;&#30456;&#24403;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#30041;&#26126;&#30830;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#29992;&#25143;&#30340;&#21382;&#21490;&#20559;&#22909;&#23545;&#32454;&#21270;KG&#21644;&#20445;&#30041;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#27905;&#30340;&#29289;&#21697;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20095v1 Announce Type: new  Abstract: The recent integration of Graph Neural Networks (GNNs) into recommendation has led to a novel family of Collaborative Filtering (CF) approaches, namely Graph Collaborative Filtering (GCF). Following the same GNNs wave, recommender systems exploiting Knowledge Graphs (KGs) have also been successfully empowered by the GCF rationale to combine the representational power of GNNs with the semantics conveyed by KGs, giving rise to Knowledge-aware Graph Collaborative Filtering (KGCF), which use KGs to mine hidden user intent. Nevertheless, empirical evidence suggests that computing and combining user-level intent might not always be necessary, as simpler approaches can yield comparable or superior results while keeping explicit semantic features. Under this perspective, user historical preferences become essential to refine the KG and retain the most discriminating features, thus leading to concise item representation. Driven by the assumptions
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#22522;&#20110;&#25991;&#26412;&#21644;&#28151;&#21512;&#31995;&#32479;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#34429;&#28982;&#23384;&#22312;&#20449;&#24687;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20294;&#28151;&#21512;&#31995;&#32479;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#21442;&#19982;&#24230;&#65292;&#20026;&#21253;&#25324;&#26234;&#21147;&#38556;&#30861;&#32773;&#22312;&#20869;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.19899</link><description>&lt;p&gt;
&#21021;&#27493;&#22522;&#20110;&#22270;&#20687;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#21253;&#23481;&#35774;&#35745;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Inclusive Design Insights from a Preliminary Image-Based Conversational Search Systems Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19899
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#22522;&#20110;&#25991;&#26412;&#21644;&#28151;&#21512;&#31995;&#32479;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#34429;&#28982;&#23384;&#22312;&#20449;&#24687;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20294;&#28151;&#21512;&#31995;&#32479;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#21442;&#19982;&#24230;&#65292;&#20026;&#21253;&#25324;&#26234;&#21147;&#38556;&#30861;&#32773;&#22312;&#20869;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#39046;&#22495;&#35265;&#35777;&#20102;&#21508;&#31181;&#25628;&#32034;&#27169;&#24335;&#30340;&#23835;&#36215;&#65292;&#20854;&#20013;&#22522;&#20110;&#22270;&#20687;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#33073;&#39062;&#32780;&#20986;&#12290;&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#35813;&#29305;&#23450;&#31995;&#32479;&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#25991;&#26412;&#21644;&#28151;&#21512;&#30340;&#23545;&#29031;&#31995;&#32479;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#22810;&#26679;&#21270;&#30340;&#21442;&#19982;&#32773;&#38431;&#20237;&#30830;&#20445;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#33539;&#22260;&#12290;&#20808;&#36827;&#24037;&#20855;&#20419;&#36827;&#24773;&#32490;&#20998;&#26512;&#65292;&#25429;&#25417;&#29992;&#25143;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#30340;&#24773;&#32490;&#65292;&#32780;&#32467;&#26500;&#21270;&#21453;&#39304;&#20250;&#35805;&#25552;&#20379;&#20102;&#23450;&#24615;&#27934;&#35265;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25991;&#26412;&#30340;&#31995;&#32479;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#29992;&#25143;&#30340;&#22256;&#24785;&#65292;&#20294;&#22522;&#20110;&#22270;&#20687;&#30340;&#31995;&#32479;&#23384;&#22312;&#30452;&#25509;&#20449;&#24687;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#31995;&#32479;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#21442;&#19982;&#24230;&#65292;&#34920;&#26126;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#26368;&#20339;&#34701;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#24335;&#65292;&#20197;&#21327;&#21161;&#26234;&#21147;&#38556;&#30861;&#32773;&#30340;&#20010;&#20307;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19899v1 Announce Type: new  Abstract: The digital realm has witnessed the rise of various search modalities, among which the Image-Based Conversational Search System stands out. This research delves into the design, implementation, and evaluation of this specific system, juxtaposing it against its text-based and mixed counterparts. A diverse participant cohort ensures a broad evaluation spectrum. Advanced tools facilitate emotion analysis, capturing user sentiments during interactions, while structured feedback sessions offer qualitative insights. Results indicate that while the text-based system minimizes user confusion, the image-based system presents challenges in direct information interpretation. However, the mixed system achieves the highest engagement, suggesting an optimal blend of visual and textual information. Notably, the potential of these systems, especially the image-based modality, to assist individuals with intellectual disabilities is highlighted. The study
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19889</link><description>&lt;p&gt;
&#26397;&#21521;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#25688;&#35201;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Robust Retrieval-Based Summarization System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#30340;&#35843;&#26597;&#12290;&#34429;&#28982;LLMs&#25552;&#20379;&#20102;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;LogicSumm&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29616;&#23454;&#22330;&#26223;&#65292;&#29992;&#26469;&#35780;&#20272;LLMs&#22312;RAG&#22522;&#30784;&#25688;&#35201;&#36807;&#31243;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;&#26681;&#25454;LogicSumm&#35782;&#21035;&#20986;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SummRAG&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#21019;&#24314;&#35757;&#32451;&#23545;&#35805;&#24182;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22312;LogicSumm&#22330;&#26223;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;SummRAG&#26159;&#25105;&#20204;&#23450;&#20041;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#27979;&#35797;LLM&#33021;&#21147;&#30340;&#30446;&#26631;&#30340;&#19968;&#20010;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#19968;&#21171;&#27704;&#36920;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;SummRAG&#30340;&#24378;&#22823;&#65292;&#23637;&#31034;&#20102;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#25688;&#35201;&#36136;&#37327;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19889v1 Announce Type: cross  Abstract: This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess LLM robustness during RAG-based summarization. Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and fine-tune a model to enhance robustness within LogicSumm's scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an LLM, rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and summarization quality. Data, corresponding model weights, and Py
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#32570;&#22833;&#27169;&#24577;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#37325;&#26032;&#26500;&#24819;&#20026;&#32570;&#22833;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#20102;&#26368;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.19841</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20256;&#25773;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dealing with Missing Modalities in Multimodal Recommendation: a Feature Propagation-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#32570;&#22833;&#27169;&#24577;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#37325;&#26032;&#26500;&#24819;&#20026;&#32570;&#22833;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#20102;&#26368;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#20174;&#25551;&#36848;&#20135;&#21697;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#25551;&#36848;&#25110;&#38899;&#39057;&#36712;&#36947;&#20013;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#26469;&#22686;&#24378;&#20135;&#21697;&#30446;&#24405;&#20013;&#20135;&#21697;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21482;&#26377;&#24456;&#23569;&#19968;&#37096;&#20998;&#20135;&#21697;&#38468;&#24102;&#22810;&#27169;&#24577;&#20869;&#23481;&#65292;&#20174;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#21464;&#24471;&#22256;&#38590;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#20851;&#20110;&#22312;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#24037;&#20316;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#20316;&#20026;&#19968;&#39033;&#21021;&#27493;&#23581;&#35797;&#65292;&#26088;&#22312;&#24418;&#24335;&#21270;&#24182;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21463;&#26368;&#36817;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#37325;&#26032;&#26500;&#24819;&#20026;&#32570;&#22833;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26368;&#32456;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#29305;&#24449;&#20256;&#25773;&#31639;&#27861;&#12290;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#29992;&#25143;-&#29289;&#21697;&#22270;&#25237;&#24433;&#21040;&#19968;&#20010;&#24453;&#23450;&#30340;&#31354;&#38388;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19841v1 Announce Type: new  Abstract: Multimodal recommender systems work by augmenting the representation of the products in the catalogue through multimodal features extracted from images, textual descriptions, or audio tracks characterising such products. Nevertheless, in real-world applications, only a limited percentage of products come with multimodal content to extract meaningful features from, making it hard to provide accurate recommendations. To the best of our knowledge, very few attention has been put into the problem of missing modalities in multimodal recommendation so far. To this end, our paper comes as a preliminary attempt to formalise and address such an issue. Inspired by the recent advances in graph representation learning, we propose to re-sketch the missing modalities problem as a problem of missing graph node features to apply the state-of-the-art feature propagation algorithm eventually. Technically, we first project the user-item graph into an item-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#65292;CAPR&#26694;&#26550;&#21019;&#26032;&#24615;&#22320;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#25552;&#31034;&#37325;&#32452;&#36807;&#31243;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.19716</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33021;&#21147;&#24863;&#30693;&#25552;&#31034;&#37325;&#32452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Capability-aware Prompt Reformulation Learning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19716
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#65292;CAPR&#26694;&#26550;&#21019;&#26032;&#24615;&#22320;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#25552;&#31034;&#37325;&#32452;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#33402;&#26415;&#21019;&#20316;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#24037;&#20855;&#65292;&#20026;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#35270;&#35273;&#33402;&#26415;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#24120;&#24120;&#23545;&#19981;&#29087;&#24713;&#25552;&#31034;&#21046;&#20316;&#30340;&#29992;&#25143;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#26085;&#24535;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#29992;&#25143;&#25552;&#31034;&#30340;&#37325;&#32452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20010;&#20307;&#29992;&#25143;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#37325;&#32452;&#23545;&#30340;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33021;&#21147;&#24863;&#30693;&#25552;&#31034;&#37325;&#32452;&#65288;CAPR&#65289;&#26694;&#26550;&#12290;CAPR&#21019;&#26032;&#24615;&#22320;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#37325;&#32452;&#36807;&#31243;&#20013;&#65306;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19716v1 Announce Type: cross  Abstract: Text-to-image generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual prompts into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided prompts, which often poses a challenge to users unfamiliar with prompt crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic prompt reformulation model. Our in-depth analysis of these logs reveals that user prompt reformulation is heavily dependent on the individual user's capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Refo
&lt;/p&gt;</description></item><item><title>STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;</title><link>https://arxiv.org/abs/2403.19710</link><description>&lt;p&gt;
STRUM-LLM: &#23646;&#24615;&#21270;&#21644;&#32467;&#26500;&#21270;&#23545;&#27604;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM: Attributed and Structured Contrastive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19710
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#32463;&#24120;&#22312;&#20004;&#20010;&#36873;&#39033;&#65288;A vs B&#65289;&#20043;&#38388;&#20570;&#20915;&#31574;&#26102;&#24863;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#22810;&#20010;&#32593;&#39029;&#19978;&#36827;&#34892;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STRUM-LLM&#65292;&#36890;&#36807;&#29983;&#25104;&#24102;&#23646;&#24615;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#65292;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;STRUM-LLM&#35782;&#21035;&#20102;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#65306;&#20004;&#20010;&#36873;&#39033;&#22312;&#21738;&#20123;&#29305;&#23450;&#23646;&#24615;&#19978;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#26377;&#21487;&#33021;&#24433;&#21709;&#29992;&#25143;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#24182;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#20316;&#20026;&#30417;&#30563;&#12290;STRUM-LLM&#23558;&#25152;&#26377;&#25552;&#21462;&#30340;&#20869;&#23481;&#23646;&#24615;&#21270;&#65292;&#20197;&#21450;&#25991;&#26412;&#35777;&#25454;&#65292;&#19988;&#19981;&#38480;&#21046;&#20854;&#22788;&#29702;&#30340;&#36755;&#20837;&#26469;&#28304;&#30340;&#38271;&#24230;&#12290;STRUM-LLM Distilled&#30340;&#21534;&#21520;&#37327;&#27604;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#27169;&#22411;&#39640;100&#20493;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;10&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19710v1 Announce Type: cross  Abstract: Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user's decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM Distilled has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;LLMHG&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25551;&#36848;&#21644;&#35299;&#37322;&#29992;&#25143;&#20852;&#36259;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.08217</link><description>&lt;p&gt;
LLM&#24341;&#23548;&#30340;&#22810;&#35270;&#22270;&#36229;&#22270;&#23398;&#20064;&#29992;&#20110;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;LLMHG&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25551;&#36848;&#21644;&#35299;&#37322;&#29992;&#25143;&#20852;&#36259;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#20449;&#24687;&#36807;&#36733;&#26102;&#20195;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20165;&#20381;&#36182;&#21382;&#21490;&#29992;&#25143;&#20132;&#20114;&#30340;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#20154;&#31867;&#20852;&#36259;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#20855;&#20154;&#20026;&#20013;&#24515;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;LLMHG&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#20248;&#21183;&#30456;&#21327;&#21516;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#25551;&#36848;&#21644;&#35299;&#37322;&#20010;&#20307;&#29992;&#25143;&#20852;&#36259;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#26126;&#30830;&#32771;&#34385;&#20154;&#31867;&#20559;&#22909;&#30340;&#22797;&#26434;&#24615;&#20801;&#35768;&#25105;&#20204;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#21644;&#21487;&#35299;&#37322;&#30340;LLMHG&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#21363;&#25554;&#21363;&#29992;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08217v2 Announce Type: replace  Abstract: As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests. To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks. By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability. We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets. The proposed plug-and-play enhancement framework delive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;</title><link>https://arxiv.org/abs/2206.01818</link><description>&lt;p&gt;
QAGCN&#65306;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20851;&#31995;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#30001;&#22810;&#20010;&#20851;&#31995;&#32452;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#38271;&#26102;&#38388;&#25512;&#29702;&#38142;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#26126;&#26174;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36880;&#27493;&#26631;&#31614;&#20256;&#25773;&#30340;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27983;&#35272;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#23427;&#20204;&#30340;&#25512;&#29702;&#26426;&#21046;&#36890;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#23454;&#29616;&#25110;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#23454;&#29616;&#22810;&#20851;&#31995;QA&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#26356;&#39640;&#25928;&#19988;&#26356;&#26131;&#20110;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; QAGCN -- &#19968;&#31181;&#22522;&#20110;&#38382;&#39064;&#24847;&#35782;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#21463;&#25511;&#38382;&#39064;&#30456;&#20851;&#20449;&#24687;&#20256;&#25773;&#30340;GCN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.01818v3 Announce Type: replace  Abstract: Multi-relation question answering (QA) is a challenging task, where given questions usually require long reasoning chains in KGs that consist of multiple relations. Recently, methods with explicit multi-step reasoning over KGs have been prominently used in this task and have demonstrated promising performance. Examples include methods that perform stepwise label propagation through KG triples and methods that navigate over KG triples based on reinforcement learning. A main weakness of these methods is that their reasoning mechanisms are usually complex and difficult to implement or train. In this paper, we argue that multi-relation QA can be achieved via end-to-end single-step implicit reasoning, which is simpler, more efficient, and easier to adopt. We propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based method that includes a novel GCN architecture with controlled question-dependent message propagation for the 
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item></channel></rss>