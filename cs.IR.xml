<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;QBBN&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36923;&#36753;&#21644;&#27010;&#29575;&#25512;&#29702;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22916;&#24819;&#38382;&#39064;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#38454;&#28436;&#31639;&#27861;&#30340;&#38190;&#20540;&#29256;&#26412;&#65292;QBBN&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#32972;&#21518;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#35299;&#30340;&#65292;&#20294;&#21487;&#20197;&#20351;&#29992;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;LBP&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.06557</link><description>&lt;p&gt;
&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65306;&#36923;&#36753;&#22270;&#27169;&#22411;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;QBBN&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36923;&#36753;&#21644;&#27010;&#29575;&#25512;&#29702;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22916;&#24819;&#38382;&#39064;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#38454;&#28436;&#31639;&#27861;&#30340;&#38190;&#20540;&#29256;&#26412;&#65292;QBBN&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#32972;&#21518;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#35299;&#30340;&#65292;&#20294;&#21487;&#20197;&#20351;&#29992;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;LBP&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;QBBN&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36923;&#36753;&#21644;&#27010;&#29575;&#25512;&#29702;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;QBBN&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21363;LLM&#20250;&#20986;&#29616;&#22916;&#24819;&#29616;&#35937;&#12290;&#30001;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26500;&#24314;&#26041;&#24335;&#65292;&#23427;&#26080;&#27861;&#20135;&#29983;&#22916;&#24819;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#36820;&#22238;&#21487;&#20197;&#35299;&#37322;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#37197;&#32622;&#19968;&#20010;&#21547;&#26377;&#26080;&#38480;&#25968;&#37327;&#24067;&#23572;&#21464;&#37327;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#32972;&#21518;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#31181;&#38190;-&#20540;&#29256;&#26412;&#30340;&#19968;&#38454;&#28436;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#20854;&#19968;&#33268;&#24615;&#21644;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#23436;&#20840;&#35266;&#27979;&#25968;&#25454;&#19978;&#26159;&#26131;&#20110;&#35757;&#32451;&#30340;&#65292;&#20294;&#25512;&#29702;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#35299;&#30340;&#65288;&#21363;$N$&#20010;&#21464;&#37327;&#30340;&#25512;&#29702;&#22797;&#26434;&#24230;&#20026;$\Omega(2^N)$&#65289;&#12290;&#23545;&#20110;&#25512;&#29702;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;LBP&#65289;&#30340;&#20351;&#29992;&#65292;&#23427;&#24182;&#19981;...
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Quantified Boolean Bayesian Network (QBBN), which provides a unified view of logical and probabilistic reasoning. The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates. A Bayesian Network, by construction, cannot hallucinate, because it can only return answers that it can explain. We show how a Bayesian Network over an unbounded number of boolean variables can be configured to represent the logical reasoning underlying human language. We do this by creating a key-value version of the First-Order Calculus, for which we can prove consistency and completeness. We show that the model is trivially trained over fully observed data, but that inference is non-trivial. Exact inference in a Bayesian Network is intractable (i.e. $\Omega(2^N)$ for $N$ variables). For inference, we investigate the use of Loopy Belief Propagation (LBP), which is not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#32593;&#32476;&#28165;&#21333;&#65292;&#29992;&#20110;&#38598;&#25104;&#21644;&#20849;&#20139;&#19982;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#33258;&#21160;&#21270;&#22320;&#23558;&#36825;&#20123;&#20449;&#24687;&#38598;&#25104;&#21040;&#32593;&#32476;&#28165;&#21333;&#20013;&#12290;&#36890;&#36807;&#23454;&#26045;&#21644;&#39564;&#35777;&#21407;&#22411;&#65292;&#35777;&#26126;&#20102;&#32593;&#32476;&#28165;&#21333;&#21487;&#20197;&#31616;&#21270;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30340;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.06511</link><description>&lt;p&gt;
&#26500;&#24314;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30340;&#35821;&#20041;&#32593;&#32476;&#28165;&#21333;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward Building a Semantic Network Inventory for Model-Driven Telemetry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#32593;&#32476;&#28165;&#21333;&#65292;&#29992;&#20110;&#38598;&#25104;&#21644;&#20849;&#20139;&#19982;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#33258;&#21160;&#21270;&#22320;&#23558;&#36825;&#20123;&#20449;&#24687;&#38598;&#25104;&#21040;&#32593;&#32476;&#28165;&#21333;&#20013;&#12290;&#36890;&#36807;&#23454;&#26045;&#21644;&#39564;&#35777;&#21407;&#22411;&#65292;&#35777;&#26126;&#20102;&#32593;&#32476;&#28165;&#21333;&#21487;&#20197;&#31616;&#21270;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#27169;&#22411;&#30340;&#32593;&#32476;&#36965;&#27979;&#34987;&#26399;&#26395;&#25104;&#20026;&#39640;&#25928;&#25910;&#38598;&#32593;&#32476;&#35774;&#22791;&#36816;&#34892;&#25968;&#25454;&#30340;&#26631;&#20934;&#26426;&#21046;&#12290;&#20294;&#26159;&#65292;&#26631;&#20934;&#21644;&#19987;&#26377;&#25968;&#25454;&#27169;&#22411;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#32593;&#32476;&#20379;&#24212;&#21830;&#25552;&#20379;&#30340;&#36965;&#27979;&#21327;&#35758;&#30340;&#19981;&#21516;&#23454;&#29616;&#65292;&#20351;&#24471;&#30417;&#25511;&#24322;&#26500;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20419;&#36827;&#19982;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#38598;&#25104;&#21644;&#20849;&#20139;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#32593;&#32476;&#28165;&#21333;&#65292;&#35813;&#28165;&#21333;&#38598;&#25104;&#20102;&#29305;&#23450;&#24320;&#21457;&#30340;&#26032;&#20449;&#24687;&#27169;&#22411;&#65292;&#20197;&#19968;&#31181;&#20379;&#24212;&#21830;&#26080;&#20851;&#30340;&#26041;&#24335;&#25429;&#33719;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20351;&#29992;&#20102;&#24403;&#21069;&#19978;&#19979;&#25991;&#31649;&#29702;&#25152;&#23450;&#20041;&#30340;&#26631;&#20934;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#22320;&#23558;&#36825;&#20123;&#19978;&#19979;&#25991;&#20449;&#24687;&#38598;&#25104;&#21040;&#32593;&#32476;&#28165;&#21333;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#23454;&#26045;&#24182;&#39564;&#35777;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#30340;&#21407;&#22411;&#65292;&#38416;&#36848;&#20102;&#32593;&#32476;&#28165;&#21333;&#22914;&#20309;&#31616;&#21270;&#27169;&#22411;&#39537;&#21160;&#36965;&#27979;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network telemetry based on data models is expected to become the standard mechanism for collecting operational data from network devices efficiently. But the wide variety of standard and proprietary data models along with the different implementations of telemetry protocols offered by network vendors, become a barrier when monitoring heterogeneous network infrastructures. To facilitate the integration and sharing of context information related to model-driven telemetry, this work proposes a semantic network inventory that integrates new information models specifically developed to capture context information in a vendor-agnostic fashion using current standards defined for context management. To automate the integration of this context information within the network inventory, a reference architecture is designed. Finally, a prototype of the solution is implemented and validated through a case study that illustrates how the network inventory can ease the operation of model-driven teleme
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;348&#20010;&#25991;&#20214;&#25910;&#34255;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#21457;&#29616;&#20154;&#20204;&#30340;&#25968;&#23383;&#25991;&#20214;&#25910;&#34255;&#20013;&#21253;&#21547;&#20016;&#23500;&#22810;&#26679;&#30340;&#20869;&#23481;&#12290;&#19981;&#21516;&#30340;&#25910;&#34255;&#31867;&#22411;&#22312;&#20869;&#23481;&#12289;&#37325;&#22797;&#29575;&#21644;&#20351;&#29992;&#30446;&#30340;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#36890;&#36807;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#22909;&#22320;&#35774;&#35745;&#21644;&#27979;&#35797;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#30340;&#26381;&#21153;&#21644;&#36719;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.06421</link><description>&lt;p&gt;
&#20154;&#20204;&#30340;&#25968;&#23383;&#25991;&#20214;&#25910;&#34255;&#20013;&#26377;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What's in People's Digital File Collections?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;348&#20010;&#25991;&#20214;&#25910;&#34255;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#21457;&#29616;&#20154;&#20204;&#30340;&#25968;&#23383;&#25991;&#20214;&#25910;&#34255;&#20013;&#21253;&#21547;&#20016;&#23500;&#22810;&#26679;&#30340;&#20869;&#23481;&#12290;&#19981;&#21516;&#30340;&#25910;&#34255;&#31867;&#22411;&#22312;&#20869;&#23481;&#12289;&#37325;&#22797;&#29575;&#21644;&#20351;&#29992;&#30446;&#30340;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#36890;&#36807;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#22909;&#22320;&#35774;&#35745;&#21644;&#27979;&#35797;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#30340;&#26381;&#21153;&#21644;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#20154;&#20204;&#25968;&#23383;&#25991;&#20214;&#25910;&#34255;&#20013;&#30340;&#20869;&#23481;&#23545;&#20110;&#35774;&#35745;&#25903;&#25345;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#65288;PIM&#65289;&#30340;&#26381;&#21153;&#21644;&#36827;&#34892;&#36719;&#20214;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20851;&#20110;&#20154;&#20204;&#30340;&#25991;&#20214;&#25910;&#34255;&#65292;&#23588;&#20854;&#26159;&#20010;&#20154;&#25910;&#34255;&#30340;&#20102;&#35299;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26368;&#36817;&#20851;&#20110;348&#20010;&#25991;&#20214;&#25910;&#34255;&#32467;&#26500;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25910;&#34255;&#30340;&#20869;&#23481;&#65292;&#37325;&#22797;&#20869;&#23481;&#30340;&#25968;&#37327;&#20197;&#21450;&#20010;&#20154;&#25910;&#34255;&#21644;&#23398;&#20064;&#24037;&#20316;&#29992;&#36884;&#25910;&#34255;&#30340;&#24046;&#24322;&#12290;&#23613;&#31649;&#25152;&#26377;&#25910;&#34255;&#37117;&#21253;&#21547;&#24456;&#22810;&#22270;&#29255;&#65292;&#20294;&#19968;&#20123;&#30452;&#35266;&#19978;&#24120;&#35265;&#30340;&#25991;&#20214;&#31867;&#22411;&#21364;&#30456;&#24403;&#31232;&#32570;&#12290;&#20010;&#20154;&#25910;&#34255;&#21253;&#21547;&#26356;&#22810;&#38899;&#39057;&#65292;&#30693;&#35782;&#22411;&#24037;&#20316;&#32773;&#30340;&#25910;&#34255;&#21253;&#21547;&#26356;&#22810;&#25991;&#26412;&#25991;&#26723;&#20294;&#25991;&#20214;&#22841;&#36739;&#23569;&#65292;IT&#25910;&#34255;&#20855;&#26377;&#19981;&#23547;&#24120;&#30340;&#29305;&#28857;&#12290;&#25910;&#34255;&#30340;&#37325;&#22797;&#19982;&#20854;&#32467;&#26500;&#29305;&#24449;&#30456;&#20851;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#25910;&#34255;&#30340;&#24180;&#40836;&#26080;&#20851;&#12290;&#25105;&#20204;&#26681;&#25454;&#20197;&#21069;&#30340;&#30740;&#31350;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21508;&#31181;&#31867;&#22411;&#20449;&#24687;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thoughtfully designing services and rigorously testing software to support personal information management (PIM) requires understanding the relevant collections, but relatively little is known about what people keep in their file collections, especially personal collections. Complementing recent work on the structure of 348 file collections, we examine those collections' contents, how much content is duplicated, and how collections used for personal matters differ from those used for study and work. Though all collections contain many images, some intuitively common file types are surprisingly scarce. Personal collections contain more audio than others, knowledge workers' collections contain more text documents but far fewer folders, and IT collections exhibit unusual traits. Collection duplication is correlated to collections' structural traits, but surprisingly, not to collection age. We discuss our findings in light of prior works and provide implications for various kinds of inform
&lt;/p&gt;</description></item><item><title>CoSearchAgent&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#65292;&#21487;&#20316;&#20026;Slack&#25554;&#20214;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.06360</link><description>&lt;p&gt;
CoSearchAgent:&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06360
&lt;/p&gt;
&lt;p&gt;
CoSearchAgent&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#65292;&#21487;&#20316;&#20026;Slack&#25554;&#20214;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#25628;&#32034;&#25903;&#25345;&#22810;&#20010;&#29992;&#25143;&#20849;&#21516;&#23436;&#25104;&#29305;&#23450;&#30340;&#25628;&#32034;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#25554;&#20214;&#35774;&#35745;&#22312;&#21363;&#26102;&#36890;&#35759;&#24179;&#21488;&#20869;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#21327;&#20316;&#20064;&#24815;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#29992;&#25143;&#20132;&#20114;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#30740;&#31350;&#19981;&#24471;&#19981;&#20381;&#36182;&#20110;"&#21561;&#29275;&#22823;&#29579;"&#33539;&#20363;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#19982;&#29992;&#25143;&#33258;&#28982;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23454;&#29616;&#22797;&#26434;&#30340;&#20449;&#24687;&#25628;&#32034;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;CoSearchAgent&#65292;&#19968;&#31181;&#30001;LLM&#39537;&#21160;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#12290;CoSearchAgent&#34987;&#35774;&#35745;&#20026;Slack&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;&#35813;&#24179;&#21488;&#19978;&#30340;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped
&lt;/p&gt;</description></item><item><title>ExaRanker-Open &#26159;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;</title><link>https://arxiv.org/abs/2402.06334</link><description>&lt;p&gt;
ExaRanker-Open: &#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#21512;&#25104;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06334
&lt;/p&gt;
&lt;p&gt;
ExaRanker-Open &#26159;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ExaRanker&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;(IR)&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20316;&#20026;&#38468;&#21152;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;IR&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21021;&#22987;&#32467;&#26524;&#26159;&#22522;&#20110;&#19987;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#65292;&#36825;&#23548;&#33268;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#20854;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ExaRanker-Open&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#19981;&#21516;&#30340;LLMs&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#65292;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20063;&#26159;&#26377;&#20248;&#21183;&#30340;&#65292;ExaRanker&#30340;&#24615;&#33021;&#36229;&#36807;&#30446;&#26631;&#22522;&#32447;0
&lt;/p&gt;
&lt;p&gt;
ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#24182;&#23454;&#26045;&#20102;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#12289;K&#26368;&#36817;&#37051;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#23478;&#23621;&#35013;&#39280;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27979;&#35797;&#19981;&#21516;&#30340;&#35780;&#20272;&#25216;&#26415;&#26469;&#30740;&#31350;&#20854;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.06233</link><description>&lt;p&gt;
&#22312;&#23478;&#23621;&#35013;&#39280;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21327;&#21516;&#36807;&#28388;&#12289;K&#26368;&#36817;&#37051;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering, K-nearest neighbor and cosine similarity in home decor recommender systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#24182;&#23454;&#26045;&#20102;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#12289;K&#26368;&#36817;&#37051;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#23478;&#23621;&#35013;&#39280;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27979;&#35797;&#19981;&#21516;&#30340;&#35780;&#20272;&#25216;&#26415;&#26469;&#30740;&#31350;&#20854;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#20351;&#29992;K&#26368;&#36817;&#37051;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#20307;&#31995;&#32467;&#26500;&#26694;&#26550;&#34987;&#24320;&#21457;&#21644;&#23454;&#26045;&#65292;&#20197;&#28385;&#36275;DecorRaid&#20844;&#21496;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#30456;&#20851;&#30340;&#35780;&#20272;&#35270;&#35282;&#65292;&#21363;&#25968;&#25454;&#38598;&#12289;&#31995;&#32479;&#21644;&#29992;&#25143;&#35270;&#35282;&#12290;&#36890;&#36807;&#36825;&#20123;&#35270;&#35282;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#20102;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#36827;&#34892;&#20102;&#22312;&#32447;A/B&#20998;&#21106;&#27979;&#35797;&#65292;&#27604;&#36739;&#20102;&#23545;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#23567;&#35843;&#25972;&#30340;&#24615;&#33021;&#65292;&#24182;&#27979;&#35797;&#20102;&#35780;&#20272;&#25216;&#26415;&#30340;&#30456;&#20851;&#24615;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#35299;&#20915;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24314;&#35758;&#36890;&#36807;&#30740;&#31350;&#23558;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25216;&#26415;&#36827;&#34892;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
An architectural framework, based on collaborative filtering using K-nearest neighbor and cosine similarity, was developed and implemented to fit the requirements for the company DecorRaid. The aim of the paper is to test different evaluation techniques within the environment to research the recommender systems performance. Three perspectives were found relevant for evaluating a recommender system in the specific environment, namely dataset, system and user perspective. With these perspectives it was possible to gain a broader view of the recommender systems performance. Online A/B split testing was conducted to compare the performance of small adjustments to the RS and to test the relevance of the evaluation techniques. Key factors are solving the sparsity and cold start problem, where the suggestion is to research a hybrid RS combining Content-based and CF based techniques.
&lt;/p&gt;</description></item><item><title>ResumeFlow&#26159;&#19968;&#31181;&#21033;&#29992;LLM&#25216;&#26415;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#27714;&#32844;&#32773;&#26681;&#25454;&#29305;&#23450;&#30340;&#32844;&#20301;&#35201;&#27714;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#31616;&#21382;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#23450;&#21046;&#31616;&#21382;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06221</link><description>&lt;p&gt;
ResumeFlow: &#19968;&#31181;&#20010;&#24615;&#21270;&#31616;&#21382;&#29983;&#25104;&#21644;&#20462;&#35746;&#30340;LLM&#36741;&#21161;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06221
&lt;/p&gt;
&lt;p&gt;
ResumeFlow&#26159;&#19968;&#31181;&#21033;&#29992;LLM&#25216;&#26415;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#27714;&#32844;&#32773;&#26681;&#25454;&#29305;&#23450;&#30340;&#32844;&#20301;&#35201;&#27714;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#31616;&#21382;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#23450;&#21046;&#31616;&#21382;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#27714;&#32844;&#32773;&#26469;&#35828;&#65292;&#21046;&#20316;&#31526;&#21512;&#29305;&#23450;&#32844;&#20301;&#35201;&#27714;&#30340;&#29702;&#24819;&#31616;&#21382;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21021;&#20837;&#32844;&#22330;&#30340;&#27714;&#32844;&#32773;&#26469;&#35828;&#12290;&#34429;&#28982;&#24378;&#28872;&#24314;&#35758;&#27714;&#32844;&#32773;&#26681;&#25454;&#20182;&#20204;&#30003;&#35831;&#30340;&#20855;&#20307;&#32844;&#20301;&#23450;&#21046;&#31616;&#21382;&#65292;&#20294;&#25163;&#21160;&#26681;&#25454;&#24037;&#20316;&#25551;&#36848;&#21644;&#32844;&#20301;&#35201;&#27714;&#26469;&#23450;&#21046;&#31616;&#21382;&#36890;&#24120; (1) &#38750;&#24120;&#32791;&#26102;&#65292;&#19988; (2) &#23481;&#26131;&#20986;&#38169;&#12290;&#27492;&#22806;&#65292;&#22312;&#30003;&#35831;&#22810;&#20010;&#32844;&#20301;&#26102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23450;&#21046;&#27493;&#39588;&#21487;&#33021;&#23548;&#33268;&#32534;&#36753;&#31616;&#21382;&#36136;&#37327;&#19981;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ResumeFlow: &#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#65292;&#20351;&#32456;&#31471;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#35814;&#32454;&#30340;&#31616;&#21382;&#21644;&#25152;&#38656;&#30340;&#32844;&#20301;&#21457;&#24067;&#20449;&#24687;&#65292;&#23601;&#33021;&#22312;&#20960;&#31186;&#38047;&#20869;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#35813;&#29305;&#23450;&#32844;&#20301;&#21457;&#24067;&#30340;&#20010;&#24615;&#21270;&#31616;&#21382;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;......&#65289;
&lt;/p&gt;
&lt;p&gt;
Crafting the ideal, job-specific resume is a challenging task for many job applicants, especially for early-career applicants. While it is highly recommended that applicants tailor their resume to the specific role they are applying for, manually tailoring resumes to job descriptions and role-specific requirements is often (1) extremely time-consuming, and (2) prone to human errors. Furthermore, performing such a tailoring step at scale while applying to several roles may result in a lack of quality of the edited resumes. To tackle this problem, in this demo paper, we propose ResumeFlow: a Large Language Model (LLM) aided tool that enables an end user to simply provide their detailed resume and the desired job posting, and obtain a personalized resume specifically tailored to that specific job posting in the matter of a few seconds. Our proposed pipeline leverages the language understanding and information extraction capabilities of state-of-the-art LLMs such as OpenAI's GPT-4 and Goog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#35780;&#20272;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20132;&#21449;&#29109;&#25439;&#22833;&#24182;&#26367;&#20195;&#20256;&#32479;&#30340;&#28857;&#23545;/&#28857;&#23545;&#25439;&#22833;&#20989;&#25968;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25928;&#26524;&#24182;&#19981;&#20687;&#23459;&#31216;&#30340;&#37027;&#26679;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.06216</link><description>&lt;p&gt;
&#20844;&#24179;&#35780;&#20272;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20132;&#21449;&#29109;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Fairly Evaluating Large Language Model-based Recommendation Needs Revisit the Cross-Entropy Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#35780;&#20272;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20132;&#21449;&#29109;&#25439;&#22833;&#24182;&#26367;&#20195;&#20256;&#32479;&#30340;&#28857;&#23545;/&#28857;&#23545;&#25439;&#22833;&#20989;&#25968;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25928;&#26524;&#24182;&#19981;&#20687;&#23459;&#31216;&#30340;&#37027;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25512;&#33616;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65307;&#19968;&#20123;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#32463;&#36807;&#20840;softmax&#32454;&#35843;&#30340;LLM&#24050;&#32463;&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35266;&#28857;&#26469;&#33258;&#20110;&#20027;&#35266;&#21644;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#37492;&#20110;&#29616;&#23454;&#20013;&#30340;&#22823;&#37327;&#29289;&#21697;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#28857;&#23545;/&#28857;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26367;&#20195;&#26041;&#27861;&#20250;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#20302;&#20272;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#36807;&#39640;&#35780;&#20272;LLM&#30340;&#25490;&#24207;&#33021;&#21147;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20132;&#21449;&#29109;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#19968;&#20123;&#22522;&#26412;&#36817;&#20284;&#26041;&#27861;&#36827;&#34892;&#36866;&#24403;&#26367;&#20195;&#30340;&#24517;&#35201;&#20462;&#25913;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#33879;&#32467;&#26524;&#35777;&#23454;&#65292;&#21363;&#20351;&#20174;&#23454;&#38469;&#24847;&#20041;&#19978;&#35762;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25928;&#26524;&#24182;&#19981;&#20687;&#23459;&#31216;&#30340;&#37027;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained much attention in the recommendation community; some studies have observed that LLMs, fine-tuned by the cross-entropy loss with a full softmax, could achieve state-of-the-art performance already. However, these claims are drawn from unobjective and unfair comparisons. In view of the substantial quantity of items in reality, conventional recommenders typically adopt a pointwise/pairwise loss function instead for training. This substitute however causes severe performance degradation, leading to under-estimation of conventional methods and over-confidence in the ranking capability of LLMs.   In this work, we theoretically justify the superiority of cross-entropy, and showcase that it can be adequately replaced by some elementary approximations with certain necessary modifications. The remarkable results across three public datasets corroborate that even in a practical sense, existing LLM-based methods are not as effective as claimed for next-item 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24863;&#30693;&#34701;&#20837;&#20154;-ChatGPT&#20132;&#20114;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#25903;&#25345;&#24615;&#21151;&#33021;&#30340;&#24179;&#21488;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#31649;&#29702;&#26399;&#26395;&#12289;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#24182;&#22686;&#21152;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06170</link><description>&lt;p&gt;
&#20219;&#21153;&#25903;&#25345;&#21644;&#20010;&#24615;&#21270;&#30340;&#20154;&#26426;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;: &#19968;&#39033;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Task Supportive and Personalized Human-Large Language Model Interaction: A User Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24863;&#30693;&#34701;&#20837;&#20154;-ChatGPT&#20132;&#20114;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#25903;&#25345;&#24615;&#21151;&#33021;&#30340;&#24179;&#21488;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#31649;&#29702;&#26399;&#26395;&#12289;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#24182;&#22686;&#21152;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#65292;&#22914;ChatGPT&#65292;&#26159;&#22312;&#32447;&#20449;&#24687;&#33719;&#21462;&#65288;IS&#65289;&#21644;&#38382;&#39064;&#35299;&#20915;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#20173;&#28982;&#38754;&#20020;&#30528;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#30340;&#35748;&#30693;&#38556;&#30861;&#21644;&#20559;&#35265;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#20219;&#21153;&#23436;&#25104;&#12290;&#36825;&#20123;&#38382;&#39064;&#21453;&#26144;&#20102;&#20449;&#24687;&#33719;&#21462;&#21644;&#20132;&#20114;&#24335;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#25152;&#38754;&#20020;&#30340;&#26356;&#24191;&#27867;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24863;&#30693;&#34701;&#20837;&#21040;&#20154;-ChatGPT&#20132;&#20114;&#20013;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;ChatGPT&#30340;&#24179;&#21488;&#65292;&#38598;&#25104;&#20102;&#25903;&#25345;&#24615;&#21151;&#33021;&#65292;&#21253;&#25324;&#24863;&#30693;&#34920;&#36798;&#12289;&#25552;&#31034;&#24314;&#35758;&#21644;&#23545;&#35805;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25903;&#25345;&#21151;&#33021;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#31649;&#29702;&#26399;&#26395;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#26356;&#22909;&#22320;&#20248;&#21270;&#25552;&#31034;&#65292;&#24182;&#22686;&#21152;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#21319;&#20102;&#25105;&#20204;&#23545;&#20110;&#35774;&#35745;&#20027;&#21160;&#24615;&#21644;&#29992;&#25143;&#20013;&#24515;&#31995;&#32479;&#19982;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) applications, such as ChatGPT, are a powerful tool for online information-seeking (IS) and problem-solving tasks. However, users still face challenges initializing and refining prompts, and their cognitive barriers and biased perceptions further impede task completion. These issues reflect broader challenges identified within the fields of IS and interactive information retrieval (IIR). To address these, our approach integrates task context and user perceptions into human-ChatGPT interactions through prompt engineering. We developed a ChatGPT-like platform integrated with supportive functions, including perception articulation, prompt suggestion, and conversation explanation. Our findings of a user study demonstrate that the supportive functions help users manage expectations, reduce cognitive loads, better refine prompts, and increase user engagement. This research enhances our comprehension of designing proactive and user-centric systems with LLMs. It offer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#38646;&#21806;&#20013;&#24102;&#26377;&#36190;&#21161;&#20135;&#21697;&#30340;&#21697;&#31867;&#35268;&#21010;&#25361;&#25112;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#22312;&#32771;&#34385;&#36190;&#21161;&#20135;&#21697;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#39044;&#26399;&#25910;&#20837;&#30340;&#30446;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.06158</link><description>&lt;p&gt;
&#24102;&#26377;&#36190;&#21161;&#20135;&#21697;&#30340;&#21697;&#31867;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Assortment Planning with Sponsored Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#38646;&#21806;&#20013;&#24102;&#26377;&#36190;&#21161;&#20135;&#21697;&#30340;&#21697;&#31867;&#35268;&#21010;&#25361;&#25112;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#22312;&#32771;&#34385;&#36190;&#21161;&#20135;&#21697;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#39044;&#26399;&#25910;&#20837;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#21806;&#34892;&#19994;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#21697;&#31867;&#35268;&#21010;&#23545;&#20110;&#20225;&#19994;&#30340;&#25104;&#21151;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#36190;&#21161;&#20135;&#21697;&#22312;&#22312;&#32447;&#24066;&#22330;&#30340;&#26085;&#30410;&#31361;&#20986;&#22320;&#20301;&#65292;&#38646;&#21806;&#21830;&#22312;&#26377;&#25928;&#31649;&#29702;&#20135;&#21697;&#21697;&#31867;&#26041;&#38754;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;&#21697;&#31867;&#35268;&#21010;&#30740;&#31350;&#22823;&#22810;&#24573;&#35270;&#20102;&#36190;&#21161;&#20135;&#21697;&#30340;&#23384;&#22312;&#21450;&#20854;&#23545;&#25972;&#20307;&#25512;&#33616;&#25928;&#26524;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#36890;&#24120;&#31616;&#21270;&#22320;&#20551;&#35774;&#25152;&#26377;&#20135;&#21697;&#37117;&#26159;&#26377;&#26426;&#20135;&#21697;&#25110;&#38750;&#36190;&#21161;&#20135;&#21697;&#12290;&#36825;&#20010;&#30740;&#31350;&#31354;&#30333;&#31361;&#26174;&#20102;&#22312;&#36190;&#21161;&#20135;&#21697;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26356;&#28145;&#20837;&#25506;&#35752;&#21697;&#31867;&#35268;&#21010;&#25361;&#25112;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;&#23384;&#22312;&#36190;&#21161;&#20135;&#21697;&#30340;&#24773;&#20917;&#19979;&#23558;&#21697;&#31867;&#35268;&#21010;&#38382;&#39064;&#24314;&#27169;&#20026;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#35745;&#31639;&#20986;&#19968;&#31181;&#26368;&#20248;&#30340;&#21697;&#31867;&#35268;&#21010;&#26041;&#26696;&#65292;&#26082;&#33021;&#20248;&#21270;&#39044;&#26399;&#25910;&#20837;&#65292;&#21448;&#33021;&#32771;&#34385;&#21040;&#36190;&#21161;&#20135;&#21697;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of retail, assortment planning plays a crucial role in determining the success of a business. With the rise of sponsored products and their increasing prominence in online marketplaces, retailers face new challenges in effectively managing their product assortment in the presence of sponsored products. Remarkably, previous research in assortment planning largely overlooks the existence of sponsored products and their potential impact on overall recommendation effectiveness. Instead, they commonly make the simplifying assumption that all products are either organic or non-sponsored. This research gap underscores the necessity for a more thorough investigation of the assortment planning challenge when sponsored products are in play. We formulate the assortment planning problem in the presence of sponsored products as a combinatorial optimization task. The ultimate objective is to compute an assortment plan that optimizes expected revenue while considerin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#29983;&#25104;&#26381;&#35013;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.05941</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#19982;&#36890;&#36807;LLMs&#36827;&#34892;&#35270;&#35273;&#22686;&#24378;&#30340;&#39118;&#26684;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#29983;&#25104;&#26381;&#35013;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#29983;&#25104;&#38382;&#39064;&#28041;&#21450;&#26681;&#25454;&#29992;&#25143;&#30340;&#20852;&#36259;&#25512;&#33616;&#19968;&#20010;&#23436;&#25972;&#30340;&#26381;&#35013;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#38170;&#23450;&#21830;&#21697;&#25110;&#25351;&#23450;&#26597;&#35810;&#39118;&#26684;&#26469;&#25512;&#33616;&#29289;&#21697;&#65292;&#20294;&#19981;&#32771;&#34385;&#29992;&#25143;&#23545;&#30005;&#24433;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#20013;&#33879;&#21517;&#20154;&#29289;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#65288;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#65289;&#29983;&#25104;&#23436;&#25972;&#30340;&#26381;&#35013;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#65288;&#20363;&#22914;&#20154;&#29289;&#20449;&#24687;&#65289;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#20934;&#30830;&#29702;&#35299;&#29992;&#25143;&#30340;&#21916;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#65288;&#20107;&#23454;&#25110;&#21453;&#20107;&#23454;&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;LLMs&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The outfit generation problem involves recommending a complete outfit to a user based on their interests. Existing approaches focus on recommending items based on anchor items or specific query styles but do not consider customer interests in famous characters from movie, social media, etc. In this paper, we define a new Character-based Outfit Generation (COG) problem, designed to accurately interpret character information and generate complete outfit sets according to customer specifications such as age and gender. To tackle this problem, we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences. Additionally, we incorporate text-to-image models to enhance the visual understanding and generation (factual or counterfactual) of cohesive outfits. Our framework integrates LLMs with text-to-image models 
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#24314;&#35758;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#22270;&#20687;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26597;&#35810;&#24314;&#35758;&#65292;&#20174;&#32780;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#24847;&#22270;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04867</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26597;&#35810;&#24314;&#35758;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#24314;&#35758;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#22270;&#20687;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26597;&#35810;&#24314;&#35758;&#65292;&#20174;&#32780;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#24847;&#22270;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#30340;&#24555;&#36895;&#21457;&#23637;&#29615;&#22659;&#20013;&#65292;&#25628;&#32034;&#24341;&#25806;&#33268;&#21147;&#20110;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#30456;&#20851;&#24615;&#26356;&#24378;&#30340;&#32467;&#26524;&#12290;&#26597;&#35810;&#24314;&#35758;&#31995;&#32479;&#22312;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#21327;&#21161;&#29992;&#25143;&#21046;&#23450;&#26377;&#25928;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26597;&#35810;&#24314;&#35758;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#36755;&#20837;&#65292;&#21487;&#33021;&#38480;&#21046;&#29992;&#25143;&#23545;&#22270;&#20687;&#26597;&#35810;&#30340;&#20307;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#24314;&#35758;&#65288;MMQS&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#26597;&#35810;&#22270;&#20687;&#29983;&#25104;&#26597;&#35810;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25628;&#32034;&#32467;&#26524;&#30340;&#24847;&#22270;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RL4Sugg&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21147;&#37327;&#26469;&#20248;&#21270;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;RL4Sugg&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20339;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#33719;&#24471;&#20102;18%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;MMQS&#24050;&#32463;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach. Moreover, the MMQS has been transferred into real-world s
&lt;/p&gt;</description></item><item><title>&#22312;&#35831;&#27714;&#32423;&#21035;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#26041;&#27861;&#21644;&#22522;&#20110;&#29289;&#21697;&#32423;&#21035;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#22312;&#27169;&#25311;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#29289;&#21697;&#32423;&#21035;&#30340;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29289;&#21697;&#29305;&#24615;&#24182;&#20248;&#21270;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.16108</link><description>&lt;p&gt;
&#35831;&#27714;&#32423;&#21035;&#25512;&#33616;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Future Impact Decomposition in Request-level Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16108
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35831;&#27714;&#32423;&#21035;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#26041;&#27861;&#21644;&#22522;&#20110;&#29289;&#21697;&#32423;&#21035;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#22312;&#27169;&#25311;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#29289;&#21697;&#32423;&#21035;&#30340;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29289;&#21697;&#29305;&#24615;&#24182;&#20248;&#21270;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#22312;&#20248;&#21270;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#24207;&#21015;&#20197;&#25552;&#39640;&#38271;&#26399;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;&#31574;&#30053;&#30340;&#21160;&#20316;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#25512;&#33616;&#19968;&#32452;&#29289;&#21697;&#20197;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#29992;&#25143;&#30340;&#39057;&#32321;&#21644;&#36830;&#32493;&#30340;&#27983;&#35272;&#35831;&#27714;&#12290;&#22312;&#36825;&#31181;&#21015;&#34920;&#24335;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#29366;&#24577;&#22312;&#30456;&#24212;&#30340;MDP&#65288;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#34920;&#36848;&#20013;&#30340;&#27599;&#20010;&#35831;&#27714;&#19978;&#37117;&#20250;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35831;&#27714;&#32423;&#21035;&#30340;&#34920;&#36848;&#19982;&#29992;&#25143;&#30340;&#29289;&#21697;&#32423;&#21035;&#34892;&#20026;&#23454;&#36136;&#19978;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35831;&#27714;&#32423;&#21035;MDP&#19979;&#65292;&#22522;&#20110;&#29289;&#21697;&#32423;&#21035;&#30340;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29289;&#21697;&#29305;&#24615;&#24182;&#20248;&#21270;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#35831;&#27714;&#32423;&#21035;&#26041;&#27861;&#21644;&#25552;&#20986;&#30340;&#22522;&#20110;&#29289;&#21697;&#32423;&#21035;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#22312;&#27169;&#25311;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#30340;&#24615;&#33021;&#26469;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommender systems, reinforcement learning solutions have shown promising results in optimizing the interaction sequence between users and the system over the long-term performance. For practical reasons, the policy's actions are typically designed as recommending a list of items to handle users' frequent and continuous browsing requests more efficiently. In this list-wise recommendation scenario, the user state is updated upon every request in the corresponding MDP formulation. However, this request-level formulation is essentially inconsistent with the user's item-level behavior. In this study, we demonstrate that an item-level optimization approach can better utilize item characteristics and optimize the policy's performance even under the request-level MDP. We support this claim by comparing the performance of standard request-level methods with the proposed item-level actor-critic framework in both simulation and online experiments. Furthermore, we show that a reward-based fut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#36793;&#24179;&#21488;&#20013;&#21453;&#39304;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#23545;&#21334;&#23478;&#23454;&#39564;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25968;&#23398;&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#24182;&#23454;&#35777;&#35780;&#20272;&#20102;&#21453;&#20107;&#23454;&#20132;&#38169;&#35774;&#35745;&#65292;&#32467;&#26524;&#26174;&#31034;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2401.15811</link><description>&lt;p&gt;
&#20004;&#36793;&#24179;&#21488;&#20013;&#21453;&#39304;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#19979;&#30340;&#21334;&#23478;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Seller-Side Experiments under Interference Induced by Feedback Loops in Two-Sided Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#36793;&#24179;&#21488;&#20013;&#21453;&#39304;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#23545;&#21334;&#23478;&#23454;&#39564;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25968;&#23398;&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#24182;&#23454;&#35777;&#35780;&#20272;&#20102;&#21453;&#20107;&#23454;&#20132;&#38169;&#35774;&#35745;&#65292;&#32467;&#26524;&#26174;&#31034;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#36793;&#24179;&#21488;&#22312;&#29616;&#20195;&#21830;&#19994;&#21644;&#20869;&#23481;&#20998;&#20139;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#24120;&#20351;&#29992;A/B&#27979;&#35797;&#26469;&#24320;&#21457;&#26032;&#21151;&#33021;&#12290;&#23613;&#31649;&#29992;&#25143;&#31471;&#23454;&#39564;&#24456;&#24120;&#35265;&#65292;&#20294;&#23545;&#20110;&#29305;&#23450;&#24178;&#39044;&#21644;&#25351;&#26631;&#65292;&#21334;&#23478;&#31471;&#23454;&#39564;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#39304;&#24490;&#29615;&#24341;&#36215;&#30340;&#24178;&#25200;&#23545;&#20004;&#36793;&#24179;&#21488;&#21334;&#23478;&#23454;&#39564;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;\citet{ha2020counterfactual,nandy2021b}&#20013;&#25552;&#20986;&#30340;&#21453;&#20107;&#23454;&#20132;&#38169;&#35774;&#35745;&#12290;&#36825;&#20123;&#21453;&#39304;&#24490;&#29615;&#36890;&#24120;&#30001;&#33410;&#22863;&#31639;&#27861;&#20135;&#29983;&#65292;&#20250;&#23548;&#33268;&#26089;&#26399;&#20250;&#35805;&#30340;&#32467;&#26524;&#24433;&#21709;&#21518;&#32493;&#20250;&#35805;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#24314;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#31181;&#24178;&#25200;&#65292;&#29702;&#35770;&#20272;&#35745;&#20854;&#24433;&#21709;&#65292;&#24182;&#22312;&#23454;&#38469;&#24773;&#22659;&#20013;&#36827;&#34892;&#21453;&#20107;&#23454;&#20132;&#38169;&#35774;&#35745;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#23545;&#22788;&#29702;&#25928;&#26524;&#20570;&#20986;&#35823;&#23548;&#24615;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-sided platforms are central to modern commerce and content sharing and often utilize A/B testing for developing new features. While user-side experiments are common, seller-side experiments become crucial for specific interventions and metrics. This paper investigates the effects of interference caused by feedback loops on seller-side experiments in two-sided platforms, with a particular focus on the counterfactual interleaving design, proposed in \citet{ha2020counterfactual,nandy2021b}. These feedback loops, often generated by pacing algorithms, cause outcomes from earlier sessions to influence subsequent ones. This paper contributes by creating a mathematical framework to analyze this interference, theoretically estimating its impact, and conducting empirical evaluations of the counterfactual interleaving design in real-world scenarios. Our research shows that feedback loops can result in misleading conclusions about the treatment effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#29992;&#25143;&#21709;&#24212;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#27169;&#25311;&#31995;&#32479;&#35201;&#20040;&#21482;&#33021;&#23545;&#26159;&#38750;&#38382;&#39064;&#36827;&#34892;&#22238;&#31572;&#65292;&#35201;&#20040;&#26080;&#27861;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#29992;&#26356;&#23567;&#20294;&#20808;&#36827;&#30340;&#31995;&#32479;&#26367;&#25442;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07944</link><description>&lt;p&gt;
&#35770;&#29992;&#25143;&#21709;&#24212;&#27169;&#25311;&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An In-depth Investigation of User Response Simulation for Conversational Search. (arXiv:2304.07944v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#29992;&#25143;&#21709;&#24212;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#27169;&#25311;&#31995;&#32479;&#35201;&#20040;&#21482;&#33021;&#23545;&#26159;&#38750;&#38382;&#39064;&#36827;&#34892;&#22238;&#31572;&#65292;&#35201;&#20040;&#26080;&#27861;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#29992;&#26356;&#23567;&#20294;&#20808;&#36827;&#30340;&#31995;&#32479;&#26367;&#25442;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#36890;&#36807;&#22810;&#27425;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#28548;&#28165;&#21644;&#35299;&#20915;&#29992;&#25143;&#30340;&#25628;&#32034;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31995;&#32479;&#26159;&#36890;&#36807;&#35760;&#24405;&#25110;&#20154;&#24037;&#23545;&#35805;&#26085;&#24535;&#36827;&#34892;&#35757;&#32451;&#21644;&#28436;&#31034;&#30340;&#12290;&#26368;&#32456;&#65292;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#24212;&#35813;&#22312;&#26410;&#35265;&#36807;&#30340;&#23545;&#35805;&#36712;&#36857;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#26679;&#30340;&#31995;&#32479;&#37117;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#19981;&#21487;&#25193;&#23637;&#12290;&#20854;&#20013;&#19968;&#31181;&#31574;&#30053;&#26159;&#27169;&#25311;&#29992;&#25143;&#65292;&#20197;&#27492;&#26469;&#20943;&#23569;&#25193;&#23637;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#35201;&#20040;&#20165;&#38480;&#20110;&#23545;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#26159;&#38750;&#38382;&#39064;&#36827;&#34892;&#22238;&#31572;&#65292;&#35201;&#20040;&#26080;&#27861;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#29992;&#19968;&#20010;&#26356;&#23567;&#20294;&#20808;&#36827;&#30340;&#31995;&#32479;&#26469;&#26367;&#25442;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#31995;&#32479;&#65292;&#33021;&#22815;&#22823;&#24133;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve a user's search need through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy for this is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only respond to yes-no questions from the conversational search system, or unable to produce high quality responses in general.  In this paper, we show that current state-of-the-art user simulation system could be significantly improved by replacing it with a smaller but advanced
&lt;/p&gt;</description></item></channel></rss>