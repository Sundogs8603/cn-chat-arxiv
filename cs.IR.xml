<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PDRec&#30340;&#25554;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#23545;&#25152;&#26377;&#39033;&#30446;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02913</link><description>&lt;p&gt;
&#20026;&#39034;&#24207;&#25512;&#33616;&#35774;&#35745;&#30340;&#25554;&#20214;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Plug-in Diffusion Model for Sequential Recommendation. (arXiv:2401.02913v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PDRec&#30340;&#25554;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#23545;&#25152;&#26377;&#39033;&#30446;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#39537;&#24615;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#25506;&#32034;&#25512;&#33616;&#20013;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#32771;&#34385;&#21040;&#25512;&#33616;&#21644;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#38024;&#23545;&#25193;&#25955;&#21644;&#21453;&#21521;&#36807;&#31243;&#36827;&#34892;&#20102;&#23450;&#21046;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#35821;&#26009;&#24211;&#20013;&#26368;&#39640;&#20998;&#30340;&#39033;&#30446;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#23548;&#33268;&#24573;&#35270;&#20102;&#20854;&#20182;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#29992;&#25143;&#24191;&#20041;&#20559;&#22909;&#65292;&#20174;&#32780;&#20173;&#28982;&#21463;&#38480;&#20110;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25554;&#20214;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#65288;PDRec&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#28789;&#27963;&#30340;&#25554;&#20214;&#65292;&#20849;&#21516;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#30340;&#29992;&#25143;&#23545;&#25152;&#26377;&#39033;&#30446;&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PDRec&#39318;&#20808;&#36890;&#36807;&#26102;&#38388;&#38388;&#38548;&#25193;&#25955;&#27169;&#22411;&#25512;&#26029;&#29992;&#25143;&#23545;&#25152;&#26377;&#39033;&#30446;&#30340;&#21160;&#24577;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#34892;&#20026;&#37325;&#26032;&#21152;&#26435;&#65288;HBR&#65289;&#26426;&#21046;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#24191;&#20041;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pioneering efforts have verified the effectiveness of the diffusion models in exploring the informative uncertainty for recommendation. Considering the difference between recommendation and image synthesis tasks, existing methods have undertaken tailored refinements to the diffusion and reverse process. However, these approaches typically use the highest-score item in corpus for user interest prediction, leading to the ignorance of the user's generalized preference contained within other items, thereby remaining constrained by the data sparsity issue. To address this issue, this paper presents a novel Plug-in Diffusion Model for Recommendation (PDRec) framework, which employs the diffusion model as a flexible plugin to jointly take full advantage of the diffusion-generating user preferences on all items. Specifically, PDRec first infers the users' dynamic preferences on all items via a time-interval diffusion model and proposes a Historical Behavior Reweighting (HBR) mechanism to ident
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Deezer&#38899;&#20048;&#26381;&#21153;&#19978;&#20419;&#36827;&#26032;&#21457;&#24067;&#20869;&#23481;&#21457;&#29616;&#33021;&#21147;&#30340;&#26368;&#26032;&#20030;&#25514;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#25512;&#33616;&#12289;&#20919;&#21551;&#21160;&#23884;&#20837;&#21644;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31561;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20030;&#25514;&#22312;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#21644;&#26032;&#21457;&#24067;&#20869;&#23481;&#26333;&#20809;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02827</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#24320;&#22987;&#21543;&#65306;&#20419;&#36827;Deezer&#38899;&#20048;&#26381;&#21153;&#19978;&#26032;&#21457;&#24067;&#20869;&#23481;&#30340;&#21457;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Let's Get It Started: Fostering the Discoverability of New Releases on Deezer. (arXiv:2401.02827v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Deezer&#38899;&#20048;&#26381;&#21153;&#19978;&#20419;&#36827;&#26032;&#21457;&#24067;&#20869;&#23481;&#21457;&#29616;&#33021;&#21147;&#30340;&#26368;&#26032;&#20030;&#25514;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#25512;&#33616;&#12289;&#20919;&#21551;&#21160;&#23884;&#20837;&#21644;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31561;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20030;&#25514;&#22312;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#21644;&#26032;&#21457;&#24067;&#20869;&#23481;&#26333;&#20809;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26368;&#36817;&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;Deezer&#19978;&#20419;&#36827;&#26032;&#21457;&#24067;&#20869;&#23481;&#21457;&#29616;&#33021;&#21147;&#26041;&#38754;&#30340;&#20030;&#25514;&#12290;&#22312;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;&#26032;&#21457;&#24067;&#20869;&#23481;&#30340;&#25628;&#32034;&#21644;&#25512;&#33616;&#21151;&#33021;&#20043;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#20174;&#32534;&#36753;&#25512;&#33616;&#21521;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#36716;&#21464;&#65292;&#21253;&#25324;&#20351;&#29992;&#20919;&#21551;&#21160;&#23884;&#20837;&#21644;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#36716;&#21464;&#22312;&#25512;&#33616;&#36136;&#37327;&#21644;&#26032;&#21457;&#24067;&#20869;&#23481;&#30340;&#26333;&#20809;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our recent initiatives to foster the discoverability of new releases on the music streaming service Deezer. After introducing our search and recommendation features dedicated to new releases, we outline our shift from editorial to personalized release suggestions using cold start embeddings and contextual bandits. Backed by online experiments, we discuss the advantages of this shift in terms of recommendation quality and exposure of new releases on the service.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DocGraphLM&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#32534;&#30721;&#22120;&#26550;&#26500;&#34920;&#31034;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#37325;&#26500;&#25991;&#26723;&#22270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#22312;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.02823</link><description>&lt;p&gt;
DocGraphLM&#65306;&#20449;&#24687;&#25277;&#21462;&#30340;&#25991;&#26723;&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DocGraphLM&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#32534;&#30721;&#22120;&#26550;&#26500;&#34920;&#31034;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#37325;&#26500;&#25991;&#26723;&#22270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#22312;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#29702;&#35299;(VrDU)&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#20855;&#26377;&#22797;&#26434;&#24067;&#23616;&#30340;&#25991;&#26723;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#25104;&#20026;&#21487;&#33021;&#12290;&#20986;&#29616;&#20102;&#20004;&#31181;&#26550;&#26500;&#30340;&#27169;&#24335;-&#21463;LLM&#21551;&#21457;&#30340;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;DocGraphLM&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#35821;&#20041;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;1)&#19968;&#31181;&#32852;&#21512;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#34920;&#31034;&#25991;&#26723;&#65292;&#20197;&#21450;2)&#19968;&#31181;&#26032;&#39062;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#26469;&#37325;&#26500;&#25991;&#26723;&#22270;&#12290;DocGraphLM&#20351;&#29992;&#19968;&#20010;&#25910;&#25947;&#30340;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#39044;&#27979;&#33410;&#28857;&#20043;&#38388;&#30340;&#26041;&#21521;&#21644;&#36317;&#31163;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#20248;&#20808;&#32771;&#34385;&#37051;&#22495;&#24674;&#22797;&#24182;&#20943;&#36731;&#36828;&#31243;&#33410;&#28857;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#22312;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#20445;&#25345;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25253;&#21578;&#35828;&#65292;&#23613;&#31649;&#20165;&#30001;&#26500;&#24314;&#32780;&#26469;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged -- transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process during training, despite being solely constructe
&lt;/p&gt;</description></item></channel></rss>