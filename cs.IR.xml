<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#22914;&#20309;&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#20197;&#21450;&#22914;&#20309;&#24230;&#37327;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#19981;&#25512;&#33616;&#8221;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12256</link><description>&lt;p&gt;
&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#21644;&#24230;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#21709;&#24212;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders. (arXiv:2308.12256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#22914;&#20309;&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#20197;&#21450;&#22914;&#20309;&#24230;&#37327;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#19981;&#25512;&#33616;&#8221;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#21319;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#22312;&#24037;&#19994;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#23398;&#20064;&#29992;&#25143;&#30340;&#27491;&#38754;&#20852;&#36259;&#65292;&#20294;&#23545;&#20110;&#20174;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#21364;&#20184;&#20986;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#26159;&#29992;&#25143;&#25511;&#21046;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#24182;&#20276;&#38543;&#30528;&#23545;&#25512;&#33616;&#31995;&#32479;&#24212;&#35813;&#24555;&#36895;&#21709;&#24212;&#21644;&#20943;&#23569;&#31867;&#20284;&#25512;&#33616;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#36127;&#38754;&#21453;&#39304;&#20449;&#21495;&#22312;&#39034;&#24207;&#26816;&#32034;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#30340;&#27491;&#38754;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#19981;&#25512;&#33616;&#8221;&#25439;&#22833;&#20989;&#25968;&#23558;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#36127;&#38754;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#22312;&#26816;&#32034;&#38454;&#27573;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20248;&#21270;&#19981;&#25512;&#33616;&#24102;&#26377;&#36127;&#38754;&#21453;&#39304;&#30340;&#39033;&#30446;&#30340;&#23545;&#25968;&#20284;&#28982;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#23454;&#26102;&#23454;&#39564;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a "not-to-recommend" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system
&lt;/p&gt;</description></item><item><title>LLMRec&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#23545;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#39034;&#24207;&#25512;&#33616;&#21644;&#30452;&#25509;&#25512;&#33616;&#31561;&#20934;&#30830;&#24615;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#29087;&#32451;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12241</link><description>&lt;p&gt;
LLMRec: &#22312;&#25512;&#33616;&#20219;&#21153;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LLMRec: Benchmarking Large Language Models on Recommendation Task. (arXiv:2308.12241v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12241
&lt;/p&gt;
&lt;p&gt;
LLMRec&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#23545;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#39034;&#24207;&#25512;&#33616;&#21644;&#30452;&#25509;&#25512;&#33616;&#31561;&#20934;&#30830;&#24615;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#29087;&#32451;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25512;&#33616;&#39046;&#22495;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMRec&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#23545;&#22810;&#31181;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;LLMs&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#28909;&#38376;&#30340;&#29616;&#25104;LLMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;ChatGLM&#65292;&#28085;&#30422;&#20102;&#35780;&#20998;&#39044;&#27979;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#30452;&#25509;&#25512;&#33616;&#12289;&#35299;&#37322;&#29983;&#25104;&#21644;&#35780;&#35770;&#25688;&#35201;&#31561;&#20116;&#20010;&#25512;&#33616;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#25552;&#39640;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#22522;&#20110;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;&#39034;&#24207;&#25512;&#33616;&#21644;&#30452;&#25509;&#25512;&#33616;&#65289;&#19978;&#21482;&#34920;&#29616;&#20986;&#20013;&#31561;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25511;&#21046;&#33021;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#22686;&#24378;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12083</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#19981;&#20844;&#24179;&#24615;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23545;&#31574;&#65306;&#21453;&#20107;&#23454;&#22270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Graph Augmentation for Consumer Unfairness Mitigation in Recommender Systems. (arXiv:2308.12083v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#22686;&#24378;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#25104;&#20026;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#32771;&#34385;&#35282;&#24230;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#21333;&#29420;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#21521;&#28040;&#36153;&#32773;&#35299;&#37322;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#26576;&#20010;&#29289;&#21697;&#25110;&#32773;&#20943;&#36731;&#25512;&#33616;&#25928;&#29992;&#20013;&#30340;&#24046;&#24322;&#24433;&#21709;&#12290;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#26469;&#25552;&#20379;&#20851;&#20110;&#19981;&#20844;&#24179;&#24615;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#22686;&#24378;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#38598;&#21512;&#65292;&#20197;&#20415;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#21487;&#20197;&#24471;&#21040;&#26356;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#24314;&#27169;&#20026;&#20108;&#20998;&#22270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#26032;&#30340;&#29992;&#25143;-&#29289;&#21697;&#36793;&#26469;&#22686;&#24378;&#20108;&#20998;&#22270;&#65292;&#36825;&#20123;&#36793;&#19981;&#20165;&#21487;&#20197;&#35299;&#37322;&#21407;&#26469;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#20943;&#36731;&#19981;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#20844;&#24179;&#24615;&#21644;&#25512;&#33616;&#25928;&#29992;&#20043;&#38388;&#25214;&#21040;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#20943;&#36731;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation literature, explainability and fairness are becoming two prominent perspectives to consider. However, prior works have mostly addressed them separately, for instance by explaining to consumers why a certain item was recommended or mitigating disparate impacts in recommendation utility. None of them has leveraged explainability techniques to inform unfairness mitigation. In this paper, we propose an approach that relies on counterfactual explanations to augment the set of user-item interactions, such that using them while inferring recommendations leads to fairer outcomes. Modeling user-item interactions as a bipartite graph, our approach augments the latter by identifying new user-item edges that not only can explain the original unfairness by design, but can also mitigate it. Experiments on two public data sets show that our approach effectively leads to a better trade-off between fairness and recommendation utility compared with state-of-the-art mitigation procedure
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312; TREC 2022 &#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#30340;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#26041;&#27861;&#65292;&#22312;&#26816;&#32034;&#38454;&#27573;&#32467;&#21512;&#20102;&#20256;&#32479;&#31232;&#30095;&#26816;&#32034;&#21644;&#31070;&#32463;&#31264;&#23494;&#26816;&#32034;&#30340;&#20004;&#31181;&#32467;&#26500;&#65292;&#22312;&#25490;&#21517;&#38454;&#27573;&#26500;&#24314;&#20102;&#20840;&#20132;&#20114;&#24335;&#25490;&#21517;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#23376;&#25490;&#21517;&#27169;&#22359;&#65292;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12039</link><description>&lt;p&gt;
TREC 2022&#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#30340;&#28151;&#21512;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track. (arXiv:2308.12039v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312; TREC 2022 &#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#30340;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#26041;&#27861;&#65292;&#22312;&#26816;&#32034;&#38454;&#27573;&#32467;&#21512;&#20102;&#20256;&#32479;&#31232;&#30095;&#26816;&#32034;&#21644;&#31070;&#32463;&#31264;&#23494;&#26816;&#32034;&#30340;&#20004;&#31181;&#32467;&#26500;&#65292;&#22312;&#25490;&#21517;&#38454;&#27573;&#26500;&#24314;&#20102;&#20840;&#20132;&#20114;&#24335;&#25490;&#21517;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#23376;&#25490;&#21517;&#27169;&#22359;&#65292;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#22312;&#21508;&#31181;&#23454;&#38469;&#19994;&#21153;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;TREC 2022&#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#20013;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#20013;&#37319;&#29992;&#30340;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#26041;&#27861;&#12290;&#26816;&#32034;&#38454;&#27573;&#32467;&#21512;&#20102;&#20256;&#32479;&#31232;&#30095;&#26816;&#32034;&#21644;&#31070;&#32463;&#31264;&#23494;&#26816;&#32034;&#30340;&#20004;&#31181;&#32467;&#26500;&#12290;&#22312;&#25490;&#21517;&#38454;&#27573;&#65292;&#38500;&#20102;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20840;&#20132;&#20114;&#24335;&#25490;&#21517;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23376;&#25490;&#21517;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#26368;&#32456;&#30340;&#25991;&#26412;&#25490;&#21517;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27573;&#33853;&#25490;&#21517;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;&#31532;1&#21517;&#21644;&#31532;4&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#22797;&#26434;&#26032;&#38395;&#25991;&#26412;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25512;&#33616;&#32467;&#26524;&#21644;&#38750;&#27963;&#36291;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.12028</link><description>&lt;p&gt;
LKPNR: &#22522;&#20110;LLM&#21644;KG&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LKPNR: LLM and KG for Personalized News Recommendation Framework. (arXiv:2308.12028v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#22797;&#26434;&#26032;&#38395;&#25991;&#26412;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25512;&#33616;&#32467;&#26524;&#21644;&#38750;&#27963;&#36291;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#24456;&#38590;&#29702;&#35299;&#26032;&#38395;&#25991;&#26412;&#20013;&#22797;&#26434;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#23613;&#20154;&#24847;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#26041;&#27861;&#26356;&#36866;&#29992;&#20110;&#20855;&#26377;&#20016;&#23500;&#21382;&#21490;&#34892;&#20026;&#30340;&#27963;&#36291;&#29992;&#25143;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#38750;&#27963;&#36291;&#29992;&#25143;&#30340;&#8220;&#38271;&#23614;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32467;&#21512;&#36827;&#20256;&#32479;&#26041;&#27861;&#30340;&#35821;&#20041;&#34920;&#31034;&#30340;&#26032;&#22411;&#36890;&#29992;&#26694;&#26550;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#22797;&#26434;&#26032;&#38395;&#25991;&#26412;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#21253;&#21547;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#26032;&#38395;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#26032;&#38395;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;KG&#20013;&#30340;&#22810;&#20010;&#36339;&#25968;&#25366;&#25496;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the "long tail problem" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs' powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus allevia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#32463;&#27982;&#25512;&#33616;&#31995;&#32479;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#25512;&#33616;&#31995;&#32479;&#23545;&#32456;&#31471;&#29992;&#25143;&#30340;&#20215;&#20540;&#65292;&#20294;&#29616;&#23454;&#20013;&#25512;&#33616;&#31995;&#32479;&#20063;&#33021;&#30452;&#25509;&#29992;&#20110;&#23454;&#29616;&#32452;&#32455;&#30340;&#32463;&#27982;&#30446;&#26631;&#65292;&#20363;&#22914;&#36890;&#36807;&#32771;&#34385;&#20215;&#26684;&#24847;&#35782;&#21644;&#30408;&#21033;&#33021;&#21147;&#31561;&#22240;&#32032;&#26469;&#25913;&#21892;&#25512;&#33616;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.11998</link><description>&lt;p&gt;
&#32463;&#27982;&#25512;&#33616;&#31995;&#32479;--&#19968;&#39033;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Economic Recommender Systems -- A Systematic Review. (arXiv:2308.11998v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11998
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#32463;&#27982;&#25512;&#33616;&#31995;&#32479;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#25512;&#33616;&#31995;&#32479;&#23545;&#32456;&#31471;&#29992;&#25143;&#30340;&#20215;&#20540;&#65292;&#20294;&#29616;&#23454;&#20013;&#25512;&#33616;&#31995;&#32479;&#20063;&#33021;&#30452;&#25509;&#29992;&#20110;&#23454;&#29616;&#32452;&#32455;&#30340;&#32463;&#27982;&#30446;&#26631;&#65292;&#20363;&#22914;&#36890;&#36807;&#32771;&#34385;&#20215;&#26684;&#24847;&#35782;&#21644;&#30408;&#21033;&#33021;&#21147;&#31561;&#22240;&#32032;&#26469;&#25913;&#21892;&#25512;&#33616;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#35768;&#22810;&#22312;&#32447;&#26381;&#21153;&#37117;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#24120;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20363;&#22914;&#22312;&#20449;&#24687;&#36807;&#36733;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#30340;&#23398;&#26415;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#23545;&#32456;&#31471;&#29992;&#25143;&#30340;&#20215;&#20540;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#20551;&#35774;&#26159;&#36890;&#36807;&#25512;&#33616;&#25152;&#23454;&#29616;&#30340;&#25913;&#36827;&#26381;&#21153;&#23558;&#31215;&#26497;&#22320;&#24433;&#21709;&#32452;&#32455;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;&#36890;&#36807;&#22686;&#21152;&#23458;&#25143;&#20445;&#30041;&#29575;&#25110;&#24544;&#35802;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#29992;&#20110;&#23454;&#29616;&#32452;&#32455;&#30340;&#32463;&#27982;&#30446;&#26631;&#65292;&#36890;&#36807;&#23558;&#20215;&#26684;&#24847;&#35782;&#21644;&#30408;&#21033;&#33021;&#21147;&#31561;&#36135;&#24065;&#32771;&#34385;&#22240;&#32032;&#32435;&#20837;&#22522;&#26412;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#26041;&#27861;&#23545;&#29616;&#26377;&#20851;&#20110;&#25105;&#20204;&#25152;&#31216;&#30340;&#32463;&#27982;&#25512;&#33616;&#31995;&#32479;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;133&#20010;&#30456;&#20851;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of today's online services provide personalized recommendations to their users. Such recommendations are typically designed to serve certain user needs, e.g., to quickly find relevant content in situations of information overload. Correspondingly, the academic literature in the field largely focuses on the value of recommender systems for the end user. In this context, one underlying assumption is that the improved service that is achieved through the recommendations will in turn positively impact the organization's goals, e.g., in the form of higher customer retention or loyalty. However, in reality, recommender systems can be used to target organizational economic goals more directly by incorporating monetary considerations such as price awareness and profitability aspects into the underlying recommendation models. In this work, we survey the existing literature on what we call Economic Recommender Systems based on a systematic review approach that helped us identify 133 relevan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11884</link><description>&lt;p&gt;
&#23558;Wikidata&#20998;&#31867;&#20307;&#31995;&#38598;&#25104;&#21040;YAGO&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating the Wikidata Taxonomy into YAGO. (arXiv:2308.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikidata&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#36890;&#29992;&#30693;&#35782;&#24211;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#30340;&#21512;&#20316;&#24615;&#36136;&#65292;&#20854;&#27169;&#24335;&#21644;&#20998;&#31867;&#20307;&#31995;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;YAGO 4&#30693;&#35782;&#24211;&#20013;&#65292;&#25105;&#20204;&#23558;Wikidata&#19982;Schema.org&#30340;&#26412;&#20307;&#35770;&#32467;&#21512;&#36215;&#26469;&#65292;&#20943;&#23569;&#21644;&#28165;&#29702;&#20998;&#31867;&#20307;&#31995;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#25968;&#25454;&#19978;&#36816;&#34892;&#33258;&#21160;&#25512;&#29702;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#33293;&#24323;&#20102;&#22823;&#37096;&#20998;&#30340;Wikidata&#20998;&#31867;&#20307;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36923;&#36753;&#32422;&#26463;&#21644;&#31867;&#19982;&#23454;&#20363;&#30340;&#32454;&#33268;&#21306;&#20998;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21019;&#24314;&#20102;YAGO 4.5&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.
&lt;/p&gt;</description></item><item><title>CLIP Multi-modal Hashing (CLIPMH) is a new baseline method that improves the retrieval performance of multi-modal hashing by using the CLIP model to extract text and image features and fusing them to generate hash codes. Compared to state-of-the-art methods, CLIPMH significantly enhances performance (maximum increase of 8.38%) and has advantages over text and visual backbone networks.</title><link>http://arxiv.org/abs/2308.11797</link><description>&lt;p&gt;
CLIP&#22810;&#27169;&#21704;&#24076;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;CLIPMH
&lt;/p&gt;
&lt;p&gt;
CLIP Multi-modal Hashing: A new baseline CLIPMH. (arXiv:2308.11797v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11797
&lt;/p&gt;
&lt;p&gt;
CLIP Multi-modal Hashing (CLIPMH) is a new baseline method that improves the retrieval performance of multi-modal hashing by using the CLIP model to extract text and image features and fusing them to generate hash codes. Compared to state-of-the-art methods, CLIPMH significantly enhances performance (maximum increase of 8.38%) and has advantages over text and visual backbone networks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#21704;&#24076;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#26816;&#32034;&#20013;&#65292;&#21487;&#20197;&#23558;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#29983;&#25104;&#20108;&#36827;&#21046;&#21704;&#24076;&#30721;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22810;&#27169;&#26041;&#27861;&#23384;&#22312;&#26816;&#32034;&#31934;&#24230;&#20302;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#22312;&#20110;&#21508;&#20010;&#20027;&#24178;&#32593;&#32476;&#30340;&#29305;&#24449;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#26410;&#32463;&#36807;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#22810;&#27169;&#25968;&#25454;&#30340;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;CLIP&#22810;&#27169;&#21704;&#24076;&#65288;CLIPMH&#65289;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;CLIP&#27169;&#22411;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#28982;&#21518;&#34701;&#21512;&#29983;&#25104;&#21704;&#24076;&#30721;&#12290;CLIP&#25913;&#21892;&#20102;&#27599;&#20010;&#27169;&#24577;&#29305;&#24449;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22810;&#27169;&#21704;&#24076;&#26041;&#27861;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#22810;&#27169;&#21704;&#24076;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CLIPMH&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65288;&#26368;&#22823;&#22686;&#21152;8.38%&#65289;&#12290;CLIP&#36824;&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However, the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem, we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features, and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way, it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods, experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.38%). CLIP also has great advantages over the text and visual backbone networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Irl4Rec&#30340;&#26032;&#39062;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#21644;&#32771;&#34385;&#34394;&#20551;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#22312;&#27604;&#36739;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11728</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant representation learning for sequential recommendation. (arXiv:2308.11728v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Irl4Rec&#30340;&#26032;&#39062;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#21644;&#32771;&#34385;&#34394;&#20551;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#22312;&#27604;&#36739;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#28041;&#21450;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#29289;&#21697;&#24207;&#21015;&#33258;&#21160;&#25512;&#33616;&#19979;&#19968;&#20010;&#29289;&#21697;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;RNN&#25110;transformer&#26041;&#27861;&#20174;&#29289;&#21697;&#24207;&#21015;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#23545;&#29983;&#25104;&#27010;&#29575;&#65292;&#24182;&#25512;&#33616;&#21069;&#20960;&#20010;&#29289;&#21697;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#34394;&#20551;&#20851;&#31995;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#29305;&#21035;&#35299;&#20915;&#20102;&#36825;&#20123;&#34394;&#20551;&#20851;&#31995;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#31216;&#20026;Irl4Rec&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#34394;&#20551;&#21464;&#37327;&#21644;&#35843;&#25972;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#34394;&#20551;&#20851;&#31995;&#12290;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#19977;&#31181;&#20856;&#22411;&#26041;&#27861;&#65292;&#20984;&#26174;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#34394;&#20551;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation involves automatically recommending the next item to users based on their historical item sequence. While most prior research employs RNN or transformer methods to glean information from the item sequence-generating probabilities for each user-item pair and recommending the top items, these approaches often overlook the challenge posed by spurious relationships. This paper specifically addresses these spurious relations. We introduce a novel sequential recommendation framework named Irl4Rec. This framework harnesses invariant learning and employs a new objective that factors in the relationship between spurious variables and adjustment variables during model training. This approach aids in identifying spurious relations. Comparative analyses reveal that our framework outperforms three typical methods, underscoring the effectiveness of our model. Moreover, an ablation study further demonstrates the critical role our model plays in detecting spurious relations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#22270;&#32467;&#26500;&#20923;&#32467;&#21644;&#21435;&#22122;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22411;FREEDOM&#65292;&#23427;&#21516;&#26102;&#20923;&#32467;&#29289;&#21697;-&#29289;&#21697;&#22270;&#21644;&#21435;&#22122;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#65292;&#21462;&#24471;&#20102;&#30456;&#27604;LATTICE&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.06924</link><description>&lt;p&gt;
&#20004;&#20010;&#22270;&#30340;&#25925;&#20107;&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#22270;&#32467;&#26500;&#20923;&#32467;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
A Tale of Two Graphs: Freezing and Denoising Graph Structures for Multimodal Recommendation. (arXiv:2211.06924v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#22270;&#32467;&#26500;&#20923;&#32467;&#21644;&#21435;&#22122;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22411;FREEDOM&#65292;&#23427;&#21516;&#26102;&#20923;&#32467;&#29289;&#21697;-&#29289;&#21697;&#22270;&#21644;&#21435;&#22122;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#65292;&#21462;&#24471;&#20102;&#30456;&#27604;LATTICE&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#65289;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#27604;&#20165;&#22522;&#20110;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#19968;&#33324;&#25512;&#33616;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#21040;&#29289;&#21697;ID&#23884;&#20837;&#20013;&#65292;&#20197;&#20016;&#23500;&#29289;&#21697;&#34920;&#31034;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#28508;&#22312;&#30340;&#35821;&#20041;&#29289;&#21697;-&#29289;&#21697;&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;LATTICE&#25552;&#20986;&#20102;&#26174;&#24335;&#23398;&#20064;&#29289;&#21697;&#20043;&#38388;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;LATTICE&#30340;&#28508;&#22312;&#22270;&#32467;&#26500;&#23398;&#20064;&#26082;&#20302;&#25928;&#21448;&#19981;&#24517;&#35201;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#20923;&#32467;&#20854;&#29289;&#21697;-&#29289;&#21697;&#32467;&#26500;&#20063;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21517;&#20026;FREEDOM&#65292;&#23427;&#21516;&#26102;&#20923;&#32467;&#29289;&#21697;-&#29289;&#21697;&#22270;&#24182;&#21435;&#22122;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal recommender systems utilizing multimodal features (e.g., images and textual descriptions) typically show better recommendation accuracy than general recommendation models based solely on user-item interactions. Generally, prior work fuses multimodal features into item ID embeddings to enrich item representations, thus failing to capture the latent semantic item-item structures. In this context, LATTICE proposes to learn the latent structure between items explicitly and achieves state-of-the-art performance for multimodal recommendations. However, we argue the latent graph structure learning of LATTICE is both inefficient and unnecessary. Experimentally, we demonstrate that freezing its item-item structure before training can also achieve competitive performance. Based on this finding, we propose a simple yet effective model, dubbed as FREEDOM, that FREEzes the item-item graph and DenOises the user-item interaction graph simultaneously for Multimodal recommendation. Theoretic
&lt;/p&gt;</description></item></channel></rss>