<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>DeepSRGM&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Raga&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#22320;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.10168</link><description>&lt;p&gt;
DeepSRGM -- &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20013;&#30340;&#24207;&#21015;&#20998;&#31867;&#21644;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10168
&lt;/p&gt;
&lt;p&gt;
DeepSRGM&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Raga&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;arXiv:2402.10168v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#12299; &#25688;&#35201;&#65306;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;(ICM)&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;Raga&#65292;&#23427;&#20316;&#20026;&#20316;&#26354;&#21644;&#21363;&#20852;&#28436;&#22863;&#30340;&#26059;&#24459;&#26694;&#26550;&#12290;Raga&#30340;&#35782;&#21035;&#26159;ICM&#20013;&#19968;&#39033;&#37325;&#35201;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#38899;&#20048;&#25512;&#33616;&#21040;&#32452;&#32455;&#22823;&#22411;&#38899;&#20048;&#25910;&#34255;&#31561;&#22810;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#65292;&#20351;&#29992;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#37319;&#26679;&#33258;&#21407;&#22987;&#38899;&#39057;&#30340;&#36739;&#23567;&#24207;&#21015;&#19978;&#36827;&#34892;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#32780;&#26368;&#32456;&#30340;&#25512;&#29702;&#21017;&#26159;&#22312;&#25972;&#20010;&#38899;&#39057;&#19978;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Comp Music Carnatic&#25968;&#25454;&#38598;&#21644;&#20854;10&#20010;Raga&#23376;&#38598;&#19978;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#20351;&#20854;&#25104;&#20026;Raga&#35782;&#21035;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20351;&#24207;&#21015;&#25490;&#24207;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10168v1 Announce Type: cross  Abstract: A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence
&lt;/p&gt;</description></item><item><title>PICS&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#33258;&#21160;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24773;&#24863;&#20998;&#26512;&#26469;&#22686;&#24378;&#20803;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#24211;&#30340;&#25628;&#32034;&#25928;&#29575;&#21644;&#35775;&#38382;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10090</link><description>&lt;p&gt;
PICS: &#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
PICS: Pipeline for Image Captioning and Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10090
&lt;/p&gt;
&lt;p&gt;
PICS&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#33258;&#21160;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24773;&#24863;&#20998;&#26512;&#26469;&#22686;&#24378;&#20803;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#24211;&#30340;&#25628;&#32034;&#25928;&#29575;&#21644;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#30340;&#22686;&#38271;&#20351;&#24471;&#39640;&#25928;&#20998;&#31867;&#21644;&#26816;&#32034;&#30340;&#20808;&#36827;&#31995;&#32479;&#25104;&#20026;&#24517;&#38656;&#65292;&#36825;&#22312;&#25968;&#25454;&#24211;&#31649;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PICS&#65288;&#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#35299;&#20915;&#32452;&#32455;&#22823;&#35268;&#27169;&#22270;&#20687;&#24211;&#20013;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;PICS&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#33258;&#21160;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#36807;&#31243;&#65292;&#25552;&#20379;&#19968;&#20010;&#36229;&#36234;&#20256;&#32479;&#25163;&#21160;&#27880;&#37322;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35748;&#35782;&#65292;&#21363;&#26377;&#24847;&#20041;&#30340;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25551;&#36848;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#22270;&#20687;&#30340;&#21487;&#25628;&#32034;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#36890;&#36807;&#23558;&#24773;&#24863;&#20998;&#26512;&#25972;&#21512;&#21040;&#27969;&#27700;&#32447;&#20013;&#65292;PICS&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20803;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#22522;&#26412;&#25551;&#36848;&#31526;&#30340;&#32454;&#33268;&#25628;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#31649;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10090v1 Announce Type: cross  Abstract: The growing volume of digital images necessitates advanced systems for efficient categorization and retrieval, presenting a significant challenge in database management and information retrieval. This paper introduces PICS (Pipeline for Image Captioning and Search), a novel approach designed to address the complexities inherent in organizing large-scale image repositories. PICS leverages the advancements in Large Language Models (LLMs) to automate the process of image captioning, offering a solution that transcends traditional manual annotation methods. The approach is rooted in the understanding that meaningful, AI-generated captions can significantly enhance the searchability and accessibility of images in large databases. By integrating sentiment analysis into the pipeline, PICS further enriches the metadata, enabling nuanced searches that extend beyond basic descriptors. This methodology not only simplifies the task of managing vas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10024</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#23545;&#20110;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Augmented In-Context Learning for Unsupervised Word Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#35774;&#32622;&#20013;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#35789;&#27719;&#32763;&#35793;&#21644;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;(BLI)&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27809;&#26377;&#31181;&#23376;&#32763;&#35793;&#23545;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#8220;&#20256;&#32479;&#8221;&#30340;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861; (SAIL) &#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65306;&#20174;&#38646;&#26679;&#26412;&#25552;&#31034;&#24320;&#22987;&#65292;SAIL&#36890;&#36807;&#36845;&#20195;&#22320;&#20174;LLM&#20013;&#24341;&#20986;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#30340;&#35789;&#27719;&#32763;&#35793;&#23545;&#65292;&#28982;&#21518;&#22312;ICL&#30340;&#26041;&#24335;&#19979;&#20877;&#27425;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#30340;BLI&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;LLM&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20063;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26144;&#23556;&#30340;&#22522;&#32447;&#12290;&#38500;&#20102;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#24615;&#33021;&#19981;&#24179;&#34913;&#21644;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.09959</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLM-based Federated Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09959
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#24615;&#33021;&#19981;&#24179;&#34913;&#21644;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#23637;&#31034;&#20102;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20855;&#22791;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#38656;&#35201;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#65292;&#36825;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#21253;&#21547;&#20102;&#25935;&#24863;&#29992;&#25143;&#20449;&#24687;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#24847;&#22806;&#27844;&#38706;&#21487;&#33021;&#20405;&#29359;&#25968;&#25454;&#20445;&#25252;&#27861;&#65292;&#24182;&#24341;&#21457;&#20262;&#29702;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#65288;Fed4Rec&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;Fed4Rec&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#23458;&#25143;&#31471;&#24615;&#33021;&#19981;&#24179;&#34913;&#21152;&#21095;&#65292;&#24433;&#21709;&#31995;&#32479;&#30340;&#25928;&#29575;&#65307;&#20854;&#27425;&#65292;&#23545;&#20110;&#26412;&#22320;&#35757;&#32451;&#21644;&#25512;&#29702;LLM&#65292;&#23545;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09959v1 Announce Type: new  Abstract: Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.09939</link><description>&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#26368;&#26032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Construction Industry: A State-of-the-art Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#26159;&#20840;&#29699;&#32463;&#27982;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#20010;&#37096;&#38376;&#65292;&#20294;&#22312;&#35774;&#35745;&#12289;&#35268;&#21010;&#12289;&#37319;&#36141;&#12289;&#26816;&#26597;&#21644;&#32500;&#25252;&#31561;&#21508;&#20010;&#29615;&#33410;&#20013;&#38754;&#20020;&#30528;&#35768;&#22810;&#29983;&#20135;&#21147;&#25361;&#25112;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#22522;&#20110;&#26576;&#20123;&#36755;&#20837;&#25110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21019;&#36896;&#26032;&#39062;&#19988;&#36924;&#30495;&#30340;&#25968;&#25454;&#25110;&#20869;&#23481;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#20195;&#30721;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#21644;&#39072;&#35206;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#20110;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#25991;&#29486;&#20013;&#23384;&#22312;&#30528;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#24314;&#31569;&#39046;&#22495;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#32570;&#65292;&#30740;&#31350;&#30446;&#26631;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23545;&#24314;&#31569;&#34892;&#19994;&#29616;&#26377;&#21644;&#26032;&#20852;&#30340;&#29983;&#25104;&#24335;AI&#26426;&#36935;&#21644;&#25361;&#25112;&#36827;&#34892;&#22238;&#39038;&#21644;&#20998;&#31867;&#65307;&#65288;2&#65289;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#21033;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#21644;&#38656;&#27714;&#26500;&#24314;&#23450;&#21046;&#21270;&#30340;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09939v1 Announce Type: new  Abstract: The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their ow
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.09784</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#25509;&#36817;&#24230;&#19978;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#25429;&#25417;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#21333;&#21521;&#21644;&#21452;&#21521;&#27169;&#24335;&#65292;&#20294;&#23545;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#22914;&#20010;&#20307;&#34892;&#20026;&#21644;&#31038;&#20250;&#36235;&#21183;&#27169;&#24335;&#65292;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#31867;&#20284;&#30340;&#26102;&#38388;&#27573;&#20869;&#38544;&#21547;&#22312;&#29992;&#25143;&#20043;&#38388;&#21457;&#29983;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22402;&#30452;&#26102;&#38388;&#25509;&#36817;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#36866;&#24212;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#36866;&#24212;&#22312;&#32771;&#34385;&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#20363;&#22914;&#21306;&#20998;&#22312;&#19968;&#21608;&#20869;&#19982;&#19968;&#20010;&#26376;&#20869;&#36141;&#20080;&#30340;&#36830;&#32493;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2402.09766</link><description>&lt;p&gt;
&#20174;&#21464;&#21160;&#24615;&#21040;&#31283;&#23450;&#24615;&#65306;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#21270;&#23454;&#36341;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
From Variability to Stability: Advancing RecSys Benchmarking Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26032;&#30340;&#31639;&#27861;&#32463;&#24120;&#36890;&#36807;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#20219;&#24847;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#22768;&#31216;&#33258;&#24049;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#21453;&#26144;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#36827;&#35780;&#20272;&#23454;&#36341;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;&#26412;&#25991;&#20171;&#32461;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;30&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;9&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;11&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23558;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#32858;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#25490;&#21517;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09766v1 Announce Type: cross  Abstract: In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;RAG&#65289;&#30340;&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32469;&#36807;&#25991;&#26412;&#20999;&#20998;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#22320;&#35821;&#22659;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#35821;&#20041;&#36830;&#36143;&#24615;&#30772;&#22351;&#21644;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09760</link><description>&lt;p&gt;
&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411; grounding
&lt;/p&gt;
&lt;p&gt;
Grounding Language Model with Chunking-Free In-Context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;RAG&#65289;&#30340;&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32469;&#36807;&#25991;&#26412;&#20999;&#20998;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#22320;&#35821;&#22659;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#35821;&#20041;&#36830;&#36143;&#24615;&#30772;&#22351;&#21644;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26080;&#22359;&#35821;&#22659;&#65288;CFIC&#65289;&#26816;&#32034;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;RAG&#31995;&#32479;&#22312;&#20351;&#29992;&#31934;&#30830;&#35777;&#25454;&#25991;&#26412;&#36827;&#34892; grounding &#26102;&#24448;&#24448;&#38754;&#20020;&#22788;&#29702;&#20887;&#38271;&#25991;&#26723;&#21644;&#36807;&#28388;&#26080;&#20851;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#25991;&#26723;&#20999;&#20998;&#21644;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20197;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#30772;&#22351;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#35299;&#20915;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;CFIC&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#20999;&#20998;&#36807;&#31243;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#25991;&#26723;&#30340;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35821;&#22659;&#26816;&#32034;&#65292;&#22312;&#23545;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#33258;&#22238;&#24402;&#35299;&#30721;&#26102;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#25152;&#38656;&#30340;&#20855;&#20307;&#35777;&#25454;&#25991;&#26412;&#65292;&#28040;&#38500;&#20102;&#20999;&#20998;&#30340;&#38656;&#27714;&#12290;CFIC &#36827;&#19968;&#27493;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09760v1 Announce Type: cross  Abstract: This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.   CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further
&lt;/p&gt;</description></item><item><title>POBEVM &#26159;&#19968;&#31181;&#23454;&#26102;&#35270;&#39057;&#25248;&#20687;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#30446;&#26631;&#20027;&#20307;&#21644;&#36793;&#32536;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#22788;&#29702;&#30446;&#26631;&#36793;&#32536;&#30340;&#27169;&#31946;&#21644;&#19981;&#27491;&#30830;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20462;&#21098;&#22270;&#30340;&#35270;&#39057;&#25248;&#20687;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#30446;&#26631;&#36793;&#32536;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09731</link><description>&lt;p&gt;
POBEVM: &#23454;&#26102;&#35270;&#39057;&#25248;&#20687;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#30446;&#26631;&#20027;&#20307;&#21644;&#36793;&#32536;
&lt;/p&gt;
&lt;p&gt;
POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09731
&lt;/p&gt;
&lt;p&gt;
POBEVM &#26159;&#19968;&#31181;&#23454;&#26102;&#35270;&#39057;&#25248;&#20687;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#30446;&#26631;&#20027;&#20307;&#21644;&#36793;&#32536;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#22788;&#29702;&#30446;&#26631;&#36793;&#32536;&#30340;&#27169;&#31946;&#21644;&#19981;&#27491;&#30830;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20462;&#21098;&#22270;&#30340;&#35270;&#39057;&#25248;&#20687;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#30446;&#26631;&#36793;&#32536;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#25248;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20026;&#30446;&#26631;&#20027;&#20307;&#20135;&#29983;&#20934;&#30830;&#30340; alpha &#20272;&#35745;&#65292;&#20294;&#36890;&#24120;&#20250;&#29983;&#25104;&#27169;&#31946;&#25110;&#19981;&#27491;&#30830;&#30340;&#30446;&#26631;&#36793;&#32536;&#12290;&#36825;&#36890;&#24120;&#26159;&#30001;&#20197;&#19979;&#21407;&#22240;&#24341;&#36215;&#30340;&#65306;1&#65289;&#24403;&#21069;&#30340;&#26041;&#27861;&#24635;&#26159;&#19981;&#21152;&#21306;&#20998;&#22320;&#22788;&#29702;&#30446;&#26631;&#20027;&#20307;&#21644;&#36793;&#32536;&#65307;2&#65289;&#30446;&#26631;&#20027;&#20307;&#22312;&#25972;&#20010;&#30446;&#26631;&#20013;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#21482;&#26377;&#24456;&#23567;&#30340;&#27604;&#20363;&#26159;&#30446;&#26631;&#36793;&#32536;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#27169;&#22359;&#65292;&#20998;&#21035;&#20248;&#21270;&#25248;&#20687;&#30340;&#30446;&#26631;&#20027;&#20307;&#21644;&#36793;&#32536;&#65288;SOBE&#65289;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#26102;&#12289;&#26080;&#38656;&#20462;&#21098;&#22270;&#30340;&#35270;&#39057;&#25248;&#20687;&#26041;&#27861;&#8212;&#8212;POBEVM&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#23427;&#26356;&#36731;&#37327;&#32423;&#65292;&#24182;&#22312;&#39044;&#27979;&#30340;&#30446;&#26631;&#36793;&#32536;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#38024;&#23545;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#32536;L1&#25439;&#22833;&#65288;ELL&#65289;&#20989;&#25968;&#65292;&#24378;&#21046;&#23558;&#25105;&#20204;&#30340;&#32593;&#32476;&#24212;&#29992;&#20110;&#25248;&#20687;&#30340;&#30446;&#26631;&#20027;&#20307;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09731v1 Announce Type: cross  Abstract: Deep convolutional neural networks (CNNs) based approaches have achieved great performance in video matting. Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges. This is usually caused by the following reasons: 1) The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge. For the first problem, we propose a CNN-based module that separately optimizes the matting target body and edge (SOBE). And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge. For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09660</link><description>&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
User Modeling and User Profiling: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24050;&#32463;&#20419;&#20351;&#20808;&#36827;&#30340;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22522;&#20110;&#19982;&#36825;&#20123;&#31995;&#32479;&#30340;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#26412;&#25991;&#23545;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21382;&#21490;&#27010;&#36848;&#65292;&#36861;&#28335;&#20102;&#20174;&#26089;&#26399;&#30340;&#21051;&#26495;&#27169;&#22411;&#21040;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#25152;&#26377;&#27963;&#21160;&#20027;&#39064;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#31361;&#20986;&#20102;&#21521;&#26356;&#22797;&#26434;&#30340;&#29992;&#25143;&#30011;&#20687;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#38544;&#24335;&#25968;&#25454;&#25910;&#38598;&#12289;&#22810;&#34892;&#20026;&#24314;&#27169;&#20197;&#21450;&#22270;&#25968;&#25454;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLM&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#25366;&#25496;&#25928;&#29575;&#21644;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#26469;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#65292;&#24182;&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.09617</link><description>&lt;p&gt;
&#22686;&#24378;LLM&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65306;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLM&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#25366;&#25496;&#25928;&#29575;&#21644;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#26469;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#65292;&#24182;&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#19981;&#20165;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#26684;&#23616;&#65292;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21331;&#36234;&#24212;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25366;&#25496;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#20851;&#31995;&#25366;&#25496;&#26041;&#38754;&#26377;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#31471;&#30740;&#31350;&#23578;&#26410;&#26377;&#25928;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#22312;&#22270;&#20851;&#31995;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#21644;&#33021;&#21147;&#21463;&#38480;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;LLM&#26080;&#27861;&#28145;&#20837;&#21033;&#29992;&#22270;&#20013;&#30340;&#36793;&#32536;&#20449;&#24687;&#65292;&#32780;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#24046;&#36317;&#38480;&#21046;&#20102;LLM&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#30340;&#28508;&#21147;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#22270;&#30340;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09617v1 Announce Type: new  Abstract: The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lite-LLM4Rec&#30340;&#31616;&#21270;&#30340;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;Lite-LLM4Rec&#36890;&#36807;&#20351;&#29992;&#30452;&#25509;&#39033;&#30446;&#25237;&#24433;&#22836;&#26469;&#29983;&#25104;&#25490;&#21517;&#20998;&#25968;&#65292;&#36991;&#20813;&#20102;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25130;&#26029;&#26426;&#21046;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09543</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Large Language Model Architectures for Sequential Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lite-LLM4Rec&#30340;&#31616;&#21270;&#30340;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;Lite-LLM4Rec&#36890;&#36807;&#20351;&#29992;&#30452;&#25509;&#39033;&#30446;&#25237;&#24433;&#22836;&#26469;&#29983;&#25104;&#25490;&#21517;&#20998;&#25968;&#65292;&#36991;&#20813;&#20102;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25130;&#26029;&#26426;&#21046;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39034;&#24207;&#25512;&#33616;&#24050;&#32463;&#34987;&#36866;&#24212;&#21040;&#20102;LLM&#33539;&#24335;&#20013;&#65292;&#20197;&#20139;&#21463;LLM&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#25512;&#33616;&#20449;&#24687;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#25512;&#29702;&#30340;&#24040;&#22823;&#35745;&#31639;&#24320;&#38144;&#23545;&#20110;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#31616;&#21270;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#27169;&#22411;Lite-LLM4Rec&#12290;Lite-LLM4Rec&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23454;&#29616;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;Lite-LLM4Rec&#36890;&#36807;&#20351;&#29992;&#30452;&#25509;&#39033;&#30446;&#25237;&#24433;&#22836;&#26469;&#29983;&#25104;&#25490;&#21517;&#20998;&#25968;&#65292;&#36991;&#20813;&#20102;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;&#36825;&#20010;&#35774;&#35745;&#28304;&#20110;&#25105;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#23545;&#20110;&#39034;&#24207;&#25512;&#33616;&#23454;&#38469;&#19978;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;Lite-LLM4Rec&#36824;&#24341;&#20837;&#20102;&#35299;&#30721;&#26102;&#38271;&#30340;&#33258;&#36866;&#24212;&#25130;&#26029;&#26426;&#21046;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09543v1 Announce Type: new  Abstract: Recently, sequential recommendation has been adapted to the LLM paradigm to enjoy the power of LLMs. LLM-based methods usually formulate recommendation information into natural language and the model is trained to predict the next item in an auto-regressive manner. Despite their notable success, the substantial computational overhead of inference poses a significant obstacle to their real-world applicability. In this work, we endeavor to streamline existing LLM-based recommendation models and propose a simple yet highly effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential recommendation task. Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation. This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential recommendations. Additionally, Lite-LLM4Rec introd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02816</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intersectional Two-sided Fairness in Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26681;&#25454;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21487;&#20998;&#20026;&#29992;&#25143;&#20844;&#24179;&#24615;&#12289;&#29289;&#21697;&#20844;&#24179;&#24615;&#21644;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#21644;&#29289;&#21697;&#20844;&#24179;&#24615;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#25512;&#33616;&#31995;&#32479;&#26159;&#21452;&#36793;&#20844;&#24179;&#30340;&#65292;&#20132;&#21449;&#21452;&#36793;&#19981;&#20844;&#24179;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#65292;&#36825;&#22312;&#26412;&#25991;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35266;&#23519;&#21644;&#23637;&#31034;&#65292;&#24182;&#19988;&#20197;&#21069;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#26469;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#26469;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#35843;&#25972;&#27491;&#38754;&#39044;&#27979;&#24471;&#20998;&#65292;&#20197;&#20844;&#24179;&#22320;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20256;&#32479;&#30340;&#20113;&#25512;&#33616;&#31995;&#32479;&#65288;CloudRSs&#65289;&#19982;&#22522;&#20110;&#35774;&#22791;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;DeviceRSs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#21644;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#23384;&#20648;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#21147;&#26469;&#35299;&#20915;&#36164;&#28304;&#28040;&#32791;&#12289;&#21709;&#24212;&#24310;&#36831;&#20197;&#21450;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.11441</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On-Device Recommender Systems: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20256;&#32479;&#30340;&#20113;&#25512;&#33616;&#31995;&#32479;&#65288;CloudRSs&#65289;&#19982;&#22522;&#20110;&#35774;&#22791;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;DeviceRSs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#21644;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#23384;&#20648;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#21147;&#26469;&#35299;&#20915;&#36164;&#28304;&#28040;&#32791;&#12289;&#21709;&#24212;&#24310;&#36831;&#20197;&#21450;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#37096;&#32626;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#20174;&#28023;&#37327;&#20449;&#24687;&#20013;&#25214;&#21040;&#24863;&#20852;&#36259;&#30340;&#20869;&#23481;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22312;&#20113;&#25968;&#25454;&#20013;&#24515;&#25910;&#38598;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#38598;&#20013;&#24335;&#27169;&#22411;&#26469;&#36827;&#34892;&#25512;&#33616;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#20113;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;CloudRSs&#65289;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#36807;&#22810;&#30340;&#36164;&#28304;&#28040;&#32791;&#12289;&#21709;&#24212;&#24310;&#36831;&#20197;&#21450;&#28041;&#21450;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#36793;&#32536;&#35774;&#22791;&#23384;&#20648;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#36827;&#27493;&#25512;&#21160;&#65292;&#20154;&#20204;&#36880;&#28176;&#20174;CloudRSs&#36716;&#21521;&#20102;&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;DeviceRSs&#65289;&#65292;&#36825;&#20123;&#31995;&#32479;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;&#38598;&#20013;&#24335;&#25968;&#25454;&#23384;&#20648;&#30340;&#35201;&#27714;&#65292;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#24102;&#26469;&#30340;&#21709;&#24212;&#24310;&#36831;&#65292;&#24182;&#36890;&#36807;&#26412;&#22320;&#21270;&#30340;&#26041;&#24335;&#22686;&#24378;&#29992;&#25143;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11441v2 Announce Type: replace  Abstract: Recommender systems have been widely deployed in various real-world applications to help users identify content of interest from massive amounts of information. Traditional recommender systems work by collecting user-item interaction data in a cloud-based data center and training a centralized model to perform the recommendation service. However, such cloud-based recommender systems (CloudRSs) inevitably suffer from excessive resource consumption, response latency, as well as privacy and security risks concerning both data and models. Recently, driven by the advances in storage, communication, and computation capabilities of edge devices, there has been a shift of focus from CloudRSs to on-device recommender systems (DeviceRSs), which leverage the capabilities of edge devices to minimize centralized data storage requirements, reduce the response latency caused by communication overheads, and enhance user privacy and security by local
&lt;/p&gt;</description></item><item><title>&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.02552</link><description>&lt;p&gt;
&#24403;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#36935;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#25143;&#27169;&#25311;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. (arXiv:2306.02552v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02552
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36275;&#22815;&#21644;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#19968;&#30452;&#26159;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20027;&#35266;&#21644;&#22797;&#26434;&#24615;&#36136;&#65292;&#21487;&#38752;&#22320;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#23637;&#31034;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20026;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#20250;&#65292;&#24182;&#26377;&#21487;&#33021;&#25913;&#21464;&#20256;&#32479;&#30340;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#25506;&#32034;&#20351;&#29992;LLM&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#29992;&#25143;&#35270;&#20026;&#22522;&#20110;LLM&#30340;&#33258;&#27835;&#26234;&#33021;&#20307;&#65292;&#24182;&#35753;&#19981;&#21516;&#26234;&#33021;&#20307;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#33258;&#30001;&#20132;&#27969;&#12289;&#34892;&#20026;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a vir
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#24515;&#29702;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#26469;&#27169;&#25311;&#21644;&#39044;&#27979;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#38899;&#20048;&#31867;&#22411;&#20559;&#22909;&#12290;&#36890;&#36807;&#20998;&#26512;&#36229;&#36807;&#21313;&#20159;&#26465;&#38899;&#20048;&#25910;&#21548;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2003.10699</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#27169;&#25311;&#38899;&#20048;&#31867;&#22411;&#20559;&#22909;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Utilizing Human Memory Processes to Model Genre Preferences for Personalized Music Recommendations. (arXiv:2003.10699v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#24515;&#29702;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#26469;&#27169;&#25311;&#21644;&#39044;&#27979;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#38899;&#20048;&#31867;&#22411;&#20559;&#22909;&#12290;&#36890;&#36807;&#20998;&#26512;&#36229;&#36807;&#21313;&#20159;&#26465;&#38899;&#20048;&#25910;&#21548;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#26469;&#27169;&#25311;&#21644;&#39044;&#27979;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#38899;&#20048;&#31867;&#22411;&#20559;&#22909;&#12290;&#36825;&#20123;&#36807;&#31243;&#25551;&#36848;&#20102;&#20154;&#31867;&#22914;&#20309;&#36890;&#36807;&#32771;&#34385;&#36807;&#21435;&#20351;&#29992;&#39057;&#29575;&#12289;&#36807;&#21435;&#20351;&#29992;&#26102;&#25928;&#20197;&#21450;&#24403;&#21069;&#19978;&#19979;&#25991;&#26469;&#35775;&#38382;&#20854;&#35760;&#24518;&#20013;&#30340;&#20449;&#24687;&#21333;&#20803;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#36229;&#36807;&#21313;&#20159;&#26465;&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;Last.fm&#19978;&#20998;&#20139;&#30340;&#38899;&#20048;&#25910;&#21548;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#25152;&#26377;&#35780;&#20272;&#30340;&#29992;&#25143;&#32676;&#20307;&#65292;&#21363;&#20302;&#20027;&#27969;&#38899;&#20048;&#21548;&#20247;&#12289;&#20013;&#31561;&#20027;&#27969;&#38899;&#20048;&#21548;&#20247;&#21644;&#39640;&#20027;&#27969;&#38899;&#20048;&#21548;&#20247;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#24515;&#29702;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#35745;&#31639;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a psychology-inspired approach to model and predict the music genre preferences of different groups of users by utilizing human memory processes. These processes describe how humans access information units in their memory by considering the factors of (i) past usage frequency, (ii) past usage recency, and (iii) the current context. Using a publicly available dataset of more than a billion music listening records shared on the music streaming platform Last.fm, we find that our approach provides significantly better prediction accuracy results than various baseline algorithms for all evaluated user groups, i.e., (i) low-mainstream music listeners, (ii) medium-mainstream music listeners, and (iii) high-mainstream music listeners. Furthermore, our approach is based on a simple psychological model, which contributes to the transparency and explainability of the calculated predictions.
&lt;/p&gt;</description></item></channel></rss>