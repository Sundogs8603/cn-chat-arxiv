<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;False Negative Elimination&#65288;FNE&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#25277;&#26679;&#36873;&#25321;&#36127;&#26679;&#26412;&#26469;&#20943;&#36731;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20013;&#30001;&#38169;&#35823;&#36127;&#26679;&#26412;&#24341;&#20837;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04380</link><description>&lt;p&gt;
&#20320;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#19981;&#26159;&#30495;&#27491;&#30340;&#36127;&#26679;&#26412;&#65306;&#36890;&#36807;&#28040;&#38500;&#38169;&#35823;&#36127;&#26679;&#26412;&#22686;&#24378;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Your Negative May not Be True Negative: Boosting Image-Text Matching with False Negative Elimination. (arXiv:2308.04380v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;False Negative Elimination&#65288;FNE&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#25277;&#26679;&#36873;&#25321;&#36127;&#26679;&#26412;&#26469;&#20943;&#36731;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20013;&#30001;&#38169;&#35823;&#36127;&#26679;&#26412;&#24341;&#20837;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#37319;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;&#36127;&#26679;&#26412;&#20197;&#26377;&#25928;&#35757;&#32451;&#27169;&#22411;&#65292;&#20363;&#22914;&#65292;&#22256;&#38590;&#36127;&#26679;&#26412;&#33021;&#22815;&#20351;&#27169;&#22411;&#23398;&#20064;&#24471;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#19982;&#38170;&#28857;&#26368;&#30456;&#20284;&#30340;&#26679;&#26412;&#20316;&#20026;&#22256;&#38590;&#36127;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#36127;&#26679;&#26412;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20855;&#26377;&#39640;&#30456;&#20284;&#24230;&#20294;&#27809;&#26377;&#19982;&#38170;&#28857;&#37197;&#23545;&#30340;&#26679;&#26412;&#21487;&#33021;&#20173;&#20855;&#26377;&#27491;&#38754;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38169;&#35823;&#36127;&#26679;&#26412;&#12290;&#22312;&#19977;&#20803;&#32452;&#25439;&#22833;&#20013;&#25490;&#38500;&#36825;&#20123;&#38169;&#35823;&#36127;&#26679;&#26412;&#20250;&#35823;&#23548;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23548;&#33268;&#26816;&#32034;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38169;&#35823;&#36127;&#26679;&#26412;&#28040;&#38500;&#65288;FNE&#65289;&#31574;&#30053;&#26469;&#36890;&#36807;&#25277;&#26679;&#36873;&#25321;&#36127;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#38169;&#35823;&#36127;&#26679;&#26412;&#24341;&#20837;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#29420;&#31435;&#26500;&#36896;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#30340;&#20998;&#24067;...
&lt;/p&gt;
&lt;p&gt;
Most existing image-text matching methods adopt triplet loss as the optimization objective, and choosing a proper negative sample for the triplet of &lt;anchor, positive, negative&gt; is important for effectively training the model, e.g., hard negatives make the model learn efficiently and effectively. However, we observe that existing methods mainly employ the most similar samples as hard negatives, which may not be true negatives. In other words, the samples with high similarity but not paired with the anchor may reserve positive semantic associations, and we call them false negatives. Repelling these false negatives in triplet loss would mislead the semantic representation learning and result in inferior retrieval performance. In this paper, we propose a novel False Negative Elimination (FNE) strategy to select negatives via sampling, which could alleviate the problem introduced by false negatives. Specifically, we first construct the distributions of positive and negative samples separat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21452;&#27969;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#36827;&#34892;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#23545;&#40784;&#27169;&#22359;&#23454;&#29616;&#26356;&#22909;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04343</link><description>&lt;p&gt;
&#29992;Transformer&#23558;&#21452;&#27969;&#32534;&#30721;&#22120;&#32479;&#19968;&#36215;&#26469;&#36827;&#34892;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval. (arXiv:2308.04343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21452;&#27969;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#36827;&#34892;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#23545;&#40784;&#27169;&#22359;&#23454;&#29616;&#26356;&#22909;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#30340;&#21452;&#27969;&#32534;&#30721;&#22120;&#65292;&#20363;&#22914;&#65292;&#29992;&#20110;&#22270;&#20687;&#30340;CNN&#21644;&#29992;&#20110;&#25991;&#26412;&#30340;RNN / Transformer&#12290;&#36825;&#31181;&#26550;&#26500;&#19978;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35821;&#20041;&#20998;&#24067;&#31354;&#38388;&#65292;&#24182;&#38480;&#21046;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36827;&#32780;&#23548;&#33268;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#21463;Transformer&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#32479;&#19968;&#20004;&#31181;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#21452;&#27969;Transformer&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;Hierarchical Alignment Transformers (HAT)&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#22270;&#20687;Transformer&#12289;&#19968;&#31181;&#25991;&#26412;Transformer&#21644;&#19968;&#20010;&#23618;&#27425;&#23545;&#40784;&#27169;&#22359;&#12290;&#36890;&#36807;&#36825;&#31181;&#30456;&#21516;&#30340;&#26550;&#26500;&#65292;&#32534;&#30721;&#22120;&#21487;&#20197;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#26356;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing cross-modal retrieval methods employ two-stream encoders with different architectures for images and texts, \textit{e.g.}, CNN for images and RNN/Transformer for texts. Such discrepancy in architectures may induce different semantic distribution spaces and limit the interactions between images and texts, and further result in inferior alignment between images and texts. To fill this research gap, inspired by recent advances of Transformers in vision tasks, we propose to unify the encoder architectures with Transformers for both modalities. Specifically, we design a cross-modal retrieval framework purely based on two-stream Transformers, dubbed \textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image Transformer, a text Transformer, and a hierarchical alignment module. With such identical architectures, the encoders could produce representations with more similar characteristics for images and texts, and make the interactions and alignments between th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#26144;&#23556;&#38899;&#39057;&#21644;&#25991;&#23383;&#25551;&#36848;&#26469;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#21644;&#39069;&#22806;&#30340;&#20154;&#24037;&#29983;&#25104;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#26159;&#20851;&#38190;&#32452;&#20214;&#65292;&#21462;&#24471;&#20102;&#22312;DCASE&#25361;&#25112;&#20013;&#30340;&#31532;&#19968;&#21517;&#65292;&#24182;&#22312;ClothoV2&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;5.6&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.04258</link><description>&lt;p&gt;
&#20351;&#29992;PaSST&#21644;&#22823;&#35268;&#27169;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#25512;&#36827;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets. (arXiv:2308.04258v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#26144;&#23556;&#38899;&#39057;&#21644;&#25991;&#23383;&#25551;&#36848;&#26469;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#21644;&#39069;&#22806;&#30340;&#20154;&#24037;&#29983;&#25104;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#26159;&#20851;&#38190;&#32452;&#20214;&#65292;&#21462;&#24471;&#20102;&#22312;DCASE&#25361;&#25112;&#20013;&#30340;&#31532;&#19968;&#21517;&#65292;&#24182;&#22312;ClothoV2&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;5.6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#25991;&#26412;&#21644;&#39057;&#35889;&#22270;&#36716;&#25442;&#22120;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#26816;&#32034;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24405;&#38899;&#21644;&#25991;&#23383;&#25551;&#36848;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#38899;&#39057;&#23383;&#24149;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#30340;&#30456;&#20851;&#31034;&#20363;&#38752;&#36817;&#12290;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31995;&#32479;&#30340;&#27599;&#20010;&#32452;&#20214;&#23545;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#22312;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65306;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#29992;&#20110;&#38899;&#39057;&#23884;&#20837;&#21644;&#21033;&#29992;&#39069;&#22806;&#30340;&#20154;&#24037;&#29983;&#25104;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23581;&#35797;&#36890;&#36807;&#28155;&#21152;&#21487;&#29992;&#20851;&#38190;&#23383;&#26469;&#22686;&#21152;ClothoV2&#23383;&#24149;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#36825;&#21482;&#24102;&#26469;&#20102;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;2023&#24180;DCASE&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#24182;&#19988;&#22312;ClothoV2&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65288;mAP@10&#65289;&#36739;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#21319;&#20102;5.6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#21644;&#22871;&#39184;&#21040;&#32676;&#32452;&#25512;&#33616;&#36825;&#22235;&#31181;&#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.04247</link><description>&lt;p&gt;
&#32479;&#19968;&#25512;&#33616;&#31995;&#32479;&#65306;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#21644;&#22871;&#39184;&#21040;&#32676;&#32452;&#30340;&#25512;&#33616;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniRecSys: A Unified Framework for Personalized, Group, Package, and Package-to-Group Recommendations. (arXiv:2308.04247v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#21644;&#22871;&#39184;&#21040;&#32676;&#32452;&#25512;&#33616;&#36825;&#22235;&#31181;&#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20026;&#21508;&#31181;&#20135;&#21697;&#21644;&#26381;&#21153;&#25552;&#20379;&#23450;&#21046;&#25512;&#33616;&#26469;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#12290;&#36825;&#20123;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#29992;&#25143;&#23545;&#24179;&#21488;&#30340;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#26045;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#65292;&#20174;&#21521;&#29992;&#25143;&#25110;&#32676;&#32452;&#25512;&#33616;&#39033;&#30446;&#25110;&#22871;&#39184;&#20013;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#23601;&#38656;&#35201;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20180;&#32454;&#25506;&#32034;&#22810;&#20010;&#27169;&#22411;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#23618;&#38754;&#30340;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20010;&#20307;&#27169;&#22411;&#24517;&#39035;&#26681;&#25454;&#19978;&#19979;&#25991;&#23494;&#20999;&#35843;&#25972;&#20854;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#65292;&#20197;&#38450;&#27490;&#20854;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#20135;&#29983;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#25512;&#33616;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#25110;&#22871;&#39184;&#21040;&#32676;&#32452;&#25512;&#33616;&#22235;&#20010;&#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems aim to enhance the overall user experience by providing tailored recommendations for a variety of products and services. These systems help users make more informed decisions, leading to greater user satisfaction with the platform. However, the implementation of these systems largely depends on the context, which can vary from recommending an item or package to a user or a group. This requires careful exploration of several models during the deployment, as there is no comprehensive and unified approach that deals with recommendations at different levels. Furthermore, these individual models must be closely attuned to their generated recommendations depending on the context to prevent significant variation in their generated recommendations. In this paper, we propose a novel unified recommendation framework that addresses all four recommendation tasks, namely personalized, group, package, or package-to-group recommendation, filling the gap in the current research lan
&lt;/p&gt;</description></item><item><title>OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.04226</link><description>&lt;p&gt;
OpinionConv: &#36890;&#36807;&#22522;&#20110;&#30495;&#23454;&#20027;&#35266;&#20307;&#39564;&#30340;&#35266;&#28857;&#23454;&#29616;&#23545;&#35805;&#24335;&#20135;&#21697;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04226
&lt;/p&gt;
&lt;p&gt;
OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#20135;&#21697;&#26102;&#65292;&#20182;&#20154;&#30340;&#35266;&#28857;&#22312;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20135;&#21697;&#30340;&#20027;&#35266;&#20307;&#39564;&#21487;&#20197;&#26159;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#22312;&#38144;&#21806;&#23545;&#35805;&#20013;&#20063;&#26159;&#22914;&#27492;&#65292;&#22312;&#36825;&#31181;&#23545;&#35805;&#20013;&#65292;&#23458;&#25143;&#21644;&#38144;&#21806;&#21161;&#25163;&#20132;&#25442;&#26377;&#20851;&#20135;&#21697;&#30340;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#27492;&#31867;&#23545;&#35805;&#30340;AI&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#32463;&#39564;&#27809;&#26377;&#30495;&#23454;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#20135;&#21697;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#30495;&#23454;&#20027;&#35266;&#21465;&#36848;&#25903;&#25345;&#23545;&#35805;&#24335;AI&#12290;&#36890;&#36807;OpinionConv&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#35266;&#28857;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21592;&#20063;&#30830;&#35748;&#20102;&#35266;&#28857;&#23545;&#20110;&#20915;&#31574;&#30340;&#20449;&#24687;&#22522;&#30784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#30701;&#35270;&#39057;&#24179;&#21488;&#20013;&#30340;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#65292;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#32771;&#34385;&#29992;&#25143;&#30340;&#27491;&#21453;&#39304;&#34892;&#20026;&#65292;&#20294;&#22312;&#36825;&#20010;&#26032;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#36890;&#36807;&#36339;&#36807;&#19981;&#21916;&#27426;&#30340;&#35270;&#39057;&#34920;&#36798;&#34987;&#21160;&#36127;&#21453;&#39304;&#65292;&#36825;&#31181;&#34987;&#21160;&#36127;&#21453;&#39304;&#21487;&#20197;&#21453;&#26144;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#29992;&#20110;&#25552;&#21462;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.04086</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#24314;&#27169;&#30701;&#35270;&#39057;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#34987;&#21160;&#36127;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Understanding and Modeling Passive-Negative Feedback for Short-video Sequential Recommendation. (arXiv:2308.04086v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#30701;&#35270;&#39057;&#24179;&#21488;&#20013;&#30340;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#65292;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#32771;&#34385;&#29992;&#25143;&#30340;&#27491;&#21453;&#39304;&#34892;&#20026;&#65292;&#20294;&#22312;&#36825;&#20010;&#26032;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#36890;&#36807;&#36339;&#36807;&#19981;&#21916;&#27426;&#30340;&#35270;&#39057;&#34920;&#36798;&#34987;&#21160;&#36127;&#21453;&#39304;&#65292;&#36825;&#31181;&#34987;&#21160;&#36127;&#21453;&#39304;&#21487;&#20197;&#21453;&#26144;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#29992;&#20110;&#25552;&#21462;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#21382;&#21490;&#34892;&#20026;&#25512;&#33616;&#19979;&#19968;&#20010;&#20132;&#20114;&#39033;&#12290;&#20256;&#32479;&#30340;&#39034;&#24207;&#25512;&#33616;&#20027;&#35201;&#32771;&#34385;&#20102;&#35832;&#22914;&#28857;&#20987;&#12289;&#36141;&#20080;&#31561;&#25910;&#38598;&#21040;&#30340;&#27491;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;TikTok&#31561;&#30701;&#35270;&#39057;&#24179;&#21488;&#19978;&#65292;&#35270;&#39057;&#27983;&#35272;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#20195;&#34920;&#27491;&#21453;&#39304;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#35270;&#39057;&#26159;&#33258;&#21160;&#25773;&#25918;&#30340;&#65292;&#29992;&#25143;&#34987;&#21160;&#22320;&#25509;&#25910;&#25512;&#33616;&#30340;&#35270;&#39057;&#12290;&#22312;&#36825;&#31181;&#26032;&#22330;&#26223;&#19979;&#65292;&#29992;&#25143;&#36890;&#36807;&#36339;&#36807;&#20182;&#20204;&#19981;&#21916;&#27426;&#30340;&#35270;&#39057;&#26469;&#34920;&#36798;&#34987;&#21160;&#36127;&#21453;&#39304;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#20851;&#20182;&#20204;&#21916;&#22909;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30740;&#31350;&#30340;&#36127;&#21453;&#39304;&#19981;&#21516;&#65292;&#36825;&#31181;&#34987;&#21160;&#36127;&#21453;&#39304;&#21487;&#20197;&#21453;&#26144;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#24182;&#20316;&#20026;&#25277;&#21462;&#29992;&#25143;&#20559;&#22909;&#30340;&#37325;&#35201;&#30417;&#30563;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#19968;&#20840;&#26032;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#20180;&#32454;&#35774;&#35745;&#21644;&#21033;&#29992;&#34987;&#21160;&#36127;&#21453;&#39304;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation is one of the most important tasks in recommender systems, which aims to recommend the next interacted item with historical behaviors as input. Traditional sequential recommendation always mainly considers the collected positive feedback such as click, purchase, etc. However, in short-video platforms such as TikTok, video viewing behavior may not always represent positive feedback. Specifically, the videos are played automatically, and users passively receive the recommended videos. In this new scenario, users passively express negative feedback by skipping over videos they do not like, which provides valuable information about their preferences. Different from the negative feedback studied in traditional recommender systems, this passive-negative feedback can reflect users' interests and serve as an important supervision signal in extracting users' preferences. Therefore, it is essential to carefully design and utilize it in this novel recommendation scenario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#20013;&#29289;&#21697;&#34920;&#31034;&#23398;&#20064;&#23545;&#25512;&#33616;&#20219;&#21153;&#30340;&#24433;&#21709;&#20197;&#21450;&#19981;&#21516;&#38454;&#27573;&#20449;&#24687;&#34701;&#21512;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#21512;&#21327;&#21516;&#23398;&#20064;&#21644;&#34701;&#21512;&#22810;&#26679;&#20449;&#24687;&#30340;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.04067</link><description>&lt;p&gt;
&#22312;&#32447;&#33976;&#39311;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Online Distillation-enhanced Multi-modal Transformer for Sequential Recommendation. (arXiv:2308.04067v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#20013;&#29289;&#21697;&#34920;&#31034;&#23398;&#20064;&#23545;&#25512;&#33616;&#20219;&#21153;&#30340;&#24433;&#21709;&#20197;&#21450;&#19981;&#21516;&#38454;&#27573;&#20449;&#24687;&#34701;&#21512;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#21512;&#21327;&#21516;&#23398;&#20064;&#21644;&#34701;&#21512;&#22810;&#26679;&#20449;&#24687;&#30340;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;, &#23427;&#25972;&#21512;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;,&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#30456;&#27604;, &#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#19981;&#21516;&#20110;&#20165;&#20381;&#36182;&#20110;&#29289;&#21697;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#20449;&#24687;&#24182;&#20851;&#27880;&#32593;&#32476;&#32467;&#26500;&#35774;&#35745;&#30340;&#20256;&#32479;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;, &#22810;&#27169;&#24577;&#25512;&#33616;&#27169;&#22411;&#38656;&#35201;&#24378;&#35843;&#29289;&#21697;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#34701;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#21697;&#34920;&#31034;&#23398;&#20064;&#23545;&#19979;&#28216;&#25512;&#33616;&#20219;&#21153;&#30340;&#24433;&#21709;, &#24182;&#26816;&#39564;&#20102;&#19981;&#21516;&#38454;&#27573;&#20449;&#24687;&#34701;&#21512;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;, &#25105;&#20204;&#35777;&#26126;&#20102;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#36866;&#21512;&#21327;&#21516;&#23398;&#20064;&#21644;&#34701;&#21512;&#22810;&#26679;&#20449;&#24687;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#27492;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#29992;&#20110;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal recommendation systems, which integrate diverse types of information, have gained widespread attention in recent years. However, compared to traditional collaborative filtering-based multi-modal recommendation systems, research on multi-modal sequential recommendation is still in its nascent stages. Unlike traditional sequential recommendation models that solely rely on item identifier (ID) information and focus on network structure design, multi-modal recommendation models need to emphasize item representation learning and the fusion of heterogeneous data sources. This paper investigates the impact of item representation learning on downstream recommendation tasks and examines the disparities in information fusion at different stages. Empirical experiments are conducted to demonstrate the need to design a framework suitable for collaborative learning and fusion of diverse information. Based on this, we propose a new model-agnostic framework for multi-modal sequential recom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#23427;&#37319;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12289;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#19988;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04033</link><description>&lt;p&gt;
&#36866;&#24212;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications. (arXiv:2308.04033v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#23427;&#37319;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12289;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#19988;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#12289;&#24320;&#21457;&#21644;&#30740;&#31350;&#29616;&#20195;&#26080;&#32447;&#36890;&#20449;&#25216;&#26415;&#30340;&#29616;&#26377;&#26041;&#27861;&#28041;&#21450;&#32791;&#26102;&#19988;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#31579;&#36873;&#22823;&#37327;&#30340;&#32593;&#39029;&#21644;&#25216;&#26415;&#35268;&#33539;&#25991;&#20214;&#65292;&#25910;&#38598;&#25152;&#38656;&#20449;&#24687;&#24182;&#36827;&#34892;&#32508;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#26368;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#23637;&#65292;&#24182;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#38468;&#21152;&#32452;&#20214;&#65306;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#26080;&#32447;&#25216;&#26415;&#35268;&#33539;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#31616;&#27905;&#30340;&#12289;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#20351;&#29992;&#30001;&#19987;&#23478;&#21019;&#24314;&#30340;&#26597;&#35810;&#21644;&#21442;&#32771;&#21709;&#24212;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.04028</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#37327;&#25991;&#26723;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#38382;&#31572;&#20381;&#36182;&#20110;&#39640;&#25928;&#30340;&#27573;&#33853;&#26816;&#32034;&#26469;&#36873;&#25321;&#20505;&#36873;&#19978;&#19979;&#25991;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#65292;&#22914;TF-IDF&#25110;BM25&#65292;&#26159;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#22312;&#32593;&#32476;&#19978;&#65292;&#27809;&#26377;&#19968;&#31687;&#25991;&#31456;&#21487;&#20197;&#25552;&#20379;&#25152;&#26377;&#21487;&#33021;&#30340;&#31572;&#26696;&#65292;&#20197;&#22238;&#31572;&#29992;&#25143;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#31264;&#23494;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#23545;&#32500;&#22522;&#30334;&#31185;2018&#24180;12&#26376;20&#26085;&#30340;&#20542;&#38144;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#29992;&#20316;&#22238;&#31572;&#38382;&#39064;&#30340;&#28304;&#25991;&#26723;&#12290;&#38382;&#31572;&#31995;&#32479;&#22312;&#22810;&#20010;&#24320;&#25918;&#39046;&#22495;&#21644;&#26426;&#22120;&#29702;&#35299;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26681;&#25454;&#22810;&#39033;&#35843;&#26597;&#65292;&#26080;&#27861;&#20174;&#32500;&#22522;&#30334;&#31185;&#20934;&#30830;&#22238;&#31572;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32654;&#39135;&#25512;&#33616;&#26381;&#21153;&#65288;OFRS&#65289;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#25552;&#21462;&#20102;&#26368;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32452;&#21512;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#26102;&#31354;&#24207;&#21015;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20248;&#21270;&#30340;&#26102;&#31354;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04019</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#32447;&#32654;&#39135;&#25512;&#33616;&#26381;&#21153;&#30340;&#26102;&#31354;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Exploring the Spatiotemporal Features of Online Food Recommendation Service. (arXiv:2308.04019v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32654;&#39135;&#25512;&#33616;&#26381;&#21153;&#65288;OFRS&#65289;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#25552;&#21462;&#20102;&#26368;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32452;&#21512;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#26102;&#31354;&#24207;&#21015;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20248;&#21270;&#30340;&#26102;&#31354;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32654;&#39135;&#25512;&#33616;&#26381;&#21153;&#65288;OFRS&#65289;&#20855;&#26377;&#26174;&#33879;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#22312;&#21450;&#26102;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#26041;&#38754;&#30340;&#20415;&#21033;&#20248;&#21183;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#25506;&#32034;&#20854;&#26102;&#31354;&#23646;&#24615;&#65292;&#20294;&#23545;OFRS&#30340;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#20840;&#38754;&#28145;&#20837;&#30340;&#20998;&#26512;&#23578;&#26410;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22522;&#20110;&#19977;&#20010;&#38382;&#39064;&#30740;&#31350;&#20102;OFRS&#65306;&#26102;&#31354;&#29305;&#24449;&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#65307;&#20026;&#20160;&#20040;&#19981;&#33021;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26469;&#24314;&#27169;OFRS&#30340;&#26102;&#31354;&#24207;&#21015;&#65307;&#22914;&#20309;&#32467;&#21512;&#26102;&#31354;&#29305;&#24449;&#20197;&#25552;&#39640;OFRS&#30340;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#21462;&#20102;OFRS&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#30830;&#23450;&#20102;&#26368;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32452;&#21512;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#26102;&#31354;&#24207;&#21015;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#33258;&#27880;&#24847;&#21147;&#22312;OFRS&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20248;&#21270;&#30340;&#26102;&#31354;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Food Recommendation Service (OFRS) has remarkable spatiotemporal characteristics and the advantage of being able to conveniently satisfy users' needs in a timely manner. There have been a variety of studies that have begun to explore its spatiotemporal properties, but a comprehensive and in-depth analysis of the OFRS spatiotemporal features is yet to be conducted. Therefore, this paper studies the OFRS based on three questions: how spatiotemporal features play a role; why self-attention cannot be used to model the spatiotemporal sequences of OFRS; and how to combine spatiotemporal features to improve the efficiency of OFRS. Firstly, through experimental analysis, we systemically extracted the spatiotemporal features of OFRS, identified the most valuable features and designed an effective combination method. Secondly, we conducted a detailed analysis of the spatiotemporal sequences, which revealed the shortcomings of self-attention in OFRS, and proposed a more optimized spatiotem
&lt;/p&gt;</description></item><item><title>&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#20026;&#32676;&#32452;&#25512;&#33616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#31890;&#24230;&#23618;&#27425;&#26469;&#25581;&#31034;&#32676;&#32452;&#25104;&#21592;&#30340;&#28508;&#22312;&#20559;&#22909;&#24182;&#20943;&#23569;&#25512;&#33616;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.04017</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#29992;&#20110;&#32676;&#32452;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-Granularity Attention Model for Group Recommendation. (arXiv:2308.04017v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04017
&lt;/p&gt;
&lt;p&gt;
&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#20026;&#32676;&#32452;&#25512;&#33616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#31890;&#24230;&#23618;&#27425;&#26469;&#25581;&#31034;&#32676;&#32452;&#25104;&#21592;&#30340;&#28508;&#22312;&#20559;&#22909;&#24182;&#20943;&#23569;&#25512;&#33616;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#32452;&#25512;&#33616;&#26159;&#22522;&#20110;&#20849;&#21516;&#20852;&#36259;&#12289;&#20559;&#22909;&#21644;&#29305;&#24449;&#20026;&#19968;&#32452;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#20010;&#20154;&#20559;&#22909;&#24182;&#20570;&#20986;&#26377;&#21033;&#20110;&#25972;&#20010;&#32676;&#32452;&#30340;&#38598;&#20307;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#34892;&#20026;&#20016;&#23500;&#30340;&#29992;&#25143;&#65292;&#24573;&#35270;&#20102;&#34892;&#20026;&#30456;&#23545;&#31232;&#30095;&#30340;&#29992;&#25143;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#23548;&#33268;&#20010;&#20154;&#20852;&#36259;&#30340;&#23398;&#20064;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;MGAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#31890;&#24230;&#23618;&#27425;&#65288;&#21363;&#23376;&#38598;&#12289;&#32676;&#32452;&#21644;&#36229;&#38598;&#65289;&#26469;&#25581;&#31034;&#32676;&#32452;&#25104;&#21592;&#28508;&#22312;&#20559;&#22909;&#21644;&#20943;&#23569;&#25512;&#33616;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#38598;&#20559;&#22909;&#25552;&#21462;&#27169;&#22359;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#20132;&#20114;&#20449;&#24687;&#21644;&#20351;&#29992;&#23618;&#27425;&#21270;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#29992;&#25143;&#28508;&#22312;&#30340;&#23376;&#38598;&#32423;&#21035;&#20559;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group recommendation provides personalized recommendations to a group of users based on their shared interests, preferences, and characteristics. Current studies have explored different methods for integrating individual preferences and making collective decisions that benefit the group as a whole. However, most of them heavily rely on users with rich behavior and ignore latent preferences of users with relatively sparse behavior, leading to insufficient learning of individual interests. To address this challenge, we present the Multi-Granularity Attention Model (MGAM), a novel approach that utilizes multiple levels of granularity (i.e., subsets, groups, and supersets) to uncover group members' latent preferences and mitigate recommendation noise. Specially, we propose a Subset Preference Extraction module that enhances the representation of users' latent subset-level preferences by incorporating their previous interactions with items and utilizing a hierarchical mechanism. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#21644;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#31561;&#20215;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#23558;&#26597;&#35810;&#26144;&#23556;&#20026;&#25628;&#32034;&#24847;&#22270;&#21521;&#37327;&#34920;&#31034;&#12289;&#35782;&#21035;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#31561;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03869</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#30340;&#35821;&#20041;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Equivalence of e-Commerce Queries. (arXiv:2308.03869v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#21644;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#31561;&#20215;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#23558;&#26597;&#35810;&#26144;&#23556;&#20026;&#25628;&#32034;&#24847;&#22270;&#21521;&#37327;&#34920;&#31034;&#12289;&#35782;&#21035;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#31561;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#21464;&#21270;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#25628;&#32034;&#24847;&#22270;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#34920;&#23618;&#24046;&#24322;&#30340;&#19981;&#21516;&#26597;&#35810;&#26469;&#34920;&#36798;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#21033;&#29992;&#26597;&#35810;&#31561;&#20215;&#24615;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#25628;&#32034;&#24847;&#22270;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#35782;&#21035;&#34920;&#36798;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#65292;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;&#34920;&#38754;&#30456;&#20284;&#24615;&#28041;&#21450;&#22522;&#20110;&#35789;&#30340;&#21464;&#24418;&#12289;&#35789;&#24207;&#12289;&#22797;&#21512;&#21644;&#22122;&#22768;&#35789;&#26469;&#35268;&#33539;&#21270;&#26597;&#35810;&#12290;&#34892;&#20026;&#30456;&#20284;&#24615;&#21033;&#29992;&#21382;&#21490;&#25628;&#32034;&#34892;&#20026;&#29983;&#25104;&#26597;&#35810;&#24847;&#22270;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#31163;&#32447;&#36807;&#31243;&#29992;&#20110;&#35757;&#32451;&#21477;&#23376;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#32780;&#22312;&#32447;&#26368;&#36817;&#37051;&#26041;&#27861;&#25903;&#25345;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.03855</link><description>&lt;p&gt;
&#31227;&#21160;&#20379;&#24212;&#65306;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#21518;&#19968;&#22359;&#25340;&#22270;
&lt;/p&gt;
&lt;p&gt;
Mobile Supply: The Last Piece of Jigsaw of Recommender System. (arXiv:2308.03855v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#22312;&#32447;&#24179;&#21488;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#38543;&#30528;&#25163;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#25512;&#33616;&#31639;&#27861;&#37096;&#32626;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#20998;&#39029;&#26426;&#21046;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36793;&#32536;&#31471;&#31227;&#21160;&#25490;&#21517;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#31227;&#21160;&#25490;&#21517;&#21482;&#33021;&#23545;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#65292;&#25152;&#20197;&#22914;&#26524;&#21482;&#35843;&#29992;&#19968;&#20004;&#27425;&#26159;&#19981;&#36215;&#20316;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29992;&#25143;&#26597;&#30475;&#20102;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#21518;&#65292;&#29992;&#25143;&#20250;&#21047;&#26032;&#39029;&#38754;&#33719;&#21462;&#26032;&#30340;&#39033;&#30446;&#12290;&#36825;&#20250;&#20351;&#31227;&#21160;&#25490;&#21517;&#27169;&#22411;&#20570;&#24456;&#22810;&#26080;&#29992;&#21151;&#65292;&#24433;&#21709;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#27969;&#27700;&#32447;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation system is a fundamental functionality of online platforms. With the development of computing power of mobile phones, some researchers have deployed recommendation algorithms on users' devices to solve the problems of data transmission delay and pagination mechanism. However, the existing edge-side mobile rankings cannot completely solve the problem of pagination mechanism. The mobile rankings can only sort the items on the current page, so it will not work if it is called once or twice. Besides, after the user has viewed the items of interest to the user on the current page, the user refresh to get a new page of items. This will make the mobile ranking model do a lot of useless work and affect the user's immersive experience. In order to solve the pagination mechanism problem, we propose a completely new module in the pipeline of recommender named Mobile Supply. The pipeline of recommender system is extended to "retrival-&gt;pre-ranking-&gt;ranking-&gt;re-ranking-&gt;Mobile Supply-&gt;
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;JinaAI&#26500;&#24314;&#38899;&#20048;&#34892;&#19994;&#30340;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#21305;&#37197;&#27468;&#26354;&#27468;&#35789;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#32467;&#26524;&#26469;&#35299;&#20915;&#29616;&#26377;&#25628;&#32034;&#24341;&#25806;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03842</link><description>&lt;p&gt;
&#20351;&#29992;JinaAI&#26500;&#24314;&#38899;&#20048;&#34892;&#19994;&#30340;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Search Engine and Recommendation System for the Music Industry built with JinaAI. (arXiv:2308.03842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;JinaAI&#26500;&#24314;&#38899;&#20048;&#34892;&#19994;&#30340;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#21305;&#37197;&#27468;&#26354;&#27468;&#35789;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#32467;&#26524;&#26469;&#35299;&#20915;&#29616;&#26377;&#25628;&#32034;&#24341;&#25806;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#38899;&#20048;&#34892;&#19994;&#20013;&#25628;&#32034;&#24341;&#25806;&#21644;&#22522;&#20110;&#25512;&#33616;&#30340;&#31995;&#32479;&#30340;&#21457;&#23637;&#26159;&#19968;&#20010;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#36777;&#35770;&#20043;&#19968;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25628;&#32034;&#24341;&#25806;&#39046;&#22495;&#23384;&#22312;&#30528;&#20005;&#37325;&#30340;&#25233;&#37057;&#65292;&#36825;&#26159;&#30001;&#20110;&#35832;&#22914;&#36895;&#24230;&#12289;&#20934;&#30830;&#24615;&#20197;&#21450;&#26597;&#35810;&#25968;&#25454;&#30340;&#26684;&#24335;&#31561;&#20196;&#20154;&#25285;&#24551;&#30340;&#22240;&#32032;&#25152;&#23548;&#33268;&#30340;&#12290;&#20154;&#20204;&#32463;&#24120;&#22312;&#20165;&#26681;&#25454;&#26631;&#39064;&#25628;&#32034;&#27468;&#26354;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21333;&#20010;&#26597;&#35810;&#36755;&#20837;&#23436;&#25104;&#25628;&#32034;&#20998;&#26512;&#65292;&#24182;&#19982;&#25968;&#25454;&#24211;&#20013;&#30340;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#21069;&#27839;&#25216;&#26415;&#24037;&#20855;&#20197;&#24320;&#21457;&#29992;&#25143;&#21451;&#22909;&#30340;&#25628;&#32034;&#24341;&#25806;&#33267;&#20851;&#37325;&#35201;&#12290;Jina AI&#26159;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#25628;&#32034;&#24341;&#25806;&#30340;MLOps&#26694;&#26550;&#65292;&#23427;&#34987;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#33719;&#24471;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;Jina AI&#26377;&#25928;&#22320;&#24110;&#21161;&#32500;&#25252;&#21644;&#25552;&#21319;&#25628;&#32034;&#24341;&#25806;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#24615;&#33021;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;JinaAI&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38899;&#20048;&#34892;&#19994;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most intriguing debates regarding a novel task is the development of search engines and recommendation-based systems in the music industry. Studies have shown a drastic depression in the search engine fields, due to concerning factors such as speed, accuracy and the format of data given for querying. Often people face difficulty in searching for a song solely based on the title, hence a solution is proposed to complete a search analysis through a single query input and is matched with the lyrics of the songs present in the database. Hence it is essential to incorporate cutting-edge technology tools for developing a user-friendly search engine. Jina AI is an MLOps framework for building neural search engines that are utilized, in order for the user to obtain accurate results. Jina AI effectively helps to maintain and enhance the quality of performance for the search engine for the query given. An effective search engine and a recommendation system for the music industry, buil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#35813;&#31639;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#19982;&#31169;&#23494;&#30340;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.03735</link><description>&lt;p&gt;
&#38543;&#26426;&#31639;&#27861;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Randomized algorithms for precise measurement of differentially-private, personalized recommendations. (arXiv:2308.03735v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#35813;&#31639;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#19982;&#31169;&#23494;&#30340;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#26159;&#24403;&#20170;&#20114;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#24110;&#21161;&#33402;&#26415;&#23478;&#21644;&#21019;&#20316;&#32773;&#21560;&#24341;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#65292;&#21516;&#26102;&#20063;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#26032;&#30340;&#26377;&#36259;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20154;&#25968;&#25454;&#21644;&#25968;&#25454;&#38544;&#31169;&#30340;&#21382;&#21490;&#19978;&#31895;&#24515;&#23545;&#24453;&#65292;&#35768;&#22810;&#29992;&#25143;&#23545;&#20010;&#24615;&#21270;&#25512;&#33616;&#24179;&#21488;&#25345;&#24576;&#30097;&#24577;&#24230;&#12290;&#29616;&#22312;&#65292;&#20381;&#36182;&#20110;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20225;&#19994;&#27491;&#36827;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#20363;&#65292;&#38656;&#35201;&#23545;&#20182;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25512;&#33616;&#31639;&#27861;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#27979;&#37327;&#65292;&#21448;&#21487;&#20197;&#20445;&#25252;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#20197;&#24191;&#21578;&#20026;&#20363;&#24212;&#29992;&#65292;&#24182;&#36827;&#34892;&#31163;&#32447;&#23454;&#39564;&#65292;&#37327;&#21270;&#25552;&#20986;&#30340;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#23545;&#29992;&#25143;&#20307;&#39564;&#12289;&#24191;&#21578;&#21830;&#20215;&#20540;&#21644;&#24179;&#21488;&#25910;&#20837;&#31561;&#20851;&#38190;&#25351;&#26631;&#30340;&#24433;&#21709;&#65292;&#19982;&#65288;&#31169;&#23494;&#30340;&#65289;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#30340;&#26497;&#31471;&#24773;&#20917;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;4&#20010;&#22522;&#26412;&#24067;&#23616;&#22359;&#21644;4&#20010;&#25991;&#26412;&#31867;&#21035;&#30340;&#24067;&#23616;&#26631;&#31614;&#30340;&#25968;&#23383;&#25991;&#26723;&#30340;&#21322;&#33258;&#21160;&#27880;&#37322;&#36807;&#31243;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#29992;&#20110;&#20844;&#20849;&#20107;&#21153;&#39046;&#22495;&#30340;DLA&#26032;&#25968;&#25454;&#24211;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#22788;&#29702;&#25968;&#23383;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2306.10046</link><description>&lt;p&gt;
&#20844;&#20849;&#20107;&#21153;&#39046;&#22495;&#20013;&#25991;&#26723;&#24067;&#23616;&#27880;&#37322;&#65306;&#25968;&#25454;&#24211;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Document Layout Annotation: Database and Benchmark in the Domain of Public Affairs. (arXiv:2306.10046v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;4&#20010;&#22522;&#26412;&#24067;&#23616;&#22359;&#21644;4&#20010;&#25991;&#26412;&#31867;&#21035;&#30340;&#24067;&#23616;&#26631;&#31614;&#30340;&#25968;&#23383;&#25991;&#26723;&#30340;&#21322;&#33258;&#21160;&#27880;&#37322;&#36807;&#31243;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#29992;&#20110;&#20844;&#20849;&#20107;&#21153;&#39046;&#22495;&#30340;DLA&#26032;&#25968;&#25454;&#24211;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#22788;&#29702;&#25968;&#23383;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#37117;&#20250;&#29983;&#25104;&#25104;&#21315;&#19978;&#19975;&#20010;&#25968;&#23383;&#25991;&#26723;&#65292;&#20854;&#20013;&#21253;&#21547;&#23545;&#20844;&#21496;&#12289;&#20844;&#20849;&#32452;&#32455;&#21644;&#20844;&#27665;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#37492;&#20110;&#25163;&#21160;&#22788;&#29702;&#36825;&#20123;&#25991;&#26723;&#30340;&#19981;&#21487;&#33021;&#24615;&#65292;&#22312;&#26576;&#20123;&#34892;&#19994;&#20013;&#33258;&#21160;&#22788;&#29702;&#36825;&#20123;&#25991;&#26723;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#26512;&#26159;&#19981;&#36275;&#20197;&#23436;&#20840;&#29702;&#35299;&#36890;&#36807;&#19981;&#21516;&#37325;&#35201;&#24615;&#30340;&#19981;&#21516;&#32452;&#20214;&#21576;&#29616;&#30340;&#20449;&#24687;&#30340;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#22810;&#24180;&#26469;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#25991;&#26723;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31243;&#24207;&#23545;&#25968;&#23383;&#25991;&#26723;&#36827;&#34892;&#21322;&#33258;&#21160;&#27880;&#37322;&#65292;&#21253;&#25324;4&#20010;&#22522;&#26412;&#24067;&#23616;&#22359;&#21644;4&#20010;&#25991;&#26412;&#31867;&#21035;&#30340;&#19981;&#21516;&#24067;&#23616;&#26631;&#31614;&#12290;&#25105;&#20204;&#24212;&#29992;&#27492;&#31243;&#24207;&#22312;20&#22235;&#20010;&#26469;&#33258;&#35199;&#29677;&#29273;&#25919;&#24220;&#30340;&#25968;&#25454;&#28304;&#19978;&#25910;&#38598;&#20102;&#19968;&#20010;&#20844;&#20849;&#20107;&#21153;&#39046;&#22495;&#30340;DLA&#26032;&#25968;&#25454;&#24211;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547;......
&lt;/p&gt;
&lt;p&gt;
Every day, thousands of digital documents are generated with useful information for companies, public organizations, and citizens. Given the impossibility of processing them manually, the automatic processing of these documents is becoming increasingly necessary in certain sectors. However, this task remains challenging, since in most cases a text-only based parsing is not enough to fully understand the information presented through different components of varying significance. In this regard, Document Layout Analysis (DLA) has been an interesting research field for many years, which aims to detect and classify the basic components of a document. In this work, we used a procedure to semi-automatically annotate digital documents with different layout labels, including 4 basic layout blocks and 4 text categories. We apply this procedure to collect a novel database for DLA in the public affairs domain, using a set of 24 data sources from the Spanish Administration. The database comprises 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;&#12290;&#36890;&#36807;&#32416;&#27491;&#20256;&#32479;&#25490;&#24207;&#26694;&#26550;&#20013;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25151;&#28304;&#21305;&#37197;&#65292;&#24182;&#21152;&#20837;&#20102;&#32771;&#34385;&#25151;&#28304;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2210.07774</link><description>&lt;p&gt;
&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning To Rank Diversely At Airbnb. (arXiv:2210.07774v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;&#12290;&#36890;&#36807;&#32416;&#27491;&#20256;&#32479;&#25490;&#24207;&#26694;&#26550;&#20013;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25151;&#28304;&#21305;&#37197;&#65292;&#24182;&#21152;&#20837;&#20102;&#32771;&#34385;&#25151;&#28304;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Airbnb&#26159;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#65292;&#23558;&#20986;&#31199;&#25151;&#28304;&#30340;&#25151;&#19996;&#19982;&#26469;&#33258;&#20840;&#29699;&#30340;&#28508;&#22312;&#23458;&#20154;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#24212;&#29992;&#21040;&#21305;&#37197;&#23458;&#20154;&#19982;&#25151;&#19996;&#30340;&#36807;&#31243;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#25490;&#24207;&#25913;&#36827;&#26159;&#36890;&#36807;&#19968;&#31181;&#26680;&#24515;&#31574;&#30053;&#39537;&#21160;&#30340;&#65306;&#25353;&#29031;&#20854;&#39044;&#35745;&#30340;&#39044;&#35746;&#27010;&#29575;&#23545;&#25151;&#28304;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#25913;&#36827;&#36825;&#20123;&#39044;&#35746;&#27010;&#29575;&#20272;&#35745;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31574;&#30053;&#26263;&#21547;&#30340;&#19968;&#20010;&#20551;&#35774;&#26159;&#65292;&#25628;&#32034;&#32467;&#26524;&#20013;&#30340;&#27599;&#20010;&#25151;&#28304;&#30340;&#39044;&#35746;&#27010;&#29575;&#21487;&#20197;&#29420;&#31435;&#30830;&#23450;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20010;&#20551;&#35774;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32416;&#27491;&#36825;&#20010;&#20551;&#35774;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#21450;&#22522;&#20110;&#35813;&#29702;&#35770;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#36890;&#36807;&#26174;&#24335;&#32771;&#34385;&#25151;&#28304;&#20043;&#38388;&#30340;&#21487;&#33021;&#30456;&#20284;&#24615;&#24182;&#20943;&#23567;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Airbnb is a two-sided marketplace, bringing together hosts who own listings for rent, with prospective guests from around the globe. Applying neural network-based learning to rank techniques has led to significant improvements in matching guests with hosts. These improvements in ranking were driven by a core strategy: order the listings by their estimated booking probabilities, then iterate on techniques to make these booking probability estimates more and more accurate. Embedded implicitly in this strategy was an assumption that the booking probability of a listing could be determined independently of other listings in search results. In this paper we discuss how this assumption, pervasive throughout the commonly-used learning to rank frameworks, is false. We provide a theoretical foundation correcting this assumption, followed by efficient neural network architectures based on the theory. Explicitly accounting for possible similarities between listings, and reducing them to diversify
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCDF&#30340;&#24182;&#34892;&#35745;&#31639;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#36190;&#21161;&#25628;&#32034;&#24191;&#21578;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#20026;&#39044;&#22788;&#29702;&#27169;&#22359;&#12289;&#24191;&#21578;&#25490;&#24207;&#27169;&#22359;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#22359;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#25512;&#26029;&#24310;&#36831;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#26377;&#26426;&#25628;&#32034;&#32467;&#26524;&#23545;&#24191;&#21578;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.12893</link><description>&lt;p&gt;
PCDF&#65306;&#19968;&#31181;&#29992;&#20110;&#36190;&#21161;&#25628;&#32034;&#24191;&#21578;&#26381;&#21153;&#30340;&#24182;&#34892;&#35745;&#31639;&#20998;&#24067;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PCDF: A Parallel-Computing Distributed Framework for Sponsored Search Advertising Serving. (arXiv:2206.12893v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCDF&#30340;&#24182;&#34892;&#35745;&#31639;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#36190;&#21161;&#25628;&#32034;&#24191;&#21578;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#20026;&#39044;&#22788;&#29702;&#27169;&#22359;&#12289;&#24191;&#21578;&#25490;&#24207;&#27169;&#22359;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#22359;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#25512;&#26029;&#24310;&#36831;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#26377;&#26426;&#25628;&#32034;&#32467;&#26524;&#23545;&#24191;&#21578;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36190;&#21161;&#25628;&#32034;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#36981;&#24490;&#32423;&#32852;&#33539;&#24335;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#39044;&#25490;&#24207;&#21644;&#25490;&#24207;&#12290;&#30001;&#20110;&#22312;&#32447;&#25512;&#26029;&#25928;&#29575;&#30340;&#20005;&#26684;&#35201;&#27714;&#65292;&#24456;&#38590;&#22312;&#25490;&#24207;&#38454;&#27573;&#37096;&#32626;&#26377;&#29992;&#20294;&#35745;&#31639;&#23494;&#38598;&#30340;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#22312;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#25490;&#24207;&#27169;&#22411;&#20551;&#35774;&#29992;&#25143;&#28857;&#20987;&#20165;&#20381;&#36182;&#20110;&#24191;&#21578;&#26412;&#36523;&#65292;&#36825;&#23548;&#33268;&#25490;&#21517;&#38454;&#27573;&#24573;&#35270;&#20102;&#26377;&#26426;&#25628;&#32034;&#32467;&#26524;&#23545;&#39044;&#27979;&#24191;&#21578;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;PCDF&#65288;&#24182;&#34892;&#35745;&#31639;&#20998;&#24067;&#24335;&#26694;&#26550;&#65289;&#65292;&#20801;&#35768;&#23558;&#35745;&#31639;&#25104;&#26412;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26816;&#32034;&#38454;&#27573;&#24182;&#34892;&#37096;&#32626;&#22312;&#39044;&#22788;&#29702;&#27169;&#22359;&#12289;&#24191;&#21578;&#25490;&#24207;&#27169;&#22359;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#22359;&#20013;&#12290;&#19982;&#32463;&#20856;&#26694;&#26550;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PCDF&#26377;&#25928;&#38477;&#20302;&#20102;&#25972;&#20307;&#25512;&#26029;&#24310;&#36831;&#12290;&#25972;&#20010;&#27169;&#22359;&#26159;&#31471;&#21040;&#31471;&#31163;&#32447;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional online advertising systems for sponsored search follow a cascade paradigm with retrieval, pre-ranking,ranking, respectively. Constrained by strict requirements on online inference efficiency, it tend to be difficult to deploy useful but computationally intensive modules in the ranking stage. Moreover, ranking models currently used in the industry assume the user click only relies on the advertisements itself, which results in the ranking stage overlooking the impact of organic search results on the predicted advertisements (ads). In this work, we propose a novel framework PCDF(Parallel-Computing Distributed Framework), allowing to split the computation cost into three parts and to deploy them in the pre-module in parallel with the retrieval stage, the middle-module for ranking ads, and the post-module for re-ranking ads with external items. Our PCDF effectively reduces the overall inference latency compared with the classic framework. The whole module is end-to-end offline 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#33616;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#33268;&#24615;&#32422;&#26463;&#65288;C$^2$-Rec&#65289;&#65292;&#21482;&#20351;&#29992;&#20004;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#29992;&#25143;&#34920;&#31034;&#24182;&#21521;&#29992;&#25143;&#25512;&#33616;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2112.06668</link><description>&lt;p&gt;
CT4Rec: &#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#33616;&#19968;&#33268;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CT4Rec: Simple yet Effective Consistency Training for Sequential Recommendation. (arXiv:2112.06668v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#33616;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#33268;&#24615;&#32422;&#26463;&#65288;C$^2$-Rec&#65289;&#65292;&#21482;&#20351;&#29992;&#20004;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#29992;&#25143;&#34920;&#31034;&#24182;&#21521;&#29992;&#25143;&#25512;&#33616;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#35760;&#24405;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#28982;&#21518;&#36827;&#34892;&#25512;&#33616;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#33719;&#24471;&#26377;&#20449;&#24687;&#37327;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;CL&#30340;&#27169;&#22411;&#38656;&#35201;&#31934;&#32454;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#32321;&#29712;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#26356;&#22909;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#24182;&#21521;&#29992;&#25143;&#25512;&#33616;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#30340;&#26377;&#25928;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65288;C$^2$-Rec&#65289;&#65292;&#20854;&#20013;&#21482;&#20351;&#29992;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#32467;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation methods play an important role in real-world recommender systems. These systems are able to catch user preferences by taking advantage of historical records and then performing recommendations. Contrastive learning(CL) is a cutting-edge technology that can assist us in obtaining informative user representations, but these CL-based models need subtle negative sampling strategies, tedious data augmentation methods, and heavy hyper-parameters tuning work. In this paper, we introduce another way to generate better user representations and recommend more attractive items to users. Particularly, we put forward an effective \textbf{C}onsistency \textbf{C}onstraint for sequential \textbf{Rec}ommendation(C$^2$-Rec) in which only two extra training objectives are used without any structural modifications and data augmentation strategies. Substantial experiments have been conducted on three benchmark datasets and one real industrial dataset, which proves that our propose
&lt;/p&gt;</description></item></channel></rss>