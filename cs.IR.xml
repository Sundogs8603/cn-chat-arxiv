<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12660</link><description>&lt;p&gt;
ERASE&#65306;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12660
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;(DRS)&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#22823;&#37327;&#29305;&#24449;&#23383;&#27573;&#26469;&#25552;&#20379;&#26356;&#31934;&#20934;&#30340;&#25512;&#33616;&#12290;&#26377;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#20248;&#21270;&#23384;&#20648;&#25928;&#29575;&#65292;&#20197;&#28385;&#36275;&#37096;&#32626;&#38656;&#27714;&#12290;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;DRS&#30340;&#32972;&#26223;&#19979;&#65292;&#23578;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#38754;&#20020;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#23454;&#39564;&#35774;&#32622;&#30340;&#24046;&#24322;&#24448;&#24448;&#23548;&#33268;&#19981;&#20844;&#24179;&#27604;&#36739;&#65292;&#36974;&#34109;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#23646;&#24615;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#36873;&#25321;&#25216;&#26415;&#21644;DRS&#39592;&#24178;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#38480;&#21046;&#24615;&#25991;&#31456;&#30340;&#36890;&#29992;&#24615;&#30740;&#31350;&#21644;&#37096;&#32626;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#24448;&#24448;&#19987;&#27880;&#20110;&#27604;&#36739;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21487;&#36798;&#21040;&#30340;&#23792;&#20540;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#22312;&#35745;&#31639;&#26041;&#38754;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20852;&#36259;&#26694;Embedding&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12649</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20852;&#36259;&#26694;Embedding&#30340;&#25512;&#33616;&#31995;&#32479;InBox
&lt;/p&gt;
&lt;p&gt;
InBox: Recommendation with Knowledge Graph using Interest Box Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#20852;&#36259;&#26694;Embedding&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;(KGs)&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#20852;&#36259;&#23545;&#24212;&#20110;&#28508;&#22312;&#25968;&#37327;&#24222;&#22823;&#30340;&#30456;&#20851;&#39033;&#30446;&#38598;&#65292;2&#65289;&#23545;KG&#20449;&#24687;&#21644;&#20852;&#36259;&#36830;&#25509;&#24615;&#32570;&#20047;&#26126;&#30830;&#12289;&#32454;&#31890;&#24230;&#30340;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;embedding&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12649v1 Announce Type: cross  Abstract: Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability. Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of knowledge graph data and interest connectivity. To address these limitations, we introduce a novel embedding-
&lt;/p&gt;</description></item><item><title>&#32654;&#22242;&#22806;&#21334;&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#20837;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24555;&#36895;&#25512;&#33616;&#31574;&#30053;&#65292;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#30456;&#20284;&#29992;&#25143;&#20559;&#22909;&#30340;&#19978;&#19979;&#25991;&#65292;&#23450;&#20301;&#30456;&#24212;&#30340;PoIs&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.12566</link><description>&lt;p&gt;
&#32654;&#22242;&#22806;&#21334;&#20013;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24555;&#36895;&#25512;&#33616;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12566
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22242;&#22806;&#21334;&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#20837;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24555;&#36895;&#25512;&#33616;&#31574;&#30053;&#65292;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#30456;&#20284;&#29992;&#25143;&#20559;&#22909;&#30340;&#19978;&#19979;&#25991;&#65292;&#23450;&#20301;&#30456;&#24212;&#30340;PoIs&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22242;&#22806;&#21334;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#30528;&#26085;&#30410;&#22686;&#38271;&#30340;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#65292;&#36825;&#32473;&#26377;&#25928;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#24102;&#26469;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#24120;&#24120;&#26080;&#27861;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#25110;&#32773;&#36807;&#20110;&#22797;&#26434;&#65292;&#20351;&#24471;&#28385;&#36275;&#32654;&#22242;&#22806;&#21334;&#29420;&#29305;&#19994;&#21153;&#38656;&#27714;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#20852;&#36259;&#65292;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#29992;&#25143;&#30340;&#20559;&#22909;&#36873;&#25321;&#30456;&#20851;&#23376;&#24207;&#21015;&#20174;&#29992;&#25143;&#24191;&#27867;&#30340;&#21382;&#21490;&#34892;&#20026;&#20013;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#29992;&#25143;&#20132;&#20114;&#30340;&#19978;&#19979;&#25991;&#23545;&#20182;&#20204;&#30340;&#20559;&#22909;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24555;&#36895;&#25512;&#33616;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#20986;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#20855;&#26377;&#31867;&#20284;&#29992;&#25143;&#20559;&#22909;&#30340;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#35782;&#21035;&#30340;&#19978;&#19979;&#25991;&#23450;&#20301;&#30456;&#24212;&#30340;PoI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12566v1 Announce Type: new  Abstract: In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21015;&#34920;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26367;&#20195;&#30340;&#21015;&#34920;&#26041;&#24335;&#26469;&#20248;&#21270;&#30456;&#20851;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#28857;&#23545;&#28857;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12499</link><description>&lt;p&gt;
&#22522;&#20110;&#24207;&#21015;&#23398;&#20064;&#36807;&#31243;&#30340;&#21015;&#34920;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Listwise Generative Retrieval Models via a Sequential Learning Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21015;&#34920;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26367;&#20195;&#30340;&#21015;&#34920;&#26041;&#24335;&#26469;&#20248;&#21270;&#30456;&#20851;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#28857;&#23545;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#65288;GR&#65289;&#33539;&#24335;&#65292;&#20854;&#20013;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#30452;&#25509;&#29983;&#25104;&#19968;&#20010;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;docids&#65289;&#21015;&#34920;&#12290;&#29616;&#26377;&#30340;GR&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#36827;&#34892;&#20248;&#21270;&#65306;&#36825;&#28041;&#21450;&#26368;&#22823;&#21270;&#32473;&#23450;&#36755;&#20837;&#26597;&#35810;&#30340;&#21333;&#20010;&#30456;&#20851;docid&#30340;&#21487;&#33021;&#24615;&#65292;&#20551;&#35774;&#21015;&#34920;&#20013;&#30340;&#27599;&#20010;docid&#30340;&#21487;&#33021;&#24615;&#19982;&#20854;&#20182;docid&#29420;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31216;&#36825;&#20123;&#27169;&#22411;&#20026;&#28857;&#23545;&#28857;&#26041;&#27861;&#12290;&#34429;&#28982;&#28857;&#23545;&#28857;&#26041;&#27861;&#22312;GR&#30340;&#32972;&#26223;&#19979;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#23427;&#24573;&#30053;&#20102;&#25490;&#24207;&#28041;&#21450;&#23545;&#21015;&#34920;&#36827;&#34892;&#39044;&#27979;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#26367;&#20195;&#30340;&#21015;&#34920;&#26041;&#24335;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#24335;&#20351;GR&#27169;&#22411;&#33021;&#22815;&#22312;docid&#21015;&#34920;&#32423;&#21035;&#19978;&#20248;&#21270;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12499v1 Announce Type: new  Abstract: Recently, a novel generative retrieval (GR) paradigm has been proposed, where a single sequence-to-sequence model is learned to directly generate a list of relevant document identifiers (docids) given a query. Existing GR models commonly employ maximum likelihood estimation (MLE) for optimization: this involves maximizing the likelihood of a single relevant docid given an input query, with the assumption that the likelihood for each docid is independent of the other docids in the list. We refer to these models as the pointwise approach in this paper. While the pointwise approach has been shown to be effective in the context of GR, it is considered sub-optimal due to its disregard for the fundamental principle that ranking involves making predictions about lists. In this paper, we address this limitation by introducing an alternative listwise approach, which empowers the GR model to optimize the relevance at the docid list level. Specific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12388</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#23545;&#35805;&#31995;&#32479;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#32780;&#21487;&#35299;&#37322;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#23545;&#20110;&#20102;&#35299;&#12289;&#35780;&#20272;&#21644;&#25345;&#32493;&#25913;&#36827;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#25991;&#26412;&#23884;&#20837;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LLMs&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;LLM&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#31034;&#20363;&#30340;&#30417;&#30563;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12388v1 Announce Type: cross  Abstract: Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it sco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.12384</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Aligning and Training Framework for Multimodal Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#27491;&#22312;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#36229;&#36234;&#29992;&#25143;&#20132;&#20114;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#35270;&#20026;&#36741;&#21161;&#65292;&#29992;&#20110;&#24110;&#21161;&#23398;&#20064;ID&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20869;&#23481;&#29305;&#24449;&#21644;ID&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#65292;&#30452;&#25509;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20351;&#29992;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;AlignRec&#20013;&#65292;&#25512;&#33616;&#30446;&#26631;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#21363;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#65292;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27599;&#20010;&#23545;&#40784;&#37096;&#20998;&#37117;&#30001;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#34920;&#24449;&#65292;&#24182;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#25968;&#25454;&#20013;&#27010;&#24565;&#28418;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#26469;&#24110;&#21161;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;</title><link>https://arxiv.org/abs/2403.12328</link><description>&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#27969;&#20013;&#28418;&#31227;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Methods for Generating Drift in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12328
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#20013;&#27010;&#24565;&#28418;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#26469;&#24110;&#21161;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.12328v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#31995;&#32479;&#21644;&#20010;&#20307;&#19981;&#26029;&#20135;&#29983;&#25968;&#25454;&#12290; &#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#20154;&#20204;&#20998;&#20139;&#20182;&#20204;&#30340;&#30693;&#35782;&#65292;&#24773;&#24863;&#21644;&#24847;&#35265;&#65292;&#25552;&#20379;&#20851;&#20110;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#35780;&#35770;&#31561;&#12290; &#33258;&#21160;&#20174;&#36825;&#20123;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#20197;&#20026;&#32452;&#32455;&#21644;&#26426;&#26500;&#25552;&#20379;&#35265;&#35299;&#65292;&#20174;&#32780;&#38450;&#27490;&#36130;&#21153;&#24433;&#21709;&#65292;&#20363;&#22914;&#12290; &#20026;&#20102;&#38543;&#26102;&#38388;&#23398;&#20064;&#25991;&#26412;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#32771;&#34385;&#27010;&#24565;&#28418;&#31227;&#12290; &#27010;&#24565;&#28418;&#31227;&#26159;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#39057;&#32321;&#29616;&#35937;&#65292;&#23545;&#24212;&#20110;&#26102;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#26356;&#25913;&#12290; &#20363;&#22914;&#65292;&#24403;&#24773;&#24863;&#21464;&#21270;&#25110;&#21333;&#35789;&#21547;&#20041;&#38543;&#26102;&#38388;&#35843;&#25972;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#27010;&#24565;&#28418;&#31227;&#12290; &#23613;&#31649;&#27010;&#24565;&#28418;&#31227;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#35265;&#12290; &#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20415;&#31616;&#21270;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;&#12290; &#36825;&#20123;&#26041;&#27861;&#24050;&#24212;&#29992;&#20110;Ye
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12328v1 Announce Type: cross  Abstract: Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Ye
&lt;/p&gt;</description></item><item><title>TnT-LLM &#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#20998;&#37197;&#26631;&#31614;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.12173</link><description>&lt;p&gt;
TnT-LLM&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
TnT-LLM: Text Mining at Scale with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12173
&lt;/p&gt;
&lt;p&gt;
TnT-LLM &#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#20998;&#37197;&#26631;&#31614;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#26377;&#29992;&#30340;&#31867;&#21035;&#26631;&#31614;&#36827;&#34892;&#32452;&#32455;&#65292;&#26159;&#25991;&#26412;&#25366;&#25496;&#20013;&#29992;&#20110;&#19979;&#28216;&#20998;&#26512;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#26500;&#24314;&#22522;&#20110;&#25991;&#26412;&#26631;&#31614;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#25972;&#29702;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#24403;&#26631;&#31614;&#31354;&#38388;&#19981;&#26126;&#30830;&#19988;&#32570;&#23569;&#22823;&#35268;&#27169;&#25968;&#25454;&#27880;&#37322;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23588;&#20026;&#20005;&#23803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20854;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#21475;&#26377;&#21161;&#20110;&#24341;&#23548;&#21644;&#20351;&#29992;&#22823;&#35268;&#27169;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TnT-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26631;&#31614;&#29983;&#25104;&#21644;&#20998;&#37197;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12173v1 Announce Type: cross  Abstract: Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#31227;&#21160;&#26641;&#8221;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#65292;&#20197;&#25552;&#21319;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12100</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#26641;&#23398;&#20064;&#26102;&#38388;&#27573;&#20559;&#22909;&#36827;&#34892;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#31227;&#21160;&#26641;&#8221;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#65292;&#20197;&#25552;&#21319;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#24403;&#21069;&#30340;&#31614;&#21040;&#36712;&#36857;&#25552;&#20379;POI&#30340;&#21160;&#24577;&#25490;&#21517;&#12290;&#26412;&#20219;&#21153;&#30340;&#25512;&#33616;&#24615;&#33021;&#21462;&#20915;&#20110;&#36890;&#36807;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#65288;LBSNs&#65289;&#25968;&#25454;&#20840;&#38754;&#20102;&#35299;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31227;&#21160;&#26641;&#8221;&#30340;&#21019;&#26032;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#20998;&#23618;&#25551;&#36848;&#29992;&#25143;&#30340;&#31614;&#21040;&#35760;&#24405;&#12290;&#31227;&#21160;&#26641;&#21253;&#21547;&#22810;&#31890;&#24230;&#26102;&#38388;&#27573;&#33410;&#28857;&#65292;&#20197;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31227;&#21160;&#26641;&#32593;&#32476;&#65288;MTNet&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12100v1 Announce Type: cross  Abstract: Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#32570;&#22833;&#30340;&#29992;&#25143;&#36141;&#29289;&#21382;&#21490;&#37096;&#20998;&#24182;&#36866;&#24403;&#20016;&#23500;&#23427;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12096</link><description>&lt;p&gt;
&#20016;&#23500;&#29992;&#25143;&#36141;&#29289;&#35760;&#24405;&#65306;&#29992;&#23618;&#27425;&#25512;&#33616;&#31995;&#32479;&#36171;&#33021;&#30005;&#23376;&#21830;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12096
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32570;&#22833;&#30340;&#29992;&#25143;&#36141;&#29289;&#21382;&#21490;&#37096;&#20998;&#24182;&#36866;&#24403;&#20016;&#23500;&#23427;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#36141;&#29289;&#21382;&#21490;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#26356;&#20016;&#23500;&#30340;&#29992;&#25143;&#21382;&#21490;&#20250;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#26356;&#20542;&#21521;&#20110;&#22312;&#29289;&#21697;&#20197;&#26368;&#20302;&#20215;&#26684;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#36141;&#29289;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22823;&#22810;&#25968;&#29992;&#25143;&#21516;&#26102;&#20174;&#22810;&#20010;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36141;&#29289;&#65307;&#29992;&#25143;&#30340;&#19981;&#21516;&#36141;&#29289;&#21382;&#21490;&#37096;&#20998;&#22312;&#19981;&#21516;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#38388;&#20849;&#20139;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20551;&#35774;&#20219;&#20309;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#37117;&#25317;&#26377;&#29992;&#25143;&#21382;&#21490;&#30340;&#23436;&#25972;&#35760;&#24405;&#65292;&#20294;&#21482;&#33021;&#35775;&#38382;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#12290;&#22914;&#26524;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#39318;&#20808;&#39044;&#27979;&#32570;&#22833;&#30340;&#37096;&#20998;&#24182;&#36866;&#24403;&#20016;&#23500;&#29992;&#25143;&#30340;&#36141;&#29289;&#21382;&#21490;&#65292;&#37027;&#20040;&#23558;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25512;&#33616;&#19979;&#19968;&#20010;&#21830;&#21697;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#29992;&#25143;&#30340;&#36141;&#29289;&#21382;&#21490;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;NDCG@10&#21644;&#20854;&#20182;&#26041;&#38754;&#37117;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12096v1 Announce Type: cross  Abstract: Recommendation systems can provide accurate recommendations by analyzing user shopping history. A richer user history results in more accurate recommendations. However, in real applications, users prefer e-commerce platforms where the item they seek is at the lowest price. In other words, most users shop from multiple e-commerce platforms simultaneously; different parts of the user's shopping history are shared between different e-commerce platforms. Consequently, we assume in this study that any e-commerce platform has a complete record of the user's history but can only access some parts of it. If a recommendation system is able to predict the missing parts first and enrich the user's shopping history properly, it will be possible to recommend the next item more accurately. Our recommendation system leverages user shopping history to improve prediction accuracy. The proposed approach shows significant improvements in both NDCG@10 and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23450;&#20041;&#24182;&#35268;&#33539;&#20102;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#21305;&#37197;&#23545;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#36317;&#31163;&#22522;&#20934;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12092</link><description>&lt;p&gt;
&#21305;&#37197;&#33521;&#35821;&#22320;&#22336;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Methods for Matching English Language Addresses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23450;&#20041;&#24182;&#35268;&#33539;&#20102;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#21305;&#37197;&#23545;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#36317;&#31163;&#22522;&#20934;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#21344;&#25454;&#30528;&#19968;&#24109;&#20043;&#22320;&#65292;&#22240;&#20026;&#27599;&#20010;&#35789;&#25152;&#20855;&#26377;&#30340;&#20301;&#32622;&#37325;&#35201;&#24615;&#21644;&#23427;&#25152;&#28041;&#21450;&#30340;&#22320;&#29702;&#33539;&#22260;&#12290;&#21305;&#37197;&#22320;&#22336;&#30340;&#20219;&#21153;&#27599;&#22825;&#37117;&#22312;&#21457;&#29983;&#65292;&#24182;&#19988;&#23384;&#22312;&#20110;&#37038;&#20214;&#37325;&#23450;&#21521;&#12289;&#23454;&#20307;&#35299;&#26512;&#31561;&#21508;&#31181;&#39046;&#22495;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23450;&#20041;&#21644;&#35268;&#33539;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33521;&#35821;&#22320;&#22336;&#30340;&#21305;&#37197;&#21644;&#19981;&#21305;&#37197;&#23545;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#33258;&#21160;&#25191;&#34892;&#22320;&#22336;&#21305;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21508;&#19981;&#30456;&#21516;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#21040;&#26368;&#36866;&#21512;&#36825;&#31181;&#22320;&#22336;&#21305;&#37197;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12092v1 Announce Type: cross  Abstract: Addresses occupy a niche location within the landscape of textual data, due to the positional importance carried by every word, and the geographical scope it refers to. The task of matching addresses happens everyday and is present in various fields like mail redirection, entity resolution, etc. Our work defines, and formalizes a framework to generate matching and mismatching pairs of addresses in the English language, and use it to evaluate various methods to automatically perform address matching. These methods vary widely from distance based approaches to deep learning models. By studying the Precision, Recall and Accuracy metrics of these approaches, we obtain an understanding of the best suited method for this setting of the address matching task.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12090</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Foundation Models and Information Retrieval in Digital Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12090
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#12289;LLM&#12289;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#26816;&#32034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12090v1 Announce Type: cross  Abstract: The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology
&lt;/p&gt;</description></item><item><title>&#22810;&#20262;&#22810;&#37117;&#20250;&#22823;&#23398;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12088</link><description>&lt;p&gt;
TMU&#21442;&#21152;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
TMU at TREC Clinical Trials Track 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12088
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20262;&#22810;&#37117;&#20250;&#22823;&#23398;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22810;&#20262;&#22810;&#37117;&#20250;&#22823;&#23398;&#21442;&#21152;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#12290;&#20316;&#20026;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21033;&#29992;&#20102;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#20020;&#24202;&#35797;&#39564;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22242;&#38431;-V-TorontoMU&#30340;&#21442;&#19982;&#30340;&#25972;&#20307;&#26041;&#27861;&#35770;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#23454;&#29616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12088v1 Announce Type: new  Abstract: This paper describes Toronto Metropolitan University's participation in the TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced natural language processing techniques and neural language models in our experiments to retrieve the most relevant clinical trials. We illustrate the overall methodology, experimental settings, and results of our implementation for the run submission as part of Team - V-TorontoMU.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#32676;&#20307;&#30005;&#24433;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20026;&#32676;&#20307;&#20915;&#31574;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;</title><link>https://arxiv.org/abs/2403.12087</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#36890;&#36947;&#24773;&#24863;&#35782;&#21035;&#30340;&#32676;&#20307;&#30005;&#24433;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Group Movie Selection using Multi-channel Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#32676;&#20307;&#30005;&#24433;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20026;&#32676;&#20307;&#20915;&#31574;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#27963;&#21160;&#20013;&#32463;&#24120;&#21253;&#25324;&#20197;&#32676;&#20307;&#24418;&#24335;&#35266;&#30475;&#30005;&#35270;&#25110;&#30005;&#24433;&#12290;&#36873;&#25321;&#19968;&#20010;&#31526;&#21512;&#19981;&#21516;&#32676;&#20307;&#24773;&#24863;&#20542;&#21521;&#30340;&#30005;&#24433;&#21487;&#33021;&#20250;&#24456;&#26840;&#25163;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#31181;&#26469;&#28304;&#65288;&#22914;&#30005;&#24433;&#28023;&#25253;&#12289;&#37197;&#20048;&#21644;&#25991;&#26412;&#65289;&#30340;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#32676;&#20307;&#30005;&#24433;&#36873;&#25321;&#30340;&#26041;&#27861;&#23398;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#38899;&#20048;&#12289;&#25991;&#26412;&#12289;&#24425;&#33394;&#22270;&#20687;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#25216;&#26415;&#21644;&#32676;&#20307;&#20915;&#31574;&#65292;&#20026;&#22312;&#32676;&#20307;&#29615;&#22659;&#20013;&#23548;&#33322;&#30005;&#24433;&#36873;&#25321;&#22797;&#26434;&#21160;&#24577;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12087v1 Announce Type: new  Abstract: Social activities often done in groups include watching television or movies. Choosing a film that appeals to the emotional inclinations of a varied group can be tricky. One of the most difficult aspects of making group movie suggestions is achieving agreement among members. At the same time, emotion is the most important component that connects the film and the viewer. Current research proposes a methodology for group movie selection that employs emotional analysis from numerous sources, such as film posters, soundtracks, and text. Our research stands at the intersection of emotion recognition technology in music, text, color images, and group decision-making, providing a practical tool for navigating the complex dynamics of film selection in a group setting. The survey participants were given emotion categories and asked to select the emotions that best suited a particular movie. Preliminary comparison results between real and predicte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Terrorizer&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#32593;&#32476;&#29702;&#35770;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24402;&#22240;&#20110;&#20844;&#21496;&#30340;&#19987;&#21033;&#20013;&#23384;&#22312;&#30340;&#21517;&#31216;&#21464;&#20307;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12083</link><description>&lt;p&gt;
&#23637;&#31034; Terrorizer&#65306;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#19987;&#21033;&#21463;&#35753;&#20154;&#20013;&#20844;&#21496;&#21517;&#31216;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Presenting Terrorizer: an algorithm for consolidating company names in patent assignees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Terrorizer&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#32593;&#32476;&#29702;&#35770;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24402;&#22240;&#20110;&#20844;&#21496;&#30340;&#19987;&#21033;&#20013;&#23384;&#22312;&#30340;&#21517;&#31216;&#21464;&#20307;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#38382;&#39064;&#22312;&#20174;&#19987;&#21033;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#30740;&#31350;&#32467;&#26524;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20302;&#20272;&#20102;&#24402;&#22240;&#20110;&#20844;&#21496;&#30340;&#19987;&#21033;&#25968;&#37327;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20197;&#20247;&#22810;&#21517;&#31216;&#65288;&#21253;&#25324;&#30456;&#21516;&#23454;&#20307;&#30340;&#26367;&#20195;&#25340;&#20889;&#21644;&#26368;&#32456;&#20844;&#21496;&#30340;&#23376;&#20844;&#21496;&#65289;&#30003;&#35831;&#19987;&#21033;&#30340;&#36328;&#22269;&#20844;&#21496;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#20381;&#36182;&#20110;&#32791;&#26102;&#30340;&#22522;&#20110;&#35789;&#20856;&#25110;&#23383;&#31526;&#20018;&#21305;&#37197;&#26041;&#27861;&#65292;&#32780;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#35299;&#20915;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#19987;&#21033;&#21463;&#35753;&#20154;&#30340;&#21327;&#35843;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102; Terrorizer &#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#25991;&#26412;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#32593;&#32476;&#29702;&#35770;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25216;&#26415;&#26469;&#21327;&#35843;&#20316;&#20026;&#19987;&#21033;&#21463;&#35753;&#20154;&#35760;&#24405;&#30340;&#20844;&#21496;&#21517;&#31216;&#30340;&#21464;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31639;&#27861;&#36981;&#24490;&#20854;&#21069;&#20307;&#30340;&#19977;&#37096;&#20998;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12083v1 Announce Type: cross  Abstract: The problem of disambiguation of company names poses a significant challenge in extracting useful information from patents. This issue biases research outcomes as it mostly underestimates the number of patents attributed to companies, particularly multinational corporations which file patents under a plethora of names, including alternate spellings of the same entity and, eventually, companies' subsidiaries. To date, addressing these challenges has relied on labor-intensive dictionary based or string matching approaches, leaving the problem of patents' assignee harmonization on large datasets mostly unresolved. To bridge this gap, this paper describes the Terrorizer algorithm, a text-based algorithm that leverages natural language processing (NLP), network theory, and rule-based techniques to harmonize the variants of company names recorded as patent assignees. In particular, the algorithm follows the tripartite structure of its antece
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25506;&#35752;&#39044;&#27979;&#27468;&#26354;&#27969;&#34892;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#27969;&#27966;&#26159;&#24433;&#21709;&#27969;&#34892;&#24230;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26102;&#38388;&#36235;&#21183;&#21644;&#29305;&#24449;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.12079</link><description>&lt;p&gt;
&#36229;&#36234;&#33410;&#22863;&#65306;&#27468;&#26354;&#27969;&#34892;&#30340;&#31192;&#35776;&#65311;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Beats: A Recipe to Song Popularity? A machine learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25506;&#35752;&#39044;&#27979;&#27468;&#26354;&#27969;&#34892;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#27969;&#27966;&#26159;&#24433;&#21709;&#27969;&#34892;&#24230;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26102;&#38388;&#36235;&#21183;&#21644;&#29305;&#24449;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#27969;&#34892;&#24230;&#39044;&#27979;&#24341;&#36215;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#36825;&#24471;&#30410;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21644;Spotify&#31561;&#27969;&#23186;&#20307;&#24179;&#21488;&#30340;&#20852;&#36215;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#28085;&#30422;1957&#24180;&#33267;2020&#24180;&#21508;&#31181;&#27969;&#27966;&#30340;30,000&#39318;&#27468;&#26354;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#27468;&#26354;&#27969;&#34892;&#24230;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#12289;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;&#65288;MARS&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31639;&#27861;&#26469;&#20998;&#26512;&#27468;&#26354;&#29305;&#24449;&#21450;&#20854;&#23545;&#27969;&#34892;&#24230;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#22238;&#24402;&#20998;&#26512;&#34920;&#26126;&#27969;&#27966;&#26159;&#27969;&#34892;&#24230;&#30340;&#20027;&#35201;&#24433;&#21709;&#22240;&#32032;&#65292;&#32780;&#19988;&#38543;&#26102;&#38388;&#21487;&#35265;&#26174;&#33879;&#30340;&#36235;&#21183;&#12290;MARS&#24314;&#27169;&#31361;&#20986;&#20102;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#19982;&#29305;&#24449;&#22914;&#22120;&#20048;&#24230;&#21644;&#26102;&#38271;&#30456;&#20851;&#12290;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#27169;&#22411;&#24378;&#35843;&#20102;&#27969;&#27966;&#65292;&#23588;&#20854;&#26159;&#30005;&#23376;&#33310;&#26354;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12079v1 Announce Type: cross  Abstract: Music popularity prediction has garnered significant attention in both industry and academia, fuelled by the rise of data-driven algorithms and streaming platforms like Spotify. This study aims to explore the predictive power of various machine learning models in forecasting song popularity using a dataset comprising 30,000 songs spanning different genres from 1957 to 2020. Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse song characteristics and their impact on popularity. Results: Ordinary Least Squares (OLS) regression analysis reveals genre as the primary influencer of popularity, with notable trends over time. MARS modelling highlights the complex relationship between variables, particularly with features like instrumentalness and duration. Random Forest and XGBoost models underscore the importance of genre, especially EDM, in predict
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;&#65292;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#22312;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12077</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12077
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#23545;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#20581;&#22766;&#24615;&#65292;&#36890;&#36807;&#23545;&#22810;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#22312;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#26377;&#28508;&#21147;&#25913;&#21464;&#20154;&#20204;&#22312;&#32447;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#21709;&#24212;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20250;&#21152;&#21095;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#33021;&#36890;&#36807;&#24494;&#22937;&#22320;&#25805;&#32437;&#22768;&#26126;&#30340;&#26368;&#34180;&#24369;&#37096;&#20998;&#25104;&#21151;&#35268;&#36991;&#25972;&#20010;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#30340;&#29616;&#23454;&#19988;&#39640;&#39118;&#38505;&#35774;&#32622;&#20013;&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#20581;&#22766;&#24615;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#20855;&#26377;&#40657;&#30418;&#31995;&#32479;&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#35797;&#22270;&#27450;&#39575;&#27169;&#22411;&#36820;&#22238;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#24517;&#24212;&#32842;&#22825;&#12289;PerplexityAI&#21644;YouChat&#31561;&#21508;&#31181;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#20107;&#23454;&#38382;&#39064;&#23545;&#35825;&#23548;&#19981;&#27491;&#30830;&#21709;&#24212;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23637;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12077v1 Announce Type: cross  Abstract: Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;</title><link>https://arxiv.org/abs/2403.06097</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;LLM&#26367;&#20195;&#20154;&#24037;&#26631;&#27880;&#65311; &#26080;&#20154;&#26426;&#20132;&#20184;&#20219;&#21153;&#19979;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22320;&#22336;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CNER-UAV&#65292;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#65292;&#21487;&#20197;&#20840;&#38754;&#35757;&#32451;&#21644;&#35780;&#20272;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#20026;&#26500;&#24314;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#32422;&#21253;&#21547;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#32463;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312; \url{https://github.com/zhhvvv/CNER-UAV} &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18590</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#24191;&#27867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18590
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#22609;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#24402;&#22240;&#20110;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#29420;&#29305;&#25512;&#29702;&#33021;&#21147;&#12290;&#19981;&#21516;&#20110;&#32570;&#20047;&#30452;&#25509;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#20256;&#32479;&#31995;&#32479;&#65292;LLMs&#22312;&#25512;&#33616;&#29289;&#21697;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26631;&#24535;&#30528;&#25512;&#33616;&#39046;&#22495;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#33539;&#24335;&#36716;&#21464;&#12290;&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#23450;&#20041;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#35813;&#30740;&#31350;&#24443;&#24213;&#25506;&#35752;&#20102;LLMs&#22312;&#25512;&#33616;&#26694;&#26550;&#20869;&#22266;&#26377;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#32454;&#33268;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#24179;&#31283;&#36807;&#28193;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#20139;&#25968;&#25454;&#27744;&#30340;&#20840;&#38754;&#23398;&#20064;&#31574;&#30053;&#65292;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;RA-Rec&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#65292;&#24182;&#35774;&#35745;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04527</link><description>&lt;p&gt;
RA-Rec:&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;RA-Rec&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#65292;&#24182;&#35774;&#35745;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20026;LLM&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#32467;&#21512;&#24102;&#26469;&#20102;&#26032;&#30340;&#28526;&#27969;&#65292;&#31216;&#20026;LLM-based RS&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#20363;&#65292;&#21363;ID&#30452;&#25509;&#20351;&#29992;&#33539;&#20363;&#21644;ID&#32763;&#35793;&#33539;&#20363;&#65292;&#25351;&#20986;&#23427;&#20204;&#30340;&#26680;&#24515;&#24369;&#28857;&#22312;&#20110;&#32570;&#20047;&#25512;&#33616;&#30693;&#35782;&#21644;&#29420;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21363;ID&#34920;&#31034;&#65292;&#23427;&#20197;&#19968;&#31181;&#20114;&#34917;&#30340;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RA-Rec&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;&#65292;&#19982;&#22810;&#31181;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#21644;LLM&#26550;&#26500;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ID&#23884;&#20837;&#35270;&#20026;&#36719;&#25552;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#30340;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#21450;&#20026;&#23545;&#40784;&#23450;&#21046;&#30340;&#25968;&#25454;&#26500;&#24314;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RA-Rec&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures. Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#30340;&#26032;&#26041;&#27861;DESAlign&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#32780;DESAlign&#36890;&#36807;&#25554;&#20540;&#32570;&#22833;&#30340;&#35821;&#20041;&#24182;&#24212;&#23545;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17859</link><description>&lt;p&gt;
&#23454;&#29616;&#35821;&#20041;&#19968;&#33268;&#24615;&#65306;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#30340;&#26032;&#26041;&#27861;DESAlign&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#32780;DESAlign&#36890;&#36807;&#25554;&#20540;&#32570;&#22833;&#30340;&#35821;&#20041;&#24182;&#24212;&#23545;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#20013;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#23545;&#20110;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#23646;&#24615;&#38388;&#30340;&#30456;&#21516;&#23454;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#23646;&#24615;&#32780;&#23548;&#33268;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#23646;&#24615;&#25554;&#20540;&#65292;&#20294;&#36825;&#24448;&#24448;&#20250;&#24341;&#20837;&#27169;&#24577;&#22122;&#22768;&#65292;&#25197;&#26354;&#21407;&#22987;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#19968;&#20010;&#26222;&#36866;&#30340;&#29702;&#35770;&#26694;&#26550;&#38480;&#21046;&#20102;&#23545;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DESAlign&#65292;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#30830;&#20445;&#35821;&#20041;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24577;&#32570;&#22833;&#26102;&#36896;&#25104;&#24615;&#33021;&#27874;&#21160;&#12290;DESAlign&#21019;&#26032;&#22320;&#24212;&#23545;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#27169;&#24577;&#25554;&#20540;&#32570;&#22833;&#30340;&#35821;&#20041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Multi-Modal Knowledge Graphs (MMKGs), Multi-Modal Entity Alignment (MMEA) is crucial for identifying identical entities across diverse modal attributes. However, semantic inconsistency, mainly due to missing modal attributes, poses a significant challenge. Traditional approaches rely on attribute interpolation, but this often introduces modality noise, distorting the original semantics. Moreover, the lack of a universal theoretical framework limits advancements in achieving semantic consistency. This study introduces a novel approach, DESAlign, which addresses these issues by applying a theoretical framework based on Dirichlet energy to ensure semantic consistency. We discover that semantic inconsistency leads to model overfitting to modality noise, causing performance fluctuations, particularly when modalities are missing. DESAlign innovatively combats over-smoothing and interpolates absent semantics using existing modalities. Our approach includes a multi-modal knowledge graph lea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;GPT 3.5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#21521;&#37327;&#30340;RAG&#25968;&#25454;&#24211;&#21644;&#38750;&#31639;&#27861;&#36719;&#25552;&#31034;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#22522;&#32447;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26465;&#20214;&#19979;&#24494;&#35843;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.05903</link><description>&lt;p&gt;
&#22312;&#38750;&#19987;&#19994;LLM&#29992;&#25143;&#20013;&#20026;&#24494;&#35843;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#36719;&#25552;&#31034;&#24314;&#31435;&#24615;&#33021;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05903
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;GPT 3.5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#21521;&#37327;&#30340;RAG&#25968;&#25454;&#24211;&#21644;&#38750;&#31639;&#27861;&#36719;&#25552;&#31034;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#22522;&#32447;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26465;&#20214;&#19979;&#24494;&#35843;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#12289;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#36719;&#25552;&#31034;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#30740;&#31350;&#24448;&#24448;&#32858;&#28966;&#20110;&#20351;&#29992;&#39640;&#24230;&#25216;&#26415;&#21270;&#25110;&#39640;&#25104;&#26412;&#30340;&#25216;&#26415;&#65292;&#20351;&#35768;&#22810;&#26032;&#21457;&#29616;&#30340;&#26041;&#27861;&#23545;&#38750;&#25216;&#26415;&#29992;&#25143;&#26469;&#35828;&#30456;&#23545;&#19981;&#26131;&#35775;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#26410;&#32463;&#20462;&#25913;&#30340;GPT 3.5&#29256;&#26412;&#12289;&#32463;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20197;&#21450;&#22312;&#21333;&#29420;&#35775;&#38382;&#22522;&#20110;&#21521;&#37327;&#30340;RAG&#25968;&#25454;&#24211;&#26102;&#30456;&#21516;&#26410;&#32463;&#20462;&#25913;&#30340;&#27169;&#22411;&#65292;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#37197;&#22791;&#20102;&#22522;&#26412;&#30340;&#38750;&#31639;&#27861;&#36719;&#25552;&#31034;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#22411;&#22238;&#31572;&#19968;&#32452;&#20027;&#35201;&#28041;&#21450;2021&#24180;9&#26376;&#20043;&#21518;&#20107;&#20214;&#30340;100&#20010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65288;&#36825;&#26159;GPT 3.5&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#32467;&#26463;&#30340;&#26102;&#38388;&#28857;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#20351;&#29992;&#21830;&#19994;&#24179;&#21488;&#24182;&#24212;&#29992;&#40664;&#35748;&#35774;&#32622;&#65292;&#19981;&#36827;&#34892;&#36845;&#20195;&#20197;&#24314;&#31435;&#19968;&#32452;&#22522;&#32447;&#36755;&#20986;&#65292;&#37027;&#20040;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05903v2 Announce Type: replace-cross  Abstract: Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperf
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item></channel></rss>