<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22522;&#20110;Retrieval Augmented Generation (RAG)&#30340;&#22270;&#26816;&#32034;&#22120;&#34987;&#25552;&#35758;&#29992;&#26469;&#20811;&#26381;LLMs&#30340;&#23616;&#38480;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#38598;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38271;&#23614;&#30693;&#35782;&#30340;&#25429;&#33719;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12352</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26816;&#32034;&#22120;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#38271;&#23614;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12352
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Retrieval Augmented Generation (RAG)&#30340;&#22270;&#26816;&#32034;&#22120;&#34987;&#25552;&#35758;&#29992;&#26469;&#20811;&#26381;LLMs&#30340;&#23616;&#38480;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#38598;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38271;&#23614;&#30693;&#35782;&#30340;&#25429;&#33719;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#27491;&#22312;&#25913;&#21464;&#20449;&#24687;&#26816;&#32034;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#24635;&#32467;&#21644;&#23637;&#31034;&#22823;&#37327;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;LLMs&#20542;&#21521;&#20110;&#31361;&#20986;&#35757;&#32451;&#38598;&#20013;&#26368;&#24120;&#35265;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#24182;&#24573;&#35270;&#32597;&#35265;&#30340;&#20449;&#24687;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#65292;&#26368;&#26032;&#30340;&#21457;&#29616;&#23545;&#20110;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#24448;&#24448;&#34987;&#22823;&#37327;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#39046;&#22495;&#25152;&#25513;&#30422;(&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;)&#12290;&#21033;&#29992;LLMs&#23637;&#29616;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#26032;&#20851;&#32852;&#65292;&#22914;&#33647;&#29289;&#12289;&#22522;&#22240;&#12289;&#30142;&#30149;&#65292;&#25104;&#20026;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#29983;&#20135;&#30340;&#38271;&#23614;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12352v1 Announce Type: new  Abstract: Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the conte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#25552;&#20379;&#19982;&#35268;&#27169;&#26657;&#20934;&#30456;&#20851;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#23545;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#28508;&#21147;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#35268;&#27169;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12276</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#35268;&#27169;&#26657;&#20934;&#35299;&#37322;&#21644;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#25552;&#20379;&#19982;&#35268;&#27169;&#26657;&#20934;&#30456;&#20851;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#23545;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#28508;&#21147;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#35268;&#27169;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#31995;&#32479;&#20013;&#30340;&#35268;&#27169;&#26657;&#20934;&#36807;&#31243;&#28041;&#21450;&#35843;&#25972;&#25490;&#24207;&#22120;&#30340;&#36755;&#20986;&#65292;&#20197;&#20351;&#20854;&#19982;&#37325;&#35201;&#21697;&#36136;&#65288;&#22914;&#28857;&#20987;&#29575;&#25110;&#30456;&#20851;&#24615;&#65289;&#30456;&#23545;&#24212;&#65292;&#36825;&#23545;&#20110;&#21453;&#26144;&#29616;&#23454;&#20215;&#20540;&#20197;&#21450;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#23398;&#20064;&#25490;&#24207;&#27169;&#22411;&#20013;&#30340;&#26657;&#20934;&#25490;&#24207;&#25439;&#22833;&#65292;&#20294;&#35843;&#25972;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#35268;&#27169;&#30340;&#29305;&#23450;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#25797;&#38271;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#20294;&#23558;&#29616;&#26377;&#35268;&#27169;&#26657;&#20934;&#25216;&#26415;&#24212;&#29992;&#21040;&#36825;&#20123;&#27169;&#22411;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#21644;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#65292;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12276v1 Announce Type: new  Abstract: The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes.   This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Instagram&#19978;&#27874;&#26031;&#26032;&#38395;&#26426;&#26500;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;&#35789;&#20849;&#29616;&#22270;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#23545;&#27874;&#26031;&#35821;Instagram&#24086;&#23376;&#30340;&#39640;&#31934;&#24230;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.12272</link><description>&lt;p&gt;
&#23545;Instagram&#19978;&#27874;&#26031;&#26032;&#38395;&#26426;&#26500;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;&#35789;&#20849;&#29616;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Instagram&#19978;&#27874;&#26031;&#26032;&#38395;&#26426;&#26500;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;&#35789;&#20849;&#29616;&#22270;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#23545;&#27874;&#26031;&#35821;Instagram&#24086;&#23376;&#30340;&#39640;&#31934;&#24230;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#30340;&#23835;&#36215;&#21644;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#20351;&#24471;&#25163;&#21160;&#25968;&#25454;&#25688;&#35201;&#21644;&#20998;&#26512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; Instagram&#31038;&#20132;&#32593;&#32476;&#26159;&#20234;&#26391;&#24191;&#27867;&#21033;&#29992;&#30340;&#19968;&#31181;&#37325;&#35201;&#31038;&#20132;&#32593;&#32476;&#65292;&#29992;&#20110;&#21508;&#20010;&#24180;&#40836;&#27573;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#21644;&#20132;&#27969;&#12290; Instagram&#30340;&#22266;&#26377;&#32467;&#26500;&#65292;&#20197;&#20854;&#25991;&#26412;&#20016;&#23500;&#30340;&#20869;&#23481;&#21644;&#31867;&#20284;&#22270;&#24418;&#30340;&#25968;&#25454;&#34920;&#31034;&#20026;&#29305;&#24449;&#65292;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#24418;&#22788;&#29702;&#25216;&#26415;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#24230;&#20998;&#24067;&#34920;&#29616;&#20986;&#26080;&#26631;&#24230;&#29305;&#24449;&#65292;&#34920;&#26126;&#20102;&#38750;&#38543;&#26426;&#30340;&#22686;&#38271;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#35789;&#20849;&#29616;&#21560;&#24341;&#20102;&#22810;&#20010;&#23398;&#31185;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#30340;&#31616;&#21333;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20851;&#38190;&#35789;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#23545;&#27874;&#26031;&#35821;Instagram&#24086;&#23376;&#30340;&#39640;&#31934;&#24230;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12272v1 Announce Type: cross  Abstract: The rise of the Internet and the exponential increase in data have made manual data summarization and analysis a challenging task. Instagram social network is a prominent social network widely utilized in Iran for information sharing and communication across various age groups. The inherent structure of Instagram, characterized by its text-rich content and graph-like data representation, enables the utilization of text and graph processing techniques for data analysis purposes. The degree distributions of these networks exhibit scale-free characteristics, indicating non-random growth patterns. Recently, word co-occurrence has gained attention from researchers across multiple disciplines due to its simplicity and practicality. Keyword extraction is a crucial task in natural language processing. In this study, we demonstrated that high-precision extraction of keywords from Instagram posts in the Persian language can be achieved using uns
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#28151;&#21512;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#26657;&#36873;&#20462;&#35838;&#31243;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#21644;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#22312;&#32852;&#37030;&#26041;&#26696;&#19979;&#35757;&#32451;&#20010;&#21035;&#23398;&#26657;&#27169;&#22411;&#20197;&#25512;&#33616;&#37327;&#36523;&#23450;&#21046;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.12202</link><description>&lt;p&gt;
&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#36328;&#26657;&#36873;&#20462;&#35838;&#25512;&#33616;&#65306;&#19968;&#31181;&#28151;&#21512;&#32852;&#37030;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#28151;&#21512;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#26657;&#36873;&#20462;&#35838;&#31243;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#21644;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#22312;&#32852;&#37030;&#26041;&#26696;&#19979;&#35757;&#32451;&#20010;&#21035;&#23398;&#26657;&#27169;&#22411;&#20197;&#25512;&#33616;&#37327;&#36523;&#23450;&#21046;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25945;&#32946;&#26102;&#20195;&#65292;&#35299;&#20915;&#36328;&#26657;&#23398;&#20064;&#32773;&#22810;&#26679;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#36873;&#20462;&#35838;&#31243;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#32463;&#24120;&#38480;&#21046;&#20102;&#36328;&#26657;&#25968;&#25454;&#20849;&#20139;&#65292;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#31232;&#30095;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#21644;&#26377;&#25928;&#22788;&#29702;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#26368;&#32456;&#23548;&#33268;&#23376;&#20248;&#21270;&#30340;&#25512;&#33616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HFRec&#65292;&#19968;&#31181;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#28151;&#21512;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#36328;&#26657;&#36873;&#20462;&#35838;&#31243;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#20026;&#27599;&#20010;&#23398;&#26657;&#26500;&#24314;&#20102;&#24322;&#26500;&#22270;&#65292;&#23558;&#23398;&#29983;&#20043;&#38388;&#30340;&#21508;&#31181;&#20132;&#20114;&#21644;&#21382;&#21490;&#34892;&#20026;&#34701;&#20837;&#20854;&#20013;&#65292;&#20197;&#25972;&#21512;&#19978;&#19979;&#25991;&#21644;&#20869;&#23481;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#24863;&#30693;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#20010;&#32852;&#37030;&#26041;&#26696;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#35774;&#32622;&#26469;&#35757;&#32451;&#20010;&#21035;&#23398;&#26657;&#27169;&#22411;&#20197;&#25512;&#33616;&#37327;&#36523;&#23450;&#21046;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12202v1 Announce Type: cross  Abstract: In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25277;&#21462;&#24335;&#30693;&#35782;&#22270;&#35889;&#24635;&#32467;&#30340;&#24212;&#29992;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#20998;&#31867;&#65292;&#20026;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2402.12001</link><description>&lt;p&gt;
&#23545;&#25277;&#21462;&#24335;&#30693;&#35782;&#22270;&#35889;&#24635;&#32467;&#30340;&#35843;&#26597;&#65306;&#24212;&#29992;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25277;&#21462;&#24335;&#30693;&#35782;&#22270;&#35889;&#24635;&#32467;&#30340;&#24212;&#29992;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#20998;&#31867;&#65292;&#20026;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19981;&#26029;&#22686;&#38271;&#65292;&#25277;&#21462;&#24335;KG&#24635;&#32467;&#25104;&#20026;&#19968;&#39033;&#28909;&#38376;&#20219;&#21153;&#12290;&#26088;&#22312;&#25552;&#28860;&#20855;&#26377;&#27987;&#32553;&#20449;&#24687;&#30340;&#32039;&#20945;&#23376;&#22270;&#65292;&#26377;&#21161;&#20110;&#21508;&#31181;&#19979;&#28216;&#22522;&#20110;KG&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#23545;&#20854;&#24212;&#29992;&#25552;&#20379;&#31995;&#32479;&#27010;&#36848;&#24182;&#20174;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#23450;&#20041;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#20043;&#19968;&#12290;&#22522;&#20110;&#25105;&#20204;&#24191;&#27867;&#32780;&#27604;&#36739;&#30340;&#35780;&#35770;&#65292;&#26410;&#26469;&#26041;&#21521;&#20063;&#24050;&#38138;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12001v1 Announce Type: new  Abstract: With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task. Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FeB4RAG&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#20869;&#30340;&#32852;&#21512;&#25628;&#32034;&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.11891</link><description>&lt;p&gt;
&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#32852;&#21512;&#25628;&#32034;&#31995;&#32479;&#65288;FeB4RAG&#65289;
&lt;/p&gt;
&lt;p&gt;
FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FeB4RAG&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#20869;&#30340;&#32852;&#21512;&#25628;&#32034;&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#25628;&#32034;&#31995;&#32479;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#25628;&#32034;&#24341;&#25806;&#30340;&#32467;&#26524;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#26469;&#28304;&#20197;&#22686;&#24378;&#32467;&#26524;&#36136;&#37327;&#24182;&#19982;&#29992;&#25143;&#24847;&#22270;&#20445;&#25345;&#19968;&#33268;&#12290;&#38543;&#30528;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31649;&#36947;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32852;&#21512;&#25628;&#32034;&#21487;&#20197;&#22312;&#36328;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#29983;&#25104;&#26126;&#26234;&#30340;&#21709;&#24212;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;&#36807;&#21435;&#30340;TREC FedWeb&#36319;&#36394;&#20013;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#65289;&#36807;&#26102;&#20102;&#65292;&#32570;&#20047;&#23545;&#29616;&#20195;&#20449;&#24687;&#26816;&#32034;&#25361;&#25112;&#30340;&#20195;&#34920;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FeB4RAG&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;RAG&#26694;&#26550;&#20869;&#30340;&#32852;&#21512;&#25628;&#32034;&#32780;&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#28304;&#33258;&#24191;&#27867;&#20351;&#29992;&#30340;beir&#22522;&#20934;&#38598;&#30340;16&#20010;&#23376;&#38598;&#21512;&#65292;&#21253;&#25324;&#20102;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;790&#20010;&#20449;&#24687;&#35831;&#27714;&#65288;&#31867;&#20284;&#20110;&#23545;&#35805;&#26597;&#35810;&#65289;&#65292;&#20197;&#21450;&#27599;&#20010;&#36164;&#28304;&#36820;&#22238;&#30340;&#26368;&#20339;&#32467;&#26524;&#20197;&#21450;&#30456;&#20851;&#30340;LLM-der
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11891v1 Announce Type: cross  Abstract: Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-der
&lt;/p&gt;</description></item><item><title>TriSampler&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20934;&#19977;&#35282;&#21407;&#21017;&#30340;&#36127;&#37319;&#26679;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#31264;&#23494;&#26816;&#32034;&#20013;&#36873;&#25321;&#24615;&#22320;&#37319;&#26679;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#36127;&#20363;</title><link>https://arxiv.org/abs/2402.11855</link><description>&lt;p&gt;
TriSampler: &#19968;&#31181;&#26356;&#22909;&#30340;&#31264;&#23494;&#26816;&#32034;&#36127;&#37319;&#26679;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
TriSampler: A Better Negative Sampling Principle for Dense Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11855
&lt;/p&gt;
&lt;p&gt;
TriSampler&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20934;&#19977;&#35282;&#21407;&#21017;&#30340;&#36127;&#37319;&#26679;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#31264;&#23494;&#26816;&#32034;&#20013;&#36873;&#25321;&#24615;&#22320;&#37319;&#26679;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#36127;&#20363;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11855v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#36127;&#37319;&#26679;&#20316;&#20026;&#31264;&#23494;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#26159;&#35757;&#32451;&#26377;&#25928;&#26816;&#32034;&#27169;&#22411;&#21644;&#26126;&#26174;&#24433;&#21709;&#26816;&#32034;&#24615;&#33021;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22256;&#38590;&#30340;&#36127;&#20363;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#29992;&#20110;&#26500;&#24314;&#36127;&#20505;&#36873;&#39033;&#21644;&#35774;&#35745;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#31264;&#23494;&#26816;&#32034;&#20013;&#30340;&#36127;&#37319;&#26679;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#36825;&#31181;&#25506;&#32034;&#26368;&#32456;&#25581;&#31034;&#20102;&#20934;&#19977;&#35282;&#21407;&#21017;&#65292;&#36825;&#26159;&#19968;&#20010;&#38416;&#26126;&#26597;&#35810;&#12289;&#27491;&#25991;&#26723;&#21644;&#36127;&#25991;&#26723;&#20043;&#38388;&#31867;&#20284;&#19977;&#35282;&#24418;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#19968;&#25351;&#23548;&#21407;&#21017;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TriSampler&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#12290;TriSampler&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#33021;&#22815;&#22312;&#35268;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#36873;&#25321;&#24615;&#22320;&#37319;&#26679;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#36127;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11855v1 Announce Type: new  Abstract: Negative sampling stands as a pivotal technique in dense retrieval, essential for training effective retrieval models and significantly impacting retrieval performance. While existing negative sampling methods have made commendable progress by leveraging hard negatives, a comprehensive guiding principle for constructing negative candidates and designing negative sampling distributions is still lacking. To bridge this gap, we embark on a theoretical analysis of negative sampling in dense retrieval. This exploration culminates in the unveiling of the quasi-triangular principle, a novel framework that elucidates the triangular-like interplay between query, positive document, and negative document. Fueled by this guiding principle, we introduce TriSampler, a straightforward yet highly effective negative sampling method. The keypoint of TriSampler lies in its ability to selectively sample more informative negatives within a prescribed constra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RetPO&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#23545;&#25628;&#32034;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#31526;&#21512;&#30446;&#26631;&#26816;&#32034;&#31995;&#32479;&#30340;&#20559;&#22909;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;RF Collection&#65292;&#29992;&#20110;&#25910;&#38598;&#26816;&#32034;&#32467;&#26524;&#20316;&#20026;&#26816;&#32034;&#22120;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.11827</link><description>&lt;p&gt;
&#35810;&#38382;&#26368;&#20339;&#38382;&#39064;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26816;&#32034;&#22120;&#20559;&#22909;&#22312;&#20250;&#35805;&#25628;&#32034;&#20013;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RetPO&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#23545;&#25628;&#32034;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#31526;&#21512;&#30446;&#26631;&#26816;&#32034;&#31995;&#32479;&#30340;&#20559;&#22909;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;RF Collection&#65292;&#29992;&#20110;&#25910;&#38598;&#26816;&#32034;&#32467;&#26524;&#20316;&#20026;&#26816;&#32034;&#22120;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25628;&#32034;&#19982;&#21333;&#36718;&#26816;&#32034;&#20219;&#21153;&#19981;&#21516;&#65292;&#38656;&#35201;&#29702;&#35299;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#30340;&#24403;&#21069;&#38382;&#39064;&#12290;&#24120;&#35265;&#30340;&#8220;&#37325;&#20889;-&#28982;&#21518;&#26816;&#32034;&#8221;&#30340;&#26041;&#27861;&#26088;&#22312;&#23558;&#38382;&#39064;&#21435;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#20854;&#23545;&#29616;&#25104;&#30340;&#26816;&#32034;&#22120;&#33258;&#32473;&#33258;&#36275;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#21147;&#26377;&#38480;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26469;&#33258;&#26816;&#32034;&#32467;&#26524;&#30340;&#20449;&#21495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;RetPO&#65288;&#26816;&#32034;&#22120;&#20559;&#22909;&#20248;&#21270;&#65289;&#65292;&#26088;&#22312;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#30446;&#26631;&#26816;&#32034;&#31995;&#32479;&#30340;&#37325;&#20889;&#25628;&#32034;&#26597;&#35810;&#30340;&#20559;&#22909;&#12290;&#35813;&#36807;&#31243;&#22987;&#20110;&#25552;&#31034;&#22823;&#22411;LM&#29983;&#25104;&#21508;&#31181;&#28508;&#22312;&#37325;&#20889;&#65292;&#28982;&#21518;&#25910;&#38598;&#36825;&#20123;&#37325;&#20889;&#30340;&#26816;&#32034;&#24615;&#33021;&#20316;&#20026;&#26816;&#32034;&#22120;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#35813;&#36807;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;RF&#22609;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23545;&#36229;&#36807;410K&#20010;&#26597;&#35810;&#30340;&#26816;&#32034;&#22120;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11827v1 Announce Type: cross  Abstract: Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K quer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#27880;&#24847;&#21147;&#31934;&#28860;&#30340;&#25104;&#21151;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#30340;&#25351;&#26631;.</title><link>https://arxiv.org/abs/2402.11794</link><description>&lt;p&gt;
&#25581;&#31034;&#39764;&#27861;&#65306;&#25506;&#31350;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#27880;&#24847;&#21147;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#27880;&#24847;&#21147;&#31934;&#28860;&#30340;&#25104;&#21151;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#30340;&#25351;&#26631;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#22312;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#27880;&#24847;&#21147;&#31934;&#28860;&#65292;&#23427;&#20351;&#29992;&#27880;&#24847;&#21147;&#20998;&#25968;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#19981;&#26159;&#25163;&#21160;&#27880;&#37322;&#30340;&#26597;&#35810;&#25991;&#26723;&#23545;&#12290;&#23613;&#31649;&#27880;&#24847;&#21147;&#31934;&#28860;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#23427;&#25104;&#21151;&#32972;&#21518;&#30340;&#35814;&#32454;&#26426;&#21046;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23427;&#21033;&#29992;&#20197;&#21463;&#30410;&#20110;&#35757;&#32451;&#30340;&#20855;&#20307;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27880;&#24847;&#21147;&#31934;&#28860;&#24037;&#20316;&#27969;&#31243;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#35782;&#21035;&#24433;&#21709;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#36136;&#37327;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21644;&#36991;&#20813;&#20302;&#25928;&#35757;&#32451;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11794v1 Announce Type: new  Abstract: Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35789;&#24178;&#25552;&#21462;&#30340;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11757</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35789;&#24178;&#25552;&#21462;&#65306;&#25215;&#35834;&#12289;&#39118;&#38505;&#21644;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Stemming: Promises, Pitfalls and Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35789;&#24178;&#25552;&#21462;&#30340;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35789;&#24178;&#25552;&#21462;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21333;&#35789;&#32553;&#20943;&#20026;&#20854;&#22522;&#26412;&#24418;&#24335;&#65292;&#20063;&#31216;&#20026;&#26681;&#24418;&#24335;&#12290;&#20256;&#32479;&#30340;&#35789;&#24178;&#25552;&#21462;&#26041;&#27861;&#20165;&#20851;&#27880;&#21333;&#20010;&#26415;&#35821;&#65292;&#24573;&#35270;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20016;&#23500;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#20854;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#26469;&#25552;&#21462;&#35789;&#24178;&#30340;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26435;&#34913;&#26041;&#26696;&#65306;&#65288;1&#65289;&#20351;&#29992;LLMs&#23545;&#38598;&#21512;&#30340;&#35789;&#27719;&#36827;&#34892;&#25552;&#21462;&#65292;&#21363;&#20986;&#29616;&#22312;&#38598;&#21512;&#20013;&#30340;&#21807;&#19968;&#21333;&#35789;&#38598;&#21512;&#65288;&#35789;&#27719;&#25552;&#21462;&#65289;&#65292;&#65288;2&#65289;&#23558;LLMs&#29992;&#20110;&#21333;&#29420;&#35789;&#26681;&#25552;&#21462; (&#19978;&#19979;&#25991;&#25552;&#21462;)&#65292;(3) &#20351;&#29992;LLMs&#20174;&#27599;&#20010;&#25991;&#26723;&#20013;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11757v1 Announce Type: cross  Abstract: Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from eac
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#22120;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#22312;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#32467;&#21512;LLM&#25512;&#26029;&#29992;&#25143;&#23545;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#20559;&#22909;&#24182;&#23558;&#22686;&#24378;&#30340;&#35757;&#32451;&#20449;&#21495;&#32435;&#20837;&#25512;&#33616;&#27169;&#22411;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.11724</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#30340;&#25968;&#25454;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Data Augmenters for Cold-Start Item Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11724
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#22120;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#22312;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#32467;&#21512;LLM&#25512;&#26029;&#29992;&#25143;&#23545;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#20559;&#22909;&#24182;&#23558;&#22686;&#24378;&#30340;&#35757;&#32451;&#20449;&#21495;&#32435;&#20837;&#25512;&#33616;&#27169;&#22411;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11724v1 &#20844;&#21578;&#31867;&#22411;: &#26032;  &#25688;&#35201;: LLM&#30340;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#21644;&#29289;&#21697;&#29305;&#24449;&#65292;&#20026;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21069;&#26223;&#12290;&#34429;&#28982;&#22312;&#29992;&#25143;&#29289;&#21697;&#20132;&#20114;&#20016;&#23500;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#27809;&#26377;&#21382;&#21490;&#20132;&#20114;&#30340;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLM&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#22120;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24357;&#21512;&#23545;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#26681;&#25454;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#26032;&#29289;&#21697;&#25551;&#36848;&#25512;&#26029;&#29992;&#25143;&#23545;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#20559;&#22909;&#12290;&#28982;&#21518;&#23558;&#22686;&#24378;&#30340;&#35757;&#32451;&#20449;&#21495;&#36890;&#36807;&#36741;&#21161;&#37197;&#23545;&#25439;&#22833;&#32435;&#20837;&#21040;&#23398;&#20064;&#19979;&#28216;&#25512;&#33616;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#22312;&#20844;&#20849;&#20122;&#39532;&#36874;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;LLM&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11724v1 Announce Type: new  Abstract: The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in c
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#25628;&#32034;&#24341;&#25806;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20854;&#21487;&#33021;&#23548;&#33268;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20449;&#24687;&#26469;&#28304;&#19981;&#36879;&#26126;&#21644;&#20869;&#23481;&#27491;&#30830;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11707</link><description>&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#22312;ChatGPT&#20043;&#21518;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#38477;&#20302;&#25628;&#32034;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11707
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#25628;&#32034;&#24341;&#25806;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20854;&#21487;&#33021;&#23548;&#33268;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20449;&#24687;&#26469;&#28304;&#19981;&#36879;&#26126;&#21644;&#20869;&#23481;&#27491;&#30830;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#30340;&#21457;&#23637;&#24615;&#36136;&#65292;&#22240;&#20026;&#23427;&#20204;&#24320;&#22987;&#29983;&#25104;&#12289;&#32034;&#24341;&#21644;&#20998;&#21457;&#30001;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21019;&#24314;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#31361;&#20986;&#20102;&#22312;GenAI&#25972;&#21512;&#30340;&#26089;&#26399;&#38454;&#27573;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22260;&#32469;&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;GenAI&#20135;&#29983;&#30340;&#36755;&#20986;&#24102;&#26469;&#20102;&#26080;&#31471;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#38477;&#20302;&#20102;&#36879;&#26126;&#24230;&#21644;&#20449;&#24687;&#26469;&#28304;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25628;&#32034;&#24341;&#25806;&#24050;&#32463;&#29992;&#21547;&#26377;&#38169;&#35823;&#30340;&#29983;&#25104;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#65292;&#36827;&#19968;&#27493;&#27169;&#31946;&#20102;&#20449;&#24687;&#26469;&#28304;&#65292;&#24433;&#21709;&#20102;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#25152;&#26377;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#38477;&#20302;&#25628;&#32034;&#24341;&#25806;&#30340;&#21487;&#38752;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26410;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11707v1 Announce Type: cross  Abstract: In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI). Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases. We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability. Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem. We argue how all these factors could reduce the reliability of search engines. Finally, we summarize some of the active research directions and open questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#20449;&#24687;&#30340;&#26032;&#22411;&#22522;&#20110;&#20551;&#29992;&#25143;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#25191;&#34892;&#25512;&#24191;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.11637</link><description>&lt;p&gt;
&#29992;&#20551;&#29992;&#25143;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Poisoning Federated Recommender Systems with Fake Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#20449;&#24687;&#30340;&#26032;&#22411;&#22522;&#20110;&#20551;&#29992;&#25143;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#25191;&#34892;&#25512;&#24191;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#29992;&#20363;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#20174;&#29992;&#25143;&#21040;&#26381;&#21153;&#22120;&#31471;&#30340;&#28431;&#27934;&#12290;&#27602;&#21270;&#25915;&#20987;&#22312;&#29992;&#25143;&#31471;&#25915;&#20987;&#20013;&#29305;&#21035;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#21442;&#19982;&#32773;&#19978;&#20256;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#26469;&#27450;&#39575;&#20840;&#23616;&#27169;&#22411;&#65292;&#36890;&#24120;&#24847;&#22270;&#25552;&#21319;&#25110;&#38477;&#20302;&#29305;&#23450;&#30446;&#26631;&#39033;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#25191;&#34892;&#25512;&#24191;&#25915;&#20987;&#30340;&#31574;&#30053;&#12290;&#24403;&#21069;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#30495;&#23454;&#29992;&#25143;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#25110;&#29289;&#21697;&#27969;&#34892;&#24230;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#24456;&#38590;&#33719;&#24471;&#36825;&#20123;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#31181;&#25915;&#20987;&#65292;&#38500;&#20102;&#20174;&#26381;&#21153;&#22120;&#33719;&#21462;&#30340;&#29289;&#21697;&#23884;&#20837;&#20043;&#22806;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonFRS&#30340;&#26032;&#22411;&#22522;&#20110;&#20551;&#29992;&#25143;&#30340;&#27602;&#21270;&#25915;&#20987;&#65292;&#29992;&#20110;&#20419;&#38144;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11637v1 Announce Type: cross  Abstract: Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems.   Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20132;&#20114;&#24335;&#26381;&#35013;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#25512;&#33616;&#26381;&#35013;&#21516;&#26102;&#23454;&#26102;&#26500;&#24314;&#29992;&#25143;&#36164;&#26009;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;Agent&#21560;&#25910;&#29992;&#25143;&#21453;&#39304;&#25913;&#21892;&#25512;&#33616;&#65292;&#33021;&#22815;&#26368;&#22823;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11627</link><description>&lt;p&gt;
&#29992;&#25143;&#21442;&#19982;&#30340;&#20132;&#20114;&#24335;&#26381;&#35013;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Interactive Garment Recommendation with User in the Loop
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20132;&#20114;&#24335;&#26381;&#35013;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#25512;&#33616;&#26381;&#35013;&#21516;&#26102;&#23454;&#26102;&#26500;&#24314;&#29992;&#25143;&#36164;&#26009;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;Agent&#21560;&#25910;&#29992;&#25143;&#21453;&#39304;&#25913;&#21892;&#25512;&#33616;&#65292;&#33021;&#22815;&#26368;&#22823;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#26102;&#23578;&#29289;&#21697;&#36890;&#24120;&#21033;&#29992;&#20016;&#23500;&#30340;&#29992;&#25143;&#36164;&#26009;&#65292;&#24182;&#26681;&#25454;&#36807;&#21435;&#30340;&#21382;&#21490;&#21644;&#20197;&#21069;&#30340;&#36141;&#20080;&#20570;&#20986;&#26377;&#38024;&#23545;&#24615;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#20551;&#35774;&#23545;&#29992;&#25143;&#27809;&#26377;&#20808;&#21069;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#38598;&#25104;&#29992;&#25143;&#21453;&#39304;&#23454;&#26102;&#26500;&#24314;&#29992;&#25143;&#36164;&#26009;&#65292;&#22312;&#25512;&#33616;&#20114;&#34917;&#29289;&#21697;&#20197;&#32452;&#25104;&#26381;&#35013;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;Agent&#65292;&#33021;&#22815;&#24314;&#35758;&#21512;&#36866;&#30340;&#26381;&#35013;&#24182;&#21560;&#25910;&#29992;&#25143;&#21453;&#39304;&#20197;&#25913;&#21892;&#25512;&#33616;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#27714;&#21161;&#20110;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#27169;&#25311;&#33719;&#24471;&#29992;&#25143;&#21453;&#39304;&#12290;&#25105;&#20204;&#22312;IQON3000&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Agent&#33021;&#22815;&#36890;&#36807;&#32771;&#34385;&#20010;&#20154;&#20559;&#22909;&#26469;&#25913;&#21892;&#20854;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;&#20110;&#26080;&#24378;&#21270;&#30340;&#27169;&#22411;&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#65292;&#20182;&#20204;&#26080;&#27861;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11627v1 Announce Type: cross  Abstract: Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MetaRAG&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#19982;&#20803;&#35748;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#35748;&#30693;&#35843;&#33410;&#27969;&#31243;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#30417;&#35270;&#12289;&#35780;&#20272;&#21644;&#35268;&#21010;&#20854;&#21709;&#24212;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#20854;&#20869;&#30465;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11626</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Retrieval-Augmented Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MetaRAG&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#19982;&#20803;&#35748;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#35748;&#30693;&#35843;&#33410;&#27969;&#31243;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#30417;&#35270;&#12289;&#35780;&#20272;&#21644;&#35268;&#21010;&#20854;&#21709;&#24212;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#20854;&#20869;&#30465;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#29983;&#25104;&#20107;&#23454;&#20869;&#23481;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#12290; &#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#21333;&#27425;&#26816;&#32034;&#65292;&#32780;&#26368;&#36817;&#26356;&#20542;&#21521;&#20110;&#22810;&#27425;&#26816;&#32034;&#20197;&#25191;&#34892;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#21463;&#21040;&#39044;&#23450;&#20041;&#25512;&#29702;&#27493;&#39588;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MetaRAG&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#19982;&#20803;&#35748;&#30693;&#30340;&#26041;&#27861;&#12290; &#20511;&#37492;&#35748;&#30693;&#24515;&#29702;&#23398;&#65292;&#20803;&#35748;&#30693;&#20351;&#23454;&#20307;&#33021;&#22815;&#33258;&#25105;&#21453;&#24605;&#24182;&#25209;&#21028;&#24615;&#35780;&#20272;&#20854;&#35748;&#30693;&#36807;&#31243;&#12290; &#36890;&#36807;&#25972;&#21512;&#36825;&#19968;&#28857;&#65292;MetaRAG&#20351;&#27169;&#22411;&#33021;&#22815;&#30417;&#35270;&#12289;&#35780;&#20272;&#21644;&#35268;&#21010;&#20854;&#21709;&#24212;&#31574;&#30053;&#65292;&#22686;&#24378;&#20854;&#20869;&#30465;&#25512;&#29702;&#33021;&#21147;&#12290; &#36890;&#36807;&#19977;&#27493;&#20803;&#35748;&#30693;&#35843;&#33410;&#27969;&#31243;&#65292;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21021;&#22987;&#35748;&#30693;&#21709;&#24212;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#21152;&#20197;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11626v1 Announce Type: new  Abstract: Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38170;&#33410;&#28857;&#30340;&#21327;&#20316;&#37051;&#23621;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21327;&#21516;&#36807;&#28388;&#20013;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.11523</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38170;&#33410;&#28857;&#30340;&#21327;&#20316;&#37051;&#23621;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21327;&#21516;&#36807;&#28388;&#20013;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#26377;&#25928;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#25216;&#26415;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#30095;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24341;&#20837;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#26080;&#24847;&#20013;&#23558;&#30446;&#26631;&#29992;&#25143;/&#39033;&#30446;&#19982;&#20182;&#20204;&#30340;&#21327;&#20316;&#37051;&#23621;&#20998;&#24320;&#65292;&#20174;&#32780;&#38480;&#21046;&#20854;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#38170;&#33410;&#28857;&#30340;&#21327;&#20316;&#37051;&#23621;&#35270;&#20026;&#26368;&#32456;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#27491;&#26679;&#26412;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#24320;&#21457;&#20004;&#20010;&#29420;&#29305;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#32467;&#21512;&#20102;&#30417;&#30563;&#20449;&#21495;&#21644;&#23545;&#27604;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#35270;&#35282;&#20998;&#26512;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#21516;&#26102;&#24433;&#21709;&#26356;&#26032;&#38170;&#33410;&#28857;&#30340;&#23884;&#20837;&#12290;&#36825;&#20123;&#26679;&#26412;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#23427;&#20204;&#19982;&#38170;&#33410;&#28857;&#21644;&#36127;&#26679;&#26412;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11523v1 Announce Type: cross  Abstract: While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings. These samples' impact depends on their similarities to the anchor node and the negative sample
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11480</link><description>&lt;p&gt;
&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pattern-wise Transparent Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11480
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#39034;&#24207;&#25512;&#33616;&#26469;&#35828;&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#20851;&#38190;&#39033;&#30446;&#20316;&#20026;&#20854;&#25512;&#33616;&#32467;&#26524;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#25512;&#33616;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23558;&#25972;&#20010;&#39033;&#30446;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#32780;&#19981;&#21152;&#31579;&#36873;&#30340;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTSR&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#23427;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20316;&#20026;&#25972;&#20010;&#25512;&#33616;&#36807;&#31243;&#30340;&#21407;&#23376;&#21333;&#20803;&#12290;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#24471;&#21040;&#37327;&#21270;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#21152;&#26435;&#26657;&#27491;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#30495;&#23454;&#20851;&#38190;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23398;&#20064;&#27169;&#24335;&#30340;&#36129;&#29486;&#12290;&#26368;&#32456;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20250;&#35805;&#33258;&#36866;&#24212;&#20256;&#25773;&#26469;&#36328;&#19981;&#21516;&#20250;&#35805;&#25429;&#33719;&#20840;&#23616;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.11302</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#33258;&#36866;&#24212;&#20256;&#25773;&#20250;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph-based Session Recommendation with Adaptive Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20250;&#35805;&#33258;&#36866;&#24212;&#20256;&#25773;&#26469;&#36328;&#19981;&#21516;&#20250;&#35805;&#25429;&#33719;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#65288;SBRSs&#65289;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#27963;&#21160;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20114;&#21160;&#39033;&#30446;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;SBRSs&#22312;&#27599;&#20010;&#20250;&#35805;&#20013;&#20165;&#23616;&#37096;&#25429;&#33719;&#36141;&#20080;&#24847;&#22270;&#65292;&#20294;&#36328;&#19981;&#21516;&#20250;&#35805;&#25429;&#33719;&#39033;&#30446;&#30340;&#20840;&#23616;&#20449;&#24687;&#23545;&#20110;&#34920;&#24449;&#20854;&#19968;&#33324;&#29305;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#37319;&#29992;&#20250;&#35805;&#33258;&#36866;&#24212;&#20256;&#25773;&#65292;&#20197;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11302v1 Announce Type: new  Abstract: Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing graphs and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction. To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation. Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items' neighbor information considering user intention within
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#35282;&#24230;&#20998;&#26512;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38236;&#20687;&#26799;&#24230;&#65288;MG&#65289;&#30340;&#26799;&#24230;&#31574;&#30053;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32531;&#35299;&#26469;&#33258;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#30340;&#19981;&#31283;&#23450;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11262</link><description>&lt;p&gt;
&#38236;&#20687;&#26799;&#24230;&#65306;&#36890;&#36807;&#25506;&#32034;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#23454;&#29616;&#40065;&#26834;&#30340;&#22810;&#27169;&#24335;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#35282;&#24230;&#20998;&#26512;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38236;&#20687;&#26799;&#24230;&#65288;MG&#65289;&#30340;&#26799;&#24230;&#31574;&#30053;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32531;&#35299;&#26469;&#33258;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#30340;&#19981;&#31283;&#23450;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21644;&#29289;&#21697;&#29305;&#24449;&#65292;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#31526;&#21512;&#20854;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25972;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#21487;&#20197;&#32531;&#35299;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#21516;&#26102;&#20250;&#25918;&#22823;&#26469;&#33258;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#30340;&#26576;&#20123;&#39118;&#38505;&#65292;&#22914;&#20449;&#24687;&#35843;&#25972;&#39118;&#38505;&#21644;&#22266;&#26377;&#22122;&#22768;&#39118;&#38505;&#12290;&#36825;&#20123;&#39118;&#38505;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26032;&#39062;&#35270;&#35282;&#20998;&#26512;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#26799;&#24230;&#31574;&#30053;&#65292;&#31216;&#20026;&#38236;&#20687;&#26799;&#24230;&#65288;MG&#65289;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#38544;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32531;&#35299;&#30001;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11262v1 Announce Type: cross  Abstract: Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evide
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11203</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11203
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;ChatGPT&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#65292;&#21560;&#24341;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;ChatGPT&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#21019;&#26032;&#65292;&#32780;&#21478;&#19968;&#20123;&#20154;&#23558;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#20135;&#21697;&#24320;&#21457;&#21644;&#24066;&#22330;&#31574;&#30053;&#30340;&#26377;&#25928;&#25972;&#21512;&#12290;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#19982;OpenAI&#30340;GPT-4&#19968;&#36215;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;AI&#30340;&#26032;&#38454;&#27573;&#65292;&#20135;&#29983;&#30340;&#20869;&#23481;&#19982;&#35757;&#32451;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;GPT-3&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;ChatGPT&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#20851;&#20110;&#25991;&#26412;&#36136;&#37327;&#20445;&#35777;&#12289;&#27169;&#22411;&#20559;&#24046;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;ChatGPT&#23545;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#20811;&#26381;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#36827;&#34892;&#26597;&#35810;&#37325;&#26500;&#26102;&#38754;&#20020;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;&#37325;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.11202</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#23454;&#29616;&#26597;&#35810;&#37325;&#26500;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Scalability and Extensibility of Query Reformulation Modeling in E-commerce Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#20811;&#26381;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#36827;&#34892;&#26597;&#35810;&#37325;&#26500;&#26102;&#38754;&#20020;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;&#37325;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39038;&#23458;&#34892;&#20026;&#25968;&#25454;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#35265;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20851;&#32852;&#30340;&#34892;&#20026;&#25968;&#25454;&#24448;&#24448;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#26080;&#27861;&#20026;&#25628;&#32034;&#26426;&#21046;&#25552;&#20379;&#36275;&#22815;&#30340;&#25903;&#25345;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#26597;&#35810;&#37325;&#26500;&#30340;&#27010;&#24565;&#12290;&#23427;&#24314;&#35758;&#23569;&#35265;&#26597;&#35810;&#21487;&#20197;&#21033;&#29992;&#20855;&#26377;&#31867;&#20284;&#21547;&#20041;&#30340;&#28909;&#38376;&#23545;&#24212;&#26597;&#35810;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#37325;&#26500;&#24050;&#26174;&#31034;&#20986;&#22312;&#25552;&#39640;&#25628;&#32034;&#30456;&#20851;&#24615;&#21644;&#22686;&#24378;&#25972;&#20307;&#25910;&#20837;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#27969;&#37327;&#36739;&#20302;&#19988;&#22797;&#26434;&#30340;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36816;&#33829;&#30340;&#36739;&#23567;&#25110;&#26032;&#20852;&#20225;&#19994;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;&#37325;&#26500;&#26041;&#26696;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#21363;&#20351;&#38754;&#23545;&#26377;&#38480;&#30340;&#20132;&#26131;&#37327;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11202v1 Announce Type: new  Abstract: Customer behavioral data significantly impacts e-commerce search systems. However, in the case of less common queries, the associated behavioral data tends to be sparse and noisy, offering inadequate support to the search mechanism. To address this challenge, the concept of query reformulation has been introduced. It suggests that less common queries could utilize the behavior patterns of their popular counterparts with similar meanings. In Amazon product search, query reformulation has displayed its effectiveness in improving search relevance and bolstering overall revenue. Nonetheless, adapting this method for smaller or emerging businesses operating in regions with lower traffic and complex multilingual settings poses the challenge in terms of scalability and extensibility. This study focuses on overcoming this challenge by constructing a query reformulation solution capable of functioning effectively, even when faced with limited tra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;QA&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#27169;&#22359;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#20449;&#24687;&#25552;&#21462;&#31867;&#22411;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#33394;&#24182;&#33021;&#26377;&#25928;&#24212;&#23545;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.11177</link><description>&lt;p&gt;
&#22522;&#20110;&#38382;&#31572;&#30340;&#20840;&#38754;&#20013;&#25991;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20449;&#24687;&#25552;&#21462;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;QA&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#27169;&#22359;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#20449;&#24687;&#25552;&#21462;&#31867;&#22411;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#33394;&#24182;&#33021;&#26377;&#25928;&#24212;&#23545;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#23545;&#30740;&#31350;&#21644;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#26041;&#24335;&#65292;&#38382;&#31572;&#65288;QA&#65289;&#21487;&#20197;&#25552;&#21462;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#28789;&#27963;&#30340;&#20449;&#24687;&#65292;&#19988;&#26356;&#26131;&#20110;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#65292;&#20294;&#20854;&#36827;&#23637;&#21463;&#21040;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;QA&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#38598;&#25104;&#20102;&#19968;&#20010;&#39044;&#22788;&#29702;&#27169;&#22359;&#65292;&#22788;&#29702;&#20102;&#19982;&#25552;&#21462;&#22411;QA&#26694;&#26550;&#19981;&#22826;&#20860;&#23481;&#30340;&#25552;&#21462;&#31867;&#22411;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20855;&#26377;&#19981;&#36830;&#32493;&#31572;&#26696;&#21644;&#22810;&#23545;&#19968;&#20851;&#31995;&#30340;&#24773;&#20917;&#12290;&#25152;&#24471;&#30340;QA&#27169;&#22411;&#22312;EHRs&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#21253;&#21547;&#26159;&#38750;&#38382;&#39064;&#30340;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#26696;&#20363;&#30740;&#31350;&#21644;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11177v1 Announce Type: new  Abstract: Electronic health records (EHRs) hold significant value for research and applications. As a new way of information extraction, question answering (QA) can extract more flexible information than conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. In this paper, we propose a novel approach that automatically generates training data for transfer learning of QA models. Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive QA frameworks, including cases with discontinuous answers and many-to-one relationships. The obtained QA model exhibits excellent performance on subtasks of information extraction in EHRs, and it can effectively handle few-shot or zero-shot settings involving yes-no questions. Case studies and ablation studies demonstrate the necessity of each component in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#31995;&#32479;&#20998;&#31867;&#24182;&#35752;&#35770;&#20102;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.11143</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#35843;&#26597;&#19982;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Foundation Models for Recommender Systems: A Survey and New Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#31995;&#32479;&#20998;&#31867;&#24182;&#35752;&#35770;&#20102;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20973;&#20511;&#20854;&#24191;&#27867;&#30340;&#30693;&#35782;&#24211;&#21644;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26426;&#20250;&#12290; &#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#23457;&#35270;&#22522;&#20110;FM&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;FM4RecSys&#65289;&#12290; &#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;FM4RecSys&#30340;&#30740;&#31350;&#32972;&#26223;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;FM4RecSys&#30740;&#31350;&#24037;&#20316;&#30340;&#31995;&#32479;&#20998;&#31867;&#65292;&#21487;&#20998;&#20026;&#25968;&#25454;&#29305;&#24449;&#65292;&#34920;&#31034;&#23398;&#20064;&#65292;&#27169;&#22411;&#31867;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#22235;&#20010;&#19981;&#21516;&#37096;&#20998;&#12290; &#22312;&#27599;&#20010;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20851;&#38190;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#27010;&#36848;&#20102;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;FM4RecSys&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#26426;&#36935;&#65292;&#26088;&#22312;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21551;&#31034;&#12290; &#24635;&#20043;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#30740;&#31350;&#32467;&#26524;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11143v1 Announce Type: new  Abstract: Recently, Foundation Models (FMs), with their extensive knowledge bases and complex architectures, have offered unique opportunities within the realm of recommender systems (RSs). In this paper, we attempt to thoroughly examine FM-based recommendation systems (FM4RecSys). We start by reviewing the research background of FM4RecSys. Then, we provide a systematic taxonomy of existing FM4RecSys research works, which can be divided into four different parts including data characteristics, representation learning, model type, and downstream tasks. Within each part, we review the key recent research developments, outlining the representative models and discussing their characteristics. Moreover, we elaborate on the open problems and opportunities of FM4RecSys aiming to shed light on future research directions in this area. In conclusion, we recap our findings and discuss the emerging trends in this field.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11060</link><description>&lt;p&gt;
Persona-DB&#65306;&#29992;&#20110;&#21709;&#24212;&#39044;&#27979;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#19982;&#21327;&#21516;&#25968;&#25454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11060
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20010;&#24615;&#21270;&#20132;&#20114;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#35782;&#21035;&#29992;&#25143;&#24847;&#35265;&#21644;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#26816;&#32034;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#31574;&#30053;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#29992;&#25143;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26816;&#32034;&#38454;&#27573;&#65292;&#24182;&#23545;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#36825;&#26159;&#20010;&#24615;&#21270;&#31561;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20110;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#65292;&#20197;&#20415;&#22312;LLM&#23450;&#21046;&#30340;&#24773;&#22659;&#19979;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Persona-DB&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#20998;&#23618;&#26500;&#24314;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#36328;&#20219;&#21153;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
&lt;/p&gt;</description></item><item><title>DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2402.11035</link><description>&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65306;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#26159;&#21542;&#22312;&#26816;&#32034;&#20013;&#65311;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11035
&lt;/p&gt;
&lt;p&gt;
DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65288;DPR&#65289;&#26159;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#20013;&#30340;&#31532;&#19968;&#27493;&#12290; DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#12290;&#23545;DPR&#24494;&#35843;&#30340;&#28145;&#20837;&#29702;&#35299;&#23558;&#38656;&#35201;&#20174;&#26681;&#26412;&#19978;&#37322;&#25918;&#35813;&#26041;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#12289;&#23618;&#28608;&#27963;&#20998;&#26512;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#32452;&#21512;&#65292;&#26426;&#26800;&#22320;&#25506;&#32034;&#20102;DPR&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPR&#35757;&#32451;&#20351;&#32593;&#32476;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#21435;&#20013;&#24515;&#21270;&#65292;&#21019;&#24314;&#20102;&#35775;&#38382;&#30456;&#21516;&#20449;&#24687;&#30340;&#22810;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#36825;&#31181;&#35757;&#32451;&#39118;&#26684;&#30340;&#23616;&#38480;&#24615;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#38480;&#21046;&#20102;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26816;&#32034;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23494;&#38598;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#26041;&#21521;&#65306;&#65288;1&#65289;&#26292;&#38706;DPR&#35757;&#32451;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#24615;&#33021;&#19981;&#24179;&#34913;&#21644;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.09959</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLM-based Federated Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09959
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#24615;&#33021;&#19981;&#24179;&#34913;&#21644;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#23637;&#31034;&#20102;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20855;&#22791;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#38656;&#35201;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#65292;&#36825;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#21253;&#21547;&#20102;&#25935;&#24863;&#29992;&#25143;&#20449;&#24687;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#24847;&#22806;&#27844;&#38706;&#21487;&#33021;&#20405;&#29359;&#25968;&#25454;&#20445;&#25252;&#27861;&#65292;&#24182;&#24341;&#21457;&#20262;&#29702;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#65288;Fed4Rec&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;Fed4Rec&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#23458;&#25143;&#31471;&#24615;&#33021;&#19981;&#24179;&#34913;&#21152;&#21095;&#65292;&#24433;&#21709;&#31995;&#32479;&#30340;&#25928;&#29575;&#65307;&#20854;&#27425;&#65292;&#23545;&#20110;&#26412;&#22320;&#35757;&#32451;&#21644;&#25512;&#29702;LLM&#65292;&#23545;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09959v1 Announce Type: new  Abstract: Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.09784</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#25509;&#36817;&#24230;&#19978;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#25429;&#25417;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#21333;&#21521;&#21644;&#21452;&#21521;&#27169;&#24335;&#65292;&#20294;&#23545;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#22914;&#20010;&#20307;&#34892;&#20026;&#21644;&#31038;&#20250;&#36235;&#21183;&#27169;&#24335;&#65292;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#31867;&#20284;&#30340;&#26102;&#38388;&#27573;&#20869;&#38544;&#21547;&#22312;&#29992;&#25143;&#20043;&#38388;&#21457;&#29983;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22402;&#30452;&#26102;&#38388;&#25509;&#36817;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#36866;&#24212;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#36866;&#24212;&#22312;&#32771;&#34385;&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#20363;&#22914;&#21306;&#20998;&#22312;&#19968;&#21608;&#20869;&#19982;&#19968;&#20010;&#26376;&#20869;&#36141;&#20080;&#30340;&#36830;&#32493;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.07092</link><description>&lt;p&gt;
&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#21033;&#29992;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#26469;&#26816;&#32034;&#30456;&#20851;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#23558;&#23545;&#35805;&#35270;&#20026;&#19968;&#31995;&#21015;&#22266;&#23450;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24573;&#35270;&#20102;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064; - &#20063;&#23601;&#26159;&#35828;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#65292;&#32780;&#36825;&#20123;&#22791;&#36873;&#23545;&#35805;&#26159;&#26410;&#35760;&#24405;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#30340;&#26694;&#26550;(ConvAug)&#12290;ConvAug&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#20197;&#25429;&#25417;&#23545;&#35805;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#30340;&#27491;&#20363;&#12289;&#36127;&#20363;&#21644;&#24187;&#35273;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#22797;&#26434;&#23545;&#35805;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;Fr\'echet&#36317;&#31163;&#26469;&#35780;&#20272;&#31232;&#30095;&#26631;&#31614;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23569;&#37327;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;Fr\'echet&#36317;&#31163;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2401.17543</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#26631;&#31614;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#31163;&#32447;&#35780;&#20272;&#30340;Fr\'echet&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Fr\'echet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;Fr\'echet&#36317;&#31163;&#26469;&#35780;&#20272;&#31232;&#30095;&#26631;&#31614;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23569;&#37327;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;Fr\'echet&#36317;&#31163;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;(IR)&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23545;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#21644;&#20934;&#30830;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20351;&#29992;&#31232;&#30095;&#26631;&#31614;&#35780;&#20272;IR&#31995;&#32479;&#65292;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#21463;&#23558;Fr\'echet Inception Distance (FID)&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;Fr\'echet&#36317;&#31163;&#26469;&#34913;&#37327;&#30456;&#20851;&#34987;&#21028;&#23450;&#39033;&#21644;&#26816;&#32034;&#32467;&#26524;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;MS MARCO V1&#25968;&#25454;&#38598;&#21644;TREC&#28145;&#24230;&#23398;&#20064;&#36712;&#36857;&#26597;&#35810;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;Fr\'echet&#36317;&#31163;&#20316;&#20026;&#35780;&#20272;IR&#31995;&#32479;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#37327;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of natural language processing, information retrieval (IR), computer vision, and other technologies has presented significant challenges in evaluating the performance of these systems. One of the main challenges is the scarcity of human-labeled data, which hinders the fair and accurate assessment of these systems. In this work, we specifically focus on evaluating IR systems with sparse labels, borrowing from recent research on evaluating computer vision tasks. taking inspiration from the success of using Fr\'echet Inception Distance (FID) in assessing text-to-image generation systems. We propose leveraging the Fr\'echet Distance to measure the distance between the distributions of relevant judged items and retrieved results. Our experimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query sets demonstrate the effectiveness of the Fr\'echet Distance as a metric for evaluating IR systems, particularly in settings where a few labels are available. 
&lt;/p&gt;</description></item><item><title>RecDCL&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#25512;&#33616;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25209;&#27425;&#23545;&#27604;&#23398;&#20064;&#65288;BCL&#65289;&#21644;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#20887;&#20313;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21448;&#19981;&#20250;&#38169;&#36807;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2401.15635</link><description>&lt;p&gt;
RecDCL: &#29992;&#20110;&#25512;&#33616;&#30340;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RecDCL: Dual Contrastive Learning for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15635
&lt;/p&gt;
&lt;p&gt;
RecDCL&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#25512;&#33616;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25209;&#27425;&#23545;&#27604;&#23398;&#20064;&#65288;BCL&#65289;&#21644;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#20887;&#20313;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21448;&#19981;&#20250;&#38169;&#36807;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#25366;&#25496;&#21327;&#21516;&#36807;&#28388;&#20013;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#33539;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#25968;&#25454;&#21644;&#22686;&#24378;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#26469;&#35299;&#20915;&#32593;&#32476;&#24179;&#21488;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;CL&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25209;&#27425;&#26041;&#24335;&#23545;&#27604;&#19978;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#32500;&#24230;&#20013;&#30340;&#28508;&#22312;&#35268;&#24459;&#65292;&#36825;&#23548;&#33268;&#22312;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#20887;&#20313;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21516;&#26102;&#21033;&#29992;&#25209;&#27425;&#23545;&#27604;&#23398;&#20064;&#65288;BCL&#65289;&#21644;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;BCL&#21644;FCL&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#32467;&#21512;BCL&#21644;FCL&#26377;&#21161;&#20110;&#28040;&#38500;&#20887;&#20313;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#27704;&#36828;&#19981;&#20250;&#38169;&#36807;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#25512;&#33616;&#26694;&#26550;-- RecDCL&#12290;&#22312;RecDCL&#20013;&#65292;FCL&#30446;&#26631;&#26088;&#22312;&#28040;&#38500; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15635v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) has recently achieved great success in mining the user-item interactions for collaborative filtering. As a major paradigm, contrastive learning (CL) based SSL helps address data sparsity in Web platforms by contrasting the embeddings between raw and augmented data. However, existing CL-based methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature dimension. This leads to redundant solutions during the representation learning of users and items. In this work, we investigate how to employ both batch-wise CL (BCL) and feature-wise CL (FCL) for recommendation. We theoretically analyze the relation between BCL and FCL, and find that combining BCL and FCL helps eliminate redundant solutions but never misses an optimal solution. We propose a dual contrastive learning recommendation framework -- RecDCL. In RecDCL, the FCL objective is designed to eli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#27880;&#20837;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#38598;&#25104;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.10325</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20851;&#27880;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10325
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#27880;&#20837;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#38598;&#25104;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290; Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36935;&#21040;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#38544;&#34255;&#34920;&#31034;&#21464;&#24471;&#31867;&#20284;&#20110;&#26631;&#35760;&#12290; &#22312;SR&#39046;&#22495;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#30456;&#21516;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290; &#25105;&#20204;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#33258;&#27880;&#24847;&#22312;SR&#20013;&#30340;&#20302;&#36890;&#28388;&#27874;&#29305;&#24615;&#65292;&#23548;&#33268;&#20102;&#36807;&#24230;&#24179;&#28369;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation&#65288;BSARec&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469; i&#65289;&#36890;&#36807;&#32771;&#34385;&#32454;&#31890;&#24230;&#30340;&#24207;&#21015;&#27169;&#24335;&#27880;&#20837;&#24402;&#32435;&#20559;&#24046;&#21644; ii&#65289;&#38598;&#25104;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#12290; &#25105;&#20204;&#30340;&#21457;&#29616;&#22312;SR&#39046;&#22495;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#26377;&#26395;&#25645;&#36215;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#65288;CDA&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#29305;&#24449;&#22686;&#24378;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.00078</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#24335;&#29305;&#24449;&#22686;&#24378;&#25552;&#39640;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cross-domain Click-Through Rate Prediction via Explicit Feature Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#65288;CDA&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#29305;&#24449;&#22686;&#24378;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#65288;CDCTR&#65289;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#30456;&#20851;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#25968;&#25454;&#26469;&#24110;&#21161;&#30446;&#26631;&#39046;&#22495;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;CDCTR&#20316;&#21697;&#35774;&#35745;&#20102;&#38544;&#24335;&#30340;&#26041;&#24335;&#26469;&#36328;&#39046;&#22495;&#20256;&#36882;&#30693;&#35782;&#65292;&#27604;&#22914;&#21442;&#25968;&#20849;&#20139;&#65292;&#29992;&#20110;&#35268;&#33539;&#21270;&#30446;&#26631;&#22495;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#26356;&#26377;&#25928;&#22320;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26174;&#24335;&#25216;&#26415;&#26469;&#25552;&#21462;&#29992;&#25143;&#20852;&#36259;&#30693;&#35782;&#24182;&#23558;&#27492;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#35201;&#38754;&#20020;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#36229;&#32423;&#39046;&#22495;&#65292;&#21363;&#19968;&#20010;&#38750;&#24120;&#22823;&#30340;&#28304;&#39046;&#22495;&#65292;&#26469;&#28085;&#30422;&#30446;&#26631;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#29992;&#25143;&#25110;&#39033;&#30446;&#65292;2&#65289;&#25552;&#21462;&#30340;&#29992;&#25143;&#20852;&#36259;&#30693;&#35782;&#26159;&#38745;&#24577;&#30340;&#65292;&#26080;&#35770;&#30446;&#26631;&#22495;&#20013;&#30340;&#22330;&#26223;&#22914;&#20309;&#12290;&#36825;&#20123;&#38480;&#21046;&#28608;&#21169;&#25105;&#20204;&#24320;&#21457;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#26356;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26174;&#24335;&#22320;&#36716;&#31227;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#65288;CDA&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00078v2 Announce Type: replace  Abstract: Cross-domain CTR (CDCTR) prediction is an important research topic that studies how to leverage meaningful data from a related domain to help CTR prediction in target domain. Most existing CDCTR works design implicit ways to transfer knowledge across domains such as parameter-sharing that regularizes the model training in target domain. More effectively, recent researchers propose explicit techniques to extract user interest knowledge and transfer this knowledge to target domain. However, the proposed method mainly faces two issues: 1) it usually requires a super domain, i.e. an extremely large source domain, to cover most users or items of target domain, and 2) the extracted user interest knowledge is static no matter what the context is in target domain. These limitations motivate us to develop a more flexible and efficient technique to explicitly transfer knowledge. In this work, we propose a cross-domain augmentation network (CDA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26102;&#24207;&#32593;&#32476;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#20013;&#24515;&#24230;&#37327;&#22312;&#36328;&#22269;&#36164;&#37329;&#36716;&#31227;&#20013;&#23454;&#29616;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.14778</link><description>&lt;p&gt;
&#36328;&#22269;&#36164;&#37329;&#36716;&#31227;&#26102;&#24207;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in cross-country money transfer temporal networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26102;&#24207;&#32593;&#32476;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#20013;&#24515;&#24230;&#37327;&#22312;&#36328;&#22269;&#36164;&#37329;&#36716;&#31227;&#20013;&#23454;&#29616;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#26102;&#24207;&#32593;&#32476;&#20998;&#26512;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#35768;&#22810;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#20381;&#36182;&#35268;&#21017;&#31639;&#27861;&#25110;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26102;&#24207;&#32593;&#32476;&#20013;&#30340;&#28436;&#21464;&#32467;&#26500;&#21644;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#37329;&#34701;&#20132;&#26131;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#31283;&#23450;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#65292;&#20363;&#22914;&#22823;&#22411;&#22269;&#38469;&#37329;&#34701;&#26426;&#26500;&#20013;&#21457;&#29616;&#30340;&#37027;&#20123;&#29983;&#24577;&#31995;&#32479;&#65292;&#21033;&#29992;&#32593;&#32476;&#20013;&#24515;&#24615;&#24230;&#37327;&#26469;&#28145;&#20837;&#20102;&#35299;&#20010;&#20307;&#33410;&#28857;&#12290;&#36890;&#36807;&#30417;&#25511;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#33410;&#28857;&#25490;&#21517;&#30340;&#26102;&#24207;&#28436;&#21464;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#29305;&#23450;&#33410;&#28857;&#35282;&#33394;&#30340;&#31361;&#28982;&#21464;&#21270;&#65292;&#20419;&#20351;&#39046;&#22495;&#19987;&#23478;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14778v2 Announce Type: replace-cross  Abstract: This paper explores anomaly detection through temporal network analysis. Unlike many conventional methods, relying on rule-based algorithms or general machine learning approaches, our methodology leverages the evolving structure and relationships within temporal networks, that can be used to model financial transactions. Focusing on minimal changes in stable ecosystems, such as those found in large international financial institutions, our approach utilizes network centrality measures to gain insights into individual nodes. By monitoring the temporal evolution of centrality-based node rankings, our method effectively identifies abrupt shifts in the roles of specific nodes, prompting further investigation by domain experts.   To demonstrate its efficacy, our methodology is applied in the Anti-Financial Crime (AFC) domain, analyzing a substantial financial dataset comprising over 80 million cross-country wire transfers. The goal 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2311.06318</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#26597;&#35810;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#23427;&#20204;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#30446;&#26631;&#21644;&#30693;&#35782;&#37327;&#23450;&#21046;&#30340;&#29983;&#25104;&#20013;&#21463;&#30410;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#32593;&#32476;&#25628;&#32034;&#65292;&#20102;&#35299;&#29992;&#25143;&#35797;&#22270;&#20570;&#20160;&#20040;&#12289;&#20851;&#24515;&#20160;&#20040;&#20197;&#21450;&#20182;&#20204;&#30693;&#36947;&#20160;&#20040;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;LLM&#20197;&#20010;&#24615;&#21270;&#20854;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#29992;&#25143;&#22312;&#32593;&#32476;&#19978;&#30340;&#25628;&#32034;&#21644;&#27983;&#35272;&#27963;&#21160;&#26500;&#24314;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#23384;&#20648;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20026;LLM&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#25552;&#31034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08699</link><description>&lt;p&gt;
&#20851;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#35013;&#26377;&#25668;&#20687;&#22836;&#30340;&#26174;&#24494;&#38236;&#25110;&#20840;&#25195;&#25551;&#20202;&#33719;&#21462;&#12290;&#21033;&#29992;&#30456;&#20284;&#24615;&#35745;&#31639;&#22522;&#20110;&#36825;&#20123;&#22270;&#20687;&#21305;&#37197;&#24739;&#32773;&#65292;&#22312;&#30740;&#31350;&#21644;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#26368;&#36817;&#25628;&#32034;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;&#32454;&#32990;&#32467;&#26500;&#36827;&#34892;&#24494;&#22937;&#30340;&#37327;&#21270;&#65292;&#20419;&#36827;&#27604;&#36739;&#65292;&#24182;&#22312;&#19982;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#30340;&#30149;&#20363;&#25968;&#25454;&#24211;&#36827;&#34892;&#27604;&#36739;&#26102;&#23454;&#29616;&#20851;&#20110;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#26032;&#24739;&#32773;&#39044;&#27979;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#20197;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06532</link><description>&lt;p&gt;
INTERS: &#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#35299;&#38145;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25628;&#32034;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#19982;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20855;&#20307;&#27010;&#24565;&#30340;&#19981;&#32463;&#24120;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#21521;LLMs&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#20419;&#36827;&#20840;&#38754;&#29702;&#35299;&#21644;&#25191;&#34892;IR&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;LLMs&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;LLMs&#22312;IR&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;3&#20010;&#22522;&#26412;IR&#31867;&#21035;&#20013;&#30340;21&#20010;&#20219;&#21153;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#25991;&#26723;&#29702;&#35299;&#21644;&#26597;&#35810;&#25991;&#26723;&#20851;&#31995;&#29702;&#35299;&#12290;&#25968;&#25454;&#26469;&#33258;43&#20010;&#19981;&#21516;&#30340;&#30001;&#25163;&#21160;&#32534;&#20889;&#30340;&#27169;&#26495;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;INTERS&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
&lt;/p&gt;</description></item><item><title>GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.16716</link><description>&lt;p&gt;
GraphPro: &#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16716
&lt;/p&gt;
&lt;p&gt;
GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#27425;&#28040;&#24687;&#20256;&#36882;&#22312;&#24314;&#27169;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#21644;&#26032;&#21040;&#36798;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphPro&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;GraphPro&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#20020;&#26102;&#25552;&#31034;&#26426;&#21046;&#21644;&#22270;&#32467;&#26500;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21040;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10302</link><description>&lt;p&gt;
&#35299;&#32806;&#35757;&#32451;&#65306;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#21333;&#22810;&#39046;&#22495;&#23398;&#20064;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#37325;&#21472;&#20294;&#38750;&#30456;&#21516;&#30340;&#39046;&#22495;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20559;&#24046;&#21644;&#39046;&#22495;&#20248;&#21183;&#30340;&#25361;&#25112;&#65292;&#20174;&#23545;&#40784;&#20998;&#24067;&#20943;&#23569;&#39046;&#22495;&#24046;&#36317;&#30340;&#35282;&#24230;&#25110;&#36890;&#36807;&#23454;&#26045;&#39046;&#22495;&#29305;&#23450;&#30340;&#22612;&#12289;&#38376;&#29978;&#33267;&#19987;&#23478;&#26469;&#20445;&#30041;&#24046;&#24322;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;MDL&#26041;&#27861;&#12290;MDL&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#25110;&#25439;&#22833;&#20989;&#25968;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#24182;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#12290;D-Train&#26159;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20174;&#19968;&#33324;&#21040;&#29305;&#27530;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#22312;&#25152;&#26377;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#28909;&#36523;&#19968;&#20010;&#26681;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#20854;&#25286;&#20998;&#20026;&#22810;&#20010;&#22836;&#37096;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36890;&#36807;&#22266;&#23450;&#39592;&#24178;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#21644;&#22871;&#39184;&#21040;&#32676;&#32452;&#25512;&#33616;&#36825;&#22235;&#31181;&#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.04247</link><description>&lt;p&gt;
&#32479;&#19968;&#25512;&#33616;&#31995;&#32479;&#65306;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#21644;&#22871;&#39184;&#21040;&#32676;&#32452;&#30340;&#25512;&#33616;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniRecSys: A Unified Framework for Personalized, Group, Package, and Package-to-Group Recommendations. (arXiv:2308.04247v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#21644;&#22871;&#39184;&#21040;&#32676;&#32452;&#25512;&#33616;&#36825;&#22235;&#31181;&#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20026;&#21508;&#31181;&#20135;&#21697;&#21644;&#26381;&#21153;&#25552;&#20379;&#23450;&#21046;&#25512;&#33616;&#26469;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#12290;&#36825;&#20123;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#29992;&#25143;&#23545;&#24179;&#21488;&#30340;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#26045;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#65292;&#20174;&#21521;&#29992;&#25143;&#25110;&#32676;&#32452;&#25512;&#33616;&#39033;&#30446;&#25110;&#22871;&#39184;&#20013;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#23601;&#38656;&#35201;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20180;&#32454;&#25506;&#32034;&#22810;&#20010;&#27169;&#22411;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#23618;&#38754;&#30340;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20010;&#20307;&#27169;&#22411;&#24517;&#39035;&#26681;&#25454;&#19978;&#19979;&#25991;&#23494;&#20999;&#35843;&#25972;&#20854;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#65292;&#20197;&#38450;&#27490;&#20854;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#20135;&#29983;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#25512;&#33616;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20010;&#24615;&#21270;&#12289;&#32676;&#32452;&#12289;&#22871;&#39184;&#25110;&#22871;&#39184;&#21040;&#32676;&#32452;&#25512;&#33616;&#22235;&#20010;&#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems aim to enhance the overall user experience by providing tailored recommendations for a variety of products and services. These systems help users make more informed decisions, leading to greater user satisfaction with the platform. However, the implementation of these systems largely depends on the context, which can vary from recommending an item or package to a user or a group. This requires careful exploration of several models during the deployment, as there is no comprehensive and unified approach that deals with recommendations at different levels. Furthermore, these individual models must be closely attuned to their generated recommendations depending on the context to prevent significant variation in their generated recommendations. In this paper, we propose a novel unified recommendation framework that addresses all four recommendation tasks, namely personalized, group, package, or package-to-group recommendation, filling the gap in the current research lan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SE-PQA&#65288;&#20010;&#24615;&#21270;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#26032;&#36164;&#28304;&#65292;&#35813;&#36164;&#28304;&#21253;&#25324;&#36229;&#36807;1&#30334;&#19975;&#20010;&#26597;&#35810;&#21644;2&#30334;&#19975;&#20010;&#22238;&#31572;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#29305;&#24449;&#27169;&#25311;&#20102;&#27969;&#34892;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#24179;&#21488;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29992;&#20110;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#21487;&#22797;&#29616;&#22522;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16261</link><description>&lt;p&gt;
SE-PQA: &#20010;&#24615;&#21270;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
SE-PQA: Personalized Community Question Answering. (arXiv:2306.16261v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16261
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SE-PQA&#65288;&#20010;&#24615;&#21270;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#26032;&#36164;&#28304;&#65292;&#35813;&#36164;&#28304;&#21253;&#25324;&#36229;&#36807;1&#30334;&#19975;&#20010;&#26597;&#35810;&#21644;2&#30334;&#19975;&#20010;&#22238;&#31572;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#29305;&#24449;&#27169;&#25311;&#20102;&#27969;&#34892;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#24179;&#21488;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29992;&#20110;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#21487;&#22797;&#29616;&#22522;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#21270;&#30340;&#20449;&#24687;&#26816;&#32034;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#28982;&#32570;&#20047;&#39640;&#36136;&#37327;&#12289;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#23637;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#24182;&#35780;&#20272;&#20010;&#24615;&#21270;&#25628;&#32034;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;SE-PQA (StackExchange - &#20010;&#24615;&#21270;&#38382;&#39064;&#22238;&#31572;)&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#31934;&#36873;&#36164;&#28304;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#19982;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#36129;&#29486;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#36229;&#36807;1&#30334;&#19975;&#20010;&#26597;&#35810;&#21644;2&#30334;&#19975;&#20010;&#22238;&#31572;&#65292;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#29305;&#24449;&#26469;&#27169;&#25311;&#19968;&#20010;&#27969;&#34892;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#24179;&#21488;&#30340;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;SE-PQA&#30340;&#29305;&#28857;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#38382;&#39064;&#21644;&#22238;&#31572;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#35813;&#36164;&#28304;&#30340;&#31038;&#21306;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#21487;&#22797;&#29616;&#22522;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#21512;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#32463;&#27982;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#25551;&#36848;&#20102;&#22312;&#26893;&#29289;&#27969;&#34892;&#30149;&#26399;&#38388;&#20892;&#30000;&#20316;&#29289;&#30340;&#32463;&#27982;&#21033;&#28070;&#65292;&#21487;&#20026;&#20892;&#30000;&#20027;&#20154;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2301.02817</link><description>&lt;p&gt;
&#26893;&#29289;&#27969;&#34892;&#30149;&#22312;&#20892;&#30000;&#20013;&#30340;&#25104;&#26412;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Cost-optimal Seeding Strategy During a Botanical Pandemic in Domesticated Fields. (arXiv:2301.02817v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02817
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#32463;&#27982;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#25551;&#36848;&#20102;&#22312;&#26893;&#29289;&#27969;&#34892;&#30149;&#26399;&#38388;&#20892;&#30000;&#20316;&#29289;&#30340;&#32463;&#27982;&#21033;&#28070;&#65292;&#21487;&#20026;&#20892;&#30000;&#20027;&#20154;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26893;&#29289;&#27969;&#34892;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#31918;&#39135;&#30701;&#32570;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26893;&#29289;&#27969;&#34892;&#30149;&#22312;&#30701;&#20013;&#26399;&#20869;&#23558;&#32487;&#32493;&#23384;&#22312;&#65292;&#20892;&#30000;&#20027;&#20154;&#21487;&#20197;&#26681;&#25454;&#31574;&#30053;&#24615;&#22320;&#22312;&#33258;&#24049;&#30340;&#20892;&#30000;&#20013;&#25773;&#31181;&#65292;&#20197;&#20248;&#21270;&#27599;&#19968;&#27425;&#20316;&#29289;&#29983;&#20135;&#30340;&#32463;&#27982;&#21033;&#28070;&#12290;&#30446;&#26631;&#65306;&#37492;&#20110;&#30149;&#21407;&#20307;&#30340;&#27969;&#34892;&#30149;&#23398;&#29305;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20892;&#30000;&#20027;&#20154;&#21644;&#20915;&#31574;&#32773;&#23547;&#25214;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#32463;&#27982;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#34892;&#30149;&#23398;-&#32463;&#27982;&#25968;&#23398;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#22312;&#26893;&#29289;&#27969;&#34892;&#30149;&#26399;&#38388;&#20892;&#30000;&#20316;&#29289;&#30340;&#32463;&#27982;&#21033;&#28070;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#25193;&#23637;&#30340;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#20197;&#21450;&#38750;&#32447;&#24615;&#36755;&#20986;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#26469;&#25551;&#36848;&#27969;&#34892;&#30149;&#23398;&#21160;&#24577;&#12290;&#32467;&#26524;&#21644;&#32467;&#35770;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20892;&#30000;&#21644;&#30149;&#21407;&#20307;&#30340;&#29305;&#24615;&#33719;&#21462;&#26368;&#20248;&#30340;&#32593;&#26684;&#24418;&#25104;&#30340;&#25773;&#31181;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#32463;&#27982;&#21033;&#28070;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#23454;&#26045;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Botanical pandemics cause enormous economic damage and food shortage around the globe. However, since botanical pandemics are here to stay in the short-medium term, domesticated field owners can strategically seed their fields to optimize each session's economic profit. Objective: Given the pathogen's epidemiological properties, we aim to find an economically optimal grid-based seeding strategy for field owners and policymakers. Methods: We propose a novel epidemiological-economic mathematical model that describes the economic profit from a field of plants during a botanical pandemic. We describe the epidemiological dynamics using a spatio-temporal extended Susceptible-Infected-Recovered epidemiological model with a non-linear output epidemiological model. Results and Conclusions: We provide an algorithm to obtain an optimal grid-formed seeding strategy to maximize economic profit, given field and pathogen properties. In addition, we implement the proposed model in realistic s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#29992;&#20110;&#23558;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#21644;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#21508;&#31181;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#21508;&#31181;&#24212;&#29992;&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#35843;&#30740;&#25104;&#26524;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#20063;&#34987;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2212.14464</link><description>&lt;p&gt;
&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#32467;&#26524;&#22810;&#26679;&#21270;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Result Diversification in Search and Recommendation: A Survey. (arXiv:2212.14464v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#29992;&#20110;&#23558;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#21644;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#21508;&#31181;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#21508;&#31181;&#24212;&#29992;&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#35843;&#30740;&#25104;&#26524;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#20063;&#34987;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#36820;&#22238;&#32467;&#26524;&#23545;&#20110;&#28385;&#36275;&#23458;&#25143;&#30340;&#21508;&#31181;&#20852;&#36259;&#21644;&#25552;&#20379;&#32773;&#30340;&#24066;&#22330;&#26333;&#20809;&#26159;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#22810;&#26679;&#21270;&#30740;&#31350;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#20276;&#38543;&#30528;&#23545;&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#30340;&#25991;&#29486;&#22823;&#37327;&#28044;&#29616;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#22810;&#26679;&#21270;&#30740;&#31350;&#32570;&#20047;&#31995;&#32479;&#32452;&#32455;&#65292;&#23384;&#22312;&#29255;&#27573;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#29992;&#20110;&#23558;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#21644;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20004;&#20010;&#39046;&#22495;&#26159;&#26816;&#32034;&#31995;&#32479;&#20013;&#30740;&#31350;&#26368;&#24191;&#27867;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#25105;&#20204;&#20174;&#31616;&#35201;&#35752;&#35770;&#20026;&#20309;&#22810;&#26679;&#24615;&#22312;&#26816;&#32034;&#31995;&#32479;&#20013;&#37325;&#35201;&#24320;&#22987;&#35843;&#30740;&#65292;&#28982;&#21518;&#24635;&#32467;&#20102;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#21508;&#31181;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24046;&#24322;&#12290;&#35843;&#30740;&#30340;&#20027;&#20307;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25551;&#36848;&#29616;&#26377;&#22810;&#26679;&#21270;&#25351;&#26631;&#21644;&#26041;&#27861;&#30340;&#35814;&#32454;&#20869;&#23481;&#65292;&#23637;&#31034;&#20102;&#21508;&#31181;&#24212;&#29992;&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#35843;&#30740;&#25104;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36235;&#21183;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversifying return results is an important research topic in retrieval systems in order to satisfy both the various interests of customers and the equal market exposure of providers. There has been growing attention on diversity-aware research during recent years, accompanied by a proliferation of literature on methods to promote diversity in search and recommendation. However, diversity-aware studies in retrieval systems lack a systematic organization and are rather fragmented. In this survey, we are the first to propose a unified taxonomy for classifying the metrics and approaches of diversification in both search and recommendation, which are two of the most extensively researched fields of retrieval systems. We begin the survey with a brief discussion of why diversity is important in retrieval systems, followed by a summary of the various diversity concerns in search and recommendation, highlighting their relationship and differences. For the survey's main body, we present a unifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#22343;&#21248;&#24615;&#21644;&#23545;&#40784;&#24615;&#29305;&#24615;&#65292;&#22686;&#24378;&#20102;GNNs&#23545;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.11461</link><description>&lt;p&gt;
&#22686;&#24378;GNNs&#30340;&#26102;&#31354;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation. (arXiv:2209.11461v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#22343;&#21248;&#24615;&#21644;&#23545;&#40784;&#24615;&#29305;&#24615;&#65292;&#22686;&#24378;&#20102;GNNs&#23545;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#21033;&#29992;&#29992;&#25143;&#30340;&#30701;&#26399;&#34892;&#20026;&#24207;&#21015;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#65292;&#32780;&#19981;&#38656;&#35201;&#35814;&#32454;&#30340;&#29992;&#25143;&#36164;&#26009;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#23558;&#20250;&#35805;&#35270;&#20026;&#39033;&#30446;&#20043;&#38388;&#30340;&#36716;&#25442;&#22270;&#65292;&#24182;&#21033;&#29992;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23545;&#39033;&#30446;&#21450;&#20854;&#37051;&#23621;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20174;&#31354;&#38388;&#22270;&#32467;&#26500;&#30340;&#35270;&#35282;&#32858;&#21512;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#22312;&#20449;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#39033;&#30446;&#30340;&#37051;&#23621;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#38382;&#39064;&#12290;&#20854;&#20182;&#30340;&#24037;&#20316;&#36890;&#36807;&#25972;&#21512;&#39069;&#22806;&#30340;&#26102;&#38388;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#20043;&#38388;&#20805;&#20998;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#22343;&#21248;&#24615;&#21644;&#23545;&#40784;&#24615;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation (SBR) systems aim to utilize the user's short-term behavior sequence to predict the next item without the detailed user profile. Most recent works try to model the user preference by treating the sessions as between-item transition graphs and utilize various graph neural networks (GNNs) to encode the representations of pair-wise relations among items and their neighbors. Some of the existing GNN-based models mainly focus on aggregating information from the view of spatial graph structure, which ignores the temporal relations within neighbors of an item during message passing and the information loss results in a sub-optimal problem. Other works embrace this challenge by incorporating additional temporal information but lack sufficient interaction between the spatial and temporal patterns. To address this issue, inspired by the uniformity and alignment properties of contrastive learning techniques, we propose a novel framework called Session-based Recommenda
&lt;/p&gt;</description></item></channel></rss>