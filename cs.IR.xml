<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35780;&#20272;&#27573;&#33853;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#23454;&#39564;&#21457;&#29616;&#21463;&#36807;&#33391;&#22909;&#25351;&#23548;&#30340;LLMs&#21487;&#20197;...</title><link>https://arxiv.org/abs/2403.19216</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23454;&#29992;&#24615;&#21028;&#26029;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Good at Utility Judgments?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19216
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35780;&#20272;&#27573;&#33853;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#23454;&#39564;&#21457;&#29616;&#21463;&#36807;&#33391;&#22909;&#25351;&#23548;&#30340;LLMs&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#35748;&#20026;&#26159;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24187;&#35273;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36817;&#26399;&#24050;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#30001;&#20110;&#26816;&#32034;&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;RAG&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;LLMs&#35782;&#21035;&#20855;&#26377;&#23454;&#29992;&#24615;&#30340;&#27573;&#33853;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#35780;&#20272;&#26816;&#32034;&#20013;&#27573;&#33853;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#35780;&#20272;&#25903;&#25345;&#38382;&#31572;&#30340;&#27573;&#33853;&#23454;&#29992;&#24615;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;LLMs&#22312;&#24320;&#25918;&#22495;QA&#23454;&#29992;&#24615;&#35780;&#20272;&#26041;&#38754;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#31243;&#24207;&#21644;&#19981;&#21516;&#29305;&#24449;&#30340;&#20505;&#36873;&#27573;&#33853;&#38598;&#21512;&#65292;&#20419;&#36827;&#20102;&#19982;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;i&#65289;&#21463;&#36807;&#33391;&#22909;&#25351;&#23548;&#30340;LLMs&#21487;&#20197;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19216v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12388</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#23545;&#35805;&#31995;&#32479;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#32780;&#21487;&#35299;&#37322;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#23545;&#20110;&#20102;&#35299;&#12289;&#35780;&#20272;&#21644;&#25345;&#32493;&#25913;&#36827;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#25991;&#26412;&#23884;&#20837;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LLMs&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#20013;&#25552;&#21462;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#21487;&#35299;&#37322;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;LLM&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#25552;&#31034;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#31034;&#20363;&#30340;&#30417;&#30563;&#36827;&#34892;&#29992;&#25143;&#28385;&#24847;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12388v1 Announce Type: cross  Abstract: Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it sco
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02630</link><description>&lt;p&gt;
FedHCDR: &#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;FedHCDR&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#22791;&#21463;&#20851;&#27880;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#25968;&#25454;&#26469;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDR&#26041;&#27861;&#38656;&#35201;&#36328;&#39046;&#22495;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#65292;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;FedCDR&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHCDR&#65292;&#19968;&#31181;&#20855;&#26377;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#36229;&#22270;&#20449;&#21495;&#35299;&#32806;&#65288;HSD&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#29305;&#24449;&#35299;&#32806;&#20026;&#39046;&#22495;&#29420;&#26377;&#21644;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#36229;&#22270;&#28388;&#27874;&#22120;&#26469;&#36827;&#34892;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26576;&#20123;&#25628;&#32034;&#24341;&#25806;&#22312;&#25928;&#29575;&#21644;&#36895;&#24230;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#31934;&#24230;&#36739;&#20302;&#65292;&#32780;&#20854;&#20182;&#25628;&#32034;&#24341;&#25806;&#30340;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#36816;&#34892;&#25928;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.03271</link><description>&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20998;&#26512;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Analysis and Validation of Image Search Engines in Histopathology. (arXiv:2401.03271v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26576;&#20123;&#25628;&#32034;&#24341;&#25806;&#22312;&#25928;&#29575;&#21644;&#36895;&#24230;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#31934;&#24230;&#36739;&#20302;&#65292;&#32780;&#20854;&#20182;&#25628;&#32034;&#24341;&#25806;&#30340;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#36816;&#34892;&#25928;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#32455;&#23398;&#21644;&#30149;&#29702;&#23398;&#22270;&#20687;&#26723;&#26696;&#20013;&#25628;&#32034;&#30456;&#20284;&#22270;&#20687;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#30446;&#30340;&#20013;&#24110;&#21161;&#24739;&#32773;&#21305;&#37197;&#65292;&#20174;&#20998;&#31867;&#21644;&#35786;&#26029;&#21040;&#39044;&#21518;&#21644;&#39044;&#27979;&#12290;&#20840;&#29627;&#29255;&#22270;&#20687;&#26159;&#32452;&#32455;&#26631;&#26412;&#30340;&#39640;&#24230;&#35814;&#32454;&#25968;&#23383;&#34920;&#31034;&#65292;&#21305;&#37197;&#20840;&#29627;&#29255;&#22270;&#20687;&#21487;&#20197;&#20316;&#20026;&#24739;&#32773;&#21305;&#37197;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26412;&#25991;&#23545;&#22235;&#31181;&#25628;&#32034;&#26041;&#27861;&#65292;&#35270;&#35273;&#35789;&#34955;&#65288;BoVW&#65289;&#12289;Yottixel&#12289;SISH&#12289;RetCCL&#21450;&#20854;&#19968;&#20123;&#28508;&#22312;&#21464;&#31181;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#31639;&#27861;&#21644;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#20010;&#20869;&#37096;&#25968;&#25454;&#38598;&#65288;1269&#20301;&#24739;&#32773;&#65289;&#21644;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;1207&#20301;&#24739;&#32773;&#65289;&#65292;&#24635;&#35745;&#36229;&#36807;200,000&#20010;&#23646;&#20110;&#20116;&#20010;&#20027;&#35201;&#37096;&#20301;&#30340;38&#20010;&#19981;&#21516;&#31867;&#21035;/&#20122;&#22411;&#30340;&#22270;&#20687;&#22359;&#12290;&#26576;&#20123;&#25628;&#32034;&#24341;&#25806;&#65292;&#20363;&#22914;BoVW&#65292;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#20294;&#31934;&#24230;&#36739;&#20302;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#25628;&#32034;&#24341;&#25806;&#20363;&#22914;SISH&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#36816;&#34892;&#25928;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for similar images in archives of histology and histopathology images is a crucial task that may aid in patient matching for various purposes, ranging from triaging and diagnosis to prognosis and prediction. Whole slide images (WSIs) are highly detailed digital representations of tissue specimens mounted on glass slides. Matching WSI to WSI can serve as the critical method for patient matching. In this paper, we report extensive analysis and validation of four search methods bag of visual words (BoVW), Yottixel, SISH, RetCCL, and some of their potential variants. We analyze their algorithms and structures and assess their performance. For this evaluation, we utilized four internal datasets ($1269$ patients) and three public datasets ($1207$ patients), totaling more than $200,000$ patches from $38$ different classes/subtypes across five primary sites. Certain search engines, for example, BoVW, exhibit notable efficiency and speed but suffer from low accuracy. Conversely, searc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#22522;&#20110;&#38544;&#24615;&#21453;&#39304;&#30340;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#29992;&#25143;&#35266;&#30475;&#27169;&#24335;&#26469;&#26500;&#24314;&#26377;&#25928;&#30340;&#35270;&#39057;&#25512;&#33616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.12743</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#29992;&#25143;&#35266;&#30475;&#27169;&#24335;&#30340;&#35270;&#39057;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video Recommendation Using Social Network Analysis and User Viewing Patterns. (arXiv:2308.12743v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#22522;&#20110;&#38544;&#24615;&#21453;&#39304;&#30340;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#29992;&#25143;&#35266;&#30475;&#27169;&#24335;&#26469;&#26500;&#24314;&#26377;&#25928;&#30340;&#35270;&#39057;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#28857;&#25773;&#24179;&#21488;&#30340;&#36805;&#29467;&#23835;&#36215;&#65292;&#29992;&#25143;&#38754;&#20020;&#30528;&#20174;&#22823;&#37327;&#20869;&#23481;&#20013;&#31579;&#36873;&#20986;&#19982;&#33258;&#24049;&#21916;&#22909;&#30456;&#31526;&#30340;&#33410;&#30446;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20449;&#24687;&#36807;&#36733;&#30340;&#22256;&#22659;&#65292;&#35270;&#39057;&#28857;&#25773;&#26381;&#21153;&#36234;&#26469;&#36234;&#22810;&#22320;&#21152;&#20837;&#20102;&#21033;&#29992;&#31639;&#27861;&#20998;&#26512;&#29992;&#25143;&#34892;&#20026;&#24182;&#24314;&#35758;&#20010;&#24615;&#21270;&#20869;&#23481;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#20381;&#36182;&#20110;&#29992;&#25143;&#26126;&#30830;&#30340;&#21453;&#39304;&#65292;&#22914;&#35780;&#20998;&#21644;&#35780;&#35770;&#65292;&#20294;&#36825;&#31181;&#21453;&#39304;&#30340;&#25910;&#38598;&#24448;&#24448;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#22312;&#24314;&#31435;&#26377;&#25928;&#30340;&#35270;&#39057;&#25512;&#33616;&#27169;&#22411;&#26041;&#38754;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38544;&#24615;&#21453;&#39304;&#27169;&#24335;&#21487;&#33021;&#25552;&#20379;&#20102;&#19968;&#26465;&#26367;&#20195;&#36884;&#24452;&#65292;&#36991;&#20813;&#20102;&#23545;&#26126;&#30830;&#35780;&#20998;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#23545;&#20110;&#22522;&#20110;&#38544;&#24615;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#35270;&#39057;&#35266;&#30475;&#34892;&#20026;&#26041;&#38754;&#65292;&#23578;&#32570;&#20047;&#36275;&#22815;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the meteoric rise of video-on-demand (VOD) platforms, users face the challenge of sifting through an expansive sea of content to uncover shows that closely match their preferences. To address this information overload dilemma, VOD services have increasingly incorporated recommender systems powered by algorithms that analyze user behavior and suggest personalized content. However, a majority of existing recommender systems depend on explicit user feedback in the form of ratings and reviews, which can be difficult and time-consuming to collect at scale. This presents a key research gap, as leveraging users' implicit feedback patterns could provide an alternative avenue for building effective video recommendation models, circumventing the need for explicit ratings. However, prior literature lacks sufficient exploration into implicit feedback-based recommender systems, especially in the context of modeling video viewing behavior. Therefore, this paper aims to bridge this research gap 
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03972</link><description>&lt;p&gt;
Mixer: &#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixer: Image to Multi-Modal Retrieval Learning for Industrial Application. (arXiv:2305.03972v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#19968;&#30452;&#26159;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#20869;&#23481;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38656;&#27714;&#65292;&#20854;&#20013;&#26597;&#35810;&#26159;&#19968;&#24352;&#22270;&#29255;&#65292;&#25991;&#26723;&#26159;&#20855;&#26377;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#31181;&#26816;&#32034;&#20219;&#21153;&#20173;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixer&#30340;&#26032;&#22411;&#21487;&#20280;&#32553;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#26597;&#35810;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;&#33539;&#24335;&#12290;Mixer&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12289;&#26356;&#39640;&#25928;&#22320;&#25366;&#25496;&#20559;&#26012;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#39640;&#36127;&#36733;&#37327;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval, where the query is an image and the doc is an item with both image and text description, is ubiquitous in e-commerce platforms and content-sharing social media. However, little research attention has been paid to this important application. This type of retrieval task is challenging due to the facts: 1)~domain gap exists between query and doc. 2)~multi-modality alignment and fusion. 3)~skewed training data and noisy labels collected from user behaviors. 4)~huge number of queries and timely responses while the large-scale candidate docs exist. To this end, we propose a novel scalable and efficient image query to multi-modal retrieval learning paradigm called Mixer, which adaptively integrates multi-modality data, mines skewed and noisy data more efficiently and scalable to high traffic. The Mixer consists of three key ingredients: First, for query and doc image, a shared encoder network followed by separate transformation networks are utilized to account for their
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#31639;&#27861;&#20013;&#31435;&#24615;&#20197;&#21450;&#19982;&#31639;&#27861;&#20559;&#35265;&#30340;&#20851;&#31995;&#65292;&#20197;&#25628;&#32034;&#24341;&#25806;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24471;&#20986;&#25628;&#32034;&#20013;&#31435;&#24615;&#26159;&#19981;&#21487;&#33021;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.05103</link><description>&lt;p&gt;
&#31639;&#27861;&#20013;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Algorithmic neutrality. (arXiv:2303.05103v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05103
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31639;&#27861;&#20013;&#31435;&#24615;&#20197;&#21450;&#19982;&#31639;&#27861;&#20559;&#35265;&#30340;&#20851;&#31995;&#65292;&#20197;&#25628;&#32034;&#24341;&#25806;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24471;&#20986;&#25628;&#32034;&#20013;&#31435;&#24615;&#26159;&#19981;&#21487;&#33021;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#24433;&#21709;&#30528;&#36234;&#26469;&#36234;&#22810;&#25484;&#25511;&#25105;&#20204;&#29983;&#27963;&#30340;&#31639;&#27861;&#12290;&#39044;&#27979;&#24615;&#35686;&#21153;&#31995;&#32479;&#38169;&#35823;&#22320;&#39640;&#20272;&#26377;&#33394;&#20154;&#31181;&#31038;&#21306;&#30340;&#29359;&#32618;&#29575;&#65307;&#25307;&#32856;&#31639;&#27861;&#21066;&#24369;&#20102;&#21512;&#26684;&#30340;&#22899;&#24615;&#20505;&#36873;&#20154;&#30340;&#26426;&#20250;&#65307;&#20154;&#33080;&#35782;&#21035;&#36719;&#20214;&#38590;&#20197;&#35782;&#21035;&#40657;&#30382;&#32932;&#30340;&#38754;&#37096;&#12290;&#31639;&#27861;&#20559;&#35265;&#24050;&#32463;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#31639;&#27861;&#20013;&#31435;&#24615;&#21364;&#22522;&#26412;&#34987;&#24573;&#35270;&#20102;&#12290;&#31639;&#27861;&#20013;&#31435;&#24615;&#26159;&#25105;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#25105;&#25552;&#20986;&#20102;&#19977;&#20010;&#38382;&#39064;&#12290;&#31639;&#27861;&#20013;&#31435;&#24615;&#26159;&#20160;&#20040;&#65311;&#31639;&#27861;&#20013;&#31435;&#24615;&#26159;&#21542;&#21487;&#33021;&#65311;&#24403;&#25105;&#20204;&#32771;&#34385;&#31639;&#27861;&#20013;&#31435;&#24615;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#31639;&#27861;&#20559;&#35265;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;&#20026;&#20102;&#20855;&#20307;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#36873;&#25321;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#25628;&#32034;&#24341;&#25806;&#12290;&#20511;&#37492;&#20851;&#20110;&#31185;&#23398;&#20013;&#31435;&#24615;&#30340;&#30740;&#31350;&#65292;&#25105;&#35748;&#20026;&#21482;&#26377;&#24403;&#25628;&#32034;&#24341;&#25806;&#30340;&#25490;&#21517;&#19981;&#21463;&#26576;&#20123;&#20215;&#20540;&#35266;&#30340;&#24433;&#21709;&#26102;&#65292;&#25628;&#32034;&#24341;&#25806;&#25165;&#26159;&#20013;&#31435;&#30340;&#65292;&#27604;&#22914;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#25110;&#25628;&#32034;&#24341;&#25806;&#36816;&#33829;&#21830;&#30340;&#32463;&#27982;&#21033;&#30410;&#12290;&#25105;&#35748;&#20026;&#25628;&#32034;&#20013;&#31435;&#24615;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias infects the algorithms that wield increasing control over our lives. Predictive policing systems overestimate crime in communities of color; hiring algorithms dock qualified female candidates; and facial recognition software struggles to recognize dark-skinned faces. Algorithmic bias has received significant attention. Algorithmic neutrality, in contrast, has been largely neglected. Algorithmic neutrality is my topic. I take up three questions. What is algorithmic neutrality? Is algorithmic neutrality possible? When we have algorithmic neutrality in mind, what can we learn about algorithmic bias? To answer these questions in concrete terms, I work with a case study: search engines. Drawing on work about neutrality in science, I say that a search engine is neutral only if certain values -- like political ideologies or the financial interests of the search engine operator -- play no role in how the search engine ranks pages. Search neutrality, I argue, is impossible. Its impossibili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>