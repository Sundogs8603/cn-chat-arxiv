<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#35821;&#26009;&#35268;&#27169;&#19979;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#25193;&#23637;&#21040;&#21253;&#21547;8.8M&#31687;&#31456;&#30340;MS MARCO&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#39640;&#36798;11B&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#25581;&#31034;&#20986;&#20102;&#22312;&#32034;&#24341;&#26399;&#38388;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#20316;&#20026;&#25991;&#26723;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11841</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#22914;&#20309;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#31687;&#25991;&#31456;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Generative Retrieval Scale to Millions of Passages?. (arXiv:2305.11841v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#35821;&#26009;&#35268;&#27169;&#19979;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#25193;&#23637;&#21040;&#21253;&#21547;8.8M&#31687;&#31456;&#30340;MS MARCO&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#39640;&#36798;11B&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#25581;&#31034;&#20986;&#20102;&#22312;&#32034;&#24341;&#26399;&#38388;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#20316;&#20026;&#25991;&#26723;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#21487;&#24494;&#25628;&#32034;&#32034;&#24341;(Differentiable Search Index)&#25512;&#24191;&#32780;&#26469;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#26032;&#20852;&#33539;&#24335;,&#23558;&#32463;&#20856;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#25918;&#24323;&#20102;&#22806;&#37096;&#32034;&#24341;&#65292;&#24182;&#23558;&#25972;&#20010;&#25991;&#26723;&#35821;&#26009;&#24211;&#32534;&#30721;&#22312;&#21333;&#20010;Transformer&#20013;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#20165;&#22312;&#32422;100k&#30340;&#25991;&#26723;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#26681;&#25454;&#19981;&#21516;&#30340;&#35821;&#26009;&#35268;&#27169;&#36827;&#34892;&#29983;&#25104;&#24335;&#26816;&#32034;&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#26368;&#32456;&#25193;&#23637;&#21040;&#25972;&#20010;8.8M&#31687;&#31456;&#30340;MS MARCO&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#39640;&#36798;11B&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#31687;&#25991;&#31456;&#30340;&#20960;&#20010;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#32034;&#24341;&#26399;&#38388;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#20316;&#20026;&#25991;&#26723;&#34920;&#31034;&#30340;&#26680;&#24515;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#32771;&#34385;&#24182;&#38750;&#29616;&#26377;&#30340;&#24314;&#35758;&#26550;&#26500;&#20462;&#25913;&#26102;&#30340;&#26080;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popularized by the Differentiable Search Index, the emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100k in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.11755</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#65306;&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25512;&#33616;&#25552;&#20379;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#26159;&#23454;&#29616;&#36879;&#26126;&#19988;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#36755;&#20986;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#30784;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#35299;&#37322;&#30446;&#26631;&#12289;&#35299;&#37322;&#33539;&#22260;&#12289;&#35299;&#37322;&#26679;&#24335;&#21644;&#35299;&#37322;&#26684;&#24335;&#36825;&#22235;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#23457;&#26597;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#35299;&#37322;&#30340;&#25991;&#29486;&#12290;&#35748;&#35782;&#21040;&#21487;&#35270;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20174;&#35299;&#37322;&#24615;&#35270;&#35273;&#26041;&#24335;&#30340;&#35282;&#24230;&#36884;&#24452;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#65292;&#21363;&#20351;&#29992;&#21487;&#35270;&#21270;&#20316;&#20026;&#35299;&#37322;&#30340;&#26174;&#31034;&#26679;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#25490;&#22120;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11744</link><description>&lt;p&gt;
&#38754;&#21521;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#26102;&#38388;&#37325;&#25490;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval. (arXiv:2305.11744v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#25490;&#22120;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#36890;&#24120;&#37319;&#29992;&#26816;&#32034;&#21644;&#37325;&#25490;&#26694;&#26550;&#65306;&#20808;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#26816;&#32034;K&#65288;&#20363;&#22914;100&#65289;&#20010;&#20505;&#36873;&#39033;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#36825;&#20123;&#20505;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#20351;&#26356;&#22909;&#30340;&#20505;&#36873;&#39033;&#25490;&#21517;&#26356;&#39640;&#12290;&#37325;&#25490;&#22120;&#36890;&#24120;&#20135;&#29983;&#27604;&#26816;&#32034;&#22120;&#26356;&#22909;&#30340;&#20505;&#36873;&#20998;&#25968;&#65292;&#20294;&#20165;&#38480;&#20110;&#26597;&#30475;&#21069;K&#20010;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#39033;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65288;&#20197;Recall @ K&#20026;&#24230;&#37327;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#37325;&#25490;&#22120;&#36890;&#36807;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#30456;&#20851;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#37325;&#25490;&#22120;&#30340;&#39044;&#27979;&#23545;&#27979;&#35797;&#23454;&#20363;&#30340;&#37325;&#35201;&#20449;&#24687;&#36827;&#34892;&#20102;&#26816;&#32034;&#22120;&#26597;&#35810;&#34920;&#31034;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25512;&#29702;&#26102;&#38388;&#33976;&#39311;&#26469;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#20351;&#26816;&#32034;&#22120;&#30340;&#20505;&#36873;&#20998;&#25968;&#26356;&#25509;&#36817;&#20110;&#37325;&#25490;&#22120;&#30340;&#20998;&#25968;&#12290;&#28982;&#21518;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#26597;&#35810;&#21521;&#37327;&#25191;&#34892;&#31532;&#20108;&#20010;&#26816;&#32034;&#27493;&#39588;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural information retrieval often adopts a retrieve-and-rerank framework: a bi-encoder network first retrieves K (e.g., 100) candidates that are then re-ranked using a more powerful cross-encoder model to rank the better candidates higher. The re-ranker generally produces better candidate scores than the retriever, but is limited to seeing only the top K retrieved candidates, thus providing no improvements in retrieval performance as measured by Recall@K. In this work, we leverage the re-ranker to also improve retrieval by providing inference-time relevance feedback to the retriever. Concretely, we update the retriever's query representation for a test instance using a lightweight inference-time distillation of the re-ranker's prediction for that instance. The distillation loss is designed to bring the retriever's candidate scores closer to those of the re-ranker. A second retrieval step is then performed with the updated query vector. We empirically show that our approach, which can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#25581;&#31034;&#20102;TCF&#31243;&#24207;&#25193;&#23637;&#30340;&#26497;&#38480;&#12290;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11700</link><description>&lt;p&gt;
&#25506;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#30340;&#26497;&#38480;&#65306;&#21457;&#29616;&#21644;&#35748;&#35782;
&lt;/p&gt;
&lt;p&gt;
Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. (arXiv:2305.11700v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#25581;&#31034;&#20102;TCF&#31243;&#24207;&#25193;&#23637;&#30340;&#26497;&#38480;&#12290;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#25104;&#20026;&#29616;&#20170;&#25991;&#26412;&#21644;&#26032;&#38395;&#25512;&#33616;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#25110;&#35821;&#35328;&#27169;&#22411;(LMs)&#34920;&#31034;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20013;&#23567;&#22411;&#30340;LMs&#19978;&#65292;&#22914;&#26524;&#23558;&#29289;&#21697;&#32534;&#30721;&#22120;&#26367;&#25442;&#20026;&#26368;&#22823;&#26368;&#24378;&#22823;&#30340;1750&#20159;&#21442;&#25968;&#30340;GPT-3&#27169;&#22411;&#65292;&#23558;&#20250;&#23545;&#25512;&#33616;&#24615;&#33021;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#23578;&#19981;&#30830;&#23450;&#12290;&#20316;&#32773;&#24320;&#23637;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#32034;TCF&#31243;&#24207;&#30340;&#24615;&#33021;&#26497;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20316;&#32773;&#23558;&#29289;&#21697;&#32534;&#30721;&#22120;&#35268;&#27169;&#20174;&#19968;&#20159;&#25193;&#22823;&#21040;&#19968;&#30334;&#20159;&#20197;&#25581;&#31034;TCF&#31243;&#24207;&#30340;&#25193;&#23637;&#26497;&#38480;&#65292;&#21516;&#26102;&#36824;&#25506;&#31350;&#20102;&#20351;&#29992;&#36229;&#22823;LMs&#26159;&#21542;&#33021;&#23454;&#29616;&#25512;&#33616;&#20219;&#21153;&#30340;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#24378;&#22823;&#30340;LMs&#21644;&#20013;&#31561;LMs&#23454;&#29616;&#30340;&#22522;&#20110;&#25991;&#26412;&#21327;&#21516;&#36807;&#28388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based collaborative filtering (TCF) has become the mainstream approach for text and news recommendation, utilizing text encoders, also known as language models (LMs), to represent items. However, existing TCF models primarily focus on using small or medium-sized LMs. It remains uncertain what impact replacing the item encoder with one of the largest and most powerful LMs, such as the 175-billion parameter GPT-3 model, would have on recommendation performance. Can we expect unprecedented results? To this end, we conduct an extensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we increase the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm. We then examine whether these extremely large LMs could enable a universal item representation for the recommendation task. Furthermore, we compare the performance of the TCF paradigm utilizing the most powerful LMs to the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#35270;&#38556;&#20154;&#22763;&#32593;&#36141;&#26381;&#35013;&#26102;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#38656;&#27714;&#65292;&#38656;&#35201;&#24320;&#21457;&#25552;&#39640;&#21487;&#35775;&#38382;&#24615;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#26381;&#35013;&#32593;&#31449;&#12290;</title><link>http://arxiv.org/abs/2305.11559</link><description>&lt;p&gt;
&#35270;&#38556;&#20154;&#22763;&#32593;&#36141;&#26381;&#35013;&#30340;&#38556;&#30861;&#65306;&#29702;&#35299;&#38656;&#27714;&#30340;&#35775;&#35848;&#21644;&#35266;&#23519;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Barriers to Online Clothing Websites for Visually Impaired People: An Interview and Observation Approach to Understanding Needs. (arXiv:2305.11559v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#35270;&#38556;&#20154;&#22763;&#32593;&#36141;&#26381;&#35013;&#26102;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#38656;&#27714;&#65292;&#38656;&#35201;&#24320;&#21457;&#25552;&#39640;&#21487;&#35775;&#38382;&#24615;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#26381;&#35013;&#32593;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#38556;&#20154;&#22763;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#36141;&#29289;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#35768;&#22810;&#20154;&#36873;&#25321;&#22312;&#32447;&#36141;&#29289;&#65292;&#20294;&#32593;&#36141;&#26381;&#35013;&#20063;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#21644;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#21644;&#35775;&#35848;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#20102;&#35299;&#35270;&#38556;&#20154;&#22763;&#22312;&#32593;&#19978;&#36873;&#36141;&#26381;&#35013;&#26102;&#30340;&#34892;&#20026;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36141;&#29289;&#32593;&#31449;&#30340;&#26381;&#35013;&#25551;&#36848;&#23384;&#22312;&#19981;&#20934;&#30830;&#12289;&#35823;&#23548;&#21644;&#30683;&#30462;&#30340;&#24773;&#20917;&#65307;&#35270;&#38556;&#20154;&#22763;&#20027;&#35201;&#20381;&#38752;&#65288;&#19981;&#21487;&#38752;&#30340;&#65289;&#25628;&#32034;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#26597;&#30475;&#23458;&#25143;&#35780;&#35770;&#26469;&#26816;&#26597;&#20135;&#21697;&#25551;&#36848;&#65307;&#27492;&#22806;&#65292;&#35270;&#38556;&#20154;&#22763;&#23545;&#32593;&#36141;&#26102;&#25509;&#21463;&#20182;&#20154;&#24110;&#21161;&#25345;&#35880;&#24910;&#24577;&#24230;&#65292;&#25285;&#24515;&#20405;&#29359;&#38544;&#31169;&#12289;&#20007;&#22833;&#29420;&#31435;&#24615;&#21644;&#20449;&#20219;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#21487;&#35775;&#38382;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#26381;&#35013;&#32593;&#31449;&#65292;&#20197;&#35299;&#20915;&#35270;&#38556;&#20154;&#22763;&#25152;&#38754;&#20020;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually impaired (VI) people often face challenges when performing everyday tasks and identify shopping for clothes as one of the most challenging. Many engage in online shopping, which eliminates some challenges of physical shopping. However, clothes shopping online suffers from many other limitations and barriers. More research is needed to address these challenges, and extant works often base their findings on interviews alone, providing only subjective, recall-biased information. We conducted two complementary studies using both observational and interview approaches to fill a gap in understanding about VI people's behaviour when selecting and purchasing clothes online. Our findings show that shopping websites suffer from inaccurate, misleading, and contradictory clothing descriptions; that VI people mainly rely on (unreliable) search tools and check product descriptions by reviewing customer comments. Our findings also indicate that VI people are hesitant to accept assistance fro
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;&#27169;&#22411;&#65288;ProCE&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#65292;&#24182;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#20197;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11498</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#20107;&#20214;&#25277;&#21462;&#20013;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;
&lt;/p&gt;
&lt;p&gt;
Recouple Event Field via Probabilistic Bias for Event Extraction. (arXiv:2305.11498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;&#27169;&#22411;&#65288;ProCE&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#65292;&#24182;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#20197;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#65288;EE&#65289;&#26088;&#22312;&#20174;&#20107;&#20214;&#25552;&#21450;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#20107;&#20214;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#65292;&#24050;&#32463;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;PLM&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#35302;&#21457;/&#21442;&#25968;&#23383;&#27573;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20107;&#20214;&#27169;&#24335;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#37325;&#26032;&#32806;&#21512;&#27169;&#22411;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65288;ProCE&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#35821;&#27861;&#30456;&#20851;&#30340;&#20107;&#20214;&#23383;&#27573;&#24314;&#27169;&#20026;&#27010;&#29575;&#20559;&#32622;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;EE&#20013;&#21516;&#19968;&#35302;&#21457;&#22120;/&#21442;&#25968;&#30340;&#22810;&#27425;&#20986;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21516;&#19968;&#35302;&#21457;&#22120;/&#21442;&#25968;&#30340;&#22810;&#20010;&#23383;&#27573;&#20043;&#38388;&#30340;&#27010;&#29575;&#20132;&#20114;&#31574;&#30053;&#65292;&#20197;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#24182;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#22312;EE&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Extraction (EE), aiming to identify and classify event triggers and arguments from event mentions, has benefited from pre-trained language models (PLMs). However, existing PLM-based methods ignore the information of trigger/argument fields, which is crucial for understanding event schemas. To this end, we propose a Probabilistic reCoupling model enhanced Event extraction framework (ProCE). Specifically, we first model the syntactic-related event fields as probabilistic biases, to clarify the event fields from ambiguous entanglement. Furthermore, considering multiple occurrences of the same triggers/arguments in EE, we explore probabilistic interaction strategies among multiple fields of the same triggers/arguments, to recouple the corresponding clarified distributions and capture more latent information fields. Experiments on EE datasets demonstrate the effectiveness and generalization of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#24212;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#20248;&#21270;&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#20013;&#30340;&#24179;&#21488;&#25910;&#30410;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22522;&#20110;&#22238;&#25253;&#30340;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20004;&#31181;&#21512;&#21516;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11381</link><description>&lt;p&gt;
&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning in a Creator Economy. (arXiv:2305.11381v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#24212;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#20248;&#21270;&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#20013;&#30340;&#24179;&#21488;&#25910;&#30410;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22522;&#20110;&#22238;&#25253;&#30340;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20004;&#31181;&#21512;&#21516;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#38761;&#26032;&#20102;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#33719;&#21462;&#21033;&#28070;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#24314;&#27169;&#20026;&#29992;&#25143;&#12289;&#24179;&#21488;&#21644;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#30340;&#19977;&#26041;&#21338;&#24328;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#24212;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#21516;&#21644;&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#24179;&#21488;&#25910;&#30410;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#21512;&#21516;&#65306;&#22522;&#20110;&#22238;&#25253;&#30340;&#21512;&#21516;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The creator economy has revolutionized the way individuals can profit through online platforms. In this paper, we initiate the study of online learning in the creator economy by modeling the creator economy as a three-party game between the users, platform, and content creators, with the platform interacting with the content creator under a principal-agent model through contracts to encourage better content. Additionally, the platform interacts with the users to recommend new content, receive an evaluation, and ultimately profit from the content, which can be modeled as a recommender system.  Our study aims to explore how the platform can jointly optimize the contract and recommender system to maximize the utility in an online learning fashion. We primarily analyze and compare two families of contracts: return-based contracts and feature-based contracts. Return-based contracts pay the content creator a fraction of the reward the platform gains. In contrast, feature-based contracts pay 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item></channel></rss>