<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#65292;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20013;&#25366;&#25496;&#20986;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#65292;&#20197;&#24110;&#21161;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#20248;&#20808;&#22788;&#29702;&#21644;&#20027;&#21160;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.01883</link><description>&lt;p&gt;
&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#20013;&#25366;&#25496;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports. (arXiv:2401.01883v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20998;&#26512;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#65292;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20013;&#25366;&#25496;&#20986;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#65292;&#20197;&#24110;&#21161;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#20248;&#20808;&#22788;&#29702;&#21644;&#20027;&#21160;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#35201;&#27714;&#20174;&#39640;&#32423;&#23545;&#25163;&#34892;&#20026;&#20837;&#25163;&#12290;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#20107;&#20214;&#30340;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#25253;&#21578;&#25551;&#36848;&#20102;&#24694;&#24847;&#34892;&#21160;&#30340;&#26102;&#38388;&#38142;&#12290;&#20026;&#20102;&#36991;&#20813;&#37325;&#22797;&#21457;&#29983;&#32593;&#32476;&#25915;&#20987;&#20107;&#20214;&#65292;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20013;&#20027;&#21160;&#35782;&#21035;&#21644;&#38450;&#24481;&#37325;&#22797;&#38142;&#24335;&#34892;&#21160; - &#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#12290;&#33258;&#21160;&#25366;&#25496;&#34892;&#21160;&#20043;&#38388;&#30340;&#27169;&#24335;&#25552;&#20379;&#20102;&#20851;&#20110;&#36807;&#21435;&#32593;&#32476;&#25915;&#20987;&#30340;&#23545;&#25163;&#34892;&#20026;&#30340;&#32467;&#26500;&#21270;&#21644;&#21487;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#20013;&#25366;&#25496;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#65292;&#24110;&#21161;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#20248;&#20808;&#22788;&#29702;&#21644;&#20027;&#21160;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChronoCTI&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#20174;&#36807;&#21435;&#30340;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#25253;&#21578;&#20013;&#25366;&#25496;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#12290;&#20026;&#26500;&#24314;ChronoCTI&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26102;&#38388;&#25915;&#20987;&#27169;&#24335;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defending from cyberattacks requires practitioners to operate on high-level adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack incidents describe the chain of malicious actions with respect to time. To avoid repeating cyberattack incidents, practitioners must proactively identify and defend against recurring chain of actions - which we refer to as temporal attack patterns. Automatically mining the patterns among actions provides structured and actionable information on the adversary behavior of past cyberattacks. The goal of this paper is to aid security practitioners in prioritizing and proactive defense against cyberattacks by mining temporal attack patterns from cyberthreat intelligence reports. To this end, we propose ChronoCTI, an automated pipeline for mining temporal attack patterns from cyberthreat intelligence (CTI) reports of past cyberattacks. To construct ChronoCTI, we build the ground truth dataset of temporal attack patterns and apply state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#24182;&#21457;&#30340;&#22836;&#33041;&#39118;&#26292;&#21644;&#20551;&#35774;&#28385;&#36275;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#39640;&#24230;&#30456;&#20851;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#24182;&#31616;&#21270;&#20102;&#26597;&#35810;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01835</link><description>&lt;p&gt;
&#24182;&#21457;&#22836;&#33041;&#39118;&#26292;&#21644;&#20551;&#35774;&#28385;&#36275;&#65306;&#19968;&#31181;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#36845;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Concurrent Brainstorming &amp; Hypothesis Satisfying: An Iterative Framework for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR). (arXiv:2401.01835v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#24182;&#21457;&#30340;&#22836;&#33041;&#39118;&#26292;&#21644;&#20551;&#35774;&#28385;&#36275;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#39640;&#24230;&#30456;&#20851;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#24182;&#31616;&#21270;&#20102;&#26597;&#35810;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20840;&#38754;&#20449;&#24687;&#26816;&#32034;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23558;&#21521;&#37327;&#31354;&#38388;&#39537;&#21160;&#30340;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#19982;&#24182;&#21457;&#30340;&#22836;&#33041;&#39118;&#26292;&#30456;&#32467;&#21512;&#65292;&#21152;&#24555;&#39640;&#24230;&#30456;&#20851;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#20174;&#32780;&#31616;&#21270;&#28508;&#22312;&#26597;&#35810;&#30340;&#29983;&#25104;&#12290;&#36825;&#20026;&#25105;&#20204;&#30340;&#26032;&#39062;&#28151;&#21512;&#36807;&#31243;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#35813;&#36807;&#31243;&#20197;&#20551;&#35774;&#24418;&#25104;&#21644;&#28385;&#36275;&#20915;&#31574;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;&#25216;&#26415;&#30830;&#23450;&#20869;&#23481;&#30340;&#20805;&#20998;&#24615;&#12290;&#36825;&#20010;&#32479;&#19968;&#30340;&#20551;&#35774;&#28385;&#24847;&#38454;&#27573;&#26234;&#33021;&#22320;&#25552;&#28860;&#20449;&#24687;&#65292;&#30830;&#23450;&#29992;&#25143;&#30340;&#26597;&#35810;&#26159;&#21542;&#24471;&#21040;&#28385;&#24847;&#30340;&#35299;&#31572;&#12290;&#36798;&#21040;&#36825;&#20010;&#26631;&#20934;&#21518;&#65292;&#31995;&#32479;&#23558;&#20854;&#36755;&#20986;&#31934;&#28860;&#20026;&#31616;&#27905;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27010;&#24565;&#23494;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20887;&#38271;&#12290;&#36845;&#20195;&#30340;&#24037;&#20316;&#27969;&#22686;&#24378;&#20102;&#22788;&#29702;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the complexity of comprehensive information retrieval, this study introduces an innovative, iterative retrieval-augmented generation system. Our approach uniquely integrates a vector-space driven re-ranking mechanism with concurrent brainstorming to expedite the retrieval of highly relevant documents, thereby streamlining the generation of potential queries. This sets the stage for our novel hybrid process, which synergistically combines hypothesis formulation with satisfying decision-making strategy to determine content adequacy, leveraging a chain of thought-based prompting technique. This unified hypothesize-satisfied phase intelligently distills information to ascertain whether user queries have been satisfactorily addressed. Upon reaching this criterion, the system refines its output into a concise representation, maximizing conceptual density with minimal verbosity. The iterative nature of the workflow enhances process efficiency and accuracy. Crucially, the concurrenc
&lt;/p&gt;</description></item><item><title>Physio&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29289;&#29702;&#27835;&#30103;&#39038;&#38382;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#22238;&#31572;&#20013;&#24341;&#29992;&#21487;&#38752;&#30340;&#20581;&#24247;&#26469;&#28304;&#65292;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#12289;&#25512;&#33616;&#24247;&#22797;&#36816;&#21160;&#21644;&#38750;&#22788;&#26041;&#33647;&#29289;&#20197;&#32531;&#35299;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2401.01825</link><description>&lt;p&gt;
Physio:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29289;&#29702;&#27835;&#30103;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
Physio: An LLM-Based Physiotherapy Advisor. (arXiv:2401.01825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01825
&lt;/p&gt;
&lt;p&gt;
Physio&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29289;&#29702;&#27835;&#30103;&#39038;&#38382;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#22238;&#31572;&#20013;&#24341;&#29992;&#21487;&#38752;&#30340;&#20581;&#24247;&#26469;&#28304;&#65292;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#12289;&#25512;&#33616;&#24247;&#22797;&#36816;&#21160;&#21644;&#38750;&#22788;&#26041;&#33647;&#29289;&#20197;&#32531;&#35299;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#21152;&#20102;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#20107;&#23454;&#22312;&#32771;&#34385;&#22312;&#20960;&#20010;&#39046;&#22495;&#20351;&#29992;&#23427;&#20204;&#26102;&#20250;&#36896;&#25104;&#38480;&#21046;&#12290;&#21307;&#30103;&#20445;&#20581;&#26159;&#19968;&#20010;&#35201;&#27714;&#25991;&#26412;&#29983;&#25104;&#21487;&#20449;&#24230;&#30340;&#39046;&#22495;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#20197;&#20445;&#38556;&#24739;&#32773;&#30340;&#20581;&#24247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Physio&#65292;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#24247;&#22797;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;Physio&#33021;&#22815;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#24341;&#29992;&#21487;&#38752;&#30340;&#20581;&#24247;&#26469;&#28304;&#26469;&#25903;&#25345;&#25552;&#20379;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;Physio&#36824;&#21487;&#20197;&#20511;&#21161;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#25512;&#33616;&#24247;&#22797;&#36816;&#21160;&#21644;&#38750;&#22788;&#26041;&#33647;&#29289;&#20197;&#32531;&#35299;&#30151;&#29366;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#21151;&#33021;&#65292;Physio&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#22788;&#29702;&#65292;&#21516;&#26102;&#23558;&#20854;&#22238;&#22797;&#26465;&#20214;&#21270;&#20026;&#21487;&#38752;&#21644;&#21487;&#39564;&#35777;&#30340;&#26469;&#28304;&#12290;Physio&#30340;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;https://phys&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of the most recent language models have increased the interest in integrating them into real-world applications. However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being. In this paper, we present Physio, a chat-based application for physical rehabilitation. Physio is capable of making an initial diagnosis while citing reliable health sources to support the information provided. Furthermore, drawing upon external knowledge databases, Physio can recommend rehabilitation exercises and over-the-counter medication for symptom relief. By combining these features, Physio can leverage the power of generative models for language processing while also conditioning its response on dependable and verifiable sources. A live demo of Physio is available at https://phys
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25991;&#31456;&#20998;&#31867;&#26469;&#35780;&#20272;&#22312;&#32447;&#26032;&#38395;&#21457;&#24067;&#32773;&#30340;&#21487;&#20449;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#20998;&#31867;&#27169;&#22411;&#22312;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#30340;&#21487;&#20449;&#24230;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#21487;&#24212;&#29992;&#20110;&#25552;&#37266;&#35835;&#32773;&#19981;&#21487;&#20449;&#30340;&#26032;&#38395;&#26469;&#28304;&#12289;&#21327;&#21161;&#26032;&#38395;&#32452;&#32455;&#35780;&#20272;&#23186;&#20307;&#26426;&#26500;&#21644;&#36873;&#25321;&#21487;&#20449;&#30340;&#25991;&#31456;&#12290;</title><link>http://arxiv.org/abs/2401.01781</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#31456;&#20998;&#31867;&#35780;&#20272;&#22312;&#32447;&#26032;&#38395;&#21457;&#24067;&#32773;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Evaluating Trustworthiness of Online News Publishers via Article Classification. (arXiv:2401.01781v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25991;&#31456;&#20998;&#31867;&#26469;&#35780;&#20272;&#22312;&#32447;&#26032;&#38395;&#21457;&#24067;&#32773;&#30340;&#21487;&#20449;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#20998;&#31867;&#27169;&#22411;&#22312;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#30340;&#21487;&#20449;&#24230;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#21487;&#24212;&#29992;&#20110;&#25552;&#37266;&#35835;&#32773;&#19981;&#21487;&#20449;&#30340;&#26032;&#38395;&#26469;&#28304;&#12289;&#21327;&#21161;&#26032;&#38395;&#32452;&#32455;&#35780;&#20272;&#23186;&#20307;&#26426;&#26500;&#21644;&#36873;&#25321;&#21487;&#20449;&#30340;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#20449;&#24687;&#26102;&#20195;&#20302;&#36136;&#37327;&#30340;&#22312;&#32447;&#20449;&#24687;&#27867;&#28389;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24378;&#22823;&#32780;&#33258;&#21160;&#30340;&#26426;&#21046;&#26469;&#35780;&#20272;&#22312;&#32447;&#26032;&#38395;&#21457;&#24067;&#32773;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;40&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;4033&#26465;&#26032;&#38395;&#25925;&#20107;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#26426;&#26500;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#20010;&#21035;&#25991;&#31456;&#30340;&#20869;&#23481;&#20998;&#31867;&#26469;&#25512;&#26029;&#26469;&#28304;&#30340;&#21487;&#20449;&#24230;&#27700;&#24179;&#12290;&#21487;&#20449;&#24230;&#26631;&#31614;&#26469;&#33258;NewsGuard&#65292;&#19968;&#20010;&#36890;&#36807;&#35782;&#21035;&#26032;&#38395;&#26469;&#28304;&#20351;&#29992;&#33391;&#22909;&#30340;&#32534;&#36753;&#21644;&#20986;&#29256;&#26631;&#20934;&#30340;&#26032;&#38395;&#32452;&#32455;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#30340;&#21487;&#20449;&#24230;&#27700;&#24179;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#25552;&#37266;&#35835;&#32773;&#21487;&#33021;&#19981;&#21487;&#20449;&#30340;&#26032;&#38395;&#26469;&#28304;&#12289;&#21327;&#21161;&#26032;&#38395;&#32452;&#32455;&#35780;&#20272;&#26032;&#30340;&#25110;&#19981;&#29087;&#24713;&#30340;&#23186;&#20307;&#26426;&#26500;&#20197;&#21450;&#25903;&#25345;&#36873;&#25321;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#25991;&#31456;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of low-quality online information in today's era has underscored the need for robust and automatic mechanisms to evaluate the trustworthiness of online news publishers. In this paper, we analyse the trustworthiness of online news media outlets by leveraging a dataset of 4033 news stories from 40 different sources. We aim to infer the trustworthiness level of the source based on the classification of individual articles' content. The trust labels are obtained from NewsGuard, a journalistic organization that evaluates news sources using well-established editorial and publishing criteria. The results indicate that the classification model is highly effective in classifying the trustworthiness levels of the news articles. This research has practical applications in alerting readers to potentially untrustworthy news sources, assisting journalistic organizations in evaluating new or unfamiliar media outlets and supporting the selection of articles for their trustworthiness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#26597;&#35810;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#20248;&#21270;&#23553;&#38381;&#24335;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#24341;&#20837;&#24187;&#35273;&#23631;&#34109;&#26426;&#21046;&#20197;&#21450;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01780</link><description>&lt;p&gt;
&#22312;&#23553;&#38381;&#24335;&#38382;&#31572;&#20013;&#20248;&#21270;API&#20381;&#36182;&#20197;&#20943;&#23569;&#24187;&#35273;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering. (arXiv:2401.01780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#26597;&#35810;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#20248;&#21270;&#23553;&#38381;&#24335;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#24341;&#20837;&#24187;&#35273;&#23631;&#34109;&#26426;&#21046;&#20197;&#21450;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33021;&#22815;&#31215;&#32047;&#21644;&#24674;&#22797;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#12290;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#26102;&#65292;LLM&#19981;&#33021;&#20165;&#20165;&#20381;&#38752;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#26469;&#20445;&#35777;&#30495;&#23454;&#21644;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#25628;&#32034;&#22806;&#37096;&#20449;&#24687;&#28304;(&#22914;&#32593;&#32476;)&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#31181;&#23558;&#30693;&#35782;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#37327;&#25991;&#26723;&#20013;&#36827;&#34892;&#25628;&#32034;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#35745;&#31639;/&#26102;&#38388;&#25104;&#26412;&#12290;&#26368;&#20339;&#31574;&#30053;&#26159;&#21482;&#26377;&#22312;LLM&#23545;&#31572;&#26696;&#19981;&#30830;&#23450;&#26102;&#25165;&#26597;&#35810;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#65292;&#33021;&#22815;&#33258;&#25105;&#35780;&#20272;&#26159;&#21542;&#33021;&#22815;&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#25110;&#32773;&#38656;&#35201;&#35831;&#27714;&#22806;&#37096;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38381;&#20070;&#38382;&#31572;&#20219;&#21153;&#20013;&#24341;&#20837;&#24187;&#35273;&#23631;&#34109;&#26426;&#21046;&#26469;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;arXiv&#19978;&#30340;&#37327;&#21270;&#37329;&#34701;&#35770;&#25991;&#65292;&#21457;&#29616;&#20102;&#20851;&#20110;&#35813;&#39046;&#22495;&#30340;&#26102;&#38388;&#36235;&#21183;&#12289;&#26368;&#24120;&#34987;&#24341;&#29992;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#26399;&#21002;&#65292;&#20197;&#21450;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01751</link><description>&lt;p&gt;
&#25991;&#26412;&#25366;&#25496;arXiv&#65306;&#23545;&#37327;&#21270;&#37329;&#34701;&#35770;&#25991;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Text mining arXiv: a look through quantitative finance papers. (arXiv:2401.01751v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;arXiv&#19978;&#30340;&#37327;&#21270;&#37329;&#34701;&#35770;&#25991;&#65292;&#21457;&#29616;&#20102;&#20851;&#20110;&#35813;&#39046;&#22495;&#30340;&#26102;&#38388;&#36235;&#21183;&#12289;&#26368;&#24120;&#34987;&#24341;&#29992;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#26399;&#21002;&#65292;&#20197;&#21450;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;arXiv&#39044;&#21360;&#26412;&#26381;&#21153;&#22120;&#19978;&#30340;&#35770;&#25991;&#65292;&#26088;&#22312;&#21457;&#29616;&#36825;&#20010;&#24222;&#22823;&#30340;&#30740;&#31350;&#38598;&#21512;&#20013;&#38544;&#34255;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;1997&#24180;&#21040;2022&#24180;&#22312;arXiv&#19978;&#21457;&#24067;&#30340;&#37327;&#21270;&#37329;&#34701;&#35770;&#25991;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#20174;&#25972;&#20010;&#25991;&#26723;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#20851;&#38190;&#20449;&#24687;&#65292;&#21253;&#25324;&#24341;&#29992;&#65292;&#20197;&#20102;&#35299;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#36235;&#21183;&#65292;&#24182;&#25214;&#20986;&#36825;&#20010;&#39046;&#22495;&#20013;&#26368;&#24120;&#34987;&#24341;&#29992;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#26399;&#21002;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#22810;&#31181;&#31639;&#27861;&#26469;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#23578;&#26410;&#22312;&#30693;&#35782;&#22270;&#35889;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01711</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs. (arXiv:2401.01711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#23578;&#26410;&#22312;&#30693;&#35782;&#22270;&#35889;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#35821;&#20041;&#35299;&#26512;&#26469;&#23454;&#29616;&#20132;&#20114;&#24335;&#20449;&#24687;&#26816;&#32034;&#65292;&#35813;&#36807;&#31243;&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#26597;&#35810;&#12290;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#65292;&#23545;&#35805;&#34920;&#36798;&#34987;&#36716;&#21270;&#20026;&#22270;&#26597;&#35810;&#30340;&#36807;&#31243;&#34987;&#31216;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#38382;&#31572;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23578;&#26410;&#26126;&#30830;&#22312;&#27492;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#25552;&#31034;&#25216;&#26415;&#30340;&#27169;&#22411;&#65292;&#24182;&#35782;&#21035;&#20986;&#29983;&#25104;&#36755;&#20986;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#22270;&#26597;&#35810;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23637;&#31034;&#36739;&#20302;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational question answering systems often rely on semantic parsing to enable interactive information retrieval, which involves the generation of structured database queries from a natural language input. For information-seeking conversations about facts stored within a knowledge graph, dialogue utterances are transformed into graph queries in a process that is called knowledge-based conversational question answering. This paper evaluates the performance of large language models that have not been explicitly pre-trained on this task. Through a series of experiments on an extensive benchmark dataset, we compare models of varying sizes with different prompting techniques and identify common issue types in the generated output. Our results demonstrate that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-sho
&lt;/p&gt;</description></item><item><title>GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01614</link><description>&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#22914;&#26524;&#26377;&#22522;&#30784;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01614
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;GPT-4V(ision)&#21644;Gemini&#65292;&#24555;&#36895;&#25512;&#21160;&#20102;&#22810;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#36229;&#36234;&#20256;&#32479;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;GPT-4V&#36825;&#26679;&#30340;LMM&#20316;&#20026;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEEACT&#65292;&#19968;&#31181;&#21033;&#29992;LMM&#30340;&#21147;&#37327;&#36827;&#34892;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#26032;&#30340;MIND2WEB&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#38500;&#20102;&#23545;&#32531;&#23384;&#32593;&#31449;&#30340;&#26631;&#20934;&#31163;&#32447;&#35780;&#20272;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#22312;&#23454;&#26102;&#32593;&#31449;&#19978;&#36816;&#34892;&#32593;&#32476;&#20195;&#29702;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35780;&#20272;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4V&#22312;&#32593;&#39029;&#20195;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;-&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#25991;&#26412;&#35745;&#21010;&#25163;&#21160;&#22320;&#23454;&#26045;&#20026;&#32593;&#31449;&#19978;&#30340;&#34892;&#21160;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23436;&#25104;50%&#30340;&#20219;&#21153;&#12290;&#27492;&#32467;&#26524;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms
&lt;/p&gt;</description></item><item><title>Team IELAB&#22312;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#20013;&#37319;&#29992;&#31070;&#32463;&#25490;&#24207;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#20020;&#24202;&#35797;&#39564;&#26816;&#32034;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#23454;&#29616;&#20102;PubmedBERT&#30340;&#24378;&#22823;&#25490;&#24207;&#22120;&#30340;&#38598;&#25104;&#65292;&#20026;&#20020;&#24202;&#35797;&#39564;&#26816;&#32034;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01566</link><description>&lt;p&gt;
Team IELAB&#22312;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;2023&#20013;&#30340;&#30740;&#31350;&#65306;&#21033;&#29992;&#31070;&#32463;&#25490;&#24207;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20020;&#24202;&#35797;&#39564;&#26816;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models. (arXiv:2401.01566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01566
&lt;/p&gt;
&lt;p&gt;
Team IELAB&#22312;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#20013;&#37319;&#29992;&#31070;&#32463;&#25490;&#24207;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#20020;&#24202;&#35797;&#39564;&#26816;&#32034;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#23454;&#29616;&#20102;PubmedBERT&#30340;&#24378;&#22823;&#25490;&#24207;&#22120;&#30340;&#38598;&#25104;&#65292;&#20026;&#20020;&#24202;&#35797;&#39564;&#26816;&#32034;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#28595;&#22823;&#21033;&#20122;&#32852;&#37030;&#31185;&#23398;&#19982;&#24037;&#19994;&#30740;&#31350;&#32452;&#32455;&#65288;CSIRO&#65289;&#21644;&#26118;&#22763;&#20848;&#22823;&#23398;&#22242;&#38431;ielab&#22312;2023&#24180;TREC&#20020;&#24202;&#35797;&#39564;&#36319;&#36394;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31070;&#32463;&#25490;&#24207;&#22120;&#65292;&#20294;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#31867;&#25490;&#24207;&#22120;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;ChatGPT&#20174;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#19982;&#38543;&#26426;&#36873;&#25321;&#30340;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#30340;&#24739;&#32773;&#25551;&#36848;&#12290;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19982;&#20043;&#21069;&#20960;&#24180;&#30340;&#20154;&#24037;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#65292;&#29992;&#20110;&#22522;&#20110;PubmedBERT&#30340;&#31264;&#23494;&#21644;&#31232;&#30095;&#26816;&#32034;&#22120;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#19968;&#20010;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#38598;&#25104;&#21040;&#31995;&#32479;&#20013;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#21319;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#20316;&#20026;TREC&#27880;&#37322;&#22120;&#25552;&#20379;&#23545;&#25105;&#20204;&#30340;&#36816;&#34892;&#25991;&#20214;&#30340;&#21028;&#26029;&#12290;&#36825;&#20123;&#21028;&#26029;&#32467;&#26524;&#38543;&#21518;&#34987;&#29992;&#26469;&#37325;&#26032;&#25490;&#24207;&#32467;&#26524;&#12290;&#36825;&#31181;&#26550;&#26500;&#23558;&#24378;&#22823;&#30340;&#22522;&#20110;PubmedBERT&#30340;&#25490;&#24207;&#22120;&#19982;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32039;&#23494;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#35797;&#39564;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthetic dataset, combined with human-annotated training data from previous years, is used to train both dense and sparse retrievers based on PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the system. To further enhance the effectiveness of our approach, we prompting GPT-4 as a TREC annotator to provide judgments on our run files. These judgments are subsequently employed to re-rank the results. This architecture tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large Language Models, demonstrating a new approach to clinical trial retrieval.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#29992;&#20110;&#27604;&#36739;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01527</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Poisoning Attacks against Recommender Systems: A Survey. (arXiv:2401.01527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#29992;&#20110;&#27604;&#36739;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#27963;&#21160;&#30340;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#27602;&#21270;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#23558;&#24694;&#24847;&#25968;&#25454;&#27880;&#20837;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#23436;&#25972;&#24615;&#65292;&#24182;&#36890;&#36807;&#25805;&#32437;&#25512;&#33616;&#32467;&#26524;&#26469;&#33719;&#21462;&#38750;&#27861;&#21033;&#28070;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#29616;&#26377;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32452;&#20214;&#29305;&#23450;&#12289;&#30446;&#26631;&#39537;&#21160;&#21644;&#33021;&#21147;&#25506;&#27979;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#20854;&#26426;&#21046;&#20197;&#21450;&#30456;&#20851;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#21644;&#22522;&#20934;&#27979;&#35797;&#27602;&#21270;&#25915;&#20987;&#30340;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#65292;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#25972;&#22871;&#27602;&#21270;&#25915;&#20987;&#27169;&#22411;&#21644;&#24120;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common
&lt;/p&gt;</description></item><item><title>&#37329;&#34701;&#31185;&#25216;&#24179;&#21488;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#25512;&#24191;&#20114;&#32852;&#32593;&#22522;&#37329;&#26102;&#39044;&#27979;&#25237;&#36164;&#37329;&#39069;&#30340;&#21464;&#21160;&#65292;&#20197;&#26368;&#22823;&#21270;&#27599;&#20010;&#23458;&#25143;&#30340;&#20132;&#26131;&#37329;&#39069;&#12290;</title><link>http://arxiv.org/abs/2401.01525</link><description>&lt;p&gt;
&#38754;&#21521;&#31934;&#32454;&#21270;&#33829;&#38144;&#30340;&#37329;&#34701;&#31185;&#25216;&#24179;&#21488;&#39044;&#26399;&#20132;&#26131;&#20215;&#20540;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Expected Transaction Value Optimization for Precise Marketing in FinTech Platforms. (arXiv:2401.01525v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01525
&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#31185;&#25216;&#24179;&#21488;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#25512;&#24191;&#20114;&#32852;&#32593;&#22522;&#37329;&#26102;&#39044;&#27979;&#25237;&#36164;&#37329;&#39069;&#30340;&#21464;&#21160;&#65292;&#20197;&#26368;&#22823;&#21270;&#27599;&#20010;&#23458;&#25143;&#30340;&#20132;&#26131;&#37329;&#39069;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#31185;&#25216;&#24179;&#21488;&#36890;&#36807;&#25968;&#23383;&#25903;&#20184;&#23454;&#29616;&#30340;&#20114;&#32852;&#32593;&#37329;&#34701;&#26381;&#21153;&#27491;&#36805;&#36895;&#22686;&#38271;&#65292;&#36890;&#36807;&#31227;&#21160;&#24212;&#29992;&#21521;&#20010;&#20154;&#25237;&#36164;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20114;&#32852;&#32593;&#22522;&#37329;&#20998;&#38144;&#12290;&#20316;&#20026;&#37329;&#34701;&#20135;&#21697;&#25237;&#36164;&#30340;&#37325;&#35201;&#20013;&#20171;&#65292;&#36825;&#20123;&#24179;&#21488;&#22312;&#22522;&#37329;&#20844;&#21496;&#30340;&#35201;&#27714;&#19979;&#65292;&#20197;&#20445;&#35777;&#20132;&#20184;&#65288;GD&#65289;&#31574;&#30053;&#20998;&#21457;&#25968;&#21315;&#21482;&#20114;&#32852;&#32593;&#22522;&#37329;&#12290;&#24179;&#21488;&#36890;&#36807;&#25512;&#24191;&#20114;&#32852;&#32593;&#22522;&#37329;&#32473;&#26377;&#20852;&#36259;&#30340;&#25237;&#36164;&#32773;&#26469;&#25512;&#21160;&#29992;&#25143;&#30340;&#22522;&#37329;&#36141;&#20080;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#27599;&#20010;&#23458;&#25143;&#30340;&#20132;&#26131;&#37329;&#39069;&#12290;&#19982;&#20256;&#32479;&#24191;&#21578;&#25110;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#20013;&#30340;&#36716;&#21270;&#19981;&#21516;&#65292;&#27599;&#27425;&#36141;&#20080;&#20013;&#30340;&#25237;&#36164;&#37329;&#39069;&#29978;&#33267;&#23545;&#20110;&#30456;&#21516;&#30340;&#37329;&#34701;&#20135;&#21697;&#26469;&#35828;&#20063;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#32473;&#20114;&#32852;&#32593;&#22522;&#37329;&#30340;&#25512;&#24191;&#25512;&#33616;&#25552;&#20379;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#20256;&#32479;&#25512;&#33616;&#20013;&#65292;&#38500;&#20102;&#39044;&#27979;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#25110;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#22806;&#65292;&#23545;&#20110;&#37329;&#34701;&#31185;&#25216;&#24179;&#21488;&#26469;&#35828;&#65292;&#39044;&#27979;&#27599;&#27425;&#36141;&#20080;&#30340;&#25237;&#36164;&#37329;&#39069;&#21516;&#26679;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
FinTech platforms facilitated by digital payments are watching growth rapidly, which enable the distribution of mutual funds personalized to individual investors via mobile Apps. As the important intermediation of financial products investment, these platforms distribute thousands of mutual funds obtaining impressions under guaranteed delivery (GD) strategy required by fund companies. Driven by the profit from fund purchases of users, the platform aims to maximize each transaction amount of customers by promoting mutual funds to these investors who will be interested in. Different from the conversions in traditional advertising or e-commerce recommendations, the investment amount in each purchase varies greatly even for the same financial product, which provides a significant challenge for the promotion recommendation of mutual funds. In addition to predicting the click-through rate (CTR) or the conversion rate (CVR) as in traditional recommendations, it is essential for FinTech platfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#22312;&#22810;&#20803;&#25991;&#21270;&#20225;&#19994;&#20013;&#20351;&#29992;RAG&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#20379;&#32473;&#31574;&#30053;&#12289;&#24187;&#35273;&#32531;&#35299;&#21644;&#38169;&#35823;&#21709;&#24212;&#39044;&#38450;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.01511</link><description>&lt;p&gt;
&#22312;&#22810;&#20803;&#21270;&#20154;&#21147;&#36164;&#28304;&#29615;&#22659;&#20013;&#25552;&#21319;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#65306;&#22810;&#20803;&#25991;&#21270;&#20225;&#19994;&#20013;RAG&#27169;&#22411;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model Implementation for Multicultural Enterprise. (arXiv:2401.01511v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#22312;&#22810;&#20803;&#25991;&#21270;&#20225;&#19994;&#20013;&#20351;&#29992;RAG&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#20379;&#32473;&#31574;&#30053;&#12289;&#24187;&#35273;&#32531;&#35299;&#21644;&#38169;&#35823;&#21709;&#24212;&#39044;&#38450;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#20449;&#24687;&#26816;&#32034;&#65292;&#24102;&#26469;&#20102;&#24191;&#27867;&#30340;&#30693;&#35782;&#33719;&#21462;&#33021;&#21147;&#30340;&#26032;&#26102;&#20195;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#25552;&#20379;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#19981;&#21516;&#35835;&#20889;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26377;&#25928;&#25552;&#21462;&#31572;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24357;&#21512;&#20102;&#20449;&#24687;&#21487;&#33719;&#21462;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#23558;RAG&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#20013;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#21508;&#31181;&#22240;&#32032;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22810;&#20803;&#25991;&#21270;&#29615;&#22659;&#20013;&#23454;&#26045;RAG&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#21253;&#25324;&#25968;&#25454;&#20379;&#32473;&#31574;&#30053;&#12289;&#21450;&#26102;&#26356;&#26032;&#12289;&#24187;&#35273;&#32531;&#35299;&#12289;&#38169;&#35823;&#21709;&#24212;&#39044;&#38450;&#21644;&#20132;&#20184;&#36895;&#24230;&#20248;&#21270;&#22312;&#20869;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28041;&#21450;&#23558;&#21508;&#31181;&#24037;&#20855;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#32454;&#33268;&#22320;&#32452;&#21512;&#20197;&#20415;&#20419;&#36827;&#26080;&#32541;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Large Language Models has revolutionized information retrieval, ushering in a new era of expansive knowledge accessibility. While these models excel in providing open-world knowledge, effectively extracting answers in diverse linguistic environments with varying levels of literacy remains a formidable challenge. Retrieval Augmented Generation (RAG) emerges as a promising solution, bridging the gap between information availability and multilingual comprehension. However, deploying RAG models in real-world scenarios demands careful consideration of various factors. This paper addresses the critical challenges associated with implementing RAG models in multicultural environments. We delve into essential considerations, including data feeding strategies, timely updates, mitigation of hallucinations, prevention of erroneous responses, and optimization of delivery speed. Our work involves the integration of a diverse array of tools, meticulously combined to facilitate the seaml
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;PrepRec&#65292;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#27969;&#34892;&#24230;&#21160;&#24577;&#23398;&#20064;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;PrepRec&#21487;&#20197;&#38646;-shot&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#21482;&#26377;&#24456;&#23567;&#19968;&#37096;&#20998;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01497</link><description>&lt;p&gt;
&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65306;&#22522;&#20110;&#27969;&#34892;&#24230;&#21160;&#24577;&#30340;&#38646;-shot&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Sequential Recommendation Framework: Popularity Dynamics for Zero-shot Transfer. (arXiv:2401.01497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;PrepRec&#65292;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#27969;&#34892;&#24230;&#21160;&#24577;&#23398;&#20064;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;PrepRec&#21487;&#20197;&#38646;-shot&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#21482;&#26377;&#24456;&#23567;&#19968;&#37096;&#20998;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#23545;&#20110;&#22312;&#32447;&#24212;&#29992;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#35270;&#39057;&#27969;&#23186;&#20307;&#21644;&#31038;&#20132;&#23186;&#20307;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#27169;&#22411;&#26550;&#26500;&#19981;&#26029;&#25913;&#36827;&#65292;&#20294;&#23545;&#20110;&#27599;&#20010;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#20174;&#22836;&#35757;&#32451;&#19968;&#20010;&#26032;&#27169;&#22411;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#24050;&#32463;&#22312;&#38646;-shot&#25110;&#23569;-shot&#36866;&#24212;&#26032;&#24212;&#29992;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#21463;&#21040;&#21516;&#34892;AI&#39046;&#22495;&#39044;&#35757;&#32451;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65306;PrepRec&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#27969;&#34892;&#24230;&#21160;&#24577;&#26469;&#23398;&#20064;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#12290;&#36890;&#36807;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;PrepRec&#22312;&#27809;&#26377;&#20219;&#20309;&#36741;&#21161;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#19981;&#20165;&#33021;&#22815;&#38646;-shot&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#24182;&#19988;&#19982;&#21516;&#31867;&#26368;&#20808;&#36827;&#30340;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30456;&#27604;&#65292;&#27169;&#22411;&#22823;&#23567;&#20165;&#30456;&#24403;&#19968;&#23567;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommenders are crucial to the success of online applications, \eg e-commerce, video streaming, and social media. While model architectures continue to improve, for every new application domain, we still have to train a new model from scratch for high quality recommendations. On the other hand, pre-trained language and vision models have shown great success in zero-shot or few-shot adaptation to new application domains. Inspired by the success of pre-trained models in peer AI fields, we propose a novel pre-trained sequential recommendation framework: PrepRec. We learn universal item representations by modeling item popularity dynamics. Through extensive experiments on five real-world datasets, we show that PrepRec, without any auxiliary information, can not only zero-shot transfer to a new domain, but achieve competitive performance compared to state-of-the-art sequential recommender models with only a fraction of the model size. In addition, with a simple post-hoc interpol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>RL-MPCA&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01369</link><description>&lt;p&gt;
RL-MPCA: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01369
&lt;/p&gt;
&lt;p&gt;
RL-MPCA&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#20174;&#22823;&#37327;&#20505;&#36873;&#39033;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#38543;&#30528;&#29992;&#25143;&#35831;&#27714;&#30340;&#22686;&#21152;&#21644;&#26381;&#21153;&#65288;&#25110;&#27169;&#22411;&#65289;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22312;&#38431;&#21015;&#25130;&#26029;&#22330;&#26223;&#20013;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#65288;&#21363;&#20998;&#37197;&#20505;&#36873;&#39033;&#30340;&#22823;&#23567;&#65289;&#65292;&#24182;&#23558;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#32422;&#26463;&#26465;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20123;&#30740;&#31350;&#38598;&#20013;&#20110;&#21333;&#38454;&#27573;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#38598;&#20013;&#20110;&#22810;&#38454;&#27573;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#20123;&#20851;&#20110;&#38431;&#21015;&#25130;&#26029;&#22330;&#26223;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#20854;&#20182;&#24773;&#26223;&#19979;&#65288;&#22914;&#26816;&#32034;&#36890;&#36947;&#36873;&#25321;&#21644;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#65289;&#26159;&#19981;&#25104;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#35831;&#27714;&#22312;&#19981;&#21516;&#38454;&#27573;&#20043;&#38388;&#30340;&#29366;&#24577;&#36716;&#31227;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiven
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20803;&#25968;&#25454;&#32034;&#24341;&#25945;&#32946;&#35270;&#39057;&#65292;&#24110;&#21161;&#20840;&#29699;&#21508;&#22320;&#30340;&#23398;&#29983;&#39640;&#25928;&#21033;&#29992;&#36825;&#20123;&#35270;&#39057;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.01356</link><description>&lt;p&gt;
&#25945;&#32946;&#35270;&#39057;&#20803;&#25968;&#25454;&#30340;&#39640;&#25928;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
Efficient Indexing of Meta-Data (Extracted from Educational Videos). (arXiv:2401.01356v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20803;&#25968;&#25454;&#32034;&#24341;&#25945;&#32946;&#35270;&#39057;&#65292;&#24110;&#21161;&#20840;&#29699;&#21508;&#22320;&#30340;&#23398;&#29983;&#39640;&#25928;&#21033;&#29992;&#36825;&#20123;&#35270;&#39057;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#25945;&#32946;&#25945;&#23460;&#25945;&#23398;&#30340;&#26222;&#21450;&#65292;&#35270;&#39057;&#35762;&#24231;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#21644;&#38656;&#27714;&#22686;&#21152;&#12290;&#22914;NPTEL&#31561;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#24050;&#32463;&#21019;&#24314;&#20102;&#39640;&#36136;&#37327;&#30340;&#25945;&#32946;&#20869;&#23481;&#65292;&#23398;&#29983;&#21487;&#20197;&#20813;&#36153;&#22312;&#32447;&#35775;&#38382;&#12290;&#20840;&#22269;&#35768;&#22810;&#22823;&#23398;&#29616;&#22312;&#37117;&#22312;&#35838;&#22530;&#19978;&#20351;&#29992;NPTEL&#35270;&#39057;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#39057;&#35762;&#24231;&#27491;&#22312;&#34987;&#35760;&#24405;&#12289;&#32500;&#25252;&#21644;&#19978;&#20256;&#12290;&#36825;&#20123;&#35270;&#39057;&#36890;&#24120;&#22312;&#35762;&#24231;&#24320;&#22987;&#21069;&#21253;&#21547;&#26377;&#20851;&#35813;&#35270;&#39057;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#24120;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#25945;&#32946;&#35270;&#39057;&#20855;&#26377;&#21253;&#21547;&#20116;&#21040;&#20845;&#20010;&#23646;&#24615;&#30340;&#20803;&#25968;&#25454;&#65306;&#23398;&#38498;&#21517;&#31216;&#12289;&#20986;&#29256;&#21830;&#21517;&#31216;&#12289;&#31995;&#21035;&#21517;&#31216;&#12289;&#25945;&#25480;&#21517;&#31216;&#12289;&#23398;&#31185;&#21517;&#31216;&#21644;&#20027;&#39064;&#21517;&#31216;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#25353;&#29031;&#23427;&#20204;&#30340;&#31867;&#21035;&#32452;&#32455;&#36825;&#20123;&#35270;&#39057;&#65292;&#23558;&#26356;&#23481;&#26131;&#32500;&#25252;&#36825;&#20123;&#35270;&#39057;&#12290;&#26681;&#25454;&#36825;&#20123;&#20449;&#24687;&#23545;&#36825;&#20123;&#35270;&#39057;&#36827;&#34892;&#32034;&#24341;&#23545;&#19990;&#30028;&#21508;&#22320;&#30340;&#23398;&#29983;&#39640;&#25928;&#21033;&#29992;&#36825;&#20123;&#35270;&#39057;&#38750;&#24120;&#26377;&#30410;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#33719;&#21462;&#20803;&#25968;&#25454;&#20449;&#24687;&#26469;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#30340;&#35270;&#39057;&#32034;&#24341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video lectures are becoming more popular and in demand as online classroom teaching is becoming more prevalent. Massive Open Online Courses (MOOCs), such as NPTEL, have been creating high-quality educational content that is freely accessible to students online. A large number of colleges across the country are now using NPTEL videos in their classrooms. So more video lectures are being recorded, maintained, and uploaded. These videos generally contain information about that video before the lecture begins. We generally observe that these educational videos have metadata containing five to six attributes: Institute Name, Publisher Name, Department Name, Professor Name, Subject Name, and Topic Name. It would be easy to maintain these videos if we could organize them according to their categories. The indexing of these videos based on this information is beneficial for students all around the world to efficiently utilise these videos. In this project, we are trying to get the metadata inf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMXRec&#26694;&#26550;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#21319;&#35299;&#37322;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20808;&#21069;&#30340;&#25512;&#33616;&#27169;&#22411;&#23494;&#20999;&#21327;&#20316;&#65292;&#22312;&#25512;&#33616;&#25928;&#26524;&#21644;&#35299;&#37322;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.15661</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21487;&#35299;&#37322;&#25512;&#33616;&#20013;&#30340;&#28508;&#21147;&#35299;&#38145;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Large Language Models for Explainable Recommendations. (arXiv:2312.15661v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMXRec&#26694;&#26550;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#21319;&#35299;&#37322;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20808;&#21069;&#30340;&#25512;&#33616;&#27169;&#22411;&#23494;&#20999;&#21327;&#20316;&#65292;&#22312;&#25512;&#33616;&#25928;&#26524;&#21644;&#35299;&#37322;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20851;&#20110;&#20026;&#20309;&#25512;&#33616;&#26576;&#20010;&#39033;&#30446;&#30340;&#29992;&#25143;&#21451;&#22909;&#35299;&#37322;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#24182;&#22312;&#20351;&#29992;&#22312;&#32447;&#26381;&#21153;&#26102;&#20419;&#36827;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23558;&#35299;&#37322;&#29983;&#25104;&#22120;&#26367;&#25442;&#20026;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#20135;&#29983;&#20309;&#31181;&#24433;&#21709;&#12290;&#25105;&#20204;&#33021;&#21542;&#26399;&#26395;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMXRec&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#21487;&#35299;&#37322;&#25512;&#33616;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#37322;&#36136;&#37327;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#24037;&#20316;&#19981;&#21516;&#65292;LLMXRec&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#23427;&#24378;&#35843;&#20808;&#21069;&#30340;&#25512;&#33616;&#27169;&#22411;&#19982;&#22522;&#20110;LLM&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#20043;&#38388;&#30340;&#23494;&#20999;&#21327;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#37319;&#29992;&#20960;&#31181;&#20851;&#38190;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21253;&#25324;&#21442;&#25968;&#35843;&#20248;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#25928;&#26524;&#21644;&#35299;&#37322;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating user-friendly explanations regarding why an item is recommended has become increasingly common, largely due to advances in language generation technology, which can enhance user trust and facilitate more informed decision-making when using online services. However, existing explainable recommendation systems focus on using small-size language models. It remains uncertain what impact replacing the explanation generator with the recently emerging large language models (LLMs) would have. Can we expect unprecedented results?  In this study, we propose LLMXRec, a simple yet effective two-stage explainable recommendation framework aimed at further boosting the explanation quality by employing LLMs. Unlike most existing LLM-based recommendation works, a key characteristic of LLMXRec is its emphasis on the close collaboration between previous recommender models and LLM-based explanation generators. Specifically, by adopting several key fine-tuning techniques, including parameter-eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;LLM&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#31934;&#30830;&#31572;&#26696;&#36873;&#25321;&#21644;&#25968;&#23383;&#25552;&#21462;&#12290;&#30740;&#31350;&#21457;&#29616;LLM&#22312;&#21333;&#36873;&#39064;&#21644;&#26159;&#38750;&#39064;&#19978;&#30340;&#25928;&#26524;&#36739;&#22909;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.07878</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#35780;&#20272;LLM&#65306;&#20351;&#29992;Cogtale&#25968;&#25454;&#38598;&#36827;&#34892;&#31934;&#30830;&#31572;&#26696;&#36873;&#25321;&#21644;&#25968;&#23383;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLMs on Document-Based QA: Exact Answer Selection and Numerical Extraction using Cogtale dataset. (arXiv:2311.07878v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;LLM&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#31934;&#30830;&#31572;&#26696;&#36873;&#25321;&#21644;&#25968;&#23383;&#25552;&#21462;&#12290;&#30740;&#31350;&#21457;&#29616;LLM&#22312;&#21333;&#36873;&#39064;&#21644;&#26159;&#38750;&#39064;&#19978;&#30340;&#25928;&#26524;&#36739;&#22909;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#20219;&#21153;&#23545;&#20110;&#31934;&#30830;&#30340;&#20449;&#24687;&#26816;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#19968;&#20123;&#24050;&#26377;&#30340;&#24037;&#20316;&#20851;&#27880;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#20174;&#39044;&#23450;&#20041;&#36873;&#39033;&#20013;&#36873;&#25321;&#31934;&#30830;&#31572;&#26696;&#21644;&#36827;&#34892;&#25968;&#23383;&#25552;&#21462;&#30340;QA&#31867;&#22411;&#65292;LLM&#30340;&#24615;&#33021;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#36825;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#32972;&#26223;&#65292;&#24182;&#23545;LLM&#65288;GPT-4&#21644;GPT-3.5&#65289;&#22312;&#21333;&#36873;&#39064;&#12289;&#26159;&#38750;&#39064;&#12289;&#22810;&#36873;&#39064;&#21644;&#25968;&#23383;&#25552;&#21462;&#38382;&#39064;&#31561;&#38382;&#31572;&#31867;&#22411;&#19978;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#20351;&#29992;CogTale&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20154;&#24037;&#19987;&#23478;&#26631;&#27880;&#30340;&#31572;&#26696;&#65292;&#20026;&#31934;&#30830;&#24615;&#21644;&#20107;&#23454;&#20381;&#25454;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#65292;&#23588;&#20854;&#26159;GPT-4&#65292;&#22312;&#30456;&#20851;&#32972;&#26223;&#19979;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#35768;&#22810;&#21333;&#36873;&#39064;&#21644;&#26159;&#38750;&#39064;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Question-Answering (QA) tasks are crucial for precise information retrieval. While some existing work focus on evaluating large language models performance on retrieving and answering questions from documents, assessing the LLMs performance on QA types that require exact answer selection from predefined options and numerical extraction is yet to be fully assessed. In this paper, we specifically focus on this underexplored context and conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types, including single-choice, yes-no, multiple-choice, and number extraction questions from documents in zero-shot setting. We use the CogTale dataset for evaluation, which provide human expert-tagged responses, offering a robust benchmark for precision and factual grounding. We found that LLMs, particularly GPT-4, can precisely answer many single-choice and yes-no questions given relevant context, demonstrating their efficacy in information retrieval tasks. However, their 
&lt;/p&gt;</description></item><item><title>GMMFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;&#12290;&#23427;&#36890;&#36807;&#38544;&#24335;&#24314;&#27169;&#21098;&#36753;&#34920;&#31034;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#21098;&#36753;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#35821;&#20041;&#24046;&#24322;&#23548;&#33268;&#30340;&#31232;&#30095;&#23884;&#20837;&#31354;&#38388;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05195</link><description>&lt;p&gt;
GMMFormer: &#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#29992;&#20110;&#39640;&#25928;&#30340;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval. (arXiv:2310.05195v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05195
&lt;/p&gt;
&lt;p&gt;
GMMFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;&#12290;&#23427;&#36890;&#36807;&#38544;&#24335;&#24314;&#27169;&#21098;&#36753;&#34920;&#31034;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#21098;&#36753;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#35821;&#20041;&#24046;&#24322;&#23548;&#33268;&#30340;&#31232;&#30095;&#23884;&#20837;&#31354;&#38388;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#20010;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;&#65288;PRVR&#65289;&#26088;&#22312;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21253;&#21547;&#30456;&#20851;&#29255;&#27573;&#30340;&#26410;&#21098;&#36753;&#35270;&#39057;&#12290;&#23545;&#20110;PRVR&#65292;&#21098;&#36753;&#24314;&#27169;&#23545;&#20110;&#25429;&#25417;&#25991;&#26412;&#21644;&#35270;&#39057;&#20043;&#38388;&#30340;&#37096;&#20998;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;PRVR&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#25195;&#25551;&#30340;&#21098;&#36753;&#26500;&#24314;&#26469;&#23454;&#29616;&#26174;&#24335;&#21098;&#36753;&#24314;&#27169;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#24687;&#20887;&#20313;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#23384;&#20648;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;PRVR&#26041;&#27861;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GMMFormer&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#65292;&#23427;&#38544;&#24335;&#22320;&#24314;&#27169;&#20102;&#21098;&#36753;&#34920;&#31034;&#12290;&#22312;&#24103;&#20132;&#20114;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32422;&#26463;&#65292;&#20351;&#27599;&#20010;&#24103;&#19987;&#27880;&#20110;&#20854;&#30456;&#37051;&#24103;&#32780;&#19981;&#26159;&#25972;&#20010;&#35270;&#39057;&#12290;&#28982;&#21518;&#29983;&#25104;&#30340;&#34920;&#31034;&#23558;&#21253;&#21547;&#22810;&#23610;&#24230;&#30340;&#21098;&#36753;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#24335;&#21098;&#36753;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;PRVR&#26041;&#27861;&#24573;&#35270;&#20102;&#19982;&#21516;&#19968;&#35270;&#39057;&#30456;&#20851;&#30340;&#25991;&#26412;&#26597;&#35810;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#23548;&#33268;&#31232;&#30095;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a text query, partially relevant video retrieval (PRVR) seeks to find untrimmed videos containing pertinent moments in a database. For PRVR, clip modeling is essential to capture the partial relationship between texts and videos. Current PRVR methods adopt scanning-based clip construction to achieve explicit clip modeling, which is information-redundant and requires a large storage overhead. To solve the efficiency problem of PRVR methods, this paper proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models clip representations implicitly. During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. Then generated representations will contain multi-scale clip information, achieving implicit clip modeling. In addition, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to
&lt;/p&gt;</description></item></channel></rss>