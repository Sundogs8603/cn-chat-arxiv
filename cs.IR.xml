<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#31169;&#26377;&#20108;&#27425;&#21333;&#39033;&#35745;&#31639;&#30340;&#25913;&#36827;&#23481;&#37327;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#20505;&#36873;&#20989;&#25968;&#25490;&#24207;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22522;&#20110;&#29109;&#30340;&#36138;&#23146;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06125</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#31169;&#26377;&#20108;&#27425;&#21333;&#39033;&#35745;&#31639;&#23481;&#37327;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Capacity Outer Bound for Private Quadratic Monomial Computation. (arXiv:2401.06125v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06125
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#31169;&#26377;&#20108;&#27425;&#21333;&#39033;&#35745;&#31639;&#30340;&#25913;&#36827;&#23481;&#37327;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#20505;&#36873;&#20989;&#25968;&#25490;&#24207;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22522;&#20110;&#29109;&#30340;&#36138;&#23146;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#35745;&#31639;&#20013;&#65292;&#29992;&#25143;&#24076;&#26395;&#22312;&#19968;&#32452;&#25968;&#25454;&#24211;&#19978;&#26816;&#32034;&#23384;&#20648;&#30340;&#28040;&#24687;&#30340;&#20989;&#25968;&#35780;&#20272;&#65292;&#32780;&#19981;&#21521;&#25968;&#25454;&#24211;&#36879;&#38706;&#20989;&#25968;&#30340;&#36523;&#20221;&#12290;Obead&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#20010;&#31169;&#26377;&#38750;&#32447;&#24615;&#35745;&#31639;&#30340;&#23481;&#37327;&#19978;&#38480;&#65292;&#21462;&#20915;&#20110;&#20505;&#36873;&#20989;&#25968;&#30340;&#39034;&#24207;&#12290;&#38024;&#23545;&#31169;&#26377;&#20108;&#27425;&#21333;&#39033;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20505;&#36873;&#20989;&#25968;&#25490;&#24207;&#26041;&#27861;&#65306;&#22270;&#30340;&#36793;&#30528;&#33394;&#26041;&#27861;&#65292;&#22270;&#30340;&#36317;&#31163;&#26041;&#27861;&#21644;&#22522;&#20110;&#29109;&#30340;&#36138;&#23146;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#35777;&#23454;&#65292;&#36825;&#19977;&#31181;&#26041;&#27861;&#23545;&#20110;$f &lt; 6$&#30340;&#28040;&#24687;&#37117;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#30340;&#25490;&#24207;&#12290;&#23545;&#20110;$6 \leq f \leq 12$&#30340;&#28040;&#24687;&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#25152;&#25552;&#26041;&#27861;&#19982;&#23450;&#21521;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;&#20960;&#20046;&#32771;&#34385;&#30340;&#25152;&#26377;&#22330;&#26223;&#20013;&#65292;&#22522;&#20110;&#29109;&#30340;&#36138;&#23146;&#26041;&#27861;&#32473;&#20986;&#20102;&#19982;&#26368;&#20339;&#25490;&#24207;&#26368;&#23567;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In private computation, a user wishes to retrieve a function evaluation of messages stored on a set of databases without revealing the function's identity to the databases. Obead \emph{et al.} introduced a capacity outer bound for private nonlinear computation, dependent on the order of the candidate functions. Focusing on private \emph{quadratic monomial} computation, we propose three methods for ordering candidate functions: a graph edge-coloring method, a graph-distance method, and an entropy-based greedy method. We confirm, via an exhaustive search, that all three methods yield an optimal ordering for $f &lt; 6$ messages. For $6 \leq f \leq 12$ messages, we numerically evaluate the performance of the proposed methods compared with a directed random search. For almost all scenarios considered, the entropy-based greedy method gives the smallest gap to the best-found ordering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05967</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20998;&#22359;&#23545;&#35282;&#27491;&#20132;&#20851;&#31995;&#21644;&#30697;&#38453;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#20197;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26059;&#36716;-based&#26041;&#27861;&#22914;RotatE&#21644;QuatE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#19982;&#23454;&#20307;&#32500;&#24230;&#25104;&#27604;&#20363;&#22320;&#22686;&#21152;&#20851;&#31995;&#22823;&#23567;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26059;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OrthogonalE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#19982;Riemannian&#20248;&#21270;&#34920;&#31034;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#26082;&#20855;&#26377;&#24191;&#27867;&#24615;&#21448;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#20851;&#31995;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.
&lt;/p&gt;</description></item><item><title>DREQ&#26159;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#30340;&#23494;&#38598;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#35843;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23454;&#20307;&#24182;&#20943;&#24369;&#19981;&#22826;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#33719;&#24471;&#19968;&#20010;&#26597;&#35810;&#29305;&#23450;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#36827;&#34892;&#28151;&#21512;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05939</link><description>&lt;p&gt;
DREQ: &#20351;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#26597;&#35810;&#29702;&#35299;&#36827;&#34892;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DREQ: Document Re-Ranking Using Entity-based Query Understanding. (arXiv:2401.05939v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05939
&lt;/p&gt;
&lt;p&gt;
DREQ&#26159;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#30340;&#23494;&#38598;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#35843;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23454;&#20307;&#24182;&#20943;&#24369;&#19981;&#22826;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#33719;&#24471;&#19968;&#20010;&#26597;&#35810;&#29305;&#23450;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#36827;&#34892;&#28151;&#21512;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#23454;&#20307;&#30340;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#32454;&#24494;&#24046;&#21035;&#65306;&#25991;&#26723;&#20013;&#21508;&#20010;&#23454;&#20307;&#23545;&#20854;&#25972;&#20307;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#31243;&#24230;&#26159;&#19981;&#21516;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DREQ&#65292;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#30340;&#23494;&#38598;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#12290;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#22312;&#25991;&#26723;&#30340;&#34920;&#31034;&#20013;&#24378;&#35843;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20943;&#24369;&#19981;&#22826;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#20010;&#26597;&#35810;&#29305;&#23450;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#31181;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#19982;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#33719;&#24471;&#25991;&#26723;&#30340;&#8220;&#28151;&#21512;&#8221;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;DREQ&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#22522;&#20110;&#23454;&#20307;&#30340;&#34920;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a "hybrid" representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#20026;&#24120;&#35265;&#30340;CRS&#25968;&#25454;&#38598;&#20013;&#30340;&#26367;&#20195;&#29289;&#21697;&#33719;&#21462;&#39069;&#22806;&#30340;&#35780;&#20215;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05783</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26041;&#26696;&#25913;&#36827;&#26102;&#23578;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#25105;&#36824;&#24819;&#35201;&#20160;&#20040;&#65311;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120; (arXiv:2401.05783v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
What Else Would I Like? A User Simulator using Alternatives for Improved Evaluation of Fashion Conversational Recommendation Systems. (arXiv:2401.05783v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#20026;&#24120;&#35265;&#30340;CRS&#25968;&#25454;&#38598;&#20013;&#30340;&#26367;&#20195;&#29289;&#21697;&#33719;&#21462;&#39069;&#22806;&#30340;&#35780;&#20215;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479; (CRS) &#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27599;&#27425;&#20132;&#20114;&#20013;&#25552;&#20379;&#23545;&#25512;&#33616;&#29289;&#21697;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#20351;CRS&#26397;&#26356;&#29702;&#24819;&#30340;&#25512;&#33616;&#26041;&#21521;&#21457;&#23637;&#12290;&#30446;&#21069;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;CRS&#25552;&#20379;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#21453;&#39304;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25110;&#22238;&#31572;&#28548;&#28165;&#38382;&#39064;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;CRS&#12290;&#36825;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#36890;&#24120;&#26681;&#25454;&#23545;&#21333;&#20010;&#30446;&#26631;&#29289;&#21697;&#30340;&#20102;&#35299;&#23545;&#24403;&#21069;&#26816;&#32034;&#21040;&#30340;&#29289;&#21697;&#36827;&#34892;&#25209;&#21028;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27169;&#25311;&#22120;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#35780;&#20272;&#31995;&#32479;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#23436;&#20840;&#20851;&#27880;&#20110;&#21333;&#20010;&#30446;&#26631;&#29289;&#21697;&#65288;&#19981;&#33021;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#30340;&#25506;&#32034;&#24615;&#36136;&#65289;&#65292;&#24182;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#32784;&#24515;&#65288;&#22312;&#22823;&#37327;&#20132;&#20114;&#19978;&#32473;&#20986;&#19968;&#33268;&#30340;&#21453;&#39304;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#24120;&#35265;&#30340;CRS&#25968;&#25454;&#38598;&#65288;Shoes&#21644;Fashion IQ Dresses&#65289;&#20013;&#20026;&#19968;&#37096;&#20998;&#26367;&#20195;&#29289;&#21697;&#33719;&#21462;&#39069;&#22806;&#30340;&#35780;&#20215;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;CRS&#30340;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Conversational Recommendation Systems (CRS), a user can provide feedback on recommended items at each interaction turn, leading the CRS towards more desirable recommendations. Currently, different types of CRS offer various possibilities for feedback, i.e., natural language feedback, or answering clarifying questions. In most cases, a user simulator is employed for training as well as evaluating the CRS. Such user simulators typically critique the current retrieved items based on knowledge of a single target item. Still, evaluating systems in offline settings with simulators suffers from problems, such as focusing entirely on a single target item (not addressing the exploratory nature of a recommender system), and exhibiting extreme patience (consistent feedback over a large number of turns). To overcome these limitations, we obtain extra judgements for a selection of alternative items in common CRS datasets, namely Shoes and Fashion IQ Dresses. Going further, we propose improved us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Lifelogging&#20316;&#20026;&#26497;&#33268;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#30340;&#24418;&#24335;&#65292;&#25506;&#35752;&#20102;&#20010;&#20154;&#25968;&#25454;&#30340;&#25628;&#38598;&#21644;&#35775;&#38382;&#26041;&#24335;&#65292;&#24182;&#21628;&#21505;&#23545;&#31532;&#19977;&#26041;&#20351;&#29992;&#20010;&#20154;&#25968;&#25454;&#30340;&#23457;&#24910;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05767</link><description>&lt;p&gt;
Lifelogging&#20316;&#20026;&#26497;&#33268;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#30340;&#19968;&#31181;&#24418;&#24335; -- &#38656;&#35201;&#23398;&#20064;&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Lifelogging As An Extreme Form of Personal Information Management -- What Lessons To Learn. (arXiv:2401.05767v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Lifelogging&#20316;&#20026;&#26497;&#33268;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#30340;&#24418;&#24335;&#65292;&#25506;&#35752;&#20102;&#20010;&#20154;&#25968;&#25454;&#30340;&#25628;&#38598;&#21644;&#35775;&#38382;&#26041;&#24335;&#65292;&#24182;&#21628;&#21505;&#23545;&#31532;&#19977;&#26041;&#20351;&#29992;&#20010;&#20154;&#25968;&#25454;&#30340;&#23457;&#24910;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#25968;&#25454;&#21253;&#25324;&#25105;&#20204;&#22312;&#26085;&#24120;&#27963;&#21160;&#20013;&#30041;&#19979;&#30340;&#25968;&#23383;&#36275;&#36857;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#36824;&#26159;&#31163;&#32447;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#12290;&#23427;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#20182;&#20154;&#25910;&#38598;&#30340;&#26377;&#20851;&#25105;&#20204;&#22312;&#32447;&#34892;&#20026;&#21644;&#27963;&#21160;&#30340;&#25968;&#25454;&#12290;&#26377;&#26102;&#20505;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#20197;&#20415;&#26816;&#26597;&#25105;&#20204;&#29983;&#27963;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20010;&#20154;&#25968;&#25454;&#34987;&#31532;&#19977;&#26041;&#65292;&#21253;&#25324;&#20114;&#32852;&#32593;&#20844;&#21496;&#65292;&#29992;&#20110;&#23450;&#21521;&#24191;&#21578;&#21644;&#25512;&#33616;&#31561;&#26381;&#21153;&#12290;Lifelogging&#26159;&#19968;&#31181;&#26497;&#31471;&#20010;&#20154;&#25968;&#25454;&#25910;&#38598;&#30340;&#24418;&#24335;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#26368;&#36817;&#19968;&#27425;Lifelog&#25628;&#32034;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#30740;&#35752;&#20250;&#19978;&#23637;&#31034;&#30340;&#29992;&#20110;&#31649;&#29702;&#23545;Lifelogs&#30340;&#35775;&#38382;&#30340;&#24037;&#20855;&#30340;&#27010;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#23454;&#39564;&#31995;&#32479;&#36890;&#36807;&#30495;&#23454;&#29992;&#25143;&#23637;&#31034;&#20102;&#23454;&#26102;&#20449;&#24687;&#25628;&#32034;&#20219;&#21153;&#12290;&#36825;&#20123;&#31995;&#32479;&#33021;&#21147;&#30340;&#27010;&#36848;&#23637;&#31034;&#20102;&#35775;&#38382;Lifelog&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal data includes the digital footprints that we leave behind as part of our everyday activities, both online and offline in the real world. It includes data we collect ourselves, such as from wearables, as well as the data collected by others about our online behaviour and activities. Sometimes we are able to use the personal data we ourselves collect, in order to examine some parts of our lives but for the most part, our personal data is leveraged by third parties including internet companies, for services like targeted advertising and recommendations. Lifelogging is a form of extreme personal data gathering and in this article we present an overview of the tools used to manage access to lifelogs as demonstrated at the most recent of the annual Lifelog Search Challenge benchmarking workshops. Here, experimental systems are showcased in live, real time information seeking tasks by real users. This overview of these systems' capabilities show the range of possibilities for accessi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19981;&#21516;&#20449;&#24687;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#20102;&#20840;&#38754;&#23545;&#27604;&#30740;&#31350;&#65292;&#21457;&#29616;&#29992;&#25143;&#23545;LLM&#26377;&#26126;&#26174;&#30340;&#20542;&#21521;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05761</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#65306;&#22312;&#19981;&#21516;&#20449;&#24687;&#26816;&#32034;&#22330;&#26223;&#20013;&#35780;&#20272;&#29992;&#25143;&#20559;&#22909;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models vs. Search Engines: Evaluating User Preferences Across Varied Information Retrieval Scenarios. (arXiv:2401.05761v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19981;&#21516;&#20449;&#24687;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#20102;&#20840;&#38754;&#23545;&#27604;&#30740;&#31350;&#65292;&#21457;&#29616;&#29992;&#25143;&#23545;LLM&#26377;&#26126;&#26174;&#30340;&#20542;&#21521;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#19981;&#21516;&#20449;&#24687;&#26816;&#32034;&#22330;&#26223;&#20013;&#20840;&#38754;&#25506;&#35752;&#20102;&#29992;&#25143;&#23545;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;&#32654;&#22269;&#21508;&#22320;&#30340;100&#21517;&#20114;&#32852;&#32593;&#29992;&#25143;&#65288;N=100&#65289;&#36827;&#34892;&#35843;&#26597;&#65292;&#30740;&#31350;&#28085;&#30422;&#20102;20&#20010;&#19981;&#21516;&#30340;&#29992;&#20363;&#65292;&#28085;&#30422;&#20102;&#20174;&#20107;&#23454;&#25628;&#32034;&#65288;&#20363;&#22914;&#26597;&#25214;COVID-19&#25351;&#21335;&#65289;&#21040;&#26356;&#20027;&#35266;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#23547;&#27714;&#20197;&#38750;&#19987;&#19994;&#26415;&#35821;&#35299;&#37322;&#22797;&#26434;&#27010;&#24565;&#65289;&#30340;&#33539;&#22260;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#36873;&#25321;&#20351;&#29992;&#20256;&#32479;&#25628;&#32034;&#24341;&#25806;&#36824;&#26159;LLM&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#28145;&#20837;&#20102;&#35299;&#29992;&#25143;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#22914;&#20309;&#35748;&#30693;&#21644;&#20351;&#29992;&#36825;&#20004;&#31181;&#20027;&#35201;&#25968;&#23383;&#24037;&#20855;&#12290;&#29992;&#20363;&#30340;&#36873;&#25321;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#29992;&#25143;&#36873;&#25321;&#20013;&#30340;&#26377;&#36259;&#27169;&#24335;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#20102;&#29992;&#25143;&#23545;LLM&#30340;&#20542;&#21521;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study embarked on a comprehensive exploration of user preferences between Search Engines and Large Language Models (LLMs) in the context of various information retrieval scenarios. Conducted with a sample size of 100 internet users (N=100) from across the United States, the research delved into 20 distinct use cases ranging from factual searches, such as looking up COVID-19 guidelines, to more subjective tasks, like seeking interpretations of complex concepts in layman's terms. Participants were asked to state their preference between using a traditional search engine or an LLM for each scenario. This approach allowed for a nuanced understanding of how users perceive and utilize these two predominant digital tools in differing contexts. The use cases were carefully selected to cover a broad spectrum of typical online queries, thus ensuring a comprehensive analysis of user preferences. The findings reveal intriguing patterns in user choices, highlighting a clear tendency for partic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#26816;&#32034;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#26816;&#32034;&#26469;&#24357;&#21512;&#23454;&#20307;&#19982;&#20854;&#35270;&#35273;&#34920;&#29616;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.05736</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#26816;&#32034;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Retrieval for Knowledge-based Visual Question Answering. (arXiv:2401.05736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#26816;&#32034;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#26816;&#32034;&#26469;&#24357;&#21512;&#23454;&#20307;&#19982;&#20854;&#35270;&#35273;&#34920;&#29616;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20174;&#22810;&#27169;&#24577;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#21629;&#21517;&#23454;&#20307;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#34920;&#29616;&#65292;&#22240;&#27492;&#24456;&#38590;&#35782;&#21035;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#21487;&#20197;&#24110;&#21161;&#24357;&#21512;&#23454;&#20307;&#19982;&#20854;&#34920;&#29616;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#24182;&#19988;&#19982;&#21333;&#27169;&#24577;&#26816;&#32034;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23545;&#26368;&#36817;&#30340;ViQuAE&#12289;InfoSeek&#21644;Encyclopedic-VQA&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#27169;&#24577;&#21452;&#32534;&#30721;&#22120;&#65288;&#21363;CLIP&#65289;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#24494;&#35843;&#31574;&#30053;&#65306;&#21333;&#27169;&#24577;&#12289;&#36328;&#27169;&#24577;&#25110;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21333;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#19982;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Visual Question Answering about Named Entities is a challenging task that requires retrieving information from a multimodal Knowledge Base. Named entities have diverse visual representations and are therefore difficult to recognize. We argue that cross-modal retrieval may help bridge the semantic gap between an entity and its depictions, and is foremost complementary with mono-modal retrieval. We provide empirical evidence through experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE, InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different strategies to fine-tune such a model: mono-modal, cross-modal, or joint training. Our method, which combines mono-and cross-modal retrieval, is competitive with billion-parameter models on the three datasets, while being conceptually simpler and computationally cheaper.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PEEKC&#25968;&#25454;&#38598;&#21644;TrueLearn Python&#24211;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#32946;&#35270;&#39057;&#30340;&#21442;&#19982;&#24230;&#12290;TrueLearn&#27169;&#22411;&#31995;&#21015;&#36981;&#24490;"&#24320;&#25918;&#23398;&#20064;&#32773;"&#27010;&#24565;&#65292;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#27169;&#22411;&#20197;&#21450;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05424</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#25945;&#32946;&#35270;&#39057;&#21442;&#19982;&#24230;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Modelling Engagement with Educational Videos. (arXiv:2401.05424v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PEEKC&#25968;&#25454;&#38598;&#21644;TrueLearn Python&#24211;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#32946;&#35270;&#39057;&#30340;&#21442;&#19982;&#24230;&#12290;TrueLearn&#27169;&#22411;&#31995;&#21015;&#36981;&#24490;"&#24320;&#25918;&#23398;&#20064;&#32773;"&#27010;&#24565;&#65292;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#27169;&#22411;&#20197;&#21450;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#21644;&#24212;&#29992;&#65292;&#23558;&#25945;&#32946;&#20010;&#24615;&#21270;&#21040;&#20840;&#29699;&#20154;&#21475;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#26032;&#25945;&#32946;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PEEKC&#25968;&#25454;&#38598;&#21644;TrueLearn Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#22312;&#32447;&#23398;&#20064;&#32773;&#29366;&#24577;&#27169;&#22411;&#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#23545;&#20110;&#20419;&#36827;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#24314;&#27169;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;TrueLearn&#27169;&#22411;&#31995;&#21015;&#36981;&#24490;&#8220;&#24320;&#25918;&#23398;&#20064;&#32773;&#8221;&#27010;&#24565;&#65292;&#20351;&#29992;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#31034;&#27861;&#36827;&#34892;&#35774;&#35745;&#12290;&#36825;&#19968;&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#27169;&#22411;&#31995;&#21015;&#36824;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#21487;&#35270;&#21270;&#23398;&#20064;&#32773;&#27169;&#22411;&#65292;&#36825;&#22312;&#26410;&#26469;&#21487;&#33021;&#20419;&#36827;&#29992;&#25143;&#19982;&#33258;&#24049;&#30340;&#27169;&#22411;/&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#12290;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#20351;&#35813;&#24211;&#23545;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21644;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#19982;&#23398;&#20064;&#20998;&#26512;&#23454;&#36341;&#32773;&#37117;&#38750;&#24120;&#26131;&#20110;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25968;&#25454;&#38598;&#21644;&#24211;&#30340;&#23454;&#29992;&#24615;&#65292;&#39044;&#27979;&#24615;&#33021;&#26126;&#26174;&#36229;&#36807;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement and utility of Artificial Intelligence (AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models was designed following the "open learner" concept, using humanly-intuitive user representations. This family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytics practitioners. The experiments show the utility of both the dataset and the library with predictive performance significantly exceeding compa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05418</link><description>&lt;p&gt;
ANALYTiC: &#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#38477;&#32500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#20415;&#25658;&#35774;&#22791;&#30340;&#20986;&#29616;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#25209;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#36235;&#21183;&#21644;&#27169;&#24335;&#30340;&#36319;&#36394;&#36816;&#21160;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#24212;&#29992;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#30340;&#32452;&#21512;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#21033;&#29992;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#24182;&#25552;&#39640;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#32452;&#21512;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#20013;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of compact, handheld devices has given us a pool of tracked movement data that could be used to infer trends and patterns that can be made to use. With this flooding of various trajectory data of animals, humans, vehicles, etc., the idea of ANALYTiC originated, using active learning to infer semantic annotations from the trajectories by learning from sets of labeled data. This study explores the application of dimensionality reduction and decision boundaries in combination with the already present active learning, highlighting patterns and clusters in data. We test these features with three different trajectory datasets with objective of exploiting the the already labeled data and enhance their interpretability. Our experimental analysis exemplifies the potential of these combined methodologies in improving the efficiency and accuracy of trajectory labeling. This study serves as a stepping-stone towards the broader integration of machine learning and visual methods in contex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SComGNN&#65289;&#29992;&#20110;&#27169;&#25311;&#21644;&#29702;&#35299;&#21830;&#21697;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#21830;&#21697;&#12290;</title><link>http://arxiv.org/abs/2401.02130</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20114;&#34917;&#21830;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Spectral-based Graph Neutral Networks for Complementary Item Recommendation. (arXiv:2401.02130v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SComGNN&#65289;&#29992;&#20110;&#27169;&#25311;&#21644;&#29702;&#35299;&#21830;&#21697;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#21830;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20114;&#34917;&#20851;&#31995;&#26497;&#22823;&#22320;&#24110;&#21161;&#25512;&#33616;&#31995;&#32479;&#22312;&#36141;&#20080;&#19968;&#20010;&#21830;&#21697;&#21518;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#30340;&#21830;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#30456;&#20284;&#20851;&#31995;&#19981;&#21516;&#65292;&#20855;&#26377;&#20114;&#34917;&#20851;&#31995;&#30340;&#21830;&#21697;&#21487;&#33021;&#20250;&#36830;&#32493;&#36141;&#20080;&#65288;&#20363;&#22914;iPhone&#21644;AirPods Pro&#65289;&#65292;&#23427;&#20204;&#19981;&#20165;&#20849;&#20139;&#30456;&#20851;&#24615;&#65292;&#36824;&#23637;&#29616;&#20986;&#19981;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#23646;&#24615;&#26159;&#30456;&#21453;&#30340;&#65292;&#24314;&#27169;&#20114;&#34917;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#23581;&#35797;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#25110;&#36807;&#24230;&#31616;&#21270;&#20102;&#19981;&#30456;&#20284;&#24615;&#23646;&#24615;&#65292;&#23548;&#33268;&#24314;&#27169;&#26080;&#25928;&#24182;&#19988;&#26080;&#27861;&#24179;&#34913;&#36825;&#20004;&#20010;&#23646;&#24615;&#12290;&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#22312;&#39057;&#35889;&#22495;&#20013;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#22522;&#20110;&#39057;&#35889;&#30340;GNNs&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24314;&#27169;&#20114;&#34917;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#39057;&#35889;&#30340;&#20114;&#34917;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SComGNN&#65289;&#65292;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#27604;&#36739;&#22909;&#22320;&#21033;&#29992;&#20114;&#34917;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling complementary relationships greatly helps recommender systems to accurately and promptly recommend the subsequent items when one item is purchased. Unlike traditional similar relationships, items with complementary relationships may be purchased successively (such as iPhone and Airpods Pro), and they not only share relevance but also exhibit dissimilarity. Since the two attributes are opposites, modeling complementary relationships is challenging. Previous attempts to exploit these relationships have either ignored or oversimplified the dissimilarity attribute, resulting in ineffective modeling and an inability to balance the two attributes. Since Graph Neural Networks (GNNs) can capture the relevance and dissimilarity between nodes in the spectral domain, we can leverage spectral-based GNNs to effectively understand and model complementary relationships. In this study, we present a novel approach called Spectral-based Complementary Graph Neural Networks (SComGNN) that utilize
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#29992;&#20110;&#27604;&#36739;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01527</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Poisoning Attacks against Recommender Systems: A Survey. (arXiv:2401.01527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#29992;&#20110;&#27604;&#36739;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#27963;&#21160;&#30340;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#27602;&#21270;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#23558;&#24694;&#24847;&#25968;&#25454;&#27880;&#20837;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#23436;&#25972;&#24615;&#65292;&#24182;&#36890;&#36807;&#25805;&#32437;&#25512;&#33616;&#32467;&#26524;&#26469;&#33719;&#21462;&#38750;&#27861;&#21033;&#28070;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#29616;&#26377;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32452;&#20214;&#29305;&#23450;&#12289;&#30446;&#26631;&#39537;&#21160;&#21644;&#33021;&#21147;&#25506;&#27979;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#20854;&#26426;&#21046;&#20197;&#21450;&#30456;&#20851;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#21644;&#22522;&#20934;&#27979;&#35797;&#27602;&#21270;&#25915;&#20987;&#30340;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#65292;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#25972;&#22871;&#27602;&#21270;&#25915;&#20987;&#27169;&#22411;&#21644;&#24120;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiCBR&#30340;&#26032;&#22411;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25414;&#32465;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#29992;&#25143;&#12289;&#25414;&#32465;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#24322;&#26500;&#20851;&#31995;&#65292;&#24182;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#25512;&#24191;&#21040;&#22810;&#20010;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2311.16751</link><description>&lt;p&gt;
&#22810;CBR&#65306;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25414;&#32465;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation. (arXiv:2311.16751v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiCBR&#30340;&#26032;&#22411;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25414;&#32465;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#29992;&#25143;&#12289;&#25414;&#32465;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#24322;&#26500;&#20851;&#31995;&#65292;&#24182;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#25512;&#24191;&#21040;&#22810;&#20010;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25414;&#32465;&#25512;&#33616;&#26088;&#22312;&#21521;&#29992;&#25143;&#25512;&#33616;&#19968;&#32452;&#30456;&#20851;&#30340;&#39033;&#30446;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#24179;&#21488;&#21033;&#28070;&#12290;&#29616;&#26377;&#30340;&#25414;&#32465;&#25512;&#33616;&#27169;&#22411;&#24050;&#32463;&#20174;&#20165;&#25429;&#25417;&#29992;&#25143;-&#25414;&#32465;&#20132;&#20114;&#21457;&#23637;&#21040;&#23545;&#29992;&#25143;&#12289;&#25414;&#32465;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22810;&#20010;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;CrossCBR&#23588;&#20854;&#23558;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;&#21040;&#20004;&#35270;&#22270;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SOTA&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#20004;&#35270;&#22270;&#30340;&#26500;&#24314;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#12289;&#25414;&#32465;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#25152;&#26377;&#24322;&#26500;&#20851;&#31995;&#65307;2&#65289;"&#26089;&#26399;&#23545;&#27604;&#21644;&#21518;&#26399;&#34701;&#21512;"&#26694;&#26550;&#22312;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#25512;&#24191;&#21040;&#22810;&#20010;&#35270;&#22270;&#26041;&#38754;&#19981;&#22815;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;MultiCBR&#65292;&#29992;&#20110;&#25414;&#32465;&#25512;&#33616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;-&#25414;&#32465;&#12289;&#29992;&#25143;-&#39033;&#30446;&#21644;...&#65288;&#25688;&#35201;&#20869;&#23481;&#19981;&#20840;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bundle recommendation seeks to recommend a bundle of related items to users to improve both user experience and the profits of platform. Existing bundle recommendation models have progressed from capturing only user-bundle interactions to the modeling of multiple relations among users, bundles and items. CrossCBR, in particular, incorporates cross-view contrastive learning into a two-view preference learning framework, significantly improving SOTA performance. It does, however, have two limitations: 1) the two-view formulation does not fully exploit all the heterogeneous relations among users, bundles and items; and 2) the "early contrast and late fusion" framework is less effective in capturing user preference and difficult to generalize to multiple views. In this paper, we present MultiCBR, a novel Multi-view Contrastive learning framework for Bundle Recommendation. First, we devise a multi-view representation learning framework capable of capturing all the user-bundle, user-item and
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#31181;&#26893;&#30340;XY&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#33258;&#26059;&#29627;&#29827;&#65292;&#22312;&#28201;&#24230;&#21644;&#38081;&#30913;&#20559;&#32622;&#24179;&#38754;&#19978;&#25512;&#23548;&#20986;&#20102;&#22797;&#21046;&#23545;&#31216;&#30456;&#22270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#27493;&#21103;&#26412;&#23545;&#31216;&#24615;&#30772;&#32570;&#20998;&#26512;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#22797;&#21046;&#23545;&#31216;&#36817;&#20284;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#27979;&#37327;&#20013;&#30340;&#20122;&#31283;&#24577;&#25968;&#30446;&#12290;</title><link>http://arxiv.org/abs/2208.06488</link><description>&lt;p&gt;
&#31181;&#26893;&#30340;XY&#27169;&#22411;&#65306;&#28909;&#21147;&#23398;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
The planted XY model: thermodynamics and inference. (arXiv:2208.06488v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#31181;&#26893;&#30340;XY&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#33258;&#26059;&#29627;&#29827;&#65292;&#22312;&#28201;&#24230;&#21644;&#38081;&#30913;&#20559;&#32622;&#24179;&#38754;&#19978;&#25512;&#23548;&#20986;&#20102;&#22797;&#21046;&#23545;&#31216;&#30456;&#22270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#27493;&#21103;&#26412;&#23545;&#31216;&#24615;&#30772;&#32570;&#20998;&#26512;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#22797;&#21046;&#23545;&#31216;&#36817;&#20284;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#27979;&#37327;&#20013;&#30340;&#20122;&#31283;&#24577;&#25968;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20840;&#36830;&#25509;&#30340;&#31181;&#26893;&#33258;&#26059;&#29627;&#29827;&#27169;&#22411;&#65292;&#31216;&#20026;&#31181;&#26893;&#30340;XY&#27169;&#22411;&#12290;&#30740;&#31350;&#35813;&#31995;&#32479;&#30340;&#21160;&#26426;&#26469;&#33258;&#33258;&#26059;&#29627;&#29827;&#39046;&#22495;&#21644;&#32479;&#35745;&#25512;&#26029;&#39046;&#22495;&#65292;&#20854;&#20013;&#23427;&#27169;&#25311;&#20102;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#65288;AMP&#65289;&#31639;&#27861;&#21644;&#20854;&#29366;&#24577;&#28436;&#21270;&#65288;SE&#65289;&#25512;&#23548;&#20986;&#20102;&#28201;&#24230;-&#38081;&#30913;&#20559;&#32622;&#24179;&#38754;&#19978;&#30340;&#22797;&#21046;&#23545;&#31216;&#65288;RS&#65289;&#30456;&#22270;&#12290;&#22312;Nishimori&#32447;&#19978;&#65288;&#21363;&#24403;&#28201;&#24230;&#19982;&#38081;&#30913;&#20559;&#24046;&#21305;&#37197;&#26102;&#65289;&#65292;RS&#39044;&#27979;&#26159;&#31934;&#30830;&#30340;&#65292;&#20294;&#24403;&#21442;&#25968;&#19981;&#21305;&#37197;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#19981;&#20934;&#30830;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#33258;&#26059;&#29627;&#29827;&#30456;&#65292;&#20854;&#20013;AMP&#26080;&#27861;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;RS&#36817;&#20284;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27493;&#21103;&#26412;&#23545;&#31216;&#24615;&#30772;&#32570;&#65288;1RSB&#65289;&#20998;&#26512;&#65292;&#22522;&#20110;&#36817;&#20284;&#35843;&#26597;&#20256;&#25773;&#65288;ASP&#65289;&#31639;&#27861;&#12290;&#21033;&#29992;ASP&#30340;&#29366;&#24577;&#28436;&#21270;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#27979;&#37327;&#20013;&#30340;&#20122;&#31283;&#24577;&#25968;&#30446;&#65292;&#25512;&#23548;&#20986;&#20102;1RSB&#33258;&#30001;&#29109;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study a fully connected planted spin glass named the planted XY model. Motivation for studying this system comes both from the spin glass field and the one of statistical inference where it models the angular synchronization problem. We derive the replica symmetric (RS) phase diagram in the temperature, ferromagnetic bias plane using the approximate message passing (AMP) algorithm and its state evolution (SE). While the RS predictions are exact on the Nishimori line (i.e. when the temperature is matched to the ferromagnetic bias), they become inaccurate when the parameters are mismatched, giving rise to a spin glass phase where AMP is not able to converge. To overcome the defects of the RS approximation we carry out a one-step replica symmetry breaking (1RSB) analysis based on the approximate survey propagation (ASP) algorithm. Exploiting the state evolution of ASP, we count the number of metastable states in the measure, derive the 1RSB free entropy and find the behav
&lt;/p&gt;</description></item></channel></rss>