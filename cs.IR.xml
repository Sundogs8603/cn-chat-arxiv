<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05200</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65306;&#29992;&#25143;&#35780;&#20272;&#21644;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05200
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#31649;&#29702;&#30693;&#35782;&#23545;&#32452;&#32455;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#25805;&#20316;&#24037;&#21378;&#21464;&#24471;&#36234;&#26469;&#36234;&#20381;&#36182;&#30693;&#35782;&#65292;&#36825;&#32473;&#24037;&#21378;&#22521;&#35757;&#21644;&#25903;&#25345;&#26032;&#25805;&#20316;&#21592;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#21387;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#24037;&#21378;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#24191;&#27867;&#30693;&#35782;&#65292;&#39640;&#25928;&#22238;&#31572;&#25805;&#20316;&#21592;&#30340;&#26597;&#35810;&#24182;&#20419;&#36827;&#26032;&#30693;&#35782;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24037;&#21378;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#30340;&#22909;&#22788;&#65292;&#21363;&#33021;&#22815;&#26356;&#24555;&#22320;&#26816;&#32034;&#20449;&#24687;&#21644;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#26356;&#20542;&#21521;&#20110;&#21521;&#20154;&#24037;&#19987;&#23478;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#20960;&#31181;&#38381;&#28304;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;GPT-4&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20687;StableBe
&lt;/p&gt;
&lt;p&gt;
Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBe
&lt;/p&gt;</description></item><item><title>&#33258;&#36866;&#24212;&#38590;&#24230;&#36127;&#37319;&#26679;&#65288;AHNS&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#38590;&#24230;&#30340;&#36127;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#36127;&#37319;&#26679;&#26041;&#27861;&#20013;&#30340;&#35823;&#21028;&#21644;&#35823;&#36127;&#38382;&#39064;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#35757;&#32451;&#20449;&#21495;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05191</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38590;&#24230;&#36127;&#37319;&#26679;&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hardness Negative Sampling for Collaborative Filtering. (arXiv:2401.05191v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05191
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#38590;&#24230;&#36127;&#37319;&#26679;&#65288;AHNS&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#38590;&#24230;&#30340;&#36127;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#36127;&#37319;&#26679;&#26041;&#27861;&#20013;&#30340;&#35823;&#21028;&#21644;&#35823;&#36127;&#38382;&#39064;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#35757;&#32451;&#20449;&#21495;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#37319;&#26679;&#23545;&#20110;&#38544;&#24335;&#21327;&#21516;&#36807;&#28388;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#25552;&#20379;&#36866;&#24403;&#30340;&#36127;&#35757;&#32451;&#20449;&#21495;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#37117;&#21482;&#33021;&#36873;&#25321;&#22266;&#23450;&#38590;&#24230;&#32423;&#21035;&#30340;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#35823;&#21028;&#38382;&#39064;&#21644;&#35823;&#36127;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#33258;&#36866;&#24212;&#38590;&#24230;&#36127;&#37319;&#26679;&#65288;AHNS&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#19977;&#20010;&#20851;&#38190;&#20934;&#21017;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#38590;&#24230;&#30340;&#36127;&#26679;&#26412;&#65292;AHNS&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#35823;&#21028;&#38382;&#39064;&#21644;&#35823;&#36127;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;AHNS&#23454;&#20363;&#8212;&#8212;AHNS_{p&lt;0}&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AHNS_{p&lt;0}&#21487;&#20197;&#24456;&#22909;&#22320;&#31526;&#21512;AHNS&#30340;&#19977;&#20010;&#20934;&#21017;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#29616;&#26377;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;AHNS&#30340;&#26356;&#23485;&#26494;&#30340;&#29305;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;c
&lt;/p&gt;
&lt;p&gt;
Negative sampling is essential for implicit collaborative filtering to provide proper negative training signals so as to achieve desirable performance. We experimentally unveil a common limitation of all existing negative sampling methods that they can only select negative samples of a fixed hardness level, leading to the false positive problem (FPP) and false negative problem (FNP). We then propose a new paradigm called adaptive hardness negative sampling (AHNS) and discuss its three key criteria. By adaptively selecting negative samples with appropriate hardnesses during the training process, AHNS can well mitigate the impacts of FPP and FNP. Next, we present a concrete instantiation of AHNS called AHNS_{p&lt;0}, and theoretically demonstrate that AHNS_{p&lt;0} can fit the three criteria of AHNS well and achieve a larger lower bound of normalized discounted cumulative gain. Besides, we note that existing negative sampling methods can be regarded as more relaxed cases of AHNS. Finally, we c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38405;&#35835;&#24207;&#21015;&#23545;Web&#25628;&#32034;&#20013;&#30340;&#30693;&#35782;&#33719;&#21462;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25193;&#23637;&#38405;&#35835;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#30693;&#35782;&#33719;&#21462;&#37327;&#36739;&#39640;&#30340;&#23398;&#20064;&#32773;&#22312;&#38405;&#35835;&#19978;&#33457;&#36153;&#20102;&#26356;&#22810;&#26102;&#38388;&#65292;&#24182;&#19988;&#24635;&#20307;&#19978;&#38405;&#35835;&#20102;&#26356;&#22810;&#30340;&#25991;&#23383;&#12290;&#26356;&#24555;&#22320;&#38405;&#35835;&#20294;&#22238;&#36864;&#36739;&#22810;&#21487;&#33021;&#26159;&#32593;&#32476;&#23398;&#20064;&#36739;&#22909;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.05148</link><description>&lt;p&gt;
&#20851;&#20110;&#38405;&#35835;&#24207;&#21015;&#23545;Web&#25628;&#32034;&#20013;&#30340;&#30693;&#35782;&#33719;&#21462;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Influence of Reading Sequences on Knowledge Gain during Web Search. (arXiv:2401.05148v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38405;&#35835;&#24207;&#21015;&#23545;Web&#25628;&#32034;&#20013;&#30340;&#30693;&#35782;&#33719;&#21462;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25193;&#23637;&#38405;&#35835;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#30693;&#35782;&#33719;&#21462;&#37327;&#36739;&#39640;&#30340;&#23398;&#20064;&#32773;&#22312;&#38405;&#35835;&#19978;&#33457;&#36153;&#20102;&#26356;&#22810;&#26102;&#38388;&#65292;&#24182;&#19988;&#24635;&#20307;&#19978;&#38405;&#35835;&#20102;&#26356;&#22810;&#30340;&#25991;&#23383;&#12290;&#26356;&#24555;&#22320;&#38405;&#35835;&#20294;&#22238;&#36864;&#36739;&#22810;&#21487;&#33021;&#26159;&#32593;&#32476;&#23398;&#20064;&#36739;&#22909;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#28041;&#21450;&#21040;&#25628;&#32034;&#24341;&#25806;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;&#30456;&#20851;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#39046;&#22495;&#8220;&#25628;&#32034;&#21363;&#23398;&#20064;&#8221;&#26088;&#22312;&#29702;&#35299;&#20154;&#20204;&#22312;&#32593;&#32476;&#19978;&#30340;&#23398;&#20064;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#20960;&#31181;&#29305;&#24449;&#31867;&#21035;&#65292;&#20197;&#39044;&#27979;&#20363;&#22914;&#22312;Web&#25628;&#32034;&#36807;&#31243;&#20013;&#39044;&#26399;&#30340;&#30693;&#35782;&#33719;&#21462;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#23578;&#26410;&#23545;&#27880;&#35270;&#36861;&#36394;&#29305;&#24449;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20043;&#21069;&#20351;&#29992;&#30340;&#22522;&#20110;&#34892;&#30340;&#38405;&#35835;&#27169;&#22411;&#25193;&#23637;&#20026;&#21487;&#20197;&#26816;&#27979;&#22810;&#34892;&#38405;&#35835;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;Web&#30340;&#23398;&#20064;&#20219;&#21153;&#30340;&#30740;&#31350;&#25968;&#25454;&#65292;&#26469;&#30740;&#31350;&#25105;&#20204;&#30340;&#29305;&#24449;&#38598;&#19982;&#21442;&#19982;&#32773;&#30340;&#27979;&#35797;&#25104;&#32489;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30693;&#35782;&#33719;&#21462;&#37327;&#36739;&#39640;&#30340;&#23398;&#20064;&#32773;&#22312;&#38405;&#35835;&#19978;&#33457;&#36153;&#20102;&#26356;&#22810;&#26102;&#38388;&#65292;&#24182;&#19988;&#24635;&#20307;&#19978;&#38405;&#35835;&#20102;&#26356;&#22810;&#30340;&#25991;&#23383;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#20197;&#29306;&#29298;&#26356;&#22810;&#22238;&#36864;&#30340;&#26041;&#24335;&#26356;&#24555;&#22320;&#38405;&#35835;&#21487;&#33021;&#26159;&#32593;&#32476;&#23398;&#20064;&#36739;&#22909;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, learning increasingly involves the usage of search engines and web resources. The related interdisciplinary research field search as learning aims to understand how people learn on the web. Previous work has investigated several feature classes to predict, for instance, the expected knowledge gain during web search. Therein, eye-tracking features have not been extensively studied so far. In this paper, we extend a previously used reading model from a line-based one to one that can detect reading sequences across multiple lines. We use publicly available study data from a web-based learning task to examine the relationship between our feature set and the participants' test scores. Our findings demonstrate that learners with higher knowledge gain spent significantly more time reading, and processing more words in total. We also find evidence that faster reading at the expense of more backward regressions may be an indicator of better web-based learning. We make our code publicl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SARA&#65292;&#19968;&#20010;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#30456;&#20851;&#24615;&#35780;&#20272;&#38598;&#21512;&#65292;&#29992;&#20110;&#24320;&#21457;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#25552;&#20379;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#21516;&#26102;&#30830;&#20445;&#19981;&#36820;&#22238;&#25935;&#24863;&#24615;&#25991;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.05144</link><description>&lt;p&gt;
SARA: &#19968;&#22871;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#30456;&#20851;&#24615;&#35780;&#20272;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SARA: A Collection of Sensitivity-Aware Relevance Assessments. (arXiv:2401.05144v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SARA&#65292;&#19968;&#20010;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#30456;&#20851;&#24615;&#35780;&#20272;&#38598;&#21512;&#65292;&#29992;&#20110;&#24320;&#21457;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#25552;&#20379;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#21516;&#26102;&#30830;&#20445;&#19981;&#36820;&#22238;&#25935;&#24863;&#24615;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#26723;&#26696;&#25910;&#34255;&#65292;&#22914;&#30005;&#23376;&#37038;&#20214;&#25110;&#25919;&#24220;&#25991;&#20214;&#65292;&#24517;&#39035;&#22312;&#21521;&#20844;&#20247;&#21457;&#24067;&#20043;&#21069;&#36827;&#34892;&#20154;&#24037;&#23457;&#26597;&#65292;&#20197;&#35782;&#21035;&#20219;&#20309;&#25935;&#24863;&#20449;&#24687;&#12290;&#25935;&#24863;&#24615;&#20998;&#31867;&#24050;&#21463;&#21040;&#25991;&#29486;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#25628;&#32034;&#24341;&#25806;&#34920;&#29616;&#20986;&#20852;&#36259;&#65292;&#36825;&#26679;&#21487;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#21516;&#26102;&#30830;&#20445;&#19981;&#21521;&#29992;&#25143;&#36820;&#22238;&#25935;&#24863;&#24615;&#25991;&#20214;&#12290;&#25935;&#24863;&#24230;&#24863;&#30693;&#25628;&#32034;&#21487;&#20197;&#20943;&#36731;&#22312;&#26723;&#26696;&#20844;&#24320;&#20043;&#21069;&#36827;&#34892;&#25163;&#21160;&#25935;&#24863;&#24230;&#23457;&#26680;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#21253;&#21547;&#19968;&#32452;&#20449;&#24687;&#38656;&#27714;&#30340;&#30456;&#20851;&#24615;&#35780;&#20272;&#20197;&#21450;&#22810;&#20010;&#25935;&#24863;&#24230;&#31867;&#21035;&#30340;&#22522;&#20934;&#26631;&#31614;&#30340;&#27979;&#35797;&#38598;&#21512;&#12290;&#33879;&#21517;&#30340;&#24681;&#40857;&#30005;&#23376;&#37038;&#20214;&#25910;&#38598;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#25935;&#24863;&#20449;&#24687;&#30340;&#20998;&#31867;&#22522;&#20934;&#26631;&#31614;&#65292;&#22914;&#8220;&#32431;&#20010;&#20154;&#8221;&#21644;&#8220;&#20010;&#20154;&#20294;&#26159;&#19987;&#19994;&#30340;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large archival collections, such as email or government documents, must be manually reviewed to identify any sensitive information before the collection can be released publicly. Sensitivity classification has received a lot of attention in the literature. However, more recently, there has been increasing interest in developing sensitivity-aware search engines that can provide users with relevant search results, while ensuring that no sensitive documents are returned to the user. Sensitivity-aware search would mitigate the need for a manual sensitivity review prior to collections being made available publicly. To develop such systems, there is a need for test collections that contain relevance assessments for a set of information needs as well as ground-truth labels for a variety of sensitivity categories. The well-known Enron email collection contains a classification ground-truth that can be used to represent sensitive information, e.g., the Purely Personal and Personal but in Profes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#30740;&#31350;&#20013;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#33616;&#22120;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.04997</link><description>&lt;p&gt;
&#20026;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#21644;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis. (arXiv:2401.04997v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#30740;&#31350;&#20013;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#33616;&#22120;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19968;&#33324;&#20219;&#21153;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#24182;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22914;&#20309;&#25512;&#24191;&#21040;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#12290;&#33267;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#25512;&#33616;&#31995;&#32479;&#65292;&#25105;&#20204;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#20998;&#26512;&#20102;&#20844;&#24320;&#21487;&#29992;&#24615;&#12289;&#35843;&#20248;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25512;&#33616;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#33267;&#20110;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#22235;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components o
&lt;/p&gt;</description></item><item><title>&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#36890;&#36807;&#38598;&#25104;&#21644;&#23398;&#20064;&#22810;&#20010;&#39046;&#22495;&#30340;&#20132;&#20114;&#20449;&#24687;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20174;&#24179;&#38754;&#36716;&#21521;&#31435;&#20307;&#12290;&#25991;&#31456;&#23545;CDSR&#38382;&#39064;&#36827;&#34892;&#20102;&#23450;&#20041;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20174;&#23439;&#35266;&#21644;&#24494;&#35266;&#20004;&#20010;&#35270;&#35282;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#23545;&#20110;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#22810;&#23618;&#34701;&#21512;&#32467;&#26500;&#21644;&#34701;&#21512;&#26725;&#26753;&#12290;&#23545;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#21644;&#36741;&#21161;&#23398;&#20064;&#25216;&#26415;&#12290;&#23637;&#31034;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04971</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Cross-Domain Sequential Recommendation. (arXiv:2401.04971v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04971
&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#36890;&#36807;&#38598;&#25104;&#21644;&#23398;&#20064;&#22810;&#20010;&#39046;&#22495;&#30340;&#20132;&#20114;&#20449;&#24687;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20174;&#24179;&#38754;&#36716;&#21521;&#31435;&#20307;&#12290;&#25991;&#31456;&#23545;CDSR&#38382;&#39064;&#36827;&#34892;&#20102;&#23450;&#20041;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20174;&#23439;&#35266;&#21644;&#24494;&#35266;&#20004;&#20010;&#35270;&#35282;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#23545;&#20110;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#22810;&#23618;&#34701;&#21512;&#32467;&#26500;&#21644;&#34701;&#21512;&#26725;&#26753;&#12290;&#23545;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#21644;&#36741;&#21161;&#23398;&#20064;&#25216;&#26415;&#12290;&#23637;&#31034;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#65288;CDSR&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#31890;&#24230;&#65288;&#20174;&#24207;&#21015;&#38388;&#21040;&#24207;&#21015;&#20869;&#65292;&#20174;&#21333;&#39046;&#22495;&#21040;&#36328;&#39046;&#22495;&#65289;&#19978;&#38598;&#25104;&#21644;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#20132;&#20114;&#20449;&#24687;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20174;&#24179;&#38754;&#36716;&#21521;&#20102;&#31435;&#20307;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22235;&#32500;&#24352;&#37327;&#23450;&#20041;&#20102;CDSR&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#22810;&#32500;&#24230;&#38477;&#32500;&#19979;&#30340;&#22810;&#31867;&#22411;&#36755;&#20837;&#34920;&#31034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#25972;&#20307;&#21644;&#32454;&#33410;&#20004;&#20010;&#35270;&#35282;&#25552;&#20379;&#20102;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;&#20174;&#25972;&#20307;&#35270;&#35282;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#22810;&#23618;&#34701;&#21512;&#32467;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#34701;&#21512;&#26725;&#26753;&#12290;&#20174;&#32454;&#33410;&#35270;&#35282;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#22522;&#30784;&#25216;&#26415;&#65292;&#24182;&#35299;&#37322;&#20102;&#36741;&#21161;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#29992;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#20195;&#34920;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain sequential recommendation (CDSR) shifts the modeling of user preferences from flat to stereoscopic by integrating and learning interaction information from multiple domains at different granularities (ranging from inter-sequence to intra-sequence and from single-domain to cross-domain).In this survey, we initially define the CDSR problem using a four-dimensional tensor and then analyze its multi-type input representations under multidirectional dimensionality reductions. Following that, we provide a systematic overview from both macro and micro views. From a macro view, we abstract the multi-level fusion structures of various models across domains and discuss their bridges for fusion. From a micro view, focusing on the existing models, we specifically discuss the basic technologies and then explain the auxiliary learning technologies. Finally, we exhibit the available public datasets and the representative experimental results as well as provide some insights into future d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#20113;&#20316;&#20026;&#22270;&#24418;&#20449;&#24687;&#26816;&#32034;&#30028;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#35270;&#35273;&#24067;&#23616;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#38598;&#30340;&#35821;&#20041;&#23494;&#24230;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#31614;&#20113;&#24067;&#23616;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04947</link><description>&lt;p&gt;
&#25552;&#21319;&#26631;&#31614;&#20113;&#20316;&#20026;&#22270;&#24418;&#20449;&#24687;&#26816;&#32034;&#30028;&#38754;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Tag-Clouds as Visual Information Retrieval Interfaces. (arXiv:2401.04947v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#20113;&#20316;&#20026;&#22270;&#24418;&#20449;&#24687;&#26816;&#32034;&#30028;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#35270;&#35273;&#24067;&#23616;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#38598;&#30340;&#35821;&#20041;&#23494;&#24230;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#31614;&#20113;&#24067;&#23616;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#30340;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#26631;&#31614;&#65288;&#33258;&#36873;&#30340;&#20851;&#38190;&#35789;&#65289;&#23545;&#32593;&#32476;&#36164;&#28304;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20415;&#20197;&#21518;&#37325;&#26032;&#25214;&#21040;&#36825;&#20123;&#36164;&#28304;&#12290;&#26631;&#31614;&#20063;&#26159;&#19968;&#31181;&#31038;&#20250;&#32534;&#21046;&#32034;&#24341;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#29992;&#25143;&#20849;&#20139;&#20182;&#20204;&#30340;&#26631;&#31614;&#21644;&#36164;&#28304;&#65292;&#26500;&#24314;&#31216;&#20026;&#20113;&#35760;&#30340;&#31038;&#20250;&#26631;&#24535;&#32034;&#24341;&#12290;&#19982;&#26631;&#31614;&#20026;&#22522;&#30784;&#30340;&#31995;&#32479;&#21516;&#26102;&#27969;&#34892;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#26631;&#31614;&#20113;&#30340;&#30028;&#38754;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#26368;&#24120;&#29992;&#30340;&#26631;&#31614;&#25353;&#29031;&#23383;&#27597;&#39034;&#24207;&#23637;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26631;&#31614;&#20113;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#35270;&#35273;&#24067;&#23616;&#65292;&#26088;&#22312;&#25552;&#39640;&#27983;&#35272;&#20307;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38477;&#20302;&#20102;&#26631;&#31614;&#38598;&#30340;&#35821;&#20041;&#23494;&#24230;&#65292;&#24182;&#25913;&#21892;&#20102;&#26631;&#31614;&#20113;&#24067;&#23616;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tagging-based systems enable users to categorize web resources by means of tags (freely chosen keywords), in order to refinding these resources later. Tagging is implicitly also a social indexing process, since users share their tags and resources, constructing a social tag index, so-called folksonomy. At the same time of tagging-based system, has been popularised an interface model for visual information retrieval known as Tag-Cloud. In this model, the most frequently used tags are displayed in alphabetical order. This paper presents a novel approach to Tag-Cloud's tags selection, and proposes the use of clustering algorithms for visual layout, with the aim of improve browsing experience. The results suggest that presented approach reduces the semantic density of tag set, and improves the visual consistency of Tag-Cloud layout.
&lt;/p&gt;</description></item><item><title>DualVAE&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21452;&#37325;&#35299;&#32806;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#33021;&#22815;&#29983;&#25104;&#38544;&#24335;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04914</link><description>&lt;p&gt;
DualVAE: &#21452;&#37325;&#35299;&#32806;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation. (arXiv:2401.04914v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04914
&lt;/p&gt;
&lt;p&gt;
DualVAE&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21452;&#37325;&#35299;&#32806;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#33021;&#22815;&#29983;&#25104;&#38544;&#24335;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#31934;&#30830;&#34920;&#31034;&#20197;&#36866;&#24212;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#25968;&#25454;&#26159;&#21327;&#21516;&#36807;&#28388;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#25512;&#26029;&#32416;&#32544;&#34920;&#31034;&#20197;&#36866;&#24212;&#36825;&#31181;&#20132;&#20114;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#22810;&#26679;&#21270;&#21305;&#37197;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#23548;&#33268;&#24615;&#33021;&#21463;&#38480;&#19988;&#35299;&#37322;&#24615;&#36739;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#25512;&#33616;&#30340;&#21452;&#37325;&#35299;&#32806;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(DualVAE)&#65292;&#23558;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#19982;&#21464;&#20998;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20197;&#20419;&#36827;&#38544;&#24335;&#20132;&#20114;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#19968;&#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#21452;&#37325;&#35299;&#32806;&#21644;&#35299;&#32806;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#35299;&#32806;&#27010;&#24565;&#65292;&#25512;&#26029;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#35299;&#32806;&#28508;&#22312;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#35299;&#32806;&#34920;&#31034;&#30340;&#23545;&#24212;&#24615;&#21644;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#37051;&#36817;&#24615;&#21305;&#37197;&#25439;&#22833;&#21644;&#19968;&#20010;&#35299;&#32806;&#22240;&#23376;&#21270;KL&#25955;&#24230;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning precise representations of users and items to fit observed interaction data is the fundamental task of collaborative filtering. Existing studies usually infer entangled representations to fit such interaction data, neglecting to model the diverse matching relationships between users and items behind their interactions, leading to limited performance and weak interpretability. To address this problem, we propose a Dual Disentangled Variational AutoEncoder (DualVAE) for collaborative recommendation, which combines disentangled representation learning with variational inference to facilitate the generation of implicit interaction data. Specifically, we first implement the disentangling concept by unifying an attention-aware dual disentanglement and disentangled variational autoencoder to infer the disentangled latent representations of users and items. Further, to encourage the correspondence and independence of disentangled representations of users and items, we design a neighbo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.04858</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#24314;&#27169;&#38271;&#26102;&#38388;&#30340;&#21382;&#21490;&#35760;&#24405;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#19981;&#26029;&#28436;&#21464;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#20559;&#22909;&#29702;&#35299;&#20013;&#24314;&#27169;&#38271;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;(UEM)&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#23884;&#20837;&#24418;&#24335;&#21387;&#32553;&#21644;&#34920;&#31034;&#65292;&#23558;&#20854;&#20316;&#20026;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#26174;&#33879;&#26356;&#38271;&#30340;&#21382;&#21490;&#35760;&#24405;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#20351;&#29992;&#34920;&#31034;&#20026;&#23884;&#20837;&#30340;&#29992;&#25143;&#20449;&#21495;&#26469;&#20559;&#32622;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#36827;&#34892;&#31572;&#26696;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#36755;&#20837;&#20449;&#24687;&#65292;&#35299;&#20915;&#24459;&#24072;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#20197;&#21450;&#27861;&#24459;&#38382;&#31572;&#32593;&#31449;&#19978;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#20869;&#23481;&#30340;&#28151;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#27861;&#24459;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04852</link><description>&lt;p&gt;
&#22312;&#27861;&#24459;&#31038;&#21306;&#38382;&#31572;&#20013;&#30340;&#31572;&#26696;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Answer Retrieval in Legal Community Question Answering. (arXiv:2401.04852v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#36827;&#34892;&#31572;&#26696;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#36755;&#20837;&#20449;&#24687;&#65292;&#35299;&#20915;&#24459;&#24072;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#20197;&#21450;&#27861;&#24459;&#38382;&#31572;&#32593;&#31449;&#19978;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#20869;&#23481;&#30340;&#28151;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#27861;&#24459;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#65292;&#31572;&#26696;&#26816;&#32034;&#30340;&#20219;&#21153;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#28023;&#37327;&#30340;&#19987;&#19994;&#22238;&#31572;&#20013;&#23547;&#27714;&#30456;&#20851;&#30340;&#27861;&#24459;&#24314;&#35758;&#12290;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#38459;&#30861;&#20102;&#23558;&#29616;&#26377;&#30340;&#31572;&#26696;&#26816;&#32034;&#26041;&#27861;&#24212;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#65306;&#65288;1&#65289;&#24459;&#24072;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#20043;&#38388;&#30340;&#24040;&#22823;&#30693;&#35782;&#24046;&#36317;&#65307;&#65288;2&#65289;&#27861;&#24459;&#38382;&#31572;&#32593;&#31449;&#19978;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#20869;&#23481;&#30340;&#28151;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CE_FS&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#32467;&#26500;&#21270;&#36755;&#20837;&#30340;&#26032;&#22411;&#20132;&#21449;&#32534;&#30721;&#22120;&#65288;CE&#65289;&#37325;&#25490;&#24207;&#22120;&#12290;CE_FS&#21033;&#29992;CQA&#25968;&#25454;&#20013;&#30340;&#39069;&#22806;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#24207;&#22120;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LegalQA&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27861;&#24459;&#39046;&#22495;&#31572;&#26696;&#26816;&#32034;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;LegalQA&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#22312;MS MARCO&#19978;&#24494;&#35843;&#30340;&#24378;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#30340;&#26032;&#21457;&#29616;&#26159;&#65292;&#22312;&#38382;&#39064;&#25551;&#36848;&#20043;&#22806;&#65292;&#21152;&#20837;&#27599;&#20010;&#38382;&#39064;&#30340;&#26631;&#31614;&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of answer retrieval in the legal domain aims to help users to seek relevant legal advice from massive amounts of professional responses. Two main challenges hinder applying existing answer retrieval approaches in other domains to the legal domain: (1) a huge knowledge gap between lawyers and non-professionals; and (2) a mix of informal and formal content on legal QA websites. To tackle these challenges, we propose CE_FS, a novel cross-encoder (CE) re-ranker based on the fine-grained structured inputs. CE_FS uses additional structured information in the CQA data to improve the effectiveness of cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world benchmark dataset for evaluating answer retrieval in the legal domain. Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel finding is that adding the question tags of each question besides the question description an
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#26631;&#20934;&#26816;&#32034;&#22522;&#20934;&#24212;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#31572;&#26696;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#25506;&#32034;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#20197;&#20449;&#24687;&#26816;&#32034;&#30456;&#20851;&#24615;&#21028;&#26029;&#20316;&#20026;&#35780;&#20272;&#30340;&#38170;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.04842</link><description>&lt;p&gt;
&#23558;&#26631;&#20934;&#26816;&#32034;&#22522;&#20934;&#24212;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers. (arXiv:2401.04842v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#26631;&#20934;&#26816;&#32034;&#22522;&#20934;&#24212;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#31572;&#26696;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#25506;&#32034;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#20197;&#20449;&#24687;&#26816;&#32034;&#30456;&#20851;&#24615;&#21028;&#26029;&#20316;&#20026;&#35780;&#20272;&#30340;&#38170;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#35768;&#22810;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#32780;&#26080;&#38656;&#24341;&#29992;&#22806;&#37096;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35780;&#20272;&#36825;&#20123;&#31572;&#26696;&#30340;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#65292;&#27604;&#36739;&#19968;&#20010;&#27169;&#22411;&#19982;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#27604;&#36739;&#19968;&#20010;&#25552;&#31034;&#19982;&#21478;&#19968;&#20010;&#25552;&#31034;&#30340;&#26041;&#27861;&#30456;&#23545;&#36739;&#23569;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#31572;&#26696;&#30340;&#36136;&#37327;&#24456;&#23569;&#19982;&#26816;&#32034;&#31572;&#26696;&#30340;&#36136;&#37327;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#25552;&#31034;&#30340;&#20462;&#25913;&#65292;&#25105;&#20204;&#27809;&#26377;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#25913;&#36827;&#65292;&#38500;&#38750;&#37319;&#29992;&#26114;&#36149;&#30340;&#20154;&#20026;&#21028;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26631;&#20934;&#26816;&#32034;&#22522;&#20934;&#24212;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#21463;BERTScore&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#22522;&#20934;&#30456;&#20851;&#24615;&#21028;&#26029;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#26469;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#30456;&#20851;&#24615;&#21028;&#26029;&#22914;&#20309;&#20316;&#20026;&#35780;&#20272;&#29983;&#25104;&#31572;&#26696;&#30340;&#38170;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the gen
&lt;/p&gt;</description></item><item><title>Translate-Distill &#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32763;&#35793;&#21644;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04810</link><description>&lt;p&gt;
Translate-Distill: &#36890;&#36807;&#32763;&#35793;&#21644;&#33976;&#39311;&#23398;&#20064;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation. (arXiv:2401.04810v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04810
&lt;/p&gt;
&lt;p&gt;
Translate-Distill &#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32763;&#35793;&#21644;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#33521;&#35821;&#21333;&#35821;&#26816;&#32034;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#37327;&#26597;&#35810;-&#25991;&#26723;&#30456;&#20851;&#24615;&#21028;&#26029;&#35757;&#32451;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#21487;&#20197;&#29992;&#20316;&#25945;&#24072;&#27169;&#22411;&#26469;&#35757;&#32451;&#26356;&#39640;&#25928;&#20294;&#21516;&#26679;&#26377;&#25928;&#30340;&#21452;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#36827;&#34892;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034; (CLIR) &#26102;&#24212;&#29992;&#31867;&#20284;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26597;&#35810;&#21644;&#25991;&#26723;&#35821;&#35328;&#19981;&#21516;&#26102;&#32570;&#20047;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#38598;&#21512;&#12290;&#29616;&#26377;&#30340; CLIR &#25216;&#26415;&#20381;&#36182;&#20110;&#20174;&#24222;&#22823;&#30340;&#33521;&#35821; MS MARCO &#35757;&#32451;&#38598;&#20013;&#32763;&#35793;&#26597;&#35810;&#12289;&#25991;&#26723;&#25110;&#20004;&#32773;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; Translate-Train&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696; Translate-Distill&#65292;&#20854;&#20013;&#20174;&#21333;&#35821;&#20132;&#20114;&#32534;&#30721;&#22120;&#25110; CLIR &#20132;&#20114;&#32534;&#30721;&#22120;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120; CLIR &#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26356;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#20351;&#24471;&#25945;&#24072;&#27169;&#22411;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Prior work on English monolingual retrieval has shown that a cross-encoder trained using a large number of relevance judgments for query-document pairs can be used as a teacher to train more efficient, but similarly effective, dual-encoder student models. Applying a similar knowledge distillation approach to training an efficient dual-encoder model for Cross-Language Information Retrieval (CLIR), where queries and documents are in different languages, is challenging due to the lack of a sufficiently large training collection when the query and document languages differ. The state of the art for CLIR thus relies on translating queries, documents, or both from the large English MS MARCO training set, an approach called Translate-Train. This paper proposes an alternative, Translate-Distill, in which knowledge distillation from either a monolingual cross-encoder or a CLIR cross-encoder is used to train a dual-encoder CLIR student model. This richer design space enables the teacher model to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;LLM&#23884;&#20837;&#19982;&#38144;&#21806;&#26448;&#26009;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20379;&#32473;&#38144;&#21806;&#20154;&#21592;&#23454;&#26102;&#25512;&#33616;&#65292;&#20174;&#32780;&#25552;&#39640;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#12290;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#19968;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#21151;&#38598;&#25104;&#21040;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#27599;&#26085;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.04732</link><description>&lt;p&gt;
MSX&#38144;&#21806;&#21327;&#21516;&#21161;&#25163;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26696;&#20363;&#30740;&#31350;: &#36890;&#36807;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#25913;&#21892;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#20197;&#23454;&#29616;&#20869;&#23481;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation. (arXiv:2401.04732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;LLM&#23884;&#20837;&#19982;&#38144;&#21806;&#26448;&#26009;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20379;&#32473;&#38144;&#21806;&#20154;&#21592;&#23454;&#26102;&#25512;&#33616;&#65292;&#20174;&#32780;&#25552;&#39640;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#12290;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#19968;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#21151;&#38598;&#25104;&#21040;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#27599;&#26085;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#19987;&#38376;&#20026;&#38144;&#21806;&#20154;&#21592;&#25552;&#20379;&#26377;&#20851;&#26448;&#26009;/&#25991;&#26723;&#30340;&#23454;&#26102;&#25512;&#33616;&#65292;&#20197;&#20415;&#19982;&#23458;&#25143;&#20998;&#20139;&#25110;&#22312;&#30005;&#35805;&#20013;&#21442;&#32771;&#12290;&#36890;&#36807;&#20351;&#29992;Seismic&#38144;&#21806;&#36164;&#26009;&#30340;&#30456;&#23545;&#36739;&#22823;&#35268;&#27169;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21334;&#26041;&#26597;&#35810;&#30340;LLM&#23884;&#20837;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#35814;&#32454;&#30340;&#26041;&#24335;&#35774;&#35745;&#25552;&#31034;&#35821;&#65292;&#24182;&#21033;&#29992;&#21487;&#29992;&#30340;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#38144;&#21806;&#32773;&#20803;&#29305;&#24449;&#38598;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#24207;&#22120;&#26550;&#26500;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#20165;&#20960;&#31186;&#38047;&#20869;&#21363;&#21487;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#37096;&#32626;&#20026;&#29992;&#20110;&#23454;&#26102;&#25512;&#29702;&#30340;AML&#31471;&#28857;&#65292;&#24182;&#24050;&#38598;&#25104;&#21040;Copilot&#30028;&#38754;&#20013;&#65292;&#35813;&#30028;&#38754;&#29616;&#24050;&#37096;&#32626;&#22312;&#27599;&#26085;&#30001;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;(MSX&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we design a real-time question-answering system specifically targeted for helping sellers get relevant material/documentation they can share live with their customers or refer to during a call. Taking the Seismic content repository as a relatively large scale example of a diverse dataset of sales material, we demonstrate how LLM embeddings of sellers' queries can be matched with the relevant content. We achieve this by engineering prompts in an elaborate fashion that makes use of the rich set of meta-features available for documents and sellers. Using a bi-encoder with cross-encoder re-ranker architecture, we show how the solution returns the most relevant content recommendations in just a few seconds even for large datasets. Our recommender system is deployed as an AML endpoint for real-time inferencing and has been integrated into a Copilot interface that is now deployed in the production version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; HyperPIE &#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#36229;&#21442;&#25968;&#20449;&#24687;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#35780;&#20272;&#22810;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;BERT&#24494;&#35843;&#27169;&#22411;&#21644;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20851;&#31995;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2312.10638</link><description>&lt;p&gt;
HyperPIE: &#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#36229;&#21442;&#25968;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
HyperPIE: Hyperparameter Information Extraction from Scientific Publications. (arXiv:2312.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; HyperPIE &#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#36229;&#21442;&#25968;&#20449;&#24687;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#35780;&#20272;&#22810;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;BERT&#24494;&#35843;&#27169;&#22411;&#21644;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20851;&#31995;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35770;&#25991;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#26159;&#23454;&#29616;&#31185;&#23398;&#30693;&#35782;&#26426;&#22120;&#21487;&#35835;&#21270;&#30340;&#20851;&#38190;&#12290;&#25552;&#21462;&#20986;&#30340;&#20449;&#24687;&#21487;&#20197;&#20419;&#36827;&#23398;&#26415;&#25628;&#32034;&#12289;&#20915;&#31574;&#21046;&#23450;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12290;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#28085;&#30422;&#30340;&#19968;&#31867;&#37325;&#35201;&#20449;&#24687;&#26159;&#36229;&#21442;&#25968;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36229;&#21442;&#25968;&#20449;&#24687;&#25552;&#21462;&#65288;HyperPIE&#65289;&#24418;&#24335;&#21270;&#20026;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#28085;&#30422;&#21508;&#31181;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#31185;&#30340;&#35770;&#25991;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#24494;&#35843;&#27169;&#22411;&#20197;&#21450;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;GPT-3.5&#12289;GALACTICA&#12289;Falcon&#12289;Vicuna&#21644;WizardLM&#12290;&#23545;&#20110;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;F1&#20540;&#25552;&#21319;&#20102;29%&#12290;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;YAML&#36755;&#20986;&#36827;&#34892;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline. For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#26816;&#27979;&#30340;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#21253;&#25324;&#38382;&#39064;&#23450;&#20041;&#12289;&#24230;&#37327;&#19981;&#21305;&#37197;&#12289;&#27169;&#22411;&#29305;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.00699</link><description>&lt;p&gt;
&#23545;&#20110;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#22270;&#20687;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#26816;&#27979;&#30340;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Rethinking Detection Based Table Structure Recognition for Visually Rich Document Images. (arXiv:2312.00699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#26816;&#27979;&#30340;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#21253;&#25324;&#38382;&#39064;&#23450;&#20041;&#12289;&#24230;&#37327;&#19981;&#21305;&#37197;&#12289;&#27169;&#22411;&#29305;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#26159;&#19968;&#20010;&#24191;&#27867;&#35752;&#35770;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26080;&#32467;&#26500;&#30340;&#34920;&#26684;&#22270;&#20687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#26684;&#24335;&#65292;&#22914;HTML&#24207;&#21015;&#65292;&#20197;&#20415;&#20351;&#29992;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#36827;&#19968;&#27493;&#22788;&#29702;&#36825;&#20123;&#34920;&#26684;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#26816;&#27979;&#27169;&#22411;&#26816;&#27979;&#34920;&#26684;&#32452;&#20214;&#65288;&#20363;&#22914;&#21015;&#21644;&#34892;&#65289;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#23558;&#26816;&#27979;&#32467;&#26524;&#36716;&#25442;&#20026;HTML&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26816;&#27979;&#30340;&#27169;&#22411;&#22312;&#21333;&#20803;&#32423;&#21035;&#30340;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#24230;&#37327;&#65288;&#22914;TEDS&#65289;&#26041;&#38754;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#20123;&#27169;&#22411;&#22312;TSR&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#20339;&#30340;&#28508;&#22312;&#21407;&#22240;&#20063;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20840;&#38754;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#22522;&#20110;&#26816;&#27979;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#38382;&#39064;&#23450;&#20041;&#19981;&#24403;&#12289;&#26816;&#27979;&#21644;TSR&#24230;&#37327;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12289;&#26816;&#27979;&#27169;&#22411;&#30340;&#29305;&#24615;&#20197;&#21450;&#23616;&#37096;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table Structure Recognition (TSR) is a widely discussed task aiming at transforming unstructured table images into structured formats, such as HTML sequences, to make text-only models, such as ChatGPT, that can further process these tables. One type of solution is using detection models to detect table components, such as columns and rows, then applying a rule-based post-processing method to convert detection results into HTML sequences. However, existing detection-based models usually cannot perform as well as other types of solutions regarding cell-level TSR metrics, such as TEDS, and the underlying reasons limiting the performance of these models on the TSR task are also not well-explored. Therefore, we revisit existing detection-based models comprehensively and explore the underlying reasons hindering these models' performance, including the improper problem definition, the mismatch issue of detection and TSR metrics, the characteristics of detection models, and the impact of local
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#65288;AGMH&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21367;&#31215;&#25551;&#36848;&#31526;&#26367;&#20195;&#27880;&#24847;&#21147;&#24341;&#23548;&#29305;&#24449;&#65292;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#32454;&#24494;&#30340;&#24046;&#24322;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.06067</link><description>&lt;p&gt;
&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval. (arXiv:2311.06067v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06067
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#65288;AGMH&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21367;&#31215;&#25551;&#36848;&#31526;&#26367;&#20195;&#27880;&#24847;&#21147;&#24341;&#23548;&#29305;&#24449;&#65292;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#32454;&#24494;&#30340;&#24046;&#24322;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21704;&#24076;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#23186;&#20307;&#25628;&#32034;&#20013;&#22240;&#20854;&#20302;&#23384;&#20648;&#21644;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#25551;&#36848;&#20855;&#26377;&#30456;&#20284;&#25972;&#20307;&#22806;&#35266;&#20294;&#32454;&#24494;&#24046;&#24322;&#30340;&#23545;&#35937;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#21704;&#24076;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#12290;&#29616;&#26377;&#30340;&#21704;&#24076;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#23545;&#30456;&#21516;&#30340;&#28145;&#23618;&#28608;&#27963;&#24352;&#37327;&#36827;&#34892;&#27880;&#24847;&#21147;&#24341;&#23548;&#26469;&#29983;&#25104;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#36825;&#38480;&#21046;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#25551;&#36848;&#31526;&#26367;&#20195;&#27880;&#24847;&#21147;&#24341;&#23548;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#65288;AGMH&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#25551;&#36848;&#31526;&#20013;&#23545;&#31867;&#21035;&#29305;&#23450;&#30340;&#35270;&#35273;&#23646;&#24615;&#36827;&#34892;&#20998;&#32452;&#21644;&#23884;&#20837;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#26377;&#25928;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#30340;&#32508;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#20998;&#25955;&#25439;&#22833;&#65288;ADL&#65289;&#26469;&#24378;&#21046;&#25551;&#36848;&#31526;&#20851;&#27880;&#21508;&#31181;&#23616;&#37096;&#21306;&#22495;&#65292;&#24182;&#25429;&#25417;&#22810;&#26679;&#30340;&#32454;&#24494;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, hashing methods have been popular in the large-scale media search for low storage and strong representation capabilities. To describe objects with similar overall appearance but subtle differences, more and more studies focus on hashing-based fine-grained image retrieval. Existing hashing networks usually generate both local and global features through attention guidance on the same deep activation tensor, which limits the diversity of feature representations. To handle this limitation, we substitute convolutional descriptors for attention-guided features and propose an Attributes Grouping and Mining Hashing (AGMH), which groups and embeds the category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. Specifically, an Attention Dispersion Loss (ADL) is designed to force the descriptors to attend to various local regions and capture diverse subtle details. Moreover, we propos
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05281</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#27169;&#22411;&#30740;&#31350;&#28798;&#23475;&#21709;&#24212;&#65306;&#20197;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28798;&#23475;&#21709;&#24212;&#23545;&#21463;&#24433;&#21709;&#30340;&#31038;&#21306;&#33267;&#20851;&#37325;&#35201;&#12290;&#24212;&#24613;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#22312;&#28798;&#23475;&#26399;&#38388;&#22312;&#20102;&#35299;&#31038;&#21306;&#25152;&#38754;&#20020;&#38382;&#39064;&#30340;&#21487;&#38752;&#21644;&#21450;&#26102;&#30340;&#25351;&#26631;&#19978;&#23558;&#21463;&#30410;&#20110;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#20016;&#23500;&#25968;&#25454;&#26469;&#28304;&#12290;&#31038;&#20132;&#23186;&#20307;&#21487;&#20197;&#21453;&#26144;&#20844;&#20247;&#20851;&#27880;&#21644;&#38656;&#27714;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#20197;&#20102;&#35299;&#19981;&#26029;&#28436;&#21464;&#30340;&#24773;&#20917;&#24182;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#20027;&#39064;&#24314;&#27169;&#23545;Twitter&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26102;&#38388;-&#31354;&#38388;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;2020&#24180;&#32654;&#22269;&#35199;&#37096;&#28779;&#28798;&#23395;&#26399;&#38388;&#22312;&#19981;&#21516;&#22320;&#21306;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#8220;&#20581;&#24247;&#24433;&#21709;&#8221;&#65292;&#8220;&#25439;&#22833;&#8221;&#65292;&#8220;&#25764;&#31163;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#29702;&#35770;&#26469;&#25506;&#32034;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;&#32467;&#26524;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#20027;&#39064;&#20256;&#25773;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
&lt;/p&gt;</description></item></channel></rss>