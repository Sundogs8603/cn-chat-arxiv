<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20449;&#24687;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#32467;&#21512;&#21040;&#23376;&#22270;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.14377</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Enhanced Recommendation with User-Centric Subgraph Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20449;&#24687;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#32467;&#21512;&#21040;&#23376;&#22270;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#24403;&#20170;&#21508;&#31181;&#24179;&#21488;&#19978;&#24471;&#21040;&#24191;&#27867;&#23454;&#26045;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#21521;&#20182;&#20204;&#25512;&#33616;&#30456;&#20851;&#30340;&#29289;&#21697;&#12290;&#20381;&#36182;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#30340;&#32463;&#20856;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#29289;&#21697;&#32570;&#20047;&#20132;&#20114;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;KG&#30340;&#26041;&#27861;&#37319;&#29992;&#33410;&#28857;&#23884;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20026;&#19981;&#21516;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20063;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#29289;&#21697;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#65288;KUCNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#26377;&#25928;&#25512;&#33616;&#30340;&#23376;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;KUCNet&#20026;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#23545;&#26500;&#24314;&#19968;&#20010;U-I&#23376;&#22270;&#65292;&#35813;&#23376;&#22270;&#25429;&#33719;&#20102;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21382;&#21490;&#20449;&#24687;&#21644;KG&#20013;&#25552;&#20379;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#36827;&#20837;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14377v1 Announce Type: cross  Abstract: Recommendation systems, as widely implemented nowadays on various platforms, recommend relevant items to users based on their preferences. The classical methods which rely on user-item interaction matrices has limitations, especially in scenarios where there is a lack of interaction data for new items. Knowledge graph (KG)-based recommendation systems have emerged as a promising solution. However, most KG-based methods adopt node embeddings, which do not provide personalized recommendations for different users and cannot generalize well to the new items. To address these limitations, we propose Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning approach with graph neural network (GNN) for effective recommendation. KUCNet constructs a U-I subgraph for each user-item pair that captures both the historical information of user-item interactions and the side information provided in KG. An attention-based GNN is d
&lt;/p&gt;</description></item><item><title>FIT-RAG&#20351;&#29992;&#20107;&#23454;&#20449;&#24687;&#21644;&#20196;&#29260;&#20943;&#23569;&#35299;&#20915;&#20102;&#40657;&#21283;&#23376;RAG&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#20449;&#24687;&#24573;&#35270;&#21644;&#20196;&#29260;&#28010;&#36153;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.14374</link><description>&lt;p&gt;
FIT-RAG: &#20855;&#26377;&#20107;&#23454;&#20449;&#24687;&#21644;&#20196;&#29260;&#20943;&#23569;&#30340;&#40657;&#21283;&#23376;RAG
&lt;/p&gt;
&lt;p&gt;
FIT-RAG: Black-Box RAG with Factual Information and Token Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14374
&lt;/p&gt;
&lt;p&gt;
FIT-RAG&#20351;&#29992;&#20107;&#23454;&#20449;&#24687;&#21644;&#20196;&#29260;&#20943;&#23569;&#35299;&#20915;&#20102;&#40657;&#21283;&#23376;RAG&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#20449;&#24687;&#24573;&#35270;&#21644;&#20196;&#29260;&#28010;&#36153;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#20197;&#26356;&#26032;&#38271;&#23614;&#25110;&#36807;&#26102;&#30693;&#35782;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#24494;&#35843;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLM&#35270;&#20026;&#19968;&#20010;&#40657;&#21283;&#23376;&#65288;&#21363;&#65292;&#20923;&#32467;LLM&#30340;&#21442;&#25968;&#65289;&#24182;&#22686;&#21152;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#65292;&#21363;&#40657;&#21283;&#23376;RAG&#12290;&#26368;&#36817;&#65292;&#40657;&#21283;&#23376;RAG&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#40657;&#21283;&#23376;RAG&#26041;&#27861;&#36890;&#24120;&#24494;&#35843;&#26816;&#32034;&#22120;&#20197;&#36814;&#21512;LLMs&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20018;&#32852;&#22312;&#19968;&#36215;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;:&#65288;1&#65289;&#23545;&#20107;&#23454;&#20449;&#24687;&#30340;&#24573;&#35270;&#12290;LLM&#20559;&#22909;&#30340;&#25991;&#26723;&#21487;&#33021;&#19981;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#26816;&#32034;&#22120;&#65292;&#24182;&#25439;&#23475;&#40657;&#21283;&#23376;RAG&#30340;&#26377;&#25928;&#24615;;&#65288;2&#65289;&#20196;&#29260;&#30340;&#28010;&#36153;&#12290;&#31616;&#21333;&#22320;&#23558;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20018;&#32852;&#22312;&#19968;&#36215;&#20250;&#24102;&#26469;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14374v1 Announce Type: new  Abstract: Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brin
&lt;/p&gt;</description></item><item><title>&#25490;&#24207;&#25439;&#22833;&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#27491;&#21453;&#39304;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22823;&#30340;&#36127;&#26679;&#26412;&#26799;&#24230;&#26469;&#25913;&#21892;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14144</link><description>&lt;p&gt;
&#20102;&#35299;&#24102;&#26377;&#31232;&#30095;&#29992;&#25143;&#21453;&#39304;&#30340;&#25512;&#33616;&#25490;&#24207;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Understanding the Ranking Loss for Recommendation with Sparse User Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14144
&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#25439;&#22833;&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#27491;&#21453;&#39304;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22823;&#30340;&#36127;&#26679;&#26412;&#26799;&#24230;&#26469;&#25913;&#21892;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14144v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22312;&#22312;&#32447;&#24191;&#21578;&#39046;&#22495;&#65292;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;BCE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20294;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#23558;BCE&#25439;&#22833;&#19982;&#25490;&#24207;&#25439;&#22833;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#25439;&#22833;&#30340;&#23436;&#25972;&#21151;&#25928;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#23384;&#22312;&#31232;&#30095;&#27491;&#21453;&#39304;&#22330;&#26223;&#65288;&#22914;CTR&#39044;&#27979;&#65289;&#20013;&#19982;BCE&#25439;&#22833;&#30456;&#20851;&#30340;&#19968;&#20010;&#26032;&#25361;&#25112;&#65306;&#36127;&#26679;&#26412;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;&#25490;&#24207;&#25439;&#22833;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#36127;&#26679;&#26412;&#19978;&#29983;&#25104;&#26356;&#22823;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#20102;&#25913;&#21892;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#24471;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14144v1 Announce Type: new  Abstract: Click-through rate (CTR) prediction holds significant importance in the realm of online advertising. While many existing approaches treat it as a binary classification problem and utilize binary cross entropy (BCE) as the optimization objective, recent advancements have indicated that combining BCE loss with ranking loss yields substantial performance improvements. However, the full efficacy of this combination loss remains incompletely understood. In this paper, we uncover a new challenge associated with BCE loss in scenarios with sparse positive feedback, such as CTR prediction: the gradient vanishing for negative samples. Subsequently, we introduce a novel perspective on the effectiveness of ranking loss in CTR prediction, highlighting its ability to generate larger gradients on negative samples, thereby mitigating their optimization issues and resulting in improved classification ability. Our perspective is supported by extensive the
&lt;/p&gt;</description></item><item><title>M3&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#26816;&#32034;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14074</link><description>&lt;p&gt;
M3: &#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#23494;&#38598;&#21477;&#23376;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14074
&lt;/p&gt;
&lt;p&gt;
M3&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#26816;&#32034;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24191;&#27867;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#35768;&#22810;&#26816;&#32034;&#25968;&#25454;&#38598;&#25903;&#25345;&#21508;&#31181;&#36229;&#36234;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20294;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#22330;&#26223;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;M3&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#36882;&#24402;&#22810;&#36339;&#23494;&#38598;&#21477;&#23376;&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#26041;&#27861;&#20043;&#19978;&#65292;&#29992;&#20110;&#23494;&#38598;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;FEVER&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;: https://github.com/TonyBY/M3
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14074v1 Announce Type: cross  Abstract: In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval. However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER. Code and data are available at: https://github.com/TonyBY/M3
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#20041;&#26631;&#35760;&#33539;&#24335;&#24182;&#24341;&#20837;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;&#26041;&#27861;UIST&#65292;&#29992;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#65292;&#26088;&#22312;&#23558;&#39033;&#30446;&#20869;&#23481;&#20449;&#24687;&#25972;&#21512;&#21040;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24182;&#22312;&#20445;&#25345;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08206</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;CTR&#39044;&#27979;&#30340;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discrete Semantic Tokenization for Deep CTR Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#20041;&#26631;&#35760;&#33539;&#24335;&#24182;&#24341;&#20837;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;&#26041;&#27861;UIST&#65292;&#29992;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#65292;&#26088;&#22312;&#23558;&#39033;&#30446;&#20869;&#23481;&#20449;&#24687;&#25972;&#21512;&#21040;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24182;&#22312;&#20445;&#25345;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39033;&#30446;&#20869;&#23481;&#20449;&#24687;&#25972;&#21512;&#21040;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#22330;&#26223;&#19979;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#32422;&#26463;&#19979;&#12290;&#20256;&#32479;&#30340;&#20869;&#23481;&#32534;&#30721;&#33539;&#24335;&#23558;&#29992;&#25143;&#21644;&#39033;&#30446;&#32534;&#30721;&#22120;&#30452;&#25509;&#25972;&#21512;&#21040;CTR&#27169;&#22411;&#20013;&#65292;&#20248;&#20808;&#32771;&#34385;&#31354;&#38388;&#32780;&#38750;&#26102;&#38388;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#33539;&#24335;&#23558;&#39033;&#30446;&#21644;&#29992;&#25143;&#35821;&#20041;&#36716;&#25442;&#20026;&#28508;&#22312;&#23884;&#20837;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#32531;&#23384;&#65292;&#20248;&#20808;&#32771;&#34385;&#31354;&#38388;&#32780;&#38750;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#20041;&#26631;&#35760;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#21363;UIST&#12290;UIST&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20445;&#23432;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UIST&#23558;&#23494;&#38598;&#23884;&#20837;&#21521;&#37327;&#37327;&#21270;&#20026;&#36739;&#30701;&#30340;&#31163;&#25955;&#26631;&#35760;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#28151;&#21512;&#25512;&#26029;&#27169;&#22359;&#26469;&#34913;&#37327;&#27599;&#20010;&#29992;&#25143;-&#39033;&#30446;&#26631;&#35760;&#23545;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UIST&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08206v1 Announce Type: new  Abstract: Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news 
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#21457;&#24615;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;$O(P^2 + T^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#26080;&#29615;&#26080;&#31454;&#20105;&#33258;&#30001;&#36873;&#25321;&#24037;&#20316;&#27969;&#32593;&#20013;&#30340;&#24182;&#21457;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16097</link><description>&lt;p&gt;
&#22312;$O(P^2 + T^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#26816;&#27979;&#26080;&#29615;&#26080;&#31454;&#20105;&#33258;&#30001;&#36873;&#25321;&#24037;&#20316;&#27969;&#32593;&#20013;&#30340;&#24182;&#21457;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits: Concurrency Detection in Acyclic Sound Free-Choice Workflow Nets in $O(P^2 + T^2)$
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#21457;&#24615;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;$O(P^2 + T^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#26080;&#29615;&#26080;&#31454;&#20105;&#33258;&#30001;&#36873;&#25321;&#24037;&#20316;&#27969;&#32593;&#20013;&#30340;&#24182;&#21457;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#21457;&#24615;&#26159;Petri&#32593;&#25551;&#36848;&#21644;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#30693;&#36947;&#21738;&#20123;&#20301;&#32622;&#21644;&#21464;&#36801;&#21487;&#20197;&#24182;&#34892;&#25191;&#34892;&#26377;&#21161;&#20110;&#29702;&#35299;&#32593;&#24182;&#21551;&#29992;&#20998;&#26512;&#25216;&#26415;&#21644;&#35745;&#31639;&#20854;&#20182;&#23646;&#24615;&#65292;&#22914;&#22240;&#26524;&#20851;&#31995;&#12289;&#25490;&#20182;&#24615;&#31561;&#12290;&#26412;&#25991;&#22312;$O\big((P+T)TP^2\big)$&#30340;&#26102;&#38388;&#20869;&#20026;&#27963;&#36291;&#26377;&#30028;&#32593;&#21644;$O\big(P(P+T)^2\big)$&#30340;&#26102;&#38388;&#20869;&#20026;&#27963;&#36291;&#26377;&#30028;&#33258;&#30001;&#36873;&#25321;&#32593;&#24320;&#21457;&#20102;&#31639;&#27861;&#65292;&#35745;&#31639;&#20986;&#25152;&#26377;&#24182;&#21457;&#20301;&#32622;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#30456;&#24403;&#19981;&#38169;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#22823;&#37327;&#24182;&#21457;&#33410;&#28857;&#23545;&#21487;&#33021;&#23548;&#33268;&#38271;&#26102;&#38388;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16097v2 Announce Type: replace-cross  Abstract: Concurrency is an important aspect of Petri nets to describe and simulate the behavior of complex systems. Knowing which places and transitions could be executed in parallel helps to understand nets and enables analysis techniques and the computation of other properties, such as causality, exclusivity, etc.. All techniques based on concurrency detection depend on the efficiency of this detection methodology. Kovalyov and Esparza have developed algorithms that compute all concurrent places in $O\big((P+T)TP^2\big)$ for live and bounded nets (where $P$ and $T$ are the numbers of places and transitions) and in $O\big(P(P+T)^2\big)$ for live and bounded free-choice nets. Although these algorithms have a reasonably good computational complexity, large numbers of concurrent pairs of nodes may still lead to long computation times. This paper complements the palette of concurrency detection algorithms with the Concurrent Paths (CP) alg
&lt;/p&gt;</description></item><item><title>RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2308.14296</link><description>&lt;p&gt;
RecMind&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25512;&#33616;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RecMind: Large Language Model Powered Agent For Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14296
&lt;/p&gt;
&lt;p&gt;
RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;RS&#26041;&#27861;&#36890;&#24120;&#22312;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26032;&#25512;&#33616;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#27169;&#22411;&#35268;&#27169;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;RecMind&#65292;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21033;&#29992;&#35880;&#24910;&#35268;&#21010;&#30340;&#24037;&#20855;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Self-Inspiring&#31639;&#27861;&#26469;&#25552;&#39640;&#35268;&#21010;&#33021;&#21147;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#27493;&#39588;&#65292;LLM&#33258;&#25105;&#28608;&#21169;&#20197;&#32771;&#34385;&#25152;&#26377;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#29366;&#24577;&#26469;&#35268;&#21010;&#19979;&#19968;&#27493;&#12290;&#36825;&#19968;&#26426;&#21046;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#29702;&#35299;&#21644;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#35268;&#21010;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RecMind&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14296v2 Announce Type: replace-cross  Abstract: While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our exper
&lt;/p&gt;</description></item><item><title>TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.02094</link><description>&lt;p&gt;
TensorBank: &#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02094
&lt;/p&gt;
&lt;p&gt;
TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#20043;&#22806;&#30340;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#23384;&#20648;&#21644;&#27969;&#24335;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#25104;&#20026;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TensorBank&#65292;&#19968;&#20010;&#33021;&#22815;&#22522;&#20110;&#22797;&#26434;&#20851;&#31995;&#26597;&#35810;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#65288;COS&#65289;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#30340;&#30334;&#20159;&#32423;&#24352;&#37327;&#28246;&#20179;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#65288;HSI&#65289;&#26469;&#21152;&#36895;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20801;&#35768;&#20351;&#29992;HTTP&#33539;&#22260;&#35835;&#21462;&#26469;&#30452;&#25509;&#35775;&#38382;&#22359;&#32423;&#21035;&#30340;&#24352;&#37327;&#12290;&#19968;&#26086;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;PyTorch&#36716;&#25442;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;PyTorch&#25968;&#25454;&#38598;&#31867;&#22411;&#65292;&#37197;&#26377;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#24037;&#21378;&#65292;&#29992;&#20110;&#23558;&#20851;&#31995;&#26597;&#35810;&#21644;&#35831;&#27714;&#30340;&#36716;&#25442;&#20316;&#20026;&#19968;&#20010;&#23454;&#20363;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;HSI&#65292;&#21487;&#20197;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#22359;&#65292;&#32780;&#26080;&#38656;&#35835;&#21462;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#32034;&#24341;&#21253;&#21547;&#19981;&#21516;&#23618;&#27425;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20869;&#23481;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#26631;&#20934;&#30340;&#26377;&#20027;&#35266;&#35266;&#28857;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
&lt;/p&gt;</description></item></channel></rss>