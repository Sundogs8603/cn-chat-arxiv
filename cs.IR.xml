<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01176</link><description>&lt;p&gt;
&#20026;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#32780;&#26500;&#24314;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#28982;&#32780;&#22312;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#26500;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25104;&#20026;&#20102;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26816;&#32034;&#27169;&#22359;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25991;&#26723;&#32034;&#24341;&#65292;&#36825;&#21487;&#33021;&#19982;&#29983;&#25104;&#20219;&#21153;&#30456;&#33073;&#31163;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#26816;&#32034;&#65288;GR&#65289;&#26041;&#27861;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#26631;&#35782;&#31526;&#65288;DocIDs&#65289;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GR&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;LLMs&#22312;GR&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#12289;&#38381;&#24335;&#29983;&#25104;&#21644;RAG&#65292;&#21033;&#29992;&#22806;&#37096;&#35821;&#26009;&#22788;&#29702;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has showcased their efficacy across various domains, yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources. To improve factual accuracy of language models, retrieval-augmented generation (RAG) has emerged as a popular solution. However, traditional retrieval modules often rely on large-scale document indexes, which can be disconnected from generative tasks. Through generative retrieval (GR) approach, language models can achieve superior retrieval performance by directly generating relevant document identifiers (DocIDs). However, the relationship between GR and downstream tasks, as well as the potential of LLMs in GR, remains unexplored. In this paper, we present a unified language model that utilizes external corpus to handle various knowledge-intensive tasks by seamlessly integrating generative retrieval, closed-book generation, and RAG. In order to achieve effective retrieval and generati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DGR&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#35299;&#20915;&#20102;&#24120;&#35268;GCN-based&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04287</link><description>&lt;p&gt;
DGR&#65306;&#19968;&#31181;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#36827;&#34892;&#25512;&#33616;&#30340;&#36890;&#29992;&#22270;&#21435;&#24179;&#28369;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DGR: A General Graph Desmoothing Framework for Recommendation via Global and Local Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04287
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DGR&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#35299;&#20915;&#20102;&#24120;&#35268;GCN-based&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs)&#24050;&#32463;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#30340;&#33410;&#28857;&#20449;&#24687;&#21644;&#25299;&#25169;&#32467;&#26500;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#38754;&#20020;&#30528;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#27169;&#31946;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#20197;&#21450;&#38477;&#20302;&#30340;&#20010;&#24615;&#21270;&#12290;&#20256;&#32479;&#30340;&#21435;&#24179;&#28369;&#26041;&#27861;&#22312;&#22522;&#20110;GCN&#30340;&#31995;&#32479;&#20013;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#32570;&#20047;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DGR&#65306;&#21435;&#24179;&#28369;&#26694;&#26550;&#29992;&#20110;GCN-based&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24120;&#35268;GCN-based&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04287v1 Announce Type: new  Abstract: Graph Convolutional Networks (GCNs) have become pivotal in recommendation systems for learning user and item embeddings by leveraging the user-item interaction graph's node information and topology. However, these models often face the famous over-smoothing issue, leading to indistinct user and item embeddings and reduced personalization. Traditional desmoothing methods in GCN-based systems are model-specific, lacking a universal solution. This paper introduces a novel, model-agnostic approach named \textbf{D}esmoothing Framework for \textbf{G}CN-based \textbf{R}ecommendation Systems (\textbf{DGR}). It effectively addresses over-smoothing on general GCN-based recommendation models by considering both global and local perspectives. Specifically, we first introduce vector perturbations during each message passing layer to penalize the tendency of node embeddings approximating overly to be similar with the guidance of the global topological
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#21387;&#32553;&#35789;&#27719;&#20449;&#21495;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#12290;</title><link>http://arxiv.org/abs/2401.11248</link><description>&lt;p&gt;
&#25918;&#24323;&#35299;&#30721;&#22120;&#65306;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval. (arXiv:2401.11248v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#34955;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#36890;&#34892;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#39640;&#25928;&#21387;&#32553;&#35789;&#27719;&#20449;&#21495;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#21021;&#22987;&#21270;&#21644;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#23427;&#36890;&#24120;&#21033;&#29992;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22359;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#21387;&#32553;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39069;&#22806;&#35299;&#30721;&#22120;&#20063;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25581;&#31034;&#22686;&#24378;&#35299;&#30721;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#26222;&#36890;BERT&#26816;&#26597;&#28857;&#22312;&#36755;&#20837;&#20196;&#29260;&#30340;&#26465;&#27454;&#35206;&#30422;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20256;&#32479;MAE&#30340;&#20462;&#25913;&#65292;&#23558;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#23436;&#20840;&#31616;&#21270;&#30340;&#35789;&#34955;&#39044;&#27979;&#20219;&#21153;&#12290;&#36825;&#31181;&#20462;&#25913;&#20351;&#24471;&#35789;&#27719;&#20449;&#21495;&#33021;&#22815;&#39640;&#25928;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked auto-encoder pre-training has emerged as a prevalent technique for initializing and enhancing dense retrieval systems. It generally utilizes additional Transformer decoder blocks to provide sustainable supervision signals and compress contextual information into dense representations. However, the underlying reasons for the effectiveness of such a pre-training technique remain unclear. The usage of additional Transformer-based decoders also incurs significant computational costs. In this study, we aim to shed light on this issue by revealing that masked auto-encoder (MAE) pre-training with enhanced decoding significantly improves the term coverage of input tokens in dense representations, compared to vanilla BERT checkpoints. Building upon this observation, we propose a modification to the traditional MAE by replacing the decoder of a masked auto-encoder with a completely simplified Bag-of-Word prediction task. This modification enables the efficient compression of lexical signa
&lt;/p&gt;</description></item><item><title>RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10940</link><description>&lt;p&gt;
RELIANCE: &#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10940
&lt;/p&gt;
&lt;p&gt;
RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#27867;&#28389;&#30340;&#26102;&#20195;&#65292;&#36776;&#21035;&#26032;&#38395;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RELIANCE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#40065;&#26834;&#20449;&#24687;&#21644;&#34394;&#20551;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20808;&#36827;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#12290;RELIANCE&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#32452;&#25104;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTMs&#65289;&#12290;RELIANCE&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#38598;&#25104;&#30340;&#26234;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RELIANCE&#22312;&#21306;&#20998;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#34920;&#26126;&#20854;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#36229;&#36807;&#20102;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#25104;&#20026;&#35780;&#20272;&#20449;&#24687;&#28304;&#21487;&#38752;&#24615;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13253</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#22810;&#26679;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Context-Enhanced Diversified Recommendation. (arXiv:2310.13253v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#19968;&#30452;&#33268;&#21147;&#20110;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36861;&#27714;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#24448;&#24448;&#23548;&#33268;&#20102;&#22810;&#26679;&#24615;&#30340;&#38477;&#20302;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#23545;&#31574;&#24212;&#36816;&#32780;&#29983;&#65292;&#23558;&#22810;&#26679;&#24615;&#19982;&#20934;&#30830;&#24615;&#21516;&#31561;&#30475;&#24453;&#65292;&#24182;&#22312;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#23454;&#36341;&#32773;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#26159;&#36830;&#25509;&#23454;&#20307;&#21644;&#39033;&#30446;&#30340;&#20449;&#24687;&#24211;&#65292;&#36890;&#36807;&#21152;&#20837;&#28145;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#22686;&#21152;&#25512;&#33616;&#22810;&#26679;&#24615;&#30340;&#26377;&#21033;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#65292;&#26377;&#25928;&#22320;&#37327;&#21270;&#20102;&#30693;&#35782;&#22270;&#35889;&#39046;&#22495;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22810;&#26679;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#26469;&#25552;&#39640;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#20013;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#30340;&#25506;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#22815;&#22522;&#20110;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08803</link><description>&lt;p&gt;
&#12298;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#22312;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#12299;&#30340;&#30740;&#31350;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
An Exploration Study of Mixed-initiative Query Reformulation in Conversational Passage Retrieval. (arXiv:2307.08803v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#20013;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#30340;&#25506;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#22815;&#22522;&#20110;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;TREC Conversational Assistance Track (CAsT) 2022&#20013;&#30340;&#26041;&#27861;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22797;&#29616;&#22810;&#38454;&#27573;&#30340;&#26816;&#32034;&#31649;&#32447;&#65292;&#24182;&#25506;&#32034;&#22312;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#22330;&#26223;&#20013;&#28041;&#21450;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#30340;&#28508;&#22312;&#22909;&#22788;&#20043;&#19968;&#65306;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#12290;&#22312;&#22810;&#38454;&#27573;&#26816;&#32034;&#31649;&#32447;&#30340;&#31532;&#19968;&#20010;&#25490;&#21517;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#27169;&#22359;&#65292;&#23427;&#36890;&#36807;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#23454;&#29616;&#26597;&#35810;&#37325;&#26500;&#65292;&#20316;&#20026;&#31070;&#32463;&#37325;&#26500;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#30456;&#20851;&#30340;&#36866;&#24403;&#38382;&#39064;&#65292;&#20197;&#21450;&#21478;&#19968;&#20010;&#31639;&#27861;&#26469;&#35299;&#26512;&#29992;&#25143;&#30340;&#21453;&#39304;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#21407;&#22987;&#26597;&#35810;&#20013;&#20197;&#36827;&#34892;&#26597;&#35810;&#37325;&#26500;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22810;&#38454;&#27573;&#31649;&#32447;&#30340;&#31532;&#19968;&#20010;&#25490;&#21517;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#31232;&#30095;&#25490;&#21517;&#20989;&#25968;&#65306;BM25&#21644;&#19968;&#20010;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#65306;TCT-ColBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we report our methods and experiments for the TREC Conversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce multi-stage retrieval pipelines and explore one of the potential benefits of involving mixed-initiative interaction in conversational passage retrieval scenarios: reformulating raw queries. Before the first ranking stage of a multi-stage retrieval pipeline, we propose a mixed-initiative query reformulation module, which achieves query reformulation based on the mixed-initiative interaction between the users and the system, as the replacement for the neural reformulation method. Specifically, we design an algorithm to generate appropriate questions related to the ambiguities in raw queries, and another algorithm to reformulate raw queries by parsing users' feedback and incorporating it into the raw query. For the first ranking stage of our multi-stage pipelines, we adopt a sparse ranking function: BM25, and a dense retrieval method: TCT-ColBERT
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item></channel></rss>